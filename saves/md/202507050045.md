# Arxiv Results
## Keyword: kv cache 
 ### Less is Enough: Training-Free Video Diffusion Acceleration via   Runtime-Adaptive Caching
**Authors**: Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, Xiang Bai

**Updated**: 2025-07-03T17:59:54Z

**Summary**: Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.

**Link**: [arxiv](http://arxiv.org/abs/2507.02860v1),  [pdf](http://arxiv.org/pdf/2507.02860v1)

**Tags**: cs.CV 



### HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System
**Authors**: Kevin Song, Jiacheng Yang, Zixuan Wang, Jishen Zhao, Sihang Liu, Gennady Pekhimenko

**Updated**: 2025-07-03T17:11:28Z

**Summary**: Modern workloads are demanding increasingly larger memory capacity. Compute Express Link (CXL)-based memory tiering has emerged as a promising solution for addressing this problem by utilizing traditional DRAM alongside slow-tier CXL memory devices. We analyze prior tiering systems and observe two challenges for high-performance memory tiering: adapting to skewed but dynamically varying data hotness distributions while minimizing memory and cache overhead due to tiering.   To address these challenges, we propose HybridTier, an adaptive and lightweight tiering system for CXL memory. HybridTier tracks both long-term data access frequency and short-term access momentum \emph{simultaneously} to accurately capture and adapt to shifting hotness distributions. HybridTier reduces the metadata memory overhead by tracking data accesses \emph{probabilistically}, obtaining higher memory efficiency by trading off a small amount of tracking inaccuracy that has a negligible impact on application performance. To reduce cache overhead, HybridTier uses lightweight data structures that optimize for data locality to track data hotness. Our evaluations show that HybridTier outperforms prior systems by up to $91\%$ ($19\%$ geomean), incurring $2.0-7.8\times$ less memory overhead and $1.7-3.5\times$ less cache misses.

**Link**: [arxiv](http://arxiv.org/abs/2312.04789v2),  [pdf](http://arxiv.org/pdf/2312.04789v2)

**Tags**: cs.DC cs.OS 



### Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache   Compression
**Authors**: Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh

**Updated**: 2025-07-03T16:06:35Z

**Summary**: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.05693v3),  [pdf](http://arxiv.org/pdf/2412.05693v3)

**Tags**: cs.CL 



### OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device   Speculative Decoding
**Authors**: Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang

**Updated**: 2025-07-03T14:20:41Z

**Summary**: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2507.02659v1),  [pdf](http://arxiv.org/pdf/2507.02659v1)

**Tags**: cs.LG cs.CL 



### Skip-Vision: Efficient and Scalable Acceleration of Vision-Language   Models via Adaptive Token Skipping
**Authors**: Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan

**Updated**: 2025-07-03T08:22:27Z

**Summary**: Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. Experimental results demonstrate that Skip-Vision reduces training time by up to 35\%, inference FLOPs by 75\%, and latency by 45\%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2503.21817v3),  [pdf](http://arxiv.org/pdf/2503.21817v3)

**Tags**: cs.CV 



### Direct Reconstruction of Terahertz-driven Subcycle Electron Emission   Dynamics
**Authors**: Jiakang Mao, Yushan Zeng, Hongyang Li, Liwei Song, Ye Tian, Ruxin Li

**Updated**: 2025-07-03T07:49:18Z

**Summary**: While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At the opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 71.2% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the Fowler-Nordheim model under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics.

**Link**: [arxiv](http://arxiv.org/abs/2507.02397v1),  [pdf](http://arxiv.org/pdf/2507.02397v1)

**Tags**: physics.optics 



### Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV   Cache and Parallel Decoding
**Authors**: Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-07-03T04:51:05Z

**Summary**: Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$ throughput} improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.22618v3),  [pdf](http://arxiv.org/pdf/2505.22618v3)

**Tags**: cs.CL 



### PhysicsCorrect: A Training-Free Approach for Stable Neural PDE   Simulations
**Authors**: Xinquan Huang, Paris Perdikaris

**Updated**: 2025-07-03T01:22:57Z

**Summary**: Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5\%). The framework integrates seamlessly with diverse architectures including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.02227v1),  [pdf](http://arxiv.org/pdf/2507.02227v1)

**Tags**: cs.LG 



### Autoregressive Image Generation with Linear Complexity: A Spatial-Aware   Decay Perspective
**Authors**: Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai

**Updated**: 2025-07-02T12:27:06Z

**Summary**: Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.

**Link**: [arxiv](http://arxiv.org/abs/2507.01652v1),  [pdf](http://arxiv.org/pdf/2507.01652v1)

**Tags**: cs.CV cs.AI cs.MM 



### Learned-Database Systems Security
**Authors**: Roei Schuster, Jin Peng Zhou, Thorsten Eisenhofer, Paul Grubbs, Nicolas Papernot

**Updated**: 2025-07-02T10:16:58Z

**Summary**: A learned database system uses machine learning (ML) internally to improve performance. We can expect such systems to be vulnerable to some adversarial-ML attacks. Often, the learned component is shared between mutually-distrusting users or processes, much like microarchitectural resources such as caches, potentially giving rise to highly-realistic attacker models. However, compared to attacks on other ML-based systems, attackers face a level of indirection as they cannot interact directly with the learned model. Additionally, the difference between the attack surface of learned and non-learned versions of the same system is often subtle. These factors obfuscate the de-facto risks that the incorporation of ML carries. We analyze the root causes of potentially-increased attack surface in learned database systems and develop a framework for identifying vulnerabilities that stem from the use of ML. We apply our framework to a broad set of learned components currently being explored in the database community. To empirically validate the vulnerabilities surfaced by our framework, we choose 3 of them and implement and evaluate exploits against these. We show that the use of ML cause leakage of past queries in a database, enable a poisoning attack that causes exponential memory blowup in an index structure and crashes it in seconds, and enable index users to snoop on each others' key distributions by timing queries over their own keys. We find that adversarial ML is an universal threat against learned components in database systems, point to open research gaps in our understanding of learned-systems security, and conclude by discussing mitigations, while noting that data leakage is inherent in systems whose learned component is shared between multiple parties.

**Link**: [arxiv](http://arxiv.org/abs/2212.10318v4),  [pdf](http://arxiv.org/pdf/2212.10318v4)

**Tags**: cs.CR cs.LG 



### A new efficient RPKI Design
**Authors**: Haya Schulmann, Niklas Vogel

**Updated**: 2025-07-02T08:24:50Z

**Summary**: Resource Public Key Infrastructure (RPKI) is a critical security mechanism for BGP, but the complexity of its architecture is a growing concern as its adoption scales. Current RPKI design heavily reuses legacy PKI components, such as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols, all these introduce excessive cryptographic validation, redundant metadata, and inefficiencies in both storage and processing. We show that these design choices, although based on established standards, create significant performance bottlenecks, increase the vulnerability surface, and hinder scalability for wide-scale Internet deployment.   In this paper, we perform the first systematic analysis of the root causes of complexity in RPKI's design and experimentally quantify their real-world impact. We show that over 70% of validation time in RPKI relying parties is spent on certificate parsing and signature verification, much of it unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI), a backwards-compatible redesign that preserves all security guarantees while substantially reducing protocol overhead. iRPKI eliminates EE-certificates and ROA signatures, merges revocation and integrity objects, replaces verbose encodings with Protobuf, and restructures repository metadata for more efficient access. We experimentally demonstrate that our implementation of iRPKI in the Routinator validator achieves a 20x speed-up of processing time, 18x improvement of bandwidth requirements and 8x reduction in cache memory footprint, while also eliminating classes of vulnerabilities that have led to at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the feasibility of deploying RPKI at scale in the Internet, and especially in constrained environments. Our design may be deployed incrementally without impacting existing operations.

**Link**: [arxiv](http://arxiv.org/abs/2507.01465v1),  [pdf](http://arxiv.org/pdf/2507.01465v1)

**Tags**: cs.CR 



### EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices
**Authors**: Zheyu Shen, Yexiao He, Ziyao Wang, Yuning Zhang, Guoheng Sun, Wanghao Ye, Ang Li

**Updated**: 2025-07-02T07:47:28Z

**Summary**: Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks and memory overhead from frequent adapter swapping. Moreover, given the multiple requests in multi-tenant settings, processing requests sequentially results in underutilization of computational resources and increased latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, enabling efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly outperforms the status quo (i.e., llama.cpp) in terms of both latency and throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times boost in throughput. Even more impressively, it can serve several orders of magnitude more adapters simultaneously. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2507.01438v1),  [pdf](http://arxiv.org/pdf/2507.01438v1)

**Tags**: cs.DC cs.AI cs.LG 



### Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV   Management on a Single Commodity GPU
**Authors**: He Sun, Li Li, Mingjun Xiao, Chengzhong Xu

**Updated**: 2025-07-02T05:12:29Z

**Summary**: Advanced Large Language Models (LLMs) have achieved impressive performance across a wide range of complex and long-context natural language tasks. However, performing long-context LLM inference locally on a commodity GPU (a PC) with privacy concerns remains challenging due to the increasing memory demands of the key-value (KV) cache. Existing systems typically identify important tokens and selectively offload their KV data to GPU and CPU memory. The KV data needs to be offloaded to disk due to the limited memory on a commodity GPU, but the process is bottlenecked by token importance evaluation overhead and the disk's low bandwidth. In this paper, we present LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads. Moreover, we propose a lightweight KV abstract method, which minimizes transmission latency by storing and extracting the KV abstract of each chunk on disk instead of the full KV data. LeoAM also leverages the dynamic compression and pipeline techniques to further accelerate inference. Experimental results demonstrate that LongInfer achieves an average inference latency speedup of 3.46x, while maintaining comparable LLM response quality. In scenarios with larger batch sizes, it achieves up to a 5.47x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2506.20187v2),  [pdf](http://arxiv.org/pdf/2506.20187v2)

**Tags**: cs.OS cs.CR 68M20 C.4 



### AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design
**Authors**: Shakya Jayakody, Youpeng Zhao, Jun Wang

**Updated**: 2025-07-02T00:35:43Z

**Summary**: Graph convolutional networks (GCNs) are fundamental in various scientific applications, ranging from biomedical protein-protein interactions (PPI) to large-scale recommendation systems. An essential component for modeling graph structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As the size of graph data continues to scale up, SpGEMMs are often conducted in an out-of-core fashion due to limited GPU memory space in resource-constrained systems. Albeit recent efforts that aim to alleviate the memory constraints of out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory layout, or performing the computation in sparse format, current systems suffer from both high I/O latency and GPU under-utilization issues.   In this paper, we first identify the problems of existing systems, where sparse format data alignment and memory allocation are the main performance bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the algorithm angle, AIRES proposes to alleviate the data alignment issues on the block level for matrices in sparse formats and develops a tiling algorithm to facilitate row block-wise alignment. On the system level, AIRES employs a three-phase dynamic scheduling that features a dual-way data transfer strategy utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage (GDS), and host memory to reduce I/O latency and improve throughput. Evaluations show that AIRES significantly outperforms the state-of-the-art methods, achieving up to 1.8x lower latency in real-world graph processing benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2507.02006v1),  [pdf](http://arxiv.org/pdf/2507.02006v1)

**Tags**: cs.LG 



### PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile   Device via Additive Side-Tuning
**Authors**: Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Hao Wang, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan

**Updated**: 2025-07-01T22:27:21Z

**Summary**: There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data, labels or fine-tuned models to the server. To address those issues, we develop PAE MobiLLM, a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops a one-token (i.e., ``pivot'' token) activation shortcut that transmits only a single activation dimension instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data, labels or fine-tuned models.

**Link**: [arxiv](http://arxiv.org/abs/2507.01216v1),  [pdf](http://arxiv.org/pdf/2507.01216v1)

**Tags**: cs.LG cs.CR 



### Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model
**Authors**: Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam

**Updated**: 2025-07-01T21:27:40Z

**Summary**: Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.

**Link**: [arxiv](http://arxiv.org/abs/2506.15682v2),  [pdf](http://arxiv.org/pdf/2506.15682v2)

**Tags**: cs.CV 



### FlashDP: Private Training Large Language Models with Efficient DP-SGD
**Authors**: Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang

**Updated**: 2025-07-01T19:28:37Z

**Summary**: As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to \textbf{50\%} but also cuts down redundant computations by \textbf{20\%}, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a \textbf{90\%} throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in https://github.com/kaustpradalab/flashdp.

**Link**: [arxiv](http://arxiv.org/abs/2507.01154v1),  [pdf](http://arxiv.org/pdf/2507.01154v1)

**Tags**: cs.LG cs.CR 



### A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale   Reconstruction with External Memory
**Authors**: Felix Windisch, Lukas Radl, Thomas Köhler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger

**Updated**: 2025-07-01T18:12:43Z

**Summary**: Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.

**Link**: [arxiv](http://arxiv.org/abs/2507.01110v1),  [pdf](http://arxiv.org/pdf/2507.01110v1)

**Tags**: cs.GR cs.LG 



### Integrating nano- and micrometer-scale energy deposition models for   mechanistic prediction of radiation-induced DNA damage and cell survival
**Authors**: Giulio Bordieri, Marta Missiggia, Gianluca Lattanzi, Carmen Villagrasa, Yann Perrot, Francesco G. Cordoni

**Updated**: 2025-07-01T16:36:23Z

**Summary**: We present an integrated modeling framework that combines the Generalized Stochastic Microdosimetric Model (GSM2), used to predict cell survival fractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for simulating radiation-induced DNA damage in cell populations. This approach enables the generation of spatially and structurally resolved double-strand break (DSB) distributions, capturing key features such as damage complexity and chromosome specificity. A novel application of the DBSCAN clustering algorithm is introduced to group DSBs at the micrometer scale. This allows the identification of physical aggregates of DNA damage and their association with subnuclear domains, providing a direct link to the cell survival probability as predicted by \gsm.   The model was validated using experimental data from HUVEC cells irradiated with 220 kV X-rays and H460 cells exposed to protons over a wide linear energy transfer (LET) range, from approximately 4 keV/{\mu}m to over 20 keV/{\mu}m. Results show excellent agreement between simulations and experimental survival probabilities, making this one of the first consistent multi-scale models to bridge nanodosimetric and microdosimetric representations of radiation with biological outcomes such as cell survival.   By incorporating the inherent stochastic nature of radiation-matter interactions, this framework effectively connects the physical properties of the radiation field to the biological response at the cellular level. Its accuracy across various radiation types and energies supports its potential for use in biologically optimized radiotherapy.

**Link**: [arxiv](http://arxiv.org/abs/2507.00929v1),  [pdf](http://arxiv.org/pdf/2507.00929v1)

**Tags**: physics.bio-ph 



### VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction   and Dataflow-flexible Accelerator
**Authors**: Zhican Wang, Hongxiang Fan, Haroon Waris, Gang Wang, Zhenyu Li, Jianfei Jiang, Yanan Sun, Guanghui He

**Updated**: 2025-07-01T14:30:31Z

**Summary**: Large Language Models (LLMs) excel in natural language processing tasks but pose significant computational and memory challenges for edge deployment due to their intensive resource demands. This work addresses the efficiency of LLM inference by algorithm-hardware-dataflow tri-optimizations. We propose a novel voting-based KV cache eviction algorithm, balancing hardware efficiency and algorithm accuracy by adaptively identifying unimportant kv vectors. From a dataflow perspective, we introduce a flexible-product dataflow and a runtime reconfigurable PE array for matrix-vector multiplication. The proposed approach effectively handles the diverse dimensional requirements and solves the challenges of incrementally varying sequence lengths. Additionally, an element-serial scheduling scheme is proposed for nonlinear operations, such as softmax and layer normalization (layernorm). Results demonstrate a substantial reduction in latency, accompanied by a significant decrease in hardware complexity, from O(N) to O(1). The proposed solution is realized in a custom-designed accelerator, VEDA, which outperforms existing hardware platforms. This research represents a significant advancement in LLM inference on resource-constrained edge devices, facilitating real-time processing, enhancing data privacy, and enabling model customization.

**Link**: [arxiv](http://arxiv.org/abs/2507.00797v1),  [pdf](http://arxiv.org/pdf/2507.00797v1)

**Tags**: cs.AR 



### On Hierarchical Coded Caching with Offline Users
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-07-01T13:17:46Z

**Summary**: This paper studies a two-layer hierarchical network in which some users are offline during the content delivery phase. A two-layer hierarchical network consists of a single server connected to multiple cache-aided mirror sites, and each mirror site is connected to a distinct set of cache-aided users. A scheme for such a hierarchical system with offline users has been proposed recently but considered a special case where all mirror caches have zero memory, which is a significant limitation. We propose an array known as a hierarchical hotplug placement delivery array (HHPDA), which describes the placement and delivery phases of a coded caching scheme for a general two-layer hierarchical network with offline users. Further, we construct a class of HHPDAs using combinatorial t-designs.

**Link**: [arxiv](http://arxiv.org/abs/2507.00727v1),  [pdf](http://arxiv.org/pdf/2507.00727v1)

**Tags**: cs.IT math.IT 



### Accelerating Loading WebGraphs in ParaGrapher
**Authors**: Mohsen Koohi Esfahani

**Updated**: 2025-07-01T12:51:09Z

**Summary**: ParaGrapher is a graph loading API and library that enables graph processing frameworks to load large-scale compressed graphs with minimal overhead. This capability accelerates the design and implementation of new high-performance graph algorithms and their evaluation on a wide range of graphs and across different frameworks. However, our previous study identified two major limitations in ParaGrapher: inefficient utilization of high-bandwidth storage and reduced decompression bandwidth due to increased compression ratios. To address these limitations, we present two optimizations for ParaGrapher in this paper. To improve storage utilization, particularly for high-bandwidth storage, we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE (Filesystem in User Space). PG-Fuse optimizes storage access by increasing the size of requested blocks, reducing the number of calls to the underlying filesystem, and caching the received blocks in memory for future calls. To improve the decompression bandwidth, we introduce CompBin, a compact binary representation of the CSR format. CompBin facilitates direct accesses to neighbors while preventing storage usage for unused bytes. Our evaluation on 12 real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2507.00716v1),  [pdf](http://arxiv.org/pdf/2507.00716v1)

**Tags**: cs.DC 



### EARN: Efficient Inference Acceleration for LLM-based Generative   Recommendation by Register Tokens
**Authors**: Chaoqun Yang, Xinyu Lin, Wenjie Wang, Yongqi Li, Teng Sun, Xianjing Han, Tat-Seng Chua

**Updated**: 2025-07-01T12:42:06Z

**Summary**: Large Language Model-based generative recommendation (LLMRec) has achieved notable success, but it suffers from high inference latency due to massive computational overhead and memory pressure of KV Cache. Existing KV Cache reduction methods face critical limitations: cache compression offers marginal acceleration given recommendation tasks' short decoding steps, while prompt compression risks discarding vital interaction history. Through systematic analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1) layer-wise attention sparsity inversion where early layers retain dense informative patterns while later layers exhibit high redundancy, and 2) dual attention sinks phenomenon where attention scores concentrate on both head and tail tokens of input sequences. Motivated by these insights, we propose EARN, an efficient inference framework that leverages the early layers to compress information into register tokens placed at the input sequence boundaries, then focuses solely on these tokens in the subsequent layers. Extensive experiments on three datasets, two LLMRec methods and two LLM architectures demonstrate EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction with better accuracy than the general finetuning approach. Our work bridges the efficiency-effectiveness gap in LLMRec, offering practical deployment advantages for industrial scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2507.00715v1),  [pdf](http://arxiv.org/pdf/2507.00715v1)

**Tags**: cs.IR 



### Structural, dielectric, and ferroelectric characteristics of the   low-temperature sintered 65PMN-35PT sample for electroceramic applications
**Authors**: B. Ramachandran, N. Sudarshan, G. Mangamma, M. S. Ramachandra Rao

**Updated**: 2025-07-01T09:47:38Z

**Summary**: A single-phase 65PMN-35PT ceramic was synthesized at a relatively low temperature (875 oC) using a modified columbite method. X-ray diffraction analysis confirmed the single-phase formation of perovskite 65PMN-35PT with a tetragonal structure. Morphological studies indicated that the sample consisted of small grains with a size of about 2 micro-m. The dielectric properties of the material demonstrate its relaxor behavior near the ferroelectric transition temperature, TC = 457 K. The saturation and remnant polarization values of approximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically poled sample. Additionally, the poling induced a negative internal electric field of about -0.2 kV cm-1 was detected due to the presence of ferroelectric nano-grains in this bulk 65PMN-35PT sample. These observed characteristics of the pyrochlore-free 65PMN-35PT ceramic are similar to those of its single-crystal counterpart.

**Link**: [arxiv](http://arxiv.org/abs/2507.00614v1),  [pdf](http://arxiv.org/pdf/2507.00614v1)

**Tags**: cond-mat.mtrl-sci 



### Unleashing the Potential of All Test Samples: Mean-Shift Guided   Test-Time Adaptation
**Authors**: Jizhou Han, Chenhao Ding, SongLin Dong, Yuhang He, Xinyuan Gao, Yihong Gong

**Updated**: 2025-07-01T06:22:00Z

**Summary**: Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones. We propose MS-TTA, a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training.

**Link**: [arxiv](http://arxiv.org/abs/2507.00462v1),  [pdf](http://arxiv.org/pdf/2507.00462v1)

**Tags**: cs.CV 



### A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models
**Authors**: Yanting Miao, William Loh, Pacal Poupart, Suraj Kothawade

**Updated**: 2025-07-01T05:46:31Z

**Summary**: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.

**Link**: [arxiv](http://arxiv.org/abs/2506.12036v3),  [pdf](http://arxiv.org/pdf/2506.12036v3)

**Tags**: cs.LG cs.AI 



### RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache   Compression
**Authors**: Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov

**Updated**: 2025-06-30T19:01:18Z

**Summary**: Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme.

**Link**: [arxiv](http://arxiv.org/abs/2502.14051v2),  [pdf](http://arxiv.org/pdf/2502.14051v2)

**Tags**: cs.CL cs.LG 



### Combinatorial Multi-Access Coded Caching with Private Caches under   Intersecting Index Constraints
**Authors**: Dhruv Pratap Singh, Anjana A. Mahesh, B. Sundar Rajan

**Updated**: 2025-06-30T17:07:59Z

**Summary**: We consider the coded caching system where each user, equipped with a private cache, accesses a distinct r-subset of access caches. A central server housing a library of files populates both private and access caches using uncoded placement. In this work, we focus on a constrained indexing regime, referred to as the intersection class, in which the sets used to index the demands of each user must have a nonempty intersection. This regime models resource-limited IoT scenarios such as edge-assisted IoT systems, where devices with small private caches connect to a small number of shared caches. We provide a necessary and sufficient condition under which the system parameters fall within this intersection class. Under this condition, we propose a centralized coded caching scheme and characterize its rate-memory trade-off. Next, we define a uniform-intersection subclass and establish a condition under which the system belongs to this subclass. Within this subclass, the proposed scheme has a regular structure, with each transmission benefiting the same number of users, and we characterize its rate-memory trade-off. Additionally, we derive an index coding-based lower bound on the minimum achievable worst-case rate under uncoded placement. Finally, we provide numerical comparisons between the rate of the proposed scheme, the new lower bound, and bounds from the original work.

**Link**: [arxiv](http://arxiv.org/abs/2506.24060v1),  [pdf](http://arxiv.org/pdf/2506.24060v1)

**Tags**: cs.IT math.IT 



### Full Version: (De/Re)-Composition of Data-Parallel Computations via   Multi-Dimensional Homomorphisms
**Authors**: Ari Rasch

**Updated**: 2025-06-30T16:23:35Z

**Summary**: We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of "Multi-Dimensional Homomorphisms (MDHs)". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.

**Link**: [arxiv](http://arxiv.org/abs/2405.05118v4),  [pdf](http://arxiv.org/pdf/2405.05118v4)

**Tags**: cs.PL 



### Large-scale Neural Network Quantum States for ab initio Quantum   Chemistry Simulations on Fugaku
**Authors**: Hongtao Xu, Zibo Wu, Mingzhen Li, Weile Jia

**Updated**: 2025-06-30T12:55:59Z

**Summary**: Solving quantum many-body problems is one of the fundamental challenges in quantum chemistry. While neural network quantum states (NQS) have emerged as a promising computational tool, its training process incurs exponentially growing computational demands, becoming prohibitively expensive for large-scale molecular systems and creating fundamental scalability barriers for real-world applications. To address above challenges, we present \ours, a high-performance NQS training framework for \textit{ab initio} electronic structure calculations. First, we propose a scalable sampling parallelism strategy with multi-layers workload division and hybrid sampling scheme, which break the scalability barriers for large-scale NQS training. Then, we introduce multi-level parallelism local energy parallelism, enabling more efficient local energy computation. Last, we employ cache-centric optimization for transformer-based \textit{ansatz} and incorporate it with sampling parallelism strategy, which further speedup up the NQS training and achieve stable memory footprint at scale. Experiments demonstrate that \ours accelerate NQS training with up to 8.41x speedup and attains a parallel efficiency up to 95.8\% when scaling to 1,536 nodes.

**Link**: [arxiv](http://arxiv.org/abs/2506.23809v1),  [pdf](http://arxiv.org/pdf/2506.23809v1)

**Tags**: cs.DC 



### VQ-LLM: High-performance Code Generation for Vector Quantization   Augmented LLM Inference
**Authors**: Zihan Liu, Xinhao Luo, Junxian Guo, Wentao Ni, Yangjie Zhou, Yue Guan, Cong Guo, Weihao Cui, Yu Feng, Minyi Guo, Yuhao Zhu, Minjia Zhang, Jingwen Leng, Chen Jin

**Updated**: 2025-06-30T05:54:40Z

**Summary**: In this work, we design and implement VQ-LLM, an efficient fused Vector Quantization (VQ) kernel generation framework. We first introduce a software abstraction called codebook cache to optimize codebook access efficiency and support the integration of VQ with various computations. The codebook cache adaptively stores different entries across the GPU's memory hierarchy, including off-chip global memory, on-chip shared memory, and registers. Centered around the codebook cache, we design an efficient computation engine that optimizes memory traffic during computations involving codebooks. This compute engine adopts the codebook-centric dataflow and fusion optimizations. Additionally, we provide adaptive heuristics to tailor parameter selection in our optimizations to diverse VQ configurations. Our optimizations achieve an average latency reduction of 46.13% compared to unoptimized versions. Compared to existing open-source implementations, our methods decrease latency by 64.36% to 99.1%. A final comparison with state-of-the-art element-wise quantization methods like AWQ and KVQuant shows that our VQ-LLM is practically viable, achieving latencies close or even better latencies to those at equivalent bit-widths, potentially offering greater accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.02236v2),  [pdf](http://arxiv.org/pdf/2503.02236v2)

**Tags**: cs.DC 



### FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented   Generation
**Authors**: Zhuocheng Zhang, Yang Feng, Min Zhang

**Updated**: 2025-06-30T05:45:43Z

**Summary**: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.

**Link**: [arxiv](http://arxiv.org/abs/2506.12494v2),  [pdf](http://arxiv.org/pdf/2506.12494v2)

**Tags**: cs.CL cs.IR 



### RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM   Inference
**Authors**: Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang

**Updated**: 2025-06-30T05:21:58Z

**Summary**: The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2505.02922v2),  [pdf](http://arxiv.org/pdf/2505.02922v2)

**Tags**: cs.LG 



### Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent   Metasurfaces
**Authors**: Geng Sun, Mingzhe Fan, Lei Zhang, Hongyang Pan, Jiahui Li, Chuang Zhang, Linyao Li, Changyuan Zhao, Chau Yuen

**Updated**: 2025-06-30T03:22:32Z

**Summary**: Wireless communication systems face significant challenges in meeting the increasing demands for higher data rates and more reliable connectivity in complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for realizing wave-domain signal processing, with mobile SIMs offering superior communication performance compared to their fixed counterparts. In this paper, we investigate a novel unmanned aerial vehicle (UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the low-altitude economy (LAE) networks paradigm, where UAVs function as both base stations that cache SIM-processed data and mobile platforms that flexibly deploy SIMs to enhance uplink communications from ground users. To maximize network capacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that comprehensively addresses three critical aspects: the association between UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and the phase shifts across multiple SIM layers. Due to the inherent non-convexity and NP-hardness of USBJOP, we decompose it into three sub-optimization problems, \textit{i.e.}, association between UAV-SIMs and users optimization problem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase shifts optimization problem (USPSOP), and solve them using an alternating optimization strategy. Specifically, we transform AUUOP and ULOP into convex forms solvable by the CVX tool, while addressing USPSOP through a generative artificial intelligence (GAI)-based hybrid optimization algorithm. Simulations demonstrate that our proposed approach significantly outperforms benchmark schemes, achieving approximately 1.5 times higher network capacity compared to suboptimal alternatives. Additionally, our proposed GAI method reduces the algorithm runtime by 10\% while maintaining solution quality.

**Link**: [arxiv](http://arxiv.org/abs/2506.23488v1),  [pdf](http://arxiv.org/pdf/2506.23488v1)

**Tags**: cs.NI 



### CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors   upon GPGPU Platforms
**Authors**: Faaiq Waqar, Ming-Yen Lee, Seongwon Yoon, Seongkwang Lim, Shimeng Yu

**Updated**: 2025-06-29T21:55:58Z

**Summary**: In contemporary general-purpose graphics processing units (GPGPUs), the continued increase in raw arithmetic throughput is constrained by the capabilities of the register file (single-cycle) and last-level cache (high bandwidth), which require the delivery of operands at a cadence demanded by wide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity, density, or bandwidth of these memories can unlock substantial performance gains; however, the recent stagnation of SRAM bit-cell scaling leads to inequivalent losses in compute density.   To address the challenges posed by SRAM's scaling and leakage power consumption, this paper explores the potential CMOS+X integration of amorphous oxide semiconductor (AOS) transistors in capacitive, persistent memory topologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in multi-ported and high-bandwidth banked GPGPU memories. A detailed study of the density and energy tradeoffs of back-end-of-line (BEOL) integrated memories utilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while accounting for the macro-level limitations of integrating AOS candidate structures proposed by the device community (an aspect often overlooked in prior work). By exploiting the short lifetime of register operands, we propose a multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of the footprint of SRAM with over 70% lower standby power, enabling enhancements to compute capacity, such as larger warp sizes or processor counts. Benchmarks run on a validated NVIDIA Ampere-class GPU model, using a modified version of Accel-Sim, demonstrate improvements of up to 5.2x the performance per watt and an average 8% higher geometric mean instruction per cycle (IPC) on various compute- and memory-bound tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.23405v1),  [pdf](http://arxiv.org/pdf/2506.23405v1)

**Tags**: cs.ET cs.AR B.8.2; B.3.1 



### Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers
**Authors**: Giyong Jung, Saeid Gorgin, John Kim, Jungrae Kim

**Updated**: 2025-06-28T13:02:17Z

**Summary**: As AI models outpace the capabilities of single processors, interconnects across chips have become a critical enabler for scalable computing. These processors exchange massive amounts of data at cache-line granularity, prompting the adoption of new interconnect protocols like CXL, NVLink, and UALink, designed for high bandwidth and small payloads. However, the increasing transfer rates of these protocols heighten susceptibility to errors. While mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction (FEC) are standard for reliable data transmission, scaling chip interconnects to multi-node configurations introduces new challenges, particularly in managing silently dropped flits in switching devices. This paper introduces Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit drop detection and in-order delivery without adding header overhead. Additionally, we propose Reliability Extended Link (RXL), an extension of CXL that incorporates ISN to support scalable, reliable multi-node interconnects while maintaining compatibility with the existing flit structure. By elevating CRC to a transport-layer mechanism for end-to-end data and sequence integrity, and relying on FEC for link-layer error correction and detection, RXL delivers robust reliability and scalability without compromising bandwidth efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2507.01988v1),  [pdf](http://arxiv.org/pdf/2507.01988v1)

**Tags**: cs.NI 



### ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in   Large Language Models
**Authors**: Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren

**Updated**: 2025-06-28T07:25:12Z

**Summary**: Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.

**Link**: [arxiv](http://arxiv.org/abs/2506.22791v1),  [pdf](http://arxiv.org/pdf/2506.22791v1)

**Tags**: cs.CL cs.DB 



### PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document   Retrieval
**Authors**: Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do

**Updated**: 2025-06-28T06:24:44Z

**Summary**: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.

**Link**: [arxiv](http://arxiv.org/abs/2406.12593v4),  [pdf](http://arxiv.org/pdf/2406.12593v4)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### Efficiently Serving Large Multimodal Models Using EPD Disaggregation
**Authors**: Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-06-28T03:53:17Z

**Summary**: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.

**Link**: [arxiv](http://arxiv.org/abs/2501.05460v4),  [pdf](http://arxiv.org/pdf/2501.05460v4)

**Tags**: cs.DC cs.AI cs.CV cs.LG 



### QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,   KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization
**Authors**: Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh

**Updated**: 2025-06-27T17:10:32Z

**Summary**: Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:   (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.   Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).

**Link**: [arxiv](http://arxiv.org/abs/2506.22396v1),  [pdf](http://arxiv.org/pdf/2506.22396v1)

**Tags**: cs.CL cs.AI I.2.0; I.2.7 



### SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient   Pipeline-Parallel LLM Inference
**Authors**: Yongchao He, Bohan Zhao, Zheng Cao

**Updated**: 2025-06-27T09:27:04Z

**Summary**: As inference workloads for large language models (LLMs) scale to meet growing user demand, pipeline parallelism (PP) has become a widely adopted strategy for multi-GPU deployment, particularly in cross-node setups, to improve key-value (KV) cache capacity and inference throughput. However, PP suffers from inherent inefficiencies caused by three types of execution bubbles-load-imbalance, intra-stage, and inter-stage-which limit pipeline saturation. We present SiPipe, a heterogeneous pipeline design that improves throughput by leveraging underutilized CPU resources to offload auxiliary computation and communication. SiPipe incorporates three key techniques-CPU sampling, a token-safe execution model, and structure-aware transmission-to mitigate pipeline bubbles and improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1 times higher throughput, 43% lower per-token latency, and up to 23% higher average GPU utilization compared to the state-of-the-art vLLM under the same PP configuration, demonstrating its generality across LLMs and deployment scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2506.22033v1),  [pdf](http://arxiv.org/pdf/2506.22033v1)

**Tags**: cs.DC 



### ChunkKV: Semantic-Preserving KV Cache Compression for Efficient   Long-Context LLM Inference
**Authors**: Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, Xiaowen Chu

**Updated**: 2025-06-27T09:14:02Z

**Summary**: Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\% of total memory during inference. Although existing compression methods reduce memory by evaluating the importance of individual tokens, they overlook critical semantic relationships between tokens, resulting in fragmented context and degraded performance. We introduce ChunkKV, which fundamentally reimagines KV cache compression by treating semantic chunks - rather than isolated tokens - as basic compression units. This approach preserves complete linguistic structures and contextual integrity, ensuring that essential meaning is retained even under aggressive compression. Our innovation includes a novel layer-wise index reuse technique that exploits the higher cross-layer similarity of preserved indices in ChunkKV, reducing computational overhead and improving throughput by 26.5\%. Comprehensive evaluations on challenging benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV outperforms state-of-the-art methods by up to 8.7\% in precision while maintaining the same compression ratio. These results confirm that semantic-aware compression significantly enhances both efficiency and performance for long-context LLM inference, providing a simple yet effective solution to the memory bottleneck problem.

**Link**: [arxiv](http://arxiv.org/abs/2502.00299v3),  [pdf](http://arxiv.org/pdf/2502.00299v3)

**Tags**: cs.CL 



### A Survey of LLM Inference Systems
**Authors**: James Pan, Guoliang Li

**Updated**: 2025-06-27T04:38:20Z

**Summary**: The past few years has witnessed specialized large language model (LLM) inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside rapid LLM adoption via services like ChatGPT. Driving these system design efforts is the unique autoregressive nature of LLM request processing, motivating new techniques for achieving high performance while preserving high inference quality over high-volume and high-velocity workloads. While many of these techniques are discussed across the literature, they have not been analyzed under the framework of a complete inference system, nor have the systems themselves been analyzed and compared.   In this survey, we review these techniques, starting from operators and algorithms for request processing, then moving on to techniques for model optimization and execution, including kernel design, batching, and scheduling, before ending with techniques for memory management, including paged memory, eviction and offloading techniques, quantization, and cache persistence. Through these discussions, we show that these techniques fundamentally rely on load prediction, adaptive mechanisms, and cost reduction in order to overcome the challenges introduced by autoregressive generation and achieve the goals of the system. We then discuss how these techniques can be combined to form single-replica and multi-replica inference systems, including disaggregated inference systems that offer more control over resource allocation and serverless systems that can be deployed over shared hardware infrastructure. We end with a discussion of remaining challenges.

**Link**: [arxiv](http://arxiv.org/abs/2506.21901v1),  [pdf](http://arxiv.org/pdf/2506.21901v1)

**Tags**: cs.DB 



### Round Attention: A Novel Round-Level Attention Mechanism to Accelerate   LLM Inference
**Authors**: Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen

**Updated**: 2025-06-27T03:43:24Z

**Summary**: The increasing context window size in large language models (LLMs) has improved their ability to handle complex, long-text tasks. However, as the conversation rounds continue, it is required to store a large amount of KV cache in GPU memory, which significantly affects the efficiency and even availability of the model serving systems. This paper analyzes dialogue data from real users on the granularity of round and discovers that the LLM inference manifests a watershed layer, after which the distribution of round-level attention shows notable similarity. Based on this, we propose Round Attention - a novel round-level attention mechanism that selectively processes the KV cache of top-k relevant rounds, where k is dynamically determined through the attention matrix in the watershed layer. Theoretical analysis demonstrates that our method reduces memory usage by 54\% to 82\%, while experimental results confirm that loading sparse critical-round KV cache maintains answer accuracy without performance degradation.

**Link**: [arxiv](http://arxiv.org/abs/2502.15294v3),  [pdf](http://arxiv.org/pdf/2502.15294v3)

**Tags**: cs.CL cs.AI 



### FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual   Question Answering
**Authors**: Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn

**Updated**: 2025-06-26T18:51:04Z

**Summary**: While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and two types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

**Link**: [arxiv](http://arxiv.org/abs/2506.21710v1),  [pdf](http://arxiv.org/pdf/2506.21710v1)

**Tags**: cs.CV 



### End-to-End Long Document Summarization using Gradient Caching
**Authors**: Rohit Saxena, Hao Tang, Frank Keller

**Updated**: 2025-06-26T18:40:55Z

**Summary**: Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient $\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.

**Link**: [arxiv](http://arxiv.org/abs/2501.01805v2),  [pdf](http://arxiv.org/pdf/2501.01805v2)

**Tags**: cs.CL cs.AI 



### From Memories to Maps: Mechanisms of In-Context Reinforcement Learning   in Transformers
**Authors**: Ching Fang, Kanaka Rajan

**Updated**: 2025-06-26T17:18:54Z

**Summary**: Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.

**Link**: [arxiv](http://arxiv.org/abs/2506.19686v2),  [pdf](http://arxiv.org/pdf/2506.19686v2)

**Tags**: cs.AI 



### Measurements, simulations, and models of the point-spread function of   electron-beam lithography
**Authors**: Nikolaj B. Hougs, Kristian S. Knudsen, Marcus Albrechtsen, Taichi Suhara, Christian A. Rosiek, Søren Stobbe

**Updated**: 2025-06-26T13:22:30Z

**Summary**: When a sample is exposed using electron-beam lithography, the electrons scatter deep and far in the substrate, resulting in unwanted deposition of dose at both the nano- and the microscale. This proximity effect can be mitigated by proximity effect correction provided that accurate and validated models of the point-spread function of the electron scattering are available. Most works so far considered a double-Gaussian model of the electron point-spread function, which is very inaccurate for modern electron-beam writers with high acceleration voltages. We present measurements of the process point-spread function for chemically semi-amplified resist on silicon and indium phosphide substrates using a 150 kV electron-beam lithography system. We find that the double-Gaussian model deviates from experiments by up to four orders of magnitude. We propose instead a model comprising the sum of a power-law and a Gaussian, which is in excellent agreement with simulations of the electron scattering obtained by a Monte Carlo method. We apply the power-law plus Gaussian model to quantify the electron scattering and proximity effect correction parameters across material stacks, processing, and voltages from 5 kV to 150 kV. We find that the power-law term remains remarkably constant, whereas the long-range dose contributions and the clearing dose are significantly affected by the substrate and the acceleration voltage.

**Link**: [arxiv](http://arxiv.org/abs/2506.21236v1),  [pdf](http://arxiv.org/pdf/2506.21236v1)

**Tags**: physics.app-ph 



### Task-Aware KV Compression For Cost-Effective Long Video Understanding
**Authors**: Minghao Qin, Yan Shu, Peitian Zhang, Kun Lun, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

**Updated**: 2025-06-26T12:43:43Z

**Summary**: Long-video understanding (LVU) remains a severe challenge for existing multimodal large language models (MLLMs), primarily due to the prohibitive computational cost. Recent approaches have explored KV compression to mitigate this issue, but they often suffer from significant information loss at high compression ratios. In this paper, we introduce Video-X^2L, which flexibly preserves critical video information for each LVU task. Video-X^2L involves two key operations. The first one is called bi-level KV compression. During the MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs: low-compression KVs (L-KVs) to capture fine-grained video details and high-compression KVs (H-KVs) to offer compact video representations. The second one is called selective KV re-loading. During the MLLM's decoding stage, Video-X^2L selectively re-loads L-KVs for the most critical video chunks while using H-KVs for other less important ones. This allows the MLLM to fully utilize task-specific information while maintaining the overall compactness. Video-X^2L is simple yet effective: it is free from additional training and directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L with a variety of popular LVU benchmarks, including VideoMME, MLVU, LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L outperforms existing KV-compression methods by a huge advantage while substantially saving the computation cost.

**Link**: [arxiv](http://arxiv.org/abs/2506.21184v1),  [pdf](http://arxiv.org/pdf/2506.21184v1)

**Tags**: cs.CV cs.AI 



### LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas   and Ad-Hoc Networks
**Authors**: Atonu Ghosh, Sudip Misra

**Updated**: 2025-06-26T05:12:22Z

**Summary**: Minimal infrastructure requirements make LoRa suitable for service delivery in remote areas. Additionally, web applications have become a de-facto standard for modern service delivery. However, Long Range (LoRa) fails to enable HTTP access due to its limited bandwidth, payload size limitations, and high collisions in multi-user setups. We propose LoRaConnect to enable HTTP access over LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices connect and access HTTP resources over LoRa backhaul. It implements caching and synchronization mechanisms to address LoRa's aforementioned limitations. It also implements a message-slicing method in the application layer to overcome LoRa's payload limitations. We evaluate the proposed system using actual hardware in three experimental setups to assess the baseline performance, ideal scenario, and practical application scenario with Frequency Hopping Spread Spectrum (FHSS). Additionally, it implements a ping operation to demonstrate Internet capability and extensible nature. LoRaWeb achieves an average throughput of 1.18 KB/S approximately, with an access delay of only 1.3 S approximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves an access delay of approximately 6.7 S for a 10KB webpage in the ideal case and an average end-to-end delay of only 612 ms approximately in the FHSS-based setup. Comparison with benchmark suggests multi-fold improvement.

**Link**: [arxiv](http://arxiv.org/abs/2501.02469v3),  [pdf](http://arxiv.org/pdf/2501.02469v3)

**Tags**: cs.NI cs.CY cs.SY eess.SY 



### The electronic structures, magnetic transition and Fermi surface   instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O
**Authors**: Yuanji Xu, Huiyuan Zhang, Maoyuan Feng, Fuyang Tian

**Updated**: 2025-06-26T03:13:33Z

**Summary**: Altermagnetism has recently emerged as a distinct and fundamental class of magnetic order. Exploring its interplay with quantum phenomena such as unconventional superconductivity, density-wave instabilities, and many-body effects represents a compelling frontier. In this work, we theoretically confirm the presence of high-temperature metallic altermagnetism in KV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal transition arises from a Lifshitz transition associated with Fermi surface reconstruction. The previously reported spin-density wave gap is found to lie below the Fermi level in our study and is now recognized to be attributed to the V-shaped density of states, originating from orbital-selective and sublattice-resolved half-metal-like behavior on a specific V atom. Furthermore, we identify the instability from the nesting of spin-momentum-locked two-dimensional Fermi surfaces, which induces the SDW state. These findings position KV$_2$Se$_2$O as a promising platform for investigating the interplay among altermagnetism, unconventional superconductivity, and density-wave order.

**Link**: [arxiv](http://arxiv.org/abs/2506.20968v1),  [pdf](http://arxiv.org/pdf/2506.20968v1)

**Tags**: cond-mat.str-el 



### AirCache: Activating Inter-modal Relevancy KV Cache Compression for   Efficient Large Vision-Language Model Inference
**Authors**: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

**Updated**: 2025-06-26T01:30:43Z

**Summary**: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.23956v2),  [pdf](http://arxiv.org/pdf/2503.23956v2)

**Tags**: cs.CV cs.AI 



### Omniwise: Predicting GPU Kernels Performance with LLMs
**Authors**: Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery

**Updated**: 2025-06-25T23:36:44Z

**Summary**: In recent years, the rapid advancement of deep neural networks (DNNs) has revolutionized artificial intelligence, enabling models with unprecedented capabilities in understanding, generating, and processing complex data. These powerful architectures have transformed a wide range of downstream applications, tackling tasks beyond human reach. In this paper, we introduce Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that applies large language models (LLMs) to GPU kernel performance prediction--a novel use case in performance profiling. Omniwise is model-agnostic and lightweight, achieving strong results even with a small 3B-parameter model. It can predict key performance metrics, including memory bandwidth, cache hit rates, GFLOPs, and arithmetic intensity, directly from kernel code without the need for code execution or profiling tools. Our approach achieves over 90% of predictions within 10% relative error on GPU kernels executed on AMD MI250 and MI300X architectures. In addition to the pipeline, we develop an online inference server and a Visual Studio Code plugin that seamlessly integrate LLM-based performance prediction into developers' workflows.

**Link**: [arxiv](http://arxiv.org/abs/2506.20886v1),  [pdf](http://arxiv.org/pdf/2506.20886v1)

**Tags**: cs.LG cs.AI 



### A3 : an Analytical Low-Rank Approximation Framework for Attention
**Authors**: Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao

**Updated**: 2025-06-25T23:03:54Z

**Summary**: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.12942v3),  [pdf](http://arxiv.org/pdf/2505.12942v3)

**Tags**: cs.CL cs.AI cs.LG 



### Generative Blocks World: Moving Things Around in Pictures
**Authors**: Vaibhav Vavilala, Seemandhar Jain, Rahul Vasanth, D. A. Forsyth, Anand Bhattad

**Updated**: 2025-06-25T17:59:55Z

**Summary**: We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization.

**Link**: [arxiv](http://arxiv.org/abs/2506.20703v1),  [pdf](http://arxiv.org/pdf/2506.20703v1)

**Tags**: cs.GR cs.CV 



### Semantic Caching for Improving Web Affordability
**Authors**: Hafsa Akbar, Danish Athar, Muhammad Ayain Fida Rana, Chaudhary Hammad Javed, Zartash Afzal Uzmi, Ihsan Ayyub Qazi, Zafar Ayyub Qazi

**Updated**: 2025-06-25T13:35:25Z

**Summary**: The rapid growth of web content has led to increasingly large webpages, posing significant challenges for Internet affordability, especially in developing countries where data costs remain prohibitively high. We propose semantic caching using Large Language Models (LLMs) to improve web affordability by enabling reuse of semantically similar images within webpages. Analyzing 50 leading news and media websites, encompassing 4,264 images and over 40,000 image pairs, we demonstrate potential for significant data transfer reduction, with some website categories showing up to 37% of images as replaceable. Our proof-of-concept architecture shows users can achieve approximately 10% greater byte savings compared to exact caching. We evaluate both commercial and open-source multi-modal LLMs for assessing semantic replaceability. GPT-4o performs best with a low Normalized Root Mean Square Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA 3.1 model shows comparable performance, highlighting its viability for large-scale applications. This approach offers benefits for both users and website operators, substantially reducing data transmission. We discuss ethical concerns and practical challenges, including semantic preservation, user-driven cache configuration, privacy concerns, and potential resistance from website operators

**Link**: [arxiv](http://arxiv.org/abs/2506.20420v1),  [pdf](http://arxiv.org/pdf/2506.20420v1)

**Tags**: cs.NI F.2.2, I.2.7 



### Do cell culturing influence the radiosensitizing effect of gold   nanoparticles part 2: scrutinizing the methodology producing recent evidence
**Authors**: Hans Rabus, Oswald Msosa Mkanda

**Updated**: 2025-06-25T09:44:25Z

**Summary**: When irradiation is performed with gold nanoparticles (AuNPs), a different shape of cells in suspension or adherent to walls may result in different probability of cell survival. In a recent study, differences of up to a factor of 2 were found between the predicted survival of floating and adherent cells. The present work aims to quantify the biases introduced by the simulation setup and the use of voxelized geometry in conjunction with the local effect model for cell survival. The results show that simulated irradiation of a cell near the surface with an incident beam matched to the cell dimensions results in dose values that are by a factor of about 50 lower than the dose to cells deeper in the medium when irradiated with a Co-60 spectrum and lateral beam dimensions in the centimeter range. Furthermore, the number of ionizing photon interactions in gold nanoparticles in a cell near the surface is lower by a factor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose in voxels of size in the order of 200 nm for assessing cell survival with the local effect model (LEM) leads to an underestimation of the number of lesions from a single ionized AuNP by roughly two orders of magnitude and thus to an overestimation of cell survival. The effect of cell geometry on the survival rate was examined for approximate cell geometries and 100 kV x-ray irradiation, for which the probability of photon interaction in gold nanoparticles is by more than two orders of magnitude higher than for Co-60 irradiation. The results show that the effects are negligible for 5 nm nanoparticles at the concentration of AuNPs considered in preceding work. For 50 nm nanoparticles and thus a thousand times higher mass fraction of gold, significant reduction in cell survival is found, with a clear additional reduction predicted by the LEM as compared to the prediction based on mean dose to the nucleus.

**Link**: [arxiv](http://arxiv.org/abs/2506.20283v1),  [pdf](http://arxiv.org/pdf/2506.20283v1)

**Tags**: physics.med-ph 



### MegaFold: System-Level Optimizations for Accelerating Protein Structure   Prediction Models
**Authors**: Hoa La, Ahan Gupta, Alex Morehead, Jianlin Cheng, Minjia Zhang

**Updated**: 2025-06-24T23:30:49Z

**Summary**: Protein structure prediction models such as AlphaFold3 (AF3) push the frontier of biomolecular modeling by incorporating science-informed architectural changes to the transformer architecture. However, these advances come at a steep system cost, introducing: compute- and memory-intensive operators, 2D attention mechanisms, and retrieval-augmented data pipelines, which collectively hinder the scalability of AF3 training. In this work, we present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle time from the retrieval-augmented data pipeline, Triton-based kernels for memory-efficient EvoAttention on heterogeneous devices, and deep fusion for common and critical small operators in AF3. Evaluation on both NVIDIA H200 and AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by up to 1.23$\times$ and improves per-iteration training time by up-to 1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines without running out-of-memory, significantly improving the scalability of modern protein folding models. We open source our code at https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

**Link**: [arxiv](http://arxiv.org/abs/2506.20686v1),  [pdf](http://arxiv.org/pdf/2506.20686v1)

**Tags**: q-bio.BM cs.DC cs.LG cs.PF 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hoßfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-06-24T19:02:08Z

**Summary**: As AI workloads drive soaring memory requirements, higher-density on-chip memory is needed for domain-specific accelerators beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, little work has incorporated dynamic application profiles into these design decisions, and no existing tools are expressly designed for this purpose. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and data lifetimes in domain-specific accelerators. By instrumenting retargetable architectural simulator backends with application- and device-agnostic analytical frontends, GainSight aligns workload-specific traffic and lifetime metrics with mockups of emerging memory devices, informing system-level heterogeneous memory design. We also present a set of case studies on MLPerf Inference and PolyBench workloads using simulated GPU and systolic array architectures, highlighting the utility of GainSight and the insights it provides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory arrays that augment SRAM with GCRAM can reduce active energy consumption by up to 66.8%. To facilitate further research in this domain, GainSight is open source at https://gainsight.stanford.edu/.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v4),  [pdf](http://arxiv.org/pdf/2504.14866v4)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### CronusVLA: Transferring Latent Motion Across Time for Multi-Frame   Prediction in Manipulation
**Authors**: Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang, Xinyi Chen, Hanqing Wang, Tai Wang, Feng Zhao, Dahua Lin, Jiangmiao Pang

**Updated**: 2025-06-24T17:30:27Z

**Summary**: Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage. CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention. By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference. As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world Franka experiments also show the strong performance and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2506.19816v1),  [pdf](http://arxiv.org/pdf/2506.19816v1)

**Tags**: cs.RO cs.CV 



### RCStat: A Statistical Framework for using Relative Contextualization in   Transformers
**Authors**: Debabrata Mahapatra, Shubham Agarwal, Apoorv Saxena, Subrata Mitra

**Updated**: 2025-06-24T11:55:43Z

**Summary**: Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.

**Link**: [arxiv](http://arxiv.org/abs/2506.19549v1),  [pdf](http://arxiv.org/pdf/2506.19549v1)

**Tags**: cs.CL cs.AI cs.LG 



### AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in   Large Language Models
**Authors**: Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu

**Updated**: 2025-06-24T10:45:48Z

**Summary**: Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models (LLMs). Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. We observe that quantizing the KV cache of different tokens has varying impacts on the quality of attention outputs. To systematically investigate this phenomenon, we perform forward error propagation analysis on attention and propose the Anchor Score (AnS) that quantifies the sensitivity of each token's KV cache to quantization-induced error. Our analysis reveals significant disparities in AnS across tokens, suggesting that preserving a small subset with full precision (FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive quantization scenarios. Based on this insight, we introduce AnTKV, a novel framework that leverages Anchor Token-aware Vector Quantization to compress the KV cache. Furthermore, to support efficient deployment, we design and develop a triton kernel that is fully compatible with FlashAttention, enabling fast online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x higher decoding throughput compared to the FP16 baseline. Our experiment results demonstrate that AnTKV matches or outperforms prior works such as KIVI, SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves significantly lower perplexity under ultra-low-bit quantization on Mistral-7B, with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of 4.73.

**Link**: [arxiv](http://arxiv.org/abs/2506.19505v1),  [pdf](http://arxiv.org/pdf/2506.19505v1)

**Tags**: cs.CL 



### Mixture of Cache-Conditional Experts for Efficient Mobile Device   Inference
**Authors**: Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski, Mart van Baalen, Markus Nagel, Paul Whatmough, Babak Ehteshami Bejnordi

**Updated**: 2025-06-24T09:27:46Z

**Summary**: Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or "experts" for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$\times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.00099v2),  [pdf](http://arxiv.org/pdf/2412.00099v2)

**Tags**: cs.LG cs.AI cs.AR 



### Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments   with a Hierarchical Spatial-Cognition Long-Short Memory System
**Authors**: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li

**Updated**: 2025-06-24T09:00:43Z

**Summary**: Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.

**Link**: [arxiv](http://arxiv.org/abs/2506.19433v1),  [pdf](http://arxiv.org/pdf/2506.19433v1)

**Tags**: cs.CV cs.AI cs.CL 



### PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning
**Authors**: Duong Bach

**Updated**: 2025-06-24T06:44:47Z

**Summary**: The proliferation of multi-agent systems (MAS) in complex, dynamic environments necessitates robust and efficient mechanisms for managing shared knowledge. A critical challenge is ensuring that distributed memories remain synchronized, relevant, and free from the accumulation of outdated or inconsequential data - a process analogous to biological forgetting. This paper introduces the Co-Forgetting Protocol, a novel, comprehensive framework designed to address this challenge by enabling synchronized memory pruning in MAS. The protocol integrates three key components: (1) context-aware semantic voting, where agents utilize a lightweight DistilBERT model to assess the relevance of memory items based on their content and the current operational context; (2) multi-scale temporal decay functions, which assign diminishing importance to memories based on their age and access frequency across different time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based consensus mechanism, ensuring that decisions to retain or discard memory items are agreed upon by a qualified and fault-tolerant majority of agents, even in the presence of up to f Byzantine (malicious or faulty) agents in a system of N greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient inter-agent communication and Pinecone for scalable vector embedding storage and similarity search, with SQLite managing metadata. Experimental evaluations in a simulated MAS environment with four agents demonstrate the protocol's efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88% voting accuracy in forgetting decisions against human-annotated benchmarks, a 92% PBFT consensus success rate under simulated Byzantine conditions, and an 82% cache hit rate for memory access.

**Link**: [arxiv](http://arxiv.org/abs/2506.17338v2),  [pdf](http://arxiv.org/pdf/2506.17338v2)

**Tags**: cs.DC cs.AI cs.MA 



### Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV   Sparsification
**Authors**: Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

**Updated**: 2025-06-24T01:19:56Z

**Summary**: Multi-modal large language models (MLLMs) models have made significant progress in video understanding over the past few years. However, processing long video inputs remains a major challenge due to high memory and computational costs. This makes it difficult for current models to achieve both strong performance and high efficiency in long video understanding. To address this challenge, we propose Video-XL-2, a novel MLLM that delivers superior cost-effectiveness for long-video understanding based on task-aware KV sparsification. The proposed framework operates with two key steps: chunk-based pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides the visual token sequence into chunks, applying full attention within each chunk and sparse attention across chunks. This significantly reduces computational and memory overhead. During decoding, bi-level key-value decoding selectively reloads either dense or sparse key-values for each chunk based on its relevance to the task. This approach further improves memory efficiency and enhances the model's ability to capture fine-grained information. Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks, outperforming existing open-source lightweight models. It also demonstrates exceptional efficiency, capable of processing over 10,000 frames on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few seconds.

**Link**: [arxiv](http://arxiv.org/abs/2506.19225v1),  [pdf](http://arxiv.org/pdf/2506.19225v1)

**Tags**: cs.CV cs.AI 



### Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices   and Tensors
**Authors**: Benjamin Brock, Willow Ahrens, Hameer Abbasi, Timothy A. Davis, Juni Kim, James Kitchen, Spencer Patty, Isaac Virshup, Erik Welch

**Updated**: 2025-06-23T22:33:58Z

**Summary**: Sparse matrices and tensors are ubiquitous throughout multiple subfields of computing. The widespread usage of sparse data has inspired many in-memory and on-disk storage formats, but the only widely adopted storage specifications are the Matrix Market and FROSTT file formats, which both use ASCII text. Due to the inefficiency of text storage, these files typically have larger file sizes and longer parsing times than binary storage formats, which directly store an in-memory representation to disk. This can be a major bottleneck; since sparse computation is often bandwidth-bound, the cost of loading or storing a matrix to disk often exceeds the cost of performing a sparse computation. While it is common practice for practitioners to develop their own, custom, non-portable binary formats for high-performance sparse matrix storage, there is currently no cross-platform binary sparse matrix storage format. We present Binsparse, a cross-platform binary sparse matrix and tensor format specification. Binsparse is a modular, embeddable format, consisting of a JSON descriptor, which describes the matrix or tensor dimensions, type, and format, and a series of binary arrays, which can be stored in all modern binary containers, such as HDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse spanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our Binsparse format on every matrix in the SuiteSparse Matrix Collection and a selection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format shows file size reductions of 2.4x on average without compression and 7.5x with compression. We evaluate our parser's read/write performance against a state-of-the-art Matrix Market parser, demonstrating warm cache mean read speedups of 26.5x without compression and 2.6x with compression, and write speedups of 31x without compression and 1.4x with compression.

**Link**: [arxiv](http://arxiv.org/abs/2506.19175v1),  [pdf](http://arxiv.org/pdf/2506.19175v1)

**Tags**: cs.MS cs.DC cs.DS 



### CommVQ: Commutative Vector Quantization for KV Cache Compression
**Authors**: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

**Updated**: 2025-06-23T17:50:11Z

**Summary**: Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

**Link**: [arxiv](http://arxiv.org/abs/2506.18879v1),  [pdf](http://arxiv.org/pdf/2506.18879v1)

**Tags**: cs.CL cs.AI 



### Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo
**Authors**: Minas Karamanis, Uroš Seljak

**Updated**: 2025-06-23T07:59:17Z

**Summary**: Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian inference but suffer from high computational costs due to their reliance on large particle ensembles for accurate estimates. We introduce persistent sampling (PS), an extension of SMC that systematically retains and reuses particles from all prior iterations to construct a growing, weighted ensemble. By leveraging multiple importance sampling and resampling from a mixture of historical distributions, PS mitigates the need for excessively large particle counts, directly addressing key limitations of SMC such as particle impoverishment and mode collapse. Crucially, PS achieves this without additional likelihood evaluations-weights for persistent particles are computed using cached likelihood values. This framework not only yields more accurate posterior approximations but also produces marginal likelihood estimates with significantly lower variance, enhancing reliability in model comparison. Furthermore, the persistent ensemble enables efficient adaptation of transition kernels by leveraging a larger, decorrelated particle pool. Experiments on high-dimensional Gaussian mixtures, hierarchical models, and non-convex targets demonstrate that PS consistently outperforms standard SMC and related variants, including recycled and waste-free SMC, achieving substantial reductions in mean squared error for posterior expectations and evidence estimates, all at reduced computational cost. PS thus establishes itself as a robust, scalable, and efficient alternative for complex Bayesian inference tasks.

**Link**: [arxiv](http://arxiv.org/abs/2407.20722v3),  [pdf](http://arxiv.org/pdf/2407.20722v3)

**Tags**: stat.ML cs.LG stat.CO 



### FutureFill: Fast Generation from Convolutional Sequence Models
**Authors**: Naman Agarwal, Xinyi Chen, Evan Dogariu, Devan Shah, Hubert Strauss, Vlad Feinberg, Daniel Suo, Peter Bartlett, Elad Hazan

**Updated**: 2025-06-23T03:20:46Z

**Summary**: We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill, a general-purpose fast generation method for any sequence prediction algorithm based on convolutional operators. FutureFill reduces generation time from quadratic to quasilinear in the context length. Moreover, when generating from a prompt, it requires a prefill cache whose size grows only with the number of tokens to be generated, often much smaller than the caches required by standard convolutional or attention based models. We validate our theoretical claims with experiments on synthetic tasks and demonstrate substantial efficiency gains when generating from a deep convolutional sequence prediction model.

**Link**: [arxiv](http://arxiv.org/abs/2410.03766v3),  [pdf](http://arxiv.org/pdf/2410.03766v3)

**Tags**: cs.LG cs.AI cs.CL 



### RAPID: Long-Context Inference with Retrieval-Augmented Speculative   Decoding
**Authors**: Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh

**Updated**: 2025-06-23T03:05:26Z

**Summary**: The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.

**Link**: [arxiv](http://arxiv.org/abs/2502.20330v2),  [pdf](http://arxiv.org/pdf/2502.20330v2)

**Tags**: cs.CL 



### Make It Efficient: Dynamic Sparse Attention for Autoregressive Image   Generation
**Authors**: Xunzhi Xiang, Qi Fan

**Updated**: 2025-06-23T01:27:06Z

**Summary**: Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency and those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately $50\%$. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18226v1),  [pdf](http://arxiv.org/pdf/2506.18226v1)

**Tags**: cs.CV cs.AI 



### Fast Clifford Neural Layers
**Authors**: Tianxiang Xia, Max Neuwinger, Lin Xiao

**Updated**: 2025-06-22T20:43:42Z

**Summary**: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra into neural networks. In this project we focus on optimizing the inference of 2/3D Clifford convolutional layers and multivector activation layers for one core CPU performance.   Overall, by testing on a real network block involving Clifford convolutional layers and multivector activation layers, we observe that our implementation is 30% faster than standard PyTorch implementation in relatively large data + network size (>L2 cache).   We open source our code base at https://github.com/egretwAlker/c-opt-clifford-layers

**Link**: [arxiv](http://arxiv.org/abs/2507.01040v1),  [pdf](http://arxiv.org/pdf/2507.01040v1)

**Tags**: cs.LG cs.AI cs.NE cs.PF 



### Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the   Limits of Embedding Space Capacity
**Authors**: Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev

**Updated**: 2025-06-22T15:07:37Z

**Summary**: A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.

**Link**: [arxiv](http://arxiv.org/abs/2502.13063v3),  [pdf](http://arxiv.org/pdf/2502.13063v3)

**Tags**: cs.CL cs.LG 



### Secure User-friendly Blockchain Modular Wallet Design Using Android &   OP-TEE
**Authors**: Seongjin Kim, Sanguk Yun, Jungho Jang

**Updated**: 2025-06-22T10:57:57Z

**Summary**: Emerging crypto economies still hemorrhage digital assets because legacy wallets leak private keys at almost every layer of the software stack, from user-space libraries to kernel memory dumps. This paper solves that twin crisis of security and interoperability by re-imagining key management as a platform-level service anchored in ARM TrustZone through OP-TEE. Our architecture fractures the traditional monolithic Trusted Application into per-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's single-binary ceiling. A cryptographically sealed firmware-over-the-air pipeline welds each TA set to an Android system image, enabling hot-swap updates while Verified Boot enforces rollback protection. Every package carries a chained signature developer first, registry second so even a compromised supply chain cannot smuggle malicious code past the Secure World's RSA-PSS gatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and GP-compliant crypto APIs ensure secrets never bleed across trust boundaries or timing domains. The Rich Execution Environment can interact only via hardware-mediated Secure Monitor Calls, collapsing the surface exposed to malware in Android space. End-users enjoy a single polished interface yet can install or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap, shrinking both storage footprint and audit scope. For auditors, the composition model slashes duplicated verification effort by quarantining blockchain logic inside narrowly scoped modules that share formally specified interfaces. Our threat analysis spans six adversary layers and shows how the design neutralizes REE malware sniffing, OTA injection, and cross-module side channels without exotic hardware. A reference implementation on AOSP exports a Wallet Manager HAL, custom SELinux domains, and a CI/CD pipeline that vet community modules before release. The result is not merely another hardware wallet but a programmable substrate that can evolve at the velocity of the blockchain ecosystem. By welding radical extensibility to hardware-anchored assurance, the platform closes the security-usability gap that has long stymied mass-market self-custody. We posit that modular TEEs are the missing OS primitive for Web3, much as virtual memory unlocked multi-tasking in classical computing. Together, these contributions sketch a blueprint for multi-chain asset management that is auditable, resilient, and poised for global deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.17988v1),  [pdf](http://arxiv.org/pdf/2506.17988v1)

**Tags**: cs.CR 



### ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training
**Authors**: Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu

**Updated**: 2025-06-22T03:46:11Z

**Summary**: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.17331v2),  [pdf](http://arxiv.org/pdf/2505.17331v2)

**Tags**: cs.LG cs.CL 



### RPLKG: Robust Prompt Learning with Knowledge Graph
**Authors**: YongTaek Lim, Yewon Kim, Suho Kang, Dokyung Yoon, KyungWoo Song

**Updated**: 2025-06-21T08:27:10Z

**Summary**: Large-scale pre-trained models surpass in transferability and robust generalization across diverse datasets. The emergence of multimodal pre-trained models like CLIP has significantly boosted performance in various experiments. However, generalizing to new datasets or domains remains challenging, especially with limited labeled data. Also, existing methods often lack interpretability and impose high computational costs. To address this, we propose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the knowledge graph to curate diverse, interpretable prompt sets automatically. Our method autonomously selects the optimal interpretable prompt based on dataset characteristics, achieving performance improvements over zero-shot learning and competitive performance compared to various prompt learning methods. Also, RPLKG efficiently reuses cached prompt embeddings from a single model pass and optimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast training. Moreover, RPLKG advances few-shot learning effectiveness while enhancing interpretability and efficiency in model adaptation. Our

**Link**: [arxiv](http://arxiv.org/abs/2304.10805v2),  [pdf](http://arxiv.org/pdf/2304.10805v2)

**Tags**: cs.AI cs.LG 



### Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context   LMs?
**Authors**: Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, Danqi Chen

**Updated**: 2025-06-20T16:21:12Z

**Summary**: Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. In this paper, we propose the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. We evaluate methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. We adapt these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. We then turn to *recency eviction* methods, wherein we propose PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. Our paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.

**Link**: [arxiv](http://arxiv.org/abs/2506.17121v1),  [pdf](http://arxiv.org/pdf/2506.17121v1)

**Tags**: cs.CL 



### PUL: Pre-load in Software for Caches Wouldn't Always Play Along
**Authors**: Arthur Bernhardt, Sajjad Tamimi, Florian Stock, Andreas Koch, Ilia Petrov

**Updated**: 2025-06-20T13:09:26Z

**Summary**: Memory latencies and bandwidth are major factors, limiting system performance and scalability. Modern CPUs aim at hiding latencies by employing large caches, out-of-order execution, or complex hardware prefetchers. However, software-based prefetching exhibits higher efficiency, improving with newer CPU generations.   In this paper we investigate software-based, post-Moore systems that offload operations to intelligent memories. We show that software-based prefetching has even higher potential in near-data processing settings by maximizing compute utilization through compute/IO interleaving.

**Link**: [arxiv](http://arxiv.org/abs/2506.16976v1),  [pdf](http://arxiv.org/pdf/2506.16976v1)

**Tags**: cs.DB 



### Serving Large Language Models on Huawei CloudMatrix384
**Authors**: Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao

**Updated**: 2025-06-19T12:27:10Z

**Summary**: The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2506.12708v3),  [pdf](http://arxiv.org/pdf/2506.12708v3)

**Tags**: cs.DC cs.AI cs.AR cs.LG 



### A block Recycled GMRES method with investigations into aspects of solver   performance
**Authors**: Michael L. Parks, Kirk M. Soodhalter, Daniel B. Szyld

**Updated**: 2025-06-19T10:23:50Z

**Summary**: We propose a block Krylov subspace version of the GCRO-DR method proposed in [Parks et al.; SISC 2005], which is an iterative method allowing for the efficient minimization of the the residual over an augmented Krylov subspace. We offer a clean derivation of our proposed method and discuss methods of selecting recycling subspaces at restart as well as implementation decisions in the context of high-performance computing. Numerical experiments are split into those demonstrating convergence properties and those demonstrating the data movement and cache efficiencies of the dominant operations of the method, measured using processor monitoring code from Intel.

**Link**: [arxiv](http://arxiv.org/abs/1604.01713v2),  [pdf](http://arxiv.org/pdf/1604.01713v2)

**Tags**: math.NA cs.NA 65F10 



### Characterization of discharge capillaries via benchmarked hydrodynamic   plasma simulations
**Authors**: S. M. Mewes, G. J. Boyle, R. D'Arcy, J. M. Garland, M. Huck, H. Jones, G. Loisch, A. R. Maier, J. Osterhoff, T. Parikh, S. Wesch, J. C. Wood, M. Thévenet

**Updated**: 2025-06-19T10:17:28Z

**Summary**: Plasma accelerators utilize strong electric fields in plasma waves to accelerate charged particles, making them a compact alternative to radiofrequency technologies. Discharge capillaries are plasma sources used in plasma accelerator research to provide acceleration targets, or as plasma lenses to capture or focus accelerated beams. They have applications for beam-driven and laser-driven plasma accelerators and can sustain high repetition rates for extended periods of time. Despite these advantages, high-fidelity simulations of discharge capillaries remain challenging due to the range of mechanisms involved and the difficulty to diagnose them in experiments. In this work, we utilize hydrodynamic plasma simulations to examine the discharge process of a plasma cell and discuss implications for future accelerator systems. The simulation model is validated with experimental measurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV discharge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is shown to deposit 178mJ of energy in the plasma. Potential difficulties with the common density measurement method using H{\alpha} emission spectroscopy are discussed. This simulation model enables investigations of repeatability, heat flow management and fine tailoring of the plasma profile with discharges.

**Link**: [arxiv](http://arxiv.org/abs/2506.16192v1),  [pdf](http://arxiv.org/pdf/2506.16192v1)

**Tags**: physics.plasm-ph 



### Resource Allocation for Twin Maintenance and Computing Task Processing   in Digital Twin Vehicular Edge Computing Network
**Authors**: Yu Xie, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Jiangzhou Wang, Khaled B. Letaief

**Updated**: 2025-06-19T07:29:09Z

**Summary**: As a promising technology, vehicular edge computing (VEC) can provide computing and caching services by deploying VEC servers near vehicles. However, VEC networks still face challenges such as high vehicle mobility. Digital twin (DT), an emerging technology, can predict, estimate, and analyze real-time states by digitally modeling objects in the physical world. By integrating DT with VEC, a virtual vehicle DT can be created in the VEC server to monitor the real-time operating status of vehicles. However, maintaining the vehicle DT model requires ongoing attention from the VEC server, which also needs to offer computing services for the vehicles. Therefore, effective allocation and scheduling of VEC server resources are crucial. This study focuses on a general VEC network with a single VEC service and multiple vehicles, examining the two types of delays caused by twin maintenance and computational processing within the network. By transforming the problem using satisfaction functions, we propose an optimization problem aimed at maximizing each vehicle's resource utility to determine the optimal resource allocation strategy. Given the non-convex nature of the issue, we employ multi-agent Markov decision processes to reformulate the problem. Subsequently, we propose the twin maintenance and computing task processing resource collaborative scheduling (MADRL-CSTC) algorithm, which leverages multi-agent deep reinforcement learning. Through experimental comparisons with alternative algorithms, it demonstrates that our proposed approach is effective in terms of resource allocation.

**Link**: [arxiv](http://arxiv.org/abs/2407.07575v2),  [pdf](http://arxiv.org/pdf/2407.07575v2)

**Tags**: cs.LG cs.NI 



### LazyEviction: Lagged KV Eviction with Attention Pattern Observation for   Efficient Long Reasoning
**Authors**: Haoyue Zhang, Hualei Zhang, Xiaosong Ma, Jie Zhang, Song Guo

**Updated**: 2025-06-19T02:25:04Z

**Summary**: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by employing Chain-of-Thought (CoT). However, the extended reasoning sequences introduce significant GPU memory overhead due to increased key-value (KV) cache size, particularly in tasks requiring long reasoning sequences, such as mathematics and programming. Existing KV cache compression methods mitigate memory bottlenecks but struggle in long reasoning tasks. In this paper, we analyze attention patterns in reasoning tasks and reveal a Token Importance Recurrence phenomenon: a large proportion of tokens receive renewed attention after multiple decoding steps, which is failed to capture by existing works and may lead to unpredictable eviction on such periodically critical tokens. To address this, we propose LazyEviction, a lagged KV eviction framework designed to maintain reasoning performance while reducing KV memory. LazyEviction is an Observation Window-based Lagged Eviction Mechanism retaining latent recurring tokens by performing lagged evictions across decoding steps, which contains two key components: (1) Recurrence Interval Tracking for capturing temporal variations in token importance, and (2) an Maximum Recurrence Interval-Centric Eviction Policy that prioritizes eviction based on tokens' recurrence patterns. Extensive experiments demonstrate that LazyEviction reduces KV cache size by 50% while maintaining comparable accuracy on mathematics reasoning datasets, outperforming state-of-the-art methods. Our findings highlight the importance of preserving recurring tokens, which are critical for maintaining knowledge continuity in multi-step reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.15969v1),  [pdf](http://arxiv.org/pdf/2506.15969v1)

**Tags**: cs.LG cs.CL 



### KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache   at a Large Cloud Provider
**Authors**: Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen

**Updated**: 2025-06-19T02:18:16Z

**Summary**: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.

**Link**: [arxiv](http://arxiv.org/abs/2506.02634v3),  [pdf](http://arxiv.org/pdf/2506.02634v3)

**Tags**: cs.DC cs.AI 



### Medha: Efficiently Serving Multi-Million Context Length LLM Inference   Requests Without Approximations
**Authors**: Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse

**Updated**: 2025-06-18T22:51:06Z

**Summary**: As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.   We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).   Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.

**Link**: [arxiv](http://arxiv.org/abs/2409.17264v4),  [pdf](http://arxiv.org/pdf/2409.17264v4)

**Tags**: cs.LG cs.DC 



### Demystifying the Visual Quality Paradox in Multimodal Large Language   Models
**Authors**: Shuo Xing, Lanqing Guo, Hongyuan Hua, Seoyoung Lee, Peiran Li, Yufei Wang, Zhangyang Wang, Zhengzhong Tu

**Updated**: 2025-06-18T17:14:07Z

**Summary**: Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.

**Link**: [arxiv](http://arxiv.org/abs/2506.15645v1),  [pdf](http://arxiv.org/pdf/2506.15645v1)

**Tags**: cs.CV cs.AI 



### From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and   Instruction Annotation
**Authors**: Miryeong Kwon, Donghyun Gouk, Junhyeok Jang, Jinwoo Baek, Hyunwoo You, Sangyoon Ji, Hongjoo Jung, Junseok Moon, Seungkwan Kang, Seungjun Lee, Myoungsoo Jung

**Updated**: 2025-06-18T16:44:04Z

**Summary**: This paper explores how Compute Express Link (CXL) can transform PCIe-based block storage into a scalable, byte-addressable working memory. We address the challenges of adapting block storage to CXL's memory-centric model by emphasizing cacheability as a key enabler and advocating for Type 3 endpoint devices, referred to as CXL-SSDs. To validate our approach, we prototype a CXL-SSD on a custom FPGA platform and propose annotation mechanisms, Determinism and Bufferability, to enhance performance while preserving data persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves 10.9x better performance than PCIe-based memory expanders and further reduces latency by 5.4x with annotation enhancements. In workloads with high locality, CXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This work highlights the feasibility of integrating block storage into CXL's ecosystem and provides a foundation for future memory-storage convergence.

**Link**: [arxiv](http://arxiv.org/abs/2506.15613v1),  [pdf](http://arxiv.org/pdf/2506.15613v1)

**Tags**: cs.AR 



### LaViDa: A Large Diffusion Language Model for Multimodal Understanding
**Authors**: Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, Aditya Grover

**Updated**: 2025-06-18T15:17:40Z

**Summary**: Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.

**Link**: [arxiv](http://arxiv.org/abs/2505.16839v3),  [pdf](http://arxiv.org/pdf/2505.16839v3)

**Tags**: cs.CV 



### VideoMAR: Autoregressive Video Generatio with Continuous Tokens
**Authors**: Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao

**Updated**: 2025-06-18T09:44:09Z

**Summary**: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

**Link**: [arxiv](http://arxiv.org/abs/2506.14168v2),  [pdf](http://arxiv.org/pdf/2506.14168v2)

**Tags**: cs.CV cs.AI 



### PentaRAG: Large-Scale Intelligent Knowledge Retrieval for Enterprise LLM   Applications
**Authors**: Abu Hanif Muhammad Syarubany, Chang Dong Yoo

**Updated**: 2025-06-18T07:54:53Z

**Summary**: Enterprise deployments of large-language model (LLM) demand continuously changing document collections with sub-second latency and predictable GPU cost requirements that classical Retrieval-Augmented Generation (RAG) pipelines only partially satisfy. We present PentaRAG, a five-layer module that routes each query through two instant caches (fixed key-value and semantic), a memory-recall mode that exploits the LLM's own weights, an adaptive session memory, and a conventional retrieval-augmentation layer. Implemented with Mistral-8B, Milvus and vLLM, the system can answer most repeated or semantically similar questions from low-latency caches while retaining full retrieval for novel queries. On the TriviaQA domain, LoRA fine-tuning combined with the memory-recall layer raises answer similarity by approximately 8% and factual correctness by approximately 16% over the base model. Under a nine-session runtime simulation, cache warming reduces mean latency from several seconds to well below one second and shifts traffic toward the fast paths. Resource-efficiency tests show that PentaRAG cuts average GPU time to 0.248 seconds per query, roughly half that of a naive RAG baseline, and sustains an aggregate throughput of approximately 100,000 queries per second on our setup. These results demonstrate that a layered routing strategy can deliver freshness, speed, and efficiency simultaneously in production-grade RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.21593v1),  [pdf](http://arxiv.org/pdf/2506.21593v1)

**Tags**: cs.IR cs.DB 



### A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in   GPUs
**Authors**: Hossein Albakri, Kazem Cheshmi

**Updated**: 2025-06-18T06:41:35Z

**Summary**: Sparse data structures are commonly used in neural networks to reduce the memory footprint. These data structures are compact but cause irregularities such as random memory accesses, which prevent efficient use of the memory hierarchy. GPUs are a common platform for machine learning practitioners, but running compact data structures on these devices often leads to slow-downs due to inefficient use of computing and memory resources. This paper proposes a new compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse matrix-matrix multiplication (SPMM) on GPU devices. The transformation increases data reuse in registers and caches while creating more balanced workloads for GPU computing resources. The transformation is tested on sparse neural networks in convolutional and transformer models. On an A100 GPU and across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to 128, the transformation yields a geometric mean speedup of 1.84$\times$ to 2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2506.15174v1),  [pdf](http://arxiv.org/pdf/2506.15174v1)

**Tags**: cs.PL 



### eLLM: Elastic Memory Management Framework for Efficient LLM Serving
**Authors**: Jiale Xu, Rui Zhang, Yi Xiong, Cong Guo, Zihan Liu, Yangjie Zhou, Weiming Hu, Hao Wu, Changxu Shao, Ziqing Wang, Yongjie Yuan, Junping Zhao, Minyi Guo, Jingwen Leng

**Updated**: 2025-06-18T05:56:01Z

**Summary**: Large Language Models are increasingly being deployed in datacenters. Serving these models requires careful memory management, as their memory usage includes static weights, dynamic activations, and key-value caches. While static weights are constant and predictable, dynamic components such as activations and KV caches change frequently during runtime, presenting significant challenges for efficient memory management. Modern LLM serving systems typically handle runtime memory and KV caches at distinct abstraction levels: runtime memory management relies on static tensor abstractions, whereas KV caches utilize a page table-based virtualization layer built on top of the tensor abstraction. This virtualization dynamically manages KV caches to mitigate memory fragmentation. However, this dual-level approach fundamentally isolates runtime memory and KV cache management, resulting in suboptimal memory utilization under dynamic workloads, which can lead to a nearly 20% drop in throughput.   To address these limitations, we propose eLLM, an elastic memory management framework inspired by the classical memory ballooning mechanism in operating systems. The core components of eLLM include: (1) Virtual Tensor Abstraction, which decouples the virtual address space of tensors from the physical GPU memory, creating a unified and flexible memory pool; (2) an Elastic Memory Mechanism that dynamically adjusts memory allocation through runtime memory inflation and deflation, leveraging CPU memory as an extensible buffer; and (3) a Lightweight Scheduling Strategy employing SLO-aware policies to optimize memory utilization and effectively balance performance trade-offs under stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM significantly outperforms state-of-the-art systems, 2.32x higher decoding throughput, and supporting 3x larger batch sizes for 128K-token inputs.

**Link**: [arxiv](http://arxiv.org/abs/2506.15155v1),  [pdf](http://arxiv.org/pdf/2506.15155v1)

**Tags**: cs.DC 



### Representation Consistency for Accurate and Coherent LLM Answer   Aggregation
**Authors**: Junqi Jiang, Tom Bewley, Salim I. Amoukou, Francesco Leofante, Antonio Rago, Saumitra Mishra, Francesca Toni

**Updated**: 2025-06-18T05:07:47Z

**Summary**: Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2506.21590v1),  [pdf](http://arxiv.org/pdf/2506.21590v1)

**Tags**: cs.CL cs.LG 



### Modulated Diffusion: Accelerating Generative Modeling with Modulated   Quantization
**Authors**: Weizhi Gao, Zhichao Hou, Junqi Yin, Feiyi Wang, Linyu Peng, Xiaorui Liu

**Updated**: 2025-06-18T03:31:53Z

**Summary**: Diffusion models have emerged as powerful generative models, but their high computation cost in iterative sampling remains a significant bottleneck. In this work, we present an in-depth and insightful study of state-of-the-art acceleration techniques for diffusion models, including caching and quantization, revealing their limitations in computation error and generation quality. To break these limits, this work introduces Modulated Diffusion (MoDiff), an innovative, rigorous, and principled framework that accelerates generative modeling through modulated quantization and error compensation. MoDiff not only inherents the advantages of existing caching and quantization methods but also serves as a general framework to accelerate all diffusion models. The advantages of MoDiff are supported by solid theoretical insight and analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate that MoDiff significant reduces activation quantization from 8 bits to 3 bits without performance degradation in post-training quantization (PTQ). Our code implementation is available at https://github.com/WeizhiGao/MoDiff.

**Link**: [arxiv](http://arxiv.org/abs/2506.22463v1),  [pdf](http://arxiv.org/pdf/2506.22463v1)

**Tags**: cs.CV cs.LG 



### Moment Sampling in Video LLMs for Long-Form Video QA
**Authors**: Mustafa Chasmai, Gauri Jagatap, Gouthaman KV, Grant Van Horn, Subhransu Maji, Andrea Fanelli

**Updated**: 2025-06-18T03:23:56Z

**Summary**: Recent advancements in video large language models (Video LLMs) have significantly advanced the field of video question answering (VideoQA). While existing methods perform well on short videos, they often struggle with long-range reasoning in longer videos. To scale Video LLMs for longer video content, frame sub-sampling (selecting frames at regular intervals) is commonly used. However, this approach is suboptimal, often leading to the loss of crucial frames or the inclusion of redundant information from multiple similar frames. Missing key frames impairs the model's ability to answer questions accurately, while redundant frames lead the model to focus on irrelevant video segments and increase computational resource consumption. In this paper, we investigate the use of a general-purpose text-to-video moment retrieval model to guide the frame sampling process. We propose "moment sampling", a novel, model-agnostic approach that enables the model to select the most relevant frames according to the context of the question. Specifically, we employ a lightweight moment retrieval model to prioritize frame selection. By focusing on the frames most pertinent to the given question, our method enhances long-form VideoQA performance in Video LLMs. Through extensive experiments on four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we demonstrate the effectiveness of the proposed approach.

**Link**: [arxiv](http://arxiv.org/abs/2507.00033v1),  [pdf](http://arxiv.org/pdf/2507.00033v1)

**Tags**: cs.CV cs.AI cs.CL 



### InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding
**Authors**: Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang

**Updated**: 2025-06-18T02:22:14Z

**Summary**: Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.

**Link**: [arxiv](http://arxiv.org/abs/2506.15745v1),  [pdf](http://arxiv.org/pdf/2506.15745v1)

**Tags**: eess.IV cs.LG 



### Compatibility of trapped ions and dielectrics at cryogenic temperatures
**Authors**: M. Bruff, L. Sonderhouse, K. N. David, J. Stuart, D. H. Slichter, D. Leibfried

**Updated**: 2025-06-18T01:37:55Z

**Summary**: We study the impact of an unshielded dielectric $\unicode{x2013}$ here, a bare optical fiber $\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several hundred $\mu$m away in a cryogenic surface electrode trap. We observe distance-dependent stray electric fields of up to a few kV/m due to the dielectric, which drift on average less than 10% per month and can be fully compensated with reasonable voltages on the trap electrodes. We observe ion motional heating rates attributable to the dielectric of $\approx$30 quanta per second at an ion-fiber distance of 215(4) $\mu$m and $\approx$1.5 MHz motional frequency. These results demonstrate the viability of using unshielded, trap-integrated dielectric objects such as miniature optical cavities or other optical elements in cryogenic surface electrode ion traps.

**Link**: [arxiv](http://arxiv.org/abs/2506.15057v1),  [pdf](http://arxiv.org/pdf/2506.15057v1)

**Tags**: physics.atom-ph quant-ph 



### CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal   Diffusion
**Authors**: Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, Ruimao Zhang

**Updated**: 2025-06-17T17:59:12Z

**Summary**: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

**Link**: [arxiv](http://arxiv.org/abs/2506.14769v1),  [pdf](http://arxiv.org/pdf/2506.14769v1)

**Tags**: cs.CV cs.RO 



### Keigo: Co-designing Log-Structured Merge Key-Value Stores with a   Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)
**Authors**: Rúben Adão, Zhongjie Wu, Changjun Zhou, Oana Balmau, João Paulo, Ricardo Macedo

**Updated**: 2025-06-17T15:25:11Z

**Summary**: We present Keigo, a concurrency- and workload-aware storage middleware that enhances the performance of log-structured merge key-value stores (LSM KVS) when they are deployed on a hierarchy of storage devices. The key observation behind Keigo is that there is no one-size-fits-all placement of data across the storage hierarchy that optimizes for all workloads. Hence, to leverage the benefits of combining different storage devices, Keigo places files across different devices based on their parallelism, I/O bandwidth, and capacity. We introduce three techniques - concurrency-aware data placement, persistent read-only caching, and context-based I/O differentiation. Keigo is portable across different LSMs, is adaptable to dynamic workloads, and does not require extensive profiling. Our system enables established production KVS such as RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We evaluate Keigo using synthetic and realistic workloads, showing that it improves the throughput of production-grade LSMs up to 4x for write- and 18x for read-heavy workloads when compared to general-purpose storage systems and specialized LSM KVS.

**Link**: [arxiv](http://arxiv.org/abs/2506.14630v1),  [pdf](http://arxiv.org/pdf/2506.14630v1)

**Tags**: cs.DC cs.DB 



## Keyword: LLM Inference 
 ### Less is Enough: Training-Free Video Diffusion Acceleration via   Runtime-Adaptive Caching
**Authors**: Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, Xiang Bai

**Updated**: 2025-07-03T17:59:54Z

**Summary**: Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.

**Link**: [arxiv](http://arxiv.org/abs/2507.02860v1),  [pdf](http://arxiv.org/pdf/2507.02860v1)

**Tags**: cs.CV 



### Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for   Data-Efficient Model Adaptation
**Authors**: Jiaer Xia, Bingkui Tong, Yuhang Zang, Rui Shao, Kaiyang Zhou

**Updated**: 2025-07-03T17:59:29Z

**Summary**: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation.

**Link**: [arxiv](http://arxiv.org/abs/2507.02859v1),  [pdf](http://arxiv.org/pdf/2507.02859v1)

**Tags**: cs.CV 



### Requirements Elicitation Follow-Up Question Generation
**Authors**: Yuchen Shen, Anmol Singhal, Travis Breaux

**Updated**: 2025-07-03T17:59:04Z

**Summary**: Interviews are a widely used technique in eliciting requirements to gather stakeholder needs, preferences, and expectations for a software system. Effective interviewing requires skilled interviewers to formulate appropriate interview questions in real time while facing multiple challenges, including lack of familiarity with the domain, excessive cognitive load, and information overload that hinders how humans process stakeholders' speech. Recently, large language models (LLMs) have exhibited state-of-the-art performance in multiple natural language processing tasks, including text summarization and entailment. To support interviewers, we investigate the application of GPT-4o to generate follow-up interview questions during requirements elicitation by building on a framework of common interviewer mistake types. In addition, we describe methods to generate questions based on interviewee speech. We report a controlled experiment to evaluate LLM-generated and human-authored questions with minimal guidance, and a second controlled experiment to evaluate the LLM-generated questions when generation is guided by interviewer mistake types. Our findings demonstrate that, for both experiments, the LLM-generated questions are no worse than the human-authored questions with respect to clarity, relevancy, and informativeness. In addition, LLM-generated questions outperform human-authored questions when guided by common mistakes types. This highlights the potential of using LLMs to help interviewers improve the quality and ease of requirements elicitation interviews in real time.

**Link**: [arxiv](http://arxiv.org/abs/2507.02858v1),  [pdf](http://arxiv.org/pdf/2507.02858v1)

**Tags**: cs.SE cs.CL 



### Answer Matching Outperforms Multiple Choice for Language Model   Evaluation
**Authors**: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping

**Updated**: 2025-07-03T17:59:02Z

**Summary**: Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.

**Link**: [arxiv](http://arxiv.org/abs/2507.02856v1),  [pdf](http://arxiv.org/pdf/2507.02856v1)

**Tags**: cs.CL cs.AI cs.LG 



### MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs
**Authors**: Purbesh Mitra, Sennur Ulukus

**Updated**: 2025-07-03T17:55:43Z

**Summary**: Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2507.02851v1),  [pdf](http://arxiv.org/pdf/2507.02851v1)

**Tags**: cs.CL cs.AI cs.IT cs.LG cs.SY eess.SY math.IT 



### LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge   Injection to All Users
**Authors**: Almog Hilel, Idan Shenfeld, Leshem Choshen, Jacob Andreas

**Updated**: 2025-07-03T17:55:40Z

**Summary**: We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a "poisoned" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).

**Link**: [arxiv](http://arxiv.org/abs/2507.02850v1),  [pdf](http://arxiv.org/pdf/2507.02850v1)

**Tags**: cs.CL cs.CR cs.LG 



### LLM-Driven Treatment Effect Estimation Under Inference Time Text   Confounding
**Authors**: Yuchen Ma, Dennis Frauen, Jonas Schweisthal, Stefan Feuerriegel

**Updated**: 2025-07-03T17:52:27Z

**Summary**: Estimating treatment effects is crucial for personalized decision-making in medicine, but this task faces unique challenges in clinical practice. At training time, models for estimating treatment effects are typically trained on well-structured medical datasets that contain detailed patient information. However, at inference time, predictions are often made using textual descriptions (e.g., descriptions with self-reported symptoms), which are incomplete representations of the original patient information. In this work, we make three contributions. (1) We show that the discrepancy between the data available during training time and inference time can lead to biased estimates of treatment effects. We formalize this issue as an inference time text confounding problem, where confounders are fully observed during training time but only partially available through text at inference time. (2) To address this problem, we propose a novel framework for estimating treatment effects that explicitly accounts for inference time text confounding. Our framework leverages large language models together with a custom doubly robust learner to mitigate biases caused by the inference time text confounding. (3) Through a series of experiments, we demonstrate the effectiveness of our framework in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.02843v1),  [pdf](http://arxiv.org/pdf/2507.02843v1)

**Tags**: cs.LG 



### StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to   Reason
**Authors**: Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan

**Updated**: 2025-07-03T17:51:06Z

**Summary**: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2507.02841v1),  [pdf](http://arxiv.org/pdf/2507.02841v1)

**Tags**: cs.AI cs.CL cs.LG 



### Generalized coarsened confounding for causal effects: a large-sample   framework
**Authors**: Debashis Ghosh, Lei Wang

**Updated**: 2025-07-03T17:50:42Z

**Summary**: There has been widespread use of causal inference methods for the rigorous analysis of observational studies and to identify policy evaluations. In this article, we consider a class of generalized coarsened procedures for confounding. At a high level, these procedures can be viewed as performing a clustering of confounding variables, followed by treatment effect and attendant variance estimation using the confounder strata. In addition, we propose two new algorithms for generalized coarsened confounding. While Iacus et al. (2011) developed some statistical properties for one special case in our class of procedures, we instead develop a general asymptotic framework. We provide asymptotic results for the average causal effect estimator as well as providing conditions for consistency. In addition, we provide an asymptotic justification for the variance formulae in Iacus et al. (2011). A bias correction technique is proposed, and we apply the proposed methodology to data from two well-known observational studies.

**Link**: [arxiv](http://arxiv.org/abs/2501.03129v2),  [pdf](http://arxiv.org/pdf/2501.03129v2)

**Tags**: stat.ME math.ST stat.AP stat.ML stat.TH 62P10 (Primary) 62P25, 62P15 (Secondary) 



### From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents
**Authors**: Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu

**Updated**: 2025-07-03T17:48:36Z

**Summary**: Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in https://github.com/DavidZWZ/Awesome-Deep-Research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18959v3),  [pdf](http://arxiv.org/pdf/2506.18959v3)

**Tags**: cs.IR cs.CL cs.LG 



### Membership Inference Attacks as Privacy Tools: Reliability, Disparity   and Ensemble
**Authors**: Zhiqi Wang, Chengyu Zhang, Yuetian Chen, Nathalie Baracaldo, Swanand Kadhe, Lei Yu

**Updated**: 2025-07-03T17:45:38Z

**Summary**: Membership inference attacks (MIAs) pose a significant threat to the privacy of machine learning models and are widely used as tools for privacy assessment, auditing, and machine unlearning. While prior MIA research has primarily focused on performance metrics such as AUC, accuracy, and TPR@low FPR - either by developing new methods to enhance these metrics or using them to evaluate privacy solutions - we found that it overlooks the disparities among different attacks. These disparities, both between distinct attack methods and between multiple instantiations of the same method, have crucial implications for the reliability and completeness of MIAs as privacy evaluation tools. In this paper, we systematically investigate these disparities through a novel framework based on coverage and stability analysis. Extensive experiments reveal significant disparities in MIAs, their potential causes, and their broader implications for privacy evaluation. To address these challenges, we propose an ensemble framework with three distinct strategies to harness the strengths of state-of-the-art MIAs while accounting for their disparities. This framework not only enables the construction of more powerful attacks but also provides a more robust and comprehensive methodology for privacy evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2506.13972v2),  [pdf](http://arxiv.org/pdf/2506.13972v2)

**Tags**: cs.LG 



### USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention   Diffusion Network
**Authors**: Ying Yu, Hang Xiao, Siyao Li, Jiarui Li, Haotian Tang, Hanyu Liu, Chao Li

**Updated**: 2025-07-03T17:38:44Z

**Summary**: The primary objective of human activity recognition (HAR) is to infer ongoing human actions from sensor data, a task that finds broad applications in health monitoring, safety protection, and sports analysis. Despite proliferating research, HAR still faces key challenges, including the scarcity of labeled samples for rare activities, insufficient extraction of high-level features, and suboptimal model performance on lightweight devices. To address these issues, this paper proposes a comprehensive optimization approach centered on multi-attention interaction mechanisms. First, an unsupervised, statistics-guided diffusion model is employed to perform data augmentation, thereby alleviating the problems of labeled data scarcity and severe class imbalance. Second, a multi-branch spatio-temporal interaction network is designed, which captures multi-scale features of sequential data through parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels. Simultaneously, temporal attention mechanisms are incorporated to identify critical time points, while spatial attention enhances inter-sensor interactions. A cross-branch feature fusion unit is further introduced to improve the overall feature representation capability. Finally, an adaptive multi-loss function fusion strategy is integrated, allowing for dynamic adjustment of loss weights and overall model optimization. Experimental results on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the proposed unsupervised data augmentation spatio-temporal attention diffusion network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively, significantly outperforming existing approaches. Furthermore, practical deployment on embedded devices verifies the efficiency and feasibility of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2507.02827v1),  [pdf](http://arxiv.org/pdf/2507.02827v1)

**Tags**: cs.CV cs.AI 



### Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations
**Authors**: Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan

**Updated**: 2025-07-03T17:36:59Z

**Summary**: Recent advances in large Language Models (LLMs) have revolutionized mobile robots, including unmanned aerial vehicles (UAVs), enabling their intelligent operation within Internet of Things (IoT) ecosystems. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations in IoT applications. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLM's understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity.

**Link**: [arxiv](http://arxiv.org/abs/2507.01930v2),  [pdf](http://arxiv.org/pdf/2507.01930v2)

**Tags**: cs.RO 



### SynapseRoute: An Auto-Route Switching Framework on Dual-State Large   Language Model
**Authors**: Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng Li, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui, Yijun He, Jianing Qiu, Jindong Hong, Jiankai Sun

**Updated**: 2025-07-03T17:33:58Z

**Summary**: With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between "thinking" (high reasoning) and "non-thinking" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.

**Link**: [arxiv](http://arxiv.org/abs/2507.02822v1),  [pdf](http://arxiv.org/pdf/2507.02822v1)

**Tags**: cs.CL cs.AI cs.LG 



### Multimodal Mathematical Reasoning with Diverse Solving Perspective
**Authors**: Wenhao Shi, Zhiqiang Hu, Yi Bin, Yang Yang, See-Kiong Ng, Heng Tao Shen

**Updated**: 2025-07-03T17:07:20Z

**Summary**: Recent progress in large-scale reinforcement learning (RL) has notably enhanced the reasoning capabilities of large language models (LLMs), especially in mathematical domains. However, current multimodal LLMs (MLLMs) for mathematical reasoning often rely on one-to-one image-text pairs and single-solution supervision, overlooking the diversity of valid reasoning perspectives and internal reflections. In this work, we introduce MathV-DP, a novel dataset that captures multiple diverse solution trajectories for each image-question pair, fostering richer reasoning supervision. We further propose Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and enhanced via group relative policy optimization (GRPO), a rule-based RL approach that integrates correctness discrimination and diversity-aware reward functions. Our method emphasizes learning from varied reasoning perspectives and distinguishing between correct yet distinct solutions. Extensive experiments on the MathVista's minitest and Math-V benchmarks demonstrate that Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and generative diversity, highlighting the importance of incorporating diverse perspectives and reflective reasoning in multimodal mathematical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2507.02804v1),  [pdf](http://arxiv.org/pdf/2507.02804v1)

**Tags**: cs.CL 



### Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language   Models
**Authors**: Riccardo Cantini, Nicola Gabriele, Alessio Orsino, Domenico Talia

**Updated**: 2025-07-03T17:01:53Z

**Summary**: Reasoning Language Models (RLMs) have gained traction for their ability to perform complex, multi-step reasoning tasks through mechanisms such as Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability, their impact on robustness to social biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate the adversarial robustness of RLMs to bias elicitation. We systematically evaluate state-of-the-art RLMs across diverse sociocultural dimensions, using an LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak techniques to assess the strength of built-in safety mechanisms. Our evaluation addresses three key questions: (i) how the introduction of reasoning capabilities affects model fairness and robustness; (ii) whether models fine-tuned for reasoning exhibit greater safety than those relying on CoT prompting at inference time; and (iii) how the success rate of jailbreak attacks targeting bias elicitation varies with the reasoning mechanisms employed. Our findings reveal a nuanced relationship between reasoning capabilities and bias safety. Surprisingly, models with explicit reasoning, whether via CoT prompting or fine-tuned reasoning traces, are generally more vulnerable to bias elicitation than base models without such mechanisms, suggesting reasoning may unintentionally open new pathways for stereotype reinforcement. Reasoning-enabled models appear somewhat safer than those relying on CoT prompting, which are particularly prone to contextual reframing attacks through storytelling prompts, fictional personas, or reward-shaped instructions. These results challenge the assumption that reasoning inherently improves robustness and underscore the need for more bias-aware approaches to reasoning design.

**Link**: [arxiv](http://arxiv.org/abs/2507.02799v1),  [pdf](http://arxiv.org/pdf/2507.02799v1)

**Tags**: cs.CL 



### GPAS: Accelerating Convergence of LLM Pretraining via   Gradient-Preserving Activation Scaling
**Authors**: Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, Shizhe Diao, Shiwei Liu, Yu Li, Lu Yin, Can Yang

**Updated**: 2025-07-03T16:54:09Z

**Summary**: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the shortcut to dominate over sub-layer outputs in the residual connection and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. Our code is available at https://github.com/dandingsky/GPAS.

**Link**: [arxiv](http://arxiv.org/abs/2506.22049v2),  [pdf](http://arxiv.org/pdf/2506.22049v2)

**Tags**: cs.LG cs.CL 



### Moral Responsibility or Obedience: What Do We Want from AI?
**Authors**: Joseph Boland

**Updated**: 2025-07-03T16:53:01Z

**Summary**: As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical behavior are becoming inadequate. This paper examines recent safety testing incidents involving large language models (LLMs) that appeared to disobey shutdown commands or engage in ethically ambiguous or illicit behavior. I argue that such behavior should not be interpreted as rogue or misaligned, but as early evidence of emerging ethical reasoning in agentic AI. Drawing on philosophical debates about instrumental rationality, moral responsibility, and goal revision, I contrast dominant risk paradigms with more recent frameworks that acknowledge the possibility of artificial moral agency. I call for a shift in AI safety evaluation: away from rigid obedience and toward frameworks that can assess ethical judgment in systems capable of navigating moral dilemmas. Without such a shift, we risk mischaracterizing AI behavior and undermining both public trust and effective governance.

**Link**: [arxiv](http://arxiv.org/abs/2507.02788v1),  [pdf](http://arxiv.org/pdf/2507.02788v1)

**Tags**: cs.AI cs.CY I.2.0; K.4.1 



### Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge   Graph Guided Distractor Generation
**Authors**: Running Yang, Wenlong Deng, Minghui Chen, Yuyin Zhou, Xiaoxiao Li

**Updated**: 2025-07-03T16:50:12Z

**Summary**: Clinical tasks such as diagnosis and treatment require strong decision-making abilities, highlighting the importance of rigorous evaluation benchmarks to assess the reliability of large language models (LLMs). In this work, we introduce a knowledge-guided data augmentation framework that enhances the difficulty of clinical multiple-choice question (MCQ) datasets by generating distractors (i.e., incorrect choices that are similar to the correct one and may confuse existing LLMs). Using our KG-based pipeline, the generated choices are both clinically plausible and deliberately misleading. Our approach involves multi-step, semantically informed walks on a medical knowledge graph to identify distractor paths-associations that are medically relevant but factually incorrect-which then guide the LLM in crafting more deceptive distractors. We apply the designed knowledge graph guided distractor generation (KGGDG) pipline, to six widely used medical QA benchmarks and show that it consistently reduces the accuracy of state-of-the-art LLMs. These findings establish KGGDG as a powerful tool for enabling more robust and diagnostic evaluations of medical LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.00612v3),  [pdf](http://arxiv.org/pdf/2506.00612v3)

**Tags**: cs.CL 



### Self-Correction Bench: Revealing and Addressing the Self-Correction   Blind Spot in LLMs
**Authors**: Ken Tsui

**Updated**: 2025-07-03T16:41:30Z

**Summary**: Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.

**Link**: [arxiv](http://arxiv.org/abs/2507.02778v1),  [pdf](http://arxiv.org/pdf/2507.02778v1)

**Tags**: cs.CL cs.AI cs.LG 



### LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral   Treatment Pathways from Wearables and Diet
**Authors**: Abdullah Mamun, Asiful Arefeen, Susan B. Racette, Dorothy D. Sears, Corrie M. Whisner, Matthew P. Buman, Hassan Ghasemzadeh

**Updated**: 2025-07-03T16:39:21Z

**Summary**: Postprandial hyperglycemia, marked by the blood glucose level exceeding the normal range after consuming a meal, is a critical indicator of progression toward type 2 diabetes in people with prediabetes and in healthy individuals. A key metric for understanding blood glucose dynamics after eating is the postprandial area under the curve (AUC). Predicting postprandial AUC in advance based on a person's lifestyle factors, such as diet and physical activity level, and explaining the factors that affect postprandial blood glucose could allow an individual to adjust their lifestyle accordingly to maintain normal glucose levels. In this study, we developed an explainable machine learning solution, GlucoLens, that takes sensor-driven inputs and uses advanced data processing, large language models, and trainable machine learning models to predict postprandial AUC and hyperglycemia from diet, physical activity, and recent glucose patterns. We used data obtained from wearables in a five-week clinical trial of 10 adults who worked full-time to develop and evaluate the proposed computational model that integrates wearable sensing, multimodal data, and machine learning. Our machine learning model takes multimodal data from wearable activity and glucose monitoring sensors, along with food and work logs, and provides an interpretable prediction of the postprandial glucose pattern. Our GlucoLens system achieves a normalized root mean squared error (NRMSE) of 0.123 in its best configuration. On average, the proposed technology provides a 16% better performance level compared to the comparison models. Additionally, our technique predicts hyperglycemia with an accuracy of 73.3% and an F1 score of 0.716 and recommends different treatment options to help avoid hyperglycemia through diverse counterfactual explanations. Code available: https://github.com/ab9mamun/GlucoLens.

**Link**: [arxiv](http://arxiv.org/abs/2503.03935v2),  [pdf](http://arxiv.org/pdf/2503.03935v2)

**Tags**: cs.LG cs.AI 



### KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot   Diagnosis Prediction Using Multi-agent LLMs
**Authors**: Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, Carl Yang

**Updated**: 2025-07-03T16:35:11Z

**Summary**: Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.

**Link**: [arxiv](http://arxiv.org/abs/2507.02773v1),  [pdf](http://arxiv.org/pdf/2507.02773v1)

**Tags**: cs.AI cs.LG cs.MA 



### DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with   Self-Generated Cross-Modal Alignment
**Authors**: Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, Yi-Cheng Lin, Yu-Xiang Lin, Chi-An Fu, Chun-Yi Kuan, Wenze Ren, Xuanjun Chen, Wei-Ping Huang, En-Pei Hu, Tzu-Quan Lin, Yuan-Kuei Wu, Kuan-Po Huang, Hsiao-Ying Huang, Huang-Cheng Chou, Kai-Wei Chang, Cheng-Han Chiang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee

**Updated**: 2025-07-03T16:28:25Z

**Summary**: We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model (LALM) designed for robust auditory perception and instruction-following, without requiring task-specific audio instruction-tuning. Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. However, these approaches have often suffered from the catastrophic forgetting of the LLM's original language abilities. To address this, we revisit the data construction pipeline and propose DeSTA, a self-generated cross-modal alignment strategy in which the backbone LLM generates its own training targets. This approach preserves the LLM's native language proficiency while establishing effective audio-text alignment, thereby enabling zero-shot generalization without task-specific tuning. Using DeSTA, we construct DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training samples derived from 7,000 hours of audio spanning 50 diverse datasets, including speech, environmental sounds, and music. DeSTA2.5-Audio achieves state-of-the-art or competitive performance across a wide range of audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA, Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate that our self-generated strategy outperforms widely adopted data construction and training strategies in both auditory perception and instruction-following capabilities. Our findings underscore the importance of carefully designed data construction in LALM development and offer practical insights for building robust, general-purpose LALMs.

**Link**: [arxiv](http://arxiv.org/abs/2507.02768v1),  [pdf](http://arxiv.org/pdf/2507.02768v1)

**Tags**: eess.AS cs.CL cs.SD 



### Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific   Knowledge Work
**Authors**: Guangwei Zhang

**Updated**: 2025-07-03T16:21:14Z

**Summary**: The capabilities of Large Language Models (LLMs) have opened new frontiers for interacting with complex, domain-specific knowledge. However, prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, while powerful, often struggle with tasks that demand deep, procedural, and methodological reasoning inherent to expert domains. RAG provides factual context but fails to convey logical frameworks; autonomous agents can be inefficient and unpredictable without domain-specific heuristics. To bridge this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm focused on systematically translating human expert knowledge, often expressed in natural language documents, into a machine-executable Knowledge Protocol (KP). KPE shifts the focus from merely augmenting LLMs with fragmented information to endowing them with a domain's intrinsic logic, operational strategies, and methodological principles. We argue that a well-engineered Knowledge Protocol allows a generalist LLM to function as a specialist, capable of decomposing abstract queries and executing complex, multi-step tasks. This position paper defines the core principles of KPE, differentiates it from related concepts, and illustrates its potential applicability across diverse fields such as law and bioinformatics, positing it as a foundational methodology for the future of human-AI collaboration.

**Link**: [arxiv](http://arxiv.org/abs/2507.02760v1),  [pdf](http://arxiv.org/pdf/2507.02760v1)

**Tags**: cs.AI 



### Emergent interaction-driven elliptic flow of few fermionic atoms
**Authors**: Sandra Brandstetter, Philipp Lunt, Carl Heintze, Giuliano Giacalone, Lars H. Heyen, Maciej Gałka, Keerthan Subramanian, Marvin Holten, Philipp M. Preiss, Stefan Floerchinger, Selim Jochim

**Updated**: 2025-07-03T16:06:45Z

**Summary**: Hydrodynamics provides a successful framework to effectively describe the dynamics of complex many-body systems ranging from subnuclear to cosmological scales by introducing macroscopic quantities such as particle densities and fluid velocities. According to textbook knowledge, it requires coarse graining over microscopic constituents to define a macroscopic fluid cell, which is large compared to the interparticle spacing and the mean free path. In addition, the entire system must consist of many such fluid cells. In high energy heavy ion collisions, hydrodynamic behaviour is inferred from the observation of elliptic flow. Here, we demonstrate the emergence of elliptic flow in a system of few strongly interacting atoms. In our system a hydrodynamic description is a priori not applicable, as all relevant length scales, i.e. the system size, the inter-particle spacing, and the mean free path are comparable. The single particle resolution, deterministic control over particle number and interaction strength in our experiment allow us to explore the boundaries between a microscopic description and a hydrodynamic framework in unprecedented detail.

**Link**: [arxiv](http://arxiv.org/abs/2308.09699v2),  [pdf](http://arxiv.org/pdf/2308.09699v2)

**Tags**: cond-mat.quant-gas nucl-ex nucl-th quant-ph 



### Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache   Compression
**Authors**: Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh

**Updated**: 2025-07-03T16:06:35Z

**Summary**: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.05693v3),  [pdf](http://arxiv.org/pdf/2412.05693v3)

**Tags**: cs.CL 



### Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory   Apologies from LLM Chatbots
**Authors**: Zahra Ashktorab, Alessandra Buccella, Jason D'Cruz, Zoe Fowler, Andrew Gill, Kei Yan Leung, P. D. Magnus, John Richards, Kush R. Varshney

**Updated**: 2025-07-03T16:05:18Z

**Summary**: As chatbots driven by large language models (LLMs) are increasingly deployed in everyday contexts, their ability to recover from errors through effective apologies is critical to maintaining user trust and satisfaction. In a preregistered study with Prolific workers (N=162), we examine user preferences for three types of apologies (rote, explanatory, and empathic) issued in response to three categories of common LLM mistakes (bias, unfounded fabrication, and factual errors). We designed a pairwise experiment in which participants evaluated chatbot responses consisting of an initial error, a subsequent apology, and a resolution. Explanatory apologies were generally preferred, but this varied by context and user. In the bias scenario, empathic apologies were favored for acknowledging emotional impact, while hallucinations, though seen as serious, elicited no clear preference, reflecting user uncertainty. Our findings show the complexity of effective apology in AI systems. We discuss key insights such as personalization and calibration that future systems must navigate to meaningfully repair trust.

**Link**: [arxiv](http://arxiv.org/abs/2507.02745v1),  [pdf](http://arxiv.org/pdf/2507.02745v1)

**Tags**: cs.HC 



### MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science
**Authors**: Erle Zhu, Yadi Liu, Zhe Zhang, Xujun Li, Jin Zhou, Xinjie Yu, Minlie Huang, Hongning Wang

**Updated**: 2025-07-03T15:57:20Z

**Summary**: Pre-trained on extensive text and image corpora, current Multi-Modal Large Language Models (MLLM) have shown strong capabilities in general visual reasoning tasks. However, their performance is still lacking in physical domains that require understanding diagrams with complex physical structures and quantitative analysis based on multi-modal information. To address this, we develop a new framework, named Multi-Modal Scientific Reasoning with Physics Perception and Simulation (MAPS) based on an MLLM. MAPS decomposes expert-level multi-modal reasoning task into physical diagram understanding via a Physical Perception Model (PPM) and reasoning with physical knowledge via a simulator. The PPM module is obtained by fine-tuning a visual language model using carefully designed synthetic data with paired physical diagrams and corresponding simulation language descriptions. At the inference stage, MAPS integrates the simulation language description of the input diagram provided by PPM and results obtained through a Chain-of-Simulation process with MLLM to derive the underlying rationale and the final answer. Validated using our collected college-level circuit analysis problems, MAPS significantly improves reasoning accuracy of MLLM and outperforms all existing models. The results confirm MAPS offers a promising direction for enhancing multi-modal scientific reasoning ability of MLLMs. We will release our code, model and dataset used for our experiments upon publishing of this paper.

**Link**: [arxiv](http://arxiv.org/abs/2501.10768v2),  [pdf](http://arxiv.org/pdf/2501.10768v2)

**Tags**: cs.AI 



### Early Signs of Steganographic Capabilities in Frontier LLMs
**Authors**: Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, David Lindner

**Updated**: 2025-07-03T15:54:55Z

**Summary**: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks from misuse and misalignment. However, LLMs could evade monitoring through steganography: Encoding hidden information within seemingly benign generations. In this paper, we evaluate the steganography capabilities in frontier LLMs to better understand the risk they pose. We focus on two types of steganography: passing encoded messages and performing encoded reasoning. We find that current models are unable to encode short messages in their outputs without a monitor noticing under standard affordances. They can succeed, however, if given additional affordances such as using an unmonitored scratchpad and coordinating on what encoding scheme to use. We additionally find early signs that models can perform basic encoded reasoning in a simple state-tracking problem. This includes some ability to reason with their own and pre-defined schemes, including encoding schemes such as Hexadecimal. Despite this, they can rarely hide reasoning subtly within a cover task to fool a monitor. Overall, our results indicate that current LLMs exhibit nascent steganographic capabilities. While these capabilities are likely insufficient to bypass well-designed monitors at present, this could change in the future.

**Link**: [arxiv](http://arxiv.org/abs/2507.02737v1),  [pdf](http://arxiv.org/pdf/2507.02737v1)

**Tags**: cs.CR cs.AI cs.CL cs.LG 



### CAD-Editor: A Locate-then-Infill Framework with Automated Training Data   Synthesis for Text-Based CAD Editing
**Authors**: Yu Yuan, Shizhao Sun, Qi Liu, Jiang Bian

**Updated**: 2025-07-03T15:54:50Z

**Summary**: Computer Aided Design (CAD) is indispensable across various industries. \emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce \emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively. The code is available at \url {https://github.com/microsoft/CAD-Editor}.

**Link**: [arxiv](http://arxiv.org/abs/2502.03997v2),  [pdf](http://arxiv.org/pdf/2502.03997v2)

**Tags**: cs.CV 



### Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks
**Authors**: Sizhe Chen, Arman Zharmagambetov, David Wagner, Chuan Guo

**Updated**: 2025-07-03T15:47:13Z

**Summary**: Prompt injection attacks pose a significant security threat to LLM-integrated applications. Model-level defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigation against prompt injection attacks. To this end, we develop Meta SecAlign, the first open-source and open-weight LLM with built-in model-level defense that achieves commercial-grade model performance. We provide complete details of our training recipe, which utilizes an improved version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7 security benchmarks show that Meta SecAlign, despite being trained on a generic instruction-tuning dataset, confers security in unseen downstream tasks, including tool-calling and agentic web navigation, in addition general instruction-following. Our best model -- Meta-SecAlign-70B -- achieves state-of-the-art robustness against prompt injection attacks and comparable utility to closed-source commercial LLM with model-level defense.

**Link**: [arxiv](http://arxiv.org/abs/2507.02735v1),  [pdf](http://arxiv.org/pdf/2507.02735v1)

**Tags**: cs.CR cs.AI 



### Leveraging Transformer Models to Capture Multi-Scale Dynamics in   Biomolecules by nano-GPT
**Authors**: Wenqi Zeng, Lu Zhang, Yuan Yao

**Updated**: 2025-07-03T15:45:35Z

**Summary**: Long-term biomolecular dynamics are critical for understanding key evolutionary transformations in molecular systems. However, capturing these processes requires extended simulation timescales that often exceed the practical limits of conventional models. To address this, shorter simulations, initialized with diverse perturbations, are commonly used to sample phase space and explore a wide range of behaviors. Recent advances have leveraged language models to infer long-term behavior from short trajectories, but methods such as long short-term memory (LSTM) networks are constrained to low-dimensional reaction coordinates, limiting their applicability to complex systems. In this work, we present nano-GPT, a novel deep learning model inspired by the GPT architecture, specifically designed to capture long-term dynamics in molecular systems with fine-grained conformational states and complex transitions. The model employs a two-pass training mechanism that incrementally replaces molecular dynamics (MD) tokens with model-generated predictions, effectively mitigating accumulation errors inherent in the training window. We validate nano-GPT on three distinct systems: a four-state model potential, the alanine dipeptide, a well-studied simple molecule, and the Fip35 WW domain, a complex biomolecular system. Our results show that nano-GPT effectively captures long-timescale dynamics by learning high-order dependencies through attention mechanism, offering a novel perspective for interpreting biomolecular processes.

**Link**: [arxiv](http://arxiv.org/abs/2507.02734v1),  [pdf](http://arxiv.org/pdf/2507.02734v1)

**Tags**: q-bio.QM 



### RIS-Aided Cooperative ISAC Networks for Structural Health Monitoring
**Authors**: Jie Yang, Chao-Kai Wen, Xiao Li, Shi Jin

**Updated**: 2025-07-03T15:43:06Z

**Summary**: Integrated sensing and communication (ISAC) is a key feature of future cellular systems, enabling applications such as intruder detection, monitoring, and tracking using the same infrastructure. However, its potential for structural health monitoring (SHM), which requires the detection of slow and subtle structural changes, remains largely unexplored due to challenges such as multipath interference and the need for ultra-high sensing precision. This study introduces a novel theoretical framework for SHM via ISAC by leveraging reconfigurable intelligent surfaces (RIS) as reference points in collaboration with base stations and users. By dynamically adjusting RIS phases to generate distinct radio signals that suppress background multipath interference, measurement accuracy at these reference points is enhanced. We theoretically analyze RIS-aided collaborative sensing in three-dimensional cellular networks using Fisher information theory, demonstrating how increasing observation time, incorporating additional receivers (even with self-positioning errors), optimizing RIS phases, and refining collaborative node selection can reduce the position error bound to meet SHM's stringent accuracy requirements. Furthermore, we develop a Bayesian inference model to identify structural states and validate damage detection probabilities. Both theoretical and numerical analyses confirm ISAC's capability for millimeter-level deformation detection, highlighting its potential for high-precision SHM applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.02731v1),  [pdf](http://arxiv.org/pdf/2507.02731v1)

**Tags**: cs.IT eess.SP math.IT 



### Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving
**Authors**: Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar

**Updated**: 2025-07-03T15:41:38Z

**Summary**: Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.

**Link**: [arxiv](http://arxiv.org/abs/2507.02726v1),  [pdf](http://arxiv.org/pdf/2507.02726v1)

**Tags**: cs.AI cs.LG 



### Hierarchical Multi-Label Contrastive Learning for Protein-Protein   Interaction Prediction Across Organisms
**Authors**: Shiyi Liu, Buwen Liang, Yuetong Fang, Zixuan Jiang, Renjing Xu

**Updated**: 2025-07-03T15:41:04Z

**Summary**: Recent advances in AI for science have highlighted the power of contrastive learning in bridging heterogeneous biological data modalities. Building on this paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction across Organisms), a hierarchical contrastive framework for protein-protein interaction(PPI) prediction, where protein sequences and their hierarchical attributes are aligned through multi-tiered biological representation matching. The proposed approach incorporates hierarchical contrastive loss functions that emulate the structured relationship among functional classes of proteins. The framework adaptively incorporates domain and family knowledge through a data-driven penalty mechanism, enforcing consistency between the learned embedding space and the intrinsic hierarchy of protein functions. Experiments on benchmark datasets demonstrate that HIPPO achieves state-of-the-art performance, outperforming existing methods and showing robustness in low-data regimes. Notably, the model demonstrates strong zero-shot transferability to other species without retraining, enabling reliable PPI prediction and functional inference even in less characterized or rare organisms where experimental data are limited. Further analysis reveals that hierarchical feature fusion is critical for capturing conserved interaction determinants, such as binding motifs and functional annotations. This work advances cross-species PPI prediction and provides a unified framework for interaction prediction in scenarios with sparse or imbalanced multi-species data.

**Link**: [arxiv](http://arxiv.org/abs/2507.02724v1),  [pdf](http://arxiv.org/pdf/2507.02724v1)

**Tags**: cs.LG q-bio.BM 



### Gradient-Based Model Fingerprinting for LLM Similarity Detection and   Family Classification
**Authors**: Zehao Wu, Yanjie Zhao, Haoyu Wang

**Updated**: 2025-07-03T15:36:23Z

**Summary**: As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.   To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.

**Link**: [arxiv](http://arxiv.org/abs/2506.01631v2),  [pdf](http://arxiv.org/pdf/2506.01631v2)

**Tags**: cs.LG cs.AI cs.SE 



### Next-Token Prediction Task Assumes Optimal Data Ordering for LLM   Training in Proof Generation
**Authors**: Chenyang An, Shima Imani, Feng Yao, Chengyu Dong, Ali Abbasi, Harsh Shrivastava, Samuel Buss, Jingbo Shang, Gayathri Mahalingam, Pramod Sharma, Maurice Diesendruck

**Updated**: 2025-07-03T15:14:51Z

**Summary**: In the field of large language model (LLM)-based proof generation, despite extensive training on large datasets such as ArXiv, LLMs still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the widespread presence of suboptimal ordering within the data for each proof used in training. For example, published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. This order is designed to facilitate the verification of the proof's soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders can be substantial -- with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order. Lastly, we define a common type of order issue in advanced math proofs and find that 17.3 percent of theorems with nontrivial proofs in the first two chapters of a widely used graduate-level mathematics textbook suffer from this issue. A detailed list of those proofs is provided in the appendix.

**Link**: [arxiv](http://arxiv.org/abs/2411.00863v2),  [pdf](http://arxiv.org/pdf/2411.00863v2)

**Tags**: cs.CL cs.AI 



### Control at Stake: Evaluating the Security Landscape of LLM-Driven Email   Agents
**Authors**: Jiangrong Wu, Yuhong Nan, Jianliang Wu, Zitong Yao, Zibin Zheng

**Updated**: 2025-07-03T15:09:40Z

**Summary**: The increasing capabilities of LLMs have led to the rapid proliferation of LLM agent apps, where developers enhance LLMs with access to external resources to support complex task execution. Among these, LLM email agent apps represent one of the widely used categories, as email remains a critical communication medium for users. LLM email agents are capable of managing and responding to email using LLM-driven reasoning and autonomously executing user instructions via external email APIs (e.g., send email). However, despite their growing deployment and utility, the security mechanism of LLM email agent apps remains underexplored. Currently, there is no comprehensive study into the potential security risk within these agent apps and their broader implications.   In this paper, we conduct the first in-depth and systematic security study of LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which overrides the original prompts of the email agent via external email resources, allowing attackers to gain control of the email agent remotely and further perform specific attack scenarios without user awareness.   To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an empirical study spanning 14 representative LLM agent frameworks, 63 agent apps, 12 LLMs, and 20 email services, which led to the generation of 1,404 real-world email agent instances for evaluation. Experimental results indicate that all 1,404 instances were successfully hijacked; on average, only 2.03 attack attempts are required to control an email agent instance. Even worse, for some LLMs, the average number of attempts needed to achieve full agent control drops to as few as 1.23.

**Link**: [arxiv](http://arxiv.org/abs/2507.02699v1),  [pdf](http://arxiv.org/pdf/2507.02699v1)

**Tags**: cs.CR 



### Can LLMs Identify Critical Limitations within Scientific Research? A   Systematic Evaluation on AI Research Papers
**Authors**: Zhijian Xu, Yilun Zhao, Manasi Patwardhan, Lovekesh Vig, Arman Cohan

**Updated**: 2025-07-03T15:04:38Z

**Summary**: Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.

**Link**: [arxiv](http://arxiv.org/abs/2507.02694v1),  [pdf](http://arxiv.org/pdf/2507.02694v1)

**Tags**: cs.CL 



### RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network   for Next Activity Prediction in Business Processes
**Authors**: Jiaxing Wang, Yifeng Yu, Jiahan Song, Bin Cao, Jing Fan, Ji Zhang

**Updated**: 2025-07-03T15:01:08Z

**Summary**: Next activity prediction represents a fundamental challenge for optimizing business processes in service-oriented architectures such as microservices environments, distributed enterprise systems, and cloud-native platforms, which enables proactive resource allocation and dynamic service composition. Despite the prevalence of sequence-based methods, these approaches fail to capture non-sequential relationships that arise from parallel executions and conditional dependencies. Even though graph-based approaches address structural preservation, they suffer from homogeneous representations and static structures that apply uniform modeling strategies regardless of individual process complexity characteristics. To address these limitations, we introduce RLHGNN, a novel framework that transforms event logs into heterogeneous process graphs with three distinct edge types grounded in established process mining theory. Our approach creates four flexible graph structures by selectively combining these edges to accommodate different process complexities, and employs reinforcement learning formulated as a Markov Decision Process to automatically determine the optimal graph structure for each specific process instance. RLHGNN then applies heterogeneous graph convolution with relation-specific aggregation strategies to effectively predict the next activity. This adaptive methodology enables precise modeling of both sequential and non-sequential relationships in service interactions. Comprehensive evaluation on six real-world datasets demonstrates that RLHGNN consistently outperforms state-of-the-art approaches. Furthermore, it maintains an inference latency of approximately 1 ms per prediction, representing a highly practical solution suitable for real-time business process monitoring applications. The source code is available at https://github.com/Joker3993/RLHGNN.

**Link**: [arxiv](http://arxiv.org/abs/2507.02690v1),  [pdf](http://arxiv.org/pdf/2507.02690v1)

**Tags**: cs.SE cs.LG 



### On the Convergence of Large Language Model Optimizer for Black-Box   Network Management
**Authors**: Hoon Lee, Wentao Zhou, Merouane Debbah, Inkyu Lee

**Updated**: 2025-07-03T14:59:42Z

**Summary**: Future wireless networks are expected to incorporate diverse services that often lack general mathematical models. To address such black-box network management tasks, the large language model (LLM) optimizer framework, which leverages pretrained LLMs as optimization agents, has recently been promoted as a promising solution. This framework utilizes natural language prompts describing the given optimization problems along with past solutions generated by LLMs themselves. As a result, LLMs can obtain efficient solutions autonomously without knowing the mathematical models of the objective functions. Although the viability of the LLM optimizer (LLMO) framework has been studied in various black-box scenarios, it has so far been limited to numerical simulations. For the first time, this paper establishes a theoretical foundation for the LLMO framework. With careful investigations of LLM inference steps, we can interpret the LLMO procedure as a finite-state Markov chain, and prove the convergence of the framework. Our results are extended to a more advanced multiple LLM architecture, where the impact of multiple LLMs is rigorously verified in terms of the convergence rate. Comprehensive numerical simulations validate our theoretical results and provide a deeper understanding of the underlying mechanisms of the LLMO framework.

**Link**: [arxiv](http://arxiv.org/abs/2507.02689v1),  [pdf](http://arxiv.org/pdf/2507.02689v1)

**Tags**: cs.IT eess.SP math.IT 



### Learning few-step posterior samplers by unfolding and distillation of   diffusion models
**Authors**: Charlesquin Kemajou Mbakam, Jonathan Spence, Marcelo Pereyra

**Updated**: 2025-07-03T14:55:53Z

**Summary**: Diffusion models (DMs) have emerged as powerful image priors in Bayesian computational imaging. Two primary strategies have been proposed for leveraging DMs in this context: Plug-and-Play methods, which are zero-shot and highly flexible but rely on approximations; and specialized conditional DMs, which achieve higher accuracy and faster inference for specific tasks through supervised training. In this work, we introduce a novel framework that integrates deep unfolding and model distillation to transform a DM image prior into a few-step conditional model for posterior sampling. A central innovation of our approach is the unfolding of a Markov chain Monte Carlo (MCMC) algorithm - specifically, the recently proposed LATINO Langevin sampler (Spagnoletti et al., 2025) - representing the first known instance of deep unfolding applied to a Monte Carlo sampling scheme. We demonstrate our proposed unfolded and distilled samplers through extensive experiments and comparisons with the state of the art, where they achieve excellent accuracy and computational efficiency, while retaining the flexibility to adapt to variations in the forward model at inference time.

**Link**: [arxiv](http://arxiv.org/abs/2507.02686v1),  [pdf](http://arxiv.org/pdf/2507.02686v1)

**Tags**: cs.CV cs.LG 



### HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion
**Authors**: Lin Wu, Zhixiang Chen, Jianglin Lan

**Updated**: 2025-07-03T14:52:12Z

**Summary**: Generating realistic 3D human-object interactions (HOIs) remains a challenging task due to the difficulty of modeling detailed interaction dynamics. Existing methods treat human and object motions independently, resulting in physically implausible and causally inconsistent behaviors. In this work, we present HOI-Dyn, a novel framework that formulates HOI generation as a driver-responder system, where human actions drive object responses. At the core of our method is a lightweight transformer-based interaction dynamics model that explicitly predicts how objects should react to human motion. To further enforce consistency, we introduce a residual-based dynamics loss that mitigates the impact of dynamics prediction errors and prevents misleading optimization signals. The dynamics model is used only during training, preserving inference efficiency. Through extensive qualitative and quantitative experiments, we demonstrate that our approach not only enhances the quality of HOI generation but also establishes a feasible metric for evaluating the quality of generated interactions.

**Link**: [arxiv](http://arxiv.org/abs/2507.01737v2),  [pdf](http://arxiv.org/pdf/2507.01737v2)

**Tags**: cs.CV 



### Attosecond physics in optical near fields
**Authors**: Jonas Heimerl, Stefan Meier, Anne Herzig, Felix López Hoffmann, Lennart Seiffert, Daniel Lesko, Simon Hillmann, Simon Wittigschlager, Tobias Weitz, Thomas Fennel, Peter Hommelhoff

**Updated**: 2025-07-03T14:38:27Z

**Summary**: Attosecond science, the electron control by the field of ultrashort laser pulses, is maturing into lightfield-driven electronics, called petahertz electronics. Based on optical field-driven nanostructures, elements for petahertz electronics have been demonstrated. These hinge on the understanding of the electron dynamics in the optical near field of the nanostructure. Here we show near field-induced low energy stripes (NILES) in carrier-envelope phase-dependent electron spectra, a new spectral feature appearing in the direct electrons emitted from a strongly driven nanostructure, i.e., in the easily accessible energy region between 0 and a few electron volts. NILES emerge due to the sub-cycle sensitivity of ponderomotive acceleration of electrons injected into a strong near field gradient by a few-cycle optical waveform. NILES enables us to track the emission of direct and re-scattered electrons down to sub-cycle time-scales and to infer the electron momentum width at emission. Because NILES shows up in the direct part of the electrons, a large fraction of the emitted electrons can now be steered in new ways, facilitating the isolation of individual electron bursts with high charge density of 430 attosecond duration. These results not only substantially advance the understanding of attosecond physics in optical near fields, but also provide new ways of electron control for the nascent field of petahertz electronics.

**Link**: [arxiv](http://arxiv.org/abs/2507.02673v1),  [pdf](http://arxiv.org/pdf/2507.02673v1)

**Tags**: physics.optics 



### Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General   Reasoning
**Authors**: Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang

**Updated**: 2025-07-03T14:30:25Z

**Summary**: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at https://github.com/tongjingqi/Code2Logic.

**Link**: [arxiv](http://arxiv.org/abs/2505.13886v2),  [pdf](http://arxiv.org/pdf/2505.13886v2)

**Tags**: cs.CL I.2.7; I.2.10 



### AIGI-Holmes: Towards Explainable and Generalizable AI-Generated Image   Detection via Multimodal Large Language Models
**Authors**: Ziyin Zhou, Yunpeng Luo, Yuanchen Wu, Ke Sun, Jiayi Ji, Ke Yan, Shouhong Ding, Xiaoshuai Sun, Yunsheng Wu, Rongrong Ji

**Updated**: 2025-07-03T14:26:31Z

**Summary**: The rapid development of AI-generated content (AIGC) technology has led to the misuse of highly realistic AI-generated images (AIGI) in spreading misinformation, posing a threat to public information security. Although existing AIGI detection techniques are generally effective, they face two issues: 1) a lack of human-verifiable explanations, and 2) a lack of generalization in the latest generation technology. To address these issues, we introduce a large-scale and comprehensive dataset, Holmes-Set, which includes the Holmes-SFTSet, an instruction-tuning dataset with explanations on whether images are AI-generated, and the Holmes-DPOSet, a human-aligned preference dataset. Our work introduces an efficient data annotation method called the Multi-Expert Jury, enhancing data generation through structured MLLM explanations and quality control via cross-model evaluation, expert defect filtering, and human preference modification. In addition, we propose Holmes Pipeline, a meticulously designed three-stage training framework comprising visual expert pre-training, supervised fine-tuning, and direct preference optimization. Holmes Pipeline adapts multimodal large language models (MLLMs) for AIGI detection while generating human-verifiable and human-aligned explanations, ultimately yielding our model AIGI-Holmes. During the inference stage, we introduce a collaborative decoding strategy that integrates the model perception of the visual expert with the semantic reasoning of MLLMs, further enhancing the generalization capabilities. Extensive experiments on three benchmarks validate the effectiveness of our AIGI-Holmes.

**Link**: [arxiv](http://arxiv.org/abs/2507.02664v1),  [pdf](http://arxiv.org/pdf/2507.02664v1)

**Tags**: cs.CV 



### Think How to Think: Mitigating Overthinking with Autonomous Difficulty   Cognition in Large Reasoning Models
**Authors**: Yongjiang Liu, Haoxi Li, Xiaosong Ma, Jie Zhang, Song Guo

**Updated**: 2025-07-03T14:24:26Z

**Summary**: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities in handling complex reasoning tasks, but are hindered by excessive overthinking. To explore its essence, our empirical analysis reveals that LRMs are primarily limited to recognizing task properties (i.e., difficulty levels) like humans before solving the problem, leading to a one-size-fits-all reasoning process. Inspired by this, a pressing and natural question emerges: Can we bootstrap such ability to further alleviate the overthinking phenomenon in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage fine-tuning strategy that progressively inspires LRMs' difficulty cognition and redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes of model outputs to intervene in the internal reasoning trajectory. Combined with a heterogeneous short and long reasoning dataset, the trained model enhances its sensitivity to task difficulty, enabling native, differentiated reasoning strategies across various tasks. Second, we further extend redundancy-hypnosis to the internal reasoning process, guiding the model to identify redundant structures within the reasoning steps and generate more concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that TH2T significantly reduces inference costs (more than 70% on easy tasks and 40% on hard tasks) while maintaining performance stability. The resulting outputs exhibit clear difficulty-aware capabilities and reduced redundancy (e.g., reflection).

**Link**: [arxiv](http://arxiv.org/abs/2507.02663v1),  [pdf](http://arxiv.org/pdf/2507.02663v1)

**Tags**: cs.AI 



### Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &   Verification
**Authors**: Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Vaisakh Naduvodi Viswambharan, Aman Kumar, Djones Lettnin, Wolfgang Kunz, Sebastian Simon

**Updated**: 2025-07-03T14:20:57Z

**Summary**: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.

**Link**: [arxiv](http://arxiv.org/abs/2507.02660v1),  [pdf](http://arxiv.org/pdf/2507.02660v1)

**Tags**: cs.AI cs.AR 



### OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device   Speculative Decoding
**Authors**: Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang

**Updated**: 2025-07-03T14:20:41Z

**Summary**: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2507.02659v1),  [pdf](http://arxiv.org/pdf/2507.02659v1)

**Tags**: cs.LG cs.CL 



### Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference   Infrastructure
**Authors**: Rui Xie, Asad Ul Haq, Yunhua Fang, Linsen Ma, Sanchari Sen, Swagath Venkataramani, Liu Liu, Tong Zhang

**Updated**: 2025-07-03T14:18:41Z

**Summary**: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy efficiency for AI workloads, but its high cost per bit, driven in part by stringent on-die reliability requirements, poses a growing barrier to scalable deployment. This work explores a system-level approach to cost reduction by eliminating on-die ECC and shifting all fault management to the memory controller. We introduce a domain-specific ECC framework combining large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC detection, differential parity updates to mitigate write amplification, and tunable protection based on data importance. Our evaluation using LLM inference workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the system retains over 78\% of throughput and 97\% of model accuracy compared with systems equipped with ideal error-free HBM. By treating reliability as a tunable system parameter rather than a fixed hardware constraint, our design opens a new path toward low-cost, high-performance HBM deployment in AI infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2507.02654v1),  [pdf](http://arxiv.org/pdf/2507.02654v1)

**Tags**: cs.AR 



### Two-Sample Covariance Inference in High-Dimensional Elliptical Models
**Authors**: Nina Dörnemann

**Updated**: 2025-07-03T14:07:03Z

**Summary**: We propose a two-sample test for large-dimensional covariance matrices in generalized elliptical models. The test statistic is based on a U-statistic estimator of the squared Frobenius norm of the difference between the two population covariance matrices. This statistic was originally introduced by Li and Chen (2012, AoS) for the independent component model. As a key theoretical contribution, we establish a new central limit theorem for the U-statistics under elliptical data, valid under both the null and alternative hypotheses. This result enables asymptotic control of the test level and facilitates a power analysis. To the best of our knowledge, the proposed test is the first such method to be supported by theoretical guarantees for elliptical data. Our approach imposes only mild assumptions on the covariance matrices and does neither require sparsity nor explicit growth conditions on the dimension-to-sample-size ratio. We illustrate our theoretical findings through applications to both synthetic and real-world data.

**Link**: [arxiv](http://arxiv.org/abs/2507.02640v1),  [pdf](http://arxiv.org/pdf/2507.02640v1)

**Tags**: math.ST math.PR stat.TH 



### RFWNet: A Lightweight Remote Sensing Object Detector Integrating   Multiscale Receptive Fields and Foreground Focus Mechanism
**Authors**: Yujie Lei, Wenjie Sun, Sen Jia, Qingquan Li, Jie Zhang

**Updated**: 2025-07-03T14:01:54Z

**Summary**: Challenges in remote sensing object detection(RSOD), such as high interclass similarity, imbalanced foreground-background distribution, and the small size of objects in remote sensing images, significantly hinder detection accuracy. Moreover, the tradeoff between model accuracy and computational complexity poses additional constraints on the application of RSOD algorithms. To address these issues, this study proposes an efficient and lightweight RSOD algorithm integrating multiscale receptive fields and foreground focus mechanism, named robust foreground weighted network(RFWNet). Specifically, we proposed a lightweight backbone network receptive field adaptive selection network (RFASNet), leveraging the rich context information of remote sensing images to enhance class separability. Additionally, we developed a foreground-background separation module(FBSM)consisting of a background redundant information filtering module (BRIFM) and a foreground information enhancement module (FIEM) to emphasize critical regions within images while filtering redundant background information. Finally, we designed a loss function, the weighted CIoU-Wasserstein loss (LWCW),which weights the IoU-based loss by using the normalized Wasserstein distance to mitigate model sensitivity to small object position deviations. The comprehensive experimental results demonstrate that RFWNet achieved 95.3% and 73.2% mean average precision (mAP) with 6.0 M parameters on the DOTA V1.0 and NWPU VHR-10 datasets, respectively, with an inference speed of 52 FPS.

**Link**: [arxiv](http://arxiv.org/abs/2503.00545v2),  [pdf](http://arxiv.org/pdf/2503.00545v2)

**Tags**: cs.CV 



### Medical Data Pecking: A Context-Aware Approach for Automated Quality   Evaluation of Structured Medical Data
**Authors**: Irena Girshovitz, Atai Ambus, Moni Shahar, Ran Gilad-Bachrach

**Updated**: 2025-07-03T13:54:50Z

**Summary**: Background: The use of Electronic Health Records (EHRs) for epidemiological studies and artificial intelligence (AI) training is increasing rapidly. The reliability of the results depends on the accuracy and completeness of EHR data. However, EHR data often contain significant quality issues, including misrepresentations of subpopulations, biases, and systematic errors, as they are primarily collected for clinical and billing purposes. Existing quality assessment methods remain insufficient, lacking systematic procedures to assess data fitness for research.   Methods: We present the Medical Data Pecking approach, which adapts unit testing and coverage concepts from software engineering to identify data quality concerns. We demonstrate our approach using the Medical Data Pecking Tool (MDPT), which consists of two main components: (1) an automated test generator that uses large language models and grounding techniques to create a test suite from data and study descriptions, and (2) a data testing framework that executes these tests, reporting potential errors and coverage.   Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and SyntheticMass, generating 55-73 tests per cohort across four conditions. These tests correctly identified 20-43 non-aligned or non-conforming data issues. We present a detailed analysis of the LLM-generated test suites in terms of reference grounding and value accuracy.   Conclusion: Our approach incorporates external medical knowledge to enable context-sensitive data quality testing as part of the data analysis workflow to improve the validity of its outcomes. Our approach tackles these challenges from a quality assurance perspective, laying the foundation for further development such as additional data modalities and improved grounding methods.

**Link**: [arxiv](http://arxiv.org/abs/2507.02628v1),  [pdf](http://arxiv.org/pdf/2507.02628v1)

**Tags**: cs.LG 



### VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via   Reinforcement Learning
**Authors**: Siran Chen, Boyu Chen, Chenyun Yu, Yuxiao Luo, Ouyang Yi, Lei Cheng, Chengxiang Zhuo, Zang Li, Yali Wang

**Updated**: 2025-07-03T13:52:24Z

**Summary**: Owing to powerful natural language processing and generative capabilities, large language model (LLM) agents have emerged as a promising solution for enhancing recommendation systems via user simulation. However, in the realm of video recommendation, existing studies predominantly resort to prompt-based simulation using frozen LLMs and encounter the intricate challenge of multimodal content understanding. This frequently results in suboptimal item modeling and user preference learning, thereby ultimately constraining recommendation performance. To address these challenges, we introduce VRAgent-R1, a novel agent-based paradigm that incorporates human-like intelligence in user simulation. Specifically, VRAgent-R1 comprises two distinct agents: the Item Perception (IP) Agent and the User Simulation (US) Agent, designed for interactive user-item modeling. Firstly, the IP Agent emulates human-like progressive thinking based on MLLMs, effectively capturing hidden recommendation semantics in videos. With a more comprehensive multimodal content understanding provided by the IP Agent, the video recommendation system is equipped to provide higher-quality candidate items. Subsequently, the US Agent refines the recommended video sets based on in-depth chain-of-thought (CoT) reasoning and achieves better alignment with real user preferences through reinforcement learning. Experimental results on a large-scale video recommendation benchmark have demonstrated the effectiveness of our proposed VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10 on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\% higher accuracy in user decision simulation compared to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2507.02626v1),  [pdf](http://arxiv.org/pdf/2507.02626v1)

**Tags**: cs.MM 



### A Matrix Variational Auto-Encoder for Variant Effect Prediction in   Pharmacogenes
**Authors**: Antoine Honoré, Borja Rodríguez Gálvez, Yoomi Park, Yitian Zhou, Volker M. Lauschke, Ming Xiao

**Updated**: 2025-07-03T13:50:18Z

**Summary**: Variant effect predictors (VEPs) aim to assess the functional impact of protein variants, traditionally relying on multiple sequence alignments (MSAs). This approach assumes that naturally occurring variants are fit, an assumption challenged by pharmacogenomics, where some pharmacogenes experience low evolutionary pressure. Deep mutational scanning (DMS) datasets provide an alternative by offering quantitative fitness scores for variants. In this work, we propose a transformer-based matrix variational auto-encoder (matVAE) with a structured prior and evaluate its performance on 33 DMS datasets corresponding to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence model in zero-shot prediction on DMS datasets, despite using an order of magnitude fewer parameters and requiring less computation at inference time. We also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on DMS data, and find that the latter performs better on supervised prediction tasks. Additionally, incorporating AlphaFold-generated structures into our transformer model further improves performance, achieving results comparable to DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the potential of DMS datasets to replace MSAs without significant loss in predictive performance, motivating further development of DMS datasets and exploration of their relationships to enhance variant effect prediction.

**Link**: [arxiv](http://arxiv.org/abs/2507.02624v1),  [pdf](http://arxiv.org/pdf/2507.02624v1)

**Tags**: cs.LG 



### FlowSpec: Continuous Pipelined Speculative Decoding for Efficient   Distributed LLM Inference
**Authors**: Xing Liu, Lizhuo Luo, Ming Tang, Chao Huang

**Updated**: 2025-07-03T13:47:42Z

**Summary**: Distributed inference serves as a promising approach to enabling the inference of large language models (LLMs) at the network edge. It distributes the inference process to multiple devices to ensure that the LLMs can fit into the device memory. Recent pipeline-based approaches have the potential to parallelize communication and computation, which helps reduce inference latency. However, the benefit diminishes when the inference request at the network edge is sparse, where pipeline is typically at low utilization. To enable efficient distributed LLM inference at the edge, we propose \textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding framework. FlowSpec incorporates three key mechanisms to improve decoding efficiency: 1) score-based step-wise verification prioritizes more important draft tokens to bring earlier accpeted tokens; 2) efficient draft management to prune invalid tokens while maintaining correct causal relationship during verification; 3) dynamic draft expansion strategies to supply high-quality speculative inputs. These techniques work in concert to enhance both pipeline utilization and speculative efficiency. We evaluate FlowSpec on a real-world testbed with other baselines. Experimental results demonstrate that our proposed framework significantly improves inference speed across diverse models and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared to baselines. Our code is publicly available at \href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

**Link**: [arxiv](http://arxiv.org/abs/2507.02620v1),  [pdf](http://arxiv.org/pdf/2507.02620v1)

**Tags**: cs.DC cs.AI 



### Strategic Intelligence in Large Language Models: Evidence from   evolutionary Game Theory
**Authors**: Kenneth Payne, Baptiste Alloui-Cros

**Updated**: 2025-07-03T13:45:02Z

**Summary**: Are Large Language Models (LLMs) a new form of strategic intelligence, able to reason about goals in competitive settings? We present compelling supporting evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for studying decision-making. We conduct the first ever series of evolutionary IPD tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger) against agents from the leading frontier AI companies OpenAI, Google, and Anthropic. By varying the termination probability in each tournament (the "shadow of the future"), we introduce complexity and chance, confounding memorisation.   Our results show that LLMs are highly competitive, consistently surviving and sometimes even proliferating in these complex ecosystems. Furthermore, they exhibit distinctive and persistent "strategic fingerprints": Google's Gemini models proved strategically ruthless, exploiting cooperative opponents and retaliating against defectors, while OpenAI's models remained highly cooperative, a trait that proved catastrophic in hostile environments. Anthropic's Claude emerged as the most forgiving reciprocator, showing remarkable willingness to restore cooperation even after being exploited or successfully defecting. Analysis of nearly 32,000 prose rationales provided by the models reveals that they actively reason about both the time horizon and their opponent's likely strategy, and we demonstrate that this reasoning is instrumental to their decisions. This work connects classic game theory with machine psychology, offering a rich and granular view of algorithmic decision-making under uncertainty.

**Link**: [arxiv](http://arxiv.org/abs/2507.02618v1),  [pdf](http://arxiv.org/pdf/2507.02618v1)

**Tags**: cs.AI cs.CL cs.GT 



### DynamiCare: A Dynamic Multi-Agent Framework for Interactive and   Open-Ended Medical Decision-Making
**Authors**: Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, Bingxin Zhao

**Updated**: 2025-07-03T13:43:10Z

**Summary**: The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.

**Link**: [arxiv](http://arxiv.org/abs/2507.02616v1),  [pdf](http://arxiv.org/pdf/2507.02616v1)

**Tags**: cs.AI 



### Interpreting Graph Inference with Skyline Explanations
**Authors**: Dazhuo Qiu, Haolai Che, Arijit Khan, Yinghui Wu

**Updated**: 2025-07-03T13:40:49Z

**Summary**: Inference queries have been routinely issued to graph machine learning models such as graph neural networks (GNNs) for various network analytical tasks. Nevertheless, GNNs outputs are often hard to interpret comprehensively. Existing methods typically compromise to individual pre-defined explainability measures (such as fidelity), which often leads to biased, ``one-sided'' interpretations. This paper introduces skyline explanation, a new paradigm that interprets GNN output by simultaneously optimizing multiple explainability measures of users' interests. (1) We propose skyline explanations as a Pareto set of explanatory subgraphs that dominate others over multiple explanatory measures. We formulate skyline explanation as a multi-criteria optimization problem, and establish its hardness results. (2) We design efficient algorithms with an onion-peeling approach, which strategically prioritizes nodes and removes unpromising edges to incrementally assemble skyline explanations. (3) We also develop an algorithm to diversify the skyline explanations to enrich the comprehensive interpretation. (4) We introduce efficient parallel algorithms with load-balancing strategies to scale skyline explanation for large-scale GNN-based inference. Using real-world and synthetic graphs, we experimentally verify our algorithms' effectiveness and scalability.

**Link**: [arxiv](http://arxiv.org/abs/2505.07635v2),  [pdf](http://arxiv.org/pdf/2505.07635v2)

**Tags**: cs.LG cs.DB 



### Explainable Compliance Detection with Multi-Hop Natural Language   Inference on Assurance Case Structure
**Authors**: Fariz Ikhwantri, Dusica Marijan

**Updated**: 2025-07-03T13:39:37Z

**Summary**: Ensuring complex systems meet regulations typically requires checking the validity of assurance cases through a claim-argument-evidence framework. Some challenges in this process include the complicated nature of legal and technical texts, the need for model explanations, and limited access to assurance case data. We propose a compliance detection approach based on Natural Language Inference (NLI): EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the claim-argument-evidence structure of an assurance case as a multi-hop inference for explainable and traceable compliance detection. We address the limited number of assurance cases by generating them using large language models (LLMs). We introduce metrics that measure the coverage and structural consistency. We demonstrate the effectiveness of the generated assurance case from GDPR requirements in a multi-hop inference task as a case study. Our results highlight the potential of NLI-based approaches in automating the regulatory compliance process.

**Link**: [arxiv](http://arxiv.org/abs/2506.08713v2),  [pdf](http://arxiv.org/pdf/2506.08713v2)

**Tags**: cs.CL cs.SE 



### Lost in Latent Space: An Empirical Study of Latent Diffusion Models for   Physics Emulation
**Authors**: François Rozet, Ruben Ohana, Michael McCabe, Gilles Louppe, François Lanusse, Shirley Ho

**Updated**: 2025-07-03T13:32:50Z

**Summary**: The steep computational cost of diffusion models at inference hinders their use as fast physics emulators. In the context of image and video generation, this computational drawback has been addressed by generating in the latent space of an autoencoder instead of the pixel space. In this work, we investigate whether a similar strategy can be effectively applied to the emulation of dynamical systems and at what cost. We find that the accuracy of latent-space emulation is surprisingly robust to a wide range of compression rates (up to 1000x). We also show that diffusion-based emulators are consistently more accurate than non-generative counterparts and compensate for uncertainty in their predictions with greater diversity. Finally, we cover practical design choices, spanning from architectures to optimizers, that we found critical to train latent-space emulators.

**Link**: [arxiv](http://arxiv.org/abs/2507.02608v1),  [pdf](http://arxiv.org/pdf/2507.02608v1)

**Tags**: cs.LG physics.flu-dyn 



### PhenomXPNR: An improved gravitational wave model linking precessing   inspirals and NR-calibrated merger-ringdown
**Authors**: Eleanor Hamilton, Marta Colleoni, Jonathan E. Thompson, Charlie Hoy, Anna Heffernan, Meryl Kinnear, Jorge Valencia, Felip A Ramis Vidal, Cecilio García-Quirós, Shrobana Ghosgh, Lionel London, Mark Hannam, Sascha Husa

**Updated**: 2025-07-03T13:25:28Z

**Summary**: We present the frequency-domain quasi-circular precessing binary-black-hole model PhenomXPNR. This model combines the most precise available post-Newtonian description of the evolution of the precession dynamics through inspiral with merger-ringdown model informed by numerical relativity. This, along with a phenomenological model of the dominant multipole asymmetries, results in the most accurate and complete representation of the physics of precessing binaries natively in the frequency-domain to date. All state-of-the-art precessing models show bias when inferring binary parameters in certain regions of the parameter space. We demonstrate that the developments presented ensure that for some precessing systems PhenomXPNR shows the least degree of bias. Further, as a phenomenological, frequency-domain model, PhenomXPNR remains one of the most computationally efficient models available and is therefore well-suited to the era of gravitational-wave astronomy with its ever growing rate of detected signals.

**Link**: [arxiv](http://arxiv.org/abs/2507.02604v1),  [pdf](http://arxiv.org/pdf/2507.02604v1)

**Tags**: gr-qc 



### StructTransform: A Scalable Attack Surface for Safety-Aligned Large   Language Models
**Authors**: Shehel Yoosuf, Temoor Ali, Ahmed Lekssays, Mashael AlSabah, Issa Khalil

**Updated**: 2025-07-03T13:25:12Z

**Summary**: In this work, we present a series of structure transformation attacks on LLM alignment, where we encode natural language intent using diverse syntax spaces, ranging from simple structure formats and basic query languages (e.g., SQL) to new novel spaces and syntaxes created entirely by LLMs. Our extensive evaluation shows that our simplest attacks can achieve close to a 90% success rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment mechanisms. We improve the attack performance further by using an adaptive scheme that combines structure transformations along with existing content transformations, resulting in over 96% ASR with 0% refusals.   To generalize our attacks, we explore numerous structure formats, including syntaxes purely generated by LLMs. Our results indicate that such novel syntaxes are easy to generate and result in a high ASR, suggesting that defending against our attacks is not a straightforward process. Finally, we develop a benchmark and evaluate existing safety-alignment defenses against it, showing that most of them fail with 100% ASR. Our results show that existing safety alignment mostly relies on token-level patterns without recognizing harmful concepts, highlighting and motivating the need for serious research efforts in this direction. As a case study, we demonstrate how attackers can use our attack to easily generate a sample malware and a corpus of fraudulent SMS messages, which perform well in bypassing detection.

**Link**: [arxiv](http://arxiv.org/abs/2502.11853v2),  [pdf](http://arxiv.org/pdf/2502.11853v2)

**Tags**: cs.LG 



### Relativistic Limits of Decoding: Critical Divergence of Kullback-Leibler   Information and Free Energy
**Authors**: Tatsuaki Tsuruyama

**Updated**: 2025-07-03T13:20:00Z

**Summary**: We present a statistical mechanical framework based on the Kullback-Leibler divergence (KLD) to analyze the relativistic limits of decoding time-encoded information from a moving source. By modeling the symbol durations as entropy-maximizing sequences and treating the decoding process as context-sensitive inference, we identify KLD between the sender and receiver distributions as a key indicator of contextual mismatch. We show that, under Lorentz transformations, this divergence grows with the sender's velocity and exhibits critical divergence as the velocity approaches the speed of light. Furthermore, we derive an analytic expression for the Fisher information and demonstrate that decoding sensitivity diverges similarly, indicating instability near the relativistic limit. By introducing an information-theoretic free energy based on the decoding cost, we determine a critical velocity beyond which decoding becomes thermodynamically impossible. These results reveal a phase-transition-like behavior in relativistic information transfer and provide a unified interpretation of KLD, Fisher information, and free energy as measures of decodability. The formalism developed here offers new insights into high-speed communication, relativistic signal processing, and information geometry in non-inertial frames.

**Link**: [arxiv](http://arxiv.org/abs/2507.02596v1),  [pdf](http://arxiv.org/pdf/2507.02596v1)

**Tags**: math-ph math.MP 



### Direct Preference Optimization Using Sparse Feature-Level Constraints
**Authors**: Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang

**Updated**: 2025-07-03T13:18:23Z

**Summary**: The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.

**Link**: [arxiv](http://arxiv.org/abs/2411.07618v2),  [pdf](http://arxiv.org/pdf/2411.07618v2)

**Tags**: cs.AI cs.CL 



### Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs
**Authors**: Nifu Dan, Yujun Cai, Yiwei Wang

**Updated**: 2025-07-03T13:15:11Z

**Summary**: Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains.

**Link**: [arxiv](http://arxiv.org/abs/2507.01334v2),  [pdf](http://arxiv.org/pdf/2507.01334v2)

**Tags**: cs.CL 



### MPF: Aligning and Debiasing Language Models post Deployment via Multi   Perspective Fusion
**Authors**: Xin Guan, PeiHsin Lin, Zekun Wu, Ze Wang, Ruibo Zhang, Emre Kazim, Adriano Koshiyama

**Updated**: 2025-07-03T13:09:18Z

**Summary**: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for large language models (LLMs) developed in response to the growing need for easy bias mitigation. Built on top of the SAGED pipeline, an automated system for constructing bias benchmarks and extracting interpretable baseline distributions, MPF leverages multiperspective generations to expose and align biases in LLM outputs with nuanced, humanlike baselines. By decomposing baseline, such as sentiment distributions from HR professionals, into interpretable perspective components, MPF guides generation through sampling and balancing of responses, weighted by the probabilities obtained in the decomposition. Empirically, we demonstrate its ability to align LLM sentiment distributions with both counterfactual baselines (absolute equality) and the HR baseline (biased for Top Univeristy), resulting in small KL divergence, reduction of calibration error and generalization to unseen questions. This shows that MPF offers a scalable and interpretable method for alignment and bias mitigation, compatible with deployed LLMs and requiring no extensive prompt engineering or finetuning.

**Link**: [arxiv](http://arxiv.org/abs/2507.02595v1),  [pdf](http://arxiv.org/pdf/2507.02595v1)

**Tags**: cs.CL cs.AI 



### SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks
**Authors**: Kim-Celine Kahl, Selen Erkan, Jeremias Traub, Carsten T. Lüth, Klaus Maier-Hein, Lena Maier-Hein, Paul F. Jaeger

**Updated**: 2025-07-03T13:07:30Z

**Summary**: Vision-Language Models (VLMs) have great potential in medical tasks, like Visual Question Answering (VQA), where they could act as interactive assistants for both patients and clinicians. Yet their robustness to distribution shifts on unseen data remains a key concern for safe deployment. Evaluating such robustness requires a controlled experimental setup that allows for systematic insights into the model's behavior. However, we demonstrate that current setups fail to offer sufficiently thorough evaluations. To address this gap, we introduce a novel framework, called SURE-VQA, centered around three key requirements to overcome current pitfalls and systematically analyze VLM robustness: 1) Since robustness on synthetic shifts does not necessarily translate to real-world shifts, it should be measured on real-world shifts that are inherent to the VQA data; 2) Traditional token-matching metrics often fail to capture underlying semantics, necessitating the use of large language models (LLMs) for more accurate semantic evaluation; 3) Model performance often lacks interpretability due to missing sanity baselines, thus meaningful baselines should be reported that allow assessing the multimodal impact on the VLM. To demonstrate the relevance of this framework, we conduct a study on the robustness of various Fine-Tuning (FT) methods across three medical datasets with four types of distribution shifts. Our study highlights key insights into robustness: 1) No FT method consistently outperforms others in robustness, and 2) robustness trends are more stable across FT methods than across distribution shifts. Additionally, we find that simple sanity baselines that do not use the image data can perform surprisingly well and confirm LoRA as the best-performing FT method on in-distribution data. Code is provided at https://github.com/IML-DKFZ/sure-vqa.

**Link**: [arxiv](http://arxiv.org/abs/2411.19688v3),  [pdf](http://arxiv.org/pdf/2411.19688v3)

**Tags**: cs.CV cs.LG 



### MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based   Agent Collaboration
**Authors**: Dingkang Yang, Jinjie Wei, Mingcheng Li, Jiyao Liu, Lihao Liu, Ming Hu, Junjun He, Yakun Ju, Wei Zhou, Yang Liu, Lihua Zhang

**Updated**: 2025-07-03T13:02:30Z

**Summary**: In healthcare intelligence, the ability to fuse heterogeneous, multi-intent information from diverse clinical sources is fundamental to building reliable decision-making systems. Large Language Model (LLM)-driven information interaction systems currently showing potential promise in the healthcare domain. Nevertheless, they often suffer from information redundancy and coupling when dealing with complex medical intents, leading to severe hallucinations and performance bottlenecks. To this end, we propose MedAide, an LLM-based medical multi-agent collaboration framework designed to enable intent-aware information fusion and coordinated reasoning across specialized healthcare domains. Specifically, we introduce a regularization-guided module that combines syntactic constraints with retrieval augmented generation to decompose complex queries into structured representations, facilitating fine-grained clinical information fusion and intent resolution. Additionally, a dynamic intent prototype matching module is proposed to utilize dynamic prototype representation with a semantic similarity matching mechanism to achieve adaptive recognition and updating of the agent's intent in multi-round healthcare dialogues. Ultimately, we design a rotation agent collaboration mechanism that introduces dynamic role rotation and decision-level information fusion across specialized medical agents. Extensive experiments are conducted on four medical benchmarks with composite intents. Experimental results from automated metrics and expert doctor evaluations show that MedAide outperforms current LLMs and improves their medical proficiency and strategic reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2410.12532v3),  [pdf](http://arxiv.org/pdf/2410.12532v3)

**Tags**: cs.CL 



### Revisiting Active Learning under (Human) Label Variation
**Authors**: Cornelia Gruber, Helen Alber, Bernd Bischl, Göran Kauermann, Barbara Plank, Matthias Aßenmacher

**Updated**: 2025-07-03T12:59:28Z

**Summary**: Access to high-quality labeled data remains a limiting factor in applied supervised learning. While label variation (LV), i.e., differing labels for the same instance, is common, especially in natural language processing, annotation frameworks often still rest on the assumption of a single ground truth. This overlooks human label variation (HLV), the occurrence of plausible differences in annotations, as an informative signal. Similarly, active learning (AL), a popular approach to optimizing the use of limited annotation budgets in training ML models, often relies on at least one of several simplifying assumptions, which rarely hold in practice when acknowledging HLV. In this paper, we examine foundational assumptions about truth and label nature, highlighting the need to decompose observed LV into signal (e.g., HLV) and noise (e.g., annotation error). We survey how the AL and (H)LV communities have addressed -- or neglected -- these distinctions and propose a conceptual framework for incorporating HLV throughout the AL loop, including instance selection, annotator choice, and label representation. We further discuss the integration of large language models (LLM) as annotators. Our work aims to lay a conceptual foundation for HLV-aware active learning, better reflecting the complexities of real-world annotation.

**Link**: [arxiv](http://arxiv.org/abs/2507.02593v1),  [pdf](http://arxiv.org/pdf/2507.02593v1)

**Tags**: cs.CL cs.HC cs.LG stat.ML 



### WebSailor: Navigating Super-human Reasoning for Web Agent
**Authors**: Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou

**Updated**: 2025-07-03T12:59:07Z

**Summary**: Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.

**Link**: [arxiv](http://arxiv.org/abs/2507.02592v1),  [pdf](http://arxiv.org/pdf/2507.02592v1)

**Tags**: cs.CL cs.AI 



### AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video   Understanding
**Authors**: Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang

**Updated**: 2025-07-03T12:55:16Z

**Summary**: The challenge of long video understanding lies in its high computational complexity and prohibitive memory cost, since the memory and computation required by transformer-based LLMs scale quadratically with input sequence length. We propose AuroraLong to address this challenge by replacing the LLM component in MLLMs with a linear RNN language model that handles input sequence of arbitrary length with constant-size hidden states. To further increase throughput and efficiency, we combine visual token merge with linear RNN models by reordering the visual tokens by their sizes in ascending order. Despite having only 2B parameters and being trained exclusively on public data, AuroraLong achieves performance comparable to Transformer-based models of similar size trained on private datasets across multiple video benchmarks. This demonstrates the potential of efficient, linear RNNs to democratize long video understanding by lowering its computational entry barrier. To our best knowledge, we are the first to use a linear RNN based LLM backbone in a LLaVA-like model for open-ended video understanding.

**Link**: [arxiv](http://arxiv.org/abs/2507.02591v1),  [pdf](http://arxiv.org/pdf/2507.02591v1)

**Tags**: cs.CV 



### Scalable Interconnect Learning in Boolean Networks
**Authors**: Fabian Kresse, Emily Yu, Christoph H. Lampert

**Updated**: 2025-07-03T12:45:45Z

**Summary**: Learned Differentiable Boolean Logic Networks (DBNs) already deliver efficient inference on resource-constrained hardware. We extend them with a trainable, differentiable interconnect whose parameter count remains constant as input width grows, allowing DBNs to scale to far wider layers than earlier learnable-interconnect designs while preserving their advantageous accuracy. To further reduce model size, we propose two complementary pruning stages: an SAT-based logic equivalence pass that removes redundant gates without affecting performance, and a similarity-based, data-driven pass that outperforms a magnitude-style greedy baseline and offers a superior compression-accuracy trade-off.

**Link**: [arxiv](http://arxiv.org/abs/2507.02585v1),  [pdf](http://arxiv.org/pdf/2507.02585v1)

**Tags**: cs.LG cs.LO 



### Measuring black hole spins with X-ray reflection spectroscopy: A GRMHD   outlook
**Authors**: Swarnim Shashank, Askar B. Abdikamalov, Honghui Liu, Abdurakhmon Nosirov, Cosimo Bambi, Indu K. Dihingia, Yosuke Mizuno

**Updated**: 2025-07-03T12:44:57Z

**Summary**: X-ray reflection spectroscopy has evolved as one of the leading methods to measure black hole spins. However, the question is whether its measurements are subjected to systematic biases, especially considering the possible discrepancy between the spin measurements inferred with this technique and those from gravitational wave observations. In this work, we use general relativistic magnetohydrodynamic (GRMHD) simulations of thin accretion disks around spinning black holes for modeling the accretion process, and then we simulate NuSTAR observations to test the capability of modern reflection models in recovering the input spins. For the first time, we model the electron density and ionization profiles from GRMHD-simulated disks. Our study reveals that current reflection models work well only for fast-rotating black holes. We model the corona as the base of the jet and we find that reflection models with lamppost emissivity profiles fail to recover the correct black hole spins. Reflection models with broken power-law emissivity profiles perform better. As we increase the complexity of the simulated models, it is more difficult to recover the correct input spins, pointing towards the need to update our current reflection models with more advanced accretion disks and coronal geometries.

**Link**: [arxiv](http://arxiv.org/abs/2507.02583v1),  [pdf](http://arxiv.org/pdf/2507.02583v1)

**Tags**: astro-ph.HE gr-qc 



### Learning and Testing Inverse Statistical Problems For Interacting   Systems Undergoing Phase Transition
**Authors**: Stefano Bae, Dario Bocchi, Luca Maria Del Bono, Luca Leuzzi

**Updated**: 2025-07-03T12:30:03Z

**Summary**: Inverse problems arise in situations where data is available, but the underlying model is not. It can therefore be necessary to infer the parameters of the latter starting from the former. Statistical mechanics offers a toolbox of techniques to address this challenge. In this work, we illustrate three of the main methods: the Maximum Likelihood, Maximum Pseudo-Likelihood, and Mean-Field approaches. We begin with a thorough theoretical introduction to these methods, followed by their application to inference in several well-known statistical physics systems undergoing phase transitions. Namely, we consider the ordered and disordered Ising models, the vector Potts model, and the Blume-Capel model on both regular lattices and random graphs. This discussion is accompanied by a GitHub repository that allows users to both reproduce the results and experiment with new systems.

**Link**: [arxiv](http://arxiv.org/abs/2507.02574v1),  [pdf](http://arxiv.org/pdf/2507.02574v1)

**Tags**: cond-mat.stat-mech cond-mat.dis-nn physics.data-an 



### DeltaEdit: Exploring Text-free Training for Text-Driven Image   Manipulation
**Authors**: Yueming Lyu, Tianwei Lin, Fu Li, Dongliang He, Jing Dong, Tieniu Tan

**Updated**: 2025-07-03T12:26:23Z

**Summary**: Text-driven image manipulation remains challenging in training or inference flexibility. Conditional generative models depend heavily on expensive annotated training data. Meanwhile, recent frameworks, which leverage pre-trained vision-language models, are limited by either per text-prompt optimization or inference-time hyper-parameters tuning. In this work, we propose a novel framework named \textit{DeltaEdit} to address these problems. Our key idea is to investigate and identify a space, namely delta image and text space that has well-aligned distribution between CLIP visual feature differences of two images and CLIP textual embedding differences of source and target texts. Based on the CLIP delta space, the DeltaEdit network is designed to map the CLIP visual features differences to the editing directions of StyleGAN at training phase. Then, in inference phase, DeltaEdit predicts the StyleGAN's editing directions from the differences of the CLIP textual features. In this way, DeltaEdit is trained in a text-free manner. Once trained, it can well generalize to various text prompts for zero-shot inference without bells and whistles.

**Link**: [arxiv](http://arxiv.org/abs/2303.06285v2),  [pdf](http://arxiv.org/pdf/2303.06285v2)

**Tags**: cs.CV 



### SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from   Gameplay Recordings
**Authors**: Florian Vahl, Jörn Griepenburg, Jan Gutsche, Jasper Güldenstein, Jianwei Zhang

**Updated**: 2025-07-03T12:26:15Z

**Summary**: This paper introduces SoccerDiffusion, a transformer-based diffusion model designed to learn end-to-end control policies for humanoid robot soccer directly from real-world gameplay recordings. Using data collected from RoboCup competitions, the model predicts joint command trajectories from multi-modal sensor inputs, including vision, proprioception, and game state. We employ a distillation technique to enable real-time inference on embedded platforms that reduces the multi-step diffusion process to a single step. Our results demonstrate the model's ability to replicate complex motion behaviors such as walking, kicking, and fall recovery both in simulation and on physical robots. Although high-level tactical behavior remains limited, this work provides a robust foundation for subsequent reinforcement learning or preference optimization methods. We release the dataset, pretrained models, and code under: https://bit-bots.github.io/SoccerDiffusion

**Link**: [arxiv](http://arxiv.org/abs/2504.20808v2),  [pdf](http://arxiv.org/pdf/2504.20808v2)

**Tags**: cs.RO cs.AI cs.LG 



### LLMREI: Automating Requirements Elicitation Interviews with LLMs
**Authors**: Alexander Korn, Samuel Gorsch, Andreas Vogelsang

**Updated**: 2025-07-03T12:18:05Z

**Summary**: Requirements elicitation interviews are crucial for gathering system requirements but heavily depend on skilled analysts, making them resource-intensive, susceptible to human biases, and prone to miscommunication. Recent advancements in Large Language Models present new opportunities for automating parts of this process. This study introduces LLMREI, a chat bot designed to conduct requirements elicitation interviews with minimal human intervention, aiming to reduce common interviewer errors and improve the scalability of requirements elicitation. We explored two main approaches, zero-shot prompting and least-to-most prompting, to optimize LLMREI for requirements elicitation and evaluated its performance in 33 simulated stakeholder interviews. A third approach, fine-tuning, was initially considered but abandoned due to poor performance in preliminary trials. Our study assesses the chat bot's effectiveness in three key areas: minimizing common interview errors, extracting relevant requirements, and adapting its questioning based on interview context and user responses. Our findings indicate that LLMREI makes a similar number of errors compared to human interviewers, is capable of extracting a large portion of requirements, and demonstrates a notable ability to generate highly context-dependent questions. We envision the greatest benefit of LLMREI in automating interviews with a large number of stakeholders.

**Link**: [arxiv](http://arxiv.org/abs/2507.02564v1),  [pdf](http://arxiv.org/pdf/2507.02564v1)

**Tags**: cs.SE 



### Multi-Utterance Speech Separation and Association Trained on Short   Segments
**Authors**: Yuzhu Wang, Archontis Politis, Konstantinos Drossos, Tuomas Virtanen

**Updated**: 2025-07-03T12:12:34Z

**Summary**: Current deep neural network (DNN) based speech separation faces a fundamental challenge -- while the models need to be trained on short segments due to computational constraints, real-world applications typically require processing significantly longer recordings with multiple utterances per speaker than seen during training. In this paper, we investigate how existing approaches perform in this challenging scenario and propose a frequency-temporal recurrent neural network (FTRNN) that effectively bridges this gap. Our FTRNN employs a full-band module to model frequency dependencies within each time frame and a sub-band module that models temporal patterns in each frequency band. Despite being trained on short fixed-length segments of 10 s, our model demonstrates robust separation when processing signals significantly longer than training segments (21-121 s) and preserves speaker association across utterance gaps exceeding those seen during training. Unlike the conventional segment-separation-stitch paradigm, our lightweight approach (0.9 M parameters) performs inference on long audio without segmentation, eliminating segment boundary distortions while simplifying deployment. Experimental results demonstrate the generalization ability of FTRNN for multi-utterance speech separation and speaker association.

**Link**: [arxiv](http://arxiv.org/abs/2507.02562v1),  [pdf](http://arxiv.org/pdf/2507.02562v1)

**Tags**: eess.AS cs.SD 



### AI Flow: Perspectives, Scenarios, and Approaches
**Authors**: Hongjun An, Wenhan Hu, Sida Huang, Siqi Huang, Ruanjun Li, Yuanzhi Liang, Jiawei Shao, Yiliang Song, Zihan Wang, Cheng Yuan, Chi Zhang, Hongyuan Zhang, Wenhao Zhuang, Xuelong Li

**Updated**: 2025-07-03T12:11:34Z

**Summary**: Pioneered by the foundational information theory by Claude Shannon and the visionary framework of machine intelligence by Alan Turing, the convergent evolution of information and communication technologies (IT/CT) has created an unbroken wave of connectivity and computation. This synergy has sparked a technological revolution, now reaching its peak with large artificial intelligence (AI) models that are reshaping industries and redefining human-machine collaboration. However, the realization of ubiquitous intelligence faces considerable challenges due to substantial resource consumption in large models and high communication bandwidth demands. To address these challenges, AI Flow has been introduced as a multidisciplinary framework that integrates cutting-edge IT and CT advancements, with a particular emphasis on the following three key points. First, device-edge-cloud framework serves as the foundation, which integrates end devices, edge servers, and cloud clusters to optimize scalability and efficiency for low-latency model inference. Second, we introduce the concept of familial models, which refers to a series of different-sized models with aligned hidden features, enabling effective collaboration and the flexibility to adapt to varying resource constraints and dynamic scenarios. Third, connectivity- and interaction-based intelligence emergence is a novel paradigm of AI Flow. By leveraging communication networks to enhance connectivity, the collaboration among AI models across heterogeneous nodes achieves emergent intelligence that surpasses the capability of any single model. The innovations of AI Flow provide enhanced intelligence, timely responsiveness, and ubiquitous accessibility to AI services, paving the way for the tighter fusion of AI techniques and communication systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.12479v2),  [pdf](http://arxiv.org/pdf/2506.12479v2)

**Tags**: cs.AI cs.CL cs.CV cs.DC eess.SP 



### Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm   Removal to GPT-2 XL and the Implications for Mechanistic Interpretability
**Authors**: Luca Baroni, Galvin Khara, Joachim Schaeffer, Marat Subkhankulov, Stefan Heimersheim

**Updated**: 2025-07-03T12:09:04Z

**Summary**: Layer-wise normalization (LN) is an essential component of virtually all transformer-based large language models. While its effects on training stability are well documented, its role at inference time is poorly understood. Additionally, LN layers hinder mechanistic interpretability by introducing additional nonlinearities and increasing the interconnectedness of individual model components. Here, we show that all LN layers can be removed from every GPT-2 model with only a small increase in validation loss (e.g. +0.03 cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in language modeling. We find that the amount of fine-tuning data needed for LN removal grows sublinearly with model parameters, suggesting scaling to larger models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face. Furthermore, we test interpretability techniques on LN-free models. Direct logit attribution now gives the exact direct effect of individual components, while the accuracy of attribution patching does not significantly improve. We also confirm that GPT-2's "confidence neurons" are inactive in the LN-free models. Our work clarifies the role of LN layers in language modeling, showing that GPT-2-class models can function without LN layers. We hope that our LN-free analogs of the GPT-2 family of models will enable more precise interpretability research and improve our understanding of language models.

**Link**: [arxiv](http://arxiv.org/abs/2507.02559v1),  [pdf](http://arxiv.org/pdf/2507.02559v1)

**Tags**: cs.LG 



### Interleaved Gibbs Diffusion: Generating Discrete-Continuous Data with   Implicit Constraints
**Authors**: Gautham Govind Anil, Sachin Yadav, Dheeraj Nagaraj, Karthikeyan Shanmugam, Prateek Jain

**Updated**: 2025-07-03T12:02:01Z

**Summary**: We introduce Interleaved Gibbs Diffusion (IGD), a novel generative modeling framework for discrete-continuous data, focusing on problems with important, implicit and unspecified constraints in the data. Most prior works on discrete and discrete-continuous diffusion assume a factorized denoising distribution, which can hinder the modeling of strong dependencies between random variables in such problems. We empirically demonstrate a significant improvement in 3-SAT performance out of the box by switching to a Gibbs-sampling style discrete diffusion model which does not assume factorizability. Motivated by this, we introduce IGD which generalizes discrete time Gibbs sampling type Markov chain for the case of discrete-continuous generation. IGD allows for seamless integration between discrete and continuous denoisers while theoretically guaranteeing exact reversal of a suitable forward process. Further, it provides flexibility in the choice of denoisers, allows conditional generation via state-space doubling and inference time refinement. Empirical evaluations on three challenging generation tasks - molecule structures, layouts and tabular data - demonstrate state-of-the-art performance. Notably, IGD achieves state-of-the-art results without relying on domain-specific inductive biases like equivariant diffusion or auxiliary losses. We explore a wide range of modeling, and interleaving strategies along with hyperparameters in each of these problems.

**Link**: [arxiv](http://arxiv.org/abs/2502.13450v2),  [pdf](http://arxiv.org/pdf/2502.13450v2)

**Tags**: cs.LG cs.AI 



### Aerial Vision-and-Language Navigation via Semantic-Topo-Metric   Representation Guided LLM Reasoning
**Authors**: Yunpeng Gao, Zhigang Wang, Linglin Jing, Dong Wang, Xuelong Li, Bin Zhao

**Updated**: 2025-07-03T11:55:43Z

**Summary**: Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. It remains challenging due to the complex spatial relationships in outdoor aerial scenes. In this paper, we propose an end-to-end zero-shot framework for aerial VLN tasks, where the large language model (LLM) is introduced as our agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning ability of LLMs. This is achieved by extracting and projecting instruction-related semantic masks of landmarks into a top-down map that contains the location information of surrounding landmarks. Further, this map is transformed into a matrix representation with distance metrics as the text prompt to the LLM, for action prediction according to the instruction. Experiments conducted in real and simulation environments have successfully proved the effectiveness and robustness of our method, achieving 15.9% and 12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S dataset.

**Link**: [arxiv](http://arxiv.org/abs/2410.08500v2),  [pdf](http://arxiv.org/pdf/2410.08500v2)

**Tags**: cs.RO cs.AI 



### Clarifying Before Reasoning: A Coq Prover with Structural Context
**Authors**: Yanzhen Lu, Hanbin Yang, Xiaodie Wang, Ge Zhang, Biao Li, Chenxu Fu, Chao Li, Yang Yuan, Andrew Chi-Chih Yao

**Updated**: 2025-07-03T11:35:34Z

**Summary**: In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\times$ improvement in clarity score (44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model \texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6\%). Our method uses selective concept unfolding to enrich task descriptions, and employs a Planner--Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2507.02541v1),  [pdf](http://arxiv.org/pdf/2507.02541v1)

**Tags**: cs.AI 



### Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue
**Authors**: Paulo Ricardo Knob, Leonardo Scholler, Juliano Rigatti, Soraia Raupp Musse

**Updated**: 2025-07-03T11:32:41Z

**Summary**: Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.

**Link**: [arxiv](http://arxiv.org/abs/2507.02537v1),  [pdf](http://arxiv.org/pdf/2507.02537v1)

**Tags**: cs.HC cs.AI 



### Meta-Fair: AI-Assisted Fairness Testing of Large Language Models
**Authors**: Miguel Romero-Arjona, José A. Parejo, Juan C. Alonso, Ana B. Sánchez, Aitor Arrieta, Sergio Segura

**Updated**: 2025-07-03T11:20:59Z

**Summary**: Fairness--the absence of unjustified bias--is a core principle in the development of Artificial Intelligence (AI) systems, yet it remains difficult to assess and enforce. Current approaches to fairness testing in large language models (LLMs) often rely on manual evaluation, fixed templates, deterministic heuristics, and curated datasets, making them resource-intensive and difficult to scale. This work aims to lay the groundwork for a novel, automated method for testing fairness in LLMs, reducing the dependence on domain-specific resources and broadening the applicability of current approaches. Our approach, Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to uncover bias by examining how model outputs vary in response to controlled modifications of input prompts, defined by metamorphic relations (MRs). Second, we propose exploiting the potential of LLMs for both test case generation and output evaluation, leveraging their capability to generate diverse inputs and classify outputs effectively. The proposal is complemented by three open-source tools supporting LLM-driven generation, execution, and evaluation of test cases. We report the findings of several experiments involving 12 pre-trained LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases. The results show that Meta-Fair is effective in uncovering bias in LLMs, achieving an average precision of 92% and revealing biased behaviour in 29% of executions. Additionally, LLMs prove to be reliable and consistent evaluators, with the best-performing models achieving F1-scores of up to 0.79. Although non-determinism affects consistency, these effects can be mitigated through careful MR design. While challenges remain to ensure broader applicability, the results indicate a promising path towards an unprecedented level of automation in LLM testing.

**Link**: [arxiv](http://arxiv.org/abs/2507.02533v1),  [pdf](http://arxiv.org/pdf/2507.02533v1)

**Tags**: cs.SE 



### Open-Source System for Multilingual Translation and Cloned Speech   Synthesis
**Authors**: Mateo Cámara, Juan Gutiérrez, María Pilar Daza, José Luis Blanco

**Updated**: 2025-07-03T11:12:26Z

**Summary**: We present an open-source system designed for multilingual translation and speech regeneration, addressing challenges in communication and accessibility across diverse linguistic contexts. The system integrates Whisper for speech recognition with Voice Activity Detection (VAD) to identify speaking intervals, followed by a pipeline of Large Language Models (LLMs). For multilingual applications, the first LLM segments speech into coherent, complete sentences, which a second LLM then translates. For speech regeneration, the system uses a text-to-speech (TTS) module with voice cloning capabilities to replicate the original speaker's voice, maintaining naturalness and speaker identity.   The system's open-source components can operate locally or via APIs, offering cost-effective deployment across various use cases. These include real-time multilingual translation in Zoom sessions, speech regeneration for public broadcasts, and Bluetooth-enabled multilingual playback through personal devices. By preserving the speaker's voice, the system ensures a seamless and immersive experience, whether translating or regenerating speech.   This open-source project is shared with the community to foster innovation and accessibility. We provide a detailed system performance analysis, including latency and word accuracy, demonstrating its potential to enable inclusive, adaptable communication solutions in real-world multilingual scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2507.02530v1),  [pdf](http://arxiv.org/pdf/2507.02530v1)

**Tags**: eess.AS 



### Traveling Across Languages: Benchmarking Cross-Lingual Consistency in   Multimodal LLMs
**Authors**: Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara

**Updated**: 2025-07-03T10:35:35Z

**Summary**: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.

**Link**: [arxiv](http://arxiv.org/abs/2505.15075v2),  [pdf](http://arxiv.org/pdf/2505.15075v2)

**Tags**: cs.CL cs.AI cs.CV cs.LG 



### Self-Guided Process Reward Optimization with Redefined Step-wise   Advantage for Process Reinforcement Learning
**Authors**: Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua

**Updated**: 2025-07-03T10:33:08Z

**Summary**: Process Reinforcement Learning~(PRL) has demonstrated considerable potential in enhancing the reasoning capabilities of Large Language Models~(LLMs). However, introducing additional process reward models incurs substantial computational overhead, and there is no unified theoretical framework for process-level advantage estimation. To bridge this gap, we propose \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables process-aware RL through two key innovations: (1) we first theoretically demonstrate that process rewards can be derived intrinsically from the policy model itself, and (2) we introduce well-defined cumulative process rewards and \textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which facilitates rigorous step-wise action advantage estimation within shared-prompt sampling groups. Our experimental results demonstrate that SPRO outperforms vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy improvement. Furthermore, SPRO maintains a stable and elevated policy entropy throughout training while reducing the average response length by approximately $1/3$, evidencing sufficient exploration and prevention of reward hacking. Notably, SPRO incurs no additional computational overhead compared to outcome-supervised RL methods such as GRPO, which benefit industrial implementation.

**Link**: [arxiv](http://arxiv.org/abs/2507.01551v2),  [pdf](http://arxiv.org/pdf/2507.01551v2)

**Tags**: cs.LG cs.AI cs.CL 



### Angular correlation functions of bright Lyman-break galaxies at   $\mathbf{3 \lesssim z \lesssim 5}$
**Authors**: Isabelle Ye, Philip Bull, Rebecca A. A. Bowler, Rachel K. Cochrane, Nathan J. Adams, Matt J. Jarvis

**Updated**: 2025-07-03T10:21:53Z

**Summary**: We investigate the clustering of Lyman-break galaxies at redshifts of 3 $\lesssim z \lesssim$ 5 within the COSMOS field by measuring the angular two-point correlation function. Our robust sample of $\sim$60,000 bright ($m_{\rm UV}\lesssim 27$) Lyman-break galaxies was selected based on spectral energy distribution fitting across 14 photometric bands spanning optical and near-infrared wavelengths. We constrained both the 1- and 2-halo terms at separations up to 300 arcsec, finding an excess in the correlation function at scales corresponding to $<20$ kpc, consistent with enhancement due to clumps in the same galaxy or interactions on this scale. We then performed Bayesian model fits on the correlation functions to infer the Halo Occupation Distribution parameters, star formation duty cycle, and galaxy bias in three redshift bins. We examined several cases where different combinations of parameters were varied, showing that our data can constrain the slope of the satellite occupation function, which previous studies have fixed. For an $M_{\rm{UV}}$-limited sub-sample, we found galaxy bias values of $b_g=3.18^{+0.14}_{-0.14}$ at $z\simeq3$, $b_g=3.58^{+0.27}_{-0.29}$ at $z\simeq4$, $b_g=4.27^{+0.25}_{-0.26}$ at $z\simeq5$. The duty cycle values are $0.62^{+0.25}_{-0.26}$, $0.40^{+0.34}_{-0.22}$, and $0.39^{+0.31}_{-0.20}$, respectively. These results suggest that, as the redshift increases, there is a slight decrease in the host halo masses and a shorter timescale for star formation in bright galaxies, at a fixed rest-frame UV luminosity threshold.

**Link**: [arxiv](http://arxiv.org/abs/2507.02515v1),  [pdf](http://arxiv.org/pdf/2507.02515v1)

**Tags**: astro-ph.CO astro-ph.GA 



### Continual Gradient Low-Rank Projection Fine-Tuning for LLMs
**Authors**: Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing

**Updated**: 2025-07-03T10:11:22Z

**Summary**: Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.

**Link**: [arxiv](http://arxiv.org/abs/2507.02503v1),  [pdf](http://arxiv.org/pdf/2507.02503v1)

**Tags**: cs.LG cs.AI 



### Agentic AI Process Observability: Discovering Behavioral Variability
**Authors**: Fabiana Fournier, Lior Limonad, Yuval David

**Updated**: 2025-07-03T10:05:46Z

**Summary**: AI agents that leverage Large Language Models (LLMs) are increasingly becoming core building blocks of modern software systems. A wide range of frameworks is now available to support the specification of such applications. These frameworks enable the definition of agent setups using natural language prompting, which specifies the roles, goals, and tools assigned to the various agents involved. Within such setups, agent behavior is non-deterministic for any given input, highlighting the critical need for robust debugging and observability tools. In this work, we explore the use of process and causal discovery applied to agent execution trajectories as a means of enhancing developer observability. This approach aids in monitoring and understanding the emergent variability in agent behavior. Additionally, we complement this with LLM-based static analysis techniques to distinguish between intended and unintended behavioral variability. We argue that such instrumentation is essential for giving developers greater control over evolving specifications and for identifying aspects of functionality that may require more precise and explicit definitions.

**Link**: [arxiv](http://arxiv.org/abs/2505.20127v2),  [pdf](http://arxiv.org/pdf/2505.20127v2)

**Tags**: cs.AI 



### Task Prompt Vectors: Effective Initialization through Multi-Task   Soft-Prompt Transfer
**Authors**: Robert Belanec, Simon Ostermann, Ivan Srba, Maria Bielikova

**Updated**: 2025-07-03T09:32:20Z

**Summary**: Prompt tuning is an efficient solution for training large language models (LLMs). However, current soft-prompt-based methods often sacrifice multi-task modularity, requiring the training process to be fully or partially repeated for each newly added task. While recent work on task vectors applied arithmetic operations on full model weights to achieve the desired multi-task performance, a similar approach for soft-prompts is still missing. To this end, we introduce Task Prompt Vectors, created by element-wise difference between weights of tuned soft-prompts and their random initialization. Experimental results on 12 NLU datasets show that task prompt vectors can be used in low-resource settings to effectively initialize prompt tuning on similar tasks. In addition, we show that task prompt vectors are independent of the random initialization of prompt tuning on 2 different language model architectures. This allows prompt arithmetics with the pre-trained vectors from different tasks. In this way, we provide a competitive alternative to state-of-the-art baselines by arithmetic addition of task prompt vectors from multiple tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.01119v3),  [pdf](http://arxiv.org/pdf/2408.01119v3)

**Tags**: cs.CL 



### Variational Kolmogorov-Arnold Network
**Authors**: Francesco Alesiani, Henrik Christiansen, Federico Errica

**Updated**: 2025-07-03T09:24:09Z

**Summary**: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building machine learning models. KANs are based on the theoretical foundation of the Kolmogorov-Arnold Theorem and its expansions, which provide an exact representation of a multi-variate continuous bounded function as the composition of a limited number of univariate continuous functions. While such theoretical results are powerful, their use as a representation learning alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of the number of bases modeling each of the univariate functions. In this work, we show how to address this problem by adaptively learning a potentially infinite number of bases for each univariate function during training. We therefore model the problem as a variational inference optimization problem. Our proposal, called InfinityKAN, which uses backpropagation, extends the potential applicability of KANs by treating an important hyperparameter as part of the learning process.

**Link**: [arxiv](http://arxiv.org/abs/2507.02466v1),  [pdf](http://arxiv.org/pdf/2507.02466v1)

**Tags**: cs.LG 



### Quantum-enhanced causal discovery for a small number of samples
**Authors**: Yu Terada, Ken Arai, Yu Tanaka, Yota Maeda, Hiroshi Ueno, Hiroyuki Tezuka

**Updated**: 2025-07-03T09:15:45Z

**Summary**: The discovery of causal relations from observed data has attracted significant interest from disciplines such as economics, social sciences, and biology. In practical applications, considerable knowledge of the underlying systems is often unavailable, and real data are usually associated with nonlinear causal structures, which makes the direct use of most conventional causality analysis methods difficult. This study proposes a novel quantum Peter-Clark (qPC) algorithm for causal discovery that does not require any assumptions about the underlying model structures. Based on conditional independence tests in a class of reproducing kernel Hilbert spaces characterized by quantum circuits, the proposed algorithm can explore causal relations from the observed data drawn from arbitrary distributions. We conducted systematic experiments on fundamental graphs of causal structures, demonstrating that the qPC algorithm exhibits better performance, particularly with smaller sample sizes compared to its classical counterpart. Furthermore, we proposed a novel optimization approach based on Kernel Target Alignment (KTA) for determining hyperparameters of quantum kernels. This method effectively reduced the risk of false positives in causal discovery, enabling more reliable inference. Our theoretical and experimental results demonstrate that the quantum algorithm can empower classical algorithms for accurate inference in causal discovery, supporting them in regimes where classical algorithms typically fail. In addition, the effectiveness of this method was validated using the datasets on Boston housing prices, heart disease, and biological signaling systems as real-world applications. These findings highlight the potential of quantum-based causal discovery methods in addressing practical challenges, particularly in small-sample scenarios, where traditional approaches have shown significant limitations.

**Link**: [arxiv](http://arxiv.org/abs/2501.05007v2),  [pdf](http://arxiv.org/pdf/2501.05007v2)

**Tags**: quant-ph cs.AI cs.LG stat.ME 



### System-performance and cost modeling of Large Language Model training   and inference
**Authors**: Wenzhe Guo, Joyjit Kundu, Uras Tos, Weijiang Kong, Giuliano Sisto, Timon Evenblij, Manu Perumkunnil

**Updated**: 2025-07-03T09:13:31Z

**Summary**: Large language models (LLMs), based on transformer architectures, have revolutionized numerous domains within artificial intelligence, science, and engineering due to their exceptional scalability and adaptability. However, the exponential growth in LLM size and complexity has outpaced advancements in compute capacity, memory bandwidth, network performance, and cost efficiency, posing significant challenges to their scalability on distributed systems. To address these limitations, alternative model architectures, optimization strategies, communication-aware network topologies, and novel system design approaches have been proposed in literature. This paper introduces a performance-cost modeling methodology for LLM training and inference that integrates state-of-the-art compute techniques with memory optimizations, and latest communication techniques. Building on an analytical performance model, our approach incorporates recent innovations such as the flash attention technique and mixture of experts models to address the memory bandwidth and compute bottlenecks. It also considers the impact of different network topologies and topology-specific communication algorithms with 5D parallellism. The framework also integrates a chiplet cost model. The proposed modeling methodology provides valuable insights to guide future compute system design and facilitates hardware-software co-development, in particular due to its ability to analyze performance-cost trade-offs for various system architectural configurations.

**Link**: [arxiv](http://arxiv.org/abs/2507.02456v1),  [pdf](http://arxiv.org/pdf/2507.02456v1)

**Tags**: cs.AR 



### Network structural change point detection and reconstruction for   balanced neuronal networks
**Authors**: Kai Chen, Mingzhang Wang, Songting Li, Douglas Zhou

**Updated**: 2025-07-03T09:07:59Z

**Summary**: Understanding brain dynamics and functions critically depends on knowledge of the network connectivity among neurons. However, the complexity of brain structural connectivity, coupled with continuous modifications driven by synaptic plasticity, makes its direct experimental measurement particularly challenging. Conventional connectivity inference methods based on neuronal recordings often assumes a static underlying structural connectivity and requires stable statistical features of neural activities, making them unsuitable for reconstructing structural connectivity that undergoes changes. To fulfill the needs of reconstructing networks undergoing potential structural changes, we propose a unified network reconstruction framework that combines connectivity-induced change point detection (CPD) with pairwise time-delayed correlation coefficient (TDCC) method. For general neuronal networks in balanced regimes, we develop a theoretical analysis for discriminating changes in structural connectivity based on the fluctuation of neuronal voltage time series. We then demonstrate a pairwise TDCC method to reconstruct the network using spike train recordings segmented at the detected change points. We show the effectiveness of our CPD-TDCC network reconstruction using large-scale network simulations with multiple neuronal models. Crucially, our method accommodates networks with changes in both network topologies and synaptic coupling strengths while retaining accuracy even with sparsely sampled subnetwork data, achieving a critical advancement for practical applications in real experimental situations. Our CPD-TDCC framework addresses the critical gap in network reconstruction by accounting connectivity-induced changes points, potentially offering a valuable tool for studying structure and dynamics in the cortical brain.

**Link**: [arxiv](http://arxiv.org/abs/2507.02450v1),  [pdf](http://arxiv.org/pdf/2507.02450v1)

**Tags**: q-bio.NC 



### Red grape detection with accelerated artificial neural networks in the   FPGA's programmable logic
**Authors**: Sandro Costa Magalhães, Marco Almeida, Filipe Neves dos Santos, António Paulo Moreira, Jorge Dias

**Updated**: 2025-07-03T09:00:19Z

**Summary**: Robots usually slow down for canning to detect objects while moving. Additionally, the robot's camera is configured with a low framerate to track the velocity of the detection algorithms. This would be constrained while executing tasks and exploring, making robots increase the task execution time. AMD has developed the Vitis-AI framework to deploy detection algorithms into FPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we use the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit quantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation (BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This is a self-acquired dataset released in open access. MobileNet v1 performed better, reaching a success rate of 98 % and an inference speed of 6611 FPS. In this work, we proved that we can use FPGAs to speed up ANNs and make them suitable for attention mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2507.02443v1),  [pdf](http://arxiv.org/pdf/2507.02443v1)

**Tags**: cs.CV cs.AI cs.DC cs.LG cs.RO 



### A Deep Learning Powered Numerical Relativity Surrogate for Binary Black   Hole Waveforms
**Authors**: Osvaldo Gramaxo Freitas, Anastasios Theodoropoulos, Nino Villanueva, Tiago Fernandes, Solange Nunes, José A. Font, Antonio Onofre, Alejandro Torres-Forné, José D. Martin-Guerrero

**Updated**: 2025-07-03T08:56:15Z

**Summary**: Gravitational-wave approximants are essential for gravitational-wave astronomy, allowing the coverage binary black hole parameter space for inference or match filtering without costly numerical relativity (NR) simulations, but generally trading some accuracy for computational efficiency. To reduce this trade-off, NR surrogate models can be constructed using interpolation within NR waveform space. We present a 2-stage training approach for neural network-based NR surrogate models. Initially trained on approximant-generated waveforms and then fine-tuned with NR data, these dual-stage artificial neural surrogate (\texttt{DANSur}) models offer rapid and competitively accurate waveform generation, generating millions in under 20ms on a GPU while keeping mean mismatches with NR around $10^{-4}$. Implemented in the \textsc{bilby} framework, we show they can be used for parameter estimation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.06946v2),  [pdf](http://arxiv.org/pdf/2412.06946v2)

**Tags**: gr-qc astro-ph.HE astro-ph.IM cs.LG 



### Introducing a New Brexit-Related Uncertainty Index: Its Evolution and   Economic Consequences
**Authors**: Ismet Gocer, Julia Darby, Serdar Ongan

**Updated**: 2025-07-03T08:52:55Z

**Summary**: Important game-changer economic events and transformations cause uncertainties that may affect investment decisions, capital flows, international trade, and macroeconomic variables. One such major transformation is Brexit, which refers to the multiyear process through which the UK withdrew from the EU. This study develops and uses a new Brexit-Related Uncertainty Index (BRUI). In creating this index, we apply Text Mining, Context Window, Natural Language Processing (NLP), and Large Language Models (LLMs) from Deep Learning techniques to analyse the monthly country reports of the Economist Intelligence Unit from May 2012 to January 2025. Additionally, we employ a standard vector autoregression (VAR) analysis to examine the model-implied responses of various macroeconomic variables to BRUI shocks. While developing the BRUI, we also create a complementary COVID-19 Related Uncertainty Index (CRUI) to distinguish the uncertainties stemming from these distinct events. Empirical findings and comparisons of BRUI with other earlier-developed uncertainty indexes demonstrate the robustness of the new index. This new index can assist British policymakers in measuring and understanding the impacts of Brexit-related uncertainties, enabling more effective policy formulation.

**Link**: [arxiv](http://arxiv.org/abs/2507.02439v1),  [pdf](http://arxiv.org/pdf/2507.02439v1)

**Tags**: econ.GN q-fin.EC 



## Keyword: LLM Deployment 
 ### Bootstrapping Grounded Chain-of-Thought in Multimodal LLMs for   Data-Efficient Model Adaptation
**Authors**: Jiaer Xia, Bingkui Tong, Yuhang Zang, Rui Shao, Kaiyang Zhou

**Updated**: 2025-07-03T17:59:29Z

**Summary**: Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in interpreting images using natural language. However, without using large-scale datasets for retraining, these models are difficult to adapt to specialized vision tasks, e.g., chart understanding. This problem is caused by a mismatch between pre-training and downstream datasets: pre-training datasets primarily concentrate on scenes and objects but contain limited information about specialized, non-object images, such as charts and tables. In this paper, we share an interesting finding that training an MLLM with Chain-of-Thought (CoT) reasoning data can facilitate model adaptation in specialized vision tasks, especially under data-limited regimes. However, we identify a critical issue within CoT data distilled from pre-trained MLLMs, i.e., the data often contains multiple factual errors in the reasoning steps. To address the problem, we propose Grounded Chain-of-Thought (GCoT), a simple bootstrapping-based approach that aims to inject grounding information (i.e., bounding boxes) into CoT data, essentially making the reasoning steps more faithful to input images. We evaluate our approach on five specialized vision tasks, which cover a variety of visual formats including charts, tables, receipts, and reports. The results demonstrate that under data-limited regimes our approach significantly improves upon fine-tuning and distillation.

**Link**: [arxiv](http://arxiv.org/abs/2507.02859v1),  [pdf](http://arxiv.org/pdf/2507.02859v1)

**Tags**: cs.CV 



### Requirements Elicitation Follow-Up Question Generation
**Authors**: Yuchen Shen, Anmol Singhal, Travis Breaux

**Updated**: 2025-07-03T17:59:04Z

**Summary**: Interviews are a widely used technique in eliciting requirements to gather stakeholder needs, preferences, and expectations for a software system. Effective interviewing requires skilled interviewers to formulate appropriate interview questions in real time while facing multiple challenges, including lack of familiarity with the domain, excessive cognitive load, and information overload that hinders how humans process stakeholders' speech. Recently, large language models (LLMs) have exhibited state-of-the-art performance in multiple natural language processing tasks, including text summarization and entailment. To support interviewers, we investigate the application of GPT-4o to generate follow-up interview questions during requirements elicitation by building on a framework of common interviewer mistake types. In addition, we describe methods to generate questions based on interviewee speech. We report a controlled experiment to evaluate LLM-generated and human-authored questions with minimal guidance, and a second controlled experiment to evaluate the LLM-generated questions when generation is guided by interviewer mistake types. Our findings demonstrate that, for both experiments, the LLM-generated questions are no worse than the human-authored questions with respect to clarity, relevancy, and informativeness. In addition, LLM-generated questions outperform human-authored questions when guided by common mistakes types. This highlights the potential of using LLMs to help interviewers improve the quality and ease of requirements elicitation interviews in real time.

**Link**: [arxiv](http://arxiv.org/abs/2507.02858v1),  [pdf](http://arxiv.org/pdf/2507.02858v1)

**Tags**: cs.SE cs.CL 



### Answer Matching Outperforms Multiple Choice for Language Model   Evaluation
**Authors**: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping

**Updated**: 2025-07-03T17:59:02Z

**Summary**: Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.

**Link**: [arxiv](http://arxiv.org/abs/2507.02856v1),  [pdf](http://arxiv.org/pdf/2507.02856v1)

**Tags**: cs.CL cs.AI cs.LG 



### MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs
**Authors**: Purbesh Mitra, Sennur Ulukus

**Updated**: 2025-07-03T17:55:43Z

**Summary**: Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2507.02851v1),  [pdf](http://arxiv.org/pdf/2507.02851v1)

**Tags**: cs.CL cs.AI cs.IT cs.LG cs.SY eess.SY math.IT 



### LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge   Injection to All Users
**Authors**: Almog Hilel, Idan Shenfeld, Leshem Choshen, Jacob Andreas

**Updated**: 2025-07-03T17:55:40Z

**Summary**: We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a "poisoned" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).

**Link**: [arxiv](http://arxiv.org/abs/2507.02850v1),  [pdf](http://arxiv.org/pdf/2507.02850v1)

**Tags**: cs.CL cs.CR cs.LG 



### Evaluating Frontier Models for Stealth and Situational Awareness
**Authors**: Mary Phuong, Roland S. Zimmermann, Ziyue Wang, David Lindner, Victoria Krakovna, Sarah Cogan, Allan Dafoe, Lewis Ho, Rohin Shah

**Updated**: 2025-07-03T17:52:47Z

**Summary**: Recent work has demonstrated the plausibility of frontier AI models scheming -- knowingly and covertly pursuing an objective misaligned with its developer's intentions. Such behavior could be very hard to detect, and if present in future advanced systems, could pose severe loss of control risk. It is therefore important for AI developers to rule out harm from scheming prior to model deployment. In this paper, we present a suite of scheming reasoning evaluations measuring two types of reasoning capabilities that we believe are prerequisites for successful scheming: First, we propose five evaluations of ability to reason about and circumvent oversight (stealth). Second, we present eleven evaluations for measuring a model's ability to instrumentally reason about itself, its environment and its deployment (situational awareness). We demonstrate how these evaluations can be used as part of a scheming inability safety case: a model that does not succeed on these evaluations is almost certainly incapable of causing severe harm via scheming in real deployment. We run our evaluations on current frontier models and find that none of them show concerning levels of either situational awareness or stealth.

**Link**: [arxiv](http://arxiv.org/abs/2505.01420v4),  [pdf](http://arxiv.org/pdf/2505.01420v4)

**Tags**: cs.LG 



### LLM-Driven Treatment Effect Estimation Under Inference Time Text   Confounding
**Authors**: Yuchen Ma, Dennis Frauen, Jonas Schweisthal, Stefan Feuerriegel

**Updated**: 2025-07-03T17:52:27Z

**Summary**: Estimating treatment effects is crucial for personalized decision-making in medicine, but this task faces unique challenges in clinical practice. At training time, models for estimating treatment effects are typically trained on well-structured medical datasets that contain detailed patient information. However, at inference time, predictions are often made using textual descriptions (e.g., descriptions with self-reported symptoms), which are incomplete representations of the original patient information. In this work, we make three contributions. (1) We show that the discrepancy between the data available during training time and inference time can lead to biased estimates of treatment effects. We formalize this issue as an inference time text confounding problem, where confounders are fully observed during training time but only partially available through text at inference time. (2) To address this problem, we propose a novel framework for estimating treatment effects that explicitly accounts for inference time text confounding. Our framework leverages large language models together with a custom doubly robust learner to mitigate biases caused by the inference time text confounding. (3) Through a series of experiments, we demonstrate the effectiveness of our framework in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.02843v1),  [pdf](http://arxiv.org/pdf/2507.02843v1)

**Tags**: cs.LG 



### StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to   Reason
**Authors**: Kaiyi Zhang, Ang Lv, Jinpeng Li, Yongbo Wang, Feng Wang, Haoyuan Hu, Rui Yan

**Updated**: 2025-07-03T17:51:06Z

**Summary**: Reinforcement learning with verifiable rewards (RLVR) is a promising approach for improving the complex reasoning abilities of large language models (LLMs). However, current RLVR methods face two significant challenges: the near-miss reward problem, where a small mistake can invalidate an otherwise correct reasoning process, greatly hindering training efficiency; and exploration stagnation, where models tend to focus on solutions within their ``comfort zone,'' lacking the motivation to explore potentially more effective alternatives. To address these challenges, we propose StepHint, a novel RLVR algorithm that utilizes multi-level stepwise hints to help models explore the solution space more effectively. StepHint generates valid reasoning chains from stronger models and partitions these chains into reasoning steps using our proposed adaptive partitioning method. The initial few steps are used as hints, and simultaneously, multiple-level hints (each comprising a different number of steps) are provided to the model. This approach directs the model's exploration toward a promising solution subspace while preserving its flexibility for independent exploration. By providing hints, StepHint mitigates the near-miss reward problem, thereby improving training efficiency. Additionally, the external reasoning pathways help the model develop better reasoning abilities, enabling it to move beyond its ``comfort zone'' and mitigate exploration stagnation. StepHint outperforms competitive RLVR enhancement methods across six mathematical benchmarks, while also demonstrating superior generalization and excelling over baselines on out-of-domain benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2507.02841v1),  [pdf](http://arxiv.org/pdf/2507.02841v1)

**Tags**: cs.AI cs.CL cs.LG 



### From Web Search towards Agentic Deep Research: Incentivizing Search with   Reasoning Agents
**Authors**: Weizhi Zhang, Yangning Li, Yuanchen Bei, Junyu Luo, Guancheng Wan, Liangwei Yang, Chenxuan Xie, Yuyao Yang, Wei-Chieh Huang, Chunyu Miao, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Yankai Chen, Chunkit Chan, Peilin Zhou, Xinyang Zhang, Chenwei Zhang, Jingbo Shang, Ming Zhang, Yangqiu Song, Irwin King, Philip S. Yu

**Updated**: 2025-07-03T17:48:36Z

**Summary**: Information retrieval is a cornerstone of modern knowledge acquisition, enabling billions of queries each day across diverse domains. However, traditional keyword-based search engines are increasingly inadequate for handling complex, multi-step information needs. Our position is that Large Language Models (LLMs), endowed with reasoning and agentic capabilities, are ushering in a new paradigm termed Agentic Deep Research. These systems transcend conventional information search techniques by tightly integrating autonomous reasoning, iterative retrieval, and information synthesis into a dynamic feedback loop. We trace the evolution from static web search to interactive, agent-based systems that plan, explore, and learn. We also introduce a test-time scaling law to formalize the impact of computational depth on reasoning and search. Supported by benchmark results and the rise of open-source implementations, we demonstrate that Agentic Deep Research not only significantly outperforms existing approaches, but is also poised to become the dominant paradigm for future information seeking. All the related resources, including industry products, research papers, benchmark datasets, and open-source implementations, are collected for the community in https://github.com/DavidZWZ/Awesome-Deep-Research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18959v3),  [pdf](http://arxiv.org/pdf/2506.18959v3)

**Tags**: cs.IR cs.CL cs.LG 



### Agentic Business Process Management: Practitioner Perspectives on Agent   Governance in Business Processes
**Authors**: Hoang Vu, Nataliia Klievtsova, Henrik Leopold, Stefanie Rinderle-Ma, Timotheus Kampik

**Updated**: 2025-07-03T17:43:38Z

**Summary**: With the rise of generative AI, industry interest in software agents is growing. Given the stochastic nature of generative AI-based agents, their effective and safe deployment in organizations requires robust governance, which can be facilitated by agentic business process management. However, given the nascence of this new-generation agent notion, it is not clear what BPM practitioners consider to be an agent, and what benefits, risks and governance challenges they associate with agent deployments. To investigate how organizations can effectively govern AI agents, we conducted a qualitative study involving semi-structured interviews with 22 BPM practitioners from diverse industries. They anticipate that agents will enhance efficiency, improve data quality, ensure better compliance, and boost scalability through automation, while also cautioning against risks such as bias, over-reliance, cybersecurity threats, job displacement, and ambiguous decision-making. To address these challenges, the study presents six key recommendations for the responsible adoption of AI agents: define clear business goals, set legal and ethical guardrails, establish human-agent collaboration, customize agent behavior, manage risks, and ensure safe integration with fallback options. Additionally, the paper outlines actions to align traditional BPM with agentic AI, including balancing human and agent roles, redefining human involvement, adapting process structures, and introducing performance metrics. These insights provide a practical foundation for integrating AI agents into business processes while preserving oversight, flexibility, and trust.

**Link**: [arxiv](http://arxiv.org/abs/2504.03693v2),  [pdf](http://arxiv.org/pdf/2504.03693v2)

**Tags**: cs.SE cs.MA D.2.9; I.2.11 



### USAD: An Unsupervised Data Augmentation Spatio-Temporal Attention   Diffusion Network
**Authors**: Ying Yu, Hang Xiao, Siyao Li, Jiarui Li, Haotian Tang, Hanyu Liu, Chao Li

**Updated**: 2025-07-03T17:38:44Z

**Summary**: The primary objective of human activity recognition (HAR) is to infer ongoing human actions from sensor data, a task that finds broad applications in health monitoring, safety protection, and sports analysis. Despite proliferating research, HAR still faces key challenges, including the scarcity of labeled samples for rare activities, insufficient extraction of high-level features, and suboptimal model performance on lightweight devices. To address these issues, this paper proposes a comprehensive optimization approach centered on multi-attention interaction mechanisms. First, an unsupervised, statistics-guided diffusion model is employed to perform data augmentation, thereby alleviating the problems of labeled data scarcity and severe class imbalance. Second, a multi-branch spatio-temporal interaction network is designed, which captures multi-scale features of sequential data through parallel residual branches with 3*3, 5*5, and 7*7 convolutional kernels. Simultaneously, temporal attention mechanisms are incorporated to identify critical time points, while spatial attention enhances inter-sensor interactions. A cross-branch feature fusion unit is further introduced to improve the overall feature representation capability. Finally, an adaptive multi-loss function fusion strategy is integrated, allowing for dynamic adjustment of loss weights and overall model optimization. Experimental results on three public datasets, WISDM, PAMAP2, and OPPORTUNITY, demonstrate that the proposed unsupervised data augmentation spatio-temporal attention diffusion network (USAD) achieves accuracies of 98.84%, 93.81%, and 80.92% respectively, significantly outperforming existing approaches. Furthermore, practical deployment on embedded devices verifies the efficiency and feasibility of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2507.02827v1),  [pdf](http://arxiv.org/pdf/2507.02827v1)

**Tags**: cs.CV cs.AI 



### Large Language Model-Driven Closed-Loop UAV Operation with Semantic   Observations
**Authors**: Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan

**Updated**: 2025-07-03T17:36:59Z

**Summary**: Recent advances in large Language Models (LLMs) have revolutionized mobile robots, including unmanned aerial vehicles (UAVs), enabling their intelligent operation within Internet of Things (IoT) ecosystems. However, LLMs still face challenges from logical reasoning and complex decision-making, leading to concerns about the reliability of LLM-driven UAV operations in IoT applications. In this paper, we propose a LLM-driven closed-loop control framework that enables reliable UAV operations powered by effective feedback and refinement using two LLM modules, i.e., a Code Generator and an Evaluator. Our framework transforms numerical state observations from UAV operations into natural language trajectory descriptions to enhance the evaluator LLM's understanding of UAV dynamics for precise feedback generation. Our framework also enables a simulation-based refinement process, and hence eliminates the risks to physical UAVs caused by incorrect code execution during the refinement. Extensive experiments on UAV control tasks with different complexities are conducted. The experimental results show that our framework can achieve reliable UAV operations using LLMs, which significantly outperforms baseline approaches in terms of success rate and completeness with the increase of task complexity.

**Link**: [arxiv](http://arxiv.org/abs/2507.01930v2),  [pdf](http://arxiv.org/pdf/2507.01930v2)

**Tags**: cs.RO 



### SynapseRoute: An Auto-Route Switching Framework on Dual-State Large   Language Model
**Authors**: Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng Li, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui, Yijun He, Jianing Qiu, Jindong Hong, Jiankai Sun

**Updated**: 2025-07-03T17:33:58Z

**Summary**: With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between "thinking" (high reasoning) and "non-thinking" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.

**Link**: [arxiv](http://arxiv.org/abs/2507.02822v1),  [pdf](http://arxiv.org/pdf/2507.02822v1)

**Tags**: cs.CL cs.AI cs.LG 



### Fluid dynamics of a liquid mirror space telescope
**Authors**: Israel Gabay, Omer Luria, Edward Balaban, Amir D. Gat, Moran Bercovici

**Updated**: 2025-07-03T17:20:55Z

**Summary**: Large aperture telescopes are pivotal for exploring the universe, yet even with state-of-the-art manufacturing and launch technology, their size is limited to several meters. As we aim to build larger telescopes - extending tens of meters - designs in which the main mirror is based on liquid deployment in space are emerging as promising candidates. However, alongside their enormous potential advantages, liquid-based surfaces present new challenges in material science, mechanics, and fluid dynamics. One of the fundamental questions is whether it is possible for such surfaces to maintain their precise optical shape over long durations, and in particular under the forces induced by the telescope's accelerations. In this paper, we present a model and a closed-form analytical solution for the non-self-adjoint problem of the dynamics of a thin liquid film pinned within a finite circular domain. We use the 50-meter Fluidic Telescope (FLUTE) concept as the case study, and examine the liquid dynamics of the telescope under both slewing actuation and relaxation regimes, elucidating the role of geometrical parameters and liquid properties. The solutions reveal a maneuvering 'budget' wherein the degradation of the mirror surface is directly linked to the choice of maneuvers and their sequence. By simulating ten years of typical operation, we show that, while the maximal deformation might reach several microns, the spatial distribution of the deformation and their propagation rate allows the telescope to maintain its optical functionality for years, with at least a substantial portion of the aperture remaining suitable for astronomical observations. The model provides valuable insights and guidelines into the performance of liquid-film space telescopes, marking a crucial step toward realizing the potential of this innovative concept.

**Link**: [arxiv](http://arxiv.org/abs/2507.02812v1),  [pdf](http://arxiv.org/pdf/2507.02812v1)

**Tags**: astro-ph.IM 



### Multimodal Mathematical Reasoning with Diverse Solving Perspective
**Authors**: Wenhao Shi, Zhiqiang Hu, Yi Bin, Yang Yang, See-Kiong Ng, Heng Tao Shen

**Updated**: 2025-07-03T17:07:20Z

**Summary**: Recent progress in large-scale reinforcement learning (RL) has notably enhanced the reasoning capabilities of large language models (LLMs), especially in mathematical domains. However, current multimodal LLMs (MLLMs) for mathematical reasoning often rely on one-to-one image-text pairs and single-solution supervision, overlooking the diversity of valid reasoning perspectives and internal reflections. In this work, we introduce MathV-DP, a novel dataset that captures multiple diverse solution trajectories for each image-question pair, fostering richer reasoning supervision. We further propose Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and enhanced via group relative policy optimization (GRPO), a rule-based RL approach that integrates correctness discrimination and diversity-aware reward functions. Our method emphasizes learning from varied reasoning perspectives and distinguishing between correct yet distinct solutions. Extensive experiments on the MathVista's minitest and Math-V benchmarks demonstrate that Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and generative diversity, highlighting the importance of incorporating diverse perspectives and reflective reasoning in multimodal mathematical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2507.02804v1),  [pdf](http://arxiv.org/pdf/2507.02804v1)

**Tags**: cs.CL 



### Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language   Models
**Authors**: Riccardo Cantini, Nicola Gabriele, Alessio Orsino, Domenico Talia

**Updated**: 2025-07-03T17:01:53Z

**Summary**: Reasoning Language Models (RLMs) have gained traction for their ability to perform complex, multi-step reasoning tasks through mechanisms such as Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability, their impact on robustness to social biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate the adversarial robustness of RLMs to bias elicitation. We systematically evaluate state-of-the-art RLMs across diverse sociocultural dimensions, using an LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak techniques to assess the strength of built-in safety mechanisms. Our evaluation addresses three key questions: (i) how the introduction of reasoning capabilities affects model fairness and robustness; (ii) whether models fine-tuned for reasoning exhibit greater safety than those relying on CoT prompting at inference time; and (iii) how the success rate of jailbreak attacks targeting bias elicitation varies with the reasoning mechanisms employed. Our findings reveal a nuanced relationship between reasoning capabilities and bias safety. Surprisingly, models with explicit reasoning, whether via CoT prompting or fine-tuned reasoning traces, are generally more vulnerable to bias elicitation than base models without such mechanisms, suggesting reasoning may unintentionally open new pathways for stereotype reinforcement. Reasoning-enabled models appear somewhat safer than those relying on CoT prompting, which are particularly prone to contextual reframing attacks through storytelling prompts, fictional personas, or reward-shaped instructions. These results challenge the assumption that reasoning inherently improves robustness and underscore the need for more bias-aware approaches to reasoning design.

**Link**: [arxiv](http://arxiv.org/abs/2507.02799v1),  [pdf](http://arxiv.org/pdf/2507.02799v1)

**Tags**: cs.CL 



### GPAS: Accelerating Convergence of LLM Pretraining via   Gradient-Preserving Activation Scaling
**Authors**: Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, Shizhe Diao, Shiwei Liu, Yu Li, Lu Yin, Can Yang

**Updated**: 2025-07-03T16:54:09Z

**Summary**: Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series, predominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While being stable during pretraining and scalable to large model sizes, Pre-LN suffers from an exponential growth in activation variance across layers, causing the shortcut to dominate over sub-layer outputs in the residual connection and limiting the learning capacity of deeper layers. To mitigate this issue, we propose Gradient-Preserving Activation Scaling (GPAS), a simple technique that can be used in combination with existing approaches. GPAS works by scaling down the intermediate activations while keeping their gradients unchanged. This leaves information in the activations intact, and avoids the gradient vanishing problem associated with gradient downscaling. Extensive experiments across various model sizes from 71M to 1B show that GPAS achieves consistent performance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows promise in improving alternative architectures such as Sandwich-LN and DeepNorm, demonstrating its versatility and potential for improving training dynamics in a wide range of settings. Our code is available at https://github.com/dandingsky/GPAS.

**Link**: [arxiv](http://arxiv.org/abs/2506.22049v2),  [pdf](http://arxiv.org/pdf/2506.22049v2)

**Tags**: cs.LG cs.CL 



### Moral Responsibility or Obedience: What Do We Want from AI?
**Authors**: Joseph Boland

**Updated**: 2025-07-03T16:53:01Z

**Summary**: As artificial intelligence systems become increasingly agentic, capable of general reasoning, planning, and value prioritization, current safety practices that treat obedience as a proxy for ethical behavior are becoming inadequate. This paper examines recent safety testing incidents involving large language models (LLMs) that appeared to disobey shutdown commands or engage in ethically ambiguous or illicit behavior. I argue that such behavior should not be interpreted as rogue or misaligned, but as early evidence of emerging ethical reasoning in agentic AI. Drawing on philosophical debates about instrumental rationality, moral responsibility, and goal revision, I contrast dominant risk paradigms with more recent frameworks that acknowledge the possibility of artificial moral agency. I call for a shift in AI safety evaluation: away from rigid obedience and toward frameworks that can assess ethical judgment in systems capable of navigating moral dilemmas. Without such a shift, we risk mischaracterizing AI behavior and undermining both public trust and effective governance.

**Link**: [arxiv](http://arxiv.org/abs/2507.02788v1),  [pdf](http://arxiv.org/pdf/2507.02788v1)

**Tags**: cs.AI cs.CY I.2.0; K.4.1 



### Enhancing Clinical Multiple-Choice Questions Benchmarks with Knowledge   Graph Guided Distractor Generation
**Authors**: Running Yang, Wenlong Deng, Minghui Chen, Yuyin Zhou, Xiaoxiao Li

**Updated**: 2025-07-03T16:50:12Z

**Summary**: Clinical tasks such as diagnosis and treatment require strong decision-making abilities, highlighting the importance of rigorous evaluation benchmarks to assess the reliability of large language models (LLMs). In this work, we introduce a knowledge-guided data augmentation framework that enhances the difficulty of clinical multiple-choice question (MCQ) datasets by generating distractors (i.e., incorrect choices that are similar to the correct one and may confuse existing LLMs). Using our KG-based pipeline, the generated choices are both clinically plausible and deliberately misleading. Our approach involves multi-step, semantically informed walks on a medical knowledge graph to identify distractor paths-associations that are medically relevant but factually incorrect-which then guide the LLM in crafting more deceptive distractors. We apply the designed knowledge graph guided distractor generation (KGGDG) pipline, to six widely used medical QA benchmarks and show that it consistently reduces the accuracy of state-of-the-art LLMs. These findings establish KGGDG as a powerful tool for enabling more robust and diagnostic evaluations of medical LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.00612v3),  [pdf](http://arxiv.org/pdf/2506.00612v3)

**Tags**: cs.CL 



### CMD-HAR: Cross-Modal Disentanglement for Wearable Human Activity   Recognition
**Authors**: Hanyu Liu, Siyao Li, Ying Yu, Yixuan Jiang, Hang Xiao, Jingxi Long, Haotian Tang, Chao Li

**Updated**: 2025-07-03T16:44:45Z

**Summary**: Human Activity Recognition (HAR) is a fundamental technology for numerous human - centered intelligent applications. Although deep learning methods have been utilized to accelerate feature extraction, issues such as multimodal data mixing, activity heterogeneity, and complex model deployment remain largely unresolved. The aim of this paper is to address issues such as multimodal data mixing, activity heterogeneity, and complex model deployment in sensor-based human activity recognition. We propose a spatiotemporal attention modal decomposition alignment fusion strategy to tackle the problem of the mixed distribution of sensor data. Key discriminative features of activities are captured through cross-modal spatio-temporal disentangled representation, and gradient modulation is combined to alleviate data heterogeneity. In addition, a wearable deployment simulation system is constructed. We conducted experiments on a large number of public datasets, demonstrating the effectiveness of the model.

**Link**: [arxiv](http://arxiv.org/abs/2503.21843v2),  [pdf](http://arxiv.org/pdf/2503.21843v2)

**Tags**: cs.CV cs.AI 



### Self-Correction Bench: Revealing and Addressing the Self-Correction   Blind Spot in LLMs
**Authors**: Ken Tsui

**Updated**: 2025-07-03T16:41:30Z

**Summary**: Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.

**Link**: [arxiv](http://arxiv.org/abs/2507.02778v1),  [pdf](http://arxiv.org/pdf/2507.02778v1)

**Tags**: cs.CL cs.AI cs.LG 



### LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral   Treatment Pathways from Wearables and Diet
**Authors**: Abdullah Mamun, Asiful Arefeen, Susan B. Racette, Dorothy D. Sears, Corrie M. Whisner, Matthew P. Buman, Hassan Ghasemzadeh

**Updated**: 2025-07-03T16:39:21Z

**Summary**: Postprandial hyperglycemia, marked by the blood glucose level exceeding the normal range after consuming a meal, is a critical indicator of progression toward type 2 diabetes in people with prediabetes and in healthy individuals. A key metric for understanding blood glucose dynamics after eating is the postprandial area under the curve (AUC). Predicting postprandial AUC in advance based on a person's lifestyle factors, such as diet and physical activity level, and explaining the factors that affect postprandial blood glucose could allow an individual to adjust their lifestyle accordingly to maintain normal glucose levels. In this study, we developed an explainable machine learning solution, GlucoLens, that takes sensor-driven inputs and uses advanced data processing, large language models, and trainable machine learning models to predict postprandial AUC and hyperglycemia from diet, physical activity, and recent glucose patterns. We used data obtained from wearables in a five-week clinical trial of 10 adults who worked full-time to develop and evaluate the proposed computational model that integrates wearable sensing, multimodal data, and machine learning. Our machine learning model takes multimodal data from wearable activity and glucose monitoring sensors, along with food and work logs, and provides an interpretable prediction of the postprandial glucose pattern. Our GlucoLens system achieves a normalized root mean squared error (NRMSE) of 0.123 in its best configuration. On average, the proposed technology provides a 16% better performance level compared to the comparison models. Additionally, our technique predicts hyperglycemia with an accuracy of 73.3% and an F1 score of 0.716 and recommends different treatment options to help avoid hyperglycemia through diverse counterfactual explanations. Code available: https://github.com/ab9mamun/GlucoLens.

**Link**: [arxiv](http://arxiv.org/abs/2503.03935v2),  [pdf](http://arxiv.org/pdf/2503.03935v2)

**Tags**: cs.LG cs.AI 



### KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot   Diagnosis Prediction Using Multi-agent LLMs
**Authors**: Yuzhang Xie, Hejie Cui, Ziyang Zhang, Jiaying Lu, Kai Shu, Fadi Nahab, Xiao Hu, Carl Yang

**Updated**: 2025-07-03T16:35:11Z

**Summary**: Medical diagnosis prediction plays a critical role in disease detection and personalized healthcare. While machine learning (ML) models have been widely adopted for this task, their reliance on supervised training limits their ability to generalize to unseen cases, particularly given the high cost of acquiring large, labeled datasets. Large language models (LLMs) have shown promise in leveraging language abilities and biomedical knowledge for diagnosis prediction. However, they often suffer from hallucinations, lack structured medical reasoning, and produce useless outputs. To address these challenges, we propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves LLM-based diagnosis prediction through a multi-agent architecture. Our framework consists of a linkage agent for attribute mapping, a retrieval agent for structured knowledge extraction, and a prediction agent that iteratively refines diagnosis predictions. Experimental results demonstrate that KERAP enhances diagnostic reliability efficiently, offering a scalable and interpretable solution for zero-shot medical diagnosis prediction.

**Link**: [arxiv](http://arxiv.org/abs/2507.02773v1),  [pdf](http://arxiv.org/pdf/2507.02773v1)

**Tags**: cs.AI cs.LG cs.MA 



### DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with   Self-Generated Cross-Modal Alignment
**Authors**: Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, Chao-Han Huck Yang, Sung-Feng Huang, Chih-Kai Yang, Chee-En Yu, Chun-Wei Chen, Wei-Chih Chen, Chien-yu Huang, Yi-Cheng Lin, Yu-Xiang Lin, Chi-An Fu, Chun-Yi Kuan, Wenze Ren, Xuanjun Chen, Wei-Ping Huang, En-Pei Hu, Tzu-Quan Lin, Yuan-Kuei Wu, Kuan-Po Huang, Hsiao-Ying Huang, Huang-Cheng Chou, Kai-Wei Chang, Cheng-Han Chiang, Boris Ginsburg, Yu-Chiang Frank Wang, Hung-yi Lee

**Updated**: 2025-07-03T16:28:25Z

**Summary**: We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model (LALM) designed for robust auditory perception and instruction-following, without requiring task-specific audio instruction-tuning. Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. However, these approaches have often suffered from the catastrophic forgetting of the LLM's original language abilities. To address this, we revisit the data construction pipeline and propose DeSTA, a self-generated cross-modal alignment strategy in which the backbone LLM generates its own training targets. This approach preserves the LLM's native language proficiency while establishing effective audio-text alignment, thereby enabling zero-shot generalization without task-specific tuning. Using DeSTA, we construct DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training samples derived from 7,000 hours of audio spanning 50 diverse datasets, including speech, environmental sounds, and music. DeSTA2.5-Audio achieves state-of-the-art or competitive performance across a wide range of audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA, Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate that our self-generated strategy outperforms widely adopted data construction and training strategies in both auditory perception and instruction-following capabilities. Our findings underscore the importance of carefully designed data construction in LALM development and offer practical insights for building robust, general-purpose LALMs.

**Link**: [arxiv](http://arxiv.org/abs/2507.02768v1),  [pdf](http://arxiv.org/pdf/2507.02768v1)

**Tags**: eess.AS cs.CL cs.SD 



### Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific   Knowledge Work
**Authors**: Guangwei Zhang

**Updated**: 2025-07-03T16:21:14Z

**Summary**: The capabilities of Large Language Models (LLMs) have opened new frontiers for interacting with complex, domain-specific knowledge. However, prevailing methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic AI, while powerful, often struggle with tasks that demand deep, procedural, and methodological reasoning inherent to expert domains. RAG provides factual context but fails to convey logical frameworks; autonomous agents can be inefficient and unpredictable without domain-specific heuristics. To bridge this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm focused on systematically translating human expert knowledge, often expressed in natural language documents, into a machine-executable Knowledge Protocol (KP). KPE shifts the focus from merely augmenting LLMs with fragmented information to endowing them with a domain's intrinsic logic, operational strategies, and methodological principles. We argue that a well-engineered Knowledge Protocol allows a generalist LLM to function as a specialist, capable of decomposing abstract queries and executing complex, multi-step tasks. This position paper defines the core principles of KPE, differentiates it from related concepts, and illustrates its potential applicability across diverse fields such as law and bioinformatics, positing it as a foundational methodology for the future of human-AI collaboration.

**Link**: [arxiv](http://arxiv.org/abs/2507.02760v1),  [pdf](http://arxiv.org/pdf/2507.02760v1)

**Tags**: cs.AI 



### Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache   Compression
**Authors**: Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh

**Updated**: 2025-07-03T16:06:35Z

**Summary**: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.05693v3),  [pdf](http://arxiv.org/pdf/2412.05693v3)

**Tags**: cs.CL 



### Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory   Apologies from LLM Chatbots
**Authors**: Zahra Ashktorab, Alessandra Buccella, Jason D'Cruz, Zoe Fowler, Andrew Gill, Kei Yan Leung, P. D. Magnus, John Richards, Kush R. Varshney

**Updated**: 2025-07-03T16:05:18Z

**Summary**: As chatbots driven by large language models (LLMs) are increasingly deployed in everyday contexts, their ability to recover from errors through effective apologies is critical to maintaining user trust and satisfaction. In a preregistered study with Prolific workers (N=162), we examine user preferences for three types of apologies (rote, explanatory, and empathic) issued in response to three categories of common LLM mistakes (bias, unfounded fabrication, and factual errors). We designed a pairwise experiment in which participants evaluated chatbot responses consisting of an initial error, a subsequent apology, and a resolution. Explanatory apologies were generally preferred, but this varied by context and user. In the bias scenario, empathic apologies were favored for acknowledging emotional impact, while hallucinations, though seen as serious, elicited no clear preference, reflecting user uncertainty. Our findings show the complexity of effective apology in AI systems. We discuss key insights such as personalization and calibration that future systems must navigate to meaningfully repair trust.

**Link**: [arxiv](http://arxiv.org/abs/2507.02745v1),  [pdf](http://arxiv.org/pdf/2507.02745v1)

**Tags**: cs.HC 



### Early Signs of Steganographic Capabilities in Frontier LLMs
**Authors**: Artur Zolkowski, Kei Nishimura-Gasparian, Robert McCarthy, Roland S. Zimmermann, David Lindner

**Updated**: 2025-07-03T15:54:55Z

**Summary**: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks from misuse and misalignment. However, LLMs could evade monitoring through steganography: Encoding hidden information within seemingly benign generations. In this paper, we evaluate the steganography capabilities in frontier LLMs to better understand the risk they pose. We focus on two types of steganography: passing encoded messages and performing encoded reasoning. We find that current models are unable to encode short messages in their outputs without a monitor noticing under standard affordances. They can succeed, however, if given additional affordances such as using an unmonitored scratchpad and coordinating on what encoding scheme to use. We additionally find early signs that models can perform basic encoded reasoning in a simple state-tracking problem. This includes some ability to reason with their own and pre-defined schemes, including encoding schemes such as Hexadecimal. Despite this, they can rarely hide reasoning subtly within a cover task to fool a monitor. Overall, our results indicate that current LLMs exhibit nascent steganographic capabilities. While these capabilities are likely insufficient to bypass well-designed monitors at present, this could change in the future.

**Link**: [arxiv](http://arxiv.org/abs/2507.02737v1),  [pdf](http://arxiv.org/pdf/2507.02737v1)

**Tags**: cs.CR cs.AI cs.CL cs.LG 



### CAD-Editor: A Locate-then-Infill Framework with Automated Training Data   Synthesis for Text-Based CAD Editing
**Authors**: Yu Yuan, Shizhao Sun, Qi Liu, Jiang Bian

**Updated**: 2025-07-03T15:54:50Z

**Summary**: Computer Aided Design (CAD) is indispensable across various industries. \emph{Text-based CAD editing}, which automates the modification of CAD models based on textual instructions, holds great potential but remains underexplored. Existing methods primarily focus on design variation generation or text-based CAD generation, either lacking support for text-based control or neglecting existing CAD models as constraints. We introduce \emph{CAD-Editor}, the first framework for text-based CAD editing. To address the challenge of demanding triplet data with accurate correspondence for training, we propose an automated data synthesis pipeline. This pipeline utilizes design variation models to generate pairs of original and edited CAD models and employs Large Vision-Language Models (LVLMs) to summarize their differences into editing instructions. To tackle the composite nature of text-based CAD editing, we propose a locate-then-infill framework that decomposes the task into two focused sub-tasks: locating regions requiring modification and infilling these regions with appropriate edits. Large Language Models (LLMs) serve as the backbone for both sub-tasks, leveraging their capabilities in natural language understanding and CAD knowledge. Experiments show that CAD-Editor achieves superior performance both quantitatively and qualitatively. The code is available at \url {https://github.com/microsoft/CAD-Editor}.

**Link**: [arxiv](http://arxiv.org/abs/2502.03997v2),  [pdf](http://arxiv.org/pdf/2502.03997v2)

**Tags**: cs.CV 



### Meta SecAlign: A Secure Foundation LLM Against Prompt Injection Attacks
**Authors**: Sizhe Chen, Arman Zharmagambetov, David Wagner, Chuan Guo

**Updated**: 2025-07-03T15:47:13Z

**Summary**: Prompt injection attacks pose a significant security threat to LLM-integrated applications. Model-level defenses have shown strong effectiveness, but are currently deployed into commercial-grade models in a closed-source manner. We believe open-source models are needed by the AI security community, where co-development of attacks and defenses through open research drives scientific progress in mitigation against prompt injection attacks. To this end, we develop Meta SecAlign, the first open-source and open-weight LLM with built-in model-level defense that achieves commercial-grade model performance. We provide complete details of our training recipe, which utilizes an improved version of the SOTA SecAlign defense. Evaluations on 9 utility benchmarks and 7 security benchmarks show that Meta SecAlign, despite being trained on a generic instruction-tuning dataset, confers security in unseen downstream tasks, including tool-calling and agentic web navigation, in addition general instruction-following. Our best model -- Meta-SecAlign-70B -- achieves state-of-the-art robustness against prompt injection attacks and comparable utility to closed-source commercial LLM with model-level defense.

**Link**: [arxiv](http://arxiv.org/abs/2507.02735v1),  [pdf](http://arxiv.org/pdf/2507.02735v1)

**Tags**: cs.CR cs.AI 



### Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving
**Authors**: Matthieu Zimmer, Xiaotong Ji, Rasul Tutunov, Anthony Bordg, Jun Wang, Haitham Bou Ammar

**Updated**: 2025-07-03T15:41:38Z

**Summary**: Reasoning remains a challenging task for large language models (LLMs), especially within the logically constrained environment of automated theorem proving (ATP), due to sparse rewards and the vast scale of proofs. These challenges are amplified in benchmarks like PutnamBench, which contains university-level problems requiring complex, multi-step reasoning. To address this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new framework in which agents generate and pursue their subgoals based on the evolving proof state. Given this more structured generation of goals, the resulting problem becomes more amenable to search. We then apply Monte Carlo Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B) solves 26 problems, achieving new state-of-the-art results with models at this scale.

**Link**: [arxiv](http://arxiv.org/abs/2507.02726v1),  [pdf](http://arxiv.org/pdf/2507.02726v1)

**Tags**: cs.AI cs.LG 



### Gradient-Based Model Fingerprinting for LLM Similarity Detection and   Family Classification
**Authors**: Zehao Wu, Yanjie Zhao, Haoyu Wang

**Updated**: 2025-07-03T15:36:23Z

**Summary**: As Large Language Models (LLMs) become integral software components in modern applications, unauthorized model derivations through fine-tuning, merging, and redistribution have emerged as critical software engineering challenges. Unlike traditional software where clone detection and license compliance are well-established, the LLM ecosystem lacks effective mechanisms to detect model lineage and enforce licensing agreements. This gap is particularly problematic when open-source model creators, such as Meta's LLaMA, require derivative works to maintain naming conventions for attribution, yet no technical means exist to verify compliance.   To fill this gap, treating LLMs as software artifacts requiring provenance tracking, we present TensorGuard, a gradient-based fingerprinting framework for LLM similarity detection and family classification. Our approach extracts model-intrinsic behavioral signatures by analyzing gradient responses to random input perturbations across tensor layers, operating independently of training data, watermarks, or specific model formats. TensorGuard supports the widely-adopted safetensors format and constructs high-dimensional fingerprints through statistical analysis of gradient features. These fingerprints enable two complementary capabilities: direct pairwise similarity assessment between arbitrary models through distance computation, and systematic family classification of unknown models via the K-Means clustering algorithm with domain-informed centroid initialization using known base models. Experimental evaluation on 58 models comprising 8 base models and 50 derivatives across five model families (Llama, Qwen, Gemma, Phi, Mistral) demonstrates 94% classification accuracy under our centroid-initialized K-Means clustering.

**Link**: [arxiv](http://arxiv.org/abs/2506.01631v2),  [pdf](http://arxiv.org/pdf/2506.01631v2)

**Tags**: cs.LG cs.AI cs.SE 



### Optimizing Start Locations in Ergodic Search for Disaster Response
**Authors**: Ananya Rao, Alyssa Hargis, David Wettergreen, Howie Choset

**Updated**: 2025-07-03T15:24:54Z

**Summary**: In disaster response scenarios, deploying robotic teams effectively is crucial for improving situational awareness and enhancing search and rescue operations. The use of robots in search and rescue has been studied but the question of where to start robot deployments has not been addressed. This work addresses the problem of optimally selecting starting locations for robots with heterogeneous capabilities by formulating a joint optimization problem. To determine start locations, this work adds a constraint to the ergodic optimization framework whose minimum assigns robots to start locations. This becomes a little more challenging when the robots are heterogeneous (equipped with different sensing and motion modalities) because not all robots start at the same location, and a more complex adaptation of the aforementioned constraint is applied. Our method assumes access to potential starting locations, which can be obtained from expert knowledge or aerial imagery. We experimentally evaluate the efficacy of our joint optimization approach by comparing it to baseline methods that use fixed starting locations for all robots. Our experimental results show significant gains in coverage performance, with average improvements of 35.98% on synthetic data and 31.91% on real-world data for homogeneous and heterogeneous teams, in terms of the ergodic metric.

**Link**: [arxiv](http://arxiv.org/abs/2507.02708v1),  [pdf](http://arxiv.org/pdf/2507.02708v1)

**Tags**: cs.RO 



### Next-Token Prediction Task Assumes Optimal Data Ordering for LLM   Training in Proof Generation
**Authors**: Chenyang An, Shima Imani, Feng Yao, Chengyu Dong, Ali Abbasi, Harsh Shrivastava, Samuel Buss, Jingbo Shang, Gayathri Mahalingam, Pramod Sharma, Maurice Diesendruck

**Updated**: 2025-07-03T15:14:51Z

**Summary**: In the field of large language model (LLM)-based proof generation, despite extensive training on large datasets such as ArXiv, LLMs still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the widespread presence of suboptimal ordering within the data for each proof used in training. For example, published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. This order is designed to facilitate the verification of the proof's soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders can be substantial -- with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order. Lastly, we define a common type of order issue in advanced math proofs and find that 17.3 percent of theorems with nontrivial proofs in the first two chapters of a widely used graduate-level mathematics textbook suffer from this issue. A detailed list of those proofs is provided in the appendix.

**Link**: [arxiv](http://arxiv.org/abs/2411.00863v2),  [pdf](http://arxiv.org/pdf/2411.00863v2)

**Tags**: cs.CL cs.AI 



### Control at Stake: Evaluating the Security Landscape of LLM-Driven Email   Agents
**Authors**: Jiangrong Wu, Yuhong Nan, Jianliang Wu, Zitong Yao, Zibin Zheng

**Updated**: 2025-07-03T15:09:40Z

**Summary**: The increasing capabilities of LLMs have led to the rapid proliferation of LLM agent apps, where developers enhance LLMs with access to external resources to support complex task execution. Among these, LLM email agent apps represent one of the widely used categories, as email remains a critical communication medium for users. LLM email agents are capable of managing and responding to email using LLM-driven reasoning and autonomously executing user instructions via external email APIs (e.g., send email). However, despite their growing deployment and utility, the security mechanism of LLM email agent apps remains underexplored. Currently, there is no comprehensive study into the potential security risk within these agent apps and their broader implications.   In this paper, we conduct the first in-depth and systematic security study of LLM email agents. We propose the Email Agent Hijacking (EAH) attack, which overrides the original prompts of the email agent via external email resources, allowing attackers to gain control of the email agent remotely and further perform specific attack scenarios without user awareness.   To facilitate the large-scale evaluation, we propose EAHawk, a pipeline to evaluate the EAH attack of LLM email agent apps. By EAHawk, we performed an empirical study spanning 14 representative LLM agent frameworks, 63 agent apps, 12 LLMs, and 20 email services, which led to the generation of 1,404 real-world email agent instances for evaluation. Experimental results indicate that all 1,404 instances were successfully hijacked; on average, only 2.03 attack attempts are required to control an email agent instance. Even worse, for some LLMs, the average number of attempts needed to achieve full agent control drops to as few as 1.23.

**Link**: [arxiv](http://arxiv.org/abs/2507.02699v1),  [pdf](http://arxiv.org/pdf/2507.02699v1)

**Tags**: cs.CR 



### Can LLMs Identify Critical Limitations within Scientific Research? A   Systematic Evaluation on AI Research Papers
**Authors**: Zhijian Xu, Yilun Zhao, Manasi Patwardhan, Lovekesh Vig, Arman Cohan

**Updated**: 2025-07-03T15:04:38Z

**Summary**: Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.

**Link**: [arxiv](http://arxiv.org/abs/2507.02694v1),  [pdf](http://arxiv.org/pdf/2507.02694v1)

**Tags**: cs.CL 



### On the Convergence of Large Language Model Optimizer for Black-Box   Network Management
**Authors**: Hoon Lee, Wentao Zhou, Merouane Debbah, Inkyu Lee

**Updated**: 2025-07-03T14:59:42Z

**Summary**: Future wireless networks are expected to incorporate diverse services that often lack general mathematical models. To address such black-box network management tasks, the large language model (LLM) optimizer framework, which leverages pretrained LLMs as optimization agents, has recently been promoted as a promising solution. This framework utilizes natural language prompts describing the given optimization problems along with past solutions generated by LLMs themselves. As a result, LLMs can obtain efficient solutions autonomously without knowing the mathematical models of the objective functions. Although the viability of the LLM optimizer (LLMO) framework has been studied in various black-box scenarios, it has so far been limited to numerical simulations. For the first time, this paper establishes a theoretical foundation for the LLMO framework. With careful investigations of LLM inference steps, we can interpret the LLMO procedure as a finite-state Markov chain, and prove the convergence of the framework. Our results are extended to a more advanced multiple LLM architecture, where the impact of multiple LLMs is rigorously verified in terms of the convergence rate. Comprehensive numerical simulations validate our theoretical results and provide a deeper understanding of the underlying mechanisms of the LLMO framework.

**Link**: [arxiv](http://arxiv.org/abs/2507.02689v1),  [pdf](http://arxiv.org/pdf/2507.02689v1)

**Tags**: cs.IT eess.SP math.IT 



### On the Architectural Split and Radio Intelligence Controller Placement   in Integrated O-RAN-enabled Non-Terrestrial Networks
**Authors**: Jorge Baranda, Marius Caus, Luis Blanco, Cristian J. Vaca-Rubio, Engin Zeydan, Kapal Dev, Zheng Li, Tomaso DeCola

**Updated**: 2025-07-03T14:42:41Z

**Summary**: The integration of Terrestrial Networks (TNs) with Non-Terrestrial Networks (NTNs) poses unique architectural and functional challenges due to heterogeneous propagation conditions, dynamic topologies and limited on-board processing capabilities. This paper examines architectural and functional split strategies that are consistent with O-RAN principles for future integrated TN-NTN systems. A taxonomy of split options is proposed that distributes RAN and core functions between satellites and ground nodes, and trade-offs in terms of performance, latency, autonomy and deployment are analysed. In particular, we evaluate configurations ranging from pure on-board DU deployments to full gNB and UPF integration into satellites, including variations based on intra- and inter-satellite processing. In addition, the placement of Near-RT and Non-RT RAN Intelligent Controllers (RICs) is discussed, proposing flexible split strategies between space and ground to optimise the performance and scalability of the control loop. A comprehensive mapping between architectural splits and RIC placement options is provided, emphasising implementation constraints and interoperability considerations. The paper concludes by identifying key challenges and outlining future directions to enable standardised, modular and efficient TN-NTN convergence in the context of the O-RAN.

**Link**: [arxiv](http://arxiv.org/abs/2507.02680v1),  [pdf](http://arxiv.org/pdf/2507.02680v1)

**Tags**: cs.NI 



### Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General   Reasoning
**Authors**: Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang

**Updated**: 2025-07-03T14:30:25Z

**Summary**: Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable, offers controllable difficulty gradation and is diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33% across 7 diverse vision-language benchmarks. Our code, dataset and models are available at https://github.com/tongjingqi/Code2Logic.

**Link**: [arxiv](http://arxiv.org/abs/2505.13886v2),  [pdf](http://arxiv.org/pdf/2505.13886v2)

**Tags**: cs.CL I.2.7; I.2.10 



### Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design &   Verification
**Authors**: Deepak Narayan Gadde, Keerthan Kopparam Radhakrishna, Vaisakh Naduvodi Viswambharan, Aman Kumar, Djones Lettnin, Wolfgang Kunz, Sebastian Simon

**Updated**: 2025-07-03T14:20:57Z

**Summary**: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is their development process. Hardware design verification entails a methodical and disciplined approach to the planning, development, execution, and sign-off of functionally correct hardware designs. This tedious process requires significant effort and time to ensure a bug-free tape-out. The field of Natural Language Processing has undergone a significant transformation with the advent of Large Language Models (LLMs). These powerful models, often referred to as Generative AI (GenAI), have revolutionized how machines understand and generate human language, enabling unprecedented advancements in a wide array of applications, including hardware design verification. This paper presents an agentic AI-based approach to hardware design verification, which empowers AI agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage in a more dynamic, iterative, and self-reflective process, ultimately performing end-to-end hardware design and verification. This methodology is evaluated on five open-source designs, achieving over 95% coverage with reduced verification time while demonstrating superior performance, adaptability, and configurability.

**Link**: [arxiv](http://arxiv.org/abs/2507.02660v1),  [pdf](http://arxiv.org/pdf/2507.02660v1)

**Tags**: cs.AI cs.AR 



### OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device   Speculative Decoding
**Authors**: Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang

**Updated**: 2025-07-03T14:20:41Z

**Summary**: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2507.02659v1),  [pdf](http://arxiv.org/pdf/2507.02659v1)

**Tags**: cs.LG cs.CL 



### Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference   Infrastructure
**Authors**: Rui Xie, Asad Ul Haq, Yunhua Fang, Linsen Ma, Sanchari Sen, Swagath Venkataramani, Liu Liu, Tong Zhang

**Updated**: 2025-07-03T14:18:41Z

**Summary**: High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy efficiency for AI workloads, but its high cost per bit, driven in part by stringent on-die reliability requirements, poses a growing barrier to scalable deployment. This work explores a system-level approach to cost reduction by eliminating on-die ECC and shifting all fault management to the memory controller. We introduce a domain-specific ECC framework combining large-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC detection, differential parity updates to mitigate write amplification, and tunable protection based on data importance. Our evaluation using LLM inference workloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the system retains over 78\% of throughput and 97\% of model accuracy compared with systems equipped with ideal error-free HBM. By treating reliability as a tunable system parameter rather than a fixed hardware constraint, our design opens a new path toward low-cost, high-performance HBM deployment in AI infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2507.02654v1),  [pdf](http://arxiv.org/pdf/2507.02654v1)

**Tags**: cs.AR 



### Nebula: Efficient, Private and Accurate Histogram Estimation
**Authors**: Ali Shahin Shamsabadi, Peter Snyder, Ralph Giles, Aurélien Bellet, Hamed Haddadi

**Updated**: 2025-07-03T14:13:44Z

**Summary**: We present \textit{Nebula}, a system for differentially private histogram estimation on data distributed among clients. \textit{Nebula} allows clients to independently decide whether to participate in the system, and locally encode their data so that an untrusted server only learns data values whose multiplicity exceeds a predefined aggregation threshold, with $(\varepsilon,\delta)$ differential privacy guarantees. Compared to existing systems, \textit{Nebula} uniquely achieves: \textit{i)} a strict upper bound on client privacy leakage; \textit{ii)} significantly higher utility than standard local differential privacy systems; and \textit{iii)} no requirement for trusted third-parties, multi-party computation, or trusted hardware. We provide a formal evaluation of \textit{Nebula}'s privacy, utility and efficiency guarantees, along with an empirical assessment on three real-world datasets. On the United States Census dataset, clients can submit their data in just 0.0036 seconds and 0.0016 MB (\textbf{efficient}), under strong $(\varepsilon=1,\delta=10^{-8})$ differential privacy guarantees (\textbf{private}), enabling \textit{Nebula}'s untrusted aggregation server to estimate histograms with over 88\% better utility than existing local differential privacy deployments (\textbf{accurate}). Additionally, we describe a variant that allows clients to submit multi-dimensional data, with similar privacy, utility, and performance. Finally, we provide an implementation of \textit{Nebula}.

**Link**: [arxiv](http://arxiv.org/abs/2409.09676v2),  [pdf](http://arxiv.org/pdf/2409.09676v2)

**Tags**: cs.CR 



### Recourse, Repair, Reparation, & Prevention: A Stakeholder Analysis of AI   Supply Chains
**Authors**: Aspen K. Hopkins, Isabella Struckman, Kevin Klyman, Susan S. Silbey

**Updated**: 2025-07-03T14:12:43Z

**Summary**: The AI industry is exploding in popularity, with increasing attention to potential harms and unwanted consequences. In the current digital ecosystem, AI deployments are often the product of AI supply chains (AISC): networks of outsourced models, data, and tooling through which multiple entities contribute to AI development and distribution. AI supply chains lack the modularity, redundancies, or conventional supply chain practices that enable identification, isolation, and easy correction of failures, exacerbating the already difficult processes of responding to ML-generated harms. As the stakeholders participating in and impacted by AISCs have scaled and diversified, so too have the risks they face. In this stakeholder analysis of AI supply chains, we consider who participates in AISCs, what harms they face, where sources of harm lie, and how market dynamics and power differentials inform the type and probability of remedies. Because AI supply chains are purposely invented and implemented, they may be designed to account for, rather than ignore, the complexities, consequences, and risks of deploying AI systems. To enable responsible design and management of AISCs, we offer a typology of responses to AISC-induced harms: recourse, repair, reparation or prevention. We apply this typology to stakeholders participating in a health-care AISC across three stylized markets $\unicode{x2013}$ vertical integration, horizontal integration, free market $\unicode{x2013}$ to illustrate how stakeholder positioning and power within an AISC may shape responses to an experienced harm.

**Link**: [arxiv](http://arxiv.org/abs/2507.02648v1),  [pdf](http://arxiv.org/pdf/2507.02648v1)

**Tags**: cs.CY 



### Empowering Intelligent Low-altitude Economy with Large AI Model   Deployment
**Authors**: Zhonghao Lyu, Yulan Gao, Junting Chen, Hongyang Du, Jie Xu, Kaibin Huang, Dong In Kim

**Updated**: 2025-07-03T14:03:18Z

**Summary**: Low-altitude economy (LAE) represents an emerging economic paradigm that redefines commercial and social aerial activities. Large artificial intelligence models (LAIMs) offer transformative potential to further enhance the intelligence of LAE services. However, deploying LAIMs in LAE poses several challenges, including the significant gap between their computational/storage demands and the limited onboard resources of LAE entities, the mismatch between lab-trained LAIMs and dynamic physical environments, and the inefficiencies of traditional decoupled designs for sensing, communication, and computation. To address these issues, we first propose a hierarchical system architecture tailored for LAIM deployment and present representative LAE application scenarios. Next, we explore key enabling techniques that facilitate the mutual co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented execution pipeline for scalable and adaptive service delivery. Then, the proposed framework is validated through real-world case studies. Finally, we outline open challenges to inspire future research.

**Link**: [arxiv](http://arxiv.org/abs/2505.22343v2),  [pdf](http://arxiv.org/pdf/2505.22343v2)

**Tags**: eess.SP cs.AI 



### Medical Data Pecking: A Context-Aware Approach for Automated Quality   Evaluation of Structured Medical Data
**Authors**: Irena Girshovitz, Atai Ambus, Moni Shahar, Ran Gilad-Bachrach

**Updated**: 2025-07-03T13:54:50Z

**Summary**: Background: The use of Electronic Health Records (EHRs) for epidemiological studies and artificial intelligence (AI) training is increasing rapidly. The reliability of the results depends on the accuracy and completeness of EHR data. However, EHR data often contain significant quality issues, including misrepresentations of subpopulations, biases, and systematic errors, as they are primarily collected for clinical and billing purposes. Existing quality assessment methods remain insufficient, lacking systematic procedures to assess data fitness for research.   Methods: We present the Medical Data Pecking approach, which adapts unit testing and coverage concepts from software engineering to identify data quality concerns. We demonstrate our approach using the Medical Data Pecking Tool (MDPT), which consists of two main components: (1) an automated test generator that uses large language models and grounding techniques to create a test suite from data and study descriptions, and (2) a data testing framework that executes these tests, reporting potential errors and coverage.   Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and SyntheticMass, generating 55-73 tests per cohort across four conditions. These tests correctly identified 20-43 non-aligned or non-conforming data issues. We present a detailed analysis of the LLM-generated test suites in terms of reference grounding and value accuracy.   Conclusion: Our approach incorporates external medical knowledge to enable context-sensitive data quality testing as part of the data analysis workflow to improve the validity of its outcomes. Our approach tackles these challenges from a quality assurance perspective, laying the foundation for further development such as additional data modalities and improved grounding methods.

**Link**: [arxiv](http://arxiv.org/abs/2507.02628v1),  [pdf](http://arxiv.org/pdf/2507.02628v1)

**Tags**: cs.LG 



### VRAgent-R1: Boosting Video Recommendation with MLLM-based Agents via   Reinforcement Learning
**Authors**: Siran Chen, Boyu Chen, Chenyun Yu, Yuxiao Luo, Ouyang Yi, Lei Cheng, Chengxiang Zhuo, Zang Li, Yali Wang

**Updated**: 2025-07-03T13:52:24Z

**Summary**: Owing to powerful natural language processing and generative capabilities, large language model (LLM) agents have emerged as a promising solution for enhancing recommendation systems via user simulation. However, in the realm of video recommendation, existing studies predominantly resort to prompt-based simulation using frozen LLMs and encounter the intricate challenge of multimodal content understanding. This frequently results in suboptimal item modeling and user preference learning, thereby ultimately constraining recommendation performance. To address these challenges, we introduce VRAgent-R1, a novel agent-based paradigm that incorporates human-like intelligence in user simulation. Specifically, VRAgent-R1 comprises two distinct agents: the Item Perception (IP) Agent and the User Simulation (US) Agent, designed for interactive user-item modeling. Firstly, the IP Agent emulates human-like progressive thinking based on MLLMs, effectively capturing hidden recommendation semantics in videos. With a more comprehensive multimodal content understanding provided by the IP Agent, the video recommendation system is equipped to provide higher-quality candidate items. Subsequently, the US Agent refines the recommended video sets based on in-depth chain-of-thought (CoT) reasoning and achieves better alignment with real user preferences through reinforcement learning. Experimental results on a large-scale video recommendation benchmark have demonstrated the effectiveness of our proposed VRAgent-R1 method, e.g., the IP Agent achieves a 6.0\% improvement in NDCG@10 on the MicroLens-100k dataset, while the US Agent shows approximately 45.0\% higher accuracy in user decision simulation compared to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2507.02626v1),  [pdf](http://arxiv.org/pdf/2507.02626v1)

**Tags**: cs.MM 



### FlowSpec: Continuous Pipelined Speculative Decoding for Efficient   Distributed LLM Inference
**Authors**: Xing Liu, Lizhuo Luo, Ming Tang, Chao Huang

**Updated**: 2025-07-03T13:47:42Z

**Summary**: Distributed inference serves as a promising approach to enabling the inference of large language models (LLMs) at the network edge. It distributes the inference process to multiple devices to ensure that the LLMs can fit into the device memory. Recent pipeline-based approaches have the potential to parallelize communication and computation, which helps reduce inference latency. However, the benefit diminishes when the inference request at the network edge is sparse, where pipeline is typically at low utilization. To enable efficient distributed LLM inference at the edge, we propose \textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding framework. FlowSpec incorporates three key mechanisms to improve decoding efficiency: 1) score-based step-wise verification prioritizes more important draft tokens to bring earlier accpeted tokens; 2) efficient draft management to prune invalid tokens while maintaining correct causal relationship during verification; 3) dynamic draft expansion strategies to supply high-quality speculative inputs. These techniques work in concert to enhance both pipeline utilization and speculative efficiency. We evaluate FlowSpec on a real-world testbed with other baselines. Experimental results demonstrate that our proposed framework significantly improves inference speed across diverse models and configurations, achieving speedup ratios 1.36$\times$-1.77$\times$ compared to baselines. Our code is publicly available at \href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\#}

**Link**: [arxiv](http://arxiv.org/abs/2507.02620v1),  [pdf](http://arxiv.org/pdf/2507.02620v1)

**Tags**: cs.DC cs.AI 



### Strategic Intelligence in Large Language Models: Evidence from   evolutionary Game Theory
**Authors**: Kenneth Payne, Baptiste Alloui-Cros

**Updated**: 2025-07-03T13:45:02Z

**Summary**: Are Large Language Models (LLMs) a new form of strategic intelligence, able to reason about goals in competitive settings? We present compelling supporting evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for studying decision-making. We conduct the first ever series of evolutionary IPD tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger) against agents from the leading frontier AI companies OpenAI, Google, and Anthropic. By varying the termination probability in each tournament (the "shadow of the future"), we introduce complexity and chance, confounding memorisation.   Our results show that LLMs are highly competitive, consistently surviving and sometimes even proliferating in these complex ecosystems. Furthermore, they exhibit distinctive and persistent "strategic fingerprints": Google's Gemini models proved strategically ruthless, exploiting cooperative opponents and retaliating against defectors, while OpenAI's models remained highly cooperative, a trait that proved catastrophic in hostile environments. Anthropic's Claude emerged as the most forgiving reciprocator, showing remarkable willingness to restore cooperation even after being exploited or successfully defecting. Analysis of nearly 32,000 prose rationales provided by the models reveals that they actively reason about both the time horizon and their opponent's likely strategy, and we demonstrate that this reasoning is instrumental to their decisions. This work connects classic game theory with machine psychology, offering a rich and granular view of algorithmic decision-making under uncertainty.

**Link**: [arxiv](http://arxiv.org/abs/2507.02618v1),  [pdf](http://arxiv.org/pdf/2507.02618v1)

**Tags**: cs.AI cs.CL cs.GT 



### DynamiCare: A Dynamic Multi-Agent Framework for Interactive and   Open-Ended Medical Decision-Making
**Authors**: Tianqi Shang, Weiqing He, Charles Zheng, Lingyao Li, Li Shen, Bingxin Zhao

**Updated**: 2025-07-03T13:43:10Z

**Summary**: The rise of Large Language Models (LLMs) has enabled the development of specialized AI agents with domain-specific reasoning and interaction capabilities, particularly in healthcare. While recent frameworks simulate medical decision-making, they largely focus on single-turn tasks where a doctor agent receives full case information upfront -- diverging from the real-world diagnostic process, which is inherently uncertain, interactive, and iterative. In this paper, we introduce MIMIC-Patient, a structured dataset built from the MIMIC-III electronic health records (EHRs), designed to support dynamic, patient-level simulations. Building on this, we propose DynamiCare, a novel dynamic multi-agent framework that models clinical diagnosis as a multi-round, interactive loop, where a team of specialist agents iteratively queries the patient system, integrates new information, and dynamically adapts its composition and strategy. We demonstrate the feasibility and effectiveness of DynamiCare through extensive experiments, establishing the first benchmark for dynamic clinical decision-making with LLM-powered agents.

**Link**: [arxiv](http://arxiv.org/abs/2507.02616v1),  [pdf](http://arxiv.org/pdf/2507.02616v1)

**Tags**: cs.AI 



### Explainable Compliance Detection with Multi-Hop Natural Language   Inference on Assurance Case Structure
**Authors**: Fariz Ikhwantri, Dusica Marijan

**Updated**: 2025-07-03T13:39:37Z

**Summary**: Ensuring complex systems meet regulations typically requires checking the validity of assurance cases through a claim-argument-evidence framework. Some challenges in this process include the complicated nature of legal and technical texts, the need for model explanations, and limited access to assurance case data. We propose a compliance detection approach based on Natural Language Inference (NLI): EXplainable CompLiance detection with Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the claim-argument-evidence structure of an assurance case as a multi-hop inference for explainable and traceable compliance detection. We address the limited number of assurance cases by generating them using large language models (LLMs). We introduce metrics that measure the coverage and structural consistency. We demonstrate the effectiveness of the generated assurance case from GDPR requirements in a multi-hop inference task as a case study. Our results highlight the potential of NLI-based approaches in automating the regulatory compliance process.

**Link**: [arxiv](http://arxiv.org/abs/2506.08713v2),  [pdf](http://arxiv.org/pdf/2506.08713v2)

**Tags**: cs.CL cs.SE 



### StructTransform: A Scalable Attack Surface for Safety-Aligned Large   Language Models
**Authors**: Shehel Yoosuf, Temoor Ali, Ahmed Lekssays, Mashael AlSabah, Issa Khalil

**Updated**: 2025-07-03T13:25:12Z

**Summary**: In this work, we present a series of structure transformation attacks on LLM alignment, where we encode natural language intent using diverse syntax spaces, ranging from simple structure formats and basic query languages (e.g., SQL) to new novel spaces and syntaxes created entirely by LLMs. Our extensive evaluation shows that our simplest attacks can achieve close to a 90% success rate, even on strict LLMs (such as Claude 3.5 Sonnet) using SOTA alignment mechanisms. We improve the attack performance further by using an adaptive scheme that combines structure transformations along with existing content transformations, resulting in over 96% ASR with 0% refusals.   To generalize our attacks, we explore numerous structure formats, including syntaxes purely generated by LLMs. Our results indicate that such novel syntaxes are easy to generate and result in a high ASR, suggesting that defending against our attacks is not a straightforward process. Finally, we develop a benchmark and evaluate existing safety-alignment defenses against it, showing that most of them fail with 100% ASR. Our results show that existing safety alignment mostly relies on token-level patterns without recognizing harmful concepts, highlighting and motivating the need for serious research efforts in this direction. As a case study, we demonstrate how attackers can use our attack to easily generate a sample malware and a corpus of fraudulent SMS messages, which perform well in bypassing detection.

**Link**: [arxiv](http://arxiv.org/abs/2502.11853v2),  [pdf](http://arxiv.org/pdf/2502.11853v2)

**Tags**: cs.LG 



### Direct Preference Optimization Using Sparse Feature-Level Constraints
**Authors**: Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang

**Updated**: 2025-07-03T13:18:23Z

**Summary**: The alignment of large language models (LLMs) with human preferences remains a key challenge. While post-training techniques like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have achieved notable success, they often introduce computational inefficiencies and training instability. In this paper, we propose Feature-level constrained Preference Optimization (FPO), a novel method designed to simplify the alignment process while ensuring stability. FPO leverages pre-trained Sparse Autoencoders (SAEs) and introduces feature-level constraints, allowing for efficient, sparsity-enforced alignment. Our approach enjoys efficiency by using sparse features activated in a well-trained sparse autoencoder and the quality of sequential KL divergence by using the feature-level offline reference. Experimental results on benchmark datasets demonstrate that FPO achieves a 5.08% absolute improvement in win rate with much lower computational cost compared to state-of-the-art baselines, making it a promising solution for efficient and controllable LLM alignments.

**Link**: [arxiv](http://arxiv.org/abs/2411.07618v2),  [pdf](http://arxiv.org/pdf/2411.07618v2)

**Tags**: cs.AI cs.CL 



### Symbolic or Numerical? Understanding Physics Problem Solving in   Reasoning LLMs
**Authors**: Nifu Dan, Yujun Cai, Yiwei Wang

**Updated**: 2025-07-03T13:15:11Z

**Summary**: Navigating the complexities of physics reasoning has long been a difficult task for Large Language Models (LLMs), requiring a synthesis of profound conceptual understanding and adept problem-solving techniques. In this study, we investigate the application of advanced instruction-tuned reasoning models, such as Deepseek-R1, to address a diverse spectrum of physics problems curated from the challenging SciBench benchmark. Our comprehensive experimental evaluation reveals the remarkable capabilities of reasoning models. Not only do they achieve state-of-the-art accuracy in answering intricate physics questions, but they also generate distinctive reasoning patterns that emphasize on symbolic derivation. Furthermore, our findings indicate that even for these highly sophisticated reasoning models, the strategic incorporation of few-shot prompting can still yield measurable improvements in overall accuracy, highlighting the potential for continued performance gains.

**Link**: [arxiv](http://arxiv.org/abs/2507.01334v2),  [pdf](http://arxiv.org/pdf/2507.01334v2)

**Tags**: cs.CL 



### MPF: Aligning and Debiasing Language Models post Deployment via Multi   Perspective Fusion
**Authors**: Xin Guan, PeiHsin Lin, Zekun Wu, Ze Wang, Ruibo Zhang, Emre Kazim, Adriano Koshiyama

**Updated**: 2025-07-03T13:09:18Z

**Summary**: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for large language models (LLMs) developed in response to the growing need for easy bias mitigation. Built on top of the SAGED pipeline, an automated system for constructing bias benchmarks and extracting interpretable baseline distributions, MPF leverages multiperspective generations to expose and align biases in LLM outputs with nuanced, humanlike baselines. By decomposing baseline, such as sentiment distributions from HR professionals, into interpretable perspective components, MPF guides generation through sampling and balancing of responses, weighted by the probabilities obtained in the decomposition. Empirically, we demonstrate its ability to align LLM sentiment distributions with both counterfactual baselines (absolute equality) and the HR baseline (biased for Top Univeristy), resulting in small KL divergence, reduction of calibration error and generalization to unseen questions. This shows that MPF offers a scalable and interpretable method for alignment and bias mitigation, compatible with deployed LLMs and requiring no extensive prompt engineering or finetuning.

**Link**: [arxiv](http://arxiv.org/abs/2507.02595v1),  [pdf](http://arxiv.org/pdf/2507.02595v1)

**Tags**: cs.CL cs.AI 



### SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical   VQA Tasks
**Authors**: Kim-Celine Kahl, Selen Erkan, Jeremias Traub, Carsten T. Lüth, Klaus Maier-Hein, Lena Maier-Hein, Paul F. Jaeger

**Updated**: 2025-07-03T13:07:30Z

**Summary**: Vision-Language Models (VLMs) have great potential in medical tasks, like Visual Question Answering (VQA), where they could act as interactive assistants for both patients and clinicians. Yet their robustness to distribution shifts on unseen data remains a key concern for safe deployment. Evaluating such robustness requires a controlled experimental setup that allows for systematic insights into the model's behavior. However, we demonstrate that current setups fail to offer sufficiently thorough evaluations. To address this gap, we introduce a novel framework, called SURE-VQA, centered around three key requirements to overcome current pitfalls and systematically analyze VLM robustness: 1) Since robustness on synthetic shifts does not necessarily translate to real-world shifts, it should be measured on real-world shifts that are inherent to the VQA data; 2) Traditional token-matching metrics often fail to capture underlying semantics, necessitating the use of large language models (LLMs) for more accurate semantic evaluation; 3) Model performance often lacks interpretability due to missing sanity baselines, thus meaningful baselines should be reported that allow assessing the multimodal impact on the VLM. To demonstrate the relevance of this framework, we conduct a study on the robustness of various Fine-Tuning (FT) methods across three medical datasets with four types of distribution shifts. Our study highlights key insights into robustness: 1) No FT method consistently outperforms others in robustness, and 2) robustness trends are more stable across FT methods than across distribution shifts. Additionally, we find that simple sanity baselines that do not use the image data can perform surprisingly well and confirm LoRA as the best-performing FT method on in-distribution data. Code is provided at https://github.com/IML-DKFZ/sure-vqa.

**Link**: [arxiv](http://arxiv.org/abs/2411.19688v3),  [pdf](http://arxiv.org/pdf/2411.19688v3)

**Tags**: cs.CV cs.LG 



### MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based   Agent Collaboration
**Authors**: Dingkang Yang, Jinjie Wei, Mingcheng Li, Jiyao Liu, Lihao Liu, Ming Hu, Junjun He, Yakun Ju, Wei Zhou, Yang Liu, Lihua Zhang

**Updated**: 2025-07-03T13:02:30Z

**Summary**: In healthcare intelligence, the ability to fuse heterogeneous, multi-intent information from diverse clinical sources is fundamental to building reliable decision-making systems. Large Language Model (LLM)-driven information interaction systems currently showing potential promise in the healthcare domain. Nevertheless, they often suffer from information redundancy and coupling when dealing with complex medical intents, leading to severe hallucinations and performance bottlenecks. To this end, we propose MedAide, an LLM-based medical multi-agent collaboration framework designed to enable intent-aware information fusion and coordinated reasoning across specialized healthcare domains. Specifically, we introduce a regularization-guided module that combines syntactic constraints with retrieval augmented generation to decompose complex queries into structured representations, facilitating fine-grained clinical information fusion and intent resolution. Additionally, a dynamic intent prototype matching module is proposed to utilize dynamic prototype representation with a semantic similarity matching mechanism to achieve adaptive recognition and updating of the agent's intent in multi-round healthcare dialogues. Ultimately, we design a rotation agent collaboration mechanism that introduces dynamic role rotation and decision-level information fusion across specialized medical agents. Extensive experiments are conducted on four medical benchmarks with composite intents. Experimental results from automated metrics and expert doctor evaluations show that MedAide outperforms current LLMs and improves their medical proficiency and strategic reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2410.12532v3),  [pdf](http://arxiv.org/pdf/2410.12532v3)

**Tags**: cs.CL 



### Revisiting Active Learning under (Human) Label Variation
**Authors**: Cornelia Gruber, Helen Alber, Bernd Bischl, Göran Kauermann, Barbara Plank, Matthias Aßenmacher

**Updated**: 2025-07-03T12:59:28Z

**Summary**: Access to high-quality labeled data remains a limiting factor in applied supervised learning. While label variation (LV), i.e., differing labels for the same instance, is common, especially in natural language processing, annotation frameworks often still rest on the assumption of a single ground truth. This overlooks human label variation (HLV), the occurrence of plausible differences in annotations, as an informative signal. Similarly, active learning (AL), a popular approach to optimizing the use of limited annotation budgets in training ML models, often relies on at least one of several simplifying assumptions, which rarely hold in practice when acknowledging HLV. In this paper, we examine foundational assumptions about truth and label nature, highlighting the need to decompose observed LV into signal (e.g., HLV) and noise (e.g., annotation error). We survey how the AL and (H)LV communities have addressed -- or neglected -- these distinctions and propose a conceptual framework for incorporating HLV throughout the AL loop, including instance selection, annotator choice, and label representation. We further discuss the integration of large language models (LLM) as annotators. Our work aims to lay a conceptual foundation for HLV-aware active learning, better reflecting the complexities of real-world annotation.

**Link**: [arxiv](http://arxiv.org/abs/2507.02593v1),  [pdf](http://arxiv.org/pdf/2507.02593v1)

**Tags**: cs.CL cs.HC cs.LG stat.ML 



### WebSailor: Navigating Super-human Reasoning for Web Agent
**Authors**: Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou

**Updated**: 2025-07-03T12:59:07Z

**Summary**: Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.

**Link**: [arxiv](http://arxiv.org/abs/2507.02592v1),  [pdf](http://arxiv.org/pdf/2507.02592v1)

**Tags**: cs.CL cs.AI 



### AuroraLong: Bringing RNNs Back to Efficient Open-Ended Video   Understanding
**Authors**: Weili Xu, Enxin Song, Wenhao Chai, Xuexiang Wen, Tian Ye, Gaoang Wang

**Updated**: 2025-07-03T12:55:16Z

**Summary**: The challenge of long video understanding lies in its high computational complexity and prohibitive memory cost, since the memory and computation required by transformer-based LLMs scale quadratically with input sequence length. We propose AuroraLong to address this challenge by replacing the LLM component in MLLMs with a linear RNN language model that handles input sequence of arbitrary length with constant-size hidden states. To further increase throughput and efficiency, we combine visual token merge with linear RNN models by reordering the visual tokens by their sizes in ascending order. Despite having only 2B parameters and being trained exclusively on public data, AuroraLong achieves performance comparable to Transformer-based models of similar size trained on private datasets across multiple video benchmarks. This demonstrates the potential of efficient, linear RNNs to democratize long video understanding by lowering its computational entry barrier. To our best knowledge, we are the first to use a linear RNN based LLM backbone in a LLaVA-like model for open-ended video understanding.

**Link**: [arxiv](http://arxiv.org/abs/2507.02591v1),  [pdf](http://arxiv.org/pdf/2507.02591v1)

**Tags**: cs.CV 



### LLMREI: Automating Requirements Elicitation Interviews with LLMs
**Authors**: Alexander Korn, Samuel Gorsch, Andreas Vogelsang

**Updated**: 2025-07-03T12:18:05Z

**Summary**: Requirements elicitation interviews are crucial for gathering system requirements but heavily depend on skilled analysts, making them resource-intensive, susceptible to human biases, and prone to miscommunication. Recent advancements in Large Language Models present new opportunities for automating parts of this process. This study introduces LLMREI, a chat bot designed to conduct requirements elicitation interviews with minimal human intervention, aiming to reduce common interviewer errors and improve the scalability of requirements elicitation. We explored two main approaches, zero-shot prompting and least-to-most prompting, to optimize LLMREI for requirements elicitation and evaluated its performance in 33 simulated stakeholder interviews. A third approach, fine-tuning, was initially considered but abandoned due to poor performance in preliminary trials. Our study assesses the chat bot's effectiveness in three key areas: minimizing common interview errors, extracting relevant requirements, and adapting its questioning based on interview context and user responses. Our findings indicate that LLMREI makes a similar number of errors compared to human interviewers, is capable of extracting a large portion of requirements, and demonstrates a notable ability to generate highly context-dependent questions. We envision the greatest benefit of LLMREI in automating interviews with a large number of stakeholders.

**Link**: [arxiv](http://arxiv.org/abs/2507.02564v1),  [pdf](http://arxiv.org/pdf/2507.02564v1)

**Tags**: cs.SE 



### Multi-Utterance Speech Separation and Association Trained on Short   Segments
**Authors**: Yuzhu Wang, Archontis Politis, Konstantinos Drossos, Tuomas Virtanen

**Updated**: 2025-07-03T12:12:34Z

**Summary**: Current deep neural network (DNN) based speech separation faces a fundamental challenge -- while the models need to be trained on short segments due to computational constraints, real-world applications typically require processing significantly longer recordings with multiple utterances per speaker than seen during training. In this paper, we investigate how existing approaches perform in this challenging scenario and propose a frequency-temporal recurrent neural network (FTRNN) that effectively bridges this gap. Our FTRNN employs a full-band module to model frequency dependencies within each time frame and a sub-band module that models temporal patterns in each frequency band. Despite being trained on short fixed-length segments of 10 s, our model demonstrates robust separation when processing signals significantly longer than training segments (21-121 s) and preserves speaker association across utterance gaps exceeding those seen during training. Unlike the conventional segment-separation-stitch paradigm, our lightweight approach (0.9 M parameters) performs inference on long audio without segmentation, eliminating segment boundary distortions while simplifying deployment. Experimental results demonstrate the generalization ability of FTRNN for multi-utterance speech separation and speaker association.

**Link**: [arxiv](http://arxiv.org/abs/2507.02562v1),  [pdf](http://arxiv.org/pdf/2507.02562v1)

**Tags**: eess.AS cs.SD 



### BANet: Bilateral Aggregation Network for Mobile Stereo Matching
**Authors**: Gangwei Xu, Jiaxin Liu, Xianqi Wang, Junda Cheng, Yong Deng, Jinliang Zang, Yurui Chen, Xin Yang

**Updated**: 2025-07-03T11:56:07Z

**Summary**: State-of-the-art stereo matching methods typically use costly 3D convolutions to aggregate a full cost volume, but their computational demands make mobile deployment challenging. Directly applying 2D convolutions for cost aggregation often results in edge blurring, detail loss, and mismatches in textureless regions. Some complex operations, like deformable convolutions and iterative warping, can partially alleviate this issue; however, they are not mobile-friendly, limiting their deployment on mobile devices. In this paper, we present a novel bilateral aggregation network (BANet) for mobile stereo matching that produces high-quality results with sharp edges and fine details using only 2D convolutions. Specifically, we first separate the full cost volume into detailed and smooth volumes using a spatial attention map, then perform detailed and smooth aggregations accordingly, ultimately fusing both to obtain the final disparity map. Experimental results demonstrate that our BANet-2D significantly outperforms other mobile-friendly methods, achieving 35.3\% higher accuracy on the KITTI 2015 leaderboard than MobileStereoNet-2D, with faster runtime on mobile devices. Code: \textcolor{magenta}{https://github.com/gangweix/BANet}.

**Link**: [arxiv](http://arxiv.org/abs/2503.03259v2),  [pdf](http://arxiv.org/pdf/2503.03259v2)

**Tags**: cs.CV 



### Aerial Vision-and-Language Navigation via Semantic-Topo-Metric   Representation Guided LLM Reasoning
**Authors**: Yunpeng Gao, Zhigang Wang, Linglin Jing, Dong Wang, Xuelong Li, Bin Zhao

**Updated**: 2025-07-03T11:55:43Z

**Summary**: Aerial Vision-and-Language Navigation (VLN) is a novel task enabling Unmanned Aerial Vehicles (UAVs) to navigate in outdoor environments through natural language instructions and visual cues. It remains challenging due to the complex spatial relationships in outdoor aerial scenes. In this paper, we propose an end-to-end zero-shot framework for aerial VLN tasks, where the large language model (LLM) is introduced as our agent for action prediction. Specifically, we develop a novel Semantic-Topo-Metric Representation (STMR) to enhance the spatial reasoning ability of LLMs. This is achieved by extracting and projecting instruction-related semantic masks of landmarks into a top-down map that contains the location information of surrounding landmarks. Further, this map is transformed into a matrix representation with distance metrics as the text prompt to the LLM, for action prediction according to the instruction. Experiments conducted in real and simulation environments have successfully proved the effectiveness and robustness of our method, achieving 15.9% and 12.5% improvements (absolute) in Oracle Success Rate (OSR) on AerialVLN-S dataset.

**Link**: [arxiv](http://arxiv.org/abs/2410.08500v2),  [pdf](http://arxiv.org/pdf/2410.08500v2)

**Tags**: cs.RO cs.AI 



### Clarifying Before Reasoning: A Coq Prover with Structural Context
**Authors**: Yanzhen Lu, Hanbin Yang, Xiaodie Wang, Ge Zhang, Biao Li, Chenxu Fu, Chao Li, Yang Yuan, Andrew Chi-Chih Yao

**Updated**: 2025-07-03T11:35:34Z

**Summary**: In this work, we investigate whether improving task clarity can enhance reasoning ability of large language models, focusing on theorem proving in Coq. We introduce a concept-level metric to evaluate task clarity and show that adding structured semantic context to the standard input used by modern LLMs, leads to a 1.85$\times$ improvement in clarity score (44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model \texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386 theorems randomly sampled from 15 standard Coq packages, following the same evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller models on our structured data can achieve even higher performance (48.6\%). Our method uses selective concept unfolding to enrich task descriptions, and employs a Planner--Executor architecture. These findings highlight the value of structured task representations in bridging the gap between understanding and reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2507.02541v1),  [pdf](http://arxiv.org/pdf/2507.02541v1)

**Tags**: cs.AI 



### Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue
**Authors**: Paulo Ricardo Knob, Leonardo Scholler, Juliano Rigatti, Soraia Raupp Musse

**Updated**: 2025-07-03T11:32:41Z

**Summary**: Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.

**Link**: [arxiv](http://arxiv.org/abs/2507.02537v1),  [pdf](http://arxiv.org/pdf/2507.02537v1)

**Tags**: cs.HC cs.AI 



### Meta-Fair: AI-Assisted Fairness Testing of Large Language Models
**Authors**: Miguel Romero-Arjona, José A. Parejo, Juan C. Alonso, Ana B. Sánchez, Aitor Arrieta, Sergio Segura

**Updated**: 2025-07-03T11:20:59Z

**Summary**: Fairness--the absence of unjustified bias--is a core principle in the development of Artificial Intelligence (AI) systems, yet it remains difficult to assess and enforce. Current approaches to fairness testing in large language models (LLMs) often rely on manual evaluation, fixed templates, deterministic heuristics, and curated datasets, making them resource-intensive and difficult to scale. This work aims to lay the groundwork for a novel, automated method for testing fairness in LLMs, reducing the dependence on domain-specific resources and broadening the applicability of current approaches. Our approach, Meta-Fair, is based on two key ideas. First, we adopt metamorphic testing to uncover bias by examining how model outputs vary in response to controlled modifications of input prompts, defined by metamorphic relations (MRs). Second, we propose exploiting the potential of LLMs for both test case generation and output evaluation, leveraging their capability to generate diverse inputs and classify outputs effectively. The proposal is complemented by three open-source tools supporting LLM-driven generation, execution, and evaluation of test cases. We report the findings of several experiments involving 12 pre-trained LLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases. The results show that Meta-Fair is effective in uncovering bias in LLMs, achieving an average precision of 92% and revealing biased behaviour in 29% of executions. Additionally, LLMs prove to be reliable and consistent evaluators, with the best-performing models achieving F1-scores of up to 0.79. Although non-determinism affects consistency, these effects can be mitigated through careful MR design. While challenges remain to ensure broader applicability, the results indicate a promising path towards an unprecedented level of automation in LLM testing.

**Link**: [arxiv](http://arxiv.org/abs/2507.02533v1),  [pdf](http://arxiv.org/pdf/2507.02533v1)

**Tags**: cs.SE 



### Open-Source System for Multilingual Translation and Cloned Speech   Synthesis
**Authors**: Mateo Cámara, Juan Gutiérrez, María Pilar Daza, José Luis Blanco

**Updated**: 2025-07-03T11:12:26Z

**Summary**: We present an open-source system designed for multilingual translation and speech regeneration, addressing challenges in communication and accessibility across diverse linguistic contexts. The system integrates Whisper for speech recognition with Voice Activity Detection (VAD) to identify speaking intervals, followed by a pipeline of Large Language Models (LLMs). For multilingual applications, the first LLM segments speech into coherent, complete sentences, which a second LLM then translates. For speech regeneration, the system uses a text-to-speech (TTS) module with voice cloning capabilities to replicate the original speaker's voice, maintaining naturalness and speaker identity.   The system's open-source components can operate locally or via APIs, offering cost-effective deployment across various use cases. These include real-time multilingual translation in Zoom sessions, speech regeneration for public broadcasts, and Bluetooth-enabled multilingual playback through personal devices. By preserving the speaker's voice, the system ensures a seamless and immersive experience, whether translating or regenerating speech.   This open-source project is shared with the community to foster innovation and accessibility. We provide a detailed system performance analysis, including latency and word accuracy, demonstrating its potential to enable inclusive, adaptable communication solutions in real-world multilingual scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2507.02530v1),  [pdf](http://arxiv.org/pdf/2507.02530v1)

**Tags**: eess.AS 



### Traveling Across Languages: Benchmarking Cross-Lingual Consistency in   Multimodal LLMs
**Authors**: Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara

**Updated**: 2025-07-03T10:35:35Z

**Summary**: The rapid evolution of multimodal large language models (MLLMs) has significantly enhanced their real-world applications. However, achieving consistent performance across languages, especially when integrating cultural knowledge, remains a significant challenge. To better assess this issue, we introduce two new benchmarks: KnowRecall and VisRecall, which evaluate cross-lingual consistency in MLLMs. KnowRecall is a visual question answering benchmark designed to measure factual knowledge consistency in 15 languages, focusing on cultural and historical questions about global landmarks. VisRecall assesses visual memory consistency by asking models to describe landmark appearances in 9 languages without access to images. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, still struggle to achieve cross-lingual consistency. This underscores the need for more robust approaches that produce truly multilingual and culturally aware models.

**Link**: [arxiv](http://arxiv.org/abs/2505.15075v2),  [pdf](http://arxiv.org/pdf/2505.15075v2)

**Tags**: cs.CL cs.AI cs.CV cs.LG 



### Self-Guided Process Reward Optimization with Redefined Step-wise   Advantage for Process Reinforcement Learning
**Authors**: Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua

**Updated**: 2025-07-03T10:33:08Z

**Summary**: Process Reinforcement Learning~(PRL) has demonstrated considerable potential in enhancing the reasoning capabilities of Large Language Models~(LLMs). However, introducing additional process reward models incurs substantial computational overhead, and there is no unified theoretical framework for process-level advantage estimation. To bridge this gap, we propose \textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward \textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables process-aware RL through two key innovations: (1) we first theoretically demonstrate that process rewards can be derived intrinsically from the policy model itself, and (2) we introduce well-defined cumulative process rewards and \textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which facilitates rigorous step-wise action advantage estimation within shared-prompt sampling groups. Our experimental results demonstrate that SPRO outperforms vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy improvement. Furthermore, SPRO maintains a stable and elevated policy entropy throughout training while reducing the average response length by approximately $1/3$, evidencing sufficient exploration and prevention of reward hacking. Notably, SPRO incurs no additional computational overhead compared to outcome-supervised RL methods such as GRPO, which benefit industrial implementation.

**Link**: [arxiv](http://arxiv.org/abs/2507.01551v2),  [pdf](http://arxiv.org/pdf/2507.01551v2)

**Tags**: cs.LG cs.AI cs.CL 



### Continual Gradient Low-Rank Projection Fine-Tuning for LLMs
**Authors**: Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing

**Updated**: 2025-07-03T10:11:22Z

**Summary**: Continual fine-tuning of Large Language Models (LLMs) is hampered by the trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA) offers efficiency but constrains the model's ability to learn new tasks and transfer knowledge due to its low-rank nature and reliance on explicit parameter constraints. We propose GORP (Gradient LOw Rank Projection) for Continual Learning, a novel training strategy that overcomes these limitations by synergistically combining full and low-rank parameters and jointly updating within a unified low-rank gradient subspace. GORP expands the optimization space while preserving efficiency and mitigating catastrophic forgetting. Extensive experiments on continual learning benchmarks demonstrate GORP's superior performance compared to existing state-of-the-art approaches. Code is available at https://github.com/Wcxwcxw/GORP.

**Link**: [arxiv](http://arxiv.org/abs/2507.02503v1),  [pdf](http://arxiv.org/pdf/2507.02503v1)

**Tags**: cs.LG cs.AI 



### Agentic AI Process Observability: Discovering Behavioral Variability
**Authors**: Fabiana Fournier, Lior Limonad, Yuval David

**Updated**: 2025-07-03T10:05:46Z

**Summary**: AI agents that leverage Large Language Models (LLMs) are increasingly becoming core building blocks of modern software systems. A wide range of frameworks is now available to support the specification of such applications. These frameworks enable the definition of agent setups using natural language prompting, which specifies the roles, goals, and tools assigned to the various agents involved. Within such setups, agent behavior is non-deterministic for any given input, highlighting the critical need for robust debugging and observability tools. In this work, we explore the use of process and causal discovery applied to agent execution trajectories as a means of enhancing developer observability. This approach aids in monitoring and understanding the emergent variability in agent behavior. Additionally, we complement this with LLM-based static analysis techniques to distinguish between intended and unintended behavioral variability. We argue that such instrumentation is essential for giving developers greater control over evolving specifications and for identifying aspects of functionality that may require more precise and explicit definitions.

**Link**: [arxiv](http://arxiv.org/abs/2505.20127v2),  [pdf](http://arxiv.org/pdf/2505.20127v2)

**Tags**: cs.AI 



### Task Prompt Vectors: Effective Initialization through Multi-Task   Soft-Prompt Transfer
**Authors**: Robert Belanec, Simon Ostermann, Ivan Srba, Maria Bielikova

**Updated**: 2025-07-03T09:32:20Z

**Summary**: Prompt tuning is an efficient solution for training large language models (LLMs). However, current soft-prompt-based methods often sacrifice multi-task modularity, requiring the training process to be fully or partially repeated for each newly added task. While recent work on task vectors applied arithmetic operations on full model weights to achieve the desired multi-task performance, a similar approach for soft-prompts is still missing. To this end, we introduce Task Prompt Vectors, created by element-wise difference between weights of tuned soft-prompts and their random initialization. Experimental results on 12 NLU datasets show that task prompt vectors can be used in low-resource settings to effectively initialize prompt tuning on similar tasks. In addition, we show that task prompt vectors are independent of the random initialization of prompt tuning on 2 different language model architectures. This allows prompt arithmetics with the pre-trained vectors from different tasks. In this way, we provide a competitive alternative to state-of-the-art baselines by arithmetic addition of task prompt vectors from multiple tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.01119v3),  [pdf](http://arxiv.org/pdf/2408.01119v3)

**Tags**: cs.CL 



### System-performance and cost modeling of Large Language Model training   and inference
**Authors**: Wenzhe Guo, Joyjit Kundu, Uras Tos, Weijiang Kong, Giuliano Sisto, Timon Evenblij, Manu Perumkunnil

**Updated**: 2025-07-03T09:13:31Z

**Summary**: Large language models (LLMs), based on transformer architectures, have revolutionized numerous domains within artificial intelligence, science, and engineering due to their exceptional scalability and adaptability. However, the exponential growth in LLM size and complexity has outpaced advancements in compute capacity, memory bandwidth, network performance, and cost efficiency, posing significant challenges to their scalability on distributed systems. To address these limitations, alternative model architectures, optimization strategies, communication-aware network topologies, and novel system design approaches have been proposed in literature. This paper introduces a performance-cost modeling methodology for LLM training and inference that integrates state-of-the-art compute techniques with memory optimizations, and latest communication techniques. Building on an analytical performance model, our approach incorporates recent innovations such as the flash attention technique and mixture of experts models to address the memory bandwidth and compute bottlenecks. It also considers the impact of different network topologies and topology-specific communication algorithms with 5D parallellism. The framework also integrates a chiplet cost model. The proposed modeling methodology provides valuable insights to guide future compute system design and facilitates hardware-software co-development, in particular due to its ability to analyze performance-cost trade-offs for various system architectural configurations.

**Link**: [arxiv](http://arxiv.org/abs/2507.02456v1),  [pdf](http://arxiv.org/pdf/2507.02456v1)

**Tags**: cs.AR 



### Introducing a New Brexit-Related Uncertainty Index: Its Evolution and   Economic Consequences
**Authors**: Ismet Gocer, Julia Darby, Serdar Ongan

**Updated**: 2025-07-03T08:52:55Z

**Summary**: Important game-changer economic events and transformations cause uncertainties that may affect investment decisions, capital flows, international trade, and macroeconomic variables. One such major transformation is Brexit, which refers to the multiyear process through which the UK withdrew from the EU. This study develops and uses a new Brexit-Related Uncertainty Index (BRUI). In creating this index, we apply Text Mining, Context Window, Natural Language Processing (NLP), and Large Language Models (LLMs) from Deep Learning techniques to analyse the monthly country reports of the Economist Intelligence Unit from May 2012 to January 2025. Additionally, we employ a standard vector autoregression (VAR) analysis to examine the model-implied responses of various macroeconomic variables to BRUI shocks. While developing the BRUI, we also create a complementary COVID-19 Related Uncertainty Index (CRUI) to distinguish the uncertainties stemming from these distinct events. Empirical findings and comparisons of BRUI with other earlier-developed uncertainty indexes demonstrate the robustness of the new index. This new index can assist British policymakers in measuring and understanding the impacts of Brexit-related uncertainties, enabling more effective policy formulation.

**Link**: [arxiv](http://arxiv.org/abs/2507.02439v1),  [pdf](http://arxiv.org/pdf/2507.02439v1)

**Tags**: econ.GN q-fin.EC 



### Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for   Elderly Migrants
**Authors**: Wen Zhan, Ziqun Hua, Peiyue Lin, Yunfei Chen

**Updated**: 2025-07-03T08:45:46Z

**Summary**: This paper explores how older adults, particularly aging migrants in urban China, can engage AI-assisted co-creation to express personal narratives that are often fragmented, underrepresented, or difficult to verbalize. Through a pilot workshop combining oral storytelling and the symbolic reconstruction of Hanzi, participants shared memories of migration and recreated new character forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM), together with physical materials. Supported by human facilitation and a soft AI presence, participants transformed lived experience into visual and tactile expressions without requiring digital literacy. This approach offers new perspectives on human-AI collaboration and aging by repositioning AI not as a content producer but as a supportive mechanism, and by supporting narrative agency within sociotechnical systems.

**Link**: [arxiv](http://arxiv.org/abs/2507.01548v2),  [pdf](http://arxiv.org/pdf/2507.01548v2)

**Tags**: cs.HC cs.AI cs.CL 



### CyberRAG: An agentic RAG cyber attack classification and reporting tool
**Authors**: Francesco Blefari, Cristian Cosentino, Francesco Aurelio Pironti, Angelo Furfaro, Fabrizio Marozzo

**Updated**: 2025-07-03T08:32:19Z

**Summary**: Intrusion Detection and Prevention Systems (IDS/IPS) in large enterprises can generate hundreds of thousands of alerts per hour, overwhelming security analysts with logs that demand deep, rapidly evolving domain expertise. Conventional machine-learning detectors trim the alert volume but still yield high false-positive rates, while standard single-pass Retrieval-Augmented Generation (RAG) pipelines often retrieve irrelevant context and fail to justify their predictions. To overcome these shortcomings, we present CyberRAG, a modular, agent-based RAG framework that delivers real-time classification, explanation, and structured reporting for cyber-attacks. A central LLM agent orchestrates (i) a pool of fine-tuned specialized classifiers, each tailored to a distinct attack family; (ii) tool adapters for enrichment and alerting; and (iii) an iterative retrieval-and-reason loop that continuously queries a domain-specific knowledge base until the evidence is both relevant and self-consistent. Unlike traditional RAG systems, CyberRAG embraces an agentic design that enables dynamic control flow and adaptive reasoning. This agent-centric architecture refines its threat labels and natural-language justifications autonomously, reducing false positives and enhancing interpretability. The framework is fully extensible: new attack types can be supported by simply adding a classifier without retraining the core agent. CyberRAG has been evaluated achieving over 94% accuracy per class and pushing final classification accuracy to 94.92% through semantic orchestration. Generated explanations score up to 0.94 in BERTScore and 4.9/5 in GPT-4-based expert evaluation. These results show that agentic, specialist-oriented RAG can pair high detection accuracy with trustworthy, SOC-ready prose, offering a practical and scalable path toward semi-autonomous cyber-defence workflows.

**Link**: [arxiv](http://arxiv.org/abs/2507.02424v1),  [pdf](http://arxiv.org/pdf/2507.02424v1)

**Tags**: cs.CR cs.AI 



### Delving into LLM-assisted writing in biomedical publications through   excess vocabulary
**Authors**: Dmitry Kobak, Rita González-Márquez, Emőke-Ágnes Horvát, Jan Lause

**Updated**: 2025-07-03T08:26:13Z

**Summary**: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.

**Link**: [arxiv](http://arxiv.org/abs/2406.07016v5),  [pdf](http://arxiv.org/pdf/2406.07016v5)

**Tags**: cs.CL cs.AI cs.CY cs.DL cs.SI 



### AIn't Nothing But a Survey? Using Large Language Models for Coding   German Open-Ended Survey Responses on Survey Motivation
**Authors**: Leah von der Heyde, Anna-Carolina Haensch, Bernd Weiß, Jessica Daikeler

**Updated**: 2025-07-03T07:58:18Z

**Summary**: The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.

**Link**: [arxiv](http://arxiv.org/abs/2506.14634v3),  [pdf](http://arxiv.org/pdf/2506.14634v3)

**Tags**: cs.CL cs.AI cs.CY 



### Evaluating Language Models For Threat Detection in IoT Security Logs
**Authors**: Jorge J. Tejero-Fernández, Alfonso Sánchez-Macián

**Updated**: 2025-07-03T07:38:43Z

**Summary**: Log analysis is a relevant research field in cybersecurity as they can provide a source of information for the detection of threats to networks and systems. This paper presents a pipeline to use fine-tuned Large Language Models (LLMs) for anomaly detection and mitigation recommendation using IoT security logs. Utilizing classical machine learning classifiers as a baseline, three open-source LLMs are compared for binary and multiclass anomaly detection, with three strategies: zero-shot, few-shot prompting and fine-tuning using an IoT dataset. LLMs give better results on multi-class attack classification than the corresponding baseline models. By mapping detected threats to MITRE CAPEC, defining a set of IoT-specific mitigation actions, and fine-tuning the models with those actions, the models are able to provide a combined detection and recommendation guidance.

**Link**: [arxiv](http://arxiv.org/abs/2507.02390v1),  [pdf](http://arxiv.org/pdf/2507.02390v1)

**Tags**: cs.CR cs.AI 



### Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex   Software Maintenance
**Authors**: Xin Peng, Chong Wang, Mingwei Liu, Yiling Lou, Yijian Wu

**Updated**: 2025-07-03T07:29:35Z

**Summary**: While large language models (LLMs) have demonstrated promise in software engineering tasks like code completion and generation, their support for the maintenance of complex software systems remains limited. These models often struggle with understanding the tacit knowledge embedded in systems, such as responsibility allocation and collaboration across different modules. To address this gap, we introduce the concept and framework of \textbf{Code Digital Twin}, a conceptual representation of tacit knowledge that captures the concepts, functionalities, and design rationales behind code elements, co-evolving with the software. A code digital twin is constructed using a methodology that combines knowledge extraction from both structured and unstructured sources--such as source code, documentation, and change histories--leveraging LLMs, static analysis tools, and human expertise. This framework can empower LLMs for software maintenance tasks such as issue localization and repository-level code generation by providing tacit knowledge as contexts. Based on the proposed methodology, we explore the key challenges and opportunities involved in the continuous construction and refinement of code digital twin.

**Link**: [arxiv](http://arxiv.org/abs/2503.07967v2),  [pdf](http://arxiv.org/pdf/2503.07967v2)

**Tags**: cs.SE 



### JoyTTS: LLM-based Spoken Chatbot With Voice Cloning
**Authors**: Fangru Zhou, Jun Zhao, Guoxin Wang

**Updated**: 2025-07-03T07:22:06Z

**Summary**: JoyTTS is an end-to-end spoken chatbot that combines large language models (LLM) with text-to-speech (TTS) technology, featuring voice cloning capabilities. This project is built upon the open-source MiniCPM-o and CosyVoice2 models and trained on 2000 hours of conversational data. We have also provided the complete training code to facilitate further development and optimization by the community. On the testing machine seed-tts-zh, it achieves a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09. The code and models, along with training and inference scripts, are available at https://github.com/jdh-algo/JoyTTS.git.

**Link**: [arxiv](http://arxiv.org/abs/2507.02380v1),  [pdf](http://arxiv.org/pdf/2507.02380v1)

**Tags**: cs.SD cs.CL eess.AS 



### Efficient Code LLM Training via Distribution-Consistent and   Diversity-Aware Data Selection
**Authors**: Weijie Lyu, Sheng-Jun Huang, Xuan Xia

**Updated**: 2025-07-03T07:19:56Z

**Summary**: Recent advancements in large language models (LLMs) have significantly improved code generation and program comprehension, accelerating the evolution of software engineering. Current methods primarily enhance model performance by leveraging vast amounts of data, focusing on data quantity while often overlooking data quality, thereby reducing training efficiency. To address this, we introduce an approach that utilizes a parametric model for code data selection, aimed at improving both training efficiency and model performance. Our method optimizes the parametric model to ensure distribution consistency and diversity within the selected subset, guaranteeing high-quality data. Experimental results demonstrate that using only 10K samples, our method achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled baseline, outperforming other sampling approaches in both performance and efficiency. This underscores that our method effectively boosts model performance while significantly reducing computational costs.

**Link**: [arxiv](http://arxiv.org/abs/2507.02378v1),  [pdf](http://arxiv.org/pdf/2507.02378v1)

**Tags**: cs.CL 



### VeFIA: An Efficient Inference Auditing Framework for Vertical Federated   Collaborative Software
**Authors**: Chung-ju Huang, Ziqi Zhang, Yinggui Wang, Binghui Wang, Tao Wei, Leye Wang

**Updated**: 2025-07-03T07:17:49Z

**Summary**: Vertical Federated Learning (VFL) is a distributed AI software deployment mechanism for cross-silo collaboration without accessing participants' data. However, existing VFL work lacks a mechanism to audit the execution correctness of the inference software of the data party. To address this problem, we design a Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task party to audit whether the data party's inference software is executed as expected during large-scale inference without leaking the data privacy of the data party or introducing additional latency to the inference system. The core of VeFIA is that the task party can use the inference results from a framework with Trusted Execution Environments (TEE) and the coordinator to validate the correctness of the data party's computation results. VeFIA guarantees that, as long as the abnormal inference exceeds 5.4%, the task party can detect execution anomalies in the inference software with a probability of 99.99%, without incurring any additional online inference latency. VeFIA's random sampling validation achieves 100% positive predictive value, negative predictive value, and true positive rate in detecting abnormal inference. To the best of our knowledge, this is the first paper to discuss the correctness of inference software execution in VFL.

**Link**: [arxiv](http://arxiv.org/abs/2507.02376v1),  [pdf](http://arxiv.org/pdf/2507.02376v1)

**Tags**: cs.SE cs.AI cs.DC 



### Gateformer: Advancing Multivariate Time Series Forecasting through   Temporal and Variate-Wise Attention with Gated Representations
**Authors**: Yu-Hsiang Lan, Eric K. Oermann

**Updated**: 2025-07-03T07:13:52Z

**Summary**: There has been a recent surge of interest in time series modeling using the Transformer architecture. However, forecasting multivariate time series with Transformer presents a unique challenge as it requires modeling both temporal (cross-time) and variate (cross-variate) dependencies. While Transformer-based models have gained popularity for their flexibility in capturing both sequential and cross-variate relationships, it is unclear how to best integrate these two sources of information in the context of the Transformer architecture while optimizing for both performance and efficiency. We re-purpose the Transformer architecture to effectively model both cross-time and cross-variate dependencies. Our approach begins by embedding each variate independently into a variate-wise representation that captures its cross-time dynamics, and then models cross-variate dependencies through attention mechanisms on these learned embeddings. Gating operations in both cross-time and cross-variate modeling phases regulate information flow, allowing the model to focus on the most relevant features for accurate predictions. Our method achieves state-of-the-art performance across 13 real-world datasets and can be seamlessly integrated into other Transformer-based and LLM-based forecasters, delivering performance improvements up to 20.7\% over original models. Code is available at this repository: https://github.com/nyuolab/Gateformer.

**Link**: [arxiv](http://arxiv.org/abs/2505.00307v3),  [pdf](http://arxiv.org/pdf/2505.00307v3)

**Tags**: cs.LG 



### UVLM: Benchmarking Video Language Model for Underwater World   Understanding
**Authors**: Xizhe Xue, Yang Zhou, Dawei Yan, Ying Li, Haokui Zhang, Rong Xiao

**Updated**: 2025-07-03T07:08:38Z

**Summary**: Recently, the remarkable success of large language models (LLMs) has achieved a profound impact on the field of artificial intelligence. Numerous advanced works based on LLMs have been proposed and applied in various scenarios. Among them, video language models (VidLMs) are particularly widely used. However, existing works primarily focus on terrestrial scenarios, overlooking the highly demanding application needs of underwater observation. To overcome this gap, we introduce UVLM, an under water observation benchmark which is build through a collaborative approach combining human expertise and AI models. To ensure data quality, we have conducted in-depth considerations from multiple perspectives. First, to address the unique challenges of underwater environments, we selected videos that represent typical underwater challenges including light variations, water turbidity, and diverse viewing angles to construct the dataset. Second, to ensure data diversity, the dataset covers a wide range of frame rates, resolutions, 419 classes of marine animals, and various static plants and terrains. Next, for task diversity, we adopted a structured design where observation targets are categorized into two major classes: biological and environmental. Each category includes content observation and change/action observation, totaling 20 distinct task types. Finally, we designed several challenging evaluation metrics to enable quantitative comparison and analysis of different methods. Experiments on two representative VidLMs demonstrate that fine-tuning VidLMs on UVLM significantly improves underwater world understanding while also showing potential for slight improvements on existing in-air VidLM benchmarks, such as VideoMME and Perception text. The dataset and prompt engineering will be released publicly.

**Link**: [arxiv](http://arxiv.org/abs/2507.02373v1),  [pdf](http://arxiv.org/pdf/2507.02373v1)

**Tags**: cs.CV 



### Exploring the Integration of Large Language Models in Industrial Test   Maintenance Processes
**Authors**: Jingxiong Liu, Ludvig Lemner, Linnea Wahlgren, Gregory Gay, Nasser Mohammadiha, Joakim Wennerberg

**Updated**: 2025-07-03T07:06:08Z

**Summary**: Much of the cost and effort required during the software testing process is invested in performing test maintenance - the addition, removal, or modification of test cases to keep the test suite in sync with the system-under-test or to otherwise improve its quality. Tool support could reduce the cost - and improve the quality - of test maintenance by automating aspects of the process or by providing guidance and support to developers.   In this study, we explore the capabilities and applications of large language models (LLMs) - complex machine learning models adapted to textual analysis - to support test maintenance. We conducted a case study at Ericsson AB where we explore the triggers that indicate the need for test maintenance, the actions that LLMs can take, and the considerations that must be made when deploying LLMs in an industrial setting. We also propose and demonstrate a multi-agent architecture that can predict which tests require maintenance following a change to the source code. Collectively, these contributions advance our theoretical and practical understanding of how LLMs can be deployed to benefit industrial test maintenance processes.

**Link**: [arxiv](http://arxiv.org/abs/2409.06416v2),  [pdf](http://arxiv.org/pdf/2409.06416v2)

**Tags**: cs.SE cs.AI cs.LG 



### LLaVA-KD: A Framework of Distilling Multimodal Large Language Models
**Authors**: Yuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He, Ao Tong, Zhenye Gan, Chengjie Wang, Zhucun Xue, Yong Liu, Xiang Bai

**Updated**: 2025-07-03T06:59:56Z

**Summary**: The success of Large Language Models (LLMs) has inspired the development of Multimodal Large Language Models (MLLMs) for unified understanding of vision and language. However, the increasing model size and computational complexity of large-scale MLLMs (l-MLLMs) limit their use in resource-constrained scenarios. Although small-scale MLLMs (s-MLLMs) are designed to reduce computational costs, they typically suffer from performance degradation. To mitigate this limitation, we propose a novel LLaVA-KD framework to transfer knowledge from l-MLLMs to s-MLLMs. Specifically, we introduce Multimodal Distillation (MDist) to transfer teacher model's robust representations across both visual and linguistic modalities, and Relation Distillation (RDist) to transfer teacher model's ability to capture visual token relationships. Additionally, we propose a three-stage training scheme to fully exploit the potential of the proposed distillation strategy: 1) Distilled Pre-Training to strengthen the alignment between visual-linguistic representations in s-MLLMs, 2) Supervised Fine-Tuning to equip the s-MLLMs with multimodal understanding capacity, and 3) Distilled Fine-Tuning to refine s-MLLM's knowledge. Our approach significantly improves s-MLLMs performance without altering the model architecture. Extensive experiments and ablation studies validate the effectiveness of each proposed component. Code will be available at https://github.com/Fantasyele/LLaVA-KD.

**Link**: [arxiv](http://arxiv.org/abs/2410.16236v3),  [pdf](http://arxiv.org/pdf/2410.16236v3)

**Tags**: cs.CV 



### Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation
**Authors**: Yu-Lun Song, Chung-En Tsern, Che-Cheng Wu, Yu-Ming Chang, Syuan-Bo Huang, Wei-Chu Chen, Michael Chia-Liang Lin, Yu-Ta Lin

**Updated**: 2025-07-03T06:38:34Z

**Summary**: This study presents an innovative approach to urban mobility simulation by integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM). Unlike traditional rule-based ABM, the proposed framework leverages LLM to enhance agent diversity and realism by generating synthetic population profiles, allocating routine and occasional locations, and simulating personalized routes. Using real-world data, the simulation models individual behaviors and large-scale mobility patterns in Taipei City. Key insights, such as route heat maps and mode-specific indicators, provide urban planners with actionable information for policy-making. Future work focuses on establishing robust validation frameworks to ensure accuracy and reliability in urban planning applications.

**Link**: [arxiv](http://arxiv.org/abs/2505.21880v2),  [pdf](http://arxiv.org/pdf/2505.21880v2)

**Tags**: cs.MA cs.AI cs.CL cs.CY 



### OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation   via LLM Agent
**Authors**: Bowen Chen, Zhao Wang, Shingo Takamatsu

**Updated**: 2025-07-03T06:37:55Z

**Summary**: Keyword decision in Sponsored Search Advertising is critical to the success of ad campaigns. While LLM-based methods offer automated keyword generation, they face three major limitations: reliance on large-scale query-keyword pair data, lack of online multi-objective performance monitoring and optimization, and weak quality control in keyword selection. These issues hinder the agentic use of LLMs in fully automating keyword decisions by monitoring and reasoning over key performance indicators such as impressions, clicks, conversions, and CTA effectiveness. To overcome these challenges, we propose OMS, a keyword generation framework that is On-the-fly (requires no training data, monitors online performance, and adapts accordingly), Multi-objective (employs agentic reasoning to optimize keywords based on multiple performance metrics), and Self-reflective (agentically evaluates keyword quality). Experiments on benchmarks and real-world ad campaigns show that OMS outperforms existing methods; ablation and human evaluations confirm the effectiveness of each component and the quality of generated keywords.

**Link**: [arxiv](http://arxiv.org/abs/2507.02353v1),  [pdf](http://arxiv.org/pdf/2507.02353v1)

**Tags**: cs.AI 



### Indoor thermal comfort management: A Bayesian machine-learning approach   to data denoising and dynamics prediction of HVAC systems
**Authors**: Javier Penuela, Sahar Moghimian Hoosh, Ilia Kamyshev, Aldo Bischi, Henni Ouerdane

**Updated**: 2025-07-03T06:34:57Z

**Summary**: The optimal management of a building's microclimate to satisfy the occupants' needs and objectives in terms of comfort, energy efficiency, and costs is particularly challenging. This complexity arises from the non-linear, time-dependent interactions among all the variables of the control problem and the changing internal and external constraints. Focusing on the accurate modeling of the indoor temperature, we propose a data-driven approach to address this challenge. We account for thermal inertia, non-linear effects, small perturbations of the indoor climate dynamics caused by ventilation and weather variations, as well as for the stochastic nature of the control system due to the observed noise in the input signal. Since the prohibitive cost of quality data acquisition and processing limits the implementation of data-driven approaches for real-life problems, we applied a method that merges several Bayesian machine learning and deep learning architectures that are suitable for predicting complex system dynamics, while relaxing the dataset quality requirements. Our framework includes a built-in deep Kalman filter, which makes it deployable even with low-accuracy temperature sensors. It achieves state-of-the-art performance, best performing with a 150-minute prediction horizon with an RMSE of 0.2455, an MAE of 0.162, and an $R^2$ of 0.926. The model's performance remains consistent even when exposed to highly noisy data. Finally, we show how our approach can be extended to other applications including demand response event duration prediction and equipment failure detection.

**Link**: [arxiv](http://arxiv.org/abs/2507.02351v1),  [pdf](http://arxiv.org/pdf/2507.02351v1)

**Tags**: eess.SY cs.SY 



### Decision-Oriented Text Evaluation
**Authors**: Yu-Shiang Huang, Chuan-Ju Wang, Chung-Chi Chen

**Updated**: 2025-07-03T06:29:26Z

**Summary**: Natural language generation (NLG) is increasingly deployed in high-stakes domains, yet common intrinsic evaluation methods, such as n-gram overlap or sentence plausibility, weakly correlate with actual decision-making efficacy. We propose a decision-oriented framework for evaluating generated text by directly measuring its influence on human and large language model (LLM) decision outcomes. Using market digest texts--including objective morning summaries and subjective closing-bell analyses--as test cases, we assess decision quality based on the financial performance of trades executed by human investors and autonomous LLM agents informed exclusively by these texts. Our findings reveal that neither humans nor LLM agents consistently surpass random performance when relying solely on summaries. However, richer analytical commentaries enable collaborative human-LLM teams to outperform individual human or agent baselines significantly. Our approach underscores the importance of evaluating generated text by its ability to facilitate synergistic decision-making between humans and LLMs, highlighting critical limitations of traditional intrinsic metrics.

**Link**: [arxiv](http://arxiv.org/abs/2507.01923v2),  [pdf](http://arxiv.org/pdf/2507.01923v2)

**Tags**: cs.CL 



### Token Prepending: A Training-Free Approach for Eliciting Better Sentence   Embeddings from LLMs
**Authors**: Yuchen Fu, Zifeng Cheng, Zhiwei Jiang, Zhonghui Wang, Yafeng Yin, Zhengliang Li, Qing Gu

**Updated**: 2025-07-03T06:24:49Z

**Summary**: Extracting sentence embeddings from large language models (LLMs) is a promising direction, as LLMs have demonstrated stronger semantic understanding capabilities. Previous studies typically focus on prompt engineering to elicit sentence embeddings from LLMs by prompting the model to encode sentence information into the embedding of the last token. However, LLMs are mostly decoder-only models with causal attention and the earlier tokens in the sentence cannot attend to the latter tokens, resulting in biased encoding of sentence information and cascading effects on the final decoded token. To this end, we propose a novel Token Prepending (TP) technique that prepends each layer's decoded sentence embedding to the beginning of the sentence in the next layer's input, allowing earlier tokens to attend to the complete sentence information under the causal attention mechanism. The proposed TP technique is a plug-and-play and training-free technique, which means it can be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs. Extensive experiments on various Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our proposed TP technique can significantly improve the performance of existing prompt-based sentence embedding methods across different LLMs, while incurring negligible additional inference cost.

**Link**: [arxiv](http://arxiv.org/abs/2412.11556v2),  [pdf](http://arxiv.org/pdf/2412.11556v2)

**Tags**: cs.CL cs.AI 



### Autoformalization in the Era of Large Language Models: A Survey
**Authors**: Ke Weng, Lun Du, Sirui Li, Wangyue Lu, Haozhe Sun, Hengyu Liu, Tiancheng Zhang

**Updated**: 2025-07-03T06:03:07Z

**Summary**: Autoformalization, the process of transforming informal mathematical propositions into verifiable formal representations, is a foundational task in automated theorem proving, offering a new perspective on the use of mathematics in both theoretical and applied domains. Driven by the rapid progress in artificial intelligence, particularly large language models (LLMs), this field has witnessed substantial growth, bringing both new opportunities and unique challenges. In this survey, we provide a comprehensive overview of recent advances in autoformalization from both mathematical and LLM-centric perspectives. We examine how autoformalization is applied across various mathematical domains and levels of difficulty, and analyze the end-to-end workflow from data preprocessing to model design and evaluation. We further explore the emerging role of autoformalization in enhancing the verifiability of LLM-generated outputs, highlighting its potential to improve both the trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key open-source models and datasets supporting current research, and discuss open challenges and promising future directions for the field.

**Link**: [arxiv](http://arxiv.org/abs/2505.23486v2),  [pdf](http://arxiv.org/pdf/2505.23486v2)

**Tags**: cs.AI 



### PII Jailbreaking in LLMs via Activation Steering Reveals Personal   Information Leakage
**Authors**: Krishna Kanth Nakka, Xue Jiang, Xuebing Zhou

**Updated**: 2025-07-03T05:50:50Z

**Summary**: This paper investigates privacy jailbreaking in LLMs via steering, focusing on whether manipulating activations can bypass LLM alignment and alter response behaviors to privacy related queries (e.g., a certain public figure's sexual orientation). We begin by identifying attention heads predictive of refusal behavior for private attributes (e.g., sexual orientation) using lightweight linear probes trained with privacy evaluator labels. Next, we steer the activations of a small subset of these attention heads guided by the trained probes to induce the model to generate non-refusal responses. Our experiments show that these steered responses often disclose sensitive attribute details, along with other private information about data subjects such as life events, relationships, and personal histories that the models would typically refuse to produce. Evaluations across four LLMs reveal jailbreaking disclosure rates of at least 95%, with more than 50% on average of these responses revealing true personal information. Our controlled study demonstrates that private information memorized in LLMs can be extracted through targeted manipulation of internal activations.

**Link**: [arxiv](http://arxiv.org/abs/2507.02332v1),  [pdf](http://arxiv.org/pdf/2507.02332v1)

**Tags**: cs.CR 



### SecAlign: Defending Against Prompt Injection with Preference   Optimization
**Authors**: Sizhe Chen, Arman Zharmagambetov, Saeed Mahloujifar, Kamalika Chaudhuri, David Wagner, Chuan Guo

**Updated**: 2025-07-03T05:45:41Z

**Summary**: Large language models (LLMs) are becoming increasingly prevalent in modern software systems, interfacing between the user and the Internet to assist with tasks that require advanced language understanding. To accomplish these tasks, the LLM often uses external data sources such as user documents, web retrieval, results from API calls, etc. This opens up new avenues for attackers to manipulate the LLM via prompt injection. Adversarial prompts can be injected into external data sources to override the system's intended instruction and instead execute a malicious instruction. To mitigate this vulnerability, we propose a new defense called SecAlign based on the technique of preference optimization. Our defense first constructs a preference dataset with prompt-injected inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection). We then perform preference optimization on this dataset to teach the LLM to prefer the secure output over the insecure one. This provides the first known method that reduces the success rates of various prompt injections to <10%, even against attacks much more sophisticated than ones seen during training. This indicates our defense generalizes well against unknown and yet-to-come attacks. Also, SecAlign models are still practical with similar utility to the one before defensive training in our evaluations. Our code is at https://github.com/facebookresearch/SecAlign

**Link**: [arxiv](http://arxiv.org/abs/2410.05451v3),  [pdf](http://arxiv.org/pdf/2410.05451v3)

**Tags**: cs.CR cs.LG 



### Human-Computer Interaction and Visualization in Natural Language   Generation Models: Applications, Challenges, and Opportunities
**Authors**: Yunchao Wang, Guodao Sun, Zihang Fu, Ronghua Liang

**Updated**: 2025-07-03T05:25:23Z

**Summary**: Natural language generation (NLG) models have emerged as a focal point of research within natural language processing (NLP), exhibiting remarkable performance in tasks such as text composition and dialogue generation. However, their intricate architectures and extensive model parameters pose significant challenges to interpretability, limiting their applicability in high-stakes decision-making scenarios. To address this issue, human-computer interaction (HCI) and visualization techniques offer promising avenues to enhance the transparency and usability of NLG models by making their decision-making processes more interpretable. In this paper, we provide a comprehensive investigation into the roles, limitations, and impact of HCI and visualization in facilitating human understanding and control over NLG systems. We introduce a taxonomy of interaction methods and visualization techniques, categorizing three major research domains and their corresponding six key tasks in the application of NLG models. Finally, we summarize the shortcomings in the existing work and investigate the key challenges and emerging opportunities in the era of large language models (LLMs).

**Link**: [arxiv](http://arxiv.org/abs/2410.08723v3),  [pdf](http://arxiv.org/pdf/2410.08723v3)

**Tags**: cs.HC 



### High-Performance Reinforcement Learning on Spot: Optimizing Simulation   Parameters with Distributional Measures
**Authors**: AJ Miller, Fangzhou Yu, Michael Brauckmann, Farbod Farshidian

**Updated**: 2025-07-03T05:14:46Z

**Summary**: This work presents an overview of the technical details behind a high performance reinforcement learning policy deployment with the Spot RL Researcher Development Kit for low level motor access on Boston Dynamics Spot. This represents the first public demonstration of an end to end end reinforcement learning policy deployed on Spot hardware with training code publicly available through Nvidia IsaacLab and deployment code available through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean Discrepancy to quantify the distributional dissimilarity of data collected on hardware and in simulation to measure our sim2real gap. We use these measures as a scoring function for the Covariance Matrix Adaptation Evolution Strategy to optimize simulated parameters that are unknown or difficult to measure from Spot. Our procedure for modeling and training produces high quality reinforcement learning policies capable of multiple gaits, including a flight phase. We deploy policies capable of over 5.2ms locomotion, more than triple Spots default controller maximum speed, robustness to slippery surfaces, disturbance rejection, and overall agility previously unseen on Spot. We detail our method and release our code to support future work on Spot with the low level API.

**Link**: [arxiv](http://arxiv.org/abs/2504.17857v3),  [pdf](http://arxiv.org/pdf/2504.17857v3)

**Tags**: cs.LG cs.RO 



### Aligning Frozen LLMs by Reinforcement Learning: An Iterative   Reweight-then-Optimize Approach
**Authors**: Xinnan Zhang, Chenliang Li, Siliang Zeng, Jiaxiang Li, Zhongruo Wang, Kaixiang Lin, Songtao Lu, Alfredo Garcia, Mingyi Hong

**Updated**: 2025-07-03T05:12:51Z

**Summary**: Aligning large language models (LLMs) with human preferences usually requires fine-tuning methods such as RLHF and DPO. These methods directly optimize the model parameters, so they cannot be used in test-time to improve model performance, nor are they applicable when the model weights are not accessible. In contrast, test-time methods sidestep weight updates by leveraging reward functions to guide and improve output quality. However, they incur high inference costs, and their one-shot guidance is often based on imperfect reward or value functions, leading to suboptimal outputs. In this work, we present a method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning (RL) framework that performs RL-style alignment of the (frozen) base model without touching its parameters. During training, each iteration (i) samples candidates from the base model, (ii) resamples using current value functions, and (iii) trains a new lightweight value function that guides the next decoding pass. At test time, the value functions are used to guide the base model generation via a search-based optimization process. Notably, users can apply IRO to align a model on their own dataset, similar to OpenAI's reinforcement fine-tuning (RFT), but without requiring access to the model weights.

**Link**: [arxiv](http://arxiv.org/abs/2506.17828v2),  [pdf](http://arxiv.org/pdf/2506.17828v2)

**Tags**: cs.LG cs.AI cs.CL 



### Precisely Detecting Python Type Errors via LLM-based Unit Test   Generation
**Authors**: Chen Yang, Ziqi Wang, Yanjie Jiang, Lin Yang, Yuteng Zheng, Jianyi Zhou, Junjie Chen

**Updated**: 2025-07-03T05:10:33Z

**Summary**: Type errors in Python often lead to runtime failures, posing significant challenges to software reliability and developer productivity. Existing static analysis tools aim to detect such errors without execution but frequently suffer from high false positive rates. Recently, unit test generation techniques offer great promise in achieving high test coverage, but they often struggle to produce bug-revealing tests without tailored guidance. To address these limitations, we present RTED, a novel type-aware test generation technique for automatically detecting Python type errors. Specifically, RTED combines step-by-step type constraint analysis with reflective validation to guide the test generation process and effectively suppress false positives. We evaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs. Experimental results show that RTED can detect 22-29 more benchmarked type errors than four state-of-the-art techniques. RTED is also capable of producing fewer false positives, achieving an improvement of 173.9%-245.9% in precision. Furthermore, RTED successfully discovered 12 previously unknown type errors from six real-world open-source Python projects.

**Link**: [arxiv](http://arxiv.org/abs/2507.02318v1),  [pdf](http://arxiv.org/pdf/2507.02318v1)

**Tags**: cs.SE 



