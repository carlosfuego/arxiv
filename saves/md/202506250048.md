# Arxiv Results
## Keyword: kv cache 
 ### CommVQ: Commutative Vector Quantization for KV Cache Compression
**Authors**: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

**Updated**: 2025-06-23T17:50:11Z

**Summary**: Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

**Link**: [arxiv](http://arxiv.org/abs/2506.18879v1),  [pdf](http://arxiv.org/pdf/2506.18879v1)

**Tags**: cs.CL cs.AI 



### Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo
**Authors**: Minas Karamanis, Uro≈° Seljak

**Updated**: 2025-06-23T07:59:17Z

**Summary**: Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian inference but suffer from high computational costs due to their reliance on large particle ensembles for accurate estimates. We introduce persistent sampling (PS), an extension of SMC that systematically retains and reuses particles from all prior iterations to construct a growing, weighted ensemble. By leveraging multiple importance sampling and resampling from a mixture of historical distributions, PS mitigates the need for excessively large particle counts, directly addressing key limitations of SMC such as particle impoverishment and mode collapse. Crucially, PS achieves this without additional likelihood evaluations-weights for persistent particles are computed using cached likelihood values. This framework not only yields more accurate posterior approximations but also produces marginal likelihood estimates with significantly lower variance, enhancing reliability in model comparison. Furthermore, the persistent ensemble enables efficient adaptation of transition kernels by leveraging a larger, decorrelated particle pool. Experiments on high-dimensional Gaussian mixtures, hierarchical models, and non-convex targets demonstrate that PS consistently outperforms standard SMC and related variants, including recycled and waste-free SMC, achieving substantial reductions in mean squared error for posterior expectations and evidence estimates, all at reduced computational cost. PS thus establishes itself as a robust, scalable, and efficient alternative for complex Bayesian inference tasks.

**Link**: [arxiv](http://arxiv.org/abs/2407.20722v3),  [pdf](http://arxiv.org/pdf/2407.20722v3)

**Tags**: stat.ML cs.LG stat.CO 



### FutureFill: Fast Generation from Convolutional Sequence Models
**Authors**: Naman Agarwal, Xinyi Chen, Evan Dogariu, Devan Shah, Hubert Strauss, Vlad Feinberg, Daniel Suo, Peter Bartlett, Elad Hazan

**Updated**: 2025-06-23T03:20:46Z

**Summary**: We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill, a general-purpose fast generation method for any sequence prediction algorithm based on convolutional operators. FutureFill reduces generation time from quadratic to quasilinear in the context length. Moreover, when generating from a prompt, it requires a prefill cache whose size grows only with the number of tokens to be generated, often much smaller than the caches required by standard convolutional or attention based models. We validate our theoretical claims with experiments on synthetic tasks and demonstrate substantial efficiency gains when generating from a deep convolutional sequence prediction model.

**Link**: [arxiv](http://arxiv.org/abs/2410.03766v3),  [pdf](http://arxiv.org/pdf/2410.03766v3)

**Tags**: cs.LG cs.AI cs.CL 



### RAPID: Long-Context Inference with Retrieval-Augmented Speculative   Decoding
**Authors**: Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh

**Updated**: 2025-06-23T03:05:26Z

**Summary**: The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.

**Link**: [arxiv](http://arxiv.org/abs/2502.20330v2),  [pdf](http://arxiv.org/pdf/2502.20330v2)

**Tags**: cs.CL 



### Make It Efficient: Dynamic Sparse Attention for Autoregressive Image   Generation
**Authors**: Xunzhi Xiang, Qi Fan

**Updated**: 2025-06-23T01:27:06Z

**Summary**: Autoregressive conditional image generation models have emerged as a dominant paradigm in text-to-image synthesis. These methods typically convert images into one-dimensional token sequences and leverage the self-attention mechanism, which has achieved remarkable success in natural language processing, to capture long-range dependencies, model global context, and ensure semantic coherence. However, excessively long contexts during inference lead to significant memory overhead caused by KV-cache and computational delays. To alleviate these challenges, we systematically analyze how global semantics, spatial layouts, and fine-grained textures are formed during inference, and propose a novel training-free context optimization method called Adaptive Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies historical tokens crucial for maintaining local texture consistency and those essential for ensuring global semantic coherence, thereby efficiently streamlining attention computation. Additionally, we introduce a dynamic KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption during inference by approximately $50\%$. Extensive qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in terms of both generation quality and resource efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18226v1),  [pdf](http://arxiv.org/pdf/2506.18226v1)

**Tags**: cs.CV cs.AI 



### Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the   Limits of Embedding Space Capacity
**Authors**: Yuri Kuratov, Mikhail Arkhipov, Aydar Bulatov, Mikhail Burtsev

**Updated**: 2025-06-22T15:07:37Z

**Summary**: A range of recent works addresses the problem of compression of sequence of tokens into a shorter sequence of real-valued vectors to be used as inputs instead of token embeddings or key-value cache. These approaches are focused on reduction of the amount of compute in existing language models rather than minimization of number of bits needed to store text. Despite relying on powerful models as encoders, the maximum attainable lossless compression ratio is typically not higher than x10. This fact is highly intriguing because, in theory, the maximum information capacity of large real-valued vectors is far beyond the presented rates even for 16-bit precision and a modest vector size. In this work, we explore the limits of compression by replacing the encoder with a per-sample optimization procedure. We show that vectors with compression ratios up to x1500 exist, which highlights two orders of magnitude gap between existing and practically attainable solutions. Furthermore, we empirically show that the compression limits are determined not by the length of the input but by the amount of uncertainty to be reduced, namely, the cross-entropy loss on this sequence without any conditioning. The obtained limits highlight the substantial gap between the theoretical capacity of input embeddings and their practical utilization, suggesting significant room for optimization in model design.

**Link**: [arxiv](http://arxiv.org/abs/2502.13063v3),  [pdf](http://arxiv.org/pdf/2502.13063v3)

**Tags**: cs.CL cs.LG 



### Secure User-friendly Blockchain Modular Wallet Design Using Android &   OP-TEE
**Authors**: Seongjin Kim, Sanguk Yun, Jungho Jang

**Updated**: 2025-06-22T10:57:57Z

**Summary**: Emerging crypto economies still hemorrhage digital assets because legacy wallets leak private keys at almost every layer of the software stack, from user-space libraries to kernel memory dumps. This paper solves that twin crisis of security and interoperability by re-imagining key management as a platform-level service anchored in ARM TrustZone through OP-TEE. Our architecture fractures the traditional monolithic Trusted Application into per-chain modules housed in a multi-tenant TA store, finally breaking OP-TEE's single-binary ceiling. A cryptographically sealed firmware-over-the-air pipeline welds each TA set to an Android system image, enabling hot-swap updates while Verified Boot enforces rollback protection. Every package carries a chained signature developer first, registry second so even a compromised supply chain cannot smuggle malicious code past the Secure World's RSA-PSS gatekeeper. Inside the TEE, strict inter-TA isolation, cache partitioning, and GP-compliant crypto APIs ensure secrets never bleed across trust boundaries or timing domains. The Rich Execution Environment can interact only via hardware-mediated Secure Monitor Calls, collapsing the surface exposed to malware in Android space. End-users enjoy a single polished interface yet can install or retire Bitcoin, Ethereum, Solana, or tomorrow's chain with one tap, shrinking both storage footprint and audit scope. For auditors, the composition model slashes duplicated verification effort by quarantining blockchain logic inside narrowly scoped modules that share formally specified interfaces. Our threat analysis spans six adversary layers and shows how the design neutralizes REE malware sniffing, OTA injection, and cross-module side channels without exotic hardware. A reference implementation on AOSP exports a Wallet Manager HAL, custom SELinux domains, and a CI/CD pipeline that vet community modules before release. The result is not merely another hardware wallet but a programmable substrate that can evolve at the velocity of the blockchain ecosystem. By welding radical extensibility to hardware-anchored assurance, the platform closes the security-usability gap that has long stymied mass-market self-custody. We posit that modular TEEs are the missing OS primitive for Web3, much as virtual memory unlocked multi-tasking in classical computing. Together, these contributions sketch a blueprint for multi-chain asset management that is auditable, resilient, and poised for global deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.17988v1),  [pdf](http://arxiv.org/pdf/2506.17988v1)

**Tags**: cs.CR 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Ho√üfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-06-22T05:23:09Z

**Summary**: As AI workloads drive soaring memory requirements, higher-density on-chip memory is needed for domain-specific accelerators beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, little work has incorporated dynamic application profiles into these design decisions, and no existing tools are expressly designed for this purpose. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and data lifetimes in domain-specific accelerators. By instrumenting retargetable architectural simulator backends with application- and device-agnostic analytical frontends, GainSight aligns workload-specific traffic and lifetime metrics with mockups of emerging memory devices, informing system-level heterogeneous memory design. We also present a set of case studies on MLPerf Inference and PolyBench workloads using simulated GPU and systolic array architectures, highlighting the utility of GainSight and the insights it provides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory arrays that augment SRAM with GCRAM can reduce active energy consumption by up to 66.8%.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v3),  [pdf](http://arxiv.org/pdf/2504.14866v3)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### ECHO-LLaMA: Efficient Caching for High-Performance LLaMA Training
**Authors**: Maryam Dialameh, Rezaul Karim, Hossein Rajabzadeh, Omar Mohamed Awad, Hyock Ju Kwon, Boxing Chen, Walid Ahmed, Yang Liu

**Updated**: 2025-06-22T03:46:11Z

**Summary**: This paper introduces ECHO-LLaMA, an efficient LLaMA architecture designed to improve both the training speed and inference throughput of LLaMA architectures while maintaining its learning capacity. ECHO-LLaMA transforms LLaMA models into shared KV caching across certain layers, significantly reducing KV computational complexity while maintaining or improving language performance. Experimental results demonstrate that ECHO-LLaMA achieves up to 77\% higher token-per-second throughput during training, up to 16\% higher Model FLOPs Utilization (MFU), and up to 14\% lower loss when trained on an equal number of tokens. Furthermore, on the 1.1B model, ECHO-LLaMA delivers approximately 7\% higher test-time throughput compared to the baseline. By introducing a computationally efficient adaptation mechanism, ECHO-LLaMA offers a scalable and cost-effective solution for pretraining and finetuning large language models, enabling faster and more resource-efficient training without compromising performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.17331v2),  [pdf](http://arxiv.org/pdf/2505.17331v2)

**Tags**: cs.LG cs.CL 



### RPLKG: Robust Prompt Learning with Knowledge Graph
**Authors**: YongTaek Lim, Yewon Kim, Suho Kang, Dokyung Yoon, KyungWoo Song

**Updated**: 2025-06-21T08:27:10Z

**Summary**: Large-scale pre-trained models surpass in transferability and robust generalization across diverse datasets. The emergence of multimodal pre-trained models like CLIP has significantly boosted performance in various experiments. However, generalizing to new datasets or domains remains challenging, especially with limited labeled data. Also, existing methods often lack interpretability and impose high computational costs. To address this, we propose Robust Prompt Learning with Knowledge Graph (RPLKG), leveraging the knowledge graph to curate diverse, interpretable prompt sets automatically. Our method autonomously selects the optimal interpretable prompt based on dataset characteristics, achieving performance improvements over zero-shot learning and competitive performance compared to various prompt learning methods. Also, RPLKG efficiently reuses cached prompt embeddings from a single model pass and optimizes prompt selection via Gumbel-Softmax, enabling low-memory, fast training. Moreover, RPLKG advances few-shot learning effectiveness while enhancing interpretability and efficiency in model adaptation. Our

**Link**: [arxiv](http://arxiv.org/abs/2304.10805v2),  [pdf](http://arxiv.org/pdf/2304.10805v2)

**Tags**: cs.AI cs.LG 



### A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models
**Authors**: Yanting Miao, William Loh, Suraj Kothawade, Pacal Poupart

**Updated**: 2025-06-20T16:59:05Z

**Summary**: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.

**Link**: [arxiv](http://arxiv.org/abs/2506.12036v2),  [pdf](http://arxiv.org/pdf/2506.12036v2)

**Tags**: cs.LG cs.AI 



### Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context   LMs?
**Authors**: Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, Danqi Chen

**Updated**: 2025-06-20T16:21:12Z

**Summary**: Language models handle increasingly long contexts for tasks such as book summarization, but this leads to growing memory costs for the key-value (KV) cache. Many prior works have proposed ways of discarding KVs from memory, but their approaches are tailored to favorable settings, obscuring caveats like high peak memory and performance degradation, and a fair comparison between methods is difficult. In this paper, we propose the *KV footprint* as a unified metric, which accounts for both the amount of KV entries stored and their lifespan in memory. We evaluate methods based on the smallest footprint they attain while preserving performance in both long-context understanding and generation, with context lengths of up to 128K tokens. This metric reveals the high peak memory of prior KV eviction methods. One class of methods -- *post-fill eviction* -- has a high footprint due to being incompatible with eviction during pre-filling. We adapt these methods to be able to evict KVs during pre-filling, achieving substantially lower KV footprints. We then turn to *recency eviction* methods, wherein we propose PruLong, an end-to-end optimization method for learning which attention heads need to retain the full KV cache and which do not. PruLong saves memory while preserving long-context performance, achieving 12% smaller KV footprint than prior methods while retaining performance in challenging recall tasks. Our paper clarifies the complex tangle of long-context inference methods and paves the way for future development to minimize the KV footprint.

**Link**: [arxiv](http://arxiv.org/abs/2506.17121v1),  [pdf](http://arxiv.org/pdf/2506.17121v1)

**Tags**: cs.CL 



### PUL: Pre-load in Software for Caches Wouldn't Always Play Along
**Authors**: Arthur Bernhardt, Sajjad Tamimi, Florian Stock, Andreas Koch, Ilia Petrov

**Updated**: 2025-06-20T13:09:26Z

**Summary**: Memory latencies and bandwidth are major factors, limiting system performance and scalability. Modern CPUs aim at hiding latencies by employing large caches, out-of-order execution, or complex hardware prefetchers. However, software-based prefetching exhibits higher efficiency, improving with newer CPU generations.   In this paper we investigate software-based, post-Moore systems that offload operations to intelligent memories. We show that software-based prefetching has even higher potential in near-data processing settings by maximizing compute utilization through compute/IO interleaving.

**Link**: [arxiv](http://arxiv.org/abs/2506.16976v1),  [pdf](http://arxiv.org/pdf/2506.16976v1)

**Tags**: cs.DB 



### PromptDSI: Prompt-based Rehearsal-free Instance-wise Incremental   Learning for Document Retrieval
**Authors**: Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do

**Updated**: 2025-06-20T12:59:40Z

**Summary**: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.

**Link**: [arxiv](http://arxiv.org/abs/2406.12593v3),  [pdf](http://arxiv.org/pdf/2406.12593v3)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### Serving Large Language Models on Huawei CloudMatrix384
**Authors**: Pengfei Zuo, Huimin Lin, Junbo Deng, Nan Zou, Xingkun Yang, Yingyu Diao, Weifeng Gao, Ke Xu, Zhangyu Chen, Shirui Lu, Zhao Qiu, Peiyang Li, Xianyu Chang, Zhengzhong Yu, Fangzheng Miao, Jia Zheng, Ying Li, Yuan Feng, Bei Wang, Zaijian Zong, Mosong Zhou, Wenli Zhou, Houjiang Chen, Xingyu Liao, Yipeng Li, Wenxiao Zhang, Ping Zhu, Yinggang Wang, Chuanjie Xiao, Depeng Liang, Dong Cao, Juncheng Liu, Yongqiang Yang, Xiaolong Bai, Yi Li, Huaguo Xie, Huatao Wu, Zhibin Yu, Lv Chen, Hu Liu, Yujun Ding, Haipei Zhu, Jing Xia, Yi Xiong, Zhou Yu, Heng Liao

**Updated**: 2025-06-19T12:27:10Z

**Summary**: The rapid evolution of large language models (LLMs), driven by growing parameter scales, adoption of mixture-of-experts (MoE) architectures, and expanding context lengths, imposes unprecedented demands on AI infrastructure. Traditional AI clusters face limitations in compute intensity, memory bandwidth, inter-chip communication, and latency, compounded by variable workloads and strict service-level objectives. Addressing these issues requires fundamentally redesigned hardware-software integration. This paper introduces Huawei CloudMatrix, a next-generation AI datacenter architecture, realized in the production-grade CloudMatrix384 supernode. It integrates 384 Ascend 910 NPUs and 192 Kunpeng CPUs interconnected via an ultra-high-bandwidth Unified Bus (UB) network, enabling direct all-to-all communication and dynamic pooling of resources. These features optimize performance for communication-intensive operations, such as large-scale MoE expert parallelism and distributed key-value cache access. To fully leverage CloudMatrix384, we propose CloudMatrix-Infer, an advanced LLM serving solution incorporating three core innovations: a peer-to-peer serving architecture that independently scales prefill, decode, and caching; a large-scale expert parallelism strategy supporting EP320 via efficient UB-based token dispatch; and hardware-aware optimizations including specialized operators, microbatch-based pipelining, and INT8 quantization. Evaluation with the DeepSeek-R1 model shows CloudMatrix-Infer achieves state-of-the-art efficiency: prefill throughput of 6,688 tokens/s per NPU and decode throughput of 1,943 tokens/s per NPU (<50 ms TPOT). It effectively balances throughput and latency, sustaining 538 tokens/s per NPU even under stringent 15 ms latency constraints, while INT8 quantization maintains model accuracy across benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2506.12708v3),  [pdf](http://arxiv.org/pdf/2506.12708v3)

**Tags**: cs.DC cs.AI cs.AR cs.LG 



### Characterization of discharge capillaries via benchmarked hydrodynamic   plasma simulations
**Authors**: S. M. Mewes, G. J. Boyle, R. D'Arcy, J. M. Garland, M. Huck, H. Jones, G. Loisch, A. R. Maier, J. Osterhoff, T. Parikh, S. Wesch, J. C. Wood, M. Th√©venet

**Updated**: 2025-06-19T10:17:28Z

**Summary**: Plasma accelerators utilize strong electric fields in plasma waves to accelerate charged particles, making them a compact alternative to radiofrequency technologies. Discharge capillaries are plasma sources used in plasma accelerator research to provide acceleration targets, or as plasma lenses to capture or focus accelerated beams. They have applications for beam-driven and laser-driven plasma accelerators and can sustain high repetition rates for extended periods of time. Despite these advantages, high-fidelity simulations of discharge capillaries remain challenging due to the range of mechanisms involved and the difficulty to diagnose them in experiments. In this work, we utilize hydrodynamic plasma simulations to examine the discharge process of a plasma cell and discuss implications for future accelerator systems. The simulation model is validated with experimental measurements in a 50-mm-long, 1-mm-wide plasma capillary operating a 12-27 kV discharge at 2-12mbar hydrogen pressure. For 20 kV at 8.7mbar the discharge is shown to deposit 178mJ of energy in the plasma. Potential difficulties with the common density measurement method using H{\alpha} emission spectroscopy are discussed. This simulation model enables investigations of repeatability, heat flow management and fine tailoring of the plasma profile with discharges.

**Link**: [arxiv](http://arxiv.org/abs/2506.16192v1),  [pdf](http://arxiv.org/pdf/2506.16192v1)

**Tags**: physics.plasm-ph 



### PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning
**Authors**: Duong Bach

**Updated**: 2025-06-24T06:44:47Z

**Summary**: The proliferation of multi-agent systems (MAS) in complex, dynamic environments necessitates robust and efficient mechanisms for managing shared knowledge. A critical challenge is ensuring that distributed memories remain synchronized, relevant, and free from the accumulation of outdated or inconsequential data - a process analogous to biological forgetting. This paper introduces the Co-Forgetting Protocol, a novel, comprehensive framework designed to address this challenge by enabling synchronized memory pruning in MAS. The protocol integrates three key components: (1) context-aware semantic voting, where agents utilize a lightweight DistilBERT model to assess the relevance of memory items based on their content and the current operational context; (2) multi-scale temporal decay functions, which assign diminishing importance to memories based on their age and access frequency across different time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based consensus mechanism, ensuring that decisions to retain or discard memory items are agreed upon by a qualified and fault-tolerant majority of agents, even in the presence of up to f Byzantine (malicious or faulty) agents in a system of N greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient inter-agent communication and Pinecone for scalable vector embedding storage and similarity search, with SQLite managing metadata. Experimental evaluations in a simulated MAS environment with four agents demonstrate the protocol's efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88% voting accuracy in forgetting decisions against human-annotated benchmarks, a 92% PBFT consensus success rate under simulated Byzantine conditions, and an 82% cache hit rate for memory access.

**Link**: [arxiv](http://arxiv.org/abs/2506.17338v2),  [pdf](http://arxiv.org/pdf/2506.17338v2)

**Tags**: cs.DC cs.AI cs.MA 



### Resource Allocation for Twin Maintenance and Computing Task Processing   in Digital Twin Vehicular Edge Computing Network
**Authors**: Yu Xie, Qiong Wu, Pingyi Fan, Nan Cheng, Wen Chen, Jiangzhou Wang, Khaled B. Letaief

**Updated**: 2025-06-19T07:29:09Z

**Summary**: As a promising technology, vehicular edge computing (VEC) can provide computing and caching services by deploying VEC servers near vehicles. However, VEC networks still face challenges such as high vehicle mobility. Digital twin (DT), an emerging technology, can predict, estimate, and analyze real-time states by digitally modeling objects in the physical world. By integrating DT with VEC, a virtual vehicle DT can be created in the VEC server to monitor the real-time operating status of vehicles. However, maintaining the vehicle DT model requires ongoing attention from the VEC server, which also needs to offer computing services for the vehicles. Therefore, effective allocation and scheduling of VEC server resources are crucial. This study focuses on a general VEC network with a single VEC service and multiple vehicles, examining the two types of delays caused by twin maintenance and computational processing within the network. By transforming the problem using satisfaction functions, we propose an optimization problem aimed at maximizing each vehicle's resource utility to determine the optimal resource allocation strategy. Given the non-convex nature of the issue, we employ multi-agent Markov decision processes to reformulate the problem. Subsequently, we propose the twin maintenance and computing task processing resource collaborative scheduling (MADRL-CSTC) algorithm, which leverages multi-agent deep reinforcement learning. Through experimental comparisons with alternative algorithms, it demonstrates that our proposed approach is effective in terms of resource allocation.

**Link**: [arxiv](http://arxiv.org/abs/2407.07575v2),  [pdf](http://arxiv.org/pdf/2407.07575v2)

**Tags**: cs.LG cs.NI 



### LazyEviction: Lagged KV Eviction with Attention Pattern Observation for   Efficient Long Reasoning
**Authors**: Haoyue Zhang, Hualei Zhang, Xiaosong Ma, Jie Zhang, Song Guo

**Updated**: 2025-06-19T02:25:04Z

**Summary**: Large Language Models (LLMs) exhibit enhanced reasoning capabilities by employing Chain-of-Thought (CoT). However, the extended reasoning sequences introduce significant GPU memory overhead due to increased key-value (KV) cache size, particularly in tasks requiring long reasoning sequences, such as mathematics and programming. Existing KV cache compression methods mitigate memory bottlenecks but struggle in long reasoning tasks. In this paper, we analyze attention patterns in reasoning tasks and reveal a Token Importance Recurrence phenomenon: a large proportion of tokens receive renewed attention after multiple decoding steps, which is failed to capture by existing works and may lead to unpredictable eviction on such periodically critical tokens. To address this, we propose LazyEviction, a lagged KV eviction framework designed to maintain reasoning performance while reducing KV memory. LazyEviction is an Observation Window-based Lagged Eviction Mechanism retaining latent recurring tokens by performing lagged evictions across decoding steps, which contains two key components: (1) Recurrence Interval Tracking for capturing temporal variations in token importance, and (2) an Maximum Recurrence Interval-Centric Eviction Policy that prioritizes eviction based on tokens' recurrence patterns. Extensive experiments demonstrate that LazyEviction reduces KV cache size by 50% while maintaining comparable accuracy on mathematics reasoning datasets, outperforming state-of-the-art methods. Our findings highlight the importance of preserving recurring tokens, which are critical for maintaining knowledge continuity in multi-step reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.15969v1),  [pdf](http://arxiv.org/pdf/2506.15969v1)

**Tags**: cs.LG cs.CL 



### KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache   at a Large Cloud Provider
**Authors**: Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen

**Updated**: 2025-06-19T02:18:16Z

**Summary**: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.

**Link**: [arxiv](http://arxiv.org/abs/2506.02634v3),  [pdf](http://arxiv.org/pdf/2506.02634v3)

**Tags**: cs.DC cs.AI 



### Medha: Efficiently Serving Multi-Million Context Length LLM Inference   Requests Without Approximations
**Authors**: Amey Agrawal, Haoran Qiu, Junda Chen, √ç√±igo Goiri, Chaojie Zhang, Rayyan Shahid, Ramachandran Ramjee, Alexey Tumanov, Esha Choukse

**Updated**: 2025-06-18T22:51:06Z

**Summary**: As large language models (LLMs) handle increasingly longer contexts, serving long inference requests of millions of tokens presents unique challenges. We show that existing work for long context inference is largely based on techniques from long context training, and does not handle the high variability in input lengths during inference. This leads to inefficient resource utilization, server fragmentation, and head-of-line (HOL) blocking.   We present Medha, an end-to-end system for efficient long-context LLM inference that addresses these challenges through fine-grained time sharing. Medha introduces three key innovations: (1) the mechanism of adaptive prefill chunking to help mitigate HOL blocking with preemption; (2) two new parallelism strategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token by pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower time-peroutput-token by distributing decoding across servers; and (3) a novel input-length aware least remaining slack scheduling to meet Service Level Objectives (SLOs).   Medha enables exact inference scaling beyond 10 million tokens, maintaining high throughput and low latency across mixed-length workloads. Compared to state-of-the-art systems, Medha reduces server fragmentation, cuts median latency by up to 30x, and improves throughput by over 5x, delivering production-scale long-context inference without compromising performance on shorter requests.

**Link**: [arxiv](http://arxiv.org/abs/2409.17264v4),  [pdf](http://arxiv.org/pdf/2409.17264v4)

**Tags**: cs.LG cs.DC 



### Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model
**Authors**: Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam

**Updated**: 2025-06-18T17:59:50Z

**Summary**: Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.

**Link**: [arxiv](http://arxiv.org/abs/2506.15682v1),  [pdf](http://arxiv.org/pdf/2506.15682v1)

**Tags**: cs.CV 



### Demystifying the Visual Quality Paradox in Multimodal Large Language   Models
**Authors**: Shuo Xing, Lanqing Guo, Hongyuan Hua, Seoyoung Lee, Peiran Li, Yufei Wang, Zhangyang Wang, Zhengzhong Tu

**Updated**: 2025-06-18T17:14:07Z

**Summary**: Recent Multimodal Large Language Models (MLLMs) excel on benchmark vision-language tasks, yet little is known about how input visual quality shapes their responses. Does higher perceptual quality of images already translate to better MLLM understanding? We conduct the first systematic study spanning leading MLLMs and a suite of vision-language benchmarks, applying controlled degradations and stylistic shifts to each image. Surprisingly, we uncover a visual-quality paradox: model, task, and even individual-instance performance can improve when images deviate from human-perceived fidelity. Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning (VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable, low-rank kernel before the frozen vision encoder to modulate frequency content; and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT dynamically adjusts each input image in a single forward pass, aligning it with task-specific model preferences. Across the evaluated MLLMs and all datasets, VQ-TTT lifts significant average accuracy, with no external models, cached features, or extra training data. These findings redefine ``better'' visual inputs for MLLMs and highlight the need for adaptive, rather than universally ``clean'', imagery, in the new era of AI being the main data customer.

**Link**: [arxiv](http://arxiv.org/abs/2506.15645v1),  [pdf](http://arxiv.org/pdf/2506.15645v1)

**Tags**: cs.CV cs.AI 



### From Block to Byte: Transforming PCIe SSDs with CXL Memory Protocol and   Instruction Annotation
**Authors**: Miryeong Kwon, Donghyun Gouk, Junhyeok Jang, Jinwoo Baek, Hyunwoo You, Sangyoon Ji, Hongjoo Jung, Junseok Moon, Seungkwan Kang, Seungjun Lee, Myoungsoo Jung

**Updated**: 2025-06-18T16:44:04Z

**Summary**: This paper explores how Compute Express Link (CXL) can transform PCIe-based block storage into a scalable, byte-addressable working memory. We address the challenges of adapting block storage to CXL's memory-centric model by emphasizing cacheability as a key enabler and advocating for Type 3 endpoint devices, referred to as CXL-SSDs. To validate our approach, we prototype a CXL-SSD on a custom FPGA platform and propose annotation mechanisms, Determinism and Bufferability, to enhance performance while preserving data persistency. Our simulation-based evaluation demonstrates that CXL-SSD achieves 10.9x better performance than PCIe-based memory expanders and further reduces latency by 5.4x with annotation enhancements. In workloads with high locality, CXL-SSD approaches DRAM-like performance due to efficient on-chip caching. This work highlights the feasibility of integrating block storage into CXL's ecosystem and provides a foundation for future memory-storage convergence.

**Link**: [arxiv](http://arxiv.org/abs/2506.15613v1),  [pdf](http://arxiv.org/pdf/2506.15613v1)

**Tags**: cs.AR 



### LaViDa: A Large Diffusion Language Model for Multimodal Understanding
**Authors**: Shufan Li, Konstantinos Kallidromitis, Hritik Bansal, Akash Gokul, Yusuke Kato, Kazuki Kozuka, Jason Kuen, Zhe Lin, Kai-Wei Chang, Aditya Grover

**Updated**: 2025-06-18T15:17:40Z

**Summary**: Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs' potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.

**Link**: [arxiv](http://arxiv.org/abs/2505.16839v3),  [pdf](http://arxiv.org/pdf/2505.16839v3)

**Tags**: cs.CV 



### VideoMAR: Autoregressive Video Generatio with Continuous Tokens
**Authors**: Hu Yu, Biao Gong, Hangjie Yuan, DanDan Zheng, Weilong Chai, Jingdong Chen, Kecheng Zheng, Feng Zhao

**Updated**: 2025-06-18T09:44:09Z

**Summary**: Masked-based autoregressive models have demonstrated promising image generation capability in continuous space. However, their potential for video generation remains under-explored. In this paper, we propose \textbf{VideoMAR}, a concise and efficient decoder-only autoregressive image-to-video model with continuous tokens, composing temporal frame-by-frame and spatial masked generation. We first identify temporal causality and spatial bi-directionality as the first principle of video AR models, and propose the next-frame diffusion loss for the integration of mask and video generation. Besides, the huge cost and difficulty of long sequence autoregressive modeling is a basic but crucial issue. To this end, we propose the temporal short-to-long curriculum learning and spatial progressive resolution training, and employ progressive temperature strategy at inference time to mitigate the accumulation error. Furthermore, VideoMAR replicates several unique capacities of language models to video generation. It inherently bears high efficiency due to simultaneous temporal-wise KV cache and spatial-wise parallel generation, and presents the capacity of spatial and temporal extrapolation via 3D rotary embeddings. On the VBench-I2V benchmark, VideoMAR surpasses the previous state-of-the-art (Cosmos I2V) while requiring significantly fewer parameters ($9.3\%$), training data ($0.5\%$), and GPU resources ($0.2\%$).

**Link**: [arxiv](http://arxiv.org/abs/2506.14168v2),  [pdf](http://arxiv.org/pdf/2506.14168v2)

**Tags**: cs.CV cs.AI 



### A Novel Compiler Transformation for Fast Sparse Matrix Multiplication in   GPUs
**Authors**: Hossein Albakri, Kazem Cheshmi

**Updated**: 2025-06-18T06:41:35Z

**Summary**: Sparse data structures are commonly used in neural networks to reduce the memory footprint. These data structures are compact but cause irregularities such as random memory accesses, which prevent efficient use of the memory hierarchy. GPUs are a common platform for machine learning practitioners, but running compact data structures on these devices often leads to slow-downs due to inefficient use of computing and memory resources. This paper proposes a new compiler transformation, enumerate-and-sparse-coarsen, that accelerates sparse matrix-matrix multiplication (SPMM) on GPU devices. The transformation increases data reuse in registers and caches while creating more balanced workloads for GPU computing resources. The transformation is tested on sparse neural networks in convolutional and transformer models. On an A100 GPU and across a columns of matrix B (bCols) in $ A \times B = C$ from range of 32 to 128, the transformation yields a geometric mean speedup of 1.84$\times$ to 2.27$\times$ compared to cuBLAS and cuSPARSE baselines, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2506.15174v1),  [pdf](http://arxiv.org/pdf/2506.15174v1)

**Tags**: cs.PL 



### eLLM: Elastic Memory Management Framework for Efficient LLM Serving
**Authors**: Jiale Xu, Rui Zhang, Yi Xiong, Cong Guo, Zihan Liu, Yangjie Zhou, Weiming Hu, Hao Wu, Changxu Shao, Ziqing Wang, Yongjie Yuan, Junping Zhao, Minyi Guo, Jingwen Leng

**Updated**: 2025-06-18T05:56:01Z

**Summary**: Large Language Models are increasingly being deployed in datacenters. Serving these models requires careful memory management, as their memory usage includes static weights, dynamic activations, and key-value caches. While static weights are constant and predictable, dynamic components such as activations and KV caches change frequently during runtime, presenting significant challenges for efficient memory management. Modern LLM serving systems typically handle runtime memory and KV caches at distinct abstraction levels: runtime memory management relies on static tensor abstractions, whereas KV caches utilize a page table-based virtualization layer built on top of the tensor abstraction. This virtualization dynamically manages KV caches to mitigate memory fragmentation. However, this dual-level approach fundamentally isolates runtime memory and KV cache management, resulting in suboptimal memory utilization under dynamic workloads, which can lead to a nearly 20% drop in throughput.   To address these limitations, we propose eLLM, an elastic memory management framework inspired by the classical memory ballooning mechanism in operating systems. The core components of eLLM include: (1) Virtual Tensor Abstraction, which decouples the virtual address space of tensors from the physical GPU memory, creating a unified and flexible memory pool; (2) an Elastic Memory Mechanism that dynamically adjusts memory allocation through runtime memory inflation and deflation, leveraging CPU memory as an extensible buffer; and (3) a Lightweight Scheduling Strategy employing SLO-aware policies to optimize memory utilization and effectively balance performance trade-offs under stringent SLO constraints. Comprehensive evaluations demonstrate that eLLM significantly outperforms state-of-the-art systems, 2.32x higher decoding throughput, and supporting 3x larger batch sizes for 128K-token inputs.

**Link**: [arxiv](http://arxiv.org/abs/2506.15155v1),  [pdf](http://arxiv.org/pdf/2506.15155v1)

**Tags**: cs.DC 



### InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding
**Authors**: Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang

**Updated**: 2025-06-18T02:22:14Z

**Summary**: Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time--quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and two streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy--even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.

**Link**: [arxiv](http://arxiv.org/abs/2506.15745v1),  [pdf](http://arxiv.org/pdf/2506.15745v1)

**Tags**: eess.IV cs.LG 



### Compatibility of trapped ions and dielectrics at cryogenic temperatures
**Authors**: M. Bruff, L. Sonderhouse, K. N. David, J. Stuart, D. H. Slichter, D. Leibfried

**Updated**: 2025-06-18T01:37:55Z

**Summary**: We study the impact of an unshielded dielectric $\unicode{x2013}$ here, a bare optical fiber $\unicode{x2013}$ on a $^{40}$Ca${^+}$ ion held several hundred $\mu$m away in a cryogenic surface electrode trap. We observe distance-dependent stray electric fields of up to a few kV/m due to the dielectric, which drift on average less than 10% per month and can be fully compensated with reasonable voltages on the trap electrodes. We observe ion motional heating rates attributable to the dielectric of $\approx$30 quanta per second at an ion-fiber distance of 215(4) $\mu$m and $\approx$1.5 MHz motional frequency. These results demonstrate the viability of using unshielded, trap-integrated dielectric objects such as miniature optical cavities or other optical elements in cryogenic surface electrode ion traps.

**Link**: [arxiv](http://arxiv.org/abs/2506.15057v1),  [pdf](http://arxiv.org/pdf/2506.15057v1)

**Tags**: physics.atom-ph quant-ph 



### CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal   Diffusion
**Authors**: Jiahua Ma, Yiran Qin, Yixiong Li, Xuanqi Liao, Yulan Guo, Ruimao Zhang

**Updated**: 2025-06-17T17:59:12Z

**Summary**: Diffusion Policy (DP) enables robots to learn complex behaviors by imitating expert demonstrations through action diffusion. However, in practical applications, hardware limitations often degrade data quality, while real-time constraints restrict model inference to instantaneous state and scene observations. These limitations seriously reduce the efficacy of learning from expert demonstrations, resulting in failures in object localization, grasp planning, and long-horizon task execution. To address these challenges, we propose Causal Diffusion Policy (CDP), a novel transformer-based diffusion model that enhances action prediction by conditioning on historical action sequences, thereby enabling more coherent and context-aware visuomotor policy learning. To further mitigate the computational cost associated with autoregressive inference, a caching mechanism is also introduced to store attention key-value pairs from previous timesteps, substantially reducing redundant computations during execution. Extensive experiments in both simulated and real-world environments, spanning diverse 2D and 3D manipulation tasks, demonstrate that CDP uniquely leverages historical action sequences to achieve significantly higher accuracy than existing methods. Moreover, even when faced with degraded input observation quality, CDP maintains remarkable precision by reasoning through temporal continuity, which highlights its practical robustness for robotic control under realistic, imperfect conditions.

**Link**: [arxiv](http://arxiv.org/abs/2506.14769v1),  [pdf](http://arxiv.org/pdf/2506.14769v1)

**Tags**: cs.CV cs.RO 



### Keigo: Co-designing Log-Structured Merge Key-Value Stores with a   Non-Volatile, Concurrency-aware Storage Hierarchy (Extended Version)
**Authors**: R√∫ben Ad√£o, Zhongjie Wu, Changjun Zhou, Oana Balmau, Jo√£o Paulo, Ricardo Macedo

**Updated**: 2025-06-17T15:25:11Z

**Summary**: We present Keigo, a concurrency- and workload-aware storage middleware that enhances the performance of log-structured merge key-value stores (LSM KVS) when they are deployed on a hierarchy of storage devices. The key observation behind Keigo is that there is no one-size-fits-all placement of data across the storage hierarchy that optimizes for all workloads. Hence, to leverage the benefits of combining different storage devices, Keigo places files across different devices based on their parallelism, I/O bandwidth, and capacity. We introduce three techniques - concurrency-aware data placement, persistent read-only caching, and context-based I/O differentiation. Keigo is portable across different LSMs, is adaptable to dynamic workloads, and does not require extensive profiling. Our system enables established production KVS such as RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. We evaluate Keigo using synthetic and realistic workloads, showing that it improves the throughput of production-grade LSMs up to 4x for write- and 18x for read-heavy workloads when compared to general-purpose storage systems and specialized LSM KVS.

**Link**: [arxiv](http://arxiv.org/abs/2506.14630v1),  [pdf](http://arxiv.org/pdf/2506.14630v1)

**Tags**: cs.DC cs.DB 



### LongSpec: Long-Context Lossless Speculative Decoding with Efficient   Drafting and Verification
**Authors**: Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An

**Updated**: 2025-06-17T05:58:01Z

**Summary**: As Large Language Models (LLMs) can now process extremely long contexts, efficient inference over these extended inputs has become increasingly important, especially for emerging applications like LLM agents that highly depend on this capability. Speculative decoding (SD) offers a promising lossless acceleration technique compared to lossy alternatives such as quantization and model cascades. However, most state-of-the-art SD methods are trained on short texts (typically fewer than 4k tokens), making them unsuitable for long-context scenarios. Specifically, adapting these methods to long contexts presents three key challenges: (1) the excessive memory demands posed by draft models due to large Key-Value (KV) cache; (2) performance degradation resulting from the mismatch between short-context training and long-context inference; and (3) inefficiencies in tree attention mechanisms when managing long token sequences. This work introduces LongSpec, a framework that addresses these challenges through three core innovations: a memory-efficient draft model with a constant-sized KV cache; novel position indices that mitigate the training-inference mismatch; and an attention aggregation strategy that combines fast prefix computation with standard tree attention to enable efficient decoding. Experimental results confirm the effectiveness of LongSpec, achieving up to a 3.26x speedup over strong Flash Attention baselines across five long-context understanding datasets, as well as a 2.25x reduction in wall-clock time on the AIME24 long reasoning task with the QwQ model, demonstrating significant latency improvements for long-context applications. The code is available at https://github.com/sail-sg/LongSpec.

**Link**: [arxiv](http://arxiv.org/abs/2502.17421v2),  [pdf](http://arxiv.org/pdf/2502.17421v2)

**Tags**: cs.CL cs.AI cs.LG 



### Cost-Efficient Serving of LLM Agents via Test-Time Plan Caching
**Authors**: Qizheng Zhang, Michael Wornow, Kunle Olukotun

**Updated**: 2025-06-17T04:42:30Z

**Summary**: LLM-based agentic applications have shown increasingly remarkable capabilities in complex workflows but incur substantial costs due to extensive planning and reasoning requirements. Existing LLM caching techniques (like context caching and semantic caching), primarily designed for serving chatbots, are insufficient for agentic applications where outputs depend on external data or environmental contexts. We propose agentic plan caching, a novel approach that extracts, stores, adapts, and reuses structured plan templates from planning stages of agentic applications across semantically similar tasks to reduce the cost of serving. Unlike traditional semantic caching, our system extracts plan templates from completed agent executions at test-time, employs keyword extraction to match new requests against cached plans, and utilizes lightweight models to adapt these templates to task-specific plans with contexts. Evaluation across multiple real-world agentic applications shows that our system can reduce costs by 46.62% on average while maintaining performance, offering a more efficient solution for serving LLM-based agents that complements existing LLM serving infrastructures.

**Link**: [arxiv](http://arxiv.org/abs/2506.14852v1),  [pdf](http://arxiv.org/pdf/2506.14852v1)

**Tags**: cs.DC cs.AI cs.CL cs.LG cs.PF 



### CXLMemSim: A pure software simulated CXL.mem for performance   characterization
**Authors**: Yiwei Yang, Brian Zhao, Yusheng Zheng, Pooneh Safayenikoo, Tanvir Ahmed Khan, Andi Quinn

**Updated**: 2025-06-17T04:00:42Z

**Summary**: CXLMemSim is a fast, lightweight simulation framework that enables performance characterization of memory systems based on Compute Express Link (CXL) .mem technology. CXL.mem allows disaggregation and pooling of memory to mitigate memory stranding (underutilized memory trapped on fully loaded servers) in cloud and datacenter environments. However, CXL-attached memory introduces additional latency and bandwidth constraints compared to local DRAM, and real CXL .mem hardware is not yet widely available for empirical evaluation. CXLMemSim addresses this gap by attaching to unmodified applications and simulating CXL-based memory pools in software. It operates by tracing memory allocations and accesses using efficient kernel probes and hardware performance counters, dividing execution into epochs, and injecting timing delays to emulate various CXL .mem latency/bandwidth characteristics. This approach incurs modest runtime overhead while preserving realistic load/store memory access patterns. We implement CXLMemSim on commodity hardware without special devices, and our evaluation shows that it runs orders of magnitude faster than cycle-accurate simulators (e.g., Gem5) for real-world workloads, while accurately modeling the performance impact of CXL .mem. We demonstrate use cases where CXLMemSim enables experimentation with memory pooling configurations, scheduling policies, data migration strategies, and caching techniques that were previously infeasible to evaluate at scale. Key findings include the viability of software-based CXL .mem emulation with low overhead, insights into latency and congestion effects in memory pools, and guidance for system designers to optimize memory disaggregation. Overall, CXLMemSim provides a practical and extensible platform for researchers and practitioners to explore CXL.mem innovations before real hardware becomes commonplace.

**Link**: [arxiv](http://arxiv.org/abs/2303.06153v2),  [pdf](http://arxiv.org/pdf/2303.06153v2)

**Tags**: cs.PF cs.AR cs.OS 



### Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache   Compression
**Authors**: Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh

**Updated**: 2025-06-17T02:24:51Z

**Summary**: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.05693v2),  [pdf](http://arxiv.org/pdf/2412.05693v2)

**Tags**: cs.CL 



### All-optical electric field sensing with nanodiamond-doped polymer thin   films
**Authors**: Roy Styles, Mengke Han, Toon Goris, James Partridge, Brett C. Johnson, Blanca del Rosal, Amanda N. Abraham, Heike Ebendorff-Heidepriem, Brant C. Gibson, Nikolai Dontschuk, Jean-Philippe Tetienne, Philipp Reineck

**Updated**: 2025-06-17T00:26:21Z

**Summary**: The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that exists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the NV's nanoscale environment. Here, we show that photoluminescence (PL) from NV centers in fluorescent nanodiamonds (FNDs) can be employed for all-optical voltage sensing based on electric field-induced NV charge state modulation. More than 95% of FNDs integrated into a capacitor device show a transient increase in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of an external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The change in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V, corresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices. The electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$. We investigate the NV charge state photodynamics on the millisecond timescale and find that the change in NV PL strongly depends on the rate of photoexcitation. We propose a model that qualitatively explains the observed changes in NV PL based on an electric field-induced redistribution of photoexcited electrons from substitutional nitrogen defects to NV centers, leading to a transient conversion of NV$^0$ to NV$^-$ centers upon application of an external voltage. Our results contribute to the development of FNDs as reliable, all-optical, nanoscale electric field sensors in solid-state systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.07350v2),  [pdf](http://arxiv.org/pdf/2505.07350v2)

**Tags**: cond-mat.mes-hall 



### glass: ordered set data structure for client-side order books
**Authors**: Viktor Krapivensky

**Updated**: 2025-06-16T20:46:20Z

**Summary**: The "ordered set" abstract data type with operations "insert", "erase", "find", "min", "max", "next" and "prev" is ubiquitous in computer science. It is usually implemented with red-black trees, $B$-trees, or $B^+$-trees. We present our implementation of ordered set based on a trie. It only supports integer keys (as opposed to keys of any strict weakly ordered type) and is optimized for market data, namely for what we call sequential locality. The following is the list of what we believe to be novelties:   * Cached path to exploit sequential locality, and fast truncation thereof on erase operation;   * A hash table (or, rather, a cache table) with hard O(1) time guarantees on any operation to speed up key lookup (up to a pre-leaf node);   * Hardware-accelerated "find next/previous set bit" operations with BMI2 instruction set extension on x86-64;   * Order book-specific features: the preemption principle and the tree restructure operation that prevent the tree from consuming too much memory.   We achieve the following speedups over C++'s standard std::map container: 6x-20x on modifying operations, 30x on lookup operations, 9x-15x on real market data, and a more modest 2x-3x speedup on iteration. In this paper, we discuss our implementation.

**Link**: [arxiv](http://arxiv.org/abs/2506.13991v1),  [pdf](http://arxiv.org/pdf/2506.13991v1)

**Tags**: cs.DS 



### Cache-Aided Variable-Length Coding with Perfect Privacy
**Authors**: Amirreza Zamani, Mikael Skoglund

**Updated**: 2025-06-16T17:17:38Z

**Summary**: A cache-aided compression problem with perfect privacy is studied, where a server has access to a database of $N$ files, $(Y_1,...,Y_N)$, each of size $F$ bits. The server is connected to $K$ users through a shared link, where each user has access to a local cache of size $MF$ bits. In the placement phase, the server fills the users$'$ caches without prior knowledge of their future demands, while the delivery phase takes place after the users send their demands to the server. We assume that each file $Y_i$ is arbitrarily correlated with a private attribute $X$, and an adversary is assumed to have access to the shared link. The users and the server have access to a shared secret key $W$. The goal is to design the cache contents and the delivered message $\cal C$ such that the average length of $\mathcal{C}$ is minimized, while satisfying: i. The response $\cal C$ does not disclose any information about $X$, i.e., $X$ and $\cal C$ are statistically independent yielding $I(X;\mathcal{C})=0$, which corresponds to the perfect privacy constraint; ii. User $i$ is able to decode its demand, $Y_{d_i}$, by using its local cache $Z_i$, delivered message $\cal C$, and the shared secret key $W$. Due to the correlation of database with the private attribute, existing codes for cache-aided delivery do not fulfill the perfect privacy constraint. Indeed, in this work, we propose a lossless variable-length coding scheme that combines privacy-aware compression with coded caching techniques. In particular, we use two-part code construction and Functional Representation Lemma. Furthermore, we propose an alternative coding scheme based on the minimum entropy coupling concept and a greedy entropy-based algorithm. We show that the proposed scheme improves the previous results obtained by Functional Representation Lemma.

**Link**: [arxiv](http://arxiv.org/abs/2306.13184v2),  [pdf](http://arxiv.org/pdf/2306.13184v2)

**Tags**: cs.IT math.IT 



### Mixture of Weight-shared Heterogeneous Group Attention Experts for   Dynamic Token-wise KV Optimization
**Authors**: Guanghui Song, Dongping Liao, Yiren Zhao, Kejiang Ye, Cheng-zhong Xu, Xitong Gao

**Updated**: 2025-06-16T14:30:17Z

**Summary**: Transformer models face scalability challenges in causal language modeling (CLM) due to inefficient memory allocation for growing key-value (KV) caches, which strains compute and storage resources. Existing methods like Grouped Query Attention (GQA) and token-level KV optimization improve efficiency but rely on rigid resource allocation, often discarding "low-priority" tokens or statically grouping them, failing to address the dynamic spectrum of token importance. We propose mixSGA, a novel mixture-of-expert (MoE) approach that dynamically optimizes token-wise computation and memory allocation. Unlike prior approaches, mixSGA retains all tokens while adaptively routing them to specialized experts with varying KV group sizes, balancing granularity and efficiency. Our key novelties include: (1) a token-wise expert-choice routing mechanism guided by learned importance scores, enabling proportional resource allocation without token discard; (2) weight-sharing across grouped attention projections to minimize parameter overhead; and (3) an auxiliary loss to ensure one-hot routing decisions for training-inference consistency in CLMs. Extensive evaluations across Llama3, TinyLlama, OPT, and Gemma2 model families show mixSGA's superiority over static baselines. On instruction-following and continued pretraining tasks, mixSGA achieves higher ROUGE-L and lower perplexity under the same KV budgets.

**Link**: [arxiv](http://arxiv.org/abs/2506.13541v1),  [pdf](http://arxiv.org/pdf/2506.13541v1)

**Tags**: cs.CL cs.LG 



### Block-wise Adaptive Caching for Accelerating Diffusion Policy
**Authors**: Kangye Ji, Yuan Meng, Hanyun Cui, Ye Li, Shengjia Hua, Lei Chen, Zhi Wang

**Updated**: 2025-06-16T13:14:58Z

**Summary**: Diffusion Policy has demonstrated strong visuomotor modeling capabilities, but its high computational cost renders it impractical for real-time robotic control. Despite huge redundancy across repetitive denoising steps, existing diffusion acceleration techniques fail to generalize to Diffusion Policy due to fundamental architectural and data divergences. In this paper, we propose Block-wise Adaptive Caching(BAC), a method to accelerate Diffusion Policy by caching intermediate action features. BAC achieves lossless action generation acceleration by adaptively updating and reusing cached features at the block level, based on a key observation that feature similarities vary non-uniformly across timesteps and locks. To operationalize this insight, we first propose the Adaptive Caching Scheduler, designed to identify optimal update timesteps by maximizing the global feature similarities between cached and skipped features. However, applying this scheduler for each block leads to signiffcant error surges due to the inter-block propagation of caching errors, particularly within Feed-Forward Network (FFN) blocks. To mitigate this issue, we develop the Bubbling Union Algorithm, which truncates these errors by updating the upstream blocks with signiffcant caching errors before downstream FFNs. As a training-free plugin, BAC is readily integrable with existing transformer-based Diffusion Policy and vision-language-action models. Extensive experiments on multiple robotic benchmarks demonstrate that BAC achieves up to 3x inference speedup for free.

**Link**: [arxiv](http://arxiv.org/abs/2506.13456v1),  [pdf](http://arxiv.org/pdf/2506.13456v1)

**Tags**: cs.AI cs.RO 



### On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed   Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains
**Authors**: Craig Steven Wright

**Updated**: 2025-06-16T08:43:56Z

**Summary**: This paper presents a formalised architecture for synthetic agents designed to retain immutable memory, verifiable reasoning, and constrained epistemic growth. Traditional AI systems rely on mutable, opaque statistical models prone to epistemic drift and historical revisionism. In contrast, we introduce the concept of the Merkle Automaton, a cryptographically anchored, deterministic computational framework that integrates formal automata theory with blockchain-based commitments. Each agent transition, memory fragment, and reasoning step is committed within a Merkle structure rooted on-chain, rendering it non-repudiable and auditably permanent. To ensure selective access and confidentiality, we derive symmetric encryption keys from ECDH exchanges contextualised by hierarchical privilege lattices. This enforces cryptographic access control over append-only DAG-structured knowledge graphs. Reasoning is constrained by formal logic systems and verified through deterministic traversal of policy-encoded structures. Updates are non-destructive and historied, preserving epistemic lineage without catastrophic forgetting. Zero-knowledge proofs facilitate verifiable, privacy-preserving inclusion attestations. Collectively, this architecture reframes memory not as a cache but as a ledger - one whose contents are enforced by protocol, bound by cryptography, and constrained by formal logic. The result is not an intelligent agent that mimics thought, but an epistemic entity whose outputs are provably derived, temporally anchored, and impervious to post hoc revision. This design lays foundational groundwork for legal, economic, and high-assurance computational systems that require provable memory, unforgeable provenance, and structural truth.

**Link**: [arxiv](http://arxiv.org/abs/2506.13246v1),  [pdf](http://arxiv.org/pdf/2506.13246v1)

**Tags**: cs.CR cs.AI cs.DC 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70, 68P25, 68T37 68Q70,
  68P25, 68T37 F.4.3; D.4.6; E.3; I.2.4 



### InfiniSST: Simultaneous Translation of Unbounded Speech with Large   Language Model
**Authors**: Siqi Ouyang, Xi Xu, Lei Li

**Updated**: 2025-06-16T06:38:23Z

**Summary**: Simultaneous translation of unbounded streaming speech remains a challenging problem due to the need for effectively processing the history speech context and past translations so that quality and latency, including computation overhead, can be balanced. Most prior works assume pre-segmented speech, limiting their real-world applicability. In this paper, we propose InfiniSST, a novel approach that formulates SST as a multi-turn dialogue task, enabling seamless translation of unbounded speech. We construct translation trajectories and robust segments from MuST-C with multi-latency augmentation during training and develop a key-value (KV) cache management strategy to facilitate efficient inference. Experiments on MuST-C En-Es, En-De, and En-Zh demonstrate that InfiniSST reduces computation-aware latency by 0.5 to 1 second while maintaining the same translation quality compared to baselines. Ablation studies further validate the contributions of our data construction and cache management strategy. We release the code and demo at https://github.com/LeiLiLab/InfiniSST

**Link**: [arxiv](http://arxiv.org/abs/2503.02969v2),  [pdf](http://arxiv.org/pdf/2503.02969v2)

**Tags**: cs.CL cs.AI 



### Multipole Attention for Efficient Long Context Reasoning
**Authors**: Coleman Hooper, Sebastian Zhao, Luca Manolache, Sehoon Kim, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami

**Updated**: 2025-06-16T03:00:40Z

**Summary**: Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.

**Link**: [arxiv](http://arxiv.org/abs/2506.13059v1),  [pdf](http://arxiv.org/pdf/2506.13059v1)

**Tags**: cs.CL cs.LG 



### Latent Multi-Head Attention for Small Language Models
**Authors**: Sushant Mehta, Raj Dandekar, Rajat Dandekar, Sreedath Panat

**Updated**: 2025-06-16T02:57:37Z

**Summary**: We present the first comprehensive study of latent multi-head attention (MLA) for small language models, revealing interesting efficiency-quality trade-offs. Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark three architectural variants: standard multi-head attention (MHA), MLA, and MLA with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory reduction while incurring only a 0.3% increase in validation loss (essentially matching MHA quality)- a Pareto improvement for memory constrained deployment. We further show that RoPE is crucial for MLA in small models: without it, MLA underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by 2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2 achieves a 1.4 times speedup over full-rank MLA while maintaining the memory savings. GPT-4 evaluations corroborate perplexity results, with ours achieving the highest quality scores (7.4/10) across grammar, creativity, and consistency metrics. Code and models will be released upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2506.09342v2),  [pdf](http://arxiv.org/pdf/2506.09342v2)

**Tags**: cs.CL cs.AI 



### BLITZSCALE: Fast and Live Large Model Autoscaling with O(1) Host Caching
**Authors**: Dingyan Zhang, Haotian Wang, Yang Liu, Xingda Wei, Yizhou Shan, Rong Chen, Haibo Chen

**Updated**: 2025-06-15T13:04:14Z

**Summary**: Model autoscaling is the key mechanism to achieve serverless model-as-a-service, but it faces a fundamental trade-off between scaling speed and storage/memory usage to cache parameters, and cannot meet frequent scaling requirements across multiple hosts. The key problem is that data plane performance is slow, and scaled instances remain stopped while parameters are loading. In this paper, we first show that the data plane can be made fast with no or O(1) caching by loading parameters through the compute network between GPUs because: (1) its speed is comparable to host cache and is underutilized, and (2) scaling multiple instances requires no or O(1) caching with network-optimized multicast. Second, autoscaling can be made live by breaking the scaling abstraction for inference from a coarse-grained instance-level to a fine-grained layer-level. This allows us to offload the layer computation from the overloaded serving instances to the scaled ones without waiting for the parameters to be fully loaded. Under real-world workloads, our system BLITZSCALE achieves up to 94 % lower tail latency reductions compared to state-of-the-art autoscaling system (ServerlessLLM), and it reduces the GPU time used for serving by 49 % when compared with serving systems that do not support autoscaling like DistServe and vLLM with the same service-level-agreement.

**Link**: [arxiv](http://arxiv.org/abs/2412.17246v2),  [pdf](http://arxiv.org/pdf/2412.17246v2)

**Tags**: cs.DC cs.OS 



### I Know What You Said: Unveiling Hardware Cache Side-Channels in Local   Large Language Model Inference
**Authors**: Zibo Gao, Junjie Hu, Feng Guo, Yixin Zhang, Yinglong Han, Siyuan Liu, Haiyang Li, Zhiqiang Lv

**Updated**: 2025-06-15T08:41:09Z

**Summary**: Large Language Models (LLMs) that can be deployed locally have recently gained popularity for privacy-sensitive tasks, with companies such as Meta, Google, and Intel playing significant roles in their development. However, the security of local LLMs through the lens of hardware cache side-channels remains unexplored. In this paper, we unveil novel side-channel vulnerabilities in local LLM inference: token value and token position leakage, which can expose both the victim's input and output text, thereby compromising user privacy. Specifically, we found that adversaries can infer the token values from the cache access patterns of the token embedding operation, and deduce the token positions from the timing of autoregressive decoding phases. To demonstrate the potential of these leaks, we design a novel eavesdropping attack framework targeting both open-source and proprietary LLM inference systems. The attack framework does not directly interact with the victim's LLM and can be executed without privilege.   We evaluate the attack on a range of practical local LLM deployments (e.g., Llama, Falcon, and Gemma), and the results show that our attack achieves promising accuracy. The restored output and input text have an average edit distance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the reconstructed texts achieve average cosine similarity scores of 98.7% (input) and 98.0% (output).

**Link**: [arxiv](http://arxiv.org/abs/2505.06738v3),  [pdf](http://arxiv.org/pdf/2505.06738v3)

**Tags**: cs.CR K.6.5 



### GTA: Grouped-head latenT Attention
**Authors**: Luoyang Sun, Jiwen Jiang, Cheng Deng, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, Jun Wang

**Updated**: 2025-06-15T07:19:33Z

**Summary**: Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.

**Link**: [arxiv](http://arxiv.org/abs/2506.17286v1),  [pdf](http://arxiv.org/pdf/2506.17286v1)

**Tags**: cs.CL cs.AI 



### ReFrame: Layer Caching for Accelerated Inference in Real-Time Rendering
**Authors**: Lufei Liu, Tor M. Aamodt

**Updated**: 2025-06-14T20:17:43Z

**Summary**: Graphics rendering applications increasingly leverage neural networks in tasks such as denoising, supersampling, and frame extrapolation to improve image quality while maintaining frame rates. The temporal coherence inherent in these tasks presents an opportunity to reuse intermediate results from previous frames and avoid redundant computations. Recent work has shown that caching intermediate features to be reused in subsequent inferences is an effective method to reduce latency in diffusion models. We extend this idea to real-time rendering and present ReFrame, which explores different caching policies to optimize trade-offs between quality and performance in rendering workloads. ReFrame can be applied to a variety of encoder-decoder style networks commonly found in rendering pipelines. Experimental results show that we achieve 1.4x speedup on average with negligible quality loss in three real-time rendering tasks. Code available: https://ubc-aamodt-group.github.io/reframe-layer-caching/

**Link**: [arxiv](http://arxiv.org/abs/2506.13814v1),  [pdf](http://arxiv.org/pdf/2506.13814v1)

**Tags**: cs.GR cs.LG eess.IV 



### Real-Time Agile Software Management for Edge and Fog Computing Based   Smart City Infrastructure
**Authors**: Debasish Jana, Pinakpani Pal, Pawan Kumar

**Updated**: 2025-06-14T20:00:53Z

**Summary**: The evolution of smart cities demands scalable, secure, and energy-efficient architectures for real-time data processing. With the number of IoT devices expected to exceed 40 billion by 2030, traditional cloud-based systems are increasingly constrained by bandwidth, latency, and energy limitations. This paper leverages the ROOF (Real-time Onsite Operations Facilitation) framework with decentralized computing at intermediary fog and peripheral edge network layers to reduce latency by processing data near its point of origin. ROOF features fog caching to avoid redundancy, ultra-low-power wireless transmission for energy savings, and AI-driven resource allocation for efficiency. Security is enhanced through TLS encryption, blockchain-based authentication, and edge-level access control. Case studies from Bhubaneswar, Barcelona and Copenhagen validate the use of ROOF in traffic systems and environmental monitoring. The paper concludes by outlining key challenges and prospects of AI-driven analytics in smart urban infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2506.12616v1),  [pdf](http://arxiv.org/pdf/2506.12616v1)

**Tags**: cs.SE 



### FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented   Generation
**Authors**: Zhuocheng Zhang, Yang Feng, Min Zhang

**Updated**: 2025-06-14T13:16:31Z

**Summary**: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.

**Link**: [arxiv](http://arxiv.org/abs/2506.12494v1),  [pdf](http://arxiv.org/pdf/2506.12494v1)

**Tags**: cs.CL cs.IR 



### Efficient Unified Caching for Accelerating Heterogeneous AI Workloads
**Authors**: Tianze Wang, Yifei Liu, Chen Chen, Pengfei Zuo, Jiawei Zhang, Qizhen Weng, Yin Chen, Zhenhua Han, Jieru Zhao, Quan Chen, Minyi Guo

**Updated**: 2025-06-14T06:36:54Z

**Summary**: Modern AI clusters, which host diverse workloads like data pre-processing, training and inference, often store the large-volume data in cloud storage and employ caching frameworks to facilitate remote data access. To avoid code-intrusion complexity and minimize cache space wastage, it is desirable to maintain a unified cache shared by all the workloads. However, existing cache management strategies, designed for specific workloads, struggle to handle the heterogeneous AI workloads in a cluster -- which usually exhibit heterogeneous access patterns and item storage granularities. In this paper, we propose IGTCache, a unified, high-efficacy cache for modern AI clusters. IGTCache leverages a hierarchical access abstraction, AccessStreamTree, to organize the recent data accesses in a tree structure, facilitating access pattern detection at various granularities. Using this abstraction, IGTCache applies hypothesis testing to categorize data access patterns as sequential, random, or skewed. Based on these detected access patterns and granularities, IGTCache tailors optimal cache management strategies including prefetching, eviction, and space allocation accordingly. Experimental results show that IGTCache increases the cache hit ratio by 55.6% over state-of-the-art caching frameworks, reducing the overall job completion time by 52.2%.

**Link**: [arxiv](http://arxiv.org/abs/2506.12370v1),  [pdf](http://arxiv.org/pdf/2506.12370v1)

**Tags**: cs.DC cs.LG 



### ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable   Compression
**Authors**: Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, Minyi Guo

**Updated**: 2025-06-14T06:17:33Z

**Summary**: Large Language Models (LLMs) have been widely deployed in a variety of applications, and the context length is rapidly increasing to handle tasks such as long-document QA and complex logical reasoning. However, long context poses significant challenges for inference efficiency, including high memory costs of key-value (KV) cache and increased latency due to extensive memory accesses. Recent works have proposed compressing KV cache to approximate computation, but these methods either evict tokens permanently, never recalling them for later inference, or recall previous tokens at the granularity of pages divided by textual positions. Both approaches degrade the model accuracy and output quality. To achieve efficient and accurate recallable KV cache compression, we introduce ClusterKV, which recalls tokens at the granularity of semantic clusters. We design and implement efficient algorithms and systems for clustering, selection, indexing and caching. Experiment results show that ClusterKV attains negligible accuracy loss across various tasks with 32k context lengths, using only a 1k to 2k KV cache budget, and achieves up to a 2$\times$ speedup in latency and a 2.5$\times$ improvement in decoding throughput. Compared to SoTA recallable KV compression methods, ClusterKV demonstrates higher model accuracy and output quality, while maintaining or exceeding inference efficiency. Our code is available at https://github.com/sjtu-zhao-lab/ClusterKV.

**Link**: [arxiv](http://arxiv.org/abs/2412.03213v2),  [pdf](http://arxiv.org/pdf/2412.03213v2)

**Tags**: cs.LG cs.AI cs.PF 



### Federated Learning Assisted Edge Caching Scheme Based on Lightweight   Architecture DDPM
**Authors**: Xun Li, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Khaled B. Letaief

**Updated**: 2025-06-14T00:52:10Z

**Summary**: Edge caching is an emerging technology that empowers caching units at edge nodes, allowing users to fetch contents of interest that have been pre-cached at the edge nodes. The key to pre-caching is to maximize the cache hit percentage for cached content without compromising users' privacy. In this letter, we propose a federated learning (FL) assisted edge caching scheme based on lightweight architecture denoising diffusion probabilistic model (LDPM). Our simulation results verify that our proposed scheme achieves a higher cache hit percentage compared to existing FL-based methods and baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2506.04593v3),  [pdf](http://arxiv.org/pdf/2506.04593v3)

**Tags**: cs.NI eess.SP 



### R-KV: Redundancy-aware KV Cache Compression for Reasoning Models
**Authors**: Zefan Cai, Wen Xiao, Hanshi Sun, Cheng Luo, Yikai Zhang, Ke Wan, Yucheng Li, Yeyang Zhou, Li-Wen Chang, Jiuxiang Gu, Zhen Dong, Anima Anandkumar, Abedelkadir Asi, Junjie Hu

**Updated**: 2025-06-13T21:01:43Z

**Summary**: Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 16% of the KV cache. This KV-cache reduction also leads to a 90% memory saving and a 6.6X throughput over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets.

**Link**: [arxiv](http://arxiv.org/abs/2505.24133v3),  [pdf](http://arxiv.org/pdf/2505.24133v3)

**Tags**: cs.CL cs.AI 



### Cartridges: Lightweight and general-purpose long context representations   via self-study
**Authors**: Sabri Eyuboglu, Ryan Ehrlich, Simran Arora, Neel Guha, Dylan Zinsley, Emily Liu, Will Tennien, Atri Rudra, James Zou, Azalia Mirhoseini, Christopher Re

**Updated**: 2025-06-13T17:58:55Z

**Summary**: Large language models are often used to answer queries grounded in large text corpora (e.g. codebases, legal documents, or chat histories) by placing the entire corpus in the context window and leveraging in-context learning (ICL). Although current models support contexts of 100K-1M tokens, this setup is costly to serve because the memory consumption of the KV cache scales with input length. We explore an alternative: training a smaller KV cache offline on each corpus. At inference time, we load this trained KV cache, which we call a Cartridge, and decode a response. Critically, the cost of training a Cartridge can be amortized across all the queries referencing the same corpus. However, we find that the naive approach of training the Cartridge with next-token prediction on the corpus is not competitive with ICL. Instead, we propose self-study, a training recipe in which we generate synthetic conversations about the corpus and train the Cartridge with a context-distillation objective. We find that Cartridges trained with self-study replicate the functionality of ICL, while being significantly cheaper to serve. On challenging long-context benchmarks, Cartridges trained with self-study match ICL performance while using 38.6x less memory and enabling 26.4x higher throughput. Self-study also extends the model's effective context length (e.g. from 128k to 484k tokens on MTOB) and surprisingly, leads to Cartridges that can be composed at inference time without retraining.

**Link**: [arxiv](http://arxiv.org/abs/2506.06266v3),  [pdf](http://arxiv.org/pdf/2506.06266v3)

**Tags**: cs.CL cs.AI cs.LG 



### CnC-PRAC: Coalesce, not Cache, Per Row Activation Counts for an   Efficient in-DRAM Rowhammer Mitigation
**Authors**: Chris S. Lin, Jeonghyun Woo, Prashant J. Nair, Gururaj Saileshwar

**Updated**: 2025-06-13T17:28:38Z

**Summary**: JEDEC has introduced the Per Row Activation Counting (PRAC) framework for DDR5 and future DRAMs to enable precise counting of DRAM row activations using per-row activation counts. While recent PRAC implementations enable holistic mitigation of Rowhammer attacks, they impose slowdowns of up to 10% due to the increased DRAM timings for performing a read-modify-write of the counter. Alternatively, recent work, Chronus, addresses these slowdowns, but incurs energy overheads due to the additional DRAM activations for counters. In this paper, we propose CnC-PRAC, a PRAC implementation that addresses both performance and energy overheads. Unlike prior works focusing on caching activation counts to reduce their overheads, our key idea is to reorder and coalesce accesses to activation counts located in the same physical row. Our design achieves this by decoupling counter access from the critical path of data accesses. This enables optimizations such as buffering counter read-modify-write requests and coalescing requests to the same row. Together, these enable a reduction in row activations for counter accesses by almost 75%-83% compared to state-of-the-art solutions like Chronus and enable a PRAC implementation with negligible slowdown and a minimal dynamic energy overhead of 0.84%-1% compared to insecure DDR5 DRAM.

**Link**: [arxiv](http://arxiv.org/abs/2506.11970v1),  [pdf](http://arxiv.org/pdf/2506.11970v1)

**Tags**: cs.CR 



### Beyond Homogeneous Attention: Memory-Efficient LLMs via   Fourier-Approximated KV Cache
**Authors**: Xiaoran Liu, Siyang He, Qiqi Wang, Ruixiao Li, Yuerong Song, Zhigeng Liu, Linlin Li, Qun Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

**Updated**: 2025-06-13T15:35:54Z

**Summary**: Large Language Models struggle with memory demands from the growing Key-Value (KV) cache as context lengths increase. Existing compression methods homogenize head dimensions or rely on attention-guided token pruning, often sacrificing accuracy or introducing computational overhead. We propose FourierAttention, a training-free framework that exploits the heterogeneous roles of transformer head dimensions: lower dimensions prioritize local context, while upper ones capture long-range dependencies. By projecting the long-context-insensitive dimensions onto orthogonal Fourier bases, FourierAttention approximates their temporal evolution with fixed-length spectral coefficients. Evaluations on LLaMA models show that FourierAttention achieves the best long-context accuracy on LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel, FlashFourierAttention, is designed to optimize memory via streamlined read-write operations, enabling efficient deployment without performance compromise.

**Link**: [arxiv](http://arxiv.org/abs/2506.11886v1),  [pdf](http://arxiv.org/pdf/2506.11886v1)

**Tags**: cs.CL 



### FlashBack:Efficient Retrieval-Augmented Language Modeling for Long   Context Inference
**Authors**: Runheng Liu, Xingchen Xiao, Heyan Huang, Zewen Chi, Zhijing Wu

**Updated**: 2025-06-13T08:32:26Z

**Summary**: Retrieval-Augmented Language Modeling (RALM) by integrating large language models (LLM) with relevant documents from an external corpus is a proven method for enabling the LLM to generate information beyond the scope of its pre-training corpus. Previous work utilizing retrieved content by simply prepending it to the input poses a high runtime issue, which degrades the inference efficiency of the LLMs because they fail to use the Key-Value (KV) cache efficiently. In this paper, we propose FlashBack, a modular RALM designed to improve the inference efficiency of RALM with appending context pattern while maintaining decent performance after fine-tuning by Low-Rank Adaption. FlashBack appends retrieved documents at the end of the context for efficiently utilizing the KV cache instead of prepending them. And we introduce Marking Token as two special prompt tokens for marking the boundary of the appending context during fine-tuning. Our experiments on testing generation quality show that FlashBack can remain decent generation quality in perplexity. And the inference speed of FlashBack is up to $4\times$ faster than the prepending counterpart on a 7B LLM (Llama 2) in the runtime test. Via bypassing unnecessary re-computation, it demonstrates an advancement by achieving significantly faster inference speed, and this heightened efficiency will substantially reduce inferential cost.

**Link**: [arxiv](http://arxiv.org/abs/2405.04065v4),  [pdf](http://arxiv.org/pdf/2405.04065v4)

**Tags**: cs.CL 



### MapQaTor: An Extensible Framework for Efficient Annotation of Map-Based   QA Datasets
**Authors**: Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez

**Updated**: 2025-06-13T07:04:46Z

**Summary**: Mapping and navigation services like Google Maps, Apple Maps, OpenStreetMap, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, an extensible open-source framework that streamlines the creation of reproducible, traceable map-based QA datasets. MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/bVv7-NYRsTw.

**Link**: [arxiv](http://arxiv.org/abs/2412.21015v2),  [pdf](http://arxiv.org/pdf/2412.21015v2)

**Tags**: cs.CL cs.HC 



### Lag-Relative Sparse Attention In Long Context Training
**Authors**: Manlai Liang, Wanyi Huang, Mandi Liu, Huaijun Li, Jinlong Li

**Updated**: 2025-06-13T06:49:53Z

**Summary**: Large Language Models (LLMs) have made significant strides in natural language processing and generation, yet their ability to handle long-context input remains constrained by the quadratic complexity of attention computation and linear-increasing key-value memory footprint. To reduce computational costs and memory, key-value cache compression techniques are commonly applied at inference time, but this often leads to severe performance degradation, as models are not trained to handle compressed context. Although there are more sophisticated compression methods, they are typically unsuitable for post-training because of their incompatibility with gradient-based optimization or high computation overhead. To fill this gap with no additional parameter and little computation overhead, we propose Lag-Relative Sparse Attention(LRSA) anchored by the LagKV compression method for long context post-training. Our method performs chunk-by-chunk prefilling, which selects the top K most relevant key-value pairs in a fixed-size lagging window, allowing the model to focus on salient historical context while maintaining efficiency. Experimental results show that our approach significantly enhances the robustness of the LLM with key-value compression and achieves better fine-tuned results in the question-answer tuning task.

**Link**: [arxiv](http://arxiv.org/abs/2506.11498v1),  [pdf](http://arxiv.org/pdf/2506.11498v1)

**Tags**: cs.CL 



### Electric field control of third-order nonlinear Hall effect
**Authors**: Jiaju Yang, Lujun Wei, Yanghui Li, Lina Chen, Wei Niu, Jiarui Chen, Jun Du, Yong Pu

**Updated**: 2025-06-13T02:54:42Z

**Summary**: The third-order nonlinear Hall effect (NLHE) serves as a sensitive probe of energy band geometric property, providing a new paradigm for revealing the Berry curvature distribution and topological response of quantum materials. In the Weyl semimetal TaIrTe4, we report for the first time that the sign of the third-order NLHE reverses with decreasing temperature. Through scaling law analysis, we think that the third-order NLHE at high (T > 23 K) and low (T < 23 K) temperatures is dominated by Berry-connection polarizability (BCP) and impurity scattering, respectively. The third-order NLHE response strength can be effectively modulated by an additional applied in-plane constant electric field. At the high temperature region, the BCP reduction induced by the electric field leads to a decrease in the third-order NLHE response strength, while at the low temperature region, the electric field cause both BCP and impurity scattering effects to weaken, resulting in a more significant modulation of the third-order NLHE response strength. At 4 K and an electric field strength of 0.3 kV/cm, the modulated relative response strength could reach up to 65.3%. This work provides a new means to explore the third-order NLHE and a valuable reference for the development of novel electronic devices.

**Link**: [arxiv](http://arxiv.org/abs/2506.10657v2),  [pdf](http://arxiv.org/pdf/2506.10657v2)

**Tags**: cond-mat.mes-hall cond-mat.mtrl-sci 



### Efficient Long-Context LLM Inference via KV Cache Clustering
**Authors**: Jie Hu, Shengnan Wang, Yutong He, Ping Gong, Jiawei Yi, Juncheng Zhang, Youhui Bai, Renhai Chen, Gong Zhang, Cheng Li, Kun Yuan

**Updated**: 2025-06-13T02:36:15Z

**Summary**: Large language models (LLMs) with extended context windows have become increasingly prevalent for tackling complex tasks. However, the substantial Key-Value (KV) cache required for long-context LLMs poses significant deployment challenges. Existing approaches either discard potentially critical information needed for future generations or offer limited efficiency gains due to high computational overhead. In this paper, we introduce Chelsea, a simple yet effective framework for online KV cache clustering. Our approach is based on the observation that key states exhibit high similarity along the sequence dimension. To enable efficient clustering, we divide the sequence into chunks and propose Chunked Soft Matching, which employs an alternating partition strategy within each chunk and identifies clusters based on similarity. Chelsea then merges the KV cache within each cluster into a single centroid. Additionally, we provide a theoretical analysis of the computational complexity and the optimality of the intra-chunk partitioning strategy. Extensive experiments across various models and long-context benchmarks demonstrate that Chelsea achieves up to 80% reduction in KV cache memory usage while maintaining comparable model performance. Moreover, with minimal computational overhead, Chelsea accelerates the decoding stage of inference by up to 3.19$\times$ and reduces end-to-end latency by up to 2.72$\times$.

**Link**: [arxiv](http://arxiv.org/abs/2506.11418v1),  [pdf](http://arxiv.org/pdf/2506.11418v1)

**Tags**: cs.CL 



### Accelerating Diffusion Large Language Models with SlowFast Sampling: The   Three Golden Principles
**Authors**: Qingyan Wei, Yaojie Zhang, Zhiyuan Liu, Dongrui Liu, Linfeng Zhang

**Updated**: 2025-06-13T02:28:47Z

**Summary**: Diffusion-based language models (dLLMs) have emerged as a promising alternative to traditional autoregressive LLMs by enabling parallel token generation and significantly reducing inference latency. However, existing sampling strategies for dLLMs, such as confidence-based or semi-autoregressive decoding, often suffer from static behavior, leading to suboptimal efficiency and limited flexibility. In this paper, we propose SlowFast Sampling, a novel dynamic sampling strategy that adaptively alternates between exploratory and accelerated decoding stages. Our method is guided by three golden principles: certainty principle, convergence principle, and positional principle, which govern when and where tokens can be confidently and efficiently decoded. We further integrate our strategy with dLLM-Cache to reduce redundant computation. Extensive experiments across benchmarks and models show that SlowFast Sampling achieves up to 15.63$\times$ speedup on LLaDA with minimal accuracy drop, and up to 34.22$\times$ when combined with caching. Notably, our approach outperforms strong autoregressive baselines like LLaMA3 8B in throughput, demonstrating that well-designed sampling can unlock the full potential of dLLMs for fast and high-quality generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.10848v2),  [pdf](http://arxiv.org/pdf/2506.10848v2)

**Tags**: cs.CL cs.AI cs.LG 



### A4: Microarchitecture-Aware LLC Management for Datacenter Servers with   Emerging I/O Devices
**Authors**: Haneul Park, Jiaqi Lou, Sangjin Lee, Yifan Yuan, Kyoung Soo Park, Yongseok Son, Ipoom Jeong, Nam Sung Kim

**Updated**: 2025-06-12T21:57:27Z

**Summary**: In modern server CPUs, the Last-Level Cache (LLC) serves not only as a victim cache for higher-level private caches but also as a buffer for low-latency DMA transfers between CPU cores and I/O devices through Direct Cache Access (DCA). However, prior work has shown that high-bandwidth network-I/O devices can rapidly flood the LLC with packets, often causing significant contention with co-running workloads. One step further, this work explores hidden microarchitectural properties of the Intel Xeon CPUs, uncovering two previously unrecognized LLC contentions triggered by emerging high-bandwidth I/O devices. Specifically, (C1) DMA-written cache lines in LLC ways designated for DCA (referred to as DCA ways) are migrated to certain LLC ways (denoted as inclusive ways) when accessed by CPU cores, unexpectedly contending with non-I/O cache lines within the inclusive ways. In addition, (C2) high-bandwidth storage-I/O devices, which are increasingly common in datacenter servers, benefit little from DCA while contending with (latency-sensitive) network-I/O devices within DCA ways. To this end, we present \design, a runtime LLC management framework designed to alleviate both (C1) and (C2) among diverse co-running workloads, using a hidden knob and other hardware features implemented in those CPUs. Additionally, we demonstrate that \design can also alleviate other previously known network-I/O-driven LLC contentions. Overall, it improves the performance of latency-sensitive, high-priority workloads by 51\% without notably compromising that of low-priority workloads.

**Link**: [arxiv](http://arxiv.org/abs/2506.11329v1),  [pdf](http://arxiv.org/pdf/2506.11329v1)

**Tags**: cs.AR 



### SwiftSpec: Ultra-Low Latency LLM Decoding by Scaling Asynchronous   Speculative Decoding
**Authors**: Ziyi Zhang, Ziheng Jiang, Chengquan Jiang, Menghan Yu, Size Zheng, Haibin Lin, Henry Hoffmann, Xin Liu

**Updated**: 2025-06-12T21:15:58Z

**Summary**: Low-latency decoding for large language models (LLMs) is crucial for applications like chatbots and code assistants, yet generating long outputs remains slow in single-query settings. Prior work on speculative decoding (which combines a small draft model with a larger target model) and tensor parallelism has each accelerated decoding. However, conventional approaches fail to apply both simultaneously due to imbalanced compute requirements (between draft and target models), KV-cache inconsistencies, and communication overheads under small-batch tensor-parallelism. This paper introduces SwiftSpec, a system that targets ultra-low latency for LLM decoding. SwiftSpec redesigns the speculative decoding pipeline in an asynchronous and disaggregated manner, so that each component can be scaled flexibly and remove draft overhead from the critical path. To realize this design, SwiftSpec proposes parallel tree generation, tree-aware KV cache management, and fused, latency-optimized kernels to overcome the challenges listed above. Across 5 model families and 6 datasets, SwiftSpec achieves an average of 1.75x speedup over state-of-the-art speculative decoding systems and, as a highlight, serves Llama3-70B at 348 tokens/s on 8 Nvidia Hopper GPUs, making it the fastest known system for low-latency LLM serving at this scale.

**Link**: [arxiv](http://arxiv.org/abs/2506.11309v1),  [pdf](http://arxiv.org/pdf/2506.11309v1)

**Tags**: cs.DC cs.LG 



### Revisiting Main Memory-Based Covert and Side Channel Attacks in the   Context of Processing-in-Memory
**Authors**: F. Nisa Bostanci, Konstantinos Kanellopoulos, Ataberk Olgun, A. Giray Yaglikci, Ismail Emir Yuksel, Nika Mansouri Ghiasi, Zulal Bingol, Mohammad Sadrosadati, Onur Mutlu

**Updated**: 2025-06-12T20:38:42Z

**Summary**: We introduce IMPACT, a set of high-throughput main memory-based timing attacks that leverage characteristics of processing-in-memory (PiM) architectures to establish covert and side channels. IMPACT enables high-throughput communication and private information leakage by exploiting the shared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates expensive cache bypassing steps required by processor-centric memory-based timing attacks and (ii) leverages the intrinsic parallelism of PiM operations. We showcase two applications of IMPACT. First, we build two covert channels that leverage different PiM approaches (i.e., processing-near-memory and processing-using-memory) to establish high-throughput covert communication channels. Our covert channels achieve 8.2 Mb/s and 14.8 Mb/s communication throughput, respectively, which is 3.6x and 6.5x higher than the state-of-the-art main memory-based covert channel. Second, we showcase a side-channel attack that leaks private information of concurrently-running victim applications with a low error rate. Our source-code is openly and freely available at https://github.com/CMU-SAFARI/IMPACT.

**Link**: [arxiv](http://arxiv.org/abs/2404.11284v4),  [pdf](http://arxiv.org/pdf/2404.11284v4)

**Tags**: cs.CR cs.AR 



### SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language   Models
**Authors**: Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar

**Updated**: 2025-06-12T13:33:52Z

**Summary**: Large language models (LLMs) have been widely adopted due to their remarkable performance across various applications, driving the accelerated development of a large number of diverse models. However, these individual LLMs show limitations in generalization and performance on complex tasks due to inherent training biases, model size constraints, and the quality or diversity of pre-training datasets. A promising direction is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. To address these limitations, we introduce a novel LLM selection algorithm called SelectLLM, which efficiently directs input queries to the most suitable subset of LLMs from a large pool, ensuring that the selected models collectively provide accurate responses. SelectLLM employs a multi-label classifier and policy based on the classifier's predictions and confidence scores in selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate that the proposed model outperforms existing ensemble-based baselines and achieves competitive performance with similarly sized top-performing LLMs while maintaining efficiency. Specifically, it achieves a huge reduction in inference latency on two challenging reasoning benchmarks: 13\% on GSM8K and 70\% on MMLU, compared to the top-performing baseline. Also, we establish a theoretical upper bound by an Oracle with LLMs and perform an in-depth linguistic analysis to understand the performance gap between the Oracle and SelectLLM.

**Link**: [arxiv](http://arxiv.org/abs/2408.08545v4),  [pdf](http://arxiv.org/pdf/2408.08545v4)

**Tags**: cs.CL 



### TransMLA: Multi-Head Latent Attention Is All You Need
**Authors**: Fanxu Meng, Pingzhi Tang, Xiaojuan Tang, Zengwei Yao, Xing Sun, Muhan Zhang

**Updated**: 2025-06-12T11:45:57Z

**Summary**: In this paper, we present TransMLA, a framework that seamlessly converts any GQA-based pre-trained model into an MLA-based model. Our approach enables direct compatibility with DeepSeek's codebase, allowing these models to fully leverage DeepSeek-specific optimizations such as vLLM and SGlang. By compressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x inference speedup at an 8K context length while preserving meaningful output quality. Additionally, the model requires only 6 billion tokens for fine-tuning to regain performance on par with the original across multiple benchmarks. TransMLA offers a practical solution for migrating GQA-based models to the MLA structure. When combined with DeepSeek's advanced features, such as FP8 quantization and Multi-Token Prediction, even greater inference acceleration can be realized.

**Link**: [arxiv](http://arxiv.org/abs/2502.07864v5),  [pdf](http://arxiv.org/pdf/2502.07864v5)

**Tags**: cs.LG cs.AI 



### VSAG: An Optimized Search Framework for Graph-based Approximate Nearest   Neighbor Search
**Authors**: Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng

**Updated**: 2025-06-12T11:26:10Z

**Summary**: Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graph-based ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index.   This paper introduces VSAG, an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations: (i) efficient memory access: it reduces L3 cache misses with pre-fetching and cache-friendly vector organization; (ii) automated parameter tuning: it automatically selects performance-optimal parameters without requiring index rebuilding; (iii) efficient distance computation: it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate VSAG on real-world datasets. The experimental results show that VSAG achieves the state-of-the-art performance and provides up to 4x speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.17911v2),  [pdf](http://arxiv.org/pdf/2503.17911v2)

**Tags**: cs.DB 



### Qronos: Correcting the Past by Shaping the Future... in Post-Training   Quantization
**Authors**: Shihao Zhang, Haoyu Zhang, Ian Colbert, Rayan Saab

**Updated**: 2025-06-12T00:25:14Z

**Summary**: We introduce Qronos -- a new state-of-the-art post-training quantization algorithm that sequentially rounds and updates neural network weights. Qronos not only explicitly corrects errors due to both weight and activation quantization, but also errors resulting from quantizing previous layers. Our iterative algorithm is based on an interpretable and disciplined optimization framework that subsumes and surpasses existing data-driven approaches. At each step, Qronos alternates between error correction and diffusion via optimal update rules. Importantly, we prove that Qronos admits an efficient implementation that uses the Cholesky decomposition for solving least-squares problems. We also demonstrate that Qronos is compatible with existing transformation techniques such as Hadamard-based incoherence processing and weight-activation scaling equalization, among others. We evaluate Qronos using recent autoregressive language generation models in the Llama3 family; Qronos consistently outperforms previous state-of-the-art adaptive rounding methods when quantizing the weights, activations, and/or KV caches.

**Link**: [arxiv](http://arxiv.org/abs/2505.11695v2),  [pdf](http://arxiv.org/pdf/2505.11695v2)

**Tags**: cs.LG cs.AI math.OC 



### Squeezed Attention: Accelerating Long Context Length LLM Inference
**Authors**: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Monishwaran Maheswaran, Sebastian Zhao, June Paik, Michael W. Mahoney, Kurt Keutzer, Amir Gholami

**Updated**: 2025-06-11T22:50:44Z

**Summary**: Emerging Large Language Model (LLM) applications require long input context in order to perform complex tasks like document analysis and code generation. For these long context length applications, the length of the input prompt poses a significant challenge in terms of inference efficiency since the inference costs increase linearly with sequence length. However, for many of these applications, much of the context in the prompt is fixed across different user inputs, thereby providing the opportunity to perform offline optimizations in order to process user inputs quickly, as they are received. We propose Squeezed Attention to accelerate LLM applications where a large portion of the input context is fixed. We first leverage K-means clustering offline to group the keys for the fixed context based on semantic similarity and represent each cluster with a single centroid value. During inference, we compare query tokens from the user input with the centroids to predict which keys from the fixed context are semantically relevant, and then compute exact attention using only the important keys, thereby reducing bandwidth and computational costs. We also present a hierarchical version of our algorithm which can reduce the complexity of attention from linear to logarithmic with respect to the fixed context length. We evaluate our method on long-context benchmarks including LongBench, where it achieves a 3.1$\times$ reduction in KV budget with no noticeable accuracy loss and up to an 8$\times$ reduction with only a 0.5 point accuracy gap for the LLaMA-2-7B-32K, LWM-Text-Chat-1M, and Longchat-7B-v1.5-32K models. Futhermore, we implement kernels for centroid comparison and sparse FlashAttention with important keys, achieving more than 4$\times$ speedups during both the prefill and generation phases for long-context inference. Our code is available at https://github.com/SqueezeAILab/SqueezedAttention.

**Link**: [arxiv](http://arxiv.org/abs/2411.09688v3),  [pdf](http://arxiv.org/pdf/2411.09688v3)

**Tags**: cs.CL 



### ETS: Efficient Tree Search for Inference-Time Scaling
**Authors**: Coleman Hooper, Sehoon Kim, Suhong Moon, Kerem Dilmen, Monishwaran Maheswaran, Nicholas Lee, Michael W. Mahoney, Sophia Shao, Kurt Keutzer, Amir Gholami

**Updated**: 2025-06-11T21:59:20Z

**Summary**: Test-time compute scaling has emerged as a new axis along which to improve model accuracy, where additional computation is used at inference time to allow the model to think longer for more challenging problems. One promising approach for test-time compute scaling is search against a process reward model, where a model generates multiple potential candidates at each step of the search, and these partial trajectories are then scored by a separate reward model in order to guide the search process. The diversity of trajectories in the tree search process affects the accuracy of the search, since increasing diversity promotes more exploration. However, this diversity comes at a cost, as divergent trajectories have less KV sharing, which means they consume more memory and slow down the search process. Previous search methods either do not perform sufficient exploration, or else explore diverse trajectories but have high latency. We address this challenge by proposing Efficient Tree Search (ETS), which promotes KV sharing by pruning redundant trajectories while maintaining necessary diverse trajectories. ETS incorporates a linear programming cost model to promote KV cache sharing by penalizing the number of nodes retained, while incorporating a semantic coverage term into the cost model to ensure that we retain trajectories which are semantically different. We demonstrate how ETS can achieve 1.8$\times$ reduction in average KV cache size during the search process, leading to 1.4$\times$ increased throughput relative to prior state-of-the-art methods, with minimal accuracy degradation and without requiring any custom kernel implementation. Code is available at: https://github.com/SqueezeAILab/ETS.

**Link**: [arxiv](http://arxiv.org/abs/2502.13575v2),  [pdf](http://arxiv.org/pdf/2502.13575v2)

**Tags**: cs.LG 



### EfficientVLA: Training-Free Acceleration and Compression for   Vision-Language-Action Models
**Authors**: Yantai Yang, Yuhao Wang, Zichen Wen, Luo Zhongwei, Chang Zou, Zhipeng Zhang, Chuan Wen, Linfeng Zhang

**Updated**: 2025-06-11T18:34:57Z

**Summary**: Vision-Language-Action (VLA) models, particularly diffusion-based architectures, demonstrate transformative potential for embodied intelligence but are severely hampered by high computational and memory demands stemming from extensive inherent and inference-time redundancies. While existing acceleration efforts often target isolated inefficiencies, such piecemeal solutions typically fail to holistically address the varied computational and memory bottlenecks across the entire VLA pipeline, thereby limiting practical deployability. We introduce EfficientVLA, a structured and training-free inference acceleration framework that systematically eliminates these barriers by cohesively exploiting multifaceted redundancies. EfficientVLA synergistically integrates three targeted strategies: (1) pruning of functionally inconsequential layers from the language module, guided by an analysis of inter-layer redundancies; (2) optimizing the visual processing pathway through a task-aware strategy that selects a compact, diverse set of visual tokens, balancing task-criticality with informational coverage; and (3) alleviating temporal computational redundancy within the iterative diffusion-based action head by strategically caching and reusing key intermediate features. We apply our method to a standard VLA model CogACT, yielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6% success rate drop in the SIMPLER benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2506.10100v1),  [pdf](http://arxiv.org/pdf/2506.10100v1)

**Tags**: cs.CV 



### Mainframe-style channel controllers for modern disaggregated memory   systems
**Authors**: Zikai Liu, Jasmin Schult, Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-06-11T14:03:13Z

**Summary**: Despite the promise of alleviating the main memory bottleneck, and the existence of commercial hardware implementations, techniques for Near-Data Processing have seen relatively little real-world deployment. The idea has received renewed interest with the appearance of disaggregated or "far" memory, for example in the use of CXL memory pools.   However, we argue that the lack of a clear OS-centric abstraction of Near-Data Processing is a major barrier to adoption of the technology. Inspired by the channel controllers which interface the CPU to disk drives in mainframe systems, we propose memory channel controllers as a convenient, portable, and virtualizable abstraction of Near-Data Processing for modern disaggregated memory systems.   In addition to providing a clean abstraction that enables OS integration while requiring no changes to CPU architecture, memory channel controllers incorporate another key innovation: they exploit the cache coherence provided by emerging interconnects to provide a much richer programming model, with more fine-grained interaction, than has been possible with existing designs.

**Link**: [arxiv](http://arxiv.org/abs/2506.09758v1),  [pdf](http://arxiv.org/pdf/2506.09758v1)

**Tags**: cs.OS cs.AR cs.ET 



### Commissioning, characterization and first high dose rate irradiations at   a compact X-ray tube for microbeam and minibeam radiation therapy
**Authors**: Christian Petrich, Johanna Winter, Anton Dimroth, Thomas Beiser, Monika Dehn, Jessica Stolz, Jacopo Frignani, Stephanie E. Combs, Franz Schilling, Ghaleb Natour, Kurt Aulenbacher, Thomas E. Schmid, Jan J. Wilkens, Stefan Bartzsch

**Updated**: 2025-06-11T09:08:59Z

**Summary**: Minibeam and microbeam radiation therapy promise improved treatment outcomes through reduced normal tissue toxicity at better tumor control rates. The lack of suitable compact radiation sources limits the clinical application of minibeams to superficial tumors and renders it impossible for microbeams. We developed and constructed the first prototype of a compact line-focus X-ray tube (LFXT) with technology potentially suitable for clinical translation of minibeams and microbeams. We give an overview of the commissioning process preceding the first operation, present optical and radiological focal spot characterization methods, and dosimetric measurements. Additionally, we report on first preclinical in vitro cell and in vivo mouse brain irradiations conducted with the LFXT prototype. The focal spot characterization resulted in a strongly eccentric electron distribution with a width of 72.3 $\mu$m. Dosimetry showed sharp microbeam dose profiles with steep lateral penumbras and a peak-to-valley dose ratio above 10 throughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was measured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at 150 mm distance from the focal spot. In vitro and in vivo experiments demonstrated the feasibility of the LFXT for minibeam and microbeam applications with field sizes of 1.5-2 cm. The mice displayed no observable side effects throughout the follow-up period after whole-brain 260 $\mu$m-minibeam irradiation. We successfully constructed and commissioned the first proof-of-concept LFXT prototype. Dosimetric characterizations of the achieved microbeam field showed the superiority of the LFXT compared to conventional X-ray tubes in terms of beam quality. In future developments, the remaining limitations of the prototype will be addressed for improved minibeam and first ever microbeam radiation therapy in a clinical setting.

**Link**: [arxiv](http://arxiv.org/abs/2506.09536v1),  [pdf](http://arxiv.org/pdf/2506.09536v1)

**Tags**: physics.med-ph 



### NestQuant: Nested Lattice Quantization for Matrix Products and LLMs
**Authors**: Semyon Savkin, Eitan Porat, Or Ordentlich, Yury Polyanskiy

**Updated**: 2025-06-11T06:01:15Z

**Summary**: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent works have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot (8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation benchmarks confirm uniform superiority of NestQuant.

**Link**: [arxiv](http://arxiv.org/abs/2502.09720v2),  [pdf](http://arxiv.org/pdf/2502.09720v2)

**Tags**: cs.LG cs.AI cs.IT math.IT 



### SAFEFLOW: A Principled Protocol for Trustworthy and Transactional   Autonomous Agent Systems
**Authors**: Peiran Li, Xinkai Zou, Zhuohang Wu, Ruifeng Li, Shuo Xing, Hanwen Zheng, Zhikai Hu, Yuping Wang, Haoxi Li, Qin Yuan, Yingmo Zhang, Zhengzhong Tu

**Updated**: 2025-06-11T03:14:10Z

**Summary**: Recent advances in large language models (LLMs) and vision-language models (VLMs) have enabled powerful autonomous agents capable of complex reasoning and multi-modal tool use. Despite their growing capabilities, today's agent frameworks remain fragile, lacking principled mechanisms for secure information flow, reliability, and multi-agent coordination. In this work, we introduce SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based agents. SAFEFLOW enforces fine-grained information flow control (IFC), precisely tracking provenance, integrity, and confidentiality of all the data exchanged between agents, tools, users, and environments. By constraining LLM reasoning to respect these security labels, SAFEFLOW prevents untrusted or adversarial inputs from contaminating high-integrity decisions. To ensure robustness in concurrent multi-agent settings, SAFEFLOW introduces transactional execution, conflict resolution, and secure scheduling over shared state, preserving global consistency across agents. We further introduce mechanisms, including write-ahead logging, rollback, and secure caches, that further enhance resilience against runtime errors and policy violations. To validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark suite designed to evaluate agent reliability under adversarial, noisy, and concurrent operational conditions. Extensive experiments demonstrate that agents built with SAFEFLOW maintain impressive task performance and security guarantees even in hostile environments, substantially outperforming state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for principled, robust, and secure agent ecosystems, advancing the frontier of reliable autonomy.

**Link**: [arxiv](http://arxiv.org/abs/2506.07564v3),  [pdf](http://arxiv.org/pdf/2506.07564v3)

**Tags**: cs.AI cs.CL 



### Autoregressive Adversarial Post-Training for Real-Time Interactive Video   Generation
**Authors**: Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, Lu Jiang

**Updated**: 2025-06-11T03:04:23Z

**Summary**: Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2

**Link**: [arxiv](http://arxiv.org/abs/2506.09350v1),  [pdf](http://arxiv.org/pdf/2506.09350v1)

**Tags**: cs.CV cs.AI cs.LG 



### ScalableHD: Scalable and High-Throughput Hyperdimensional Computing   Inference on Multi-Core CPUs
**Authors**: Dhruv Parikh, Viktor Prasanna

**Updated**: 2025-06-10T22:46:12Z

**Summary**: Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that represents and manipulates information using high-dimensional vectors, called hypervectors (HV). Traditional HDC methods, while robust to noise and inherently parallel, rely on single-pass, non-parametric training and often suffer from low accuracy. To address this, recent approaches adopt iterative training of base and class HVs, typically accelerated on GPUs. Inference, however, remains lightweight and well-suited for real-time execution. Yet, efficient HDC inference has been studied almost exclusively on specialized hardware such as FPGAs and GPUs, with limited attention to general-purpose multi-core CPUs. To address this gap, we propose ScalableHD for scalable and high-throughput HDC inference on multi-core CPUs. ScalableHD employs a two-stage pipelined execution model, where each stage is parallelized across cores and processes chunks of base and class HVs. Intermediate results are streamed between stages using a producer-consumer mechanism, enabling on-the-fly consumption and improving cache locality. To maximize performance, ScalableHD integrates memory tiling and NUMA-aware worker-to-core binding. Further, it features two execution variants tailored for small and large batch sizes, each designed to exploit compute parallelism based on workload characteristics while mitigating the memory-bound compute pattern that limits HDC inference performance on modern multi-core CPUs. ScalableHD achieves up to 10x speedup in throughput (samples per second) over state-of-the-art baselines such as TorchHD, across a diverse set of tasks ranging from human activity recognition to image classification, while preserving task accuracy. Furthermore, ScalableHD exhibits robust scalability: increasing the number of cores yields near-proportional throughput improvements.

**Link**: [arxiv](http://arxiv.org/abs/2506.09282v1),  [pdf](http://arxiv.org/pdf/2506.09282v1)

**Tags**: cs.DC cs.LG 



### A Stable Whitening Optimizer for Efficient Neural Network Training
**Authors**: Kevin Frans, Sergey Levine, Pieter Abbeel

**Updated**: 2025-06-10T22:01:14Z

**Summary**: In this work, we take an experimentally grounded look at neural network optimization. Building on the Shampoo family of algorithms, we identify and alleviate three key issues, resulting in the proposed SPlus method. First, we find that naive Shampoo is prone to divergence when matrix-inverses are cached for long periods. We introduce an alternate bounded update combining a historical eigenbasis with instantaneous normalization, resulting in across-the-board stability and significantly lower computational requirements. Second, we adapt a shape-aware scaling to enable learning rate transfer across network width. Third, we find that high learning rates result in large parameter noise, and propose a simple iterate-averaging scheme which unblocks faster learning. To properly confirm these findings, we introduce a pointed Transformer training benchmark, considering three objectives (language modelling, image classification, and diffusion modelling) across different stages of training. On average, SPlus is able to reach the validation performance of Adam within 44% of the gradient steps and 62% of the wallclock time.

**Link**: [arxiv](http://arxiv.org/abs/2506.07254v2),  [pdf](http://arxiv.org/pdf/2506.07254v2)

**Tags**: cs.LG 



### MagCache: Fast Video Generation with Magnitude-Aware Cache
**Authors**: Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, Qi Tian

**Updated**: 2025-06-10T17:59:02Z

**Summary**: Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically and steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.1x and 2.68x speedups on Open-Sora and Wan 2.1, respectively, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under comparable computational budgets.

**Link**: [arxiv](http://arxiv.org/abs/2506.09045v1),  [pdf](http://arxiv.org/pdf/2506.09045v1)

**Tags**: cs.CV 



### STI-SNN: A 0.14 GOPS/W/PE Single-Timestep Inference FPGA-based SNN   Accelerator with Algorithm and Hardware Co-Design
**Authors**: Kainan Wang, Chengyi Yang, Chengting Yu, Yee Sin Ang, Bo Wang, Aili Wang

**Updated**: 2025-06-10T14:29:02Z

**Summary**: Brain-inspired Spiking Neural Networks (SNNs) have attracted attention for their event-driven characteristics and high energy efficiency. However, the temporal dependency and irregularity of spikes present significant challenges for hardware parallel processing and data reuse, leading to some existing accelerators falling short in processing latency and energy efficiency. To overcome these challenges, we introduce the STI-SNN accelerator, designed for resource-constrained applications with high energy efficiency, flexibility, and low latency. The accelerator is designed through algorithm and hardware co-design. Firstly, STI-SNN can perform inference in a single timestep. At the algorithm level, we introduce a temporal pruning approach based on the temporal efficient training (TET) loss function. This approach alleviates spike disappearance during timestep reduction, maintains inference accuracy, and expands TET's application. In hardware design, we analyze data access patterns and adopt the output stationary (OS) dataflow, eliminating the need to store membrane potentials and access memory operations. Furthermore, based on the OS dataflow, we propose a compressed and sorted representation of spikes, then cached in the line buffer to reduce the memory access cost and improve reuse efficiency. Secondly, STI-SNN supports different convolution methods. By adjusting the computation mode of processing elements (PEs) and parameterizing the computation array, STI-SNN can accommodate lightweight models based on depthwise separable convolutions (DSCs), further enhancing hardware flexibility. Lastly, STI-SNN also supports both inter-layer and intra-layer parallel processing. For inter-layer parallelism, we ...

**Link**: [arxiv](http://arxiv.org/abs/2506.08842v1),  [pdf](http://arxiv.org/pdf/2506.08842v1)

**Tags**: cs.AR 



### Activated LoRA: Fine-tuned LLMs for Intrinsics
**Authors**: Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox

**Updated**: 2025-06-10T13:50:34Z

**Summary**: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), an adapter architecture which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We train a set of aLoRA-based intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits. The codebase is at https://github.com/IBM/activated-lora.

**Link**: [arxiv](http://arxiv.org/abs/2504.12397v4),  [pdf](http://arxiv.org/pdf/2504.12397v4)

**Tags**: cs.LG cs.AI 



### LiftVSR: Lifting Image Diffusion to Video Super-Resolution via Hybrid   Temporal Modeling with Only 4$\times$RTX 4090s
**Authors**: Xijun Wang, Xin Li, Bingchen Li, Zhibo Chen

**Updated**: 2025-06-10T07:49:33Z

**Summary**: Diffusion models have significantly advanced video super-resolution (VSR) by enhancing perceptual quality, largely through elaborately designed temporal modeling to ensure inter-frame consistency. However, existing methods usually suffer from limited temporal coherence and prohibitively high computational costs (e.g., typically requiring over 8 NVIDIA A100-80G GPUs), especially for long videos. In this work, we propose LiftVSR, an efficient VSR framework that leverages and elevates the image-wise diffusion prior from PixArt-$\alpha$, achieving state-of-the-art results using only 4$\times$RTX 4090 GPUs. To balance long-term consistency and efficiency, we introduce a hybrid temporal modeling mechanism that decomposes temporal learning into two complementary components: (i) Dynamic Temporal Attention (DTA) for fine-grained temporal modeling within short frame segment ($\textit{i.e.}$, low complexity), and (ii) Attention Memory Cache (AMC) for long-term temporal modeling across segments ($\textit{i.e.}$, consistency). Specifically, DTA identifies multiple token flows across frames within multi-head query and key tokens to warp inter-frame contexts in the value tokens. AMC adaptively aggregates historical segment information via a cache unit, ensuring long-term coherence with minimal overhead. To further stabilize the cache interaction during inference, we introduce an asymmetric sampling strategy that mitigates feature mismatches arising from different diffusion sampling steps. Extensive experiments on several typical VSR benchmarks have demonstrated that LiftVSR achieves impressive performance with significantly lower computational costs.

**Link**: [arxiv](http://arxiv.org/abs/2506.08529v1),  [pdf](http://arxiv.org/pdf/2506.08529v1)

**Tags**: cs.CV 



### Draft-based Approximate Inference for LLMs
**Authors**: Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee

**Updated**: 2025-06-10T02:37:46Z

**Summary**: Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, which leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. To the best of our knowledge, this is the first work to use draft models for approximate LLM inference acceleration, extending their utility beyond traditional lossless speculative decoding. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.

**Link**: [arxiv](http://arxiv.org/abs/2506.08373v1),  [pdf](http://arxiv.org/pdf/2506.08373v1)

**Tags**: cs.CL cs.AI 



### GATE: Geometry-Aware Trained Encoding
**Authors**: Jakub Bok≈°ansk√Ω, Daniel Meister, Carsten Benthin

**Updated**: 2025-06-09T19:13:16Z

**Summary**: The encoding of input parameters is one of the fundamental building blocks of neural network algorithms. Its goal is to map the input data to a higher-dimensional space, typically supported by trained feature vectors. The mapping is crucial for the efficiency and approximation quality of neural networks. We propose a novel geometry-aware encoding called GATE that stores feature vectors on the surface of triangular meshes. Our encoding is suitable for neural rendering-related algorithms, for example, neural radiance caching. It also avoids limitations of previous hash-based encoding schemes, such as hash collisions, selection of resolution versus scene size, and divergent memory access. Our approach decouples feature vector density from geometry density using mesh colors, while allowing for finer control over neural network training and adaptive level-of-detail.

**Link**: [arxiv](http://arxiv.org/abs/2506.08161v1),  [pdf](http://arxiv.org/pdf/2506.08161v1)

**Tags**: cs.GR 



### Self Forcing: Bridging the Train-Test Gap in Autoregressive Video   Diffusion
**Authors**: Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, Eli Shechtman

**Updated**: 2025-06-09T17:59:55Z

**Summary**: We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models. Project website: http://self-forcing.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2506.08009v1),  [pdf](http://arxiv.org/pdf/2506.08009v1)

**Tags**: cs.CV cs.AI cs.LG 



### DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal   Performance
**Authors**: Xuanfan Ni, Liyan Xu, Chenyang Lyu, Longyue Wang, Mo Yu, Lemao Liu, Fandong Meng, Jie Zhou, Piji Li

**Updated**: 2025-06-09T15:31:53Z

**Summary**: To alleviate memory burden during inference of large language models (LLMs), numerous studies have focused on compressing the KV cache by exploring aspects such as attention sparsity. These techniques are often designed with a pre-defined KV budget; however, as the optimal budget varies by different input lengths and task types, the existence of a fixed budget could result in inconsistent performance accepting inputs of diverse domains. To address this limitation, we propose a new KV cache compression objective: to always ensure the full-cache performance regardless of specific inputs, while maximizing KV cache pruning as much as possible. To achieve this goal, we introduce a novel KV cache compression method dubbed DBudgetKV, which features an attention-based metric to signal when the remaining KV cache is unlikely to match the full-cache performance, then halting the pruning process. Empirical evaluation spanning diverse context lengths, task types, and model sizes suggests that our method achieves lossless KV pruning effectively and robustly, exceeding 25% compression ratio on average. Furthermore, our method is easy to integrate within LLM inference, not only optimizing memory space, but also showing reduced inference time compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2502.16886v2),  [pdf](http://arxiv.org/pdf/2502.16886v2)

**Tags**: cs.CL cs.AI 



### $d$-Wave Flat Fermi Surface in Altermagnets Enables Maximum   Charge-to-Spin Conversion
**Authors**: Junwen Lai, Tianye Yu, Peitao Liu, Long Liu, Guozhong Xing, Xing-Qiu Chen, Yan Sun

**Updated**: 2025-06-09T12:41:31Z

**Summary**: Altermagnets combine antiferromagnetic order with ferromagnet-like spin splitting, a duality that unlocks ultrafast spin-dependent responses. This unique property creates unprecedented opportunities for spin-current generation, overcoming the intrinsic limitations of conventional spin-transfer and spin-orbit torque approaches in magnetic memory technologies. Here, we establish a fundamental relationship between Fermi surface geometry and time-reversal-odd ($\mathcal{T}$-odd) spin currents in altermagnets through combined model analysis and first-principles calculations. We demonstrate that a $d$-wave altermagnet with a flat Fermi surface can achieve a theoretical upper limit of charge-to-spin conversion efficiency (CSE) of 100%. This mechanism is realized in the newly discovered room-temperature altermagnetic metal KV$_2$O$_2$Se, which exhibits a CSE of $\sim$78% at the charge neutrality point, nearly double that of RuO$_2$, setting a new record for $\mathcal{T}$-odd CSE. Under electron doping, this efficiency further increases to $\sim$98%, approaching the theoretical limit. Our work advances the fundamental understanding of $\mathcal{T}$-odd spin currents via Fermi surface geometry engineering and provides key insights for developing next-generation altermagnet-based memory devices.

**Link**: [arxiv](http://arxiv.org/abs/2506.07703v1),  [pdf](http://arxiv.org/pdf/2506.07703v1)

**Tags**: cond-mat.mtrl-sci 



### Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse
**Authors**: Zhekai Duan, Yuan Zhang, Shikai Geng, Gaowen Liu, Joschka Boedecker, Chris Xiaoxuan Lu

**Updated**: 2025-06-09T11:04:13Z

**Summary**: Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action (VLA) models by improving performance and interpretability through intermediate reasoning steps. However, its sequential autoregressive token generation introduces significant inference latency, limiting real-time deployment. We propose Fast ECoT, an inference-time acceleration method that exploits the structured and repetitive nature of ECoT to (1) cache and reuse high-level reasoning across timesteps and (2) parallelise the generation of modular reasoning steps. Additionally, we introduce an asynchronous scheduler that decouples reasoning from action decoding, further boosting responsiveness. Fast ECoT requires no model changes or additional training and integrates easily into existing VLA pipelines. Experiments in both simulation (LIBERO) and real-world robot tasks show up to a 7.5% reduction in latency with comparable or improved task success rate and reasoning faithfulness, bringing ECoT policies closer to practical real-time deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.07639v1),  [pdf](http://arxiv.org/pdf/2506.07639v1)

**Tags**: cs.RO 



### ParallelComp: Parallel Long-Context Compressor for Length Extrapolation
**Authors**: Jing Xiong, Jianghan Shen, Chuanyang Zheng, Zhongwei Wan, Chenyang Zhao, Chiwun Yang, Fanghua Ye, Hongxia Yang, Lingpeng Kong, Ngai Wong

**Updated**: 2025-06-09T09:48:43Z

**Summary**: Extrapolating ultra-long contexts (text length >128K) remains a major challenge for large language models (LLMs), as most training-free extrapolation methods are not only severely limited by memory bottlenecks, but also suffer from the attention sink, which restricts their scalability and effectiveness in practice. In this work, we propose ParallelComp, a parallel long-context compression method that effectively overcomes the memory bottleneck, enabling 8B-parameter LLMs to extrapolate from 8K to 128K tokens on a single A100 80GB GPU in a training-free setting. ParallelComp splits the input into chunks, dynamically evicting redundant chunks and irrelevant tokens, supported by a parallel KV cache eviction mechanism. Importantly, we present a systematic theoretical and empirical analysis of attention biases in parallel attention-including the attention sink, recency bias, and middle bias-and reveal that these biases exhibit distinctive patterns under ultra-long context settings. We further design a KV cache eviction technique to mitigate this phenomenon. Experimental results show that ParallelComp enables an 8B model (trained on 8K context) to achieve 91.17% of GPT-4's performance under ultra-long contexts, outperforming closed-source models such as Claude-2 and Kimi-Chat. We achieve a 1.76x improvement in chunk throughput, thereby achieving a 23.50x acceleration in the prefill stage with negligible performance loss and pave the way for scalable and robust ultra-long contexts extrapolation in LLMs. We release the code at https://github.com/menik1126/ParallelComp.

**Link**: [arxiv](http://arxiv.org/abs/2502.14317v2),  [pdf](http://arxiv.org/pdf/2502.14317v2)

**Tags**: cs.CL 



### MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via   Mixture of Quantization-Aware Experts
**Authors**: Wei Tao, Haocheng Lu, Xiaoyang Qu, Bin Zhang, Kai Lu, Jiguang Wan, Jianzong Wang

**Updated**: 2025-06-09T08:16:24Z

**Summary**: One of the primary challenges in optimizing large language models (LLMs) for long-context inference lies in the high memory consumption of the Key-Value (KV) cache. Existing approaches, such as quantization, have demonstrated promising results in reducing memory usage. However, current quantization methods cannot take both effectiveness and efficiency into account. In this paper, we propose MoQAE, a novel mixed-precision quantization method via mixture of quantization-aware experts. First, we view different quantization bit-width configurations as experts and use the traditional mixture of experts (MoE) method to select the optimal configuration. To avoid the inefficiency caused by inputting tokens one by one into the router in the traditional MoE method, we input the tokens into the router chunk by chunk. Second, we design a lightweight router-only fine-tuning process to train MoQAE with a comprehensive loss to learn the trade-off between model accuracy and memory usage. Finally, we introduce a routing freezing (RF) and a routing sharing (RS) mechanism to further reduce the inference overhead. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms state-of-the-art KV cache quantization approaches in both efficiency and effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2506.07533v1),  [pdf](http://arxiv.org/pdf/2506.07533v1)

**Tags**: cs.CV 



### LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas   and Ad-Hoc Networks
**Authors**: Atonu Ghosh, Sudip Misra

**Updated**: 2025-06-09T07:58:19Z

**Summary**: The minimal infrastructure requirements of LoRa make it suitable for deployments in remote and disaster-stricken areas. Concomitantly, the modern era is witnessing the proliferation of web applications in all aspects of human life, including IoT and other network services. Contemporary IoT and network solutions heavily rely on web applications to render services. However, despite the recent research and development pivoted around LoRa, there is still a lack of studies focusing on web application access over LoRa networks. Specifically, technical challenges like payload size limitation, low data rate, and contentions in multi-user setups limit the applicability of LoRa for web applications. Hence, we propose LoRaWeb, which enables web access over LoRa networks. The LoRaWeb hardware tethers a WiFi hotspot to which the client devices connect and access the web pages using a web browser. LoRa backbone of the network handles the web page transmission from the requester and receiver devices. LoRaWeb implements a synchronization procedure to address the aforementioned challenges for effective message exchange between requesters and responders. The system implements a caching mechanism to reduce latency and contention. Additionally, it implements a message-slicing mechanism in the application layer to overcome the hardware limitations on the message length. The actual hardware-based implementation results indicate seamless deployment, and the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and ~$6 S$ for a $10 KB$ size web page.

**Link**: [arxiv](http://arxiv.org/abs/2501.02469v2),  [pdf](http://arxiv.org/pdf/2501.02469v2)

**Tags**: cs.NI cs.CY cs.SY eess.SY 



### Graph-KV: Breaking Sequence via Injecting Structural Biases into Large   Language Models
**Authors**: Haoyu Wang, Peihao Wang, Mufei Li, Shikun Liu, Siqi Miao, Zhangyang Wang, Pan Li

**Updated**: 2025-06-09T00:30:08Z

**Summary**: Modern large language models (LLMs) are inherently auto-regressive, requiring input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model's ability to leverage structural inductive biases, especially in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures, where inter-segment dependencies are crucial. We introduce Graph-KV with the potential to overcome this limitation. Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. In this framework, 'target' segments selectively attend only to the KV-caches of their designated 'source' segments, rather than all preceding segments in a serialized sequence. This approach induces a graph-structured block mask, sparsifying attention and enabling a message-passing-like step within the LLM. Furthermore, strategically allocated positional encodings for source and target segments reduce positional bias and context window consumption. We evaluate Graph-KV across three scenarios: (1) seven RAG benchmarks spanning direct inference, multi-hop reasoning, and long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with full-text scientific papers structured as citation ego-graphs; and (3) paper topic classification within a citation network. By effectively reducing positional bias and harnessing structural inductive biases, Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings. Code and the Graph-KV data are publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2506.07334v1),  [pdf](http://arxiv.org/pdf/2506.07334v1)

**Tags**: cs.LG 



### Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency   in Deployed Inference
**Authors**: Thomas Joshi, Herman Saini, Neil Dhillon, Antoni Viros i Martin, Kaoutar El Maghraoui

**Updated**: 2025-06-08T22:59:20Z

**Summary**: Large Language Models (LLMs) encounter severe memory inefficiencies during long-context inference due to conventional handling of key-value (KV) caches. In this work, we introduce a novel integration of PagedAttention with PyTorch's FlexAttention, addressing internal fragmentation and inefficiencies associated with monolithic KV cache allocations. Implemented within IBM's Foundation Model Stack (FMS), our fused attention kernel efficiently gathers scattered KV data. Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced inference latency, growing only linearly (~2x) with sequence length from 128 to 2048 tokens when utilizing a global KV cache, compared to exponential latency increases without caching. While peak memory usage remains largely unchanged for single-step evaluations (dominated by model weights and activations), paged attention causes minimal incremental memory usage, observable only at sequence lengths exceeding 2048 tokens due to its power-of-two cache allocations. We open-source the full implementation and discuss its implications for future long-context model deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.07311v1),  [pdf](http://arxiv.org/pdf/2506.07311v1)

**Tags**: cs.LG cs.AI 



### MiniKV: Pushing the Limits of LLM Inference via 2-Bit   Layer-Discriminative KV Cache
**Authors**: Akshat Sharma, Hangliang Ding, Jianping Li, Neel Dani, Minjia Zhang

**Updated**: 2025-06-08T21:23:22Z

**Summary**: How to efficiently serve LLMs in practice has become exceptionally challenging due to their prohibitive memory and computation requirements. In this study, we investigate optimizing the KV cache, whose memory footprint poses a critical bottleneck in LLM inference, especially when dealing with long context tasks. To tackle the challenge, we introduce MiniKV, a KV cache optimization method that simultaneously preserves long context task accuracy while significantly reducing KV cache size via a novel 2-bit layer-discriminative KV cache. More importantly, we develop specialized CUDA kernels to make MiniKV compatible with FlashAttention. Experiments on a wide range of long context tasks show that MiniKV effectively achieves 86% KV cache compression ratio while recovering over 98.5% of accuracy, outperforming state-of-the-art methods while achieving excellent measured system performance improvements.

**Link**: [arxiv](http://arxiv.org/abs/2411.18077v3),  [pdf](http://arxiv.org/pdf/2411.18077v3)

**Tags**: cs.CL cs.LG 



### FDC: Fast KV Dimensionality Compression for Efficient LLM Inference
**Authors**: Zeyu Zhang, Haiying Shen

**Updated**: 2025-06-08T20:04:17Z

**Summary**: In large-language models, memory constraints in the Key-Value Cache (KVC) pose a challenge during inference. In this work, we propose FDC, a fast KV dimensionality compression system that eliminates the decompression overhead incurred in the existing KV dimensionality compression system, Palu, and reduces attention time. Moreover, FDC employs adaptive compression, tailoring KV compression rates across heads and layers based on their contributions to inference to maximize overall compression while maintaining an accuracy loss constraint. Additionally, FDC enhances the attention kernel to balance the uneven workloads caused by the adaptive compression approach to further reduce attention computation latency. Comprehensive experiments demonstrate that compared to Palu, FDC can reduce Job Completion Time (JCT) by up to 64%, and delivers up to 1.97X throughput under the same latency, while maintaining 99% of the accuracy without compression. When state-of-the-art eviction and quantization methods are combined with FDC, they exhibit similar improvements compared to those combined with Palu. We open-sourced the code.

**Link**: [arxiv](http://arxiv.org/abs/2408.04107v3),  [pdf](http://arxiv.org/pdf/2408.04107v3)

**Tags**: cs.LG cs.DC 



### RevaMp3D: Architecting the Processor Core and Cache Hierarchy for   Systems with Monolithically-Integrated Logic and Memory
**Authors**: Nika Mansouri Ghiasi, Mohammad Sadrosadati, Geraldo F. Oliveira, Konstantinos Kanellopoulos, Rachata Ausavarungnirun, Juan G√≥mez Luna, Jo√£o Ferreira, Jeremie S. Kim, Christina Giannoula, Nandita Vijaykumar, Jisung Park, Onur Mutlu

**Updated**: 2025-06-08T16:07:44Z

**Summary**: Recent nano-technological advances enable the Monolithic 3D (M3D) integration of multiple memory and logic layers in a single chip, allowing for fine-grained connections between layers and significantly alleviating main memory bottlenecks. We show for a variety of workloads, on a state-of-the-art M3D-based system, that the performance and energy bottlenecks shift from main memory to the processor core and cache hierarchy. Therefore, there is a need to revisit current designs that have been conventionally tailored to tackle the memory bottleneck. Based on the insights from our design space exploration, we propose RevaMp3D, introducing five key changes. First, we propose removing the shared last-level cache, as this delivers speedups comparable to or exceeding those from increasing its size or reducing its latency across all workloads. Second, since improving L1 cache latency has a large impact on performance, we reduce L1 latency by leveraging an M3D layout to shorten its wires. Third, we repurpose the area from the removed cache to widen and scale up pipeline structures, accommodating more in-flight requests that are efficiently served by M3D memory. To avoid latency penalties from these larger structures, we leverage M3D layouts. Fourth, to facilitate high thread-level parallelism, we propose a new fine-grained synchronization technique, using M3D's dense inter-layer connectivity. Fifth, we leverage the M3D main memory to mitigate the core bottlenecks. We propose a processor frontend design that memoizes the repetitive fetched, decoded, and reordered instructions, stores them in main memory, and turns off the relevant parts of the core when possible. RevaMp3D provides 1.2x-2.9x speedup and 1.2x-1.4x energy reduction compared to a state-of-the-art M3D system. We also analyze RevaMp3D's design decisions across various memory latencies to facilitate latency-aware design decisions.

**Link**: [arxiv](http://arxiv.org/abs/2210.08508v2),  [pdf](http://arxiv.org/pdf/2210.08508v2)

**Tags**: cs.AR cs.DC 



### Value Residual Learning
**Authors**: Zhanchao Zhou, Tianyi Wu, Zhiyun Jiang, Fares Obeid, Zhenzhong Lan

**Updated**: 2025-06-08T16:04:59Z

**Summary**: While Transformer models have achieved remarkable success in various domains, the effectiveness of information propagation through deep networks remains a critical challenge. Standard hidden state residuals often fail to adequately preserve initial token-level information in deeper layers. This paper introduces ResFormer, a novel architecture that enhances information flow by incorporating value residual connections in addition to hidden state residuals. And a variant is SVFormer, where all layers share the first layer's value embedding. Comprehensive empirical evidence demonstrates ResFormer achieves equivalent validation loss with 16.11\% fewer model parameters and 20.3\% less training data compared to Transformer, while maintaining similar memory usage and computational cost. Besides, SVFormer reduces KV cache size by nearly half with only a small performance penalty and can be integrated with other KV-efficient methods, yielding further reductions in KV cache, with performance influenced by sequence length and cumulative learning rate.

**Link**: [arxiv](http://arxiv.org/abs/2410.17897v5),  [pdf](http://arxiv.org/pdf/2410.17897v5)

**Tags**: cs.CL 



## Keyword: LLM Inference 
 ### FilMaster: Bridging Cinematic Principles and Generative AI for Automated   Film Generation
**Authors**: Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, Xihui Liu

**Updated**: 2025-06-23T17:59:16Z

**Summary**: AI-driven content creation has shown potential in film production. However, existing film generation systems struggle to implement cinematic principles and thus fail to generate professional-quality films, particularly lacking diverse camera language and cinematic rhythm. This results in templated visuals and unengaging narratives. To address this, we introduce FilMaster, an end-to-end AI system that integrates real-world cinematic principles for professional-grade film generation, yielding editable, industry-standard outputs. FilMaster is built on two key principles: (1) learning cinematography from extensive real-world film data and (2) emulating professional, audience-centric post-production workflows. Inspired by these principles, FilMaster incorporates two stages: a Reference-Guided Generation Stage which transforms user input to video clips, and a Generative Post-Production Stage which transforms raw footage into audiovisual outputs by orchestrating visual and auditory elements for cinematic rhythm. Our generation stage highlights a Multi-shot Synergized RAG Camera Language Design module to guide the AI in generating professional camera language by retrieving reference clips from a vast corpus of 440,000 film clips. Our post-production stage emulates professional workflows by designing an Audience-Centric Cinematic Rhythm Control module, including Rough Cut and Fine Cut processes informed by simulated audience feedback, for effective integration of audiovisual elements to achieve engaging content. The system is empowered by generative AI models like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a comprehensive benchmark for evaluating AI-generated films. Extensive experiments show FilMaster's superior performance in camera language design and cinematic rhythm control, advancing generative AI in professional filmmaking.

**Link**: [arxiv](http://arxiv.org/abs/2506.18899v1),  [pdf](http://arxiv.org/pdf/2506.18899v1)

**Tags**: cs.CV 



### Vision as a Dialect: Unifying Visual Understanding and Generation via   Text-Aligned Representations
**Authors**: Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang

**Updated**: 2025-06-23T17:59:14Z

**Summary**: This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com

**Link**: [arxiv](http://arxiv.org/abs/2506.18898v1),  [pdf](http://arxiv.org/pdf/2506.18898v1)

**Tags**: cs.CV cs.AI cs.CL cs.MM 



### ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs
**Authors**: Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang

**Updated**: 2025-06-23T17:59:02Z

**Summary**: Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux

**Link**: [arxiv](http://arxiv.org/abs/2506.18896v1),  [pdf](http://arxiv.org/pdf/2506.18896v1)

**Tags**: cs.CL 



### Steering Conceptual Bias via Transformer Latent-Subspace Activation
**Authors**: Vansh Sharma, Venkat Raman

**Updated**: 2025-06-23T17:56:34Z

**Summary**: This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (0-6) improving the probe classification accuracy by 61.5% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.18887v1),  [pdf](http://arxiv.org/pdf/2506.18887v1)

**Tags**: cs.AI cs.LG cs.SY eess.SY I.2.7; I.2.6; I.2.1; D.3.3; C.4 



### Probing neutrino mass ordering with supernova neutrinos at NO$ŒΩ$A   including the effect of sterile neutrinos
**Authors**: Papia Panda, Rukmani Mohanta

**Updated**: 2025-06-23T17:54:20Z

**Summary**: In this work, we explore the possibility of probing the mass ordering sensitivity as a function of supernova distance in the context of the ongoing neutrino experiment NO$\nu$A. We provide a detailed study of the active-active and active-sterile mixing frameworks, illustrating how supernova neutrinos can be used to realize the existence of sterile neutrinos. Interestingly, we infer that observation of the NC channel alone can differentiate between the presence and absence of sterile neutrinos. Our results indicate that the primary channel of NO$\nu$A can distinguish normal mass ordering from inverted mass ordering at $5 \sigma$ confidence level for a supernova explosion occurring at a distance of 5 kpc. Additionally, we examine the impact of systematic uncertainties on mass ordering sensitivity, showing that higher levels of systematics lead to a reduction in sensitivity. Similarly, the inclusion of energy smearing significantly diminishes ordering sensitivity.

**Link**: [arxiv](http://arxiv.org/abs/2412.05213v3),  [pdf](http://arxiv.org/pdf/2412.05213v3)

**Tags**: hep-ph 



### Let Your Video Listen to Your Music!
**Authors**: Xinyu Zhang, Dong Gong, Zicheng Duan, Anton van den Hengel, Lingqiao Liu

**Updated**: 2025-06-23T17:52:16Z

**Summary**: Aligning the rhythm of visual motion in a video with a given music track is a practical need in multimedia production, yet remains an underexplored task in autonomous video editing. Effective alignment between motion and musical beats enhances viewer engagement and visual appeal, particularly in music videos, promotional content, and cinematic editing. Existing methods typically depend on labor-intensive manual cutting, speed adjustments, or heuristic-based editing techniques to achieve synchronization. While some generative models handle joint video and music generation, they often entangle the two modalities, limiting flexibility in aligning video to music beats while preserving the full visual content. In this paper, we propose a novel and efficient framework, termed MVAA (Music-Video Auto-Alignment), that automatically edits video to align with the rhythm of a given music track while preserving the original visual content. To enhance flexibility, we modularize the task into a two-step process in our MVAA: aligning motion keyframes with audio beats, followed by rhythm-aware video inpainting. Specifically, we first insert keyframes at timestamps aligned with musical beats, then use a frame-conditioned diffusion model to generate coherent intermediate frames, preserving the original video's semantic content. Since comprehensive test-time training can be time-consuming, we adopt a two-stage strategy: pretraining the inpainting module on a small video set to learn general motion priors, followed by rapid inference-time fine-tuning for video-specific adaptation. This hybrid approach enables adaptation within 10 minutes with one epoch on a single NVIDIA 4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show that our approach can achieve high-quality beat alignment and visual smoothness.

**Link**: [arxiv](http://arxiv.org/abs/2506.18881v1),  [pdf](http://arxiv.org/pdf/2506.18881v1)

**Tags**: cs.CV cs.MM 



### OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory,   Compositional, and Transformative Generalization
**Authors**: Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song

**Updated**: 2025-06-23T17:51:40Z

**Summary**: Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18880v1),  [pdf](http://arxiv.org/pdf/2506.18880v1)

**Tags**: cs.CL cs.AI 



### CommVQ: Commutative Vector Quantization for KV Cache Compression
**Authors**: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

**Updated**: 2025-06-23T17:50:11Z

**Summary**: Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

**Link**: [arxiv](http://arxiv.org/abs/2506.18879v1),  [pdf](http://arxiv.org/pdf/2506.18879v1)

**Tags**: cs.CL cs.AI 



### Direct nonparametric multimessenger constraints on the equation of state   of cold dense nuclear matter
**Authors**: Iuliu Cuceu, Sandra Robles

**Updated**: 2025-06-23T17:48:35Z

**Summary**: We utilize the now substantial amount of astrophysical observations of neutron stars (NSs), along with perturbative quantum chromodynamics (pQCD) calculations at high density, to directly constrain the NS equation of state (EOS). To this end, we construct nonparametric EOS priors by using Gaussian processes trained on 75 EOSs, which include models with either hadrons, hyperons, or quarks at high densities. We create a prior using the full EOS sample (model agnostic), and one prior for each EOS family to test model discrimination. We introduce a novel inference approach, which allows the simultaneous sampling of intrinsic and extrinsic parameters of binary NS mergers, as well as a nonparametric equation of state. We showcase this method in a Bayesian updating scheme by first performing a complete analysis of the binary NS merger event GW170817 with minimal assumptions, and sequentially adding information from x-ray and radio NS observations, along with pQCD calculations. Besides providing standard constraints, such as the pressure at twice nuclear saturation density $p(2\rho_\text{sat})=4.3^{+0.6}_{-0.6}\,\times 10^{34}\text{dyne/cm}^{2}$, at $95\%$ confidence level, for the model agnostic prior, our methodology shows how the choice of EOS families used in conditioning changes the inferred astrophysical properties of the EOS, namely tidal deformability and maximum supported NS mass. We find hyperonic priors predicting higher tidal deformabilities for a $1.4M_\odot$ NS, and hadronic priors being preferred by the considered astrophysical data.

**Link**: [arxiv](http://arxiv.org/abs/2410.23407v2),  [pdf](http://arxiv.org/pdf/2410.23407v2)

**Tags**: astro-ph.HE nucl-th 



### A field-level reaction for screened modified gravity
**Authors**: Daniela Saadeh, Kazuya Koyama, Xan Morice-Atkinson

**Updated**: 2025-06-23T17:48:23Z

**Summary**: We present a field-level reaction framework to emulate the nonlinear effects of screened modified gravity on the cosmic web. This approach is designed to enable field-level inference with data from Stage IV cosmological surveys. Building on the reaction method, which models the nonlinear matter power spectrum in modified gravity as corrections to a "pseudo" $\Lambda$CDM cosmology, we extend the method to full field-level predictions by applying it to the output of $N$-body simulations, including both positions and velocities. We focus on modifications to gravity that are scale-independent at the linear level, allowing us to isolate and emulate nonlinear deviations, particularly screening effects. Our neural network predicts the field-level correction ("reaction") to a pseudo$\Lambda$CDM simulation whose linear clustering matches that of the target. The emulator achieves sub-percent accuracy across a broad range of summary statistics, including 0.4\% agreement in the matter power spectrum at scales $k < 1$ Mpc$/h$, and 2\% accuracy in redshift-space distortion multipoles at $k < 0.3$ Mpc$/h$. We also validate the emulator against $N$-body simulations with increased force resolution and time steps, confirming the robustness of its performance. These results demonstrate that our framework is a practical and reliable tool for incorporating screened modified gravity models into field-level cosmological inference, enabling stringent tests of extra fundamental forces at cosmological scales.

**Link**: [arxiv](http://arxiv.org/abs/2506.18876v1),  [pdf](http://arxiv.org/pdf/2506.18876v1)

**Tags**: astro-ph.CO gr-qc 



### Improved Baselines with Synchronized Encoding for Universal Medical   Image Segmentation
**Authors**: Sihan Yang, Jiadong Feng, Xuande Mi, Haixia Bi, Hai Zhang, Jian Sun

**Updated**: 2025-06-23T17:43:31Z

**Summary**: Large foundation models, known for their strong zero-shot generalization capabilities, can be applied to a wide range of downstream tasks. However, developing foundation models for medical image segmentation poses a significant challenge due to the domain gap between natural and medical images. While fine-tuning techniques based on the Segment Anything Model (SAM) have been explored, they primarily focus on scaling up data or refining inference strategies without incorporating domain-specific architectural designs, limiting their zero-shot performance. To optimize segmentation performance under standard inference settings and provide a strong baseline for future research, we introduce SyncSAM, which employs a synchronized dual-branch encoder that integrates convolution and Transformer features in a synchronized manner to enhance medical image encoding, and a multi-scale dual-branch decoder to preserve image details. SyncSAM is trained on two of the largest medical image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series of pre-trained models for universal medical image segmentation. Experimental results demonstrate that SyncSAM not only achieves state-of-the-art performance on test sets but also exhibits strong zero-shot capabilities on unseen datasets. Code and checkpoints are available at https://github.com/Hhankyangg/SyncSAM.

**Link**: [arxiv](http://arxiv.org/abs/2408.09886v4),  [pdf](http://arxiv.org/pdf/2408.09886v4)

**Tags**: cs.CV 



### Talking to GDELT Through Knowledge Graphs
**Authors**: Audun Myers, Max Vargas, Sinan G. Aksoy, Cliff Joslyn, Benjamin Wilson, Lee Burke, Tom Grimes

**Updated**: 2025-06-24T10:10:56Z

**Summary**: In this work we study various Retrieval Augmented Regeneration (RAG) approaches to gain an understanding of the strengths and weaknesses of each approach in a question-answering analysis. To gain this understanding we use a case-study subset of the Global Database of Events, Language, and Tone (GDELT) dataset as well as a corpus of raw text scraped from the online news articles. To retrieve information from the text corpus we implement a traditional vector store RAG as well as state-of-the-art large language model (LLM) based approaches for automatically constructing KGs and retrieving the relevant subgraphs. In addition to these corpus approaches, we develop a novel ontology-based framework for constructing knowledge graphs (KGs) from GDELT directly which leverages the underlying schema of GDELT to create structured representations of global events. For retrieving relevant information from the ontology-based KGs we implement both direct graph queries and state-of-the-art graph retrieval approaches. We compare the performance of each method in a question-answering task. We find that while our ontology-based KGs are valuable for question-answering, automated extraction of the relevant subgraphs is challenging. Conversely, LLM-generated KGs, while capturing event summaries, often lack consistency and interpretability. Our findings suggest benefits of a synergistic approach between ontology and LLM-based KG construction, with proposed avenues toward that end.

**Link**: [arxiv](http://arxiv.org/abs/2503.07584v3),  [pdf](http://arxiv.org/pdf/2503.07584v3)

**Tags**: cs.IR 



### Amplifying Machine Learning Attacks Through Strategic Compositions
**Authors**: Yugeng Liu, Zheng Li, Hai Huang, Michael Backes, Yang Zhang

**Updated**: 2025-06-23T17:38:48Z

**Summary**: Machine learning (ML) models are proving to be vulnerable to a variety of attacks that allow the adversary to learn sensitive information, cause mispredictions, and more. While these attacks have been extensively studied, current research predominantly focuses on analyzing each attack type individually. In practice, however, adversaries may employ multiple attack strategies simultaneously rather than relying on a single approach. This prompts a crucial yet underexplored question: When the adversary has multiple attacks at their disposal, are they able to mount or amplify the effect of one attack with another? In this paper, we take the first step in studying the strategic interactions among different attacks, which we define as attack compositions. Specifically, we focus on four well-studied attacks during the model's inference phase: adversarial examples, attribute inference, membership inference, and property inference. To facilitate the study of their interactions, we propose a taxonomy based on three stages of the attack pipeline: preparation, execution, and evaluation. Using this taxonomy, we identify four effective attack compositions, such as property inference assisting attribute inference at its preparation level and adversarial examples assisting property inference at its execution level. We conduct extensive experiments on the attack compositions using three ML model architectures and three benchmark image datasets. Empirical results demonstrate the effectiveness of these four attack compositions. We implement and release a modular reusable toolkit, COAT. Arguably, our work serves as a call for researchers and practitioners to consider advanced adversarial settings involving multiple attack strategies, aiming to strengthen the security and robustness of AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.18870v1),  [pdf](http://arxiv.org/pdf/2506.18870v1)

**Tags**: cs.CR 



### CDI: Copyrighted Data Identification in Diffusion Models
**Authors**: Jan Dubi≈Ñski, Antoni Kowalczuk, Franziska Boenisch, Adam Dziedzic

**Updated**: 2025-06-23T17:31:25Z

**Summary**: Diffusion Models (DMs) benefit from large and diverse datasets for their training. Since this data is often scraped from the Internet without permission from the data owners, this raises concerns about copyright and intellectual property protections. While (illicit) use of data is easily detected for training samples perfectly re-created by a DM at inference time, it is much harder for data owners to verify if their data was used for training when the outputs from the suspect DM are not close replicas. Conceptually, membership inference attacks (MIAs), which detect if a given data point was used during training, present themselves as a suitable tool to address this challenge. However, we demonstrate that existing MIAs are not strong enough to reliably determine the membership of individual images in large, state-of-the-art DMs. To overcome this limitation, we propose CDI, a framework for data owners to identify whether their dataset was used to train a given DM. CDI relies on dataset inference techniques, i.e., instead of using the membership signal from a single data point, CDI leverages the fact that most data owners, such as providers of stock photography, visual media companies, or even individual artists, own datasets with multiple publicly exposed data points which might all be included in the training of a given DM. By selectively aggregating signals from existing MIAs and using new handcrafted methods to extract features for these datasets, feeding them to a scoring model, and applying rigorous statistical testing, CDI allows data owners with as little as 70 data points to identify with a confidence of more than 99% whether their data was used to train a given DM. Thereby, CDI represents a valuable tool for data owners to claim illegitimate use of their copyrighted data. We make the code available at https://github.com/sprintml/copyrighted_data_identification

**Link**: [arxiv](http://arxiv.org/abs/2411.12858v3),  [pdf](http://arxiv.org/pdf/2411.12858v3)

**Tags**: cs.LG cs.CR 



### Variational Bayesian Channel Estimation and Data Detection for Cell-Free   Massive MIMO with Low-Resolution Quantized Fronthaul Links
**Authors**: Sajjad Nassirpour, Toan-Van Nguyen, Hien Q. Ngo, Le-Nam Tran, Tharmalingam Ratnarajah, Duy H. N. Nguyen

**Updated**: 2025-06-23T17:26:40Z

**Summary**: We study the joint channel estimation and data detection (JED) problem in a cell-free massive multiple-input multiple-output (CF-mMIMO) network, where access points (APs) communicate with a central processing unit (CPU) over fronthaul links. However, the bandwidth of these links is limited, and thus, presents challenges to the applicability of CF-mMIMO, especially with an ever-increasing number of users. To address this, we propose a method based on variational Bayesian (VB) inference for performing the JED process, where the APs forward low-resolution quantized versions of the signals to the CPU. We consider two approaches: \emph{quantization-and-estimation} (Q-E) and \emph{estimation-and-quantization} (E-Q). In the Q-E approach, each AP uses a low-bit quantizer to quantize the signal before forwarding it to the CPU, while in the E-Q approach, each AP first performs local channel estimation and then sends a low-bit quantized version of the estimated channel to the CPU. We evaluate the performance of our VB-based approach under perfect fronthaul link (PFL) with unquantized received signals, Q-E, and E-Q in terms of symbol error rate (SER), normalized mean square error (NMSE) of the channel estimation, computational complexity, and fronthaul signaling overhead. We also compare these results with those of the linear minimum mean squared error (LMMSE) method under the PFL scenario. Our numerical results show that both the VB(Q-E) and VB(E-Q) approaches achieve superior performance compared to LMMSE(PFL), benefiting from the nonlinear modeling inherent in VB. Furthermore, the VB(Q-E) method outperforms VB(E-Q) due to errors in the local channel estimation process at the APs within the VB(E-Q) approach.

**Link**: [arxiv](http://arxiv.org/abs/2506.18863v1),  [pdf](http://arxiv.org/pdf/2506.18863v1)

**Tags**: eess.SP 



### Bayesian decomposition using Besov priors
**Authors**: Andreas Horst, Babak Maboudi Afkham, Yiqiu Dong, Jakob Lemvig

**Updated**: 2025-06-23T17:07:04Z

**Summary**: In many inverse problems, the unknown is composed of multiple components with different regularities, for example, in imaging problems, where the unknown can have both rough and smooth features. We investigate linear Bayesian inverse problems, where the unknown consists of two components: one smooth and one piecewise constant. We model the unknown as a sum of two components and assign individual priors on each component to impose the assumed behavior. We propose and compare two prior models: (i) a combination of a Haar wavelet-based Besov prior and a smoothing Besov prior, and (ii) a hierarchical Gaussian prior on the gradient coupled with a smoothing Besov prior. To achieve a balanced reconstruction, we place hyperpriors on the prior parameters and jointly infer both the components and the hyperparameters. We propose Gibbs sampling schemes for posterior inference in both prior models. We demonstrate the capabilities of our approach on 1D and 2D deconvolution problems, where the unknown consists of smooth parts with jumps. The numerical results indicate that our methods improve the reconstruction quality compared to single-prior approaches and that the prior parameters can be successfully estimated to yield a balanced decomposition.

**Link**: [arxiv](http://arxiv.org/abs/2506.18846v1),  [pdf](http://arxiv.org/pdf/2506.18846v1)

**Tags**: stat.CO cs.NA math.NA G.3; G.4 



### LIGHTHOUSE: Fast and precise distance to shoreline calculations from   anywhere on earth
**Authors**: Patrick Beukema, Henry Herzog, Yawen Zhang, Hunter Pitelka, Favyen Bastani

**Updated**: 2025-06-23T17:00:34Z

**Summary**: We introduce a new dataset and algorithm for fast and efficient coastal distance calculations from Anywhere on Earth (AoE). Existing global coastal datasets are only available at coarse resolution (e.g. 1-4 km) which limits their utility. Publicly available satellite imagery combined with computer vision enable much higher precision. We provide a global coastline dataset at 10 meter resolution, a 100+ fold improvement in precision over existing data. To handle the computational challenge of querying at such an increased scale, we introduce a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM to achieve millisecond online inference, making it well suited for real-time applications in resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2506.18842v1),  [pdf](http://arxiv.org/pdf/2506.18842v1)

**Tags**: cs.DB cs.CV cs.LG 



### LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement   Learning
**Authors**: Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li

**Updated**: 2025-06-23T16:59:02Z

**Summary**: Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B

**Link**: [arxiv](http://arxiv.org/abs/2506.18841v1),  [pdf](http://arxiv.org/pdf/2506.18841v1)

**Tags**: cs.CL cs.AI cs.LG 



### LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated   Data Generation
**Authors**: Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas

**Updated**: 2025-06-23T16:58:26Z

**Summary**: Large foundation models trained on large-scale vision-language data can boost Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the hand-crafted pipelines often introduce bias and overfit to specific prompts. We sidestep this issue by directly fusing hidden states from Large Language Models (LLMs) into detectors-an avenue surprisingly under-explored. This paper presents a systematic method to enhance visual grounding by utilizing decoder layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention adapter to enable efficient knowledge fusion from LLMs to object detectors, a new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We find that intermediate LLM layers already encode rich spatial semantics; adapting only the early layers yields most of the gain. With Swin-T as the vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to 6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths further corroborate our design.

**Link**: [arxiv](http://arxiv.org/abs/2503.13794v4),  [pdf](http://arxiv.org/pdf/2503.13794v4)

**Tags**: cs.CV cs.AI 



### STU-PID: Steering Token Usage via PID Controller for Efficient Large   Language Model Reasoning
**Authors**: Aryasomayajula Ram Bharadwaj

**Updated**: 2025-06-23T16:47:19Z

**Summary**: Large Language Models employing extended chain-of-thought (CoT) reasoning often suffer from the overthinking phenomenon, generating excessive and redundant reasoning steps that increase computational costs while potentially degrading performance. While recent work has explored static steering approaches to mitigate this issue, they lack the adaptability to dynamically adjust intervention strength based on real-time reasoning quality. We propose STUPID (Steering Token Usage via PID controller), a novel training-free method that employs a PID controller to dynamically modulate activation steering strength during inference. Our approach combines a chunk-level classifier for detecting redundant reasoning patterns with a PID control mechanism that adaptively adjusts steering intensity based on the predicted redundancy probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves a 6% improvement in accuracy while reducing token usage by 32%, outperforming static steering baselines. Our method provides a principled framework for dynamic reasoning calibration that maintains reasoning quality while significantly improving computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18831v1),  [pdf](http://arxiv.org/pdf/2506.18831v1)

**Tags**: cs.CL 



### Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories
**Authors**: Islem Bouzenia, Michael Pradel

**Updated**: 2025-06-23T16:34:52Z

**Summary**: Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: \textsc{RepairAgent}, \textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs into a common format, capturing 120 trajectories and 2822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics such as iteration counts and token consumption, recurring action sequences, and the semantic coherence linking thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents.

**Link**: [arxiv](http://arxiv.org/abs/2506.18824v1),  [pdf](http://arxiv.org/pdf/2506.18824v1)

**Tags**: cs.SE cs.AI 



### A Survey on Data Selection for LLM Instruction Tuning
**Authors**: Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu

**Updated**: 2025-06-23T16:30:46Z

**Summary**: Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.

**Link**: [arxiv](http://arxiv.org/abs/2402.05123v2),  [pdf](http://arxiv.org/pdf/2402.05123v2)

**Tags**: cs.CL 



### RWESummary: A Framework and Test for Choosing Large Language Models to   Summarize Real-World Evidence (RWE) Studies
**Authors**: Arjun Mukerji, Michael L. Jackson, Jason Jones, Neil Sanghavi

**Updated**: 2025-06-23T16:28:03Z

**Summary**: Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.

**Link**: [arxiv](http://arxiv.org/abs/2506.18819v1),  [pdf](http://arxiv.org/pdf/2506.18819v1)

**Tags**: cs.CL cs.AI 



### Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large   Language Models
**Authors**: Aradhye Agarwal, Suhas K Ramesh, Ayan Sengupta, Tanmoy Chakraborty

**Updated**: 2025-06-23T16:25:27Z

**Summary**: Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. Selective PEFT, a class of parameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these computational challenges by selectively fine-tuning only a small fraction of the model parameters. Although parameter-efficient, these techniques often fail to match the performance of fully fine-tuned models, primarily due to inherent biases introduced during parameter selection. Traditional selective PEFT techniques use a fixed set of parameters selected using different importance heuristics, failing to capture parameter importance dynamically and often leading to suboptimal performance. We introduce $\text{ID}^3$, a novel selective PEFT method that calculates parameter importance continually, and dynamically unmasks parameters by balancing exploration and exploitation in parameter selection. Our empirical study on 16 tasks spanning natural language understanding, mathematical reasoning and summarization demonstrates the effectiveness of our method compared to fixed-masking selective PEFT techniques. We analytically show that $\text{ID}^3$ reduces the number of gradient updates by a factor of two, enhancing computational efficiency. Since $\text{ID}^3$ is robust to random initialization of neurons and operates directly on the optimization process, it is highly flexible and can be integrated with existing additive and reparametrization-based PEFT techniques such as adapters and LoRA respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.14470v3),  [pdf](http://arxiv.org/pdf/2408.14470v3)

**Tags**: cs.CL 



### A Practical Introduction to Regression-based Causal Inference in   Meteorology (I): All confounders measured
**Authors**: Caren Marzban, Yikun Zhang, Nicholas Bond, Michael Richman

**Updated**: 2025-06-23T16:19:02Z

**Summary**: Whether a variable is the cause of another, or simply associated with it, is often an important scientific question. Causal Inference is the name associated with the body of techniques for addressing that question in a statistical setting. Although assessing causality is relatively straightforward in the presence of temporal information, outside of that setting - the situation considered here - it is more difficult to assess causal effects. The development of the field of causal inference has involved concepts from a wide range of topics, thereby limiting its adoption across some fields, including meteorology. However, at its core, the requisite knowledge for causal inference involves little more than basic probability theory and regression, topics familiar to most meteorologists. By focusing on these core areas, this and a companion article provide a steppingstone for the meteorology community into the field of (non-temporal) causal inference. Although some theoretical foundations are presented, the main goal is the application of a specific method, called matching, to a problem in meteorology. The data for the application are in public domain, and R code is provided as well, forming an easy path for meteorology students and researchers to enter the field.

**Link**: [arxiv](http://arxiv.org/abs/2506.18808v1),  [pdf](http://arxiv.org/pdf/2506.18808v1)

**Tags**: stat.AP 



### Episodic mass loss in the very luminous red supergiant [W60] B90 in the   Large Magellanic Cloud
**Authors**: G. Munoz-Sanchez, S. de Wit, A. Z. Bonanos, K. Antoniadis, K. Boutsia, P. Boumis, E. Christodoulou, M. Kalitsounaki, A. Udalski

**Updated**: 2025-06-23T16:14:12Z

**Summary**: This study delves into [W60] B90, one of the most luminous and extreme Red Supergiants (RSGs) in the Large Magellanic Cloud (LMC), aiming to search for evidence of episodic mass loss. Our discovery of a bar-like nebular structure at 1 pc, reminiscent of the bar around Betelgeuse, raised the question of whether [W60] B90 also has a bow shock. We collected and analyzed proper motion data from Gaia, as well as new multi-epoch spectroscopic and imaging data, and archival time-series photometry in the optical and mid-infrared. We found [W60] B90 to be a walkaway star, with a supersonic peculiar velocity in the direction of the bar. We detected shocked emission between the bar and the star, based on the [S II]/H$\alpha$ > 0.4 criterion, providing strong evidence for a bow shock. The 30-year optical light curve revealed semi-regular variability, showing three similar dimming events with $\Delta V \sim 1$ mag, a recurrence of $\sim$12 yr, and a rise time of 400 d. We found the mid-IR light curve to vary by 0.51 mag and 0.37 mag in the WISE1 and WISE2 bands, respectively, and by 0.42 mag and 0.25 mag during the last dimming event. During this event, optical spectroscopy revealed spectral variability (M3I to M4I), a correlation between the $T_{\rm eff}$ and the brightness, increased extinction, and, after the minimum, spectral features incompatible with the models. We also found a difference of >300 K between the $T_{\rm eff}$ measured from the TiO bands in the optical and the atomic lines from our $J$-band spectroscopy. We inferred that [W60] B90 is a more massive analog of Betelgeuse in the LMC and the first extragalactic single RSG with a suspected bow shock. Its high luminosity $\log(L/L_{\odot})=5.32$, mass-loss rate, and mid-IR variability compared to other RSGs in the LMC, indicate that it is in an unstable evolutionary state undergoing episodes of mass loss.

**Link**: [arxiv](http://arxiv.org/abs/2405.11019v2),  [pdf](http://arxiv.org/pdf/2405.11019v2)

**Tags**: astro-ph.SR astro-ph.GA 



### The Simons Observatory: Validation of reconstructed power spectra from   simulated filtered maps for the Small Aperture Telescope survey
**Authors**: Carlos Herv√≠as-Caimapo, Kevin Wolz, Adrien La Posta, Susanna Azzoni, David Alonso, Kam Arnold, Carlo Baccigalupi, Simon Biquard, Michael L. Brown, Erminia Calabrese, Yuji Chinone, Samuel Day-Weiss, Jo Dunkley, Rolando D√ºnner, Josquin Errard, Giulio Fabbian, Ken Ganga, Serena Giardiello, Emilie Hertig, Kevin M. Huffenberger, Bradley R. Johnson, Baptiste Jost, Reijo Keskitalo, Theodore S. Kisner, Thibaut Louis, Magdy Morshed, Lyman A. Page, Christian L. Reichardt, Erik Rosenberg, Max Silva-Feaver, Wuhyun Sohn, Yoshinori Sueno, Dan B. Thomas, Ema Tsang King Sang, Amalia Villarrubia-Aguilar, Kyohei Yamada

**Updated**: 2025-06-23T16:07:38Z

**Summary**: We present a transfer function-based method to estimate angular power spectra from filtered maps for cosmic microwave background (CMB) surveys. This is especially relevant for experiments targeting the faint primordial gravitational wave signatures in CMB polarisation at large scales, such as the Simons Observatory (SO) small aperture telescopes. While timestreams can be filtered to mitigate the contamination from low-frequency noise, usual methods that calculate the mode coupling at individual multipoles can be challenging for experiments covering large sky areas or reaching few-arcminute resolution. The method we present here, although approximate, is more practical and faster for larger data volumes. We validate it through the use of simulated observations approximating the first year of SO data, going from half-wave plate-modulated timestreams to maps, and using simulations to estimate the mixing of polarisation modes induced by an example of time-domain filtering. We show its performance through an example null test and with an end-to-end pipeline that performs inference on cosmological parameters, including the tensor-to-scalar ratio $r$. The performance demonstration uses simulated observations at multiple frequency bands. We find that the method can recover unbiased parameters for our simulated noise levels.

**Link**: [arxiv](http://arxiv.org/abs/2502.00946v2),  [pdf](http://arxiv.org/pdf/2502.00946v2)

**Tags**: astro-ph.CO 



### OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by   Object-Centric Awareness
**Authors**: Helin Cao, Sven Behnke

**Updated**: 2025-06-23T16:03:53Z

**Summary**: Autonomous driving perception faces significant challenges due to occlusions and incomplete scene data in the environment. To overcome these issues, the task of semantic occupancy prediction (SOP) is proposed, which aims to jointly infer both the geometry and semantic labels of a scene from images. However, conventional camera-based methods typically treat all categories equally and primarily rely on local features, leading to suboptimal predictions, especially for dynamic foreground objects. To address this, we propose Object-Centric SOP (OC-SOP), a framework that integrates high-level object-centric cues extracted via a detection branch into the semantic occupancy prediction pipeline. This object-centric integration significantly enhances the prediction accuracy for foreground objects and achieves state-of-the-art performance among all categories on SemanticKITTI.

**Link**: [arxiv](http://arxiv.org/abs/2506.18798v1),  [pdf](http://arxiv.org/pdf/2506.18798v1)

**Tags**: cs.CV cs.AI cs.RO 



### FORGE: An LLM-driven Framework for Large-Scale Smart Contract   Vulnerability Dataset Construction
**Authors**: Jiachi Chen, Yiming Shen, Jiashuo Zhang, Zihao Li, John Grundy, Zhenzhe Shao, Yanlin Wang, Jiashui Wang, Ting Chen, Zibin Zheng

**Updated**: 2025-06-23T16:03:16Z

**Summary**: High-quality smart contract vulnerability datasets are critical for evaluating security tools and advancing smart contract security research. Two major limitations of current manual dataset construction are (1) labor-intensive and error-prone annotation processes limiting the scale, quality, and evolution of the dataset, and (2) absence of standardized classification rules results in inconsistent vulnerability categories and labeling results across different datasets. To address these limitations, we present FORGE, the first automated approach for constructing smart contract vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract high-quality vulnerabilities from real-world audit reports and classify them according to the CWE, the most widely recognized classification in software security. FORGE employs a divide-and-conquer strategy to extract structured and self-contained vulnerability information from these reports. Additionally, it uses a tree-of-thoughts technique to classify the vulnerability information into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we run FORGE on 6,454 real-world audit reports and generate a dataset comprising 81,390 solidity files and 27,497 vulnerability findings across 296 CWE categories. Manual assessment of the dataset demonstrates high extraction precision and classification consistency with human experts (precision of 95.6% and inter-rater agreement k-$\alpha$ of 0.87). We further validate the practicality of our dataset by benchmarking 13 existing security tools on our dataset. The results reveal the significant limitations in current detection capabilities. Furthermore, by analyzing the severity-frequency distribution patterns through a unified CWE perspective in our dataset, we highlight inconsistency between current smart contract research focus and priorities identified from real-world vulnerabilities...

**Link**: [arxiv](http://arxiv.org/abs/2506.18795v1),  [pdf](http://arxiv.org/pdf/2506.18795v1)

**Tags**: cs.CR cs.SE D.2.4; I.2.7 



### Learning to Insert for Constructive Neural Vehicle Routing Solver
**Authors**: Fu Luo, Xi Lin, Mengyuan Zhong, Fei Liu, Zhenkun Wang, Jianyong Sun, Qingfu Zhang

**Updated**: 2025-06-23T16:00:03Z

**Summary**: Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes.

**Link**: [arxiv](http://arxiv.org/abs/2505.13904v2),  [pdf](http://arxiv.org/pdf/2505.13904v2)

**Tags**: cs.LG cs.AI cs.RO math.OC 



### SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy   Prediction in Autonomous Driving
**Authors**: Helin Cao, Rafael Materla, Sven Behnke

**Updated**: 2025-06-23T15:54:28Z

**Summary**: Perception systems in autonomous driving rely on sensors such as LiDAR and cameras to perceive the 3D environment. However, due to occlusions and data sparsity, these sensors often fail to capture complete information. Semantic Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy and semantics of unobserved regions. Existing transformer-based SOP methods lack explicit modeling of spatial structure in attention computation, resulting in limited geometric awareness and poor performance in sparse or occluded areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel mechanism that incorporates local spatial context into attention. SWA significantly improves scene completion and achieves state-of-the-art results on LiDAR-based SOP benchmarks. We further validate its generality by integrating SWA into a camera-based SOP pipeline, where it also yields consistent gains across modalities.

**Link**: [arxiv](http://arxiv.org/abs/2506.18785v1),  [pdf](http://arxiv.org/pdf/2506.18785v1)

**Tags**: cs.CV cs.AI cs.RO 



### TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation
**Authors**: Kamil Szczepanik, Jaros≈Çaw A. Chudziak

**Updated**: 2025-06-23T15:53:14Z

**Summary**: TRIZ, the Theory of Inventive Problem Solving, is a structured, knowledge-based framework for innovation and abstracting problems to find inventive solutions. However, its application is often limited by the complexity and deep interdisciplinary knowledge required. Advancements in Large Language Models (LLMs) have revealed new possibilities for automating parts of this process. While previous studies have explored single LLMs in TRIZ applications, this paper introduces a multi-agent approach. We propose an LLM-based multi-agent system, called TRIZ agents, each with specialized capabilities and tool access, collaboratively solving inventive problems based on the TRIZ methodology. This multi-agent system leverages agents with various domain expertise to efficiently navigate TRIZ steps. The aim is to model and simulate an inventive process with language agents. We assess the effectiveness of this team of agents in addressing complex innovation challenges based on a selected case study in engineering. We demonstrate the potential of agent collaboration to produce diverse, inventive solutions. This research contributes to the future of AI-driven innovation, showcasing the advantages of decentralized problem-solving in complex ideation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.18783v1),  [pdf](http://arxiv.org/pdf/2506.18783v1)

**Tags**: cs.AI cs.MA 68T07 I.2.11; I.2.7; I.2.8 



### The Impact of Input Order Bias on Large Language Models for Software   Fault Localization
**Authors**: Md Nakhla Rafi, Dong Jae Kim, Tse-Hsun Chen, Shaowei Wang

**Updated**: 2025-06-23T15:51:16Z

**Summary**: Large Language Models (LLMs) have shown significant potential in software engineering tasks such as Fault Localization (FL) and Automatic Program Repair (APR). This study investigates how input order and context size influence LLM performance in FL, a crucial step for many downstream software engineering tasks. We evaluate different method orderings using Kendall Tau distances, including "perfect" (where ground truths appear first) and "worst" (where ground truths appear last), across two benchmarks containing Java and Python projects. Our results reveal a strong order bias: in Java projects, Top-1 FL accuracy drops from 57% to 20% when reversing the order, while in Python projects, it decreases from 38% to approximately 3%. However, segmenting inputs into smaller contexts mitigates this bias, reducing the performance gap in FL from 22% and 6% to just 1% across both benchmarks. We replaced method names with semantically meaningful alternatives to determine whether this bias is due to data leakage. The observed trends remained consistent, suggesting that the bias is not caused by memorization from training data but rather by the inherent effect of input order. Additionally, we explored ordering methods based on traditional FL techniques and metrics, finding that DepGraph's ranking achieves 48% Top-1 accuracy, outperforming simpler approaches such as CallGraph(DFS). These findings highlight the importance of structuring inputs, managing context effectively, and selecting appropriate ordering strategies to enhance LLM performance in FL and other software engineering applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.18750v3),  [pdf](http://arxiv.org/pdf/2412.18750v3)

**Tags**: cs.SE cs.AI cs.LG 



### Existing LLMs Are Not Self-Consistent For Simple Tasks
**Authors**: Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao

**Updated**: 2025-06-23T15:50:21Z

**Summary**: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency -- no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods -- a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at https://github.com/scorpio-nova/llm-self-consistency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18781v1),  [pdf](http://arxiv.org/pdf/2506.18781v1)

**Tags**: cs.CL 



### Likelihood Ratio test for Poisson graph
**Authors**: Chen Shuyan, Liu Xin, Wang Shaoli

**Updated**: 2025-06-23T15:47:09Z

**Summary**: Directed acyclic graphs are widely used to describe the causal effects among random variables, and the inference of those causal effects has become an popular topic in statistics and machine learning, and has wide applications in neuroinformatics, bioinformatics and so on. However, most studies focus on the estimation or inference of the directional relations among continuous random variables, those among discrete random variables have not gained much attentions. In this article we focus on the inference of directed linkages and directed pathways in a Poisson directed graphical model. We employ likelihood ratio tests subject to non-convex acyclicity constraints, and derive the asymptotic distributions of the test statistic under the null hypothesis is true in high-dimensional situations. The power analysis and simulations suggest that the tests achieve the desired objectives of inference. An analysis of a basketball statistics dataset of NBA players during 2016-2017 season illustrates the utility of the proposed method to infer directed linkages and directed pathways in player's statistics network.

**Link**: [arxiv](http://arxiv.org/abs/2506.18778v1),  [pdf](http://arxiv.org/pdf/2506.18778v1)

**Tags**: stat.ME math.ST stat.TH Primary: 62F03, Secondary: 62F30 



### Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions   During Code Training
**Authors**: Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis

**Updated**: 2025-06-23T15:45:44Z

**Summary**: Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.

**Link**: [arxiv](http://arxiv.org/abs/2506.18777v1),  [pdf](http://arxiv.org/pdf/2506.18777v1)

**Tags**: cs.AI cs.CL cs.LG 



### Embedded Model Form Uncertainty Quantification with Measurement Noise   for Bayesian Model Calibration
**Authors**: Daniel Andr√©s Arcones, Martin Weiser, Phaedon-Stelios Koutsourelakis, J√∂rg F. Unger

**Updated**: 2025-06-23T15:29:04Z

**Summary**: A key factor in ensuring the accuracy of computer simulations that model physical systems is the proper calibration of their parameters based on real-world observations or experimental data. Inevitably, uncertainties arise, and Bayesian methods provide a robust framework for quantifying and propagating these uncertainties to model predictions. Nevertheless, Bayesian methods paired with inexact models usually produce predictions unable to represent the observed datapoints. Additionally, the quantified uncertainties of these overconfident models cannot be propagated to other Quantities of Interest (QoIs) reliably. A promising solution involves embedding a model inadequacy term in the inference parameters, allowing the quantified model form uncertainty to influence non-observed QoIs. This paper introduces a more interpretable framework for embedding the model inadequacy compared to existing methods. To overcome the limitations of current approaches, we adapt the existing likelihood models to properly account for noise in the measurements and propose two new formulations designed to address their shortcomings. Moreover, we evaluate the performance of this inadequacy-embedding approach in the presence of discrepancies between measurements and model predictions, including noise and outliers. Particular attention is given to how the uncertainty associated with the model inadequacy term propagates to the QoIs, enabling a more comprehensive statistical analysis of prediction's reliability. Finally, the proposed approach is applied to estimate the uncertainty in the predicted heat flux from a transient thermal simulation using temperature observations.

**Link**: [arxiv](http://arxiv.org/abs/2410.12037v2),  [pdf](http://arxiv.org/pdf/2410.12037v2)

**Tags**: cs.CE J.2 



### ALMA reveals bright circumgalactic emission and a biconical outflow in   z~6.4 quasar PSOJ183+05
**Authors**: Manuela Bischetti, Chiara Feruglio, Stefano Carniani, Valentina D'Odorico, Francesco Salvestrini, Fabrizio Fiore

**Updated**: 2025-06-23T15:29:00Z

**Summary**: Understanding gas flows between galaxies and their surrounding circum-galactic medium (CGM) is crucial to unveil the mechanisms regulating galaxy evolution, especially in the early Universe. However, observations of the CGM around massive galaxies at $z>6$ remain limited, particularly in the cold gas phase. In this work, we present multi-configuration ALMA observations of [CII]$\lambda158\mu$m and millimetre continuum emission in the $z\sim6.4$ quasar PSOJ183+05. We find clumpy [CII] emission, tracing gas up to a $\sim6$ kpc radius, consistent with the interface region between the interstellar medium (ISM) and CGM. The [CII] kinematics shows a rotating disk and a high-velocity, biconical outflow extending up to 5 kpc. The inferred mass outflow rate is $\dot{M}_{\rm of}\sim930$ M$_\odot$ yr$^{-1}$, among the highest at $z>6$, and comparable to the star-formation rate. These findings suggest that quasar-driven outflows can rapidly transfer energy and momentum to the CGM, without immediately quenching star formation in the host galaxy ISM. This supports a delayed feedback scenario, in which outflows reshape CGM conditions and regulate future gas accretion over longer timescales. We find that neither the high-velocity component nor the extended CGM emission in PSOJ183+05 are recovered when using the high-resolution dataset alone, which may explain the conflicting results reported regarding [CII] sizes and the detection of outflows at $z\gtrsim6$. Combining multi-configuration ALMA data with observations from JWST and MUSE will be crucial to map the CGM across its different phases and build a comprehensive picture of the baryon cycle in the first massive galaxies.

**Link**: [arxiv](http://arxiv.org/abs/2504.15357v2),  [pdf](http://arxiv.org/pdf/2504.15357v2)

**Tags**: astro-ph.GA 



### SEAL: Scaling to Emphasize Attention for Long-Context Retrieval
**Authors**: Changhun Lee, Minsang Seok, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park

**Updated**: 2025-06-23T15:24:16Z

**Summary**: While many advanced LLMs are designed to handle long sequence data, we can still observe notable quality degradation even within the sequence limit. In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over long contexts. We observe that specific attention heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores, and adjusting the strength of these heads boosts the quality of LLMs in long context by a large margin. Built on this insight, we propose a learning-based mechanism that leverages generated data to emphasize these heads. By applying SEAL, we achieve significant improvements in long-context retrieval performance across various tasks and models. Additionally, when combined with existing training-free context extension techniques, SEAL extends the contextual limits of LLMs while maintaining highly reliable outputs.

**Link**: [arxiv](http://arxiv.org/abs/2501.15225v2),  [pdf](http://arxiv.org/pdf/2501.15225v2)

**Tags**: cs.CL cs.AI cs.LG 



### Fast State-Augmented Learning for Wireless Resource Allocation with Dual   Variable Regression
**Authors**: Yigit Berkay Uslu, Navid NaderiAlizadeh, Mark Eisen, Alejandro Ribeiro

**Updated**: 2025-06-23T15:20:58Z

**Summary**: We consider resource allocation problems in multi-user wireless networks, where the goal is to optimize a network-wide utility function subject to constraints on the ergodic average performance of users. We demonstrate how a state-augmented graph neural network (GNN) parametrization for the resource allocation policy circumvents the drawbacks of the ubiquitous dual subgradient methods by representing the network configurations (or states) as graphs and viewing dual variables as dynamic inputs to the model, viewed as graph signals supported over the graphs. Lagrangian maximizing state-augmented policies are learned during the offline training phase, and the dual variables evolve through gradient updates while executing the learned state-augmented policies during the inference phase. Our main contributions are to illustrate how near-optimal initialization of dual multipliers for faster inference can be accomplished with dual variable regression, leveraging a secondary GNN parametrization, and how maximization of the Lagrangian over the multipliers sampled from the dual descent dynamics substantially improves the training of state-augmented models. We demonstrate the superior performance of the proposed algorithm with extensive numerical experiments in a case study of transmit power control. Finally, we prove a convergence result and an exponential probability bound on the excursions of the dual function (iterate) optimality gaps.

**Link**: [arxiv](http://arxiv.org/abs/2506.18748v1),  [pdf](http://arxiv.org/pdf/2506.18748v1)

**Tags**: eess.SP cs.LG 



### Self-Normalized Inference in (Quantile, Expected Shortfall) Regressions   for Time Series
**Authors**: Yannick Hoga, Christian Schulz

**Updated**: 2025-06-23T15:19:40Z

**Summary**: This paper proposes valid inference tools, based on self-normalization, in time series expected shortfall regressions and, as a corollary, also in quantile regressions. Extant methods for such time series regressions, based on a bootstrap or direct estimation of the long-run variance, are computationally more involved, require the choice of tuning parameters and have serious size distortions when the regression errors are strongly serially dependent. In contrast, our inference tools only require estimates of the (quantile, expected shortfall) regression parameters that are computed on an expanding window, and are correctly sized as we show in simulations. Two empirical applications to stock return predictability and to Growth-at-Risk demonstrate the practical usefulness of the developed inference tools.

**Link**: [arxiv](http://arxiv.org/abs/2502.10065v2),  [pdf](http://arxiv.org/pdf/2502.10065v2)

**Tags**: econ.EM math.ST stat.TH 



### Towards Group Fairness with Multiple Sensitive Attributes in Federated   Foundation Models
**Authors**: Yuning Yang, Han Yu, Tianrun Gao, Xiaodong Xu, Guangyu Wang

**Updated**: 2025-06-23T15:09:14Z

**Summary**: The deep integration of foundation models (FM) with federated learning (FL) enhances personalization and scalability for diverse downstream tasks, making it crucial in sensitive domains like healthcare. Achieving group fairness has become an increasingly prominent issue in the era of federated foundation models (FFMs), since biases in sensitive attributes might lead to inequitable treatment for under-represented demographic groups. Existing studies mostly focus on achieving fairness with respect to a single sensitive attribute. This renders them unable to provide clear interpretability of dependencies among multiple sensitive attributes which is required to achieve group fairness. Our paper takes the first attempt towards a causal analysis of the relationship between group fairness across various sensitive attributes in the FFM. We extend the FFM structure to trade off multiple sensitive attributes simultaneously and quantify the causal effect behind the group fairness through causal discovery and inference. Extensive experiments validate its effectiveness, offering insights into interpretability towards building trustworthy and fair FFM systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.18732v1),  [pdf](http://arxiv.org/pdf/2506.18732v1)

**Tags**: cs.LG 



### PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries
**Authors**: Steven Kolawole, Keshav Santhanam, Virginia Smith, Pratiksha Thaker

**Updated**: 2025-06-23T15:05:54Z

**Summary**: LLM serving systems typically treat user prompts as monolithic inputs, optimizing inference through decoding tricks or inter-query batching. However, many real-world prompts contain latent semantic parallelism--decomposable structures where subtasks can be executed independently to reduce latency while preserving meaning. We introduce PARALLELPROMPT, the first benchmark for measuring intra-query parallelism in natural user prompts. Our dataset comprises over 37,000 real-world prompts from public LLM chat logs, each annotated with a structured schema capturing task templates, shared context, and iteration inputs. These schemas are extracted using LLM-assisted prompting with rule-based multilingual validation. To evaluate the benefits of decomposition, we provide an execution suite that benchmarks serial vs. parallel strategies, measuring latency, structural adherence, and semantic fidelity. Our results show that intra-query parallelism can be successfully parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks like translation, comprehension, and comparative analysis, with minimal quality degradation. By releasing this benchmark, curation pipeline, and evaluation suite, we provide the first standardized testbed for studying structure-aware execution in LLM serving pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2506.18728v1),  [pdf](http://arxiv.org/pdf/2506.18728v1)

**Tags**: cs.LG 



### Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs   with POLLUX
**Authors**: Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova

**Updated**: 2025-06-23T15:01:31Z

**Summary**: We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.

**Link**: [arxiv](http://arxiv.org/abs/2505.24616v2),  [pdf](http://arxiv.org/pdf/2505.24616v2)

**Tags**: cs.CL cs.AI 



### Tail Flexibility in the Degrees of Preferential Attachment Networks
**Authors**: Thomas Boughen, Clement Lee, Vianey Palacios Ramirez

**Updated**: 2025-06-23T15:00:22Z

**Summary**: Devising the underlying generating mechanism of a real-life network is difficult as, more often than not, only its snapshots are available, but not its full evolution. One candidate for the generating mechanism is preferential attachment which, in its simplest form, results in a degree distribution that follows the power law. Consequently, the growth of real-life networks that roughly display such power-law behaviour is commonly modelled by preferential attachment. However, the validity of the power law has been challenged by the presence of alternatives with comparable performance, as well as the recent findings that the right tail of the degree distribution is often lighter than implied by the body, whilst still being heavy. In this paper, we study a modified version of the model with a flexible preference function that allows super/sub-linear behaviour whilst also guaranteeing that the limiting degree distribution has a heavy tail. We relate the distributions tail index directly to the model parameters, allowing direct inference of the parameters from the degree distribution alone.

**Link**: [arxiv](http://arxiv.org/abs/2506.18726v1),  [pdf](http://arxiv.org/pdf/2506.18726v1)

**Tags**: stat.ME 



### LLM-enhanced Interactions in Human-Robot Collaborative Drawing with   Older Adults
**Authors**: Marianne Bossema, Somaya Ben Allouch, Aske Plaat, Rob Saunders

**Updated**: 2025-06-23T14:49:03Z

**Summary**: The goal of this study is to identify factors that support and enhance older adults' creative experiences in human-robot co-creativity. Because the research into the use of robots for creativity support with older adults remains underexplored, we carried out an exploratory case study. We took a participatory approach and collaborated with professional art educators to design a course Drawing with Robots for adults aged 65 and over. The course featured human-human and human-robot drawing activities with various types of robots. We observed collaborative drawing interactions, interviewed participants on their experiences, and analyzed collected data. Findings show that participants preferred acting as curators, evaluating creative suggestions from the robot in a teacher or coach role. When we enhanced a robot with a multimodal Large Language Model (LLM), participants appreciated its spoken dialogue capabilities. They reported however, that the robot's feedback sometimes lacked an understanding of the context, and sensitivity to their artistic goals and preferences. Our findings highlight the potential of LLM-enhanced robots to support creativity and offer future directions for advancing human-robot co-creativity with older adults.

**Link**: [arxiv](http://arxiv.org/abs/2506.18711v1),  [pdf](http://arxiv.org/pdf/2506.18711v1)

**Tags**: cs.HC 



### Benchmarking the Pedagogical Knowledge of Large Language Models
**Authors**: Maxime Leli√®vre, Amy Waldock, Meng Liu, Natalia Vald√©s Aspillaga, Alasdair Mackintosh, Mar√≠a Jos√© Ogando Portela, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod

**Updated**: 2025-06-24T12:36:22Z

**Summary**: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.

**Link**: [arxiv](http://arxiv.org/abs/2506.18710v2),  [pdf](http://arxiv.org/pdf/2506.18710v2)

**Tags**: cs.CL cs.AI 



### A long-term study of Mrk 50 : Appearance and disappearance of soft   excess
**Authors**: Narendranath Layek, Prantik Nandi, Sachindra Naik, Arghajit Jana

**Updated**: 2025-06-23T14:48:52Z

**Summary**: We present an extensive temporal and spectral study of the Seyfert 1 AGN Mrk 50 using 15 years (2007-2022) of multiwavelength observations from XMM-Newton, Swift, and NuSTAR for the first time. From the timing analysis, we found that the source exhibited variability of $\sim$20 % during the 2007 observation, which reduced to below 10 % in the subsequent observations and became non-variable in the observations from 2010 onward. From the spectral study, we found that the spectra are nearly featureless. Non-detection of absorption in the low-energy domain during the 15 years of observation infers the absence of obscuration around the central engine, rendering the nucleus a `bare' type. A prominent soft X-ray excess below 2 keV was detected in the source spectrum during the observations between 2007 and 2010, which vanished during the later observations. To describe the nature of the soft excess, we use two physical models, such as warm Comptonization and blurred reflection from the ionized accretion disk. Both the physical models explain the nature and origin of the soft excess in this source. Our analysis found that Mrk~50 accretes at sub-Eddington accretion rate ($\lambda_{Edd}=0.13-0.02$) during all the observations used in this work.

**Link**: [arxiv](http://arxiv.org/abs/2501.09300v2),  [pdf](http://arxiv.org/pdf/2501.09300v2)

**Tags**: astro-ph.HE 



### Small Term Reachability and Related Problems for Terminating Term   Rewriting Systems
**Authors**: Franz Baader, J√ºrgen Giesl

**Updated**: 2025-06-23T14:45:16Z

**Summary**: Motivated by an application where we try to make proofs for Description Logic inferences smaller by rewriting, we consider the following decision problem, which we call the small term reachability problem: given a term rewriting system $R$, a term $s$, and a natural number $n$, decide whether there is a term $t$ of size $\leq n$ reachable from $s$ using the rules of $R$. We investigate the complexity of this problem depending on how termination of $R$ can be established. We show that the problem is in general NP-complete for length-reducing term rewriting systems. Its complexity increases to N2ExpTime-complete (NExpTime-complete) if termination is proved using a (linear) polynomial order and to PSpace-complete for systems whose termination can be shown using a restricted class of Knuth-Bendix orders. Confluence reduces the complexity to P for the length-reducing case, but has no effect on the worst-case complexity in the other two cases. Finally, we consider the large term reachability problem, a variant of the problem where we are interested in reachability of a term of size $\geq n$. It turns out that this seemingly innocuous modification in some cases changes the complexity of the problem, which may also become dependent on whether the number $n$ is is represented in unary or binary encoding, whereas this makes no difference for the complexity of the small term reachability problem.

**Link**: [arxiv](http://arxiv.org/abs/2412.06047v2),  [pdf](http://arxiv.org/pdf/2412.06047v2)

**Tags**: cs.LO 



### Handling Numeric Expressions in Automatic Speech Recognition
**Authors**: Christian Huber, Alexander Waibel

**Updated**: 2025-06-23T14:45:07Z

**Summary**: This paper addresses the problem of correctly formatting numeric expressions in automatic speech recognition (ASR) transcripts. This is challenging since the expected transcript format depends on the context, e.g., 1945 (year) vs. 19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize and format numeric expressions such as years, timestamps, currency amounts, and quantities. For the end-to-end approach, we employed a data generation strategy using a large language model (LLM) together with a text to speech (TTS) model to generate adaptation data. The results on our test data set show that while approaches based on LLMs perform well in recognizing formatted numeric expressions, adapted end-to-end models offer competitive performance with the advantage of lower latency and inference cost.

**Link**: [arxiv](http://arxiv.org/abs/2408.00004v2),  [pdf](http://arxiv.org/pdf/2408.00004v2)

**Tags**: eess.AS cs.AI cs.CL 



### Probing fermionic asymmetric dark matter cores using global neutron star   properties
**Authors**: Nathan Rutherford, Chanda Prescod-Weinstein, Anna Watts

**Updated**: 2025-06-23T14:44:25Z

**Summary**: It is possible for asymmetric dark matter (ADM) to accumulate in neutron star interiors and affect their global properties. Considering the effects of this accumulation, neutron star mass-radius measurements can deliver new insights into the cold dense matter equation of state (EoS). In this paper, we employ Bayesian parameter estimation using real and synthetic neutron star mass-radius data to infer constraints on the combined baryonic matter and fermionic ADM EoS, where the fermionic ADM forms a core in the neutron star interior. Using currently available mass-radius data, we find that the lower bound of the ratio between ADM effective self-repulsion strength ($g_\chi/m_\phi$) and particle mass ($m_\chi$) can be constrained at the 68\% (95\%) credible level to $10^{-6.59}$ ($10^{-7.77}$). We also find that, if neutron star mass-radius measurement uncertainties are reduced to the 2\% level, the constraints on the lower bound of the ratio of $g_\chi/m_\phi$ to $m_\chi$ can be improved to $10^{-6.49}$ and $10^{-7.68}$ at the 68\% and 95\% credible levels, respectively. However, all other combinations, of $m_\chi$, $g_\chi$, and the ADM mass-fraction, $F_\chi$, (i.e., the ratio of the gravitational ADM mass to the gravitational mass of the neutron star) are unconstrained. Furthermore, in the pressure-energy density and mass-radius planes, the inferences which include the possibility of fermionic ADM cores are nearly identical with the inferences that neglect fermionic ADM for $F_\chi \leq 1.7\%$ and neutron star mass-radius uncertainties $\geq 2\%$. Therefore, we find that neutron star mass-radius measurements can constrain the ratio of $g_\chi/m_\phi$ to $m_\chi$ and that neutron stars with ADM are indistinguishable from purely baryonic stars. This implies that neutron stars with ADM are equally as consistent with the available mass-radius data as neutron stars without ADM.

**Link**: [arxiv](http://arxiv.org/abs/2410.00140v2),  [pdf](http://arxiv.org/pdf/2410.00140v2)

**Tags**: astro-ph.HE astro-ph.SR hep-ph nucl-th 



### Context-Aware Human Behavior Prediction Using Multimodal Large Language   Models: Challenges and Insights
**Authors**: Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello

**Updated**: 2025-06-23T14:43:46Z

**Summary**: Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.

**Link**: [arxiv](http://arxiv.org/abs/2504.00839v2),  [pdf](http://arxiv.org/pdf/2504.00839v2)

**Tags**: cs.RO cs.AI 



### Context Biasing for Pronunciations-Orthography Mismatch in Automatic   Speech Recognition
**Authors**: Christian Huber, Alexander Waibel

**Updated**: 2025-06-23T14:42:03Z

**Summary**: Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 11\%, while maintaining a competitive overall word error rate.

**Link**: [arxiv](http://arxiv.org/abs/2506.18703v1),  [pdf](http://arxiv.org/pdf/2506.18703v1)

**Tags**: cs.CL cs.LG 



### LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review   and Taxonomy
**Authors**: Muhammed Golec, Yaser Khamayseh, Suhib Bani Melhem, Abdulmalik Alwarafy

**Updated**: 2025-06-23T14:37:53Z

**Summary**: Sixth Generation (6G) wireless networks, which are expected to be deployed in the 2030s, have already created great excitement in academia and the private sector with their extremely high communication speed and low latency rates. However, despite the ultra-low latency, high throughput, and AI-assisted orchestration capabilities they promise, they are vulnerable to stealthy and long-term Advanced Persistent Threats (APTs). Large Language Models (LLMs) stand out as an ideal candidate to fill this gap with their high success in semantic reasoning and threat intelligence. In this paper, we present a comprehensive systematic review and taxonomy study for LLM-assisted APT detection in 6G networks. We address five research questions, namely, semantic merging of fragmented logs, encrypted traffic analysis, edge distribution constraints, dataset/modeling techniques, and reproducibility trends, by leveraging most recent studies on the intersection of LLMs, APTs, and 6G wireless networks. We identify open challenges such as explainability gaps, data scarcity, edge hardware limitations, and the need for real-time slicing-aware adaptation by presenting various taxonomies such as granularity, deployment models, and kill chain stages. We then conclude the paper by providing several research gaps in 6G infrastructures for future researchers. To the best of our knowledge, this paper is the first comprehensive systematic review and classification study on LLM-based APT detection in 6G networks.

**Link**: [arxiv](http://arxiv.org/abs/2505.18846v2),  [pdf](http://arxiv.org/pdf/2505.18846v2)

**Tags**: cs.CR 



### NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed   Target Tracking in Unstructured GPS-Denied Environments
**Authors**: Alessandro Saviolo, Giuseppe Loianno

**Updated**: 2025-06-23T14:28:30Z

**Summary**: Autonomous aerial target tracking in unstructured and GPS-denied environments remains a fundamental challenge in robotics. Many existing methods rely on motion capture systems, pre-mapped scenes, or feature-based localization to ensure safety and control, limiting their deployment in real-world conditions. We introduce NOVA, a fully onboard, object-centric framework that enables robust target tracking and collision-aware navigation using only a stereo camera and an IMU. Rather than constructing a global map or relying on absolute localization, NOVA formulates perception, estimation, and control entirely in the target's reference frame. A tightly integrated stack combines a lightweight object detector with stereo depth completion, followed by histogram-based filtering to infer robust target distances under occlusion and noise. These measurements feed a visual-inertial state estimator that recovers the full 6-DoF pose of the robot relative to the target. A nonlinear model predictive controller (NMPC) plans dynamically feasible trajectories in the target frame. To ensure safety, high-order control barrier functions are constructed online from a compact set of high-risk collision points extracted from depth, enabling real-time obstacle avoidance without maps or dense representations. We validate NOVA across challenging real-world scenarios, including urban mazes, forest trails, and repeated transitions through buildings with intermittent GPS loss and severe lighting changes that disrupt feature-based localization. Each experiment is repeated multiple times under similar conditions to assess resilience, showing consistent and reliable performance. NOVA achieves agile target following at speeds exceeding 50 km/h. These results show that high-speed vision-based tracking is possible in the wild using only onboard sensing, with no reliance on external localization or environment assumptions.

**Link**: [arxiv](http://arxiv.org/abs/2506.18689v1),  [pdf](http://arxiv.org/pdf/2506.18689v1)

**Tags**: cs.RO cs.AI 



### One Step Diffusion via Shortcut Models
**Authors**: Kevin Frans, Danijar Hafner, Sergey Levine, Pieter Abbeel

**Updated**: 2025-06-23T14:26:35Z

**Summary**: Diffusion models and flow-matching models have enabled generating diverse and realistic images by learning to transfer noise to data. However, sampling from these models involves iterative denoising over many neural network passes, making generation slow and expensive. Previous approaches for speeding up sampling require complex training regimes, such as multiple training phases, multiple networks, or fragile scheduling. We introduce shortcut models, a family of generative models that use a single network and training phase to produce high-quality samples in a single or multiple sampling steps. Shortcut models condition the network not only on the current noise level but also on the desired step size, allowing the model to skip ahead in the generation process. Across a wide range of sampling step budgets, shortcut models consistently produce higher quality samples than previous approaches, such as consistency models and reflow. Compared to distillation, shortcut models reduce complexity to a single network and training phase and additionally allow varying step budgets at inference time.

**Link**: [arxiv](http://arxiv.org/abs/2410.12557v3),  [pdf](http://arxiv.org/pdf/2410.12557v3)

**Tags**: cs.LG cs.CV 



### SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape   Point Clouds from RGB Images for 2D Classification
**Authors**: Youcef Sklab, Hanane Ariouat, Eric Chenin, Edi Prifti, Jean-Daniel Zucker

**Updated**: 2025-06-23T14:25:40Z

**Summary**: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image classification architecture that integrates 3D point cloud representations inferred directly from RGB images. Our key contribution lies in a pixel-to-point transformation that converts 2D object masks into 3D point clouds, enabling the fusion of texture-based and geometric features for enhanced classification performance. SIM-Net is particularly well-suited for the classification of digitized herbarium specimens (a task made challenging by heterogeneous backgrounds), non-plant elements, and occlusions that compromise conventional image-based models. To address these issues, SIM-Net employs a segmentation-based preprocessing step to extract object masks prior to 3D point cloud generation. The architecture comprises a CNN encoder for 2D image features and a PointNet-based encoder for geometric features, which are fused into a unified latent space. Experimental evaluations on herbarium datasets demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several transformer-based state-of-the-art architectures, highlighting the benefits of incorporating 3D structural reasoning into 2D image classification tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.18683v1),  [pdf](http://arxiv.org/pdf/2506.18683v1)

**Tags**: cs.CV cs.AI 



### "I understand why I got this grade": Automatic Short Answer Grading with   Feedback
**Authors**: Dishank Aggarwal, Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya

**Updated**: 2025-06-23T14:24:28Z

**Summary**: In recent years, there has been a growing interest in using Artificial Intelligence (AI) to automate student assessment in education. Among different types of assessments, summative assessments play a crucial role in evaluating a student's understanding level of a course. Such examinations often involve short-answer questions. However, grading these responses and providing meaningful feedback manually at scale is both time-consuming and labor-intensive. Feedback is particularly important, as it helps students recognize their strengths and areas for improvement. Despite the importance of this task, there is a significant lack of publicly available datasets that support automatic short-answer grading with feedback generation. To address this gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset designed for automatic short-answer grading with feedback. The dataset covers a diverse range of subjects, questions, and answer patterns from multiple engineering domains and contains ~5.8k data points. We incorporate feedback into our dataset by leveraging the generative capabilities of state-of-the-art large language models (LLMs) using our Label-Aware Synthetic Feedback Generation (LASFG) strategy. This paper underscores the importance of enhanced feedback in practical educational settings, outlines dataset annotation and feedback generation processes, conducts a thorough EngSAF analysis, and provides different LLMs-based zero-shot and finetuned baselines for future comparison. The best-performing model (Mistral-7B) achieves an overall accuracy of 75.4% and 58.7% on unseen answers and unseen question test sets, respectively. Additionally, we demonstrate the efficiency and effectiveness of our ASAG system through its deployment in a real-world end-semester exam at a reputed institute.

**Link**: [arxiv](http://arxiv.org/abs/2407.12818v2),  [pdf](http://arxiv.org/pdf/2407.12818v2)

**Tags**: cs.CL cs.AI cs.CY 



### DuetGen: Music Driven Two-Person Dance Generation via Hierarchical   Masked Modeling
**Authors**: Anindita Ghosh, Bing Zhou, Rishabh Dabral, Jian Wang, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek, Chuan Guo

**Updated**: 2025-06-23T14:22:50Z

**Summary**: We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers' motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination.

**Link**: [arxiv](http://arxiv.org/abs/2506.18680v1),  [pdf](http://arxiv.org/pdf/2506.18680v1)

**Tags**: cs.GR cs.CV cs.SD eess.AS 



### Is There a Case for Conversation Optimized Tokenizers in Large Language   Models?
**Authors**: Raquel Ferrando, Javier Conde, Gonzalo Mart√≠nez, Pedro Reviriego

**Updated**: 2025-06-23T14:18:46Z

**Summary**: The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.

**Link**: [arxiv](http://arxiv.org/abs/2506.18674v1),  [pdf](http://arxiv.org/pdf/2506.18674v1)

**Tags**: cs.CL cs.AI 



### Harnessing the Power of Reinforcement Learning for Language-Model-Based   Information Retriever via Query-Document Co-Augmentation
**Authors**: Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou

**Updated**: 2025-06-23T14:14:43Z

**Summary**: Recent studies have proposed leveraging Large Language Models (LLMs) as information retrievers through query rewriting. However, for challenging corpora, we argue that enhancing queries alone is insufficient for robust semantic matching; the LLM should also have sufficient understanding of the corpus by directly handling and augmenting the documents themselves. To this end, we present an LLM-based retriever empowered to augment both user queries and corpus documents, with its policy fully explored via reinforcement learning (RL) and minimal human inductive bias. Notably, we find that simply allowing the LLM to modify documents yields little benefit unless paired with our carefully designed bidirectional RL framework, which enables the LLM to simultaneously learn and collaborate on both query and document augmentation policies. A key technical challenge in realizing such a framework lies in jointly updating both policies during training, where the rewards for the two directions depend on each other, making their entangled reward intractable. Our approach addresses this by introducing a reward sampling strategy and a specifically designed RL algorithm that enables effective training with these sampled rewards. Experimental results demonstrate that our approach significantly enhances LLM-based retrieval performance in both sparse and dense settings, particularly in difficult retrieval domains, and achieves strong cross-benchmark generalization. Our code is released at https://github.com/liujm2001/CoAugRetriever.

**Link**: [arxiv](http://arxiv.org/abs/2506.18670v1),  [pdf](http://arxiv.org/pdf/2506.18670v1)

**Tags**: cs.IR 



### Residual test to search for microlensing signatures in strongly lensed   gravitational wave signals
**Authors**: Eungwang Seo, Xikai Shan, Justin Janquart, Otto A. Hannuksela, Martin A. Hendry, Bin Hu

**Updated**: 2025-06-23T14:01:07Z

**Summary**: When a gravitational wave signal encounters a massive object, such as a galaxy or galaxy cluster, it undergoes strong gravitational lensing, producing multiple copies of the original signal. These strongly lensed signals exhibit identical waveform morphology in the frequency domain, allowing analysis without the need for complex lens models. However, stellar fields and dark matter substructures within the galactic lens introduce microlensing effects that alter individual signal morphologies. Identifying these microlensing signatures is computationally challenging within Bayesian frameworks. In this study, we propose a residual test to efficiently search for microlensing signatures by leveraging the fact that current Bayesian inference pipelines are optimized solely for the strong lensing hypothesis. Using cross-correlation techniques, we investigate the microlensing-induced deviations from the strong hypothesis, which are imprinted in the residuals. Most simulated signals from our realistic microlensing populations exhibit small mismatches between the microlensed and unlensed waveforms, but a fraction show significant deviations. We find that 28% (52%) and 34% (66%)of microlensed events with mismatch > 0.03 and > 0.1, respectively, can be discerned with O4 (O5) detector sensitivities, which demonstrates that high-mismatch events are more likely to be identified as microlensed. Including all events from a realistic population, 11% (21.5%) are identifiable with O4 (O5) sensitivity using our approach.

**Link**: [arxiv](http://arxiv.org/abs/2503.02186v2),  [pdf](http://arxiv.org/pdf/2503.02186v2)

**Tags**: gr-qc astro-ph.IM 



### A Random Matrix Analysis of In-context Memorization for Nonlinear   Attention
**Authors**: Zhenyu Liao, Jiaqing Liu, TianQi Hou, Difan Zou, Zenan Ling

**Updated**: 2025-06-23T13:56:43Z

**Summary**: Attention mechanisms have revolutionized machine learning (ML) by enabling efficient modeling of global dependencies across inputs. Their inherently parallelizable structures allow for efficient scaling with the exponentially increasing size of both pretrained data and model parameters. Yet, despite their central role as the computational backbone of modern large language models (LLMs), the theoretical understanding of Attentions, especially in the nonlinear setting, remains limited.   In this paper, we provide a precise characterization of the \emph{in-context memorization error} of \emph{nonlinear Attention}, in the high-dimensional proportional regime where the number of input tokens $n$ and their embedding dimension $p$ are both large and comparable. Leveraging recent advances in the theory of large kernel random matrices, we show that nonlinear Attention typically incurs higher memorization error than linear ridge regression on random inputs. However, this gap vanishes, and can even be reversed, when the input exhibits statistical structure, particularly when the Attention weights align with the input signal direction. Our results reveal how nonlinearity and input structure interact with each other to govern the memorization performance of nonlinear Attention. The theoretical insights are supported by numerical experiments.

**Link**: [arxiv](http://arxiv.org/abs/2506.18656v1),  [pdf](http://arxiv.org/pdf/2506.18656v1)

**Tags**: stat.ML cs.LG math.ST stat.TH 



### C-SEO Bench: Does Conversational SEO Work?
**Authors**: Haritz Puerto, Martin Gubri, Tommaso Green, Seong Joon Oh, Sangdoo Yun

**Updated**: 2025-06-23T13:56:31Z

**Summary**: Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not understand whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are largely ineffective, contrary to reported results in the literature. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at https://github.com/parameterlab/c-seo-bench and https://huggingface.co/datasets/parameterlab/c-seo-bench.

**Link**: [arxiv](http://arxiv.org/abs/2506.11097v2),  [pdf](http://arxiv.org/pdf/2506.11097v2)

**Tags**: cs.CL cs.AI cs.IR 



### A Practical Introduction to Regression-based Causal Inference in   Meteorology (II): Unmeasured confounders
**Authors**: Caren Marzban, Yikun Zhang, Nicholas Bond, Michael Richman

**Updated**: 2025-06-23T13:54:52Z

**Summary**: One obstacle to ``elevating" correlation to causation is the phenomenon of confounding, i.e., when a correlation between two variables exists because both variables are in fact caused by a third variable. The situation where the confounders are measured is examined in an earlier, accompanying article. Here, it is shown that even when the confounding variables are not measured, it is still possible to estimate the causal effect via a regression-based method that uses the notion of Instrumental Variables. Using meteorological data set, similar to that in the sister article, a number of different estimates of the causal effect are compared and contrasted. It is shown that the Instrumental Variable results based on unmeasured confounders are consistent with those of the sister article where confounders are measured.

**Link**: [arxiv](http://arxiv.org/abs/2506.18652v1),  [pdf](http://arxiv.org/pdf/2506.18652v1)

**Tags**: stat.AP 



### ReDit: Reward Dithering for Improved LLM Policy Optimization
**Authors**: Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu

**Updated**: 2025-06-24T07:07:57Z

**Summary**: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.

**Link**: [arxiv](http://arxiv.org/abs/2506.18631v2),  [pdf](http://arxiv.org/pdf/2506.18631v2)

**Tags**: cs.LG cs.AI cs.CL 



### AggTruth: Contextual Hallucination Detection using Aggregated Attention   Scores in LLMs
**Authors**: Piotr Matys, Jan Eliasz, Konrad Kie≈Çczy≈Ñski, Miko≈Çaj Langner, Teddy Ferdinan, Jan Koco≈Ñ, Przemys≈Çaw Kazienko

**Updated**: 2025-06-23T13:35:05Z

**Summary**: In real-world applications, Large Language Models (LLMs) often hallucinate, even in Retrieval-Augmented Generation (RAG) settings, which poses a significant challenge to their deployment. In this paper, we introduce AggTruth, a method for online detection of contextual hallucinations by analyzing the distribution of internal attention scores in the provided context (passage). Specifically, we propose four different variants of the method, each varying in the aggregation technique used to calculate attention scores. Across all LLMs examined, AggTruth demonstrated stable performance in both same-task and cross-task setups, outperforming the current SOTA in multiple scenarios. Furthermore, we conducted an in-depth analysis of feature selection techniques and examined how the number of selected attention heads impacts detection performance, demonstrating that careful selection of heads is essential to achieve optimal results.

**Link**: [arxiv](http://arxiv.org/abs/2506.18628v1),  [pdf](http://arxiv.org/pdf/2506.18628v1)

**Tags**: cs.AI cs.CL 



### Efficient and Generalizable Speaker Diarization via Structured Pruning   of Self-Supervised Models
**Authors**: Jiangyu Han, Petr P√°lka, Marc Delcroix, Federico Landini, Johan Rohdin, Jan Cernock√Ω, Luk√°≈° Burget

**Updated**: 2025-06-23T13:29:51Z

**Summary**: Self-supervised learning (SSL) models such as WavLM have brought substantial improvements to speaker diarization by providing rich contextual representations. However, the high computational and memory costs of these models hinder their deployment in real-time and resource-constrained scenarios. In this work, we present a comprehensive study on compressing SSL-based diarization models through structured pruning guided by knowledge distillation. Building upon our previous work, we extend the analysis to include pruning objectives based on multiply-accumulate operations (MACs), investigate module-wise and progressive pruning strategies, and examine the impact of training data quantity. Experimental results show that our method reduces model size by up to 80% without degrading performance, achieving up to 4x faster inference on a single GPU. We further perform large-scale evaluations on a diverse compound dataset comprising eight public diarization corpora, where our best pruned model achieves state-of-the-art performance across most conditions. Additionally, we show strong generalization to the CHiME-6 dataset, attaining performance comparable to the third-place system in the CHiME-7 challenge without any domain adaptation. All models and code are publicly released to support reproducibility and future research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18623v1),  [pdf](http://arxiv.org/pdf/2506.18623v1)

**Tags**: eess.AS 



### The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified   Speeches
**Authors**: Alisa Barkar, Mathieu Chollet, Matthieu Labeau, Beatrice Biancardi, Chloe Clavel

**Updated**: 2025-06-23T13:28:33Z

**Summary**: This study examines how large language models understand the concept of persuasiveness in public speaking by modifying speech transcripts from PhD candidates in the "Ma These en 180 Secondes" competition, using the 3MT French dataset. Our contributions include a novel methodology and an interpretable textual feature set integrating rhetorical devices and discourse markers. We prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic shifts between original and generated speech in terms of the new features. Results indicate that GPT-4o applies systematic stylistic modifications rather than optimizing persuasiveness in a human-like manner. Notably, it manipulates emotional lexicon and syntactic structures (such as interrogative and exclamatory clauses) to amplify rhetorical impact.

**Link**: [arxiv](http://arxiv.org/abs/2506.18621v1),  [pdf](http://arxiv.org/pdf/2506.18621v1)

**Tags**: cs.CL 



### The Risk-Neutral Equivalent Pricing of Model-Uncertainty
**Authors**: Ken Kangda Wren

**Updated**: 2025-06-23T13:08:00Z

**Summary**: Existing approaches to asset-pricing under model-uncertainty adapt classical utility-maximization frameworks and seek theoretical comprehensiveness. We move toward practice by considering binary model-risks and by emphasizing 'constraints' over 'preference'. This decomposes viable economic asset-pricing into that of model and non-model risks separately, leading to a unique and convenient model-risk pricing formula. Its parameter, a dynamically conserved constant of model-risk inference, allows an integrated representation of ex-ante risk-pricing and bias such that their ex-post impacts are disentangled via well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns acquire a fresh significance: peak-reward reveals ex-ante risk-premia, and peak-location, bias.

**Link**: [arxiv](http://arxiv.org/abs/2502.13744v5),  [pdf](http://arxiv.org/pdf/2502.13744v5)

**Tags**: q-fin.MF econ.EM 



### Semantic similarity estimation for domain specific data using BERT and   other techniques
**Authors**: R. Prashanth

**Updated**: 2025-06-23T13:03:59Z

**Summary**: Estimation of semantic similarity is an important research problem both in natural language processing and the natural language understanding, and that has tremendous application on various downstream tasks such as question answering, semantic search, information retrieval, document clustering, word-sense disambiguation and machine translation. In this work, we carry out the estimation of semantic similarity using different state-of-the-art techniques including the USE (Universal Sentence Encoder), InferSent and the most recent BERT, or Bidirectional Encoder Representations from Transformers, models. We use two question pairs datasets for the analysis, one is a domain specific in-house dataset and the other is a public dataset which is the Quora's question pairs dataset. We observe that the BERT model gave much superior performance as compared to the other methods. This should be because of the fine-tuning procedure that is involved in its training process, allowing it to learn patterns based on the training data that is used. This works demonstrates the applicability of BERT on domain specific datasets. We infer from the analysis that BERT is the best technique to use in the case of domain specific data.

**Link**: [arxiv](http://arxiv.org/abs/2506.18602v1),  [pdf](http://arxiv.org/pdf/2506.18602v1)

**Tags**: cs.CL stat.AP 



### API Agents vs. GUI Agents: Divergence and Convergence
**Authors**: Chaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang

**Updated**: 2025-06-23T13:01:02Z

**Summary**: Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.11069v2),  [pdf](http://arxiv.org/pdf/2503.11069v2)

**Tags**: cs.AI cs.HC 



### Reply to "Emergent LLM behaviors are observationally equivalent to data   leakage"
**Authors**: Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli

**Updated**: 2025-06-23T12:59:34Z

**Summary**: A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.

**Link**: [arxiv](http://arxiv.org/abs/2506.18600v1),  [pdf](http://arxiv.org/pdf/2506.18600v1)

**Tags**: cs.CL cs.GT cs.MA 



### No Training Wheels: Steering Vectors for Bias Correction at Inference   Time
**Authors**: Aviral Gupta, Armaan Sethi, Ameesh Sethi

**Updated**: 2025-06-23T12:58:54Z

**Summary**: Neural network classifiers trained on datasets with uneven group representation often inherit class biases and learn spurious correlations. These models may perform well on average but consistently fail on atypical groups. For example, in hair color classification, datasets may over-represent females with blond hair, reinforcing stereotypes. Although various algorithmic and data-centric methods have been proposed to address such biases, they often require retraining or significant compute. In this work, we propose a cheap, training-free method inspired by steering vectors used to edit behaviors in large language models. We compute the difference in mean activations between majority and minority groups to define a "bias vector," which we subtract from the model's residual stream. This leads to reduced classification bias and improved worst-group accuracy. We explore multiple strategies for extracting and applying these vectors in transformer-like classifiers, showing that steering vectors, traditionally used in generative models, can also be effective in classification. More broadly, we showcase an extremely cheap, inference time, training free method to mitigate bias in classification models.

**Link**: [arxiv](http://arxiv.org/abs/2506.18598v1),  [pdf](http://arxiv.org/pdf/2506.18598v1)

**Tags**: cs.LG cs.CL cs.CV 



### Goal-oriented Spectrum Sharing: Trading Edge Inference Power for Data   Streaming Performance
**Authors**: Mattia Merluzzi, Miltiadis C. Filippou

**Updated**: 2025-06-23T12:56:31Z

**Summary**: We study the problem of spectrum sharing between goal-oriented (GO) and legacy data-oriented (DO) systems. For the former, data quality and representation is no longer optimized based on classical communication key performance indicators, but rather configured on the fly to achieve the goal of communication with the least resource overhead. This paradigm can be followed to flexibly adapt wireless and in-network artificial intelligence operations across different nodes (e.g., access points, users, sensors or actuators) to data traffic, channel conditions, energy availability and distributed computing capabilities. In this paper, we argue and demonstrate that computing and learning/inference operation performance strongly affect lower layers, calling for a real cross-layer optimization that encompasses physical and computation resource orchestration, up to the application level. Focusing on a communication channel shared among a GO and a DO user, we define a goal-effective achievable rate region (GEARR), to assess the maximum data rate attainable by the latter, subject to goal achievement guarantees for the former. Finally, we propose a cross-layer dynamic resource orchestration able to reach the boundaries of the GEARR, under different goal-effectiveness and compute resource consumption constraints.

**Link**: [arxiv](http://arxiv.org/abs/2503.11552v2),  [pdf](http://arxiv.org/pdf/2503.11552v2)

**Tags**: eess.SP 



### $B$ meson decays to vector charmonium(like) states and a $K$ meson: the   role of final-state interactions
**Authors**: Qi-Wei Yuan, Qi Wu, Ming-Zhu Liu

**Updated**: 2025-06-23T12:47:53Z

**Summary**: A series of vector charmonium(like) states, accompanied by a $K$ meson, have been observed in the decays of $B$ meson. These processes are color-suppressed at the quark level, as inferred from topological diagram analysis. In this work, we calculate the branching fractions of the decays $B \to \psi K$, where $\psi$ denotes the charmonium(like) states $\psi(1S)$, $\psi(2S)$, $\psi(4040)$, $\psi(3770)$, and $\psi(4160)$. Our analysis incorporates both short-distance (naive factorization approach) and long-distance (final-state interactions) contributions. Within reasonable parameters, our results align with experimental data except for the $ \psi(4160)$, suggesting its possible exotic nature. Furthermore, we find that long-distance contributions dominate these decay processes, highlighting the crucial role of final-state interactions in the productions of charmonium(like) states in $B$ decays.

**Link**: [arxiv](http://arxiv.org/abs/2504.11121v2),  [pdf](http://arxiv.org/pdf/2504.11121v2)

**Tags**: hep-ph 



### Parallel Continuous Chain-of-Thought with Jacobi Iteration
**Authors**: Haoyi Wu, Zhihao Teng, Kewei Tu

**Updated**: 2025-06-23T12:35:41Z

**Summary**: Continuous chain-of-thought has been shown to be effective in saving reasoning tokens for large language models. By reasoning with continuous latent thought tokens, continuous CoT is able to perform implicit reasoning in a compact manner. However, the sequential dependencies between latent thought tokens spoil parallel training, leading to long training time. In this paper, we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi iteration on the latent thought tokens, updating them iteratively in parallel instead of sequentially and thus improving both training and inference efficiency of continuous CoT. Experiments demonstrate that by choosing the proper number of iterations, we are able to achieve comparable or even better performance while saving nearly 50% of the training and inference time. Moreover, PCCoT shows better stability and robustness in the training process. Our code is available at https://github.com/whyNLP/PCCoT.

**Link**: [arxiv](http://arxiv.org/abs/2506.18582v1),  [pdf](http://arxiv.org/pdf/2506.18582v1)

**Tags**: cs.CL 



### A Modular Taxonomy for Hate Speech Definitions and Its Impact on   Zero-Shot LLM Classification Performance
**Authors**: Matteo Melis, Gabriella Lapesa, Dennis Assenmacher

**Updated**: 2025-06-23T12:28:13Z

**Summary**: Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it, and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 Conceptual Elements-building blocks that capture different aspects of hate speech definitions, such as references to the target of hate (individual or groups) or of the potential consequences of it. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.

**Link**: [arxiv](http://arxiv.org/abs/2506.18576v1),  [pdf](http://arxiv.org/pdf/2506.18576v1)

**Tags**: cs.CL cs.CY 



### Lemmanaid: Neuro-Symbolic Lemma Conjecturing
**Authors**: Yousef Alhessi, S√≥lr√∫n Halla Einarsd√≥ttir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone

**Updated**: 2025-06-23T12:21:10Z

**Summary**: Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Lemmanaid outperforms both neural and symbolic methods on test sets from Isabelle's HOL library and from its Archive of Formal Proofs, discovering between 29-39.5% of the gold standard human written lemmas. This is 8-15% more lemmas than the neural-only method. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization.

**Link**: [arxiv](http://arxiv.org/abs/2504.04942v2),  [pdf](http://arxiv.org/pdf/2504.04942v2)

**Tags**: cs.AI cs.LO 



### Step1X-Edit: A Practical Framework for General Image Editing
**Authors**: Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, Daxin Jiang

**Updated**: 2025-06-23T12:11:44Z

**Summary**: In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.

**Link**: [arxiv](http://arxiv.org/abs/2504.17761v4),  [pdf](http://arxiv.org/pdf/2504.17761v4)

**Tags**: cs.CV 



### T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing   Logic-RAG Agent
**Authors**: Hong Qing Yu

**Updated**: 2025-06-23T12:11:15Z

**Summary**: Large language models excel at generating fluent text but frequently struggle with structured reasoning involving temporal constraints, causal relationships, and probabilistic reasoning. To address these limitations, we propose Temporal Causal Probabilistic Description Logic (T-CPDL), an integrated framework that extends traditional Description Logic with temporal interval operators, explicit causal relationships, and probabilistic annotations. We present two distinct variants of T-CPDL: one capturing qualitative temporal relationships through Allen's interval algebra, and another variant enriched with explicit timestamped causal assertions. Both variants share a unified logical structure, enabling complex reasoning tasks ranging from simple temporal ordering to nuanced probabilistic causation. Empirical evaluations on temporal reasoning and causal inference benchmarks confirm that T-CPDL substantially improves inference accuracy, interpretability, and confidence calibration of language model outputs. By delivering transparent reasoning paths and fine-grained temporal and causal semantics, T-CPDL significantly enhances the capability of language models to support robust, explainable, and trustworthy decision-making. This work also lays the groundwork for developing advanced Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially boosting the reasoning capabilities and efficiency of knowledge graph-enhanced RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.18559v1),  [pdf](http://arxiv.org/pdf/2506.18559v1)

**Tags**: cs.AI cs.LO I.2.7; F.4.1 



### Flexible inference of evolutionary accumulation dynamics using uncertain   observational data
**Authors**: Jessica Renz, Morten Brun, Iain G. Johnston

**Updated**: 2025-06-23T12:02:25Z

**Summary**: Understanding and predicting evolutionary accumulation pathways is a key objective in many fields of research, ranging from classical evolutionary biology to diverse applications in medicine. In this context, we are often confronted with the problem that data is sparse and uncertain. To use the available data as best as possible, inference approaches that can handle this uncertainty are required. One way that allows us to use not only cross-sectional data, but also phylogenetic related and longitudinal data, is using `hypercubic inference' models. In this article we introduce HyperLAU, a new algorithm for hypercubic inference that makes it possible to use datasets including uncertainties for learning evolutionary pathways. Expanding the flexibility of accumulation modelling, HyperLAU allows us to infer dynamic pathways and interactions between features, even when large sets of particular features are unobserved across the source dataset. We show that HyperLAU is able to highlight the main pathways found by other tools, even when up to 50% of the features in the input data are uncertain. Additionally, we demonstrate how it can help to overcome possible biases that can occur then reducing the used data by excluding uncertain parts. We illustrate the approach with a case study on multidrug resistance in tuberculosis, showing that HyperLAU allows more flexible data and provides new information about evolutionary pathways compared to existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2502.05872v2),  [pdf](http://arxiv.org/pdf/2502.05872v2)

**Tags**: q-bio.PE 



### Accurate early detection of Parkinson's disease from SPECT imaging   through Convolutional Neural Networks
**Authors**: R. Prashanth

**Updated**: 2025-06-23T11:59:45Z

**Summary**: Early and accurate detection of Parkinson's disease (PD) is a crucial diagnostic challenge carrying immense clinical significance, for effective treatment regimens and patient management. For instance, a group of subjects termed SWEDD who are clinically diagnosed as PD, but show normal Single Photon Emission Computed Tomography (SPECT) scans, change their diagnosis as non-PD after few years of follow up, and in the meantime, they are treated with PD medications which do more harm than good. In this work, machine learning models are developed using features from SPECT images to detect early PD and SWEDD subjects from normal. These models were observed to perform with high accuracy. It is inferred from the study that these diagnostic models carry potential to help PD clinicians in the diagnostic process

**Link**: [arxiv](http://arxiv.org/abs/2412.05348v2),  [pdf](http://arxiv.org/pdf/2412.05348v2)

**Tags**: eess.IV cs.CV cs.LG stat.AP 



### AutoPDL: Automatic Prompt Optimization for LLM Agents
**Authors**: Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel

**Updated**: 2025-06-23T11:56:03Z

**Summary**: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.06\pm15.3$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.04365v2),  [pdf](http://arxiv.org/pdf/2504.04365v2)

**Tags**: cs.LG cs.AI cs.PL 



### Security Assessment of DeepSeek and GPT Series Models against Jailbreak   Attacks
**Authors**: Xiaodong Wu, Xiangman Li, Jianbing Ni

**Updated**: 2025-06-23T11:53:31Z

**Summary**: The widespread deployment of large language models (LLMs) has raised critical concerns over their vulnerability to jailbreak attacks, i.e., adversarial prompts that bypass alignment mechanisms and elicit harmful or policy-violating outputs. While proprietary models like GPT-4 have undergone extensive evaluation, the robustness of emerging open-source alternatives such as DeepSeek remains largely underexplored, despite their growing adoption in real-world applications. In this paper, we present the first systematic jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and GPT-4 using the HarmBench benchmark. We evaluate seven representative attack strategies across 510 harmful behaviors categorized by both function and semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE) architecture introduces routing sparsity that offers selective robustness against optimization-based attacks such as TAP-T, but leads to significantly higher vulnerability under prompt-based and manually engineered attacks. In contrast, GPT-4 Turbo demonstrates stronger and more consistent safety alignment across diverse behaviors, likely due to its dense Transformer design and reinforcement learning from human feedback. Fine-grained behavioral analysis and case studies further show that DeepSeek often routes adversarial prompts to under-aligned expert modules, resulting in inconsistent refusal behaviors. These findings highlight a fundamental trade-off between architectural efficiency and alignment generalization, emphasizing the need for targeted safety tuning and modular alignment strategies to ensure secure deployment of open-source LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.18543v1),  [pdf](http://arxiv.org/pdf/2506.18543v1)

**Tags**: cs.CR cs.AI 



### The more accurately the metal-dependent star formation rate is modeled,   the larger the predicted excess of binary black hole mergers
**Authors**: Cecilia Sgalletta, Michela Mapelli, Lumen Boco, Filippo Santoliquido, M. Celeste Artale, Giuliano Iorio, Andrea Lapi, Mario Spera

**Updated**: 2025-06-23T11:50:47Z

**Summary**: As the number of gravitational-wave detections grows, the merger rate of binary black holes (BBHs) can help us to constrain their formation, the properties of their progenitors, and their birth environment. Here, we aim to address the impact of the metal-dependent star formation rate (SFR) on the BBH merger rate. To this end, we have developed a fully data-driven approach to model the metal-dependent SFR and coupled it to BBH evolution. We have adopted the most up-to-date scaling relations, based on recent observational results, and we have studied how the BBH merger rate density varies over a wide grid of galaxy and binary evolution parameters. Our results show that including a realistic metal-dependent SFR evolution yields a value of the merger rate density which is too high compared to the one inferred from gravitational-wave data. Moreover, variations in the SFR in low-mass galaxies ($M_\ast \lesssim 10^8 \mathrm{M}_{\odot}$) do not contribute more than a factor $\sim 2$ to the overall merger rate density at redshift $z=0$. These results suggest that the discrepancy between the BBH merger rate density inferred from data and theoretical models is not caused by approximations in the treatment of the metal-dependent SFR, but rather stems from stellar evolution models and/or BBH formation channels.

**Link**: [arxiv](http://arxiv.org/abs/2410.21401v2),  [pdf](http://arxiv.org/pdf/2410.21401v2)

**Tags**: astro-ph.HE astro-ph.GA 



### LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety   Inconsistencies
**Authors**: Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting

**Updated**: 2025-06-23T11:45:09Z

**Summary**: Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we conduct a large-scale, comprehensive safety evaluation of the current LLM landscape. For this purpose, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, with category-wise annotations. Our extensive experiments on 39 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in category crime_tax for Italian but remains safe in other languages. Similar inconsistencies can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure responsible usage across diverse communities.

**Link**: [arxiv](http://arxiv.org/abs/2412.15035v3),  [pdf](http://arxiv.org/pdf/2412.15035v3)

**Tags**: cs.CL 



### Affordable AI Assistants with Knowledge Graph of Thoughts
**Authors**: Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, J√≥n Gunnar Hannesson, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, Nils Blach, Haiqiang Zhang, Tao Zhang, Peiran Ma, Grzegorz Kwa≈õniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler

**Updated**: 2025-06-23T11:43:03Z

**Summary**: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively while also minimizing bias and noise. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover, harnessing a smaller model dramatically reduces operational costs by over 36x compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a scalable, affordable, versatile, and high-performing solution for AI assistants.

**Link**: [arxiv](http://arxiv.org/abs/2504.02670v4),  [pdf](http://arxiv.org/pdf/2504.02670v4)

**Tags**: cs.AI cs.CL cs.IR cs.LG 



### Embedded FPGA Acceleration of Brain-Like Neural Networks: Online   Learning to Scalable Inference
**Authors**: Muhammad Ihsan Al Hafiz, Naresh Ravichandran, Anders Lansner, Pawel Herman, Artur Podobas

**Updated**: 2025-06-23T11:35:20Z

**Summary**: Edge AI applications increasingly require models that can learn and adapt on-device with minimal energy budget. Traditional deep learning models, while powerful, are often overparameterized, energy-hungry, and dependent on cloud connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian Confidence Propagation Neural Network (BCPNN), propose a neuromorphic alternative by mimicking cortical architecture and biologically-constrained learning. They offer sparse architectures with local learning rules and unsupervised/semi-supervised learning, making them well-suited for low-power edge intelligence. However, existing BCPNN implementations rely on GPUs or datacenter FPGAs, limiting their applicability to embedded systems. This work presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+ SoC using High-Level Synthesis. We implement both online learning and inference-only kernels with support for variable and mixed precision. Evaluated on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to 17.5x latency and 94% energy savings over ARM baselines, without sacrificing accuracy. This work enables practical neuromorphic computing on edge devices, bridging the gap between brain-like learning and real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.18530v1),  [pdf](http://arxiv.org/pdf/2506.18530v1)

**Tags**: cs.AR cs.AI 



### Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts,   or Black Magic?
**Authors**: Jean-Baptiste D√∂derlein, Nguessan Hermann Kouadio, Mathieu Acher, Djamel Eddine Khelladi, Benoit Combemale

**Updated**: 2025-06-23T11:31:33Z

**Summary**: Language models are promising solutions for tackling increasing complex problems. In software engineering, they recently gained attention in code assistants, which generate programs from a natural language task description (prompt). They have the potential to save time and effort but remain poorly understood, limiting their optimal use. In this article, we investigate the impact of input variations on two configurations of a language model, focusing on parameters such as task description, surrounding context, model creativity, and the number of generated solutions. We design specific operators to modify these inputs and apply them to three LLM-based code assistants (Copilot, Codex, StarCoder2) and two benchmarks representing algorithmic problems (HumanEval, LeetCode). Our study examines whether these variations significantly affect program quality and how these effects generalize across models. Our results show that varying input parameters can greatly improve performance, achieving up to 79.27% success in one-shot generation compared to 22.44% for Codex and 31.1% for Copilot in default settings. Actioning this potential in practice is challenging due to the complex interplay in our study - the optimal settings for temperature, prompt, and number of generated solutions vary by problem. Reproducing our study with StarCoder2 confirms these findings, indicating they are not model-specific. We also uncover surprising behaviors (e.g., fully removing the prompt can be effective), revealing model brittleness and areas for improvement.

**Link**: [arxiv](http://arxiv.org/abs/2210.14699v3),  [pdf](http://arxiv.org/pdf/2210.14699v3)

**Tags**: cs.SE cs.CL cs.PL 68T50 



### DDOT: A Derivative-directed Dual-decoder Ordinary Differential Equation   Transformer for Dynamic System Modeling
**Authors**: Yang Chang, Kuang-Da Wang, Ping-Chun Hsieh, Cheng-Kuan Lin, Wen-Chih Peng

**Updated**: 2025-06-23T11:24:52Z

**Summary**: Uncovering the underlying ordinary differential equations (ODEs) that govern dynamic systems is crucial for advancing our understanding of complex phenomena. Traditional symbolic regression methods often struggle to capture the temporal dynamics and intervariable correlations inherent in ODEs. ODEFormer, a state-of-the-art method for inferring multidimensional ODEs from single trajectories, has made notable progress. However, its focus on single-trajectory evaluation is highly sensitive to initial starting points, which may not fully reflect true performance. To address this, we propose the divergence difference metric (DIV-diff), which evaluates divergence over a grid of points within the target region, offering a comprehensive and stable analysis of the variable space. Alongside, we introduce DDOT (Derivative-Directed Dual-Decoder Ordinary Differential Equation Transformer), a transformer-based model designed to reconstruct multidimensional ODEs in symbolic form. By incorporating an auxiliary task predicting the ODE's derivative, DDOT effectively captures both structure and dynamic behavior. Experiments on ODEBench show DDOT outperforms existing symbolic regression methods, achieving an absolute improvement of 4.58% and 1.62% in $P(R^2 > 0.9)$ for reconstruction and generalization tasks, respectively, and an absolute reduction of 3.55% in DIV-diff. Furthermore, DDOT demonstrates real-world applicability on an anesthesia dataset, highlighting its practical impact.

**Link**: [arxiv](http://arxiv.org/abs/2506.18522v1),  [pdf](http://arxiv.org/pdf/2506.18522v1)

**Tags**: cs.LG 



### Boosting Virtual Agent Learning and Reasoning: A Step-Wise,   Multi-Dimensional, and Generalist Reward Model with Benchmark
**Authors**: Bingchen Miao, Yang Wu, Minghe Gao, Qifan Yu, Wendong Bu, Wenqiao Zhang, Yunfei Li, Siliang Tang, Tat-Seng Chua, Juncheng Li

**Updated**: 2025-06-23T11:17:25Z

**Summary**: The development of Generalist Virtual Agents (GVAs) has shown significant promise in autonomous task execution. However, current training paradigms face critical limitations, including reliance on outcome supervision and labor-intensive human annotations. To address these challenges, we propose Similar, a Step-Wise Multi-Dimensional Generalist Reward Model, which offers fine-grained signals for agent training and can choose better action for inference-time scaling. Specifically, we begin by systematically defining five dimensions for evaluating agent actions. Building on this framework, we design an MCTS-P algorithm to automatically collect and annotate step-wise, five-dimensional agent execution data. Using this data, we train Similar with the Triple-M strategy. Furthermore, we introduce the first benchmark in the virtual agent domain for step-wise, multi-dimensional reward model training and evaluation, named SRM. This benchmark consists of two components: SRMTrain, which serves as the training set for Similar, and SRMEval, a manually selected test set for evaluating the reward model. Experimental results demonstrate that Similar, through its step-wise, multi-dimensional assessment and synergistic gain, provides GVAs with effective intermediate signals during both training and inference-time scaling. The project is available at https://github.com/antgroup/Similar.

**Link**: [arxiv](http://arxiv.org/abs/2503.18665v2),  [pdf](http://arxiv.org/pdf/2503.18665v2)

**Tags**: cs.CV 



### ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for   Detection of Bias, Discrimination and Stereotyping
**Authors**: Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Dhiman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy

**Updated**: 2025-06-23T11:11:32Z

**Summary**: The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.

**Link**: [arxiv](http://arxiv.org/abs/2502.02072v2),  [pdf](http://arxiv.org/pdf/2502.02072v2)

**Tags**: cs.CL cs.AI cs.CY 



### HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge
**Authors**: Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen, Kaili Ma, Hongzhi Chen, James Cheng

**Updated**: 2025-06-23T11:08:00Z

**Summary**: Graph-based Retrieval-Augmented Generation (RAG) methods have significantly enhanced the performance of large language models (LLMs) in domain-specific tasks. However, existing RAG methods do not adequately utilize the naturally inherent hierarchical knowledge in human cognition, which limits the capabilities of RAG systems. In this paper, we introduce a new RAG approach, called HiRAG, which utilizes hierarchical knowledge to enhance the semantic understanding and structure capturing capabilities of RAG systems in the indexing and retrieval processes. Our extensive experiments demonstrate that HiRAG achieves significant performance improvements over the state-of-the-art baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2503.10150v2),  [pdf](http://arxiv.org/pdf/2503.10150v2)

**Tags**: cs.CL cs.AI 



### MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis
**Authors**: Yuting Zhang, Kaishen Yuan, Hao Lu, Yutao Yue, Jintai Chen, Kaishun Wu

**Updated**: 2025-06-23T11:06:31Z

**Summary**: Accurate and interpretable multi-disease diagnosis remains a critical challenge in medical research, particularly when leveraging heterogeneous multimodal medical data. Current approaches often rely on single-modal data, limiting their ability to comprehensively understand complex diseases. To address this, we propose MedTVT-R1, a novel Multimodal Large Language Model (MLLM) framework designed to integrate clinical multimodal data for reasoning and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction dataset that provides question-answer pairs for physiological-level interpretations and disease-level diagnoses with a Chain of Evidence approach. MedTVT-R1 incorporates a modality perception layer to capture inter-modal dependencies and adaptively weight modality contributions. Additionally, we employ Group Relative Policy Optimization (GRPO)-based Reinforcement Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning. Experimental results demonstrate MedTVT-R1's superiority in multimodal feature utilization and multi-disease diagnosis, offering significant potential for clinical applications such as diagnostic report generation and comorbidity reasoning. The dataset and code are available at https://github.com/keke-nice/MedTVT-R1.

**Link**: [arxiv](http://arxiv.org/abs/2506.18512v1),  [pdf](http://arxiv.org/pdf/2506.18512v1)

**Tags**: cs.CV 



### Standard Applicability Judgment and Cross-jurisdictional Reasoning: A   RAG-based Framework for Medical Device Compliance
**Authors**: Yu Han, Aaron Ceross, Jeroen H. M. Bergmann

**Updated**: 2025-06-23T11:04:58Z

**Summary**: Identifying the appropriate regulatory standard applicability remains a critical yet understudied challenge in medical device compliance, frequently necessitating expert interpretation of fragmented and heterogeneous documentation across different jurisdictions. To address this challenge, we introduce a modular AI system that leverages a retrieval-augmented generation (RAG) pipeline to automate standard applicability determination. Given a free-text device description, our system retrieves candidate standards from a curated corpus and uses large language models to infer jurisdiction-specific applicability, classified as Mandatory, Recommended, or Not Applicable, with traceable justifications. We construct an international benchmark dataset of medical device descriptions with expert-annotated standard mappings, and evaluate our system against retrieval-only, zero-shot, and rule-based baselines. The proposed approach attains a classification accuracy of 73% and a Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying relevant regulatory standards. We introduce the first end-to-end system for standard applicability reasoning, enabling scalable and interpretable AI-supported regulatory science. Notably, our region-aware RAG agent performs cross-jurisdictional reasoning between Chinese and U.S. standards, supporting conflict resolution and applicability justification across regulatory frameworks.

**Link**: [arxiv](http://arxiv.org/abs/2506.18511v1),  [pdf](http://arxiv.org/pdf/2506.18511v1)

**Tags**: cs.AI 



### Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich   Transcripts
**Authors**: Duygu Altinok

**Updated**: 2025-06-23T11:04:20Z

**Summary**: Accurate detection of disfluencies in spoken language is crucial for enhancing the performance of automatic speech and language processing systems, as well as fostering the development of more inclusive speech and language technologies. Leveraging the growing trend of large language models (LLMs) as versatile learners capable of processing both lexical and non-lexical inputs (e.g., audio and video), we propose a novel approach to transcribing disfluencies as explicit tokens with timestamps, enabling the generation of fully annotated disfluency-rich transcripts. Our method integrates acoustic representations extracted from an audio encoder with textual inputs of varying quality: clean transcriptions without disfluencies, time-aligned transcriptions from aligners, or outputs from phoneme-based ASR models -- all of which may contain imperfections. Importantly, our experiments demonstrate that textual inputs do not need to be flawless. As long as they include timestamp-related cues, LLMs can effectively smooth the input and produce fully disfluency-annotated transcripts, underscoring their robustness in handling imperfect hints.

**Link**: [arxiv](http://arxiv.org/abs/2506.18510v1),  [pdf](http://arxiv.org/pdf/2506.18510v1)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### Indeterminate Probability Theory
**Authors**: Tao Yang, Chuang Liu, Xiaofeng Ma, Weijia Lu, Ning Wu, Bingyang Li, Zhifei Yang, Peng Liu, Lin Sun, Xiaodong Zhang, Can Zhang

**Updated**: 2025-06-23T10:56:46Z

**Summary**: Complex continuous or mixed joint distributions (e.g., P(Y | z_1, z_2, ..., z_N)) generally lack closed-form solutions, often necessitating approximations such as MCMC. This paper proposes Indeterminate Probability Theory (IPT), which makes the following contributions: (1) An observer-centered framework in which experimental outcomes are represented as distributions combining ground truth with observation error; (2) The introduction of three independence candidate axioms that enable a two-phase probabilistic inference framework; (3) The derivation of closed-form solutions for arbitrary complex joint distributions under this framework. Both the Indeterminate Probability Neural Network (IPNN) model and the non-neural multivariate time series forecasting application demonstrate IPT's effectiveness in modeling high-dimensional distributions, with successful validation up to 1000 dimensions. Importantly, IPT is consistent with classical probability theory and subsumes the frequentist equation in the limit of vanishing observation error.

**Link**: [arxiv](http://arxiv.org/abs/2303.11536v2),  [pdf](http://arxiv.org/pdf/2303.11536v2)

**Tags**: cs.LG cs.AI cs.CV math.ST stat.ML stat.TH 



### Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks:   Strengths, Weaknesses, and Domain-Specific Performance
**Authors**: Wael Etaiwi, Bushra Alhijawi

**Updated**: 2025-06-23T10:52:54Z

**Summary**: The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.

**Link**: [arxiv](http://arxiv.org/abs/2506.18501v1),  [pdf](http://arxiv.org/pdf/2506.18501v1)

**Tags**: cs.CL cs.AI 



### Infrared observations reveal the reprocessing envelope in the tidal   disruption event AT 2019azh
**Authors**: Thomas M. Reynolds, Lars Thomsen, Seppo Mattila, Takashi Nagao, Joseph P. Anderson, Franz E. Bauer, Panos Charalampopoulos, Lixin Dai, Sara Faris, Mariusz Gromadzki, Claudia P. Guti√©rrez, Hanin Kuncarayakti, Cosimo Inserra, Erkki Kankare, Timo Kravtsov, Shane Moran, Phil Wiseman

**Updated**: 2025-06-23T10:43:04Z

**Summary**: Tidal disruption events (TDEs) are expected to release much of their energy in the far-ultraviolet (UV), which we do not observe directly. However, infrared (IR) observations can observe re-radiation of the optical/UV emission from dust, and if this dust is observed in the process of sublimation, we can infer the un-observed UV radiated energy. TDEs have also been predicted to show spectra shallower than a blackbody in the IR, but this has not yet been observed. We present near/mid-IR observations of the TDE AT 2019azh spanning from -3 d before peak until >1750 d after. We evaluate these observations for consistency with dust emission or direct emission from the TDE. We fit the IR data with a modified blackbody associated with dust emission. The UV+optical+IR data are compared with simulated spectra produced from general relativistic radiation magnetohydrodynamics simulations of super-Eddington accretion. We model the data at later times (> 200 d) as an IR echo. The IR data at the maximum light can not be self-consistently fit with dust emission. Instead, the data can be better fit with a reprocessing model, with the IR excess arising due to the absorption opacity being dominated by free-free processes in the dense reprocessing envelope. We infer a large viewing angle of $\sim$60$^{\circ}$, consistent with previously reported X-ray observations, and a tidally disrupted star with mass > 2 M$_{\odot}$. The IR emission at later times is consistent with cool dust emission. We model these data as an IR echo and find that the dust is distant (0.65 pc), and clumpy, with a low covering factor. We show that TDEs can have an IR excess not arising from dust and that IR observations at early times can constrain the viewing angle for the TDE in the unified model. Near-IR observations are therefore essential to distinguish between hot dust and a non-thermal IR excess.

**Link**: [arxiv](http://arxiv.org/abs/2506.18489v1),  [pdf](http://arxiv.org/pdf/2506.18489v1)

**Tags**: astro-ph.HE 



## Keyword: LLM Deployment 
 ### FilMaster: Bridging Cinematic Principles and Generative AI for Automated   Film Generation
**Authors**: Kaiyi Huang, Yukun Huang, Xintao Wang, Zinan Lin, Xuefei Ning, Pengfei Wan, Di Zhang, Yu Wang, Xihui Liu

**Updated**: 2025-06-23T17:59:16Z

**Summary**: AI-driven content creation has shown potential in film production. However, existing film generation systems struggle to implement cinematic principles and thus fail to generate professional-quality films, particularly lacking diverse camera language and cinematic rhythm. This results in templated visuals and unengaging narratives. To address this, we introduce FilMaster, an end-to-end AI system that integrates real-world cinematic principles for professional-grade film generation, yielding editable, industry-standard outputs. FilMaster is built on two key principles: (1) learning cinematography from extensive real-world film data and (2) emulating professional, audience-centric post-production workflows. Inspired by these principles, FilMaster incorporates two stages: a Reference-Guided Generation Stage which transforms user input to video clips, and a Generative Post-Production Stage which transforms raw footage into audiovisual outputs by orchestrating visual and auditory elements for cinematic rhythm. Our generation stage highlights a Multi-shot Synergized RAG Camera Language Design module to guide the AI in generating professional camera language by retrieving reference clips from a vast corpus of 440,000 film clips. Our post-production stage emulates professional workflows by designing an Audience-Centric Cinematic Rhythm Control module, including Rough Cut and Fine Cut processes informed by simulated audience feedback, for effective integration of audiovisual elements to achieve engaging content. The system is empowered by generative AI models like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a comprehensive benchmark for evaluating AI-generated films. Extensive experiments show FilMaster's superior performance in camera language design and cinematic rhythm control, advancing generative AI in professional filmmaking.

**Link**: [arxiv](http://arxiv.org/abs/2506.18899v1),  [pdf](http://arxiv.org/pdf/2506.18899v1)

**Tags**: cs.CV 



### Vision as a Dialect: Unifying Visual Understanding and Generation via   Text-Aligned Representations
**Authors**: Jiaming Han, Hao Chen, Yang Zhao, Hanyu Wang, Qi Zhao, Ziyan Yang, Hao He, Xiangyu Yue, Lu Jiang

**Updated**: 2025-06-23T17:59:14Z

**Summary**: This paper presents a multimodal framework that attempts to unify visual understanding and generation within a shared discrete semantic representation. At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into discrete tokens using a text-aligned codebook projected from a large language model's (LLM) vocabulary. By integrating vision and text into a unified space with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input and output through a shared interface, without the need for modality-specific designs. Additionally, we propose scale-adaptive encoding and decoding to balance efficiency and visual detail, along with a generative de-tokenizer to produce high-fidelity visual outputs. To address diverse decoding needs, we utilize two complementary de-tokenizers: a fast autoregressive model and a diffusion-based model. To enhance modality fusion, we investigate advanced pre-training tasks, demonstrating improvements in both visual understanding and generation. Experiments across benchmarks show that Tar matches or surpasses existing multimodal LLM methods, achieving faster convergence and greater training efficiency. Code, models, and data are available at https://tar.csuhan.com

**Link**: [arxiv](http://arxiv.org/abs/2506.18898v1),  [pdf](http://arxiv.org/pdf/2506.18898v1)

**Tags**: cs.CV cs.AI cs.CL cs.MM 



### ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought   Reasoning in LLMs
**Authors**: Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang

**Updated**: 2025-06-23T17:59:02Z

**Summary**: Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux

**Link**: [arxiv](http://arxiv.org/abs/2506.18896v1),  [pdf](http://arxiv.org/pdf/2506.18896v1)

**Tags**: cs.CL 



### Steering Conceptual Bias via Transformer Latent-Subspace Activation
**Authors**: Vansh Sharma, Venkat Raman

**Updated**: 2025-06-23T17:56:34Z

**Summary**: This work examines whether activating latent subspaces in language models (LLMs) can steer scientific code generation toward a specific programming language. Five causal LLMs were first evaluated on scientific coding prompts to quantify their baseline bias among four programming languages. A static neuron-attribution method, perturbing the highest activated MLP weight for a C++ or CPP token, proved brittle and exhibited limited generalization across prompt styles and model scales. To address these limitations, a gradient-refined adaptive activation steering framework (G-ACT) was developed: per-prompt activation differences are clustered into a small set of steering directions, and lightweight per-layer probes are trained and refined online to select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably biases generation towards the CPP language by increasing the average probe classification accuracy by 15% and the early layers (0-6) improving the probe classification accuracy by 61.5% compared to the standard ACT framework. For LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted injections at key layers still improve language selection. Although per-layer probing introduces a modest inference overhead, it remains practical by steering only a subset of layers and enables reproducible model behavior. These results demonstrate a scalable, interpretable and efficient mechanism for concept-level control for practical agentic systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.18887v1),  [pdf](http://arxiv.org/pdf/2506.18887v1)

**Tags**: cs.AI cs.LG cs.SY eess.SY I.2.7; I.2.6; I.2.1; D.3.3; C.4 



### OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory,   Compositional, and Transformative Generalization
**Authors**: Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song

**Updated**: 2025-06-23T17:51:40Z

**Summary**: Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18880v1),  [pdf](http://arxiv.org/pdf/2506.18880v1)

**Tags**: cs.CL cs.AI 



### CommVQ: Commutative Vector Quantization for KV Cache Compression
**Authors**: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

**Updated**: 2025-06-23T17:50:11Z

**Summary**: Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

**Link**: [arxiv](http://arxiv.org/abs/2506.18879v1),  [pdf](http://arxiv.org/pdf/2506.18879v1)

**Tags**: cs.CL cs.AI 



### Talking to GDELT Through Knowledge Graphs
**Authors**: Audun Myers, Max Vargas, Sinan G. Aksoy, Cliff Joslyn, Benjamin Wilson, Lee Burke, Tom Grimes

**Updated**: 2025-06-24T10:10:56Z

**Summary**: In this work we study various Retrieval Augmented Regeneration (RAG) approaches to gain an understanding of the strengths and weaknesses of each approach in a question-answering analysis. To gain this understanding we use a case-study subset of the Global Database of Events, Language, and Tone (GDELT) dataset as well as a corpus of raw text scraped from the online news articles. To retrieve information from the text corpus we implement a traditional vector store RAG as well as state-of-the-art large language model (LLM) based approaches for automatically constructing KGs and retrieving the relevant subgraphs. In addition to these corpus approaches, we develop a novel ontology-based framework for constructing knowledge graphs (KGs) from GDELT directly which leverages the underlying schema of GDELT to create structured representations of global events. For retrieving relevant information from the ontology-based KGs we implement both direct graph queries and state-of-the-art graph retrieval approaches. We compare the performance of each method in a question-answering task. We find that while our ontology-based KGs are valuable for question-answering, automated extraction of the relevant subgraphs is challenging. Conversely, LLM-generated KGs, while capturing event summaries, often lack consistency and interpretability. Our findings suggest benefits of a synergistic approach between ontology and LLM-based KG construction, with proposed avenues toward that end.

**Link**: [arxiv](http://arxiv.org/abs/2503.07584v3),  [pdf](http://arxiv.org/pdf/2503.07584v3)

**Tags**: cs.IR 



### LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement   Learning
**Authors**: Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li

**Updated**: 2025-06-23T16:59:02Z

**Summary**: Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B

**Link**: [arxiv](http://arxiv.org/abs/2506.18841v1),  [pdf](http://arxiv.org/pdf/2506.18841v1)

**Tags**: cs.CL cs.AI cs.LG 



### LED: LLM Enhanced Open-Vocabulary Object Detection without Human Curated   Data Generation
**Authors**: Yang Zhou, Shiyu Zhao, Yuxiao Chen, Zhenting Wang, Can Jin, Dimitris N. Metaxas

**Updated**: 2025-06-23T16:58:26Z

**Summary**: Large foundation models trained on large-scale vision-language data can boost Open-Vocabulary Object Detection (OVD) via synthetic training data, yet the hand-crafted pipelines often introduce bias and overfit to specific prompts. We sidestep this issue by directly fusing hidden states from Large Language Models (LLMs) into detectors-an avenue surprisingly under-explored. This paper presents a systematic method to enhance visual grounding by utilizing decoder layers of the LLM of an MLLM. We introduce a zero-initialized cross-attention adapter to enable efficient knowledge fusion from LLMs to object detectors, a new approach called LED (LLM Enhanced Open-Vocabulary Object Detection). We find that intermediate LLM layers already encode rich spatial semantics; adapting only the early layers yields most of the gain. With Swin-T as the vision encoder, Qwen2-0.5B + LED lifts GroundingDINO by 3.82 % on OmniLabel at just 8.7 % extra GFLOPs, and a larger vision backbone pushes the improvement to 6.22 %. Extensive ablations on adapter variants, LLM scales and fusion depths further corroborate our design.

**Link**: [arxiv](http://arxiv.org/abs/2503.13794v4),  [pdf](http://arxiv.org/pdf/2503.13794v4)

**Tags**: cs.CV cs.AI 



### Understanding Software Engineering Agents: A Study of   Thought-Action-Result Trajectories
**Authors**: Islem Bouzenia, Michael Pradel

**Updated**: 2025-06-23T16:34:52Z

**Summary**: Large Language Model (LLM)-based agents are increasingly employed to automate complex software engineering tasks such as program repair and issue resolution. These agents operate by autonomously generating natural language thoughts, invoking external tools, and iteratively refining their solutions. Despite their widespread adoption, the internal decision-making processes of these agents remain largely unexplored, limiting our understanding of their operational dynamics and failure modes. In this paper, we present a large-scale empirical study of the thought-action-result trajectories of three state-of-the-art LLM-based agents: \textsc{RepairAgent}, \textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs into a common format, capturing 120 trajectories and 2822 LLM interactions focused on program repair and issue resolution. Our study combines quantitative analyses of structural properties, action patterns, and token usage with qualitative assessments of reasoning coherence and feedback integration. We identify key trajectory characteristics such as iteration counts and token consumption, recurring action sequences, and the semantic coherence linking thoughts, actions, and their results. Our findings reveal behavioral motifs and anti-patterns that distinguish successful from failed executions, providing actionable insights for improving agent design, including prompting strategies, failure diagnosis, and anti-pattern detection. We release our dataset and annotation framework to support further research on transparent and robust autonomous software engineering agents.

**Link**: [arxiv](http://arxiv.org/abs/2506.18824v1),  [pdf](http://arxiv.org/pdf/2506.18824v1)

**Tags**: cs.SE cs.AI 



### A Survey on Data Selection for LLM Instruction Tuning
**Authors**: Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu

**Updated**: 2025-06-23T16:30:46Z

**Summary**: Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.

**Link**: [arxiv](http://arxiv.org/abs/2402.05123v2),  [pdf](http://arxiv.org/pdf/2402.05123v2)

**Tags**: cs.CL 



### RWESummary: A Framework and Test for Choosing Large Language Models to   Summarize Real-World Evidence (RWE) Studies
**Authors**: Arjun Mukerji, Michael L. Jackson, Jason Jones, Neil Sanghavi

**Updated**: 2025-06-23T16:28:03Z

**Summary**: Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.

**Link**: [arxiv](http://arxiv.org/abs/2506.18819v1),  [pdf](http://arxiv.org/pdf/2506.18819v1)

**Tags**: cs.CL cs.AI 



### Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large   Language Models
**Authors**: Aradhye Agarwal, Suhas K Ramesh, Ayan Sengupta, Tanmoy Chakraborty

**Updated**: 2025-06-23T16:25:27Z

**Summary**: Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. Selective PEFT, a class of parameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these computational challenges by selectively fine-tuning only a small fraction of the model parameters. Although parameter-efficient, these techniques often fail to match the performance of fully fine-tuned models, primarily due to inherent biases introduced during parameter selection. Traditional selective PEFT techniques use a fixed set of parameters selected using different importance heuristics, failing to capture parameter importance dynamically and often leading to suboptimal performance. We introduce $\text{ID}^3$, a novel selective PEFT method that calculates parameter importance continually, and dynamically unmasks parameters by balancing exploration and exploitation in parameter selection. Our empirical study on 16 tasks spanning natural language understanding, mathematical reasoning and summarization demonstrates the effectiveness of our method compared to fixed-masking selective PEFT techniques. We analytically show that $\text{ID}^3$ reduces the number of gradient updates by a factor of two, enhancing computational efficiency. Since $\text{ID}^3$ is robust to random initialization of neurons and operates directly on the optimization process, it is highly flexible and can be integrated with existing additive and reparametrization-based PEFT techniques such as adapters and LoRA respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.14470v3),  [pdf](http://arxiv.org/pdf/2408.14470v3)

**Tags**: cs.CL 



### PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision   Applications
**Authors**: Pietro Bonazzi, Nicola Farronato, Stefan Zihlmann, Haotong Qin, Michele Magno

**Updated**: 2025-06-24T09:56:22Z

**Summary**: Real-time, on-device segmentation is critical for latency-sensitive and privacy-aware applications like smart glasses and IoT devices. We introduce PicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation model optimized for edge and in-sensor execution, including the Sony IMX500. It builds on a depthwise separable U-Net, with knowledge distillation and fixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2). On COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized model (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it the only model meeting both memory and compute constraints for in-sensor deployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP. These results demonstrate that efficient, promptable segmentation is feasible directly on-camera, enabling privacy-preserving vision without cloud or host processing.

**Link**: [arxiv](http://arxiv.org/abs/2506.18807v2),  [pdf](http://arxiv.org/pdf/2506.18807v2)

**Tags**: cs.CV 



### FORGE: An LLM-driven Framework for Large-Scale Smart Contract   Vulnerability Dataset Construction
**Authors**: Jiachi Chen, Yiming Shen, Jiashuo Zhang, Zihao Li, John Grundy, Zhenzhe Shao, Yanlin Wang, Jiashui Wang, Ting Chen, Zibin Zheng

**Updated**: 2025-06-23T16:03:16Z

**Summary**: High-quality smart contract vulnerability datasets are critical for evaluating security tools and advancing smart contract security research. Two major limitations of current manual dataset construction are (1) labor-intensive and error-prone annotation processes limiting the scale, quality, and evolution of the dataset, and (2) absence of standardized classification rules results in inconsistent vulnerability categories and labeling results across different datasets. To address these limitations, we present FORGE, the first automated approach for constructing smart contract vulnerability datasets. FORGE leverages an LLM-driven pipeline to extract high-quality vulnerabilities from real-world audit reports and classify them according to the CWE, the most widely recognized classification in software security. FORGE employs a divide-and-conquer strategy to extract structured and self-contained vulnerability information from these reports. Additionally, it uses a tree-of-thoughts technique to classify the vulnerability information into the hierarchical CWE classification. To evaluate FORGE's effectiveness, we run FORGE on 6,454 real-world audit reports and generate a dataset comprising 81,390 solidity files and 27,497 vulnerability findings across 296 CWE categories. Manual assessment of the dataset demonstrates high extraction precision and classification consistency with human experts (precision of 95.6% and inter-rater agreement k-$\alpha$ of 0.87). We further validate the practicality of our dataset by benchmarking 13 existing security tools on our dataset. The results reveal the significant limitations in current detection capabilities. Furthermore, by analyzing the severity-frequency distribution patterns through a unified CWE perspective in our dataset, we highlight inconsistency between current smart contract research focus and priorities identified from real-world vulnerabilities...

**Link**: [arxiv](http://arxiv.org/abs/2506.18795v1),  [pdf](http://arxiv.org/pdf/2506.18795v1)

**Tags**: cs.CR cs.SE D.2.4; I.2.7 



### Focus Your Attention: Towards Data-Intuitive Lightweight Vision   Transformers
**Authors**: Suyash Gaurav, Muhammad Farhan Humayun, Jukka Heikkonen, Jatin Chaudhary

**Updated**: 2025-06-23T16:00:57Z

**Summary**: The evolution of Vision Transformers has led to their widespread adaptation to different domains. Despite large-scale success, there remain significant challenges including their reliance on extensive computational and memory resources for pre-training on huge datasets as well as difficulties in task-specific transfer learning. These limitations coupled with energy inefficiencies mainly arise due to the computation-intensive self-attention mechanism. To address these issues, we propose a novel Super-Pixel Based Patch Pooling (SPPP) technique that generates context-aware, semantically rich, patch embeddings to effectively reduce the architectural complexity and improve efficiency. Additionally, we introduce the Light Latent Attention (LLA) module in our pipeline by integrating latent tokens into the attention mechanism allowing cross-attention operations to significantly reduce the time and space complexity of the attention module. By leveraging the data-intuitive patch embeddings coupled with dynamic positional encodings, our approach adaptively modulates the cross-attention process to focus on informative regions while maintaining the global semantic structure. This targeted attention improves training efficiency and accelerates convergence. Notably, the SPPP module is lightweight and can be easily integrated into existing transformer architectures. Extensive experiments demonstrate that our proposed architecture provides significant improvements in terms of computational efficiency while achieving comparable results with the state-of-the-art approaches, highlighting its potential for energy-efficient transformers suitable for edge deployment. (The code is available on our GitHub repository: https://github.com/zser092/Focused-Attention-ViT).

**Link**: [arxiv](http://arxiv.org/abs/2506.18791v1),  [pdf](http://arxiv.org/pdf/2506.18791v1)

**Tags**: cs.CV cs.LG 



### TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation
**Authors**: Kamil Szczepanik, Jaros≈Çaw A. Chudziak

**Updated**: 2025-06-23T15:53:14Z

**Summary**: TRIZ, the Theory of Inventive Problem Solving, is a structured, knowledge-based framework for innovation and abstracting problems to find inventive solutions. However, its application is often limited by the complexity and deep interdisciplinary knowledge required. Advancements in Large Language Models (LLMs) have revealed new possibilities for automating parts of this process. While previous studies have explored single LLMs in TRIZ applications, this paper introduces a multi-agent approach. We propose an LLM-based multi-agent system, called TRIZ agents, each with specialized capabilities and tool access, collaboratively solving inventive problems based on the TRIZ methodology. This multi-agent system leverages agents with various domain expertise to efficiently navigate TRIZ steps. The aim is to model and simulate an inventive process with language agents. We assess the effectiveness of this team of agents in addressing complex innovation challenges based on a selected case study in engineering. We demonstrate the potential of agent collaboration to produce diverse, inventive solutions. This research contributes to the future of AI-driven innovation, showcasing the advantages of decentralized problem-solving in complex ideation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.18783v1),  [pdf](http://arxiv.org/pdf/2506.18783v1)

**Tags**: cs.AI cs.MA 68T07 I.2.11; I.2.7; I.2.8 



### The Impact of Input Order Bias on Large Language Models for Software   Fault Localization
**Authors**: Md Nakhla Rafi, Dong Jae Kim, Tse-Hsun Chen, Shaowei Wang

**Updated**: 2025-06-23T15:51:16Z

**Summary**: Large Language Models (LLMs) have shown significant potential in software engineering tasks such as Fault Localization (FL) and Automatic Program Repair (APR). This study investigates how input order and context size influence LLM performance in FL, a crucial step for many downstream software engineering tasks. We evaluate different method orderings using Kendall Tau distances, including "perfect" (where ground truths appear first) and "worst" (where ground truths appear last), across two benchmarks containing Java and Python projects. Our results reveal a strong order bias: in Java projects, Top-1 FL accuracy drops from 57% to 20% when reversing the order, while in Python projects, it decreases from 38% to approximately 3%. However, segmenting inputs into smaller contexts mitigates this bias, reducing the performance gap in FL from 22% and 6% to just 1% across both benchmarks. We replaced method names with semantically meaningful alternatives to determine whether this bias is due to data leakage. The observed trends remained consistent, suggesting that the bias is not caused by memorization from training data but rather by the inherent effect of input order. Additionally, we explored ordering methods based on traditional FL techniques and metrics, finding that DepGraph's ranking achieves 48% Top-1 accuracy, outperforming simpler approaches such as CallGraph(DFS). These findings highlight the importance of structuring inputs, managing context effectively, and selecting appropriate ordering strategies to enhance LLM performance in FL and other software engineering applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.18750v3),  [pdf](http://arxiv.org/pdf/2412.18750v3)

**Tags**: cs.SE cs.AI cs.LG 



### Existing LLMs Are Not Self-Consistent For Simple Tasks
**Authors**: Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao

**Updated**: 2025-06-23T15:50:21Z

**Summary**: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency -- no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods -- a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at https://github.com/scorpio-nova/llm-self-consistency.

**Link**: [arxiv](http://arxiv.org/abs/2506.18781v1),  [pdf](http://arxiv.org/pdf/2506.18781v1)

**Tags**: cs.CL 



### Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions   During Code Training
**Authors**: Jonathan Cook, Silvia Sapora, Arash Ahmadian, Akbir Khan, Tim Rocktaschel, Jakob Foerster, Laura Ruis

**Updated**: 2025-06-23T15:45:44Z

**Summary**: Training large language models (LLMs) on source code significantly enhances their general-purpose reasoning abilities, but the mechanisms underlying this generalisation are poorly understood. In this paper, we propose Programming by Backprop (PBB) as a potential driver of this effect - teaching a model to evaluate a program for inputs by training on its source code alone, without ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of programs representing simple maths problems and algorithms: one with source code and I/O examples (w/ IO), the other with source code only (w/o IO). We find evidence that LLMs have some ability to evaluate w/o IO programs for inputs in a range of experimental settings, and make several observations. Firstly, PBB works significantly better when programs are provided as code rather than semantically equivalent language descriptions. Secondly, LLMs can produce outputs for w/o IO programs directly, by implicitly evaluating the program within the forward pass, and more reliably when stepping through the program in-context via chain-of-thought. We further show that PBB leads to more robust evaluation of programs across inputs than training on I/O pairs drawn from a distribution that mirrors naturally occurring data. Our findings suggest a mechanism for enhanced reasoning through code training: it allows LLMs to internalise reusable algorithmic abstractions. Significant scope remains for future work to enable LLMs to more effectively learn from symbolic procedures, and progress in this direction opens other avenues like model alignment by training on formal constitutional principles.

**Link**: [arxiv](http://arxiv.org/abs/2506.18777v1),  [pdf](http://arxiv.org/pdf/2506.18777v1)

**Tags**: cs.AI cs.CL cs.LG 



### SEAL: Scaling to Emphasize Attention for Long-Context Retrieval
**Authors**: Changhun Lee, Minsang Seok, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park

**Updated**: 2025-06-23T15:24:16Z

**Summary**: While many advanced LLMs are designed to handle long sequence data, we can still observe notable quality degradation even within the sequence limit. In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over long contexts. We observe that specific attention heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores, and adjusting the strength of these heads boosts the quality of LLMs in long context by a large margin. Built on this insight, we propose a learning-based mechanism that leverages generated data to emphasize these heads. By applying SEAL, we achieve significant improvements in long-context retrieval performance across various tasks and models. Additionally, when combined with existing training-free context extension techniques, SEAL extends the contextual limits of LLMs while maintaining highly reliable outputs.

**Link**: [arxiv](http://arxiv.org/abs/2501.15225v2),  [pdf](http://arxiv.org/pdf/2501.15225v2)

**Tags**: cs.CL cs.AI cs.LG 



### PARALLELPROMPT: Extracting Parallelism from Large Language Model Queries
**Authors**: Steven Kolawole, Keshav Santhanam, Virginia Smith, Pratiksha Thaker

**Updated**: 2025-06-23T15:05:54Z

**Summary**: LLM serving systems typically treat user prompts as monolithic inputs, optimizing inference through decoding tricks or inter-query batching. However, many real-world prompts contain latent semantic parallelism--decomposable structures where subtasks can be executed independently to reduce latency while preserving meaning. We introduce PARALLELPROMPT, the first benchmark for measuring intra-query parallelism in natural user prompts. Our dataset comprises over 37,000 real-world prompts from public LLM chat logs, each annotated with a structured schema capturing task templates, shared context, and iteration inputs. These schemas are extracted using LLM-assisted prompting with rule-based multilingual validation. To evaluate the benefits of decomposition, we provide an execution suite that benchmarks serial vs. parallel strategies, measuring latency, structural adherence, and semantic fidelity. Our results show that intra-query parallelism can be successfully parsed in over 75% of curated datasets, unlocking up to 5x speedups on tasks like translation, comprehension, and comparative analysis, with minimal quality degradation. By releasing this benchmark, curation pipeline, and evaluation suite, we provide the first standardized testbed for studying structure-aware execution in LLM serving pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2506.18728v1),  [pdf](http://arxiv.org/pdf/2506.18728v1)

**Tags**: cs.LG 



### Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs   with POLLUX
**Authors**: Nikita Martynov, Anastasia Mordasheva, Dmitriy Gorbetskiy, Danil Astafurov, Ulyana Isaeva, Elina Basyrova, Sergey Skachkov, Victoria Berestova, Nikolay Ivanov, Valeriia Zanina, Alena Fenogenova

**Updated**: 2025-06-23T15:01:31Z

**Summary**: We introduce POLLUX, a comprehensive open-source benchmark designed to evaluate the generative capabilities of large language models (LLMs) in Russian. Our main contribution is a novel evaluation methodology that enhances the interpretability of LLM assessment. For each task type, we define a set of detailed criteria and develop a scoring protocol where models evaluate responses and provide justifications for their ratings. This enables transparent, criteria-driven evaluation beyond traditional resource-consuming, side-by-side human comparisons. POLLUX includes a detailed, fine-grained taxonomy of 35 task types covering diverse generative domains such as code generation, creative writing, and practical assistant use cases, totaling 2,100 manually crafted and professionally authored prompts. Each task is categorized by difficulty (easy/medium/hard), with experts constructing the dataset entirely from scratch. We also release a family of LLM-as-a-Judge (7B and 32B) evaluators trained for nuanced assessment of generative outputs. This approach provides scalable, interpretable evaluation and annotation tools for model development, effectively replacing costly and less precise human judgments.

**Link**: [arxiv](http://arxiv.org/abs/2505.24616v2),  [pdf](http://arxiv.org/pdf/2505.24616v2)

**Tags**: cs.CL cs.AI 



### LLM-enhanced Interactions in Human-Robot Collaborative Drawing with   Older Adults
**Authors**: Marianne Bossema, Somaya Ben Allouch, Aske Plaat, Rob Saunders

**Updated**: 2025-06-23T14:49:03Z

**Summary**: The goal of this study is to identify factors that support and enhance older adults' creative experiences in human-robot co-creativity. Because the research into the use of robots for creativity support with older adults remains underexplored, we carried out an exploratory case study. We took a participatory approach and collaborated with professional art educators to design a course Drawing with Robots for adults aged 65 and over. The course featured human-human and human-robot drawing activities with various types of robots. We observed collaborative drawing interactions, interviewed participants on their experiences, and analyzed collected data. Findings show that participants preferred acting as curators, evaluating creative suggestions from the robot in a teacher or coach role. When we enhanced a robot with a multimodal Large Language Model (LLM), participants appreciated its spoken dialogue capabilities. They reported however, that the robot's feedback sometimes lacked an understanding of the context, and sensitivity to their artistic goals and preferences. Our findings highlight the potential of LLM-enhanced robots to support creativity and offer future directions for advancing human-robot co-creativity with older adults.

**Link**: [arxiv](http://arxiv.org/abs/2506.18711v1),  [pdf](http://arxiv.org/pdf/2506.18711v1)

**Tags**: cs.HC 



### Benchmarking the Pedagogical Knowledge of Large Language Models
**Authors**: Maxime Leli√®vre, Amy Waldock, Meng Liu, Natalia Vald√©s Aspillaga, Alasdair Mackintosh, Mar√≠a Jos√© Ogando Portela, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod

**Updated**: 2025-06-24T12:36:22Z

**Summary**: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.

**Link**: [arxiv](http://arxiv.org/abs/2506.18710v2),  [pdf](http://arxiv.org/pdf/2506.18710v2)

**Tags**: cs.CL cs.AI 



### Handling Numeric Expressions in Automatic Speech Recognition
**Authors**: Christian Huber, Alexander Waibel

**Updated**: 2025-06-23T14:45:07Z

**Summary**: This paper addresses the problem of correctly formatting numeric expressions in automatic speech recognition (ASR) transcripts. This is challenging since the expected transcript format depends on the context, e.g., 1945 (year) vs. 19:45 (timestamp). We compare cascaded and end-to-end approaches to recognize and format numeric expressions such as years, timestamps, currency amounts, and quantities. For the end-to-end approach, we employed a data generation strategy using a large language model (LLM) together with a text to speech (TTS) model to generate adaptation data. The results on our test data set show that while approaches based on LLMs perform well in recognizing formatted numeric expressions, adapted end-to-end models offer competitive performance with the advantage of lower latency and inference cost.

**Link**: [arxiv](http://arxiv.org/abs/2408.00004v2),  [pdf](http://arxiv.org/pdf/2408.00004v2)

**Tags**: eess.AS cs.AI cs.CL 



### Context-Aware Human Behavior Prediction Using Multimodal Large Language   Models: Challenges and Insights
**Authors**: Yuchen Liu, Lino Lerch, Luigi Palmieri, Andrey Rudenko, Sebastian Koch, Timo Ropinski, Marco Aiello

**Updated**: 2025-06-23T14:43:46Z

**Summary**: Predicting human behavior in shared environments is crucial for safe and efficient human-robot interaction. Traditional data-driven methods to that end are pre-trained on domain-specific datasets, activity types, and prediction horizons. In contrast, the recent breakthroughs in Large Language Models (LLMs) promise open-ended cross-domain generalization to describe various human activities and make predictions in any context. In particular, Multimodal LLMs (MLLMs) are able to integrate information from various sources, achieving more contextual awareness and improved scene understanding. The difficulty in applying general-purpose MLLMs directly for prediction stems from their limited capacity for processing large input sequences, sensitivity to prompt design, and expensive fine-tuning. In this paper, we present a systematic analysis of applying pre-trained MLLMs for context-aware human behavior prediction. To this end, we introduce a modular multimodal human activity prediction framework that allows us to benchmark various MLLMs, input variations, In-Context Learning (ICL), and autoregressive techniques. Our evaluation indicates that the best-performing framework configuration is able to reach 92.8% semantic similarity and 66.1% exact label accuracy in predicting human behaviors in the target frame.

**Link**: [arxiv](http://arxiv.org/abs/2504.00839v2),  [pdf](http://arxiv.org/pdf/2504.00839v2)

**Tags**: cs.RO cs.AI 



### Better Language Model Inversion by Compactly Representing Next-Token   Distributions
**Authors**: Murtaza Nazir, Matthew Finlayson, John X. Morris, Xiang Ren, Swabha Swayamdipta

**Updated**: 2025-06-23T14:39:37Z

**Summary**: Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known.

**Link**: [arxiv](http://arxiv.org/abs/2506.17090v2),  [pdf](http://arxiv.org/pdf/2506.17090v2)

**Tags**: cs.CL 



### LLM-Driven APT Detection for 6G Wireless Networks: A Systematic Review   and Taxonomy
**Authors**: Muhammed Golec, Yaser Khamayseh, Suhib Bani Melhem, Abdulmalik Alwarafy

**Updated**: 2025-06-23T14:37:53Z

**Summary**: Sixth Generation (6G) wireless networks, which are expected to be deployed in the 2030s, have already created great excitement in academia and the private sector with their extremely high communication speed and low latency rates. However, despite the ultra-low latency, high throughput, and AI-assisted orchestration capabilities they promise, they are vulnerable to stealthy and long-term Advanced Persistent Threats (APTs). Large Language Models (LLMs) stand out as an ideal candidate to fill this gap with their high success in semantic reasoning and threat intelligence. In this paper, we present a comprehensive systematic review and taxonomy study for LLM-assisted APT detection in 6G networks. We address five research questions, namely, semantic merging of fragmented logs, encrypted traffic analysis, edge distribution constraints, dataset/modeling techniques, and reproducibility trends, by leveraging most recent studies on the intersection of LLMs, APTs, and 6G wireless networks. We identify open challenges such as explainability gaps, data scarcity, edge hardware limitations, and the need for real-time slicing-aware adaptation by presenting various taxonomies such as granularity, deployment models, and kill chain stages. We then conclude the paper by providing several research gaps in 6G infrastructures for future researchers. To the best of our knowledge, this paper is the first comprehensive systematic review and classification study on LLM-based APT detection in 6G networks.

**Link**: [arxiv](http://arxiv.org/abs/2505.18846v2),  [pdf](http://arxiv.org/pdf/2505.18846v2)

**Tags**: cs.CR 



### NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed   Target Tracking in Unstructured GPS-Denied Environments
**Authors**: Alessandro Saviolo, Giuseppe Loianno

**Updated**: 2025-06-23T14:28:30Z

**Summary**: Autonomous aerial target tracking in unstructured and GPS-denied environments remains a fundamental challenge in robotics. Many existing methods rely on motion capture systems, pre-mapped scenes, or feature-based localization to ensure safety and control, limiting their deployment in real-world conditions. We introduce NOVA, a fully onboard, object-centric framework that enables robust target tracking and collision-aware navigation using only a stereo camera and an IMU. Rather than constructing a global map or relying on absolute localization, NOVA formulates perception, estimation, and control entirely in the target's reference frame. A tightly integrated stack combines a lightweight object detector with stereo depth completion, followed by histogram-based filtering to infer robust target distances under occlusion and noise. These measurements feed a visual-inertial state estimator that recovers the full 6-DoF pose of the robot relative to the target. A nonlinear model predictive controller (NMPC) plans dynamically feasible trajectories in the target frame. To ensure safety, high-order control barrier functions are constructed online from a compact set of high-risk collision points extracted from depth, enabling real-time obstacle avoidance without maps or dense representations. We validate NOVA across challenging real-world scenarios, including urban mazes, forest trails, and repeated transitions through buildings with intermittent GPS loss and severe lighting changes that disrupt feature-based localization. Each experiment is repeated multiple times under similar conditions to assess resilience, showing consistent and reliable performance. NOVA achieves agile target following at speeds exceeding 50 km/h. These results show that high-speed vision-based tracking is possible in the wild using only onboard sensing, with no reliance on external localization or environment assumptions.

**Link**: [arxiv](http://arxiv.org/abs/2506.18689v1),  [pdf](http://arxiv.org/pdf/2506.18689v1)

**Tags**: cs.RO cs.AI 



### "I understand why I got this grade": Automatic Short Answer Grading with   Feedback
**Authors**: Dishank Aggarwal, Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya

**Updated**: 2025-06-23T14:24:28Z

**Summary**: In recent years, there has been a growing interest in using Artificial Intelligence (AI) to automate student assessment in education. Among different types of assessments, summative assessments play a crucial role in evaluating a student's understanding level of a course. Such examinations often involve short-answer questions. However, grading these responses and providing meaningful feedback manually at scale is both time-consuming and labor-intensive. Feedback is particularly important, as it helps students recognize their strengths and areas for improvement. Despite the importance of this task, there is a significant lack of publicly available datasets that support automatic short-answer grading with feedback generation. To address this gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset designed for automatic short-answer grading with feedback. The dataset covers a diverse range of subjects, questions, and answer patterns from multiple engineering domains and contains ~5.8k data points. We incorporate feedback into our dataset by leveraging the generative capabilities of state-of-the-art large language models (LLMs) using our Label-Aware Synthetic Feedback Generation (LASFG) strategy. This paper underscores the importance of enhanced feedback in practical educational settings, outlines dataset annotation and feedback generation processes, conducts a thorough EngSAF analysis, and provides different LLMs-based zero-shot and finetuned baselines for future comparison. The best-performing model (Mistral-7B) achieves an overall accuracy of 75.4% and 58.7% on unseen answers and unseen question test sets, respectively. Additionally, we demonstrate the efficiency and effectiveness of our ASAG system through its deployment in a real-world end-semester exam at a reputed institute.

**Link**: [arxiv](http://arxiv.org/abs/2407.12818v2),  [pdf](http://arxiv.org/pdf/2407.12818v2)

**Tags**: cs.CL cs.AI cs.CY 



### Is There a Case for Conversation Optimized Tokenizers in Large Language   Models?
**Authors**: Raquel Ferrando, Javier Conde, Gonzalo Mart√≠nez, Pedro Reviriego

**Updated**: 2025-06-23T14:18:46Z

**Summary**: The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.

**Link**: [arxiv](http://arxiv.org/abs/2506.18674v1),  [pdf](http://arxiv.org/pdf/2506.18674v1)

**Tags**: cs.CL cs.AI 



### Spectrum Opportunities for the Wireless Future: From Direct-to-Device   Satellite Applications to 6G Cellular
**Authors**: Theodore S. Rappaport, Todd E. Humphreys, Shuai Nie

**Updated**: 2025-06-23T14:16:36Z

**Summary**: For the next-generation wireless networks and beyond, both the upper mid-band (7 GHz-24 GHz) and terahertz (100 GHz-1 THz) spectra are gaining global attention from service providers, academic research groups, policy makers, and standards organizations. This article provides an in-depth analysis of recent regulatory rulings and spectrum preferences issued by international standard bodies such as the International Telecommunications Union and Federal Communications Commission as they seek to identify feasible bands for future wireless networks. In this paper, we present the promising spectrum allocations earmarked for 6G and beyond. We also provide exemplars that illuminate the passive service protections and spectrum feasibility for coexistence between terrestrial wireless networks and satellites and other non-terrestrial networks (NTN), and discuss key technical constraints that will challenge future spectrum use for the wireless industry. The findings highlight promising frequency bands while addressing regulatory and technological challenges for future wireless service deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.18672v1),  [pdf](http://arxiv.org/pdf/2506.18672v1)

**Tags**: eess.SY cs.SY 



### Harnessing the Power of Reinforcement Learning for Language-Model-Based   Information Retriever via Query-Document Co-Augmentation
**Authors**: Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou

**Updated**: 2025-06-23T14:14:43Z

**Summary**: Recent studies have proposed leveraging Large Language Models (LLMs) as information retrievers through query rewriting. However, for challenging corpora, we argue that enhancing queries alone is insufficient for robust semantic matching; the LLM should also have sufficient understanding of the corpus by directly handling and augmenting the documents themselves. To this end, we present an LLM-based retriever empowered to augment both user queries and corpus documents, with its policy fully explored via reinforcement learning (RL) and minimal human inductive bias. Notably, we find that simply allowing the LLM to modify documents yields little benefit unless paired with our carefully designed bidirectional RL framework, which enables the LLM to simultaneously learn and collaborate on both query and document augmentation policies. A key technical challenge in realizing such a framework lies in jointly updating both policies during training, where the rewards for the two directions depend on each other, making their entangled reward intractable. Our approach addresses this by introducing a reward sampling strategy and a specifically designed RL algorithm that enables effective training with these sampled rewards. Experimental results demonstrate that our approach significantly enhances LLM-based retrieval performance in both sparse and dense settings, particularly in difficult retrieval domains, and achieves strong cross-benchmark generalization. Our code is released at https://github.com/liujm2001/CoAugRetriever.

**Link**: [arxiv](http://arxiv.org/abs/2506.18670v1),  [pdf](http://arxiv.org/pdf/2506.18670v1)

**Tags**: cs.IR 



### A Random Matrix Analysis of In-context Memorization for Nonlinear   Attention
**Authors**: Zhenyu Liao, Jiaqing Liu, TianQi Hou, Difan Zou, Zenan Ling

**Updated**: 2025-06-23T13:56:43Z

**Summary**: Attention mechanisms have revolutionized machine learning (ML) by enabling efficient modeling of global dependencies across inputs. Their inherently parallelizable structures allow for efficient scaling with the exponentially increasing size of both pretrained data and model parameters. Yet, despite their central role as the computational backbone of modern large language models (LLMs), the theoretical understanding of Attentions, especially in the nonlinear setting, remains limited.   In this paper, we provide a precise characterization of the \emph{in-context memorization error} of \emph{nonlinear Attention}, in the high-dimensional proportional regime where the number of input tokens $n$ and their embedding dimension $p$ are both large and comparable. Leveraging recent advances in the theory of large kernel random matrices, we show that nonlinear Attention typically incurs higher memorization error than linear ridge regression on random inputs. However, this gap vanishes, and can even be reversed, when the input exhibits statistical structure, particularly when the Attention weights align with the input signal direction. Our results reveal how nonlinearity and input structure interact with each other to govern the memorization performance of nonlinear Attention. The theoretical insights are supported by numerical experiments.

**Link**: [arxiv](http://arxiv.org/abs/2506.18656v1),  [pdf](http://arxiv.org/pdf/2506.18656v1)

**Tags**: stat.ML cs.LG math.ST stat.TH 



### C-SEO Bench: Does Conversational SEO Work?
**Authors**: Haritz Puerto, Martin Gubri, Tommaso Green, Seong Joon Oh, Sangdoo Yun

**Updated**: 2025-06-23T13:56:31Z

**Summary**: Large Language Models (LLMs) are transforming search engines into Conversational Search Engines (CSE). Consequently, Search Engine Optimization (SEO) is being shifted into Conversational Search Engine Optimization (C-SEO). We are beginning to see dedicated C-SEO methods for modifying web documents to increase their visibility in CSE responses. However, they are often tested only for a limited breadth of application domains; we do not understand whether certain C-SEO methods would be effective for a broad range of domains. Moreover, existing evaluations consider only a single-actor scenario where only one web document adopts a C-SEO method; in reality, multiple players are likely to competitively adopt the cutting-edge C-SEO techniques, drawing an analogy from the dynamics we have seen in SEO. We present C-SEO Bench, the first benchmark designed to evaluate C-SEO methods across multiple tasks, domains, and number of actors. We consider two search tasks, question answering and product recommendation, with three domains each. We also formalize a new evaluation protocol with varying adoption rates among involved actors. Our experiments reveal that most current C-SEO methods are largely ineffective, contrary to reported results in the literature. Instead, traditional SEO strategies, those aiming to improve the ranking of the source in the LLM context, are significantly more effective. We also observe that as we increase the number of C-SEO adopters, the overall gains decrease, depicting a congested and zero-sum nature of the problem. Our code and data are available at https://github.com/parameterlab/c-seo-bench and https://huggingface.co/datasets/parameterlab/c-seo-bench.

**Link**: [arxiv](http://arxiv.org/abs/2506.11097v2),  [pdf](http://arxiv.org/pdf/2506.11097v2)

**Tags**: cs.CL cs.AI cs.IR 



### ReDit: Reward Dithering for Improved LLM Policy Optimization
**Authors**: Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu

**Updated**: 2025-06-24T07:07:57Z

**Summary**: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages.

**Link**: [arxiv](http://arxiv.org/abs/2506.18631v2),  [pdf](http://arxiv.org/pdf/2506.18631v2)

**Tags**: cs.LG cs.AI cs.CL 



### AggTruth: Contextual Hallucination Detection using Aggregated Attention   Scores in LLMs
**Authors**: Piotr Matys, Jan Eliasz, Konrad Kie≈Çczy≈Ñski, Miko≈Çaj Langner, Teddy Ferdinan, Jan Koco≈Ñ, Przemys≈Çaw Kazienko

**Updated**: 2025-06-23T13:35:05Z

**Summary**: In real-world applications, Large Language Models (LLMs) often hallucinate, even in Retrieval-Augmented Generation (RAG) settings, which poses a significant challenge to their deployment. In this paper, we introduce AggTruth, a method for online detection of contextual hallucinations by analyzing the distribution of internal attention scores in the provided context (passage). Specifically, we propose four different variants of the method, each varying in the aggregation technique used to calculate attention scores. Across all LLMs examined, AggTruth demonstrated stable performance in both same-task and cross-task setups, outperforming the current SOTA in multiple scenarios. Furthermore, we conducted an in-depth analysis of feature selection techniques and examined how the number of selected attention heads impacts detection performance, demonstrating that careful selection of heads is essential to achieve optimal results.

**Link**: [arxiv](http://arxiv.org/abs/2506.18628v1),  [pdf](http://arxiv.org/pdf/2506.18628v1)

**Tags**: cs.AI cs.CL 



### Efficient and Generalizable Speaker Diarization via Structured Pruning   of Self-Supervised Models
**Authors**: Jiangyu Han, Petr P√°lka, Marc Delcroix, Federico Landini, Johan Rohdin, Jan Cernock√Ω, Luk√°≈° Burget

**Updated**: 2025-06-23T13:29:51Z

**Summary**: Self-supervised learning (SSL) models such as WavLM have brought substantial improvements to speaker diarization by providing rich contextual representations. However, the high computational and memory costs of these models hinder their deployment in real-time and resource-constrained scenarios. In this work, we present a comprehensive study on compressing SSL-based diarization models through structured pruning guided by knowledge distillation. Building upon our previous work, we extend the analysis to include pruning objectives based on multiply-accumulate operations (MACs), investigate module-wise and progressive pruning strategies, and examine the impact of training data quantity. Experimental results show that our method reduces model size by up to 80% without degrading performance, achieving up to 4x faster inference on a single GPU. We further perform large-scale evaluations on a diverse compound dataset comprising eight public diarization corpora, where our best pruned model achieves state-of-the-art performance across most conditions. Additionally, we show strong generalization to the CHiME-6 dataset, attaining performance comparable to the third-place system in the CHiME-7 challenge without any domain adaptation. All models and code are publicly released to support reproducibility and future research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18623v1),  [pdf](http://arxiv.org/pdf/2506.18623v1)

**Tags**: eess.AS 



### The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified   Speeches
**Authors**: Alisa Barkar, Mathieu Chollet, Matthieu Labeau, Beatrice Biancardi, Chloe Clavel

**Updated**: 2025-06-23T13:28:33Z

**Summary**: This study examines how large language models understand the concept of persuasiveness in public speaking by modifying speech transcripts from PhD candidates in the "Ma These en 180 Secondes" competition, using the 3MT French dataset. Our contributions include a novel methodology and an interpretable textual feature set integrating rhetorical devices and discourse markers. We prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic shifts between original and generated speech in terms of the new features. Results indicate that GPT-4o applies systematic stylistic modifications rather than optimizing persuasiveness in a human-like manner. Notably, it manipulates emotional lexicon and syntactic structures (such as interrogative and exclamatory clauses) to amplify rhetorical impact.

**Link**: [arxiv](http://arxiv.org/abs/2506.18621v1),  [pdf](http://arxiv.org/pdf/2506.18621v1)

**Tags**: cs.CL 



### API Agents vs. GUI Agents: Divergence and Convergence
**Authors**: Chaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang

**Updated**: 2025-06-23T13:01:02Z

**Summary**: Large language models (LLMs) have evolved beyond simple text generation to power software agents that directly translate natural language commands into tangible actions. While API-based LLM agents initially rose to prominence for their robust automation capabilities and seamless integration with programmatic endpoints, recent progress in multimodal LLM research has enabled GUI-based LLM agents that interact with graphical user interfaces in a human-like manner. Although these two paradigms share the goal of enabling LLM-driven task automation, they diverge significantly in architectural complexity, development workflows, and user interaction models.   This paper presents the first comprehensive comparative study of API-based and GUI-based LLM agents, systematically analyzing their divergence and potential convergence. We examine key dimensions and highlight scenarios in which hybrid approaches can harness their complementary strengths. By proposing clear decision criteria and illustrating practical use cases, we aim to guide practitioners and researchers in selecting, combining, or transitioning between these paradigms. Ultimately, we indicate that continuing innovations in LLM-based automation are poised to blur the lines between API- and GUI-driven agents, paving the way for more flexible, adaptive solutions in a wide range of real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.11069v2),  [pdf](http://arxiv.org/pdf/2503.11069v2)

**Tags**: cs.AI cs.HC 



### Reply to "Emergent LLM behaviors are observationally equivalent to data   leakage"
**Authors**: Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli

**Updated**: 2025-06-23T12:59:34Z

**Summary**: A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.

**Link**: [arxiv](http://arxiv.org/abs/2506.18600v1),  [pdf](http://arxiv.org/pdf/2506.18600v1)

**Tags**: cs.CL cs.GT cs.MA 



### PG-LIO: Photometric-Geometric fusion for Robust LiDAR-Inertial Odometry
**Authors**: Nikhil Khedekar, Kostas Alexis

**Updated**: 2025-06-23T12:37:01Z

**Summary**: LiDAR-Inertial Odometry (LIO) is widely used for accurate state estimation and mapping which is an essential requirement for autonomous robots. Conventional LIO methods typically rely on formulating constraints from the geometric structure sampled by the LiDAR. Hence, in the lack of geometric structure, these tend to become ill-conditioned (degenerate) and fail. Robustness of LIO to such conditions is a necessity for its broader deployment. To address this, we propose PG-LIO, a real-time LIO method that fuses photometric and geometric information sampled by the LiDAR along with inertial constraints from an Inertial Measurement Unit (IMU). This multi-modal information is integrated into a factor graph optimized over a sliding window for real-time operation. We evaluate PG-LIO on multiple datasets that include both geometrically well-conditioned as well as self-similar scenarios. Our method achieves accuracy on par with state-of-the-art LIO in geometrically well-structured settings while significantly improving accuracy in degenerate cases including against methods that also fuse intensity. Notably, we demonstrate only 1 m drift over a 1 km manually piloted aerial trajectory through a geometrically self-similar tunnel at an average speed of 7.5m/s (max speed 10.8 m/s). For the benefit of the community, we shall also release our source code https://github.com/ntnu-arl/mimosa

**Link**: [arxiv](http://arxiv.org/abs/2506.18583v1),  [pdf](http://arxiv.org/pdf/2506.18583v1)

**Tags**: cs.RO 



### A Modular Taxonomy for Hate Speech Definitions and Its Impact on   Zero-Shot LLM Classification Performance
**Authors**: Matteo Melis, Gabriella Lapesa, Dennis Assenmacher

**Updated**: 2025-06-23T12:28:13Z

**Summary**: Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it, and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 Conceptual Elements-building blocks that capture different aspects of hate speech definitions, such as references to the target of hate (individual or groups) or of the potential consequences of it. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.

**Link**: [arxiv](http://arxiv.org/abs/2506.18576v1),  [pdf](http://arxiv.org/pdf/2506.18576v1)

**Tags**: cs.CL cs.CY 



### Cosmic Ray Detection with the IceTop Enhancement
**Authors**: Megha Venugopal

**Updated**: 2025-06-23T12:21:16Z

**Summary**: IceTop is the cosmic-ray detector located on the surface of the IceCube Neutrino Observatory at the South Pole, consisting of 81 pairs of ice-Cherenkov tanks. The rise in the energy threshold of air-shower measurements in IceTop due to accumulating snow emphasized the need for the next generation of IceCube surface detectors. For this purpose, the Surface Array Enhancement (SAE) is set to comprise elevated scintillator panels and radio antennas controlled by hybrid DAQ systems. The detectors of the SAE are also expected to extend to the planned IceCube-Gen2 Surface Array. An initial study with a prototype station is already conducted. We briefly review the SAE and the deployment as well as the calibration status of the upcoming stations of the planned array of 32 stations. The focus of this contribution is on the radio detection of extensive air showers. A preliminary estimation of the position of the shower maximum ($X_\mathrm{max}$), that is sensitive to the primary mass, with data from the 3 antennas of the prototype station was carried out. An extension of the method from previous analyses is also briefly discussed.

**Link**: [arxiv](http://arxiv.org/abs/2506.18566v1),  [pdf](http://arxiv.org/pdf/2506.18566v1)

**Tags**: astro-ph.HE astro-ph.IM 



### Lemmanaid: Neuro-Symbolic Lemma Conjecturing
**Authors**: Yousef Alhessi, S√≥lr√∫n Halla Einarsd√≥ttir, George Granberry, Emily First, Moa Johansson, Sorin Lerner, Nicholas Smallbone

**Updated**: 2025-06-23T12:21:10Z

**Summary**: Automatically conjecturing useful, interesting and novel lemmas would greatly improve automated reasoning tools and lower the bar for formalizing mathematics in proof assistants. It is however a very challenging task for both neural and symbolic approaches. We present the first steps towards a practical neuro-symbolic lemma conjecturing tool, Lemmanaid, that combines Large Language Models (LLMs) and symbolic methods, and evaluate it on proof libraries for the Isabelle proof assistant. We train an LLM to generate lemma templates that describe the shape of a lemma, and use symbolic methods to fill in the details. We compare Lemmanaid against an LLM trained to generate complete lemma statements as well as previous fully symbolic conjecturing methods. Lemmanaid outperforms both neural and symbolic methods on test sets from Isabelle's HOL library and from its Archive of Formal Proofs, discovering between 29-39.5% of the gold standard human written lemmas. This is 8-15% more lemmas than the neural-only method. By leveraging the best of both symbolic and neural methods we can generate useful lemmas for a wide range of input domains, facilitating computer-assisted theory development and formalization.

**Link**: [arxiv](http://arxiv.org/abs/2504.04942v2),  [pdf](http://arxiv.org/pdf/2504.04942v2)

**Tags**: cs.AI cs.LO 



### Step1X-Edit: A Practical Framework for General Image Editing
**Authors**: Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, Guopeng Li, Yuang Peng, Quan Sun, Jingwei Wu, Yan Cai, Zheng Ge, Ranchen Ming, Lei Xia, Xianfang Zeng, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Gang Yu, Daxin Jiang

**Updated**: 2025-06-23T12:11:44Z

**Summary**: In recent years, image editing models have witnessed remarkable and rapid development. The recent unveiling of cutting-edge multimodal models such as GPT-4o and Gemini2 Flash has introduced highly promising image editing capabilities. These models demonstrate an impressive aptitude for fulfilling a vast majority of user-driven editing requirements, marking a significant advancement in the field of image manipulation. However, there is still a large gap between the open-source algorithm with these closed-source models. Thus, in this paper, we aim to release a state-of-the-art image editing model, called Step1X-Edit, which can provide comparable performance against the closed-source models like GPT-4o and Gemini2 Flash. More specifically, we adopt the Multimodal LLM to process the reference image and the user's editing instruction. A latent embedding has been extracted and integrated with a diffusion image decoder to obtain the target image. To train the model, we build a data generation pipeline to produce a high-quality dataset. For evaluation, we develop the GEdit-Bench, a novel benchmark rooted in real-world user instructions. Experimental results on GEdit-Bench demonstrate that Step1X-Edit outperforms existing open-source baselines by a substantial margin and approaches the performance of leading proprietary models, thereby making significant contributions to the field of image editing.

**Link**: [arxiv](http://arxiv.org/abs/2504.17761v4),  [pdf](http://arxiv.org/pdf/2504.17761v4)

**Tags**: cs.CV 



### AutoPDL: Automatic Prompt Optimization for LLM Agents
**Authors**: Claudio Spiess, Mandana Vaziri, Louis Mandel, Martin Hirzel

**Updated**: 2025-06-23T11:56:03Z

**Summary**: The performance of large language models (LLMs) depends on how they are prompted, with choices spanning both the high-level prompting pattern (e.g., Zero-Shot, CoT, ReAct, ReWOO) and the specific prompt content (instructions and few-shot demonstrations). Manually tuning this combination is tedious, error-prone, and specific to a given LLM and task. Therefore, this paper proposes AutoPDL, an automated approach to discovering good LLM agent configurations. Our approach frames this as a structured AutoML problem over a combinatorial space of agentic and non-agentic prompting patterns and demonstrations, using successive halving to efficiently navigate this space. We introduce a library implementing common prompting patterns using the PDL prompt programming language. AutoPDL solutions are human-readable, editable, and executable PDL programs that use this library. This approach also enables source-to-source optimization, allowing human-in-the-loop refinement and reuse. Evaluations across three tasks and seven LLMs (ranging from 3B to 70B parameters) show consistent accuracy gains ($9.06\pm15.3$ percentage points), up to 68.9pp, and reveal that selected prompting strategies vary across models and tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.04365v2),  [pdf](http://arxiv.org/pdf/2504.04365v2)

**Tags**: cs.LG cs.AI cs.PL 



### Security Assessment of DeepSeek and GPT Series Models against Jailbreak   Attacks
**Authors**: Xiaodong Wu, Xiangman Li, Jianbing Ni

**Updated**: 2025-06-23T11:53:31Z

**Summary**: The widespread deployment of large language models (LLMs) has raised critical concerns over their vulnerability to jailbreak attacks, i.e., adversarial prompts that bypass alignment mechanisms and elicit harmful or policy-violating outputs. While proprietary models like GPT-4 have undergone extensive evaluation, the robustness of emerging open-source alternatives such as DeepSeek remains largely underexplored, despite their growing adoption in real-world applications. In this paper, we present the first systematic jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and GPT-4 using the HarmBench benchmark. We evaluate seven representative attack strategies across 510 harmful behaviors categorized by both function and semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE) architecture introduces routing sparsity that offers selective robustness against optimization-based attacks such as TAP-T, but leads to significantly higher vulnerability under prompt-based and manually engineered attacks. In contrast, GPT-4 Turbo demonstrates stronger and more consistent safety alignment across diverse behaviors, likely due to its dense Transformer design and reinforcement learning from human feedback. Fine-grained behavioral analysis and case studies further show that DeepSeek often routes adversarial prompts to under-aligned expert modules, resulting in inconsistent refusal behaviors. These findings highlight a fundamental trade-off between architectural efficiency and alignment generalization, emphasizing the need for targeted safety tuning and modular alignment strategies to ensure secure deployment of open-source LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.18543v1),  [pdf](http://arxiv.org/pdf/2506.18543v1)

**Tags**: cs.CR cs.AI 



### LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety   Inconsistencies
**Authors**: Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting

**Updated**: 2025-06-23T11:45:09Z

**Summary**: Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we conduct a large-scale, comprehensive safety evaluation of the current LLM landscape. For this purpose, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, with category-wise annotations. Our extensive experiments on 39 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in category crime_tax for Italian but remains safe in other languages. Similar inconsistencies can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure responsible usage across diverse communities.

**Link**: [arxiv](http://arxiv.org/abs/2412.15035v3),  [pdf](http://arxiv.org/pdf/2412.15035v3)

**Tags**: cs.CL 



### Affordable AI Assistants with Knowledge Graph of Thoughts
**Authors**: Maciej Besta, Lorenzo Paleari, Jia Hao Andrea Jiang, Robert Gerstenberger, You Wu, J√≥n Gunnar Hannesson, Patrick Iff, Ales Kubicek, Piotr Nyczyk, Diana Khimey, Nils Blach, Haiqiang Zhang, Tao Zhang, Peiran Ma, Grzegorz Kwa≈õniewski, Marcin Copik, Hubert Niewiadomski, Torsten Hoefler

**Updated**: 2025-06-23T11:43:03Z

**Summary**: Large Language Models (LLMs) are revolutionizing the development of AI assistants capable of performing diverse tasks across domains. However, current state-of-the-art LLM-driven agents face significant challenges, including high operational costs and limited success rates on complex benchmarks like GAIA. To address these issues, we propose Knowledge Graph of Thoughts (KGoT), an innovative AI assistant architecture that integrates LLM reasoning with dynamically constructed knowledge graphs (KGs). KGoT extracts and structures task-relevant knowledge into a dynamic KG representation, iteratively enhanced through external tools such as math solvers, web crawlers, and Python scripts. Such structured representation of task-relevant knowledge enables low-cost models to solve complex tasks effectively while also minimizing bias and noise. For example, KGoT achieves a 29% improvement in task success rates on the GAIA benchmark compared to Hugging Face Agents with GPT-4o mini. Moreover, harnessing a smaller model dramatically reduces operational costs by over 36x compared to GPT-4o. Improvements for other models (e.g., Qwen2.5-32B and Deepseek-R1-70B) and benchmarks (e.g., SimpleQA) are similar. KGoT offers a scalable, affordable, versatile, and high-performing solution for AI assistants.

**Link**: [arxiv](http://arxiv.org/abs/2504.02670v4),  [pdf](http://arxiv.org/pdf/2504.02670v4)

**Tags**: cs.AI cs.CL cs.IR cs.LG 



### Embedded FPGA Acceleration of Brain-Like Neural Networks: Online   Learning to Scalable Inference
**Authors**: Muhammad Ihsan Al Hafiz, Naresh Ravichandran, Anders Lansner, Pawel Herman, Artur Podobas

**Updated**: 2025-06-23T11:35:20Z

**Summary**: Edge AI applications increasingly require models that can learn and adapt on-device with minimal energy budget. Traditional deep learning models, while powerful, are often overparameterized, energy-hungry, and dependent on cloud connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian Confidence Propagation Neural Network (BCPNN), propose a neuromorphic alternative by mimicking cortical architecture and biologically-constrained learning. They offer sparse architectures with local learning rules and unsupervised/semi-supervised learning, making them well-suited for low-power edge intelligence. However, existing BCPNN implementations rely on GPUs or datacenter FPGAs, limiting their applicability to embedded systems. This work presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+ SoC using High-Level Synthesis. We implement both online learning and inference-only kernels with support for variable and mixed precision. Evaluated on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to 17.5x latency and 94% energy savings over ARM baselines, without sacrificing accuracy. This work enables practical neuromorphic computing on edge devices, bridging the gap between brain-like learning and real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.18530v1),  [pdf](http://arxiv.org/pdf/2506.18530v1)

**Tags**: cs.AR cs.AI 



### Piloting Copilot, Codex, and StarCoder2: Hot Temperature, Cold Prompts,   or Black Magic?
**Authors**: Jean-Baptiste D√∂derlein, Nguessan Hermann Kouadio, Mathieu Acher, Djamel Eddine Khelladi, Benoit Combemale

**Updated**: 2025-06-23T11:31:33Z

**Summary**: Language models are promising solutions for tackling increasing complex problems. In software engineering, they recently gained attention in code assistants, which generate programs from a natural language task description (prompt). They have the potential to save time and effort but remain poorly understood, limiting their optimal use. In this article, we investigate the impact of input variations on two configurations of a language model, focusing on parameters such as task description, surrounding context, model creativity, and the number of generated solutions. We design specific operators to modify these inputs and apply them to three LLM-based code assistants (Copilot, Codex, StarCoder2) and two benchmarks representing algorithmic problems (HumanEval, LeetCode). Our study examines whether these variations significantly affect program quality and how these effects generalize across models. Our results show that varying input parameters can greatly improve performance, achieving up to 79.27% success in one-shot generation compared to 22.44% for Codex and 31.1% for Copilot in default settings. Actioning this potential in practice is challenging due to the complex interplay in our study - the optimal settings for temperature, prompt, and number of generated solutions vary by problem. Reproducing our study with StarCoder2 confirms these findings, indicating they are not model-specific. We also uncover surprising behaviors (e.g., fully removing the prompt can be effective), revealing model brittleness and areas for improvement.

**Link**: [arxiv](http://arxiv.org/abs/2210.14699v3),  [pdf](http://arxiv.org/pdf/2210.14699v3)

**Tags**: cs.SE cs.CL cs.PL 68T50 



### DUMB and DUMBer: Is Adversarial Training Worth It in the Real World?
**Authors**: Francesco Marchiori, Marco Alecci, Luca Pajola, Mauro Conti

**Updated**: 2025-06-23T11:16:21Z

**Summary**: Adversarial examples are small and often imperceptible perturbations crafted to fool machine learning models. These attacks seriously threaten the reliability of deep neural networks, especially in security-sensitive domains. Evasion attacks, a form of adversarial attack where input is modified at test time to cause misclassification, are particularly insidious due to their transferability: adversarial examples crafted against one model often fool other models as well. This property, known as adversarial transferability, complicates defense strategies since it enables black-box attacks to succeed without direct access to the victim model. While adversarial training is one of the most widely adopted defense mechanisms, its effectiveness is typically evaluated on a narrow and homogeneous population of models. This limitation hinders the generalizability of empirical findings and restricts practical adoption.   In this work, we introduce DUMBer, an attack framework built on the foundation of the DUMB (Dataset soUrces, Model architecture, and Balance) methodology, to systematically evaluate the resilience of adversarially trained models. Our testbed spans multiple adversarial training techniques evaluated across three diverse computer vision tasks, using a heterogeneous population of uniquely trained models to reflect real-world deployment variability. Our experimental pipeline comprises over 130k evaluations spanning 13 state-of-the-art attack algorithms, allowing us to capture nuanced behaviors of adversarial training under varying threat models and dataset conditions. Our findings offer practical, actionable insights for AI practitioners, identifying which defenses are most effective based on the model, dataset, and attacker setup.

**Link**: [arxiv](http://arxiv.org/abs/2506.18516v1),  [pdf](http://arxiv.org/pdf/2506.18516v1)

**Tags**: cs.CR 



### ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for   Detection of Bias, Discrimination and Stereotyping
**Authors**: Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Dhiman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy

**Updated**: 2025-06-23T11:11:32Z

**Summary**: The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.

**Link**: [arxiv](http://arxiv.org/abs/2502.02072v2),  [pdf](http://arxiv.org/pdf/2502.02072v2)

**Tags**: cs.CL cs.AI cs.CY 



### HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge
**Authors**: Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen, Kaili Ma, Hongzhi Chen, James Cheng

**Updated**: 2025-06-23T11:08:00Z

**Summary**: Graph-based Retrieval-Augmented Generation (RAG) methods have significantly enhanced the performance of large language models (LLMs) in domain-specific tasks. However, existing RAG methods do not adequately utilize the naturally inherent hierarchical knowledge in human cognition, which limits the capabilities of RAG systems. In this paper, we introduce a new RAG approach, called HiRAG, which utilizes hierarchical knowledge to enhance the semantic understanding and structure capturing capabilities of RAG systems in the indexing and retrieval processes. Our extensive experiments demonstrate that HiRAG achieves significant performance improvements over the state-of-the-art baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2503.10150v2),  [pdf](http://arxiv.org/pdf/2503.10150v2)

**Tags**: cs.CL cs.AI 



### MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis
**Authors**: Yuting Zhang, Kaishen Yuan, Hao Lu, Yutao Yue, Jintai Chen, Kaishun Wu

**Updated**: 2025-06-23T11:06:31Z

**Summary**: Accurate and interpretable multi-disease diagnosis remains a critical challenge in medical research, particularly when leveraging heterogeneous multimodal medical data. Current approaches often rely on single-modal data, limiting their ability to comprehensively understand complex diseases. To address this, we propose MedTVT-R1, a novel Multimodal Large Language Model (MLLM) framework designed to integrate clinical multimodal data for reasoning and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction dataset that provides question-answer pairs for physiological-level interpretations and disease-level diagnoses with a Chain of Evidence approach. MedTVT-R1 incorporates a modality perception layer to capture inter-modal dependencies and adaptively weight modality contributions. Additionally, we employ Group Relative Policy Optimization (GRPO)-based Reinforcement Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning. Experimental results demonstrate MedTVT-R1's superiority in multimodal feature utilization and multi-disease diagnosis, offering significant potential for clinical applications such as diagnostic report generation and comorbidity reasoning. The dataset and code are available at https://github.com/keke-nice/MedTVT-R1.

**Link**: [arxiv](http://arxiv.org/abs/2506.18512v1),  [pdf](http://arxiv.org/pdf/2506.18512v1)

**Tags**: cs.CV 



### Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich   Transcripts
**Authors**: Duygu Altinok

**Updated**: 2025-06-23T11:04:20Z

**Summary**: Accurate detection of disfluencies in spoken language is crucial for enhancing the performance of automatic speech and language processing systems, as well as fostering the development of more inclusive speech and language technologies. Leveraging the growing trend of large language models (LLMs) as versatile learners capable of processing both lexical and non-lexical inputs (e.g., audio and video), we propose a novel approach to transcribing disfluencies as explicit tokens with timestamps, enabling the generation of fully annotated disfluency-rich transcripts. Our method integrates acoustic representations extracted from an audio encoder with textual inputs of varying quality: clean transcriptions without disfluencies, time-aligned transcriptions from aligners, or outputs from phoneme-based ASR models -- all of which may contain imperfections. Importantly, our experiments demonstrate that textual inputs do not need to be flawless. As long as they include timestamp-related cues, LLMs can effectively smooth the input and produce fully disfluency-annotated transcripts, underscoring their robustness in handling imperfect hints.

**Link**: [arxiv](http://arxiv.org/abs/2506.18510v1),  [pdf](http://arxiv.org/pdf/2506.18510v1)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks:   Strengths, Weaknesses, and Domain-Specific Performance
**Authors**: Wael Etaiwi, Bushra Alhijawi

**Updated**: 2025-06-23T10:52:54Z

**Summary**: The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.

**Link**: [arxiv](http://arxiv.org/abs/2506.18501v1),  [pdf](http://arxiv.org/pdf/2506.18501v1)

**Tags**: cs.CL cs.AI 



### SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task   Policies in Model-Free RL
**Authors**: Jacques Cloete, Nikolaus Vertovec, Alessandro Abate

**Updated**: 2025-06-23T10:50:00Z

**Summary**: To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work, we present theoretical results that place a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setting. This bound, based on a maximum policy ratio computed with respect to a 'safe' base policy, can also be applied to temporally-extended properties (beyond safety) and to robust control problems. To utilize these results, we introduce SPoRt, which provides a data-driven method for computing this bound for the base policy using the scenario approach, and includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. SPoRt thus enables users to trade off safety guarantees against task-specific performance. Complementing our theoretical results, we present experimental results demonstrating this trade-off and comparing the theoretical bound to posterior bounds derived from empirical violation rates.

**Link**: [arxiv](http://arxiv.org/abs/2504.06386v2),  [pdf](http://arxiv.org/pdf/2504.06386v2)

**Tags**: cs.LG 



### Biased Teacher, Balanced Student
**Authors**: Seonghak Kim

**Updated**: 2025-06-23T10:46:44Z

**Summary**: Knowledge Distillation (KD) is a widely adopted model compression technique where a compact student model learns from the output of a larger, pre-trained teacher. While effective in balanced settings, conventional KD suffers significantly when applied to long-tailed data distributions, as the teacher model tends to be biased toward head classes and provides limited supervision for tail classes. In this paper, we propose Long-Tailed Knowledge Distillation (LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by reformulating the standard KD objective into two components: inter-group and intra-group Kullback-Leibler (KL) divergence, corresponding to the prediction distributions across and within class groups (head, medium, tail), respectively. This decomposition allows us to identify and quantify the sources of teacher bias. To address them, we introduce (1) a rebalanced inter-group loss that calibrates the teacher's group-level predictions and (2) a uniform intra-group loss that ensures equal contribution from all groups during distillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and ImageNet-LT show that LTKD consistently outperforms existing KD methods, achieving significant gains in both overall accuracy and tail-class performance. Our results demonstrate that LTKD enables effective knowledge transfer even from biased teachers, making it a strong candidate for real-world deployment in resource-constrained and imbalanced settings.

**Link**: [arxiv](http://arxiv.org/abs/2506.18496v1),  [pdf](http://arxiv.org/pdf/2506.18496v1)

**Tags**: cs.CV 



### RealSR-R1: Reinforcement Learning for Real-World Image Super-Resolution   with Vision-Language Chain-of-Thought
**Authors**: Junbo Qiao, Miaomiao Cai, Wei Li, Yutong Liu, Xudong Huang, Gaoqi He, Jiao Xie, Jie Hu, Xinghao Chen, Shaohui Lin

**Updated**: 2025-06-23T10:42:18Z

**Summary**: Real-World Image Super-Resolution is one of the most challenging task in image restoration. However, existing methods struggle with an accurate understanding of degraded image content, leading to reconstructed results that are both low-fidelity and unnatural. We present RealSR-R1 in this work, which empowers the RealSR models with understanding and reasoning capabilities. Inspired by the success of Chain of Thought (CoT) in large language models (LLMs), we simulate the human process of handling degraded images and propose the VLCoT framework, which integrates vision and language reasoning. The framework aims to precisely restore image details by progressively generating more comprehensive text and higher-resolution images. To overcome the challenge of traditional supervised learning CoT failing to generalize to real-world scenarios, we introduce, for the first time, Group Relative Policy Optimization (GRPO) into the Real-World Image Super-Resolution task. We propose VLCoT-GRPO as a solution, which designs four reward functions: (1) Format reward, used to standardize the CoT process; (2) Degradation reward, to incentivize accurate degradation estimation; (3) Understanding reward, to ensure the accuracy of the generated content; and (4) Generation reward, where we propose using a visual expert model to evaluate the quality of generated images, encouraging the model to generate more realistic images. Extensive experiments demonstrate that our proposed RealSR-R1 can generate realistic details and accurately understand image content, particularly in semantically rich scenes or images with severe degradation.

**Link**: [arxiv](http://arxiv.org/abs/2506.16796v2),  [pdf](http://arxiv.org/pdf/2506.16796v2)

**Tags**: cs.CV 



### MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning   Models
**Authors**: Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, Dacheng Tao

**Updated**: 2025-06-23T10:37:57Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle complex reasoning tasks. However, existing RLVR methods overlook one of the most distinctive capabilities of LLMs, their in-context learning ability, as prominently demonstrated by the success of Chain-of-Thought (CoT) prompting. This motivates us to explore how reinforcement learning can be effectively combined with in-context learning to better improve the reasoning capabilities of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement learning of LLMs by involving ``telling LLMs the rules of the game''. Specifically, MeRF directly injects the reward specification into the prompt, which serves as an in-context motivation for model to improve its responses with awareness of the optimization objective. This simple modification leverages the in-context learning ability of LLMs aligning generation with optimization, thereby incentivizing the model to generate desired outputs from both inner motivation and external reward. Empirical evaluations on the Knights and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that \texttt{MeRF} achieves substantial performance gains over baselines. Moreover, ablation studies show that performance improves with greater consistency between the in-context motivation and the external reward function, while the model also demonstrates an ability to adapt to misleading motivations through reinforcement learning.

**Link**: [arxiv](http://arxiv.org/abs/2506.18485v1),  [pdf](http://arxiv.org/pdf/2506.18485v1)

**Tags**: cs.CL cs.AI 



### MORTAR: Multi-turn Metamorphic Testing for LLM-based Dialogue Systems
**Authors**: Guoxiang Guo, Aldeida Aleti, Neelofar Neelofar, Chakkrit Tantithamthavorn, Yuanyuan Qi, Tsong Yueh Chen

**Updated**: 2025-06-23T10:23:35Z

**Summary**: With the widespread application of LLM-based dialogue systems in daily life, quality assurance has become more important than ever. Recent research has successfully introduced methods to identify unexpected behaviour in single-turn testing scenarios. However, multi-turn interaction is the common real-world usage of dialogue systems, yet testing methods for such interactions remain underexplored. This is largely due to the oracle problem in multi-turn testing, which continues to pose a significant challenge for dialogue system developers and researchers. In this paper, we propose MORTAR, a metamorphic multi-turn dialogue testing approach, which mitigates the test oracle problem in testing LLM-based dialogue systems. MORTAR formalises the multi-turn testing for dialogue systems, and automates the generation of question-answer dialogue test cases with multiple dialogue-level perturbations and metamorphic relations (MRs). The automated MR matching mechanism allows MORTAR more flexibility and efficiency in metamorphic testing. The proposed approach is fully automated without reliance on LLM judges. In testing six popular LLM-based dialogue systems, MORTAR reaches significantly better effectiveness with over 150\% more bugs revealed per test case when compared to the single-turn metamorphic testing baseline. Regarding the quality of bugs, MORTAR reveals higher-quality bugs in terms of diversity, precision and uniqueness. MORTAR is expected to inspire more multi-turn testing approaches, and assist developers in evaluating the dialogue system performance more comprehensively with constrained test resources and budget.

**Link**: [arxiv](http://arxiv.org/abs/2412.15557v3),  [pdf](http://arxiv.org/pdf/2412.15557v3)

**Tags**: cs.SE cs.CL 



### Adaptive alert prioritisation in security operations centres via   learning to defer with human feedback
**Authors**: Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, C√©cile Paris

**Updated**: 2025-06-23T09:59:58Z

**Summary**: Alert prioritisation (AP) is crucial for security operations centres (SOCs) to manage the overwhelming volume of alerts and ensure timely detection and response to genuine threats, while minimising alert fatigue. Although predictive AI can process large alert volumes and identify known patterns, it struggles with novel and evolving scenarios that demand contextual understanding and nuanced judgement. A promising solution is Human-AI teaming (HAT), which combines human expertise with AI's computational capabilities. Learning to Defer (L2D) operationalises HAT by enabling AI to "defer" uncertain or unfamiliar cases to human experts. However, traditional L2D models rely on static deferral policies that do not evolve with experience, limiting their ability to learn from human feedback and adapt over time. To overcome this, we introduce Learning to Defer with Human Feedback (L2DHF), an adaptive deferral framework that leverages Deep Reinforcement Learning from Human Feedback (DRLHF) to optimise deferral decisions. By dynamically incorporating human feedback, L2DHF continuously improves AP accuracy and reduces unnecessary deferrals, enhancing SOC effectiveness and easing analyst workload. Experiments on two widely used benchmark datasets, UNSW-NB15 and CICIDS2017, demonstrate that L2DHF significantly outperforms baseline models. Notably, it achieves 13-16% higher AP accuracy for critical alerts on UNSW-NB15 and 60-67% on CICIDS2017. It also reduces misprioritisations, for example, by 98% for high-category alerts on CICIDS2017. Moreover, L2DHF decreases deferrals, for example, by 37% on UNSW-NB15, directly reducing analyst workload. These gains are achieved with efficient execution, underscoring L2DHF's practicality for real-world SOC deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.18462v1),  [pdf](http://arxiv.org/pdf/2506.18462v1)

**Tags**: cs.CR I.2 



### Benchmarking Large Language Models for Handwritten Text Recognition
**Authors**: Giorgia Crosilla, Lukas Klic, Giovanni Colavizza

**Updated**: 2025-06-23T09:48:02Z

**Summary**: Traditional machine learning models for Handwritten Text Recognition (HTR) rely on supervised training, requiring extensive manual annotations, and often produce errors due to the separation between layout and text processing. In contrast, Multimodal Large Language Models (MLLMs) offer a general approach to recognizing diverse handwriting styles without the need for model-specific training. The study benchmarks various proprietary and open-source LLMs against Transkribus models, evaluating their performance on both modern and historical datasets written in English, French, German, and Italian. In addition, emphasis is placed on testing the models' ability to autonomously correct previously generated outputs. Findings indicate that proprietary models, especially Claude 3.5 Sonnet, outperform open-source alternatives in zero-shot settings. MLLMs achieve excellent results in recognizing modern handwriting and exhibit a preference for the English language due to their pre-training dataset composition. Comparisons with Transkribus show no consistent advantage for either approach. Moreover, LLMs demonstrate limited ability to autonomously correct errors in zero-shot transcriptions.

**Link**: [arxiv](http://arxiv.org/abs/2503.15195v3),  [pdf](http://arxiv.org/pdf/2503.15195v3)

**Tags**: cs.CV 



### CODS : A Theoretical Model for Computational Design Based on Design   Space
**Authors**: Nan Cao, Xiaoyu Qi, Chuer Chen, Xiaoke Yan

**Updated**: 2025-06-23T09:47:14Z

**Summary**: We introduce CODS (Computational Optimization in Design Space), a theoretical model that frames computational design as a constrained optimization problem over a structured, multi-dimensional design space. Unlike existing methods that rely on handcrafted heuristics or domain-specific rules, CODS provides a generalizable and interpretable framework that supports diverse design tasks. Given a user requirement and a well-defined design space, CODS automatically derives soft and hard constraints using large language models through a structured prompt engineering pipeline. These constraints guide the optimization process to generate design solutions that are coherent, expressive, and aligned with user intent. We validate our approach across two domains-visualization design and knitwear generation-demonstrating superior performance in design quality, intent alignment, and user preference compared to existing LLM-based methods. CODS offers a unified foundation for scalable, controllable, and AI-powered design automation.

**Link**: [arxiv](http://arxiv.org/abs/2506.18455v1),  [pdf](http://arxiv.org/pdf/2506.18455v1)

**Tags**: cs.HC 



### Large Language Models powered Malicious Traffic Detection: Architecture,   Opportunities and Case Study
**Authors**: Xinggong Zhang, Haotian Meng, Qingyang Li, Yunpeng Tan, Lei Zhang

**Updated**: 2025-06-23T09:35:39Z

**Summary**: Malicious traffic detection is a pivotal technology for network security to identify abnormal network traffic and detect network attacks. Large Language Models (LLMs) are trained on a vast corpus of text, have amassed remarkable capabilities of context-understanding and commonsense knowledge. This has opened up a new door for network attacks detection. Researchers have already initiated discussions regarding the application of LLMs on specific cyber-security tasks. Unfortunately, there remains a lack of comprehensive analysis on harnessing LLMs for traffic detection, as well as the opportunities and challenges. In this paper, we focus on unleashing the full potential of Large Language Models (LLMs) in malicious traffic detection. We present a holistic view of the architecture of LLM-powered malicious traffic detection, including the procedures of Pre-training, Fine-tuning, and Detection. Especially, by exploring the knowledge and capabilities of LLM, we identify three distinct roles LLM can act in traffic classification: Classifier, Encoder, and Predictor. For each of them, the modeling paradigm, opportunities and challenges are elaborated. Finally, we present our design on LLM-powered DDoS detection as a case study. The proposed framework attains accurate detection on carpet bombing DDoS by exploiting LLMs' capabilities in contextual mining. The evaluation shows its efficacy, exhibiting a nearly 35% improvement compared to existing systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.18487v2),  [pdf](http://arxiv.org/pdf/2503.18487v2)

**Tags**: cs.NI cs.AI cs.CR 



### TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided   Subspace Partitioning
**Authors**: Sheng Wang, Pengan Chen, Jingqi Zhou, Qintong Li, Jingwei Dong, Jiahui Gao, Boyang Xue, Jiyue Jiang, Lingpeng Kong, Chuan Wu

**Updated**: 2025-06-23T09:32:03Z

**Summary**: Model customization necessitates high-quality and diverse datasets, but acquiring such data remains time-consuming and labor-intensive. Despite the great potential of large language models (LLMs) for data synthesis, current approaches are constrained by limited seed data, model biases, and low-variation prompts, resulting in limited diversity and biased distributions with the increase of data scales. To tackle this challenge, we introduce TREESYNTH, a tree-guided subspace-based data synthesis approach inspired by decision trees. It constructs a spatial partitioning tree to recursively divide a task-specific full data space (i.e., root node) into numerous atomic subspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes to ensure both distinctiveness and comprehensiveness before synthesizing samples within each atomic subspace. This globally dividing-and-synthesizing method finally collects subspace samples into a comprehensive dataset, effectively circumventing repetition and space collapse to ensure the diversity of large-scale data synthesis. Furthermore, the spatial partitioning tree enables sample allocation into atomic subspaces, allowing the rebalancing of existing datasets for more balanced and comprehensive distributions. Empirically, extensive experiments across diverse benchmarks consistently demonstrate the superior data diversity, model performance, and robust scalability of TREESYNTH compared to both human-crafted datasets and peer data synthesis methods, with an average performance gain reaching 10%. Besides, the consistent improvements of TREESYNTH-balanced datasets highlight its efficacious application to redistribute existing datasets for more comprehensive coverage and the induced performance enhancement. The code is available at https://github.com/cpa2001/TreeSynth.

**Link**: [arxiv](http://arxiv.org/abs/2503.17195v2),  [pdf](http://arxiv.org/pdf/2503.17195v2)

**Tags**: cs.LG cs.AI 



### Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for   Prognosis Prediction in Medical Imaging
**Authors**: Filippo Ruffini, Elena Mulero Ayllon, Linlin Shen, Paolo Soda, Valerio Guarrasi

**Updated**: 2025-06-23T09:16:04Z

**Summary**: Artificial Intelligence (AI) holds significant promise for improving prognosis prediction in medical imaging, yet its effective application remains challenging. In this work, we introduce a structured benchmark explicitly designed to evaluate and compare the transferability of Convolutional Neural Networks and Foundation Models in predicting clinical outcomes in COVID-19 patients, leveraging diverse publicly available Chest X-ray datasets. Our experimental methodology extensively explores a wide set of fine-tuning strategies, encompassing traditional approaches such as Full Fine-Tuning and Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were conducted across multiple learning paradigms, including both extensive full-data scenarios and more clinically realistic Few-Shot Learning settings, which are critical for modeling rare disease outcomes and rapidly emerging health threats. By implementing a large-scale comparative analysis involving a diverse selection of pretrained models, including general-purpose architectures pretrained on large-scale datasets such as CLIP and DINOv2, to biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we rigorously assess each model's capacity to effectively adapt and generalize to prognosis tasks, particularly under conditions of severe data scarcity and pronounced class imbalance. The benchmark was designed to capture critical conditions common in prognosis tasks, including variations in dataset size and class distribution, providing detailed insights into the strengths and limitations of each fine-tuning strategy. This extensive and structured evaluation aims to inform the practical deployment and adoption of robust, efficient, and generalizable AI-driven solutions in real-world clinical prognosis prediction workflows.

**Link**: [arxiv](http://arxiv.org/abs/2506.18434v1),  [pdf](http://arxiv.org/pdf/2506.18434v1)

**Tags**: cs.CV cs.AI 



### A New Pathway to Integrated Learning and Communication (ILAC): Large AI   Model and Hyperdimensional Computing for Communication
**Authors**: Wei Xu, Zhaohui Yang, Derrick Wing Kwan Ng, Robert Schober, H. Vincent Poor, Zhaoyang Zhang, Xiaohu You

**Updated**: 2025-06-24T06:26:34Z

**Summary**: The rapid evolution of forthcoming sixth-generation (6G) wireless networks necessitates the seamless integration of artificial intelligence (AI) with wireless communications to support emerging intelligent applications that demand both efficient communication and robust learning performance. This dual requirement calls for a unified framework of integrated learning and communication (ILAC), where AI enhances communication through intelligent signal processing and adaptive resource management, while wireless networks support AI model deployment by enabling efficient and reliable data exchanges. However, achieving this integration presents significant challenges in practice. Communication constraints, such as limited bandwidth and fluctuating channels, hinder learning accuracy and convergence. Simultaneously, AI-driven learning dynamics, including model updates and task-driven inference, introduce excessive burdens on communication systems, necessitating flexible, context-aware transmission strategies. Finally, we present a case study on a cost-to-performance optimization problem, where task assignments, model size selection, bandwidth allocation, and transmission power control are jointly optimized, considering computational cost, communication efficiency, and inference accuracy. Leveraging the Dinkelbach and alternating optimization algorithms, we offer a practical and effective solution to achieve an optimal balance between learning performance and communication constraints.

**Link**: [arxiv](http://arxiv.org/abs/2506.18432v2),  [pdf](http://arxiv.org/pdf/2506.18432v2)

**Tags**: eess.SP 



### Compromising Honesty and Harmlessness in Language Models via Deception   Attacks
**Authors**: Laur√®ne Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff

**Updated**: 2025-06-23T09:04:32Z

**Summary**: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce "deception attacks" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.

**Link**: [arxiv](http://arxiv.org/abs/2502.08301v2),  [pdf](http://arxiv.org/pdf/2502.08301v2)

**Tags**: cs.CL cs.AI cs.CY 



### A Large Language Model-based Multi-Agent Framework for Analog Circuits'   Sizing Relationships Extraction
**Authors**: Chengjie Liu, Weiyu Chen, Huiyao Xu, Yuan Du, Jun Yang, Li Du

**Updated**: 2025-06-23T09:03:58Z

**Summary**: In the design process of the analog circuit pre-layout phase, device sizing is an important step in determining whether an analog circuit can meet the required performance metrics. Many existing techniques extract the circuit sizing task as a mathematical optimization problem to solve and continuously improve the optimization efficiency from a mathematical perspective. But they ignore the automatic introduction of prior knowledge, fail to achieve effective pruning of the search space, which thereby leads to a considerable compression margin remaining in the search space. To alleviate this problem, we propose a large language model (LLM)-based multi-agent framework for analog circuits' sizing relationships extraction from academic papers. The search space in the sizing process can be effectively pruned based on the sizing relationship extracted by this framework. Eventually, we conducted tests on 3 types of circuits, and the optimization efficiency was improved by $2.32 \sim 26.6 \times$. This work demonstrates that the LLM can effectively prune the search space for analog circuit sizing, providing a new solution for the combination of LLMs and conventional analog circuit design automation methods.

**Link**: [arxiv](http://arxiv.org/abs/2506.18424v1),  [pdf](http://arxiv.org/pdf/2506.18424v1)

**Tags**: cs.AI cs.ET 



### TReB: A Comprehensive Benchmark for Evaluating Table Reasoning   Capabilities of Large Language Models
**Authors**: Ce Li, Xiaofan Liu, Zhiyan Song, Ce Chi, Chen Zhao, Jingjing Yang, Zhendong Wang, Kexin Yang, Boshen Shi, Xing Wang, Chao Deng, Junlan Feng

**Updated**: 2025-06-23T09:02:04Z

**Summary**: The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on [HuggingFace] and the framework on [GitHub].

**Link**: [arxiv](http://arxiv.org/abs/2506.18421v1),  [pdf](http://arxiv.org/pdf/2506.18421v1)

**Tags**: cs.CL cs.AI 



### Generative Diffusion Receivers: Achieving Pilot-Efficient MIMO-OFDM   Communications
**Authors**: Yuzhi Yang, Omar Alhussein, Atefeh Arani, Zhaoyang Zhang, M√©rouane Debbah

**Updated**: 2025-06-23T09:00:50Z

**Summary**: This paper focuses on wireless multiple-input multiple-output (MIMO)-orthogonal frequency division multiplex (OFDM) receivers. Traditional wireless receivers have relied on mathematical modeling and Bayesian inference, achieving remarkable success in most areas but falling short in their ability to characterize channel matrices. Neural networks (NNs) have demonstrated significant potential in this aspect. Nevertheless, integrating traditional inference methods with NNs presents challenges, particularly in tracking the error progression. Given the inevitable presence of noise in wireless systems, generative models that are more resilient to noise are garnering increased attention. In this paper, we propose re-evaluating the MIMO-OFDM receiver using diffusion models, which is a common generative approach. With diffusion models, we can effectively leverage prior knowledge of channel matrices and incorporate traditional signal estimation components. Specifically, we explore the diffusion system and introduce an imagination-screening strategy to guide the diffusion process. Furthermore, diffusion models enable adaptation to varying noise levels and pilot schemes using the same NN, significantly reducing training and deployment costs. Simulated results reveal that, for pilot densities ranging from 4-6 pilots per 64-subcarrier block and signal-to-noise ratios (SNRs) from -4 dB to 0 dB, our proposed receiver reduces channel-reconstruction error by up to two times compared to leading deep-learning models, with the most pronounced improvements observed in low-pilot conditions. Additionally, performance enhancements can be achieved with a larger imagination size, despite increased computational complexity.

**Link**: [arxiv](http://arxiv.org/abs/2506.18419v1),  [pdf](http://arxiv.org/pdf/2506.18419v1)

**Tags**: eess.SP 



### Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased   Reinforcement Learning in Multimodal Small Language Models
**Authors**: Zeyu Liu, Yuhang Liu, Guanghao Zhu, Congkai Xie, Zhen Li, Jianbo Yuan, Xinyao Wang, Qing Li, Shing-Chi Cheung, Shengyu Zhang, Fei Wu, Hongxia Yang

**Updated**: 2025-06-23T08:47:25Z

**Summary**: Recent advancements in large language models (LLMs) have demonstrated substantial progress in reasoning capabilities, such as DeepSeek-R1, which leverages rule-based reinforcement learning to enhance logical reasoning significantly. However, extending these achievements to multimodal large language models (MLLMs) presents critical challenges, which are frequently more pronounced for Multimodal Small Language Models (MSLMs) given their typically weaker foundational reasoning abilities: (1) the scarcity of high-quality multimodal reasoning datasets, (2) the degradation of reasoning capabilities due to the integration of visual processing, and (3) the risk that direct application of reinforcement learning may produce complex yet incorrect reasoning processes. To address these challenges, we design a novel framework Infi-MMR to systematically unlock the reasoning potential of MSLMs through a curriculum of three carefully structured phases and propose our multimodal reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning Activation, leverages high-quality textual reasoning datasets to activate and strengthen the model's logical reasoning capabilities. The second phase, Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to facilitate the progressive transfer of reasoning skills to multimodal contexts. The third phase, Multimodal Reasoning Enhancement, employs curated, caption-free multimodal data to mitigate linguistic biases and promote robust cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on MathVista testmini). Resources are available at https://huggingface.co/Reallm-Labs/Infi-MMR-3B.

**Link**: [arxiv](http://arxiv.org/abs/2505.23091v3),  [pdf](http://arxiv.org/pdf/2505.23091v3)

**Tags**: cs.AI cs.CL 



### The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs
**Authors**: Muntasir Adnan, Carlos C. N. Kuhn

**Updated**: 2025-06-23T08:40:45Z

**Summary**: The effectiveness of AI debugging follows a predictable exponential decay pattern; most models lose 60-80% of their debugging capability within just 2-3 attempts, despite iterative debugging being a critical capability for practical code generation systems. We introduce the Debugging Decay Index (DDI), a mathematical framework that quantifies when debugging becomes ineffective and predicts intervention points. Our strategic fresh start approach shifts from exploitation to exploration at strategic points in the debugging process, demonstrating that well-timed interventions can rescue the effectiveness of debugging. DDI reveals a fundamental limitation in current AI debugging and provides the first quantitative framework for optimising iterative code generation strategies.

**Link**: [arxiv](http://arxiv.org/abs/2506.18403v1),  [pdf](http://arxiv.org/pdf/2506.18403v1)

**Tags**: cs.SE cs.AI 



### MCP-Zero: Active Tool Discovery for Autonomous LLM Agents
**Authors**: Xiang Fei, Xiawu Zheng, Hao Feng

**Updated**: 2025-06-24T06:27:29Z

**Summary**: True intelligence requires active capability acquisition, yet current LLM agents inject pre-defined tool schemas into prompts, reducing models to passive selectors and falling short of robust general-purpose agency. We introduce MCP-Zero, an active agent framework that restores tool discovery autonomy to LLMs themselves. Instead of overwhelming models with all available tools, MCP-Zero enables agents to actively identify capability gaps, and request specific tools on-demand, transforming them from large-scale retrievers into genuine autonomous agents. The framework operates through three core mechanisms: (1) Active Tool Request, where models autonomously generate structured requests specifying their exact tool requirements; (2) Hierarchical Semantic Routing, a two-stage algorithm that matches requests to relevant servers and tools through improved semantic alignment; (3) Iterative Capability Extension, enabling agents to progressively build cross-domain toolchains while maintaining minimal context footprint. We construct MCP-tools, a comprehensive dataset of 308 MCP servers and 2,797 tools from the official Model-Context-Protocol repository. Experiments demonstrate that MCP-Zero preserves agent autonomy while achieving substantial efficiency gains: (i) accurate tool selection from nearly 3k candidates across 248.1k tokens; (ii) 98\% reduction in token consumption on APIBank while maintaining high accuracy; and (iii) consistent multi-turn performance that scales with tool ecosystem growth. This work establishes active tool discovery as a fundamental design pattern for scalable autonomous agent systems.

**Link**: [arxiv](http://arxiv.org/abs/2506.01056v4),  [pdf](http://arxiv.org/pdf/2506.01056v4)

**Tags**: cs.AI cs.SE 



### Tracing Errors, Constructing Fixes: Repository-Level Memory Error Repair   via Typestate-Guided Context Retrieval
**Authors**: Xiao Cheng, Zhihao Guo, Huan Huo, Yulei Sui

**Updated**: 2025-06-23T08:30:00Z

**Summary**: Memory-related errors in C programming continue to pose significant challenges in software development, primarily due to the complexities of manual memory management inherent in the language. These errors frequently serve as vectors for severe vulnerabilities, while their repair requires extensive knowledge of program logic and C's memory model. Automated Program Repair (APR) has emerged as a critical research area to address these challenges. Traditional APR approaches rely on expert-designed strategies and predefined templates, which are labor-intensive and constrained by the effectiveness of manual specifications. Deep learning techniques offer a promising alternative by automatically extracting repair patterns, but they require substantial training datasets and often lack interpretability.   This paper introduces LTFix, a novel approach that harnesses the potential of Large Language Models (LLMs) for automated memory error repair, especially for complex repository-level errors that span multiple functions and files. We address two fundamental challenges in LLM-based memory error repair: a limited understanding of interprocedural memory management patterns and context window limitations for repository-wide analysis. Our approach utilizes a finite typestate automaton to guide the tracking of error-propagation paths and context trace, capturing both spatial (memory states) and temporal (execution history) dimensions of error behavior. This typestate-guided context retrieval strategy provides the LLM with concise yet semantically rich information relevant to erroneous memory management, effectively addressing the token limitation of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.18394v1),  [pdf](http://arxiv.org/pdf/2506.18394v1)

**Tags**: cs.SE 



### SLR: An Automated Synthesis Framework for Scalable Logical Reasoning
**Authors**: Lukas Helff, Ahmad Omar, Felix Friedrich, Wolfgang Stammer, Antonia W√ºst, Tim Woydt, Rupert Mitchell, Patrick Schramowski, Kristian Kersting

**Updated**: 2025-06-23T08:27:44Z

**Summary**: We introduce SLR, an end-to-end framework for systematic evaluation and training of Large Language Models (LLMs) via Scalable Logical Reasoning. Given a user's task specification, SLR enables scalable, automated synthesis of inductive reasoning tasks with precisely controlled difficulty. For each task, SLR synthesizes (i) a latent ground-truth rule, (ii) an executable validation program used by a symbolic judge to deterministically verify model outputs, and (iii) an instruction prompt for the reasoning task. Using SLR, we create SLR-Bench, a benchmark comprising over 19k prompts spanning 20 curriculum levels that progressively increase in relational, arithmetic, and recursive complexity. Large-scale evaluation reveals that contemporary LLMs readily produce syntactically valid rules, yet often fail at correct logical inference. Recent reasoning LLMs do somewhat better, but incur substantial increases in test-time compute, sometimes exceeding 15k completion tokens. Finally, logic-tuning via SLR doubles Llama-3-8B accuracy on SLR-Bench, achieving parity with Gemini-Flash-Thinking at a fraction of computational cost. SLR is fully automated, requires no human annotation, ensures dataset novelty, and offers a scalable environment for probing and advancing LLMs' reasoning capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2506.15787v2),  [pdf](http://arxiv.org/pdf/2506.15787v2)

**Tags**: cs.AI cs.CL cs.LG 



### Evaluating Causal Explanation in Medical Reports with LLM-Based and   Human-Aligned Metrics
**Authors**: Yousang Cho, Key-Sun Choi

**Updated**: 2025-06-23T08:19:21Z

**Summary**: This study investigates how accurately different evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment across two input types: observation-based and multiple-choice-based report generation. Two weighting strategies are applied: one reflecting task-specific priorities, and the other assigning equal weights to all metrics. Our results show that GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives. GPT-White also aligns well with expert evaluations, while similarity-based metrics diverge from clinical reasoning quality. These findings emphasize the impact of metric selection and weighting on evaluation outcomes, supporting the use of LLM-based evaluation for tasks requiring interpretability and causal reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2506.18387v1),  [pdf](http://arxiv.org/pdf/2506.18387v1)

**Tags**: cs.CL cs.AI 



### LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using   LLMs and Preference Optimization
**Authors**: Koushik Viswanadha, Deepanway Ghosal, Somak Aditya

**Updated**: 2025-06-23T08:15:24Z

**Summary**: Logical reasoning is a key task for artificial intelligence due to it's role in major downstream tasks such as Question Answering, Summarization. Recent methods in improving the reasoning ability of LLMs fall short in correctly converting a natural language reasoning problem to an equivalent logical formulation, which hinders the framework's overall ability to reason. Towards this, we propose to use finetuning on a preference optimization dataset to learn to parse and represent a natural language problem as a whole to a consistent logical program by 1) introducing a new supervised and preference optimization dataset LogicPO, and 2) adopting popular techniques such as Direct Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune open-source LLMs. Our best model with Phi-3.5 consistently outperforms GPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14% less syntax errors. Through the framework and our improved evaluation metrics, we offer a promising direction in improving the logical reasoning of LLMs by better representing them in their logical formulations.

**Link**: [arxiv](http://arxiv.org/abs/2506.18383v1),  [pdf](http://arxiv.org/pdf/2506.18383v1)

**Tags**: cs.LG cs.AI 



### A Rigorous Evaluation of LLM Data Generation Strategies for Low-Resource   Languages
**Authors**: Tatiana Anikina, Jan Cegin, Jakub Simko, Simon Ostermann

**Updated**: 2025-06-23T07:52:34Z

**Summary**: Large Language Models (LLMs) are increasingly used to generate synthetic textual data for training smaller specialized models. However, a comparison of various generation strategies for low-resource language settings is lacking. While various prompting strategies have been proposed, such as demonstrations, label-based summaries, and self-revision, their comparative effectiveness remains unclear, especially for low-resource languages. In this paper, we systematically evaluate the performance of these generation strategies and their combinations across 11 typologically diverse languages, including several extremely low-resource ones. Using three NLP tasks and four open-source LLMs, we assess downstream model performance on generated versus gold-standard data. Our results show that strategic combinations of generation methods, particularly target-language demonstrations with LLM-based revisions, yield strong performance, narrowing the gap with real data to as little as 5% in some settings. We also find that smart prompting techniques can reduce the advantage of larger LLMs, highlighting efficient generation strategies for synthetic data generation in low-resource scenarios with smaller models.

**Link**: [arxiv](http://arxiv.org/abs/2506.12158v2),  [pdf](http://arxiv.org/pdf/2506.12158v2)

**Tags**: cs.CL 



### DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy
**Authors**: Kaixuan Xu, Jiajun Chai, Sicheng Li, Yuqian Fu, Yuanheng Zhu, Dongbin Zhao

**Updated**: 2025-06-23T07:49:08Z

**Summary**: Diplomacy is a complex multiplayer game that requires both cooperation and competition, posing significant challenges for AI systems. Traditional methods rely on equilibrium search to generate extensive game data for training, which demands substantial computational resources. Large Language Models (LLMs) offer a promising alternative, leveraging pre-trained knowledge to achieve strong performance with relatively small-scale fine-tuning. However, applying LLMs to Diplomacy remains challenging due to the exponential growth of possible action combinations and the intricate strategic interactions among players. To address this challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns equilibrium policies for Diplomacy. DipLLM employs an autoregressive factorization framework to simplify the complex task of multi-unit action assignment into a sequence of unit-level decisions. By defining an equilibrium policy within this framework as the learning objective, we fine-tune the model using only 1.5% of the data required by the state-of-the-art Cicero model, surpassing its performance. Our results demonstrate the potential of fine-tuned LLMs for tackling complex strategic decision-making in multiplayer games.

**Link**: [arxiv](http://arxiv.org/abs/2506.09655v2),  [pdf](http://arxiv.org/pdf/2506.09655v2)

**Tags**: cs.AI cs.LG 



### A Survey on Large Language Model based Human-Agent Systems
**Authors**: Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu

**Updated**: 2025-06-23T07:45:18Z

**Summary**: Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00753v3),  [pdf](http://arxiv.org/pdf/2505.00753v3)

**Tags**: cs.CL cs.LG 



### Recipe for Discovery: A Framework for Systematic Open Source Project   Identification
**Authors**: Juanita Gomez, Emily Lovell, Stephanie Lieggi, Alvaro A. Cardenas, James Davis

**Updated**: 2025-06-23T07:43:21Z

**Summary**: Open source software development, particularly within institutions such as universities and research laboratories, is often decentralized and difficult to track. Despite producing highly impactful tools in science, these efforts often go unrecognized due to a lack of visibility and institutional awareness. This paper addresses the challenge of discovering, classifying, and analyzing open source software projects developed across distributed institutional systems. We present a framework for systematically identifying institutional affiliated repositories, using the University of California (UC) system as a case study.   Using GitHub's REST API, we build a pipeline to discover relevant repositories and extract meaningful metadata. We then propose and evaluate multiple classification strategies, including both traditional machine learning models and large language models (LLMs), to distinguish affiliated projects from unrelated repositories and generate accurate insights into the academic open source landscape. Our results show that the framework is effective at scale, discovering over 52,000 repositories and predicting institutional affiliation with high accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2506.18359v1),  [pdf](http://arxiv.org/pdf/2506.18359v1)

**Tags**: cs.SE 



### SlimMoE: Structured Compression of Large MoE Models via Expert Slimming   and Distillation
**Authors**: Zichong Li, Chen Liang, Zixuan Zhang, Ilgee Hong, Young Jin Kim, Weizhu Chen, Tuo Zhao

**Updated**: 2025-06-23T07:15:59Z

**Summary**: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm for scaling large language models (LLMs) while maintaining inference efficiency. However, their enormous memory requirements make them prohibitively expensive to fine-tune or deploy in resource-constrained environments. To address this challenge, we introduce SlimMoE, a multi-stage compression framework for transforming large MoE models into much smaller, efficient variants without incurring the prohibitive costs of training from scratch. Our method systematically reduces parameter counts by slimming experts and transferring knowledge through intermediate stages, effectively mitigating the performance degradation common in one-shot pruning approaches. Using this framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE (3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of the original model's training data. These compressed models can be fine-tuned on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them highly suitable for academic and resource-limited settings. Our experiments demonstrate that these compressed models outperform others of similar size and remain competitive with larger models. For instance, Phi-mini-MoE achieves similar or better performance to Phi-3-mini using only 2/3 of the activated parameters and yields comparable MMLU scores to Llama 3.1 8B despite having significantly lower latency. Our findings demonstrate that structured pruning combined with staged distillation offers an effective path to creating high-quality, compact MoE models, paving the way for broader adoption of MoE architectures. We make our models publicly available at https://huggingface.co/microsoft/Phi-mini-MoE-instruct and https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

**Link**: [arxiv](http://arxiv.org/abs/2506.18349v1),  [pdf](http://arxiv.org/pdf/2506.18349v1)

**Tags**: cs.LG cs.CL 



### Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team
**Authors**: Weilun Yu, Shixiang Tang, Yonggui Huang, Nanqing Dong, Li Fan, Honggang Qi, Wei Liu, Xiaoli Diao, Xi Chen, Wanli Ouyang

**Updated**: 2025-06-23T07:12:08Z

**Summary**: Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18348v1),  [pdf](http://arxiv.org/pdf/2506.18348v1)

**Tags**: cs.AI 



### Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration
**Authors**: Junqi Gao, Zhichang Guo, Dazhi Zhang, Dong Li, Runze Liu, Pengfei Li, Kai Tian, Biqing Qi

**Updated**: 2025-06-23T07:03:18Z

**Summary**: Heterogeneous Large Language Model (LLM) fusion integrates the strengths of multiple source LLMs with different architectures into a target LLM with low computational overhead. While promising, existing methods suffer from two major limitations: 1) reliance on real data from limited domain for knowledge fusion, preventing the target LLM from fully acquiring knowledge across diverse domains, and 2) fixed data allocation proportions across domains, failing to dynamically adjust according to the target LLM's varying capabilities across domains, leading to a capability imbalance. To overcome these limitations, we propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework. Through the organization of knowledge domains into a hierarchical tree structure, Bohdi enables automatic domain exploration and multi-domain data generation through multi-model collaboration, thereby comprehensively extracting knowledge from source LLMs. By formalizing domain expansion and data sampling proportion allocation on the knowledge tree as a Hierarchical Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism to adaptively adjust sampling proportions based on the target LLM's performance feedback across domains. Integrated with our proposed Introspection-Rebirth (IR) mechanism, DynaBranches dynamically tracks capability shifts during target LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT), further enhancing its online adaptation capability. Comparative experimental results on a comprehensive suite of benchmarks demonstrate that Bohdi significantly outperforms existing baselines on multiple target LLMs, exhibits higher data efficiency, and virtually eliminates the imbalance in the target LLM's capabilities. Our code is available at https://github.com/gjq100/Bohdi.git.

**Link**: [arxiv](http://arxiv.org/abs/2506.15721v2),  [pdf](http://arxiv.org/pdf/2506.15721v2)

**Tags**: cs.LG 



### LoopSR: Looping Sim-and-Real for Lifelong Policy Adaptation of Legged   Robots
**Authors**: Peilin Wu, Weiji Xie, Jiahang Cao, Hang Lai, Weinan Zhang

**Updated**: 2025-06-23T06:59:08Z

**Summary**: Reinforcement Learning (RL) has shown its remarkable and generalizable capability in legged locomotion through sim-to-real transfer. However, while adaptive methods like domain randomization are expected to enhance policy robustness across diverse environments, they potentially compromise the policy's performance in any specific environment, leading to suboptimal real-world deployment due to the No Free Lunch theorem. To address this, we propose LoopSR, a lifelong policy adaptation framework that continuously refines RL policies in the post-deployment stage. LoopSR employs a transformer-based encoder to map real-world trajectories into a latent space and reconstruct a digital twin of the real world for further improvement. Autoencoder architecture and contrastive learning methods are adopted to enhance feature extraction of real-world dynamics. Simulation parameters for continual training are derived by combining predicted values from the decoder with retrieved parameters from a pre-collected simulation trajectory dataset. By leveraging simulated continual training, LoopSR achieves superior data efficiency compared with strong baselines, yielding eminent performance with limited data in both sim-to-sim and sim-to-real experiments.

**Link**: [arxiv](http://arxiv.org/abs/2409.17992v2),  [pdf](http://arxiv.org/pdf/2409.17992v2)

**Tags**: cs.RO cs.LG 



### TritonZ: A Remotely Operated Underwater Rover with Manipulator Arm for   Exploration and Rescue Operations
**Authors**: Kawser Ahmed, Mir Shahriar Fardin, Md Arif Faysal Nayem, Fahim Hafiz, Swakkhar Shatabda

**Updated**: 2025-06-23T06:52:38Z

**Summary**: The increasing demand for underwater exploration and rescue operations enforces the development of advanced wireless or semi-wireless underwater vessels equipped with manipulator arms. This paper presents the implementation of a semi-wireless underwater vehicle, "TritonZ" equipped with a manipulator arm, tailored for effective underwater exploration and rescue operations. The vehicle's compact design enables deployment in different submarine surroundings, addressing the need for wireless systems capable of navigating challenging underwater terrains. The manipulator arm can interact with the environment, allowing the robot to perform sophisticated tasks during exploration and rescue missions in emergency situations. TritonZ is equipped with various sensors such as Pi-Camera, Humidity, and Temperature sensors to send real-time environmental data. Our underwater vehicle controlled using a customized remote controller can navigate efficiently in the water where Pi-Camera enables live streaming of the surroundings. Motion control and video capture are performed simultaneously using this camera. The manipulator arm is designed to perform various tasks, similar to grasping, manipulating, and collecting underwater objects. Experimental results shows the efficacy of the proposed remotely operated vehicle in performing a variety of underwater exploration and rescue tasks. Additionally, the results show that TritonZ can maintain an average of 13.5cm/s with a minimal delay of 2-3 seconds. Furthermore, the vehicle can sustain waves underwater by maintaining its position as well as average velocity. The full project details and source code can be accessed at this link: https://github.com/kawser-ahmed-byte/TritonZ

**Link**: [arxiv](http://arxiv.org/abs/2506.18343v1),  [pdf](http://arxiv.org/pdf/2506.18343v1)

**Tags**: cs.RO 



### Less Data Less Tokens: Multilingual Unification Learning for Efficient   Test-Time Reasoning in LLMs
**Authors**: Kang Chen, Mengdi Zhang, Yixin Cao

**Updated**: 2025-06-23T06:47:28Z

**Summary**: This paper explores the challenges of test-time scaling of large language models (LLMs), regarding both the data and inference efficiency. We highlight the diversity of multi-lingual reasoning based on our pilot studies, and then introduce a novel approach, \(L^2\) multi-lingual unification learning with a decoding intervention strategy for further investigation. The basic idea of \(L^2\) is that the reasoning process varies across different languages, which may be mutually beneficial to enhance both model performance and efficiency. In specific, there are two types of multi-lingual data: the entire long chain-of-thought annotations in different languages and the step-wise mixture of languages. By further tuning based on them, we show that even small amounts of data can significantly improve reasoning capabilities. Our findings suggest that multilingual learning reduces both the required data and the number of inference tokens while maintaining a comparable performance. Furthermore, \(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize the importance of diverse data selection. The \(L^2\) method offers a promising solution to the challenges of data collection and test-time compute efficiency in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.18341v1),  [pdf](http://arxiv.org/pdf/2506.18341v1)

**Tags**: cs.CL 



### Position is Power: System Prompts as a Mechanism of Bias in Large   Language Models (LLMs)
**Authors**: Anna Neumann, Elisabeth Kirsten, Muhammad Bilal Zafar, Jatinder Singh

**Updated**: 2025-06-23T06:43:45Z

**Summary**: System prompts in Large Language Models (LLMs) are predefined directives that guide model behaviour, taking precedence over user inputs in text processing and generation. LLM deployers increasingly use them to ensure consistent responses across contexts. While model providers set a foundation of system prompts, deployers and third-party developers can append additional prompts without visibility into others' additions, while this layered implementation remains entirely hidden from end-users. As system prompts become more complex, they can directly or indirectly introduce unaccounted for side effects. This lack of transparency raises fundamental questions about how the position of information in different directives shapes model outputs. As such, this work examines how the placement of information affects model behaviour. To this end, we compare how models process demographic information in system versus user prompts across six commercially available LLMs and 50 demographic groups. Our analysis reveals significant biases, manifesting in differences in user representation and decision-making scenarios. Since these variations stem from inaccessible and opaque system-level configurations, they risk representational, allocative and potential other biases and downstream harms beyond the user's ability to detect or correct. Our findings draw attention to these critical issues, which have the potential to perpetuate harms if left unexamined. Further, we argue that system prompt analysis must be incorporated into AI auditing processes, particularly as customisable system prompts become increasingly prevalent in commercial AI deployments.

**Link**: [arxiv](http://arxiv.org/abs/2505.21091v3),  [pdf](http://arxiv.org/pdf/2505.21091v3)

**Tags**: cs.CY cs.AI cs.CL 



### TranslationCorrect: A Unified Framework for Machine Translation   Post-Editing with Predictive Error Assistance
**Authors**: Syed Mekael Wasti, Shou-Yi Hung, Christopher Collins, En-Shiun Annie Lee

**Updated**: 2025-06-23T06:38:49Z

**Summary**: Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce TranslationCorrect, an integrated framework designed to streamline these tasks. TranslationCorrect combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, TranslationCorrect exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that TranslationCorrect significantly improves translation efficiency and user satisfaction over traditional annotation methods.

**Link**: [arxiv](http://arxiv.org/abs/2506.18337v1),  [pdf](http://arxiv.org/pdf/2506.18337v1)

**Tags**: cs.CL 



### Confucius3-Math: A Lightweight High-Performance Reasoning LLM for   Chinese K-12 Mathematics Learning
**Authors**: Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan

**Updated**: 2025-06-23T06:23:53Z

**Summary**: We introduce Confucius3-Math, an open-source large language model with 14B parameters that (1) runs efficiently on a single consumer-grade GPU; (2) achieves SOTA performances on a range of mathematical reasoning tasks, outperforming many models with significantly larger sizes. In particular, as part of our mission to enhancing education and knowledge dissemination with AI, Confucius3-Math is specifically committed to mathematics learning for Chinese K-12 students and educators. Built via post-training with large-scale reinforcement learning (RL), Confucius3-Math aligns with national curriculum and excels at solving main-stream Chinese K-12 mathematical problems with low cost. In this report we share our development recipe, the challenges we encounter and the techniques we develop to overcome them. In particular, we introduce three technical innovations: Targeted Entropy Regularization, Recent Sample Recovery and Policy-Specific Hardness Weighting. These innovations encompass a new entropy regularization, a novel data scheduling policy, and an improved group-relative advantage estimator. Collectively, they significantly stabilize the RL training, improve data efficiency, and boost performance. Our work demonstrates the feasibility of building strong reasoning models in a particular domain at low cost. We open-source our model and code at https://github.com/netease-youdao/Confucius3-Math.

**Link**: [arxiv](http://arxiv.org/abs/2506.18330v1),  [pdf](http://arxiv.org/pdf/2506.18330v1)

**Tags**: cs.LG cs.AI cs.CL 



### A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for   Low-Light Image Enhancement
**Authors**: Muhammad Azeem Aslam, Hassan Khalid, Nisar Ahmed

**Updated**: 2025-06-23T06:11:55Z

**Summary**: Low-light image enhancement remains a challenging task, particularly in the absence of paired training data. In this study, we present LucentVisionNet, a novel zero-shot learning framework that addresses the limitations of traditional and deep learning-based enhancement methods. The proposed approach integrates multi-scale spatial attention with a deep curve estimation network, enabling fine-grained enhancement while preserving semantic and perceptual fidelity. To further improve generalization, we adopt a recurrent enhancement strategy and optimize the model using a composite loss function comprising six tailored components, including a novel no-reference image quality loss inspired by human visual perception. Extensive experiments on both paired and unpaired benchmark datasets demonstrate that LucentVisionNet consistently outperforms state-of-the-art supervised, unsupervised, and zero-shot methods across multiple full-reference and no-reference image quality metrics. Our framework achieves high visual quality, structural consistency, and computational efficiency, making it well-suited for deployment in real-world applications such as mobile photography, surveillance, and autonomous navigation.

**Link**: [arxiv](http://arxiv.org/abs/2506.18323v1),  [pdf](http://arxiv.org/pdf/2506.18323v1)

**Tags**: cs.CV cs.AI 



### Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi   Ranging
**Authors**: Emerson Sie, Enguang Fan, Federico Cifuentes-Urtubey, Deepak Vasisht

**Updated**: 2025-06-23T06:04:45Z

**Summary**: Indoor localization opens the path to potentially transformative applications. Although many indoor localization methods have been proposed over the years, they remain too impractical for widespread deployment in the real world. In this paper, we introduce PeepLoc, a deployable and scalable Wi-Fi-based solution for indoor localization that relies only on pre-existing devices and infrastructure. Specifically, PeepLoc works on any mobile device with an unmodified Wi-Fi transceiver and in any indoor environment with a sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and crowdsourcing to opportunistically initialize pre-existing APs as anchor points within an environment. We implement PeepLoc using commodity hardware and evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a mean and median positional error of 3.41 m and 3.06 m respectively, which is superior to existing deployed indoor localization systems and is competitive with commodity GPS in outdoor environments.

**Link**: [arxiv](http://arxiv.org/abs/2506.18317v1),  [pdf](http://arxiv.org/pdf/2506.18317v1)

**Tags**: cs.HC cs.NI cs.RO 



### Team LA at SCIDOCA shared task 2025: Citation Discovery via   relation-based zero-shot retrieval
**Authors**: Trieu An, Long Nguyen, Minh Le Nguyen

**Updated**: 2025-06-23T06:01:21Z

**Summary**: The Citation Discovery Shared Task focuses on predicting the correct citation from a given candidate pool for a given paragraph. The main challenges stem from the length of the abstract paragraphs and the high similarity among candidate abstracts, making it difficult to determine the exact paper to cite. To address this, we develop a system that first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph. From this subset, we leverage a Large Language Model (LLM) to accurately identify the most relevant citation. We evaluate our framework on the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its effectiveness in citation prediction.

**Link**: [arxiv](http://arxiv.org/abs/2506.18316v1),  [pdf](http://arxiv.org/pdf/2506.18316v1)

**Tags**: cs.IR cs.CL 



### Use Property-Based Testing to Bridge LLM Code Generation and Validation
**Authors**: Lehan He, Zeren Chen, Zhe Zhang, Jing Shao, Xiang Gao, Lu Sheng

**Updated**: 2025-06-23T06:01:12Z

**Summary**: Large Language Models (LLMs) excel at code generation, but ensuring their outputs to be functionally correct, especially in complex programming tasks, is a persistent challenge. While traditional Test-Driven Development (TDD) offers a path for code refinement, its efficacy with LLMs is often undermined by the scarcity of high-quality test cases or the pitfalls of automated test generation, including biased tests or inaccurate output predictions that can misdirect the correction process. This paper introduces Property-Generated Solver, a novel framework that leverages Property-Based Testing (PBT) to validate high-level program properties or invariants, instead of relying on specific input-output examples. These properties are often simpler to define and verify than directly predicting exhaustive test oracles, breaking the "cycle of self-deception" where tests might share flaws with the code they are meant to validate. Property-Generated Solver employs two collaborative LLM-based agents: a Generator dedicated to code generation and iterative refinement, and a Tester that manages the PBT life-cycle and formulate semantically rich feedback from property violations. The resulting comprehensive and actionable feedback then guides the Generator in its refinement efforts. By establishing PBT as the core validation engine within this iterative, closed-loop paradigm, Property-Generated Solver provides a robust mechanism for steering LLMs towards more correct and generalizable code. Extensive experimental results on multiple code generation benchmarks demonstrate that Property-Generated Solver achieves substantial pass@1 improvements, ranging from 23.1% to 37.3% relative gains over established TDD methods.

**Link**: [arxiv](http://arxiv.org/abs/2506.18315v1),  [pdf](http://arxiv.org/pdf/2506.18315v1)

**Tags**: cs.SE cs.AI 



### Enhancing Document Retrieval in COVID-19 Research: Leveraging Large   Language Models for Hidden Relation Extraction
**Authors**: Hoang-An Trieu, Dinh-Truong Do, Chau Nguyen, Vu Tran, Minh Le Nguyen

**Updated**: 2025-06-23T05:55:53Z

**Summary**: In recent years, with the appearance of the COVID-19 pandemic, numerous publications relevant to this disease have been issued. Because of the massive volume of publications, an efficient retrieval system is necessary to provide researchers with useful information if an unexpected pandemic happens so suddenly, like COVID-19. In this work, we present a method to help the retrieval system, the Covrelex-SE system, to provide more high-quality search results. We exploited the power of the large language models (LLMs) to extract the hidden relationships inside the unlabeled publication that cannot be found by the current parsing tools that the system is using. Since then, help the system to have more useful information during retrieval progress.

**Link**: [arxiv](http://arxiv.org/abs/2506.18311v1),  [pdf](http://arxiv.org/pdf/2506.18311v1)

**Tags**: cs.IR cs.CL 



