# Arxiv Results
## Keyword: kv cache 
 ### Superposed Decoding: Multiple Generations from a Single Autoregressive   Inference Pass
**Authors**: Ethan Shen, Alan Fan, Sarah M. Pratt, Jae Sung Park, Matthew Wallingford, Sham M. Kakade, Ari Holtzman, Ranjay Krishna, Ali Farhadi, Aditya Kusupati

**Updated**: 2024-10-21T22:56:06Z

**Summary**: Many applications today provide users with multiple auto-complete drafts as they type, including GitHub's code completion, Gmail's smart compose, and Apple's messaging auto-suggestions. Under the hood, language models support this by running an autoregressive inference pass to provide a draft. Consequently, providing $k$ drafts to the user requires running an expensive language model $k$ times. To alleviate the computation cost of running $k$ inference passes, we propose Superposed Decoding, a new decoding algorithm that generates $k$ drafts at the computation cost of one autoregressive inference pass. We achieve this by feeding a superposition of the most recent token embeddings from the $k$ drafts as input to the next decoding step of the language model. At every inference step we combine the $k$ drafts with the top-$k$ tokens to get $k^2$ new drafts and cache the $k$ most likely options, using an n-gram interpolation with minimal compute overhead to filter out incoherent generations. Our experiments show that $k$ drafts from Superposed Decoding are at least as coherent and factual as Nucleus Sampling and Greedy Decoding respectively, while being at least $2.44\times$ faster for $k\ge3$. In a compute-normalized setting, user evaluations demonstrably favor text generated by Superposed Decoding over Nucleus Sampling. Superposed Decoding can also be combined with other decoding strategies, resulting in universal coverage gains when scaling inference time compute. Code and more examples open-sourced at https://github.com/RAIVNLab/SuperposedDecoding.

**Link**: [arxiv](http://arxiv.org/abs/2405.18400v5),  [pdf](http://arxiv.org/pdf/2405.18400v5)

**Tags**: cs.CL cs.LG 



### 3 kV Monolithic Bidirectional GaN HEMT on Sapphire
**Authors**: Md Tahmidul Alam, Swarnav Mukhopadhyay, Md Mobinul Haque, Shubhra S. Pasayat, Chirag Gupta

**Updated**: 2024-10-21T17:23:03Z

**Summary**: More than 3 kV breakdown voltage was demonstrated in monolithic bidirectional GaN HEMTs for the first time having potential applications in 1200V or 1700V-class novel power converters. The on resistance of the fabricated transistors was ~20 ohm.mm or ~11 mili ohm.cm^2. Breakdown voltage was optimized by utilizing two field plates in either side of the transistor and optimizing their geometry. Shorter first field plate lengths (less than 2 micron) resulted in higher breakdown voltage and the possible reason for this was discussed. The transistors had a steep subthreshold swing of 92 mV / dec. The on/off ratio was greater than 10^5 and it was limited by the tool capacity. The fabricated 3 kV transistor was benchmarked against the state-of-the-art monolithic bidirectional GaN HEMTs in the performance matrices of breakdown voltage and on resistance, that showed crucial progress.

**Link**: [arxiv](http://arxiv.org/abs/2410.16218v1),  [pdf](http://arxiv.org/pdf/2410.16218v1)

**Tags**: physics.app-ph 



### MagicPIG: LSH Sampling for Efficient LLM Generation
**Authors**: Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen

**Updated**: 2024-10-21T16:44:51Z

**Summary**: Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by $1.9\sim3.9\times$ across various GPU hardware and achieve 110ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at \url{https://github.com/Infini-AI-Lab/MagicPIG}.

**Link**: [arxiv](http://arxiv.org/abs/2410.16179v1),  [pdf](http://arxiv.org/pdf/2410.16179v1)

**Tags**: cs.CL cs.LG 



### Do Large Language Models Need a Content Delivery Network?
**Authors**: Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang

**Updated**: 2024-10-21T15:59:18Z

**Summary**: As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling flexible and efficient injection of new knowledge in LLM inference is critical. Three high-level options exist: (i) embedding the knowledge in LLM's weights (i.e., fine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e., in-context learning), or (iii) injecting the KV caches of the new knowledge to LLM during prefill. This paper argues that, although fine-tuning and in-context learning are popular, using KV caches as the medium of knowledge could simultaneously enable more modular management of knowledge injection and more efficient LLM serving with low cost and fast response. To realize these benefits, we envision a Knowledge Delivery Network (KDN), a new system component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. We believe that, just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. We have open-sourced a KDN prototype at https://github.com/LMCache/LMCache.

**Link**: [arxiv](http://arxiv.org/abs/2409.13761v2),  [pdf](http://arxiv.org/pdf/2409.13761v2)

**Tags**: cs.CL cs.AI 



### Formalising CXL Cache Coherence
**Authors**: Chengsong Tan, Alastair F. Donaldson, John Wickerson

**Updated**: 2024-10-21T11:29:49Z

**Summary**: We report our experience formally modelling and verifying CXL.cache, the inter-device cache coherence protocol of the Compute Express Link standard. We have used the Isabelle proof assistant to create a formal model for CXL.cache based on the prose English specification. This led to us identifying and proposing fixes to several problems we identified as unclear, ambiguous or inaccurate, some of which could lead to incoherence if left unfixed. Nearly all our issues and proposed fixes have been confirmed and tentatively accepted by the CXL consortium for adoption, save for one which is still under discussion. To validate the faithfulness of our model we performed scenario verification of essential restrictions such as "Snoop-pushes-GO", and produced a fully mechanised proof of a coherence property of the model. The considerable size of this proof, comprising tens of thousands of lemmas, prompted us to develop new proof automation tools, which we have made available for other Isabelle users working with similarly cumbersome proofs.

**Link**: [arxiv](http://arxiv.org/abs/2410.15908v1),  [pdf](http://arxiv.org/pdf/2410.15908v1)

**Tags**: cs.AR cs.PL 



### Secure Collaborative Computation Offloading and Resource Allocation in   Cache-Assisted Ultra-Dense IoT Networks With Multi-Slope Channels
**Authors**: Tianqing Zhou, Bobo Wang, Dong Qin, Xuefang Nie, Nan Jiang, Chunguo Li

**Updated**: 2024-10-21T07:24:53Z

**Summary**: Cache-assisted ultra-dense mobile edge computing (MEC) networks are a promising solution for meeting the increasing demands of numerous Internet-of-Things mobile devices (IMDs). To address the complex interferences caused by small base stations (SBSs) deployed densely in such networks, this paper explores the combination of orthogonal frequency division multiple access (OFDMA), non-orthogonal multiple access (NOMA), and base station (BS) clustering. Additionally, security measures are introduced to protect IMDs' tasks offloaded to BSs from potential eavesdropping and malicious attacks. As for such a network framework, a computation offloading scheme is proposed to minimize IMDs' energy consumption while considering constraints such as delay, power, computing resources, and security costs, optimizing channel selections, task execution decisions, device associations, power controls, security service assignments, and computing resource allocations. To solve the formulated problem efficiently, we develop a further improved hierarchical adaptive search (FIHAS) algorithm, giving some insights into its parallel implementation, computation complexity, and convergence. Simulation results demonstrate that the proposed algorithms can achieve lower total energy consumption and delay compared to other algorithms when strict latency and cost constraints are imposed.

**Link**: [arxiv](http://arxiv.org/abs/2410.14142v2),  [pdf](http://arxiv.org/pdf/2410.14142v2)

**Tags**: cs.IT math.IT 



### Residual vector quantization for KV cache compression in large language   model
**Authors**: Ankur Kumar

**Updated**: 2024-10-21T07:20:41Z

**Summary**: KV cache compression methods have mainly relied on scalar quantization techniques to reduce the memory requirements during decoding. In this work, we apply residual vector quantization, which has been widely used for high fidelity audio compression, to compress KV cache in large language models (LLM). We adapt the standard recipe with minimal changes to compress the output of any key or value projection matrix in a pretrained LLM: we scale the vector by its standard deviation, divide channels into groups and then quantize each group with the same residual vector quantizer. We learn the codebook using exponential moving average and there are no other learnable parameters including the input and output projections normally used in a vector quantization set up. We find that a residual depth of 8 recovers most of the performance of the unquantized model. We also find that grouping non-contiguous channels together works better than grouping contiguous channels for compressing key matrix and the method further benefits from a light weight finetuning of LLM together with the quantization. Overall, the proposed technique is competitive with existing quantization methods while being much simpler and results in 5.5x compression compared to half precision.

**Link**: [arxiv](http://arxiv.org/abs/2410.15704v1),  [pdf](http://arxiv.org/pdf/2410.15704v1)

**Tags**: cs.LG 



### AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned   Quantization
**Authors**: Yifan Tan, Haoze Wang, Chao Yan, Yangdong Deng

**Updated**: 2024-10-21T05:06:01Z

**Summary**: Model quantization has become a crucial technique to address the issues of large memory consumption and long inference times associated with LLMs. Mixed-precision quantization, which distinguishes between important and unimportant parameters, stands out among numerous quantization schemes as it achieves a balance between precision and compression rate. However, existing approaches can only identify important parameters through qualitative analysis and manual experiments without quantitatively analyzing how their importance is determined. We propose a new criterion, so-called 'precision alignment', to build a quantitative framework to holistically evaluate the importance of parameters in mixed-precision quantization. Our observations on floating point addition under various real-world scenarios suggest that two addends should have identical precision, otherwise the information in the higher-precision number will be wasted. Such an observation offers an essential principle to determine the precision of each parameter in matrix multiplication operation. As the first step towards applying the above discovery to large model inference, we develop a dynamic KV-Cache quantization technique to effectively reduce memory access latency. Different from existing quantization approaches that focus on memory saving, this work directly aims to accelerate LLM inference through quantifying floating numbers. The proposed technique attains a 25% saving of memory access and delivers up to 1.3x speedup in the computation of attention in the decoding phase of LLM, with almost no loss of precision.

**Link**: [arxiv](http://arxiv.org/abs/2409.16546v2),  [pdf](http://arxiv.org/pdf/2409.16546v2)

**Tags**: cs.LG 



### WarmSwap: Sharing Dependencies for Accelerating Cold Starts in   Serverless Functions
**Authors**: Rui Li, Devesh Tiwari, Gene Cooperman

**Updated**: 2024-10-21T02:35:08Z

**Summary**: This work presents WarmSwap, a novel provider-side cold-start optimization for serverless computing. This optimization reduces cold-start time when booting and loading dependencies at runtime inside a function container. Previous approaches to the optimization of cold starts tend to fall into two categories: optimizing the infrastructure of serverless computing to benefit all serverless functions; or function-specific tuning for individual serverless functions. In contrast, WarmSwap offers a broad middle ground, which optimizes entire categories of serverless functions. WarmSwap eliminates the need to initialize middleware or software dependencies when launching a new serverless container, by migrating a pre-initialized live dependency image to the new function instance. WarmSwap respects the provider's cache constraints, as a single pre-warmed dependency image in the cache is shared among all serverless functions requiring that software dependency image. WarmSwap has been tested on seven representative functions from FunctionBench. In those tests, WarmSwap accelerates dependency loading for serverless functions with large dependency requirements by a factor ranging from 2.2 to 3.2. Simulation experiments using Azure traces indicate that WarmSwap can save 88\% of optimization space when sharing a dependency image among ten different functions.

**Link**: [arxiv](http://arxiv.org/abs/2409.09202v2),  [pdf](http://arxiv.org/pdf/2409.09202v2)

**Tags**: cs.DC 



### Edge AI: A Taxonomy, Systematic Review and Future Directions
**Authors**: Sukhpal Singh Gill, Muhammed Golec, Jianmin Hu, Minxian Xu, Junhui Du, Huaming Wu, Guneet Kaur Walia, Subramaniam Subramanian Murugesan, Babar Ali, Mohit Kumar, Kejiang Ye, Prabal Verma, Surendra Kumar, Felix Cuadrado, Steve Uhlig

**Updated**: 2024-10-20T13:37:46Z

**Summary**: Edge Artificial Intelligence (AI) incorporates a network of interconnected systems and devices that receive, cache, process, and analyze data in close communication with the location where the data is captured with AI technology. Recent advancements in AI efficiency, the widespread use of Internet of Things (IoT) devices, and the emergence of edge computing have unlocked the enormous scope of Edge AI. Edge AI aims to optimize data processing efficiency and velocity while ensuring data confidentiality and integrity. Despite being a relatively new field of research from 2014 to the present, it has shown significant and rapid development over the last five years. This article presents a systematic literature review for Edge AI to discuss the existing research, recent advancements, and future research directions. We created a collaborative edge AI learning system for cloud and edge computing analysis, including an in-depth study of the architectures that facilitate this mechanism. The taxonomy for Edge AI facilitates the classification and configuration of Edge AI systems while examining its potential influence across many fields through compassing infrastructure, cloud computing, fog computing, services, use cases, ML and deep learning, and resource management. This study highlights the significance of Edge AI in processing real-time data at the edge of the network. Additionally, it emphasizes the research challenges encountered by Edge AI systems, including constraints on resources, vulnerabilities to security threats, and problems with scalability. Finally, this study highlights the potential future research directions that aim to address the current limitations of Edge AI by providing innovative solutions.

**Link**: [arxiv](http://arxiv.org/abs/2407.04053v2),  [pdf](http://arxiv.org/pdf/2407.04053v2)

**Tags**: cs.DC 



### LLC Intra-set Write Balancing
**Authors**: Keshav Krishna, Ayush Verma

**Updated**: 2024-10-20T09:37:07Z

**Summary**: The increasing use of Non-Volatile Memory (NVM) in computer architecture has brought about new challenges, one of which is the write endurance problem. Frequent writes to a particular cache cell in NVM can lead to degradation of the memory cell and reduce its lifespan. To solve this problem, we propose a sample-based blocking technique for the Last Level Cache (LLC). Our approach involves defining a threshold value and sampling a subset of cache sets. If the number of writes to a way in a sampled set exceeds the threshold, the way is blocked, and writes are redirected to other ways. We also maintain a history structure to record the number of writes in a set and a PC-Table to use for blocking in unsampled sets. Based on blocking on sampled sets, variance of values stored in history is used to determine whether blocking had a positive impact or not, and on this basis, value corresponding to instruction pointer is incremented or decremented. This value is later used for blocking in unsampled sets. Our results show that our approach significantly balances write traffic to the cache and improves the overall lifespan of the memory cells while having better performance to the base-line system. Our approach can also be applied to other cache hierarchies and NVM technologies to mitigate the problem of write endurance.

**Link**: [arxiv](http://arxiv.org/abs/2410.15344v1),  [pdf](http://arxiv.org/pdf/2410.15344v1)

**Tags**: cs.AR 



### EPIC: Efficient Position-Independent Context Caching for Serving Large   Language Models
**Authors**: Junhao Hu, Wenrui Huang, Haoyi Wang, Weidong Wang, Tiancheng Hu, Qin Zhang, Hao Feng, Xusheng Chen, Yizhou Shan, Tao Xie

**Updated**: 2024-10-20T08:42:29Z

**Summary**: Large Language Models (LLMs) are critical for a wide range of applications, but serving them efficiently becomes increasingly challenging as inputs become more complex. Context caching improves serving performance by exploiting inter-request dependency and reusing key-value (KV) cache across requests, thus improving time-to-first-token (TTFT). However, existing prefix-based context caching requires exact token prefix matches, limiting cache reuse in few-shot learning, multi-document QA, or retrieval-augmented generation, where prefixes may vary. In this paper, we present EPIC, an LLM serving system that introduces position-independent context caching (PIC), enabling modular KV cache reuse regardless of token chunk position (or prefix). EPIC features two key designs: AttnLink, which leverages static attention sparsity to minimize recomputation for accuracy recovery, and KVSplit, a customizable chunking method that preserves semantic coherence. Our experiments demonstrate that Epic delivers up to 8x improvements in TTFT and 7x throughput over existing systems, with negligible or no accuracy loss. By addressing the limitations of traditional caching approaches, Epic enables more scalable and efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.15332v1),  [pdf](http://arxiv.org/pdf/2410.15332v1)

**Tags**: cs.LG cs.CL cs.DC cs.PF 



### Lossless KV Cache Compression to 2%
**Authors**: Zhen Yang, J. N. Han, Kan Wu, Ruobing Xie, An Wang, Xingwu Sun, Zhanhui Kang

**Updated**: 2024-10-20T02:17:35Z

**Summary**: Large language models have revolutionized data processing in numerous domains, with their ability to handle extended context reasoning receiving notable recognition. To speed up inference, maintaining a key-value (KV) cache memory is essential. Nonetheless, the growing demands for KV cache memory create significant hurdles for efficient implementation. This work introduces a novel architecture, Cross-Layer Latent Attention (CLLA), aimed at compressing the KV cache to less than 2% of its original size while maintaining comparable performance levels. CLLA integrates multiple aspects of KV cache compression, including attention head/dimension reduction, layer sharing, and quantization techniques, into a cohesive framework. Our extensive experiments demonstrate that CLLA achieves lossless performance on most tasks while utilizing minimal KV cache, marking a significant advancement in practical KV cache compression.

**Link**: [arxiv](http://arxiv.org/abs/2410.15252v1),  [pdf](http://arxiv.org/pdf/2410.15252v1)

**Tags**: cs.CL cs.AI 



### Online Paging with Heterogeneous Cache Slots
**Authors**: Marek Chrobak, Samuel Haney, Mehraneh Liaee, Debmalya Panigrahi, Rajmohan Rajaraman, Ravi Sundaram, Neal E. Young

**Updated**: 2024-10-19T12:15:50Z

**Summary**: It is natural to generalize the online $k$-Server problem by allowing each request to specify not only a point $p$, but also a subset $S$ of servers that may serve it. For uniform metrics, the problem is equivalent to a generalization of Paging in which each request specifies not only a page $p$, but also a subset $S$ of cache slots, and is satisfied by having a copy of $p$ in some slot in $S$. We call this problem Slot-Heterogenous Paging.   We parameterize the problem by specifying a family $\mathcal S \subseteq 2^{[k]}$ of requestable slot sets, and we establish bounds on the competitive ratio as a function of the cache size $k$ and family $\mathcal S$:   - If all request sets are allowed ($\mathcal S=2^{[k]}\setminus\{\emptyset\}$), the optimal deterministic and randomized competitive ratios are exponentially worse than for standard \Paging ($\mathcal S=\{[k]\}$).   - As a function of $|\mathcal S|$ and $k$, the optimal deterministic ratio is polynomial: at most $O(k^2|\mathcal S|)$ and at least $\Omega(\sqrt{|\mathcal S|})$.   - For any laminar family $\mathcal S$ of height $h$, the optimal ratios are $O(hk)$ (deterministic) and $O(h^2\log k)$ (randomized).   - The special case of laminar $\mathcal S$ that we call All-or-One Paging extends standard Paging by allowing each request to specify a specific slot to put the requested page in. The optimal deterministic ratio for weighted All-or-One Paging is $\Theta(k)$. Offline All-or-One Paging is NP-hard.   Some results for the laminar case are shown via a reduction to the generalization of Paging in which each request specifies a set $\mathcal P of pages, and is satisfied by fetching any page from $\mathcal P into the cache. The optimal ratios for the latter problem (with laminar family of height $h$) are at most $hk$ (deterministic) and $h\,H_k$ (randomized).

**Link**: [arxiv](http://arxiv.org/abs/2206.05579v4),  [pdf](http://arxiv.org/pdf/2206.05579v4)

**Tags**: cs.DS F.2.0; F.1.2; C.0 



### In-context KV-Cache Eviction for LLMs via Attention-Gate
**Authors**: Zihao Zeng, Bokai Lin, Tianqi Hou, Hao Zhang, Zhijie Deng

**Updated**: 2024-10-19T08:45:11Z

**Summary**: The KV-Cache technique has become the standard for the inference of large language models (LLMs). It caches states of self-attention to avoid recomputation. Yet, it is widely criticized that KV-Cache can become a bottleneck of the LLM inference system, especially when confronted with ultra-large models and long-context queries. A natural remedy is to discard the KV-Cache for less important tokens, with StreamingLLM as an example, but the used static eviction strategies cannot flexibly adapt to varying contexts. Remedies like H2O leverage accumulative attention scores to perform dynamic eviction but suffer from the attention bias issue in capturing contextual information. This paper bridges this gap by devising a parameterized KV-Cache eviction mechanism, dubbed as Attention-Gate, which accepts the whole context as input and yields eviction flags for each token to realize in-context eviction. The subsequent self-attention module proceeds according to the flags and only the KV states for the remaining tokens need to be cached. The Attention-Gates can vary among different heads and layers and be trivially plugged into pre-trained LLMs, tuned by cost-effective continual pre-training or supervised fine-tuning objectives to acquire what to discard. The computational and memory overhead introduced by Attention-Gates is minimal. Our method is validated across multiple tasks, demonstrating both efficiency and adaptability. After a highly efficient continual pre-training, it achieves higher average accuracy and evicts more tokens compared to traditional training-free methods. In supervised fine-tuning, it not only evicts many tokens but also outperforms LoRA-finetuned LLMs on some datasets, such as RTE, where it improves accuracy by 13.9% while evicting 62.8% of tokens, showing that effective eviction of redundant tokens can even enhance performance.

**Link**: [arxiv](http://arxiv.org/abs/2410.12876v2),  [pdf](http://arxiv.org/pdf/2410.12876v2)

**Tags**: cs.CL cs.LG 



### CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context   Scenarios
**Authors**: Luning Wang, Shiyao Li, Xuefei Ning, Zhihang Yuan, Shengen Yan, Guohao Dai, Yu Wang

**Updated**: 2024-10-18T19:30:35Z

**Summary**: Large Language Models (LLMs) have been widely adopted to process long-context tasks. However, the large memory overhead of the key-value (KV) cache poses significant challenges in long-context scenarios. Existing training-free KV cache compression methods typically focus on quantization and token pruning, which have compression limits, and excessive sparsity can lead to severe performance degradation. Other methods design new architectures with less KV overhead but require significant training overhead. To address the above two drawbacks, we further explore the redundancy in the channel dimension and apply an architecture-level design with minor training costs. Therefore, we introduce CSKV, a training-efficient Channel Shrinking technique for KV cache compression: (1) We first analyze the singular value distribution of the KV cache, revealing significant redundancy and compression potential along the channel dimension. Based on this observation, we propose using low-rank decomposition for key and value layers and storing the low-dimension features. (2) To preserve model performance, we introduce a bi-branch KV cache, including a window-based full-precision KV cache and a low-precision compressed KV cache. (3) To reduce the training costs, we minimize the layer-wise reconstruction loss for the compressed KV cache instead of retraining the entire LLMs. Extensive experiments show that CSKV can reduce the memory overhead of the KV cache by 80% while maintaining the model's long-context capability. Moreover, we show that our method can be seamlessly combined with quantization to further reduce the memory overhead, achieving a compression ratio of up to 95%. Code is available at https://github.com/wln20/CSKV.

**Link**: [arxiv](http://arxiv.org/abs/2409.10593v3),  [pdf](http://arxiv.org/pdf/2409.10593v3)

**Tags**: cs.LG cs.AI cs.CL 



### Improving Retrieval in Sponsored Search by Leveraging Query Context   Signals
**Authors**: Akash Kumar Mohankumar, Gururaj K, Gagan Madan, Amit Singh

**Updated**: 2024-10-18T13:59:54Z

**Summary**: Accurately retrieving relevant bid keywords for user queries is critical in Sponsored Search but remains challenging, particularly for short, ambiguous queries. Existing dense and generative retrieval models often fail to capture nuanced user intent in these cases. To address this, we propose an approach to enhance query understanding by augmenting queries with rich contextual signals derived from web search results and large language models, stored in an online cache. Specifically, we use web search titles and snippets to ground queries in real-world information and utilize GPT-4 to generate query rewrites and explanations that clarify user intent. These signals are efficiently integrated through a Fusion-in-Decoder based Unity architecture, enabling both dense and generative retrieval with serving costs on par with traditional context-free models. To address scenarios where context is unavailable in the cache, we introduce context glancing, a curriculum learning strategy that improves model robustness and performance even without contextual signals during inference. Extensive offline experiments demonstrate that our context-aware approach substantially outperforms context-free models. Furthermore, online A/B testing on a prominent search engine across 160+ countries shows significant improvements in user engagement and revenue.

**Link**: [arxiv](http://arxiv.org/abs/2407.14346v2),  [pdf](http://arxiv.org/pdf/2407.14346v2)

**Tags**: cs.IR cs.CL 



### A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference
**Authors**: You Wu, Haoyi Wu, Kewei Tu

**Updated**: 2024-10-18T13:01:14Z

**Summary**: Recently, sharing key-value (KV) cache across layers has been found effective in efficient inference of large language models (LLMs). To systematically investigate different techniques of cross-layer KV sharing, we propose a unified framework that covers several recent methods and their novel variants. We conduct comprehensive experiments on all the configurations of the framework, evaluating their generation throughput and performance in language modeling and downstream tasks. We find that when reducing the size of the KV cache by 2x, most configurations can achieve competitive performance to and higher throughput than standard transformers, but when further reducing the size of the KV cache, pairing queries of all layers with KVs of upper layers can better maintain performance, although it also introduces additional training cost and prefilling latency. We hope that this work will help users choose the appropriate approach according to their requirements and facilitate research on the acceleration of LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.14442v1),  [pdf](http://arxiv.org/pdf/2410.14442v1)

**Tags**: cs.CL 



### FAME: Towards Factual Multi-Task Model Editing
**Authors**: Li Zeng, Yingyu Shan, Zeming Liu, Jiashu Yao, Yuhang Guo

**Updated**: 2024-10-18T10:02:03Z

**Summary**: Large language models (LLMs) embed extensive knowledge and utilize it to perform exceptionally well across various tasks. Nevertheless, outdated knowledge or factual errors within LLMs can lead to misleading or incorrect responses, causing significant issues in practical applications. To rectify the fatal flaw without the necessity for costly model retraining, various model editing approaches have been proposed to correct inaccurate knowledge within LLMs in a cost-efficient way. To evaluate these model editing methods, previous work introduced a series of datasets. However, most of the previous datasets only contain fabricated data in a single format, which diverges from real-world model editing scenarios, raising doubts about their usability in practice. To facilitate the application of model editing in real-world scenarios, we propose the challenge of practicality. To resolve such challenges and effectively enhance the capabilities of LLMs, we present FAME, an factual, comprehensive, and multi-task dataset, which is designed to enhance the practicality of model editing. We then propose SKEME, a model editing method that uses a novel caching mechanism to ensure synchronization with the real world. The experiments demonstrate that SKEME performs excellently across various tasks and scenarios, confirming its practicality.

**Link**: [arxiv](http://arxiv.org/abs/2410.10859v2),  [pdf](http://arxiv.org/pdf/2410.10859v2)

**Tags**: cs.CL cs.AI 



### Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time   Systems
**Authors**: Connor Sullivan, Alex Manley, Mohammad Alian, Heechul Yun

**Updated**: 2024-10-17T20:11:34Z

**Summary**: Modern commercial-off-the-shelf (COTS) multicore processors have advanced memory hierarchies that enhance memory-level parallelism (MLP), which is crucial for high performance. To support high MLP, shared last-level caches (LLCs) are divided into multiple banks, allowing parallel access. However, uneven distribution of cache requests from the cores, especially when requests from multiple cores are concentrated on a single bank, can result in significant contention affecting all cores that access the cache. Such cache bank contention can even be maliciously induced -- known as cache bank-aware denial-of-service (DoS) attacks -- in order to jeopardize the system's timing predictability.   In this paper, we propose a per-bank bandwidth regulation approach for multi-banked shared LLC based multicore real-time systems. By regulating bandwidth on a per-bank basis, the approach aims to prevent unnecessary throttling of cache accesses to non-contended banks, thus improving overall performance (throughput) without compromising isolation benefits of throttling. We implement our approach on a RISC-V system-on-chip (SoC) platform using FireSim and evaluate extensively using both synthetic and real-world workloads. Our evaluation results show that the proposed per-bank regulation approach effectively protects real-time tasks from co-running cache bank-aware DoS attacks, and offers up to a 3.66$\times$ performance improvement for the throttled benign best-effort tasks compared to prior bank-oblivious bandwidth throttling approaches.

**Link**: [arxiv](http://arxiv.org/abs/2410.14003v1),  [pdf](http://arxiv.org/pdf/2410.14003v1)

**Tags**: cs.AR 



### SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction
**Authors**: Xuan Zhang, Cunxiao Du, Chao Du, Tianyu Pang, Wei Gao, Min Lin

**Updated**: 2024-10-17T17:58:14Z

**Summary**: Recent advancements in large language models (LLMs) have extended their capabilities to handle long contexts. However, increasing the number of model layers and the length of input sequences significantly escalates the memory required to store key-value (KV) cache, posing challenges for efficient inference. To mitigate this issue, we present SimLayerKV, a simple yet effective method that reduces inter-layer KV cache redundancies by selectively dropping cache in identified lazy layers. Our approach is based on the observation that certain layers in long-context LLMs exhibit "lazy" behavior, contributing less to modeling long-range dependencies compared to non-lazy layers. By analyzing attention weight patterns, we find that the behavior of these lazy layers is consistent across tokens during generation for a given input. This insight motivates our SimLayerKV, which identifies lazy layers and reduces their KV cache accordingly. SimLayerKV is training-free, generalizable, and can be implemented with only seven lines of code. We conduct extensive experiments on three representative LLMs, e.g., LLaMA2-7B, LLaMA3-8B, and Mistral-7B across 16 tasks from the LongBench benchmark. The results demonstrate that SimLayerKV achieves a KV cache compression ratio of 5$\times$ with only a 1.2% performance drop when combined with 4-bit quantization. Our code is available at https://github.com/sail-sg/SimLayerKV.

**Link**: [arxiv](http://arxiv.org/abs/2410.13846v1),  [pdf](http://arxiv.org/pdf/2410.13846v1)

**Tags**: cs.CL cs.AI cs.LG 



### Block-Attention for Efficient RAG
**Authors**: East Sun, Yan Wang, Lan Tian

**Updated**: 2024-10-17T15:27:30Z

**Summary**: We introduce Block-Attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. Traditional approaches often encode the entire context. Instead, Block-Attention divides retrieved documents into discrete blocks, with each block independently calculating key-value (KV) states except for the final block. In RAG scenarios, by defining each passage as a block, Block-Attention enables us to reuse the KV states of passages that have been seen before, thereby significantly reducing the latency and the computation overhead during inference. The implementation of Block-Attention involves block segmentation, position re-encoding, and fine-tuning the LLM to adapt to the Block-Attention mechanism. Experiments on four RAG benchmarks demonstrate that after block fine-tuning, the Block-Attention model achieves performance comparable to self-attention models (68.4\% vs 67.9\% on Llama3) or even superior performance (62.8\% vs 59.6\% on Mistral). Notably, Block-Attention significantly reduces the time to first token (TTFT) and floating point operations (FLOPs) to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared to the self-attention models, the time consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2409.15355v4),  [pdf](http://arxiv.org/pdf/2409.15355v4)

**Tags**: cs.LG cs.AI cs.CL 



### LLoCO: Learning Long Contexts Offline
**Authors**: Sijun Tan, Xiuyu Li, Shishir Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, Raluca Ada Popa

**Updated**: 2024-10-17T08:54:37Z

**Summary**: Processing long contexts remains a challenge for large language models (LLMs) due to the quadratic computational and memory overhead of the self-attention mechanism and the substantial KV cache sizes during generation. We propose LLoCO, a novel approach to address this problem by learning contexts offline through context compression and in-domain parameter-efficient finetuning with LoRA. Our method enables an LLM to create a concise representation of the original context and efficiently retrieve relevant information to answer questions accurately. Our approach extends the effective context window of a 4k token LLaMA2-7B model to handle up to 128k tokens. We evaluate our approach on several long-context question-answering datasets, demonstrating that LLoCO significantly outperforms in-context learning while using $30\times$ fewer tokens during inference. LLoCO achieves up to $7.62\times$ speed-up during inference and $11.52\times$ higher throughput during finetuning, substantially reduces the cost of long document question answering. This makes it a promising solution for efficient long context processing. Our code is publicly available on https://github.com/jeffreysijuntan/lloco.

**Link**: [arxiv](http://arxiv.org/abs/2404.07979v2),  [pdf](http://arxiv.org/pdf/2404.07979v2)

**Tags**: cs.CL cs.AI cs.LG 



### Harnessing Your DRAM and SSD for Sustainable and Accessible LLM   Inference with Mixed-Precision and Multi-level Caching
**Authors**: Jie Peng, Zhang Cao, Huaizhi Qu, Zhengyu Zhang, Chang Guo, Yanyong Zhang, Zhichao Cao, Tianlong Chen

**Updated**: 2024-10-23T01:08:59Z

**Summary**: Although Large Language Models (LLMs) have demonstrated remarkable capabilities, their massive parameter counts and associated extensive computing make LLMs' deployment the main part of carbon emission from nowadays AI applications. Compared to modern GPUs like H$100$, it would be significantly carbon-sustainable if we could leverage old-fashioned GPUs such as M$40$ (as shown in Figure 1, M$40$ only has one third carbon emission of H$100$'s) for LLM servings. However, the limited High Bandwidth Memory (HBM) available on such GPU often cannot support the loading of LLMs due to the gigantic model size and intermediate activation data, making their serving challenging. For instance, a LLaMA2 model with $70$B parameters typically requires $128$GB for inference, which substantially surpasses $24$GB HBM in a $3090$ GPU and remains infeasible even considering the additional $64$GB DRAM. To address this challenge, this paper proposes a mixed-precision with a model modularization algorithm to enable LLM inference on outdated hardware with resource constraints. (The precision denotes the numerical precision like FP16, INT8, INT4) and multi-level caching (M2Cache).)   Specifically, our M2Cache first modulizes neurons in LLM and creates their importance ranking. Then, it adopts a dynamic sparse mixed-precision quantization mechanism in weight space to reduce computational demands and communication overhead at each decoding step. It collectively lowers the operational carbon emissions associated with LLM inference. Moreover, M2Cache introduces a three-level cache management system with HBM, DRAM, and SSDs that complements the dynamic sparse mixed-precision inference. To enhance communication efficiency, M2Cache maintains a neuron-level mixed-precision LRU cache in HBM, a larger layer-aware cache in DRAM, and a full model in SSD.

**Link**: [arxiv](http://arxiv.org/abs/2410.14740v2),  [pdf](http://arxiv.org/pdf/2410.14740v2)

**Tags**: cs.LG cs.DC 



### AsymKV: Enabling 1-Bit Quantization of KV Cache with Layer-Wise   Asymmetric Quantization Configurations
**Authors**: Qian Tao, Wenyuan Yu, Jingren Zhou

**Updated**: 2024-10-17T04:35:57Z

**Summary**: Large language models have shown exceptional capabilities in a wide range of tasks, such as text generation and video generation, among others. However, due to their massive parameter count, these models often require substantial storage space, imposing significant constraints on the machines deploying LLMs. To overcome this limitation, one research direction proposes to compress the models using integer replacements for floating-point numbers, in a process known as Quantization. Some recent studies suggest quantizing the key and value cache (KV Cache) of LLMs, and designing quantization techniques that treat the key and value matrices equivalently.   This work delves deeper into the asymmetric structural roles of KV Cache, a phenomenon where the transformer's output loss is more sensitive to the quantization of key matrices. We conduct a systematic examination of the attention output error resulting from key and value quantization. The phenomenon inspires us to propose an asymmetric quantization strategy. Our approach allows for 1-bit quantization of the KV cache by implementing distinct configurations for key and value matrices. We carry out experiments across a variety of datasets, demonstrating that our proposed model allows for the quantization of up to 75% decoder layers with 1 bit, while simultaneously maintaining performance levels comparable to those of the models with floating parameters.

**Link**: [arxiv](http://arxiv.org/abs/2410.13212v1),  [pdf](http://arxiv.org/pdf/2410.13212v1)

**Tags**: cs.LG cs.AI 



### cedar: Optimized and Unified Machine Learning Input Data Pipelines
**Authors**: Mark Zhao, Emanuel Adamiak, Christos Kozyrakis

**Updated**: 2024-10-16T17:54:15Z

**Summary**: The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources - or worse - underutilize expensive accelerators.   To address these demands, we present cedar, an optimized and unified programming framework for ML input data pipelines. cedar allows users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. cedar introduces an extensible optimizer that systematically applies a complex combination of optimizations (e.g., offloading, caching, prefetching, fusion, and reordering). It orchestrates processing across a customizable set of local and distributed compute resources in order to improve processing performance and efficiency, all without user input. Across eight pipelines, cedar improves performance by up to 1.87x to 10.65x compared to state-of-the-art input data systems.

**Link**: [arxiv](http://arxiv.org/abs/2401.08895v3),  [pdf](http://arxiv.org/pdf/2401.08895v3)

**Tags**: cs.LG cs.DC cs.PF 



### Toleo: Scaling Freshness to Tera-scale Memory using CXL and PIM
**Authors**: Juechu Dong, Jonah Rosenblum, Satish Narayanasamy

**Updated**: 2024-10-16T17:10:48Z

**Summary**: Trusted hardware's freshness guarantee ensures that an adversary cannot replay an old value in response to a memory read request. They rely on maintaining a version number for each cache block and ensuring their integrity using a Merkle tree. However, these existing solutions protect only a small amount of main memory (few MBs), as the extraneous memory accesses to the Merkle tree increase prohibitively with the protected memory size. We present Toleo, which uses trusted smart memory connected through a secure CXL IDE network to safely store version numbers. Toleo eliminates the need for an unscalable Merkle tree to protect the integrity of version numbers by instead using smart memory as the root of trust. Additionally, Toleo ensures version confidentiality which enables stealth versions that reduce the version storage overhead in half.   Furthermore, in the absence of Merkle tree imposed constraints, we effectively exploit version locality at page granularity to compress version number by a factor of 240. These space optimizations make it feasible for one 168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded main memory pool in a rack server for a negligible performance overhead. We analyze the benefits of Toleo using several privacy-sensitive genomics, graph, generative AI, and database workloads.

**Link**: [arxiv](http://arxiv.org/abs/2410.12749v1),  [pdf](http://arxiv.org/pdf/2410.12749v1)

**Tags**: cs.AR cs.CR 



### Advancing Web Browser Forensics: Critical Evaluation of Emerging Tools   and Techniques
**Authors**: Rishal Ravikesh Chand, Neeraj Anand Sharma, Muhammad Ashad Kabir

**Updated**: 2024-10-16T14:24:16Z

**Summary**: As the use of web browsers continues to grow, the potential for cybercrime and web-related criminal activities also increases. Digital forensic investigators must understand how different browsers function and the critical areas to consider during web forensic analysis. Web forensics, a subfield of digital forensics, involves collecting and analyzing browser artifacts, such as browser history, search keywords, and downloads, which serve as potential evidence. While existing research has provided valuable insights, many studies focus on individual browsing modes or limited forensic scenarios, leaving gaps in understanding the full scope of data retention and recovery across different modes and browsers. This paper addresses these gaps by defining four browsing scenarios and critically analyzing browser artifacts across normal, private, and portable modes using various forensic tools. We define four browsing scenarios to perform a comprehensive evaluation of popular browsers -- Google Chrome, Mozilla Firefox, Brave, Tor, and Microsoft Edge -- by monitoring changes in key data storage areas such as cache files, cookies, browsing history, and local storage across different browsing modes. Overall, this paper contributes to a deeper understanding of browser forensic analysis and identifies key areas for enhancing privacy protection and forensic methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2410.12605v1),  [pdf](http://arxiv.org/pdf/2410.12605v1)

**Tags**: cs.CR 



### FiRST: Finetuning Router-Selective Transformers for Input-Adaptive   Latency Reduction
**Authors**: Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal

**Updated**: 2024-10-16T12:45:35Z

**Summary**: Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across domanins such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FIRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during prefill stage) decides which layers will be skipped during decoding. FIRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FIRST is model-agnostic and can be easily enabled on any pre-trained LLM. We further improve performance by incorporating LoRA adapters for fine-tuning on external datasets, enhancing task-specific accuracy while maintaining latency benefits. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on task. Extensive experiments show that FIRST significantly reduces latency while retaining competitive performance (as compared to baselines), making our approach an efficient solution for LLM deployment in low-resource environments.

**Link**: [arxiv](http://arxiv.org/abs/2410.12513v1),  [pdf](http://arxiv.org/pdf/2410.12513v1)

**Tags**: cs.CL 



### An O(m+n)-Space Spatiotemporal Denoising Filter with Cache-Like Memories   for Dynamic Vision Sensors
**Authors**: Qinghang Zhao, Jiaqi Wang, Yixi Ji, Jinjian Wu, Guangming Shi

**Updated**: 2024-10-16T10:06:22Z

**Summary**: Dynamic vision sensor (DVS) is novel neuromorphic imaging device that generates asynchronous events. Despite the high temporal resolution and high dynamic range features, DVS is faced with background noise problem. Spatiotemporal filter is an effective and hardware-friendly solution for DVS denoising but previous designs have large memory overhead or degraded performance issues. In this paper, we present a lightweight and real-time spatiotemporal denoising filter with set-associative cache-like memories, which has low space complexity of \text{O(m+n)} for DVS of $m\times n$ resolution. A two-stage pipeline for memory access with read cancellation feature is proposed to reduce power consumption. Further the bitwidth redundancy for event storage is exploited to minimize the memory footprint. We implemented our design on FPGA and experimental results show that it achieves state-of-the-art performance compared with previous spatiotemporal filters while maintaining low resource utilization and low power consumption of about 125mW to 210mW at 100MHz clock frequency.

**Link**: [arxiv](http://arxiv.org/abs/2410.12423v1),  [pdf](http://arxiv.org/pdf/2410.12423v1)

**Tags**: cs.AR 



### MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal   Projection
**Authors**: Bokai Lin, Zihao Zeng, Zipeng Xiao, Siqi Kou, Tianqi Hou, Xiaofeng Gao, Hao Zhang, Zhijie Deng

**Updated**: 2024-10-16T08:34:51Z

**Summary**: KV cache has become a de facto technique for the inference of large language models (LLMs), where tensors of shape (layer number, head number, sequence length, feature dimension) are introduced to cache historical information for self-attention. As the size of the model and data grows, the KV cache can quickly become a bottleneck within the system in both storage and memory transfer. To address this, prior studies usually focus on the first three axes of the cache tensors for compression. This paper supplements them, focusing on the feature dimension axis, by utilizing low-rank projection matrices to transform the cache features into spaces with reduced dimensions. We begin by investigating the canonical orthogonal projection method for data compression through principal component analysis (PCA). We observe the issue with PCA projection where significant performance degradation is observed at low compression rates. To bridge the gap, we propose to directly tune the orthogonal projection matrices with a distillation objective using an elaborate Matryoshka training strategy. After training, we adaptively search for the optimal compression rates for various layers and heads given varying compression budgets. Compared to previous works, our method can easily embrace pre-trained LLMs and hold a smooth tradeoff between performance and compression rate. We empirically witness the high data efficiency of our training procedure and find that our method can sustain over 90% performance with an average KV cache compression rate of 60% (and up to 75% in certain extreme scenarios) for popular LLMs like LLaMA2-7B-base and Mistral-7B-v0.3-base.

**Link**: [arxiv](http://arxiv.org/abs/2410.14731v1),  [pdf](http://arxiv.org/pdf/2410.14731v1)

**Tags**: cs.LG cs.AI cs.CL 



### COMET: Towards Partical W4A4KV4 LLMs Serving
**Authors**: Lian Liu, Haimeng Ren, Long Cheng, Zhaohui Xu, Yudong Pan, Mengdi Wang, Xiaowei Li, Yinhe Han, Ying Wang

**Updated**: 2024-10-16T02:16:53Z

**Summary**: Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of \textbf{$2.88\times$} over cuBLAS and a \textbf{$2.02 \times$} throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.

**Link**: [arxiv](http://arxiv.org/abs/2410.12168v1),  [pdf](http://arxiv.org/pdf/2410.12168v1)

**Tags**: cs.AR cs.LG 



### Mitigate Position Bias in Large Language Models via Scaling a Single   Dimension
**Authors**: Yijiong Yu, Huiqiang Jiang, Xufang Luo, Qianhui Wu, Chin-Yew Lin, Dongsheng Li, Yuqing Yang, Yongfeng Huang, Lili Qiu

**Updated**: 2024-10-15T15:58:07Z

**Summary**: Large Language Models (LLMs) are increasingly applied in various real-world scenarios due to their excellent generalization capabilities and robust generative abilities. However, they exhibit position bias, also known as "lost in the middle", a phenomenon that is especially pronounced in long-context scenarios, which indicates the placement of the key information in different positions of a prompt can significantly affect accuracy. This paper first explores the micro-level manifestations of position bias, concluding that attention weights are a micro-level expression of position bias. It further identifies that, in addition to position embeddings, causal attention mask also contributes to position bias by creating position-specific hidden states. Based on these insights, we propose a method to mitigate position bias by scaling this positional hidden states. Experiments on the NaturalQuestions Multi-document QA, KV retrieval, LongBench and timeline reorder tasks, using various models including RoPE models, context windowextended models, and Alibi models, demonstrate the effectiveness and generalizability of our approach. Our method can improve performance by up to 15.2% by modifying just one dimension of hidden states. Our code is available at https://aka.ms/PositionalHidden.

**Link**: [arxiv](http://arxiv.org/abs/2406.02536v2),  [pdf](http://arxiv.org/pdf/2406.02536v2)

**Tags**: cs.CL cs.LG 



### VidCompress: Memory-Enhanced Temporal Compression for Video   Understanding in Large Language Models
**Authors**: Xiaohan Lan, Yitian Yuan, Zequn Jie, Lin Ma

**Updated**: 2024-10-15T09:07:25Z

**Summary**: Video-based multimodal large language models (Video-LLMs) possess significant potential for video understanding tasks. However, most Video-LLMs treat videos as a sequential set of individual frames, which results in insufficient temporal-spatial interaction that hinders fine-grained comprehension and difficulty in processing longer videos due to limited visual token capacity. To address these challenges, we propose VidCompress, a novel Video-LLM featuring memory-enhanced temporal compression. VidCompress employs a dual-compressor approach: a memory-enhanced compressor captures both short-term and long-term temporal relationships in videos and compresses the visual tokens using a multiscale transformer with a memory-cache mechanism, while a text-perceived compressor generates condensed visual tokens by utilizing Q-Former and integrating temporal contexts into query embeddings with cross attention. Experiments on several VideoQA datasets and comprehensive benchmarks demonstrate that VidCompress efficiently models complex temporal-spatial relations and significantly outperforms existing Video-LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.11417v1),  [pdf](http://arxiv.org/pdf/2410.11417v1)

**Tags**: cs.CV cs.MM 



### MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer   Decoding
**Authors**: Zayd Muhammad Kawakibi Zuhri, Muhammad Farid Adilazuarda, Ayu Purwarianti, Alham Fikri Aji

**Updated**: 2024-10-15T08:45:18Z

**Summary**: Auto-regressive inference of transformers benefit greatly from Key-Value (KV) caching, but can lead to major memory bottlenecks as model size, batch size, and sequence length grow at scale. We introduce Multi-Layer Key-Value (MLKV) sharing, a novel approach extending KV sharing across transformer layers to reduce memory usage beyond what was possible with Multi-Query Attention (MQA) and Grouped-Query Attention (GQA). Evaluations on various NLP benchmarks and inference metrics using uptrained Pythia-160M variants demonstrate that MLKV significantly reduces memory usage with minimal performance loss, reducing KV cache size down to a factor of 6x compared to MQA. These results highlight MLKV's potential for efficient deployment of transformer models at scale. We provide code at https://github.com/zaydzuhri/pythia-mlkv

**Link**: [arxiv](http://arxiv.org/abs/2406.09297v3),  [pdf](http://arxiv.org/pdf/2406.09297v3)

**Tags**: cs.LG 



### A Training-free Sub-quadratic Cost Transformer Model Serving Framework   With Hierarchically Pruned Attention
**Authors**: Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyoung Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2024-10-15T06:09:35Z

**Summary**: In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. While many recent transformer models attempt to extend their context length over a million tokens, they remain impractical due to the quadratic time and space complexities. Although recent works on linear and sparse attention mechanisms can achieve this goal, their real-world applicability is often limited by the need to re-train from scratch and significantly worse performance. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which reduces the time complexity of the attention mechanism to $O(T \log T)$ and the space complexity to $O(T)$, where $T$ is the sequence length. We notice a pattern in the attention scores of pretrained LLMs where tokens close together tend to have similar scores, which we call ``attention locality''. Based on this observation, we utilize a novel tree-search-like algorithm that estimates the top-$k$ key tokens for a given query on the fly, which is mathematically guaranteed to have better performance than random attention pruning. In addition to improving the time complexity of the attention mechanism, we further optimize GPU memory usage by implementing KV cache offloading, which stores only $O(\log T)$ tokens on the GPU while maintaining similar decoding throughput. Experiments on benchmarks show that HiP, with its training-free nature, significantly reduces both prefill and decoding latencies, as well as memory usage, while maintaining high-quality generation with minimal degradation. HiP enables pretrained LLMs to scale up to millions of tokens on commodity GPUs, potentially unlocking long-context LLM applications previously deemed infeasible.

**Link**: [arxiv](http://arxiv.org/abs/2406.09827v2),  [pdf](http://arxiv.org/pdf/2406.09827v2)

**Tags**: cs.CL cs.CV cs.DC cs.LG 



### QSpec: Speculative Decoding with Complementary Quantization Schemes
**Authors**: Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu

**Updated**: 2024-10-15T05:57:51Z

**Summary**: Quantization has been substantially adopted to accelerate inference and reduce memory consumption of large language models (LLMs). While activation-weight joint quantization speeds up the inference process through low-precision kernels, we demonstrate that it suffers severe performance degradation on multi-step reasoning tasks, rendering it ineffective. We propose a novel quantization paradigm called QSPEC, which seamlessly integrates two complementary quantization schemes for speculative decoding. Leveraging nearly cost-free execution switching, QSPEC drafts tokens with low-precision, fast activation-weight quantization, and verifies them with high-precision weight-only quantization, effectively combining the strengths of both quantization schemes. Compared to high-precision quantization methods, QSPEC empirically boosts token generation throughput by up to 1.80x without any quality compromise, distinguishing it from other low-precision quantization approaches. This enhancement is also consistent across various serving tasks, model sizes, quantization methods, and batch sizes. Unlike existing speculative decoding techniques, our approach reuses weights and the KV cache, avoiding additional memory overhead. Furthermore, QSPEC offers a plug-and-play advantage without requiring any training. We believe that QSPEC demonstrates unique strengths for future deployment of high-fidelity quantization schemes, particularly in memory-constrained scenarios (e.g., edge devices).

**Link**: [arxiv](http://arxiv.org/abs/2410.11305v1),  [pdf](http://arxiv.org/pdf/2410.11305v1)

**Tags**: cs.LG cs.AI 



### Recommenadation aided Caching using Combinatorial Multi-armed Bandits
**Authors**: Pavamana K J, Chandramani Kishore Singh

**Updated**: 2024-10-15T05:34:07Z

**Summary**: We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache. We assume a fixed set of contents with unknown user preferences and content popularities. The base station can cache a subset of the contents and can also recommend subsets of the contents to different users in order to encourage them to request the recommended contents. Recommendations, depending on their acceptability, can thus be used to increase cache hits. We first assume that the users' recommendation acceptabilities are known and formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB). We propose a UCB-based algorithm to decide which contents to cache and recommend and provide an upper bound on the regret of this algorithm. Subsequently, we consider a more general scenario where the users' recommendation acceptabilities are also unknown and propose another UCB-based algorithm that learns these as well. We numerically demonstrate the performance of our algorithms and compare these to state-of-the-art algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2405.00080v3),  [pdf](http://arxiv.org/pdf/2405.00080v3)

**Tags**: cs.LG cs.IR cs.NI 



### A Zoned Storage Optimized Flash Cache on ZNS SSDs
**Authors**: Chongzhuo Yang, Chang Guo, Ming Zhao, Zhichao Cao

**Updated**: 2024-10-15T04:35:49Z

**Summary**: Zoned Namespace SSDs (ZNS) are introduced recently to mitigate the block interface penalties of flash-based SSDs. It is a good opportunity for flash cache to address cache throughput and write amplification (WA) issues by fully controlling data allocation and garbage collection via zone-based interfaces. However, there are several critical challenges that need to be addressed including zone-interface compatibility, data management of large zone size, and a better tradeoff between throughput, cache hit ratio, and WA.   In this paper, we present Z-CacheLib, a zoned storage optimized flash cache on ZNS SSDs. In Z-CacheLib, we propose: 1) a new zStorage Engine for ZNS SSDs with low mapping and operational overhead, and 2) a novel zCache Engine with cross-layer optimizations to resolve the throughput regression and WA issues of garbage collection, which consists of delayed data eviction with virtual over-provisioning (vOP), a top-down eviction policy (zLRU) optimized from LRU, and a bottom-up drop mechanism (zDrop) for low WA. Our evaluation shows that Z-CacheLib can achieve up to 2X throughput, 5% improvement hit ratio, and almost no WA compared to CacheLib with compatible regular SSDs, demonstrating benefits of using ZNS SSDs for cache. Moreover, Z-CacheLib can achieve up to 6X throughput and 92% WA reduction compared with F2FS-based scheme.

**Link**: [arxiv](http://arxiv.org/abs/2410.11260v1),  [pdf](http://arxiv.org/pdf/2410.11260v1)

**Tags**: cs.PF 



### Enhancing High-Level Synthesis with Automated Pragma Insertion and Code   Transformation Framework
**Authors**: Stphane Pouget, Louis-Nol Pouchet, Jason Cong

**Updated**: 2024-10-14T19:12:48Z

**Summary**: High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.

**Link**: [arxiv](http://arxiv.org/abs/2405.03058v4),  [pdf](http://arxiv.org/pdf/2405.03058v4)

**Tags**: cs.SE cs.PL 



### DuoAttention: Efficient Long-Context LLM Inference with Retrieval and   Streaming Heads
**Authors**: Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Junxian Guo, Shang Yang, Haotian Tang, Yao Fu, Song Han

**Updated**: 2024-10-14T17:59:58Z

**Summary**: Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.

**Link**: [arxiv](http://arxiv.org/abs/2410.10819v1),  [pdf](http://arxiv.org/pdf/2410.10819v1)

**Tags**: cs.CL 



### When Attention Sink Emerges in Language Models: An Empirical View
**Authors**: Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin

**Updated**: 2024-10-14T17:50:28Z

**Summary**: Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how optimization, data distribution, loss function, and model architecture in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, storing extra attention scores, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters. The code is available at https://github.com/sail-sg/Attention-Sink.

**Link**: [arxiv](http://arxiv.org/abs/2410.10781v1),  [pdf](http://arxiv.org/pdf/2410.10781v1)

**Tags**: cs.CL cs.AI cs.LG 



### Customize Your Visual Autoregressive Recipe with Set Autoregressive   Modeling
**Authors**: Wenze Liu, Le Zhuo, Yi Xin, Sheng Xia, Peng Gao, Xiangyu Yue

**Updated**: 2024-10-14T13:49:06Z

**Summary**: We introduce a new paradigm for AutoRegressive (AR) image generation, termed Set AutoRegressive Modeling (SAR). SAR generalizes the conventional AR to the next-set setting, i.e., splitting the sequence into arbitrary sets containing multiple tokens, rather than outputting each token in a fixed raster order. To accommodate SAR, we develop a straightforward architecture termed Fully Masked Transformer. We reveal that existing AR variants correspond to specific design choices of sequence order and output intervals within the SAR framework, with AR and Masked AR (MAR) as two extreme instances. Notably, SAR facilitates a seamless transition from AR to MAR, where intermediate states allow for training a causal model that benefits from both few-step inference and KV cache acceleration, thus leveraging the advantages of both AR and MAR. On the ImageNet benchmark, we carefully explore the properties of SAR by analyzing the impact of sequence order and output intervals on performance, as well as the generalization ability regarding inference order and steps. We further validate the potential of SAR by training a 900M text-to-image model capable of synthesizing photo-realistic images with any resolution. We hope our work may inspire more exploration and application of AR-based modeling across diverse modalities.

**Link**: [arxiv](http://arxiv.org/abs/2410.10511v1),  [pdf](http://arxiv.org/pdf/2410.10511v1)

**Tags**: cs.CV 



### Accelerating Diffusion Transformers with Token-wise Feature Caching
**Authors**: Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang

**Updated**: 2024-10-14T09:35:35Z

**Summary**: Diffusion transformers have shown significant effectiveness in both image and video synthesis at the expense of huge computation costs. To address this problem, feature caching methods have been introduced to accelerate diffusion transformers by caching the features in previous timesteps and reusing them in the following timesteps. However, previous caching methods ignore that different tokens exhibit different sensitivities to feature caching, and feature caching on some tokens may lead to 10$\times$ more destruction to the overall generation quality compared with other tokens. In this paper, we introduce token-wise feature caching, allowing us to adaptively select the most suitable tokens for caching, and further enable us to apply different caching ratios to neural layers in different types and depths. Extensive experiments on PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image and video generation with no requirements for training. For instance, 2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and PixArt-$\alpha$ with almost no drop in generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2410.05317v2),  [pdf](http://arxiv.org/pdf/2410.05317v2)

**Tags**: cs.LG cs.AI cs.CV 



### FedCache 2.0: Federated Edge Learning with Knowledge Caching and Dataset   Distillation
**Authors**: Quyang Pan, Sheng Sun, Zhiyuan Wu, Yuwei Wang, Min Liu, Bo Gao, Jingyuan Wang

**Updated**: 2024-10-14T07:58:39Z

**Summary**: Federated Edge Learning (FEL) has emerged as a promising approach for enabling edge devices to collaboratively train machine learning models while preserving data privacy. Despite its advantages, practical FEL deployment faces significant challenges related to device constraints and device-server interactions, necessitating heterogeneous, user-adaptive model training with limited and uncertain communication. In this paper, we introduce FedCache 2.0, a novel personalized FEL architecture that simultaneously addresses these challenges. FedCache 2.0 incorporates the benefits of both dataset distillation and knowledge cache-driven federated learning by storing and organizing distilled data as knowledge in the server-side knowledge cache. Moreover, a device-centric cache sampling strategy is introduced to tailor transferred knowledge for individual devices within controlled communication bandwidth. Extensive experiments on five datasets covering image recognition, audio understanding, and mobile sensor data mining tasks demonstrate that (1) FedCache 2.0 significantly outperforms state-of-the-art methods regardless of model structures, data distributions, and modalities. (2) FedCache 2.0 can train splendid personalized on-device models with at least $\times$28.6 improvement in communication efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2405.13378v2),  [pdf](http://arxiv.org/pdf/2405.13378v2)

**Tags**: cs.LG 



### Caching Content Placement and Beamforming Co-design for IRS-Aided MIMO   Systems with Imperfect CSI
**Authors**: Meng Gao, Yang Wang, Huafu Li, Junqi Guo

**Updated**: 2024-10-14T04:49:22Z

**Summary**: When offloading links encounter deep fading and obstruction, edge caching cannot fully enhance wireless network performance and improve the QoS of edge nodes, as it fails to effectively reduce backhaul burden. The emerging technology of intelligent reflecting surfaces (IRS) compensates for this disadvantage by creating a smart and reconfigurable wireless environment. Subsequently, we jointly design content placement and active/passive beamforming to minimize network costs under imperfect channel state information (CSI) in the IRS-oriented edge caching system. This minimization problem is decomposed into two subproblems. The content placement subproblem is addressed by applying KKT optimality conditions. We then develop the alternating optimization method to resolve precoder and reflection beamforming. Specifically, we reduce transmission power by first fixing the phase shift, reducing the problem to a convex one relative to the precoder, which is solved through convex optimization. Next, we fix the precoder and resolve the resulting reflection beamforming problem using the penalty convex-concave procedure (CCP) method. Results demonstrate that our proposed method outperforms uniform caching and random phase approaches in reducing transmission power and saving network costs. Eventually, the proposed approach offers potential improvements in the caching optimization and transmission robustness of wireless communication with imperfect CSI.

**Link**: [arxiv](http://arxiv.org/abs/2410.10157v1),  [pdf](http://arxiv.org/pdf/2410.10157v1)

**Tags**: eess.SP 



### Fast and Accurate Neural Rendering Using Semi-Gradients
**Authors**: In-Young Cho, Jaewoong Cho

**Updated**: 2024-10-14T04:30:38Z

**Summary**: We propose a simple yet effective neural network-based framework for global illumination rendering. Recently, rendering techniques that learn neural radiance caches by minimizing the difference (i.e., residual) between the left and right sides of the rendering equation have been suggested. Due to their ease of implementation and the advantage of excluding path integral calculations, these techniques have been applied to various fields, such as free-viewpoint rendering, differentiable rendering, and real-time rendering. However, issues of slow training and occasionally darkened renders have been noted. We identify the cause of these issues as the bias and high variance present in the gradient estimates of the existing residual-based objective function. To address this, we introduce a new objective function that maintains the same global optimum as before but allows for unbiased and low-variance gradient estimates, enabling faster and more accurate training of neural networks. In conclusion, this method is simply implemented by ignoring the partial derivatives of the right-hand side, and theoretical and experimental analyses demonstrate the effectiveness of the proposed loss.

**Link**: [arxiv](http://arxiv.org/abs/2410.10149v1),  [pdf](http://arxiv.org/pdf/2410.10149v1)

**Tags**: cs.CV cs.GR cs.LG 



### Content Caching-Assisted Vehicular Edge Computing Using Multi-Agent   Graph Attention Reinforcement Learning
**Authors**: Jinjin Shen, Yan Lin, Yijin Zhang, Weibin Zhang, Feng Shu, Jun Li

**Updated**: 2024-10-14T01:25:56Z

**Summary**: In order to avoid repeated task offloading and realize the reuse of popular task computing results, we construct a novel content caching-assisted vehicular edge computing (VEC) framework. In the face of irregular network topology and unknown environmental dynamics, we further propose a multi-agent graph attention reinforcement learning (MGARL) based edge caching scheme, which utilizes the graph attention convolution kernel to integrate the neighboring nodes' features of each agent and further enhance the cooperation among agents. Our simulation results show that our proposed scheme is capable of improving the utilization of caching resources while reducing the long-term task computing latency compared to the baselines.

**Link**: [arxiv](http://arxiv.org/abs/2410.10071v1),  [pdf](http://arxiv.org/pdf/2410.10071v1)

**Tags**: cs.MA cs.ET 



### Leveraging Semantic Cues from Foundation Vision Models for Enhanced   Local Feature Correspondence
**Authors**: Felipe Cadar, Guilherme Potje, Renato Martins, Cdric Demonceaux, Erickson R. Nascimento

**Updated**: 2024-10-12T13:45:26Z

**Summary**: Visual correspondence is a crucial step in key computer vision tasks, including camera localization, image registration, and structure from motion. The most effective techniques for matching keypoints currently involve using learned sparse or dense matchers, which need pairs of images. These neural networks have a good general understanding of features from both images, but they often struggle to match points from different semantic areas. This paper presents a new method that uses semantic cues from foundation vision model features (like DINOv2) to enhance local feature matching by incorporating semantic reasoning into existing descriptors. Therefore, the learned descriptors do not require image pairs at inference time, allowing feature caching and fast matching using similarity search, unlike learned matchers. We present adapted versions of six existing descriptors, with an average increase in performance of 29% in camera localization, with comparable accuracy to existing matchers as LightGlue and LoFTR in two existing benchmarks. Both code and trained models are available at https://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24

**Link**: [arxiv](http://arxiv.org/abs/2410.09533v1),  [pdf](http://arxiv.org/pdf/2410.09533v1)

**Tags**: cs.CV 



### Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle
**Authors**: KVS Chaithanya, Sumesh P. Thampi

**Updated**: 2024-10-12T10:38:39Z

**Summary**: Understanding the hydrodynamics of microswimmers in viscoelastic fluids and confined environments is crucial for interpreting their behaviour in natural settings and designing synthetic microswimmers for practical applications like cargo transport. In this study, we explore the hydrodynamics of a concentric active compound particle - a model microswimmer (a squirmer) positioned at the centre of a viscoelastic fluid droplet (a model cargo) suspended in another viscoelastic medium. We consider the Oldroyd-B constitutive model to characterize the fluids and employ a perturbative approach in the Deborah number to analyze viscoelastic effects analytically, assuming a small Capillary number so that the droplet remains spherical and does not deform. We examine three cases: (i) a squirmer confined within a viscoelastic fluid droplet suspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian fluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined within a viscoelastic fluid droplet suspended in another viscoelastic fluid. Our findings reveal that the swimming speeds of the squirmer and the droplet are determined by the complex interplay of viscoelasticity, the size ratio of the droplet to the squirmer (confinement strength), and the viscosity ratio of the surrounding fluid to the droplet fluid. A critical aspect of this interaction is the positioning of stagnation points within the fluid flow, which governs the distribution of polymeric stress. This distribution, in turn, plays a crucial role in determining the influence of viscoelasticity on the squirmer's dynamics. Our analysis suggests that viscoelastic effects can either enhance or hinder the swimming speed of the squirmer when confined in a droplet, depending on the specific configuration of the system.

**Link**: [arxiv](http://arxiv.org/abs/2410.09479v1),  [pdf](http://arxiv.org/pdf/2410.09479v1)

**Tags**: physics.flu-dyn cond-mat.soft 



### Fine-grained Attention I/O Complexity: Comprehensive Analysis for   Backward Passes
**Authors**: Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yufa Zhou

**Updated**: 2024-10-12T07:01:30Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in processing long-context information. However, the quadratic complexity of attention computation with respect to sequence length poses significant computational challenges, and I/O aware algorithms have been proposed. This paper presents a comprehensive analysis of the I/O complexity for attention mechanisms, focusing on backward passes by categorizing into small and large cache scenarios. Using the red-blue pebble game framework, we establish tight bounds on I/O complexity across all cache sizes. We confirm that the de facto standard I/O aware algorithm FlashAttention is optimal for both forward and backward passes for the large cache size scenario. For small cache sizes, we provide an algorithm that improves over existing methods and achieves the tight bounds. Additionally, we extend our analysis to sparse attention, a mainstream speeding-up approach, deriving fine-grained lower bounds for both forward and backward passes and both small and large caches. Our findings complete the theoretical foundation for I/O complexity in attention mechanisms, offering insights for designing efficient algorithms of LLM training and inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.09397v1),  [pdf](http://arxiv.org/pdf/2410.09397v1)

**Tags**: cs.LG cs.AI cs.CC cs.CL 



### In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs
**Authors**: Xufeng Yang, Zhengjian Cong, Congming Gao

**Updated**: 2024-10-12T02:11:14Z

**Summary**: Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage systems. To increase capacity, high bit-density cells, such as Triple-Level Cell (TLC), are utilized within 3D SSDs. However, due to the inferior performance of TLC, a portion of TLCs is configured to operate as Single-Level Cell (SLC) to provide high performance, with host data initially directed to the SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated as an SLC cache to achieve high SSD performance by writing host data at the SLC speed. Given the limited size of the SLC cache, block reclamation is necessary to free up the SLC cache during idle periods. However, our preliminary studies indicate that the SLC cache can lead to a performance cliff if filled rapidly and cause significant write amplification when data migration occurs during idle times.   In this work, we propose leveraging a reprogram operation to address these challenges. Specifically, when the SLC cache is full or during idle periods, a reprogram operation is performed to switch used SLC pages to TLC pages in place (termed In-place Switch, IPS). Subsequently, other free TLC space is allocated as the new SLC cache. IPS can continuously provide sufficient SLC cache within SSDs, significantly improving write performance and reducing write amplification. Experimental results demonstrate that IPS can reduce write latency and write amplification by up to 0.75 times and 0.53 times, respectively, compared to state-of-the-art SLC cache technologies.

**Link**: [arxiv](http://arxiv.org/abs/2409.14360v3),  [pdf](http://arxiv.org/pdf/2409.14360v3)

**Tags**: cs.AR 



### Foundation Model-Powered 3D Few-Shot Class Incremental Learning via   Training-free Adaptor
**Authors**: Sahar Ahmadi, Ali Cheraghian, Morteza Saberi, Md. Towsif Abir, Hamidreza Dastmalchi, Farookh Hussain, Shafin Rahman

**Updated**: 2024-10-11T20:23:00Z

**Summary**: Recent advances in deep learning for processing point clouds hold increased interest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision. This paper introduces a new method to tackle the Few-Shot Continual Incremental Learning (FSCIL) problem in 3D point cloud environments. We leverage a foundational 3D model trained extensively on point cloud data. Drawing from recent improvements in foundation models, known for their ability to work well across different tasks, we propose a novel strategy that does not require additional training to adapt to new tasks. Our approach uses a dual cache system: first, it uses previous test samples based on how confident the model was in its predictions to prevent forgetting, and second, it includes a small number of new task samples to prevent overfitting. This dynamic adaptation ensures strong performance across different learning tasks without needing lots of fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet, ScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and demonstrating its effectiveness and versatility. The code is available at \url{https://github.com/ahmadisahar/ACCV_FCIL3D}.

**Link**: [arxiv](http://arxiv.org/abs/2410.09237v1),  [pdf](http://arxiv.org/pdf/2410.09237v1)

**Tags**: cs.CV 



### Calibrated Cache Model for Few-Shot Vision-Language Model Adaptation
**Authors**: Kun Ding, Qiang Yu, Haojian Zhang, Gaofeng Meng, Shiming Xiang

**Updated**: 2024-10-11T15:12:30Z

**Summary**: Cache-based approaches stand out as both effective and efficient for adapting vision-language models (VLMs). Nonetheless, the existing cache model overlooks three crucial aspects. 1) Pre-trained VLMs are mainly optimized for image-text similarity, neglecting the importance of image-image similarity, leading to a gap between pre-training and adaptation. 2) The current cache model is based on the Nadaraya-Watson (N-W) estimator, which disregards the intricate relationships among training samples while constructing weight function. 3) Under the condition of limited samples, the logits generated by cache model are of high uncertainty, directly using these logits without accounting for the confidence could be problematic. This work presents three calibration modules aimed at addressing the above challenges. Similarity Calibration refines the image-image similarity by using unlabeled images. We add a learnable projection layer with residual connection on top of the pre-trained image encoder of CLIP and optimize the parameters by minimizing self-supervised contrastive loss. Weight Calibration introduces a precision matrix into the weight function to adequately model the relation between training samples, transforming the existing cache model to a Gaussian Process (GP) regressor, which could be more accurate than N-W estimator. Confidence Calibration leverages the predictive variances computed by GP Regression to dynamically re-scale the logits of cache model, ensuring that the cache model's outputs are appropriately adjusted based on their confidence levels. Besides, to reduce the high complexity of GPs, we further propose a group-based learning strategy. Integrating the above designs, we propose both training-free and training-required variants. Extensive experiments on 11 few-shot classification datasets validate that the proposed methods can achieve state-of-the-art performance.

**Link**: [arxiv](http://arxiv.org/abs/2410.08895v1),  [pdf](http://arxiv.org/pdf/2410.08895v1)

**Tags**: cs.CV 



### Unlocking FedNL: Self-Contained Compute-Optimized Implementation
**Authors**: Konstantin Burlachenko, Peter Richtrik

**Updated**: 2024-10-11T12:19:18Z

**Summary**: Federated Learning (FL) is an emerging paradigm that enables intelligent agents to collaboratively train Machine Learning (ML) models in a distributed manner, eliminating the need for sharing their local data. The recent work (arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL) algorithms, marking a significant step towards applying second-order methods to FL and large-scale optimization. However, the reference FedNL prototype exhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch a single experiment in a sever-grade workstation; (ii) The prototype only simulates multi-node setting; (iii) Prototype integration into resource-constrained applications is challenging. To bridge the gap between theory and practice, we present a self-contained implementation of FedNL, FedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves the aforementioned issues and reduces the wall clock time by x1000. With this FedNL outperforms alternatives for training logistic regression in a single-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark (arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose two practical-orientated compressors for FedNL - adaptive TopLEK and cache-aware RandSeqK, which fulfill the theory of FedNL.

**Link**: [arxiv](http://arxiv.org/abs/2410.08760v1),  [pdf](http://arxiv.org/pdf/2410.08760v1)

**Tags**: cs.LG cs.AI cs.MS cs.PF math.OC G.4; C.3; I.2.11 



### AsyncFS: Metadata Updates Made Asynchronous for Distributed Filesystems   with In-Network Coordination
**Authors**: Jingwei Xu, Mingkai Dong, Qiulin Tian, Ziyi Tian, Tong Xin, Haibo Chen

**Updated**: 2024-10-11T08:33:58Z

**Summary**: Distributed filesystems typically employ synchronous metadata updates, facing inherent challenges for access efficiency, load balancing, and directory contention, especially under dynamic and skewed workloads. This paper argues that synchronous updates are overly conservative for distributed filesystems. We propose AsyncFS with asynchronous metadata updates, allowing operations to return early and defer directory updates until respective read to enable latency hiding and conflict resolution. The key challenge is efficiently maintaining the synchronous semantics of metadata updates. To address this, AsyncFS is co-designed with a programmable switch, leveraging the constrained on-switch resources to holistically track directory states in the network with negligible cost. This allows AsyncFS to timely aggregate and efficiently apply delayed updates using batching and consolidation before directory reads. Evaluation shows that AsyncFS achieves up to 13.34$\times$ and 3.85$\times$ higher throughput, and 61.6% and 57.3% lower latency than two state-of-the-art distributed filesystems, InfiniFS and CFS-KV, respectively, on skewed workloads. For real-world workloads, AsyncFS improves end-to-end throughput by 21.1$\times$, 1.1$\times$ and 30.1% over Ceph, IndexFS and CFS-KV, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2410.08618v1),  [pdf](http://arxiv.org/pdf/2410.08618v1)

**Tags**: cs.DC cs.OS cs.PF 



### ZipVL: Efficient Large Vision-Language Models with Dynamic Token   Sparsification and KV Cache Compression
**Authors**: Yefei He, Feng Chen, Jing Liu, Wenqi Shao, Hong Zhou, Kaipeng Zhang, Bohan Zhuang

**Updated**: 2024-10-11T07:24:21Z

**Summary**: The efficiency of large vision-language models (LVLMs) is constrained by the computational bottleneck of the attention mechanism during the prefill phase and the memory bottleneck of fetching the key-value (KV) cache in the decoding phase, particularly in scenarios involving high-resolution images or videos. Visual content often exhibits substantial redundancy, resulting in highly sparse attention maps within LVLMs. This sparsity can be leveraged to accelerate attention computation or compress the KV cache through various approaches. However, most studies focus on addressing only one of these bottlenecks and do not adequately support dynamic adjustment of sparsity concerning distinct layers or tasks. In this paper, we present ZipVL, an efficient inference framework designed for LVLMs that resolves both computation and memory bottlenecks through a dynamic ratio allocation strategy of important tokens. This ratio is adaptively determined based on the layer-specific distribution of attention scores, rather than fixed hyper-parameters, thereby improving efficiency for less complex tasks while maintaining high performance for more challenging ones. Then we select important tokens based on their normalized attention scores and perform attention mechanism solely on those important tokens to accelerate the prefill phase. To mitigate the memory bottleneck in the decoding phase, we employ mixed-precision quantization to the KV cache, where high-bit quantization is used for caches of important tokens, while low-bit quantization is applied to those of less importance. Our experiments demonstrate that ZipVL can accelerate the prefill phase by 2.6$\times$ and reduce GPU memory usage by 50.0%, with a minimal accuracy reduction of only 0.2% on Video-MME benchmark over LongVA-7B model, effectively enhancing the generation efficiency of LVLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.08584v1),  [pdf](http://arxiv.org/pdf/2410.08584v1)

**Tags**: cs.CV cs.AI 



### Long Context Compression with Activation Beacon
**Authors**: Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou

**Updated**: 2024-10-11T02:18:24Z

**Summary**: Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. 1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts). 2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. 3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance. 4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, achieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache. Our data, model, and code have been released at \url{https://github.com/FlagOpen/FlagEmbedding/}.

**Link**: [arxiv](http://arxiv.org/abs/2401.03462v3),  [pdf](http://arxiv.org/pdf/2401.03462v3)

**Tags**: cs.CL cs.AI 



### KV Prediction for Improved Time to First Token
**Authors**: Maxwell Horton, Qingqing Cao, Chenfan Sun, Yanzi Jin, Sachin Mehta, Mohammad Rastegari, Moin Nabi

**Updated**: 2024-10-10T21:55:11Z

**Summary**: Inference with transformer-based language models begins with a prompt processing step. In this step, the model generates the first output token and stores the KV cache needed for future generation steps. This prompt processing step can be computationally expensive, taking 10s of seconds or more for billion-parameter models on edge devices when prompt lengths or batch sizes rise. This degrades user experience by introducing significant latency into the model's outputs. To reduce the time spent producing the first output (known as the ``time to first token'', or TTFT) of a pretrained model, we introduce a novel method called KV Prediction. In our method, a small auxiliary model is used to process the prompt and produce an approximation of the KV cache used by a base model. This approximated KV cache is then used with the base model for autoregressive generation without the need to query the auxiliary model again. We demonstrate that our method produces a pareto-optimal efficiency-accuracy trade-off when compared to baselines. On TriviaQA, we demonstrate relative accuracy improvements in the range of $15\%-50\%$ across a range of TTFT FLOPs budgets. We also demonstrate accuracy improvements of up to $30\%$ on HumanEval python code completion at fixed TTFT FLOPs budgets. Additionally, we benchmark models on an Apple M2 Pro CPU and demonstrate that our improvement in FLOPs translates to a TTFT speedup on hardware. We release our code at https://github.com/apple/corenet/tree/main/projects/kv-prediction .

**Link**: [arxiv](http://arxiv.org/abs/2410.08391v1),  [pdf](http://arxiv.org/pdf/2410.08391v1)

**Tags**: cs.CL cs.AI 



### Amplifying Main Memory-Based Timing Covert and Side Channels using   Processing-in-Memory Operations
**Authors**: Konstantinos Kanellopoulos, F. Nisa Bostanci, Ataberk Olgun, A. Giray Yaglikci, Ismail Emir Yuksel, Nika Mansouri Ghiasi, Zulal Bingol, Mohammad Sadrosadati, Onur Mutlu

**Updated**: 2024-10-10T16:57:34Z

**Summary**: The adoption of processing-in-memory (PiM) architectures has been gaining momentum because they provide high performance and low energy consumption by alleviating the data movement bottleneck. Yet, the security of such architectures has not been thoroughly explored. The adoption of PiM solutions provides a new way to directly access main memory, which malicious user applications can exploit. We show that this new way to access main memory opens opportunities for high-throughput timing attacks that are hard-to-mitigate without significant performance overhead.   We introduce IMPACT, a set of high-throughput main memory-based timing attacks that leverage characteristics of PiM architectures to establish covert and side channels. IMPACT enables high-throughput communication and private information leakage by exploiting the shared DRAM row buffer. To achieve high throughput, IMPACT (i) eliminates cache bypassing steps required by processor-centric main memory and cache-based timing attacks and (ii) leverages the intrinsic parallelism of PiM operations. We showcase two applications of IMPACT. First, we build two covert-channel attacks that run on the host CPU and leverage different PiM approaches to gain direct and fast access to main memory and establish high-throughput communication covert channels. Second, we showcase a side-channel attack that leaks private information of concurrently running victim applications that are accelerated with PiM. Our results demonstrate that (i) our covert channels achieve 12.87 Mb/s and 14.16 Mb/s communication throughput, respectively, which is up to 4.91x and 5.41x faster than the state-of-the-art main memory-based covert channels, and (ii) our side-channel attack allows the attacker to leak secrets with a low error rate. To avoid such covert and side channels in emerging PiM systems, we propose and evaluate three defenses.

**Link**: [arxiv](http://arxiv.org/abs/2404.11284v3),  [pdf](http://arxiv.org/pdf/2404.11284v3)

**Tags**: cs.CR cs.AR 



### RecurFormer: Not All Transformer Heads Need Self-Attention
**Authors**: Ruiqing Yan, Linghan Zheng, Xingbo Du, Han Zou, Yufeng Guo, Jianfei Yang

**Updated**: 2024-10-10T15:24:12Z

**Summary**: Transformer-based large language models (LLMs) excel in modeling complex language patterns but face significant computational costs during inference, especially with long inputs due to the attention mechanism's memory overhead. We observe that certain attention heads exhibit a distribution where the attention weights concentrate on tokens near the query token, termed as recency aware, which focuses on local and short-range dependencies. Leveraging this insight, we propose RecurFormer, a novel architecture that replaces these attention heads with linear recurrent neural networks (RNNs), specifically the Mamba architecture. This replacement reduces the cache size without evicting tokens, thus maintaining generation quality. RecurFormer retains the ability to model long-range dependencies through the remaining attention heads and allows for reusing pre-trained Transformer-based LLMs weights with continual training. Experiments demonstrate that RecurFormer matches the original model's performance while significantly enhancing inference efficiency. Our approach provides a practical solution to the computational challenges of Transformer-based LLMs inference, making it highly attractive for tasks involving long inputs.

**Link**: [arxiv](http://arxiv.org/abs/2410.12850v1),  [pdf](http://arxiv.org/pdf/2410.12850v1)

**Tags**: cs.CL cs.AI cs.LG 



### C$^3$P-VoxelMap: Compact, Cumulative and Coalescible Probabilistic Voxel   Mapping
**Authors**: Xu Yang, Wenhao Li, Qijie Ge, Lulu Suo, Weijie Tang, Zhengyu Wei, Longxiang Huang, Bo Wang

**Updated**: 2024-10-10T11:01:44Z

**Summary**: This work presents a compact, cumulative and coalescible probabilistic voxel mapping method to enhance performance, accuracy and memory efficiency in LiDAR odometry. Probabilistic voxel mapping requires storing past point clouds and re-iterating on them to update the uncertainty every iteration, which consumes large memory space and CPU cycles. To solve this problem, we propose a two-folded strategy. First, we introduce a compact point-free representation for probabilistic voxels and derive a cumulative update of the planar uncertainty without caching original point clouds. Our voxel structure only keeps track of a predetermined set of statistics for points that lie inside it. This method reduces the runtime complexity from $O(MN)$ to $O(N)$ and the space complexity from $O(N)$ to $O(1)$ where $M$ is the number of iterations and $N$ is the number of points. Second, to further minimize memory usage and enhance mapping accuracy, we provide a strategy to dynamically merge voxels associated with the same physical planes by taking advantage of the geometric features in the real world. Rather than scanning for these coalescible voxels constantly at every iteration, our merging strategy accumulates voxels in a locality-sensitive hash and triggers merging lazily. On-demand merging not only reduces memory footprint with minimal computational overhead but also improves localization accuracy thanks to cross-voxel denoising. Experiments exhibit 20% higher accuracy, 20% faster performance and 70% lower memory consumption than the state-of-the-art.

**Link**: [arxiv](http://arxiv.org/abs/2406.01195v2),  [pdf](http://arxiv.org/pdf/2406.01195v2)

**Tags**: cs.RO 



### SqueezeAttention: 2D Management of KV-Cache in LLM Inference via   Layer-wise Optimal Budget
**Authors**: Zihao Wang, Bin Cui, Shaoduo Gan

**Updated**: 2024-10-10T05:11:52Z

**Summary**: Optimizing the Key-Value (KV) cache of the Large Language Model (LLM) has been considered critical to saving the cost of inference. Most of the existing KV-cache compression algorithms attempted to sparsify the sequence of tokens by taking advantage of the different importance of tokens. However, most of these methods treat all layers equally, allocating the same KV budget to each layer. This approach is suboptimal, as some layers may be less sensitive to input tokens yet still receive the same budget as others. In this work, we found that by identifying the importance of attention layers, we could optimize the KV-cache jointly from two dimensions, i.e., sequence-wise and layer-wise. Based on our observations regarding layer-wise importance in inference, we propose SqueezeAttention to precisely optimize the allocation of KV-cache budget among layers on-the-fly and then incorporate three representative sequence-wise algorithms to compress the KV-cache for each layer with its very own budget. Specifically, we first measure each layer's importance by calculating the cosine similarity of the input prompt differences before and after the self-attention layers. Based on this similarity, we then categorize the layers into two groups and adjust their KV budgets accordingly. By optimizing the KV-cache from both sequence's and layer's dimensions, SqueezeAttention achieves around 30% to 70% of the memory reductions and up to 2.2 times of throughput improvements in a wide range of LLMs and benchmarks. The code is available at https://github.com/hetailang/SqueezeAttention.

**Link**: [arxiv](http://arxiv.org/abs/2404.04793v2),  [pdf](http://arxiv.org/pdf/2404.04793v2)

**Tags**: cs.LG cs.CL 



### TurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed   KV Caches for Chunked Text
**Authors**: Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, Yaohua Tang

**Updated**: 2024-10-10T03:52:54Z

**Summary**: Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2410.07590v1),  [pdf](http://arxiv.org/pdf/2410.07590v1)

**Tags**: cs.CV cs.CL 



### Teddy: Efficient Large-Scale Dataset Distillation via   Taylor-Approximated Matching
**Authors**: Ruonan Yu, Songhua Liu, Jingwen Ye, Xinchao Wang

**Updated**: 2024-10-10T03:28:46Z

**Summary**: Dataset distillation or condensation refers to compressing a large-scale dataset into a much smaller one, enabling models trained on this synthetic dataset to generalize effectively on real data. Tackling this challenge, as defined, relies on a bi-level optimization algorithm: a novel model is trained in each iteration within a nested loop, with gradients propagated through an unrolled computation graph. However, this approach incurs high memory and time complexity, posing difficulties in scaling up to large datasets such as ImageNet. Addressing these concerns, this paper introduces Teddy, a Taylor-approximated dataset distillation framework designed to handle large-scale dataset and enhance efficiency. On the one hand, backed up by theoretical analysis, we propose a memory-efficient approximation derived from Taylor expansion, which transforms the original form dependent on multi-step gradients to a first-order one. On the other hand, rather than repeatedly training a novel model in each iteration, we unveil that employing a pre-cached pool of weak models, which can be generated from a single base model, enhances both time efficiency and performance concurrently, particularly when dealing with large-scale datasets. Extensive experiments demonstrate that the proposed Teddy attains state-of-the-art efficiency and performance on the Tiny-ImageNet and original-sized ImageNet-1K dataset, notably surpassing prior methods by up to 12.8%, while reducing 46.6% runtime. Our code will be available at https://github.com/Lexie-YU/Teddy.

**Link**: [arxiv](http://arxiv.org/abs/2410.07579v1),  [pdf](http://arxiv.org/pdf/2410.07579v1)

**Tags**: cs.CV 



### Laser Interactions with Gas Jets: EMP Emission and Nozzle Damage
**Authors**: Philip Wykeham Bradford, Valeria Ospina-Bohorquez, Michael Ehret, Jose-Luis Henares, Pilar Puyuelo-Valdes, Tomasz Chodukowski, Tadeusz Pisarczyk, Zofia Rusiniak, Carlos Salgado-Lopez, Christos Vlachos, Massimiliano Sciscio, Martina Salvadori, Claudio Verona, George Hicks, Oliver Ettlinger, Zulfikar Najmudin, Jean-Raphael Marques, Laurent Gremillet, Joao Jorge Santos, Fabrizio Consoli, Vladimir Tikhonchuk

**Updated**: 2024-10-09T15:57:03Z

**Summary**: Understanding the physics of electromagnetic pulse emission and nozzle damage is critical for the long-term operation of laser experiments with gas targets, particularly at facilities looking to produce stable sources of radiation at high repetition rate. We present a theoretical model of plasma formation and electrostatic charging when high-power lasers are focused inside gases. The model can be used to estimate the amplitude of gigahertz electromagnetic pulses (EMPs) produced by the laser and the extent of damage to the gas jet nozzle. Looking at a range of laser and target properties relevant to existing high-power laser systems, we find that EMP fields of tens to hundreds of kV/m can be generated several metres from the gas jet. Model predictions are compared with measurements of EMP, plasma formation and nozzle damage from two experiments on the VEGA-3 laser and one experiment on the Vulcan Petawatt laser.

**Link**: [arxiv](http://arxiv.org/abs/2403.19519v3),  [pdf](http://arxiv.org/pdf/2403.19519v3)

**Tags**: physics.plasm-ph 



### VEC-Sim: A Simulation Platform for Evaluating Service Caching and   Computation Offloading Policies in Vehicular Edge Networks
**Authors**: Fan Wu, Xiaolong Xu, Muhammad Bilal, Xiangwei Wang, Hao Cheng, Siyu Wu

**Updated**: 2024-10-09T14:28:59Z

**Summary**: Computer simulation platforms offer an alternative solution by emulating complex systems in a controlled manner. However, existing Edge Computing (EC) simulators, as well as general-purpose vehicular network simulators, are not tailored for VEC and lack dedicated support for modeling the distinct access pattern, entity mobility trajectory and other unique characteristics of VEC networks. To fill this gap, this paper proposes VEC-Sim, a versatile simulation platform for in-depth evaluation and analysis of various service caching and computation offloading policies in VEC networks. VEC-Sim incorporates realistic mechanisms to replicate real-world access patterns, including service feature vector, vehicle mobility modeling, evolving service popularity, new service upload and user preference shifts, etc. Moreover, its modular architecture and extensive Application Programming Interfaces (APIs) allow seamless integration of customized scheduling policies and user-defined metrics. A comprehensive evaluation of VEC-Sim's capabilities is undertaken in comparison to real-world ground truths. Results prove it to be accurate in reproducing classical scheduling algorithms and extremely effective in conducting case studies.

**Link**: [arxiv](http://arxiv.org/abs/2410.06934v1),  [pdf](http://arxiv.org/pdf/2410.06934v1)

**Tags**: cs.NI 



### LayerKV: Optimizing Large Language Model Serving with Layer-wise KV   Cache Management
**Authors**: Yi Xiong, Hao Wu, Changxu Shao, Ziqing Wang, Rui Zhang, Yuhong Guo, Junping Zhao, Ke Zhang, Zhenxuan Pan

**Updated**: 2024-10-09T11:40:31Z

**Summary**: The expanding context windows in large language models (LLMs) have greatly enhanced their capabilities in various applications, but they also introduce significant challenges in maintaining low latency, particularly in Time to First Token (TTFT). This paper identifies that the sharp rise in TTFT as context length increases is predominantly driven by queuing delays, which are caused by the growing demands for GPU Key-Value (KV) cache allocation clashing with the limited availability of KV cache blocks. To address this issue, we propose LayerKV, a simple yet effective plug-in method that effectively reduces TTFT without requiring additional hardware or compromising output performance, while seamlessly integrating with existing parallelism strategies and scheduling techniques. Specifically, LayerKV introduces layer-wise KV block allocation, management, and offloading for fine-grained control over system memory, coupled with an SLO-aware scheduler to optimize overall Service Level Objectives (SLOs). Comprehensive evaluations on representative models, ranging from 7B to 70B parameters, across various GPU configurations, demonstrate that LayerKV improves TTFT latency up to 69x and reduces SLO violation rates by 28.7%, significantly enhancing the user experience.

**Link**: [arxiv](http://arxiv.org/abs/2410.00428v3),  [pdf](http://arxiv.org/pdf/2410.00428v3)

**Tags**: cs.DC cs.AI cs.LG I.2.11; C.4 



### Variations in Multi-Agent Actor-Critic Frameworks for Joint   Optimizations in UAV Swarm Networks: Recent Evolution, Challenges, and   Directions
**Authors**: Muhammad Morshed Alam, Muhammad Yeasir Aarafat, Tamim Hossain

**Updated**: 2024-10-09T07:22:40Z

**Summary**: Autonomous unmanned aerial vehicle (UAV) swarm networks (UAVSNs) can effectively execute surveillance, connectivity, and computing services to ground users (GUs). These missions require trajectory planning, UAV-GUs association, task offloading, next-hop selection, and resources such as transmit power, bandwidth, caching, and computing allocation to improve network performances. Owing to the highly dynamic topology, limited resources, and non-availability of global knowledge, optimizing network performance in UAVSNs is very intricate. Hence, it requires an adaptive joint optimization framework that can tackle both discrete and continuous decision variables to ensure optimal network performance under dynamic constraints. Multi-agent deep reinforcement learning-based adaptive actor-critic framework can efficiently address these problems. This paper investigates the recent evolutions of actor-critic frameworks to deal with joint optimization problems in UAVSNs. In addition, challenges and potential solutions are addressed as research directions.

**Link**: [arxiv](http://arxiv.org/abs/2410.06627v1),  [pdf](http://arxiv.org/pdf/2410.06627v1)

**Tags**: eess.SY cs.SY 



### UpDLRM: Accelerating Personalized Recommendation using Real-World PIM   Architecture
**Authors**: Sitian Chen, Haobin Tan, Amelie Chi Zhou, Yusen Li, Pavan Balaji

**Updated**: 2024-10-09T04:11:28Z

**Summary**: Deep Learning Recommendation Models (DLRMs) have gained popularity in recommendation systems due to their effectiveness in handling large-scale recommendation tasks. The embedding layers of DLRMs have become the performance bottleneck due to their intensive needs on memory capacity and memory bandwidth. In this paper, we propose UpDLRM, which utilizes real-world processingin-memory (PIM) hardware, UPMEM DPU, to boost the memory bandwidth and reduce recommendation latency. The parallel nature of the DPU memory can provide high aggregated bandwidth for the large number of irregular memory accesses in embedding lookups, thus offering great potential to reduce the inference latency. To fully utilize the DPU memory bandwidth, we further studied the embedding table partitioning problem to achieve good workload-balance and efficient data caching. Evaluations using real-world datasets show that, UpDLRM achieves much lower inference time for DLRM compared to both CPU-only and CPU-GPU hybrid counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2406.13941v2),  [pdf](http://arxiv.org/pdf/2406.13941v2)

**Tags**: cs.IR cs.AI 



### ERCache: An Efficient and Reliable Caching Framework for Large-Scale   User Representations in Meta's Ads System
**Authors**: Fang Zhou, Yaning Huang, Dong Liang, Dai Li, Zhongke Zhang, Kai Wang, Xiao Xin, Abdallah Aboelela, Zheliang Jiang, Yang Wang, Jeff Song, Wei Zhang, Chen Liang, Huayu Li, ChongLin Sun, Hang Yang, Lei Qu, Zhan Shu, Mindi Yuan, Emanuele Maccherani, Taha Hayat, John Guo, Varna Puvvada, Uladzimir Pashkevich

**Updated**: 2024-10-09T02:51:27Z

**Summary**: The increasing complexity of deep learning models used for calculating user representations presents significant challenges, particularly with limited computational resources and strict service-level agreements (SLAs). Previous research efforts have focused on optimizing model inference but have overlooked a critical question: is it necessary to perform user model inference for every ad request in large-scale social networks? To address this question and these challenges, we first analyze user access patterns at Meta and find that most user model inferences occur within a short timeframe. T his observation reveals a triangular relationship among model complexity, embedding freshness, and service SLAs. Building on this insight, we designed, implemented, and evaluated ERCache, an efficient and robust caching framework for large-scale user representations in ads recommendation systems on social networks. ERCache categorizes cache into direct and failover types and applies customized settings and eviction policies for each model, effectively balancing model complexity, embedding freshness, and service SLAs, even considering the staleness introduced by caching. ERCache has been deployed at Meta for over six months, supporting more than 30 ranking models while efficiently conserving computational resources and complying with service SLA requirements.

**Link**: [arxiv](http://arxiv.org/abs/2410.06497v1),  [pdf](http://arxiv.org/pdf/2410.06497v1)

**Tags**: cs.IR cs.AI cs.DC cs.LG 



### Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in   Fine-tuning LLMs for Simultaneous Translation
**Authors**: Matthew Raffel, Victor Agostinelli, Lizhong Chen

**Updated**: 2024-10-09T01:12:19Z

**Summary**: Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.

**Link**: [arxiv](http://arxiv.org/abs/2405.10443v4),  [pdf](http://arxiv.org/pdf/2405.10443v4)

**Tags**: cs.CL cs.LG 



### KV Cache Compression, But What Must We Give in Return? A Comprehensive   Benchmark of Long Context Capable Approaches
**Authors**: Jiayi Yuan, Hongyi Liu, Shaochen Zhong, Yu-Neng Chuang, Songchen Li, Guanchu Wang, Duy Le, Hongye Jin, Vipin Chaudhary, Zhaozhuo Xu, Zirui Liu, Xia Hu

**Updated**: 2024-10-08T19:34:03Z

**Summary**: Long context capability is a crucial competency for large language models (LLMs) as it mitigates the human struggle to digest long-form texts. This capability enables complex task-solving scenarios such as book summarization, code assistance, and many more tasks that are traditionally manpower-intensive. However, transformer-based LLMs face significant challenges with long context input due to the growing size of the KV cache and the intrinsic complexity of attending to extended inputs; where multiple schools of efficiency-driven approaches - such as KV cache quantization, token dropping, prompt compression, linear-time sequence models, and hybrid architectures - have been proposed to produce efficient yet long context-capable models. Despite these advancements, no existing work has comprehensively benchmarked these methods in a reasonably aligned environment. In this work, we fill this gap by providing a taxonomy of current methods and evaluating 10+ state-of-the-art approaches across seven categories of long context tasks. Our work reveals numerous previously unknown phenomena and offers insights - as well as a friendly workbench - for the future development of long context-capable LLMs. The source code is available at https://github.com/henryzhongsc/longctx_bench.

**Link**: [arxiv](http://arxiv.org/abs/2407.01527v2),  [pdf](http://arxiv.org/pdf/2407.01527v2)

**Tags**: cs.CL 



### Numerical analysis of partial discharge ignition in H2 bubbles floating   in dielectric oils, for High-Voltage Solid State Transformer applications
**Authors**: Konstantinos Kourtzanidis, Panagiotis Dimitrakellis, Dimitrios Rakopoulos

**Updated**: 2024-10-08T11:28:30Z

**Summary**: We report on a self-consistent numerical analysis campaign of partial discharge (PD) ignition in H2 bubbles floating in biobased dielectric oils. We investigate various configurations (bubble sizes, bubble position, existence of protrusion) on a cylinder-to-cylinder setup that emulates a specific SST module (from SSTAR Horizon Europe project) under transient overvoltage as well as in its design operational conditions (VRMS = 66 kV, AC excitation of 50 Hz). Our results on electrical characteristics and plasma dynamics leading to the PD ignition, indicate that under transient overvoltage and for mm size bubbles (diameter 1 -4.5 mm), the smaller the bubble the less the inception voltage, while the peak inception voltage is higher than 70 kV. The existence of metallic protrusion can affect the inception voltage of a remote floating bubble only slightly and when this is close to the sharp tip. The extreme scenario of a protrusion in contact (inside) a gas bubble severely affects the insulation properties and drops the PD inception voltage remarkably. The larger the bubble and the sharper the tip of the protrusion the lower the inception peak voltage, that can reach values well below 40 kV. On the contrary and under design operation, larger bubbles increase the severity and probability of PD events, leading to lower instantaneous inception voltages. Current pulses produced in bubbles can quickly transit to intense streamer discharges (which can also transit to catastrophic arcing) if the operational frequency is reduced and/or under transient, HF overvoltage.

**Link**: [arxiv](http://arxiv.org/abs/2410.05927v1),  [pdf](http://arxiv.org/pdf/2410.05927v1)

**Tags**: physics.plasm-ph physics.comp-ph 



### Enhancing Playback Performance in Video Recommender Systems with an   On-Device Gating and Ranking Framework
**Authors**: Yunfei Yang, Zhenghao Qi, Honghuan Wu, Qi Song, Tieyao Zhang, Hao Li, Yimin Tu, Kaiqiao Zhan, Ben Wang

**Updated**: 2024-10-08T09:53:10Z

**Summary**: Video recommender systems (RSs) have gained increasing attention in recent years. Existing mainstream RSs focus on optimizing the matching function between users and items. However, we noticed that users frequently encounter playback issues such as slow loading or stuttering while browsing the videos, especially in weak network conditions, which will lead to a subpar browsing experience, and may cause users to leave, even when the video content and recommendations are superior. It is quite a serious issue, yet easily overlooked. To tackle this issue, we propose an on-device Gating and Ranking Framework (GRF) that cooperates with server-side RS. Specifically, we utilize a gate model to identify videos that may have playback issues in real-time, and then we employ a ranking model to select the optimal result from a locally-cached pool to replace the stuttering videos. Our solution has been fully deployed on Kwai, a large-scale short video platform with hundreds of millions of users globally. Moreover, it significantly enhances video playback performance and improves overall user experience and retention rates.

**Link**: [arxiv](http://arxiv.org/abs/2410.05863v1),  [pdf](http://arxiv.org/pdf/2410.05863v1)

**Tags**: cs.IR 



### A Scalable State Sharing Protocol for Low-Resource Validator Nodes in   Blockchain Networks
**Authors**: Ruben Hias, Weihong Wang, Jan Vanhoof, Tom Van Cutsem

**Updated**: 2024-10-08T09:46:38Z

**Summary**: The perpetual growth of data stored on popular blockchains such as Ethereum leads to significant scalability challenges and substantial storage costs for operators of full nodes. Increasing costs may lead to fewer independently operated nodes in the network, which poses risks to decentralization (and hence network security), but also pushes decentralized app developers towards centrally hosted API services.   This paper introduces a new protocol that allows validator nodes to participate in a blockchain network without the need to store the full state of the network on each node. The key idea is to use the blockchain network as both a replicated state machine and as a distributed storage system. By distributing states across nodes and enabling efficient data retrieval through a Kademlia-inspired routing protocol, we reduce storage costs for validators. Cryptographic proofs (such as Merkle proofs) are used to allow nodes to verify data stored by other nodes without having to trust those nodes directly. While the protocol trades off data storage for increased network bandwidth, we show how gossiping and caching can minimize the increased bandwidth needs.   To validate our state sharing protocol, we conduct an extensive quantitative analysis of Ethereum's data storage and data access patterns. Our findings indicate that while our protocol significantly lowers storage needs, it comes with an increased bandwidth usage ranging from 1.5 MB to 5 MB per block, translating to an additional monthly bandwidth of 319 GB to 1,065 GB. Despite this, the size remains small enough such that it can be passed to all nodes and validated within Ethereum's 12-second block validation window. Further analysis shows that Merkle proofs are the most significant contributor to the additional bandwidth. To address this concern, we also analyze the impact of switching to the more space-efficient Verkle Proofs.

**Link**: [arxiv](http://arxiv.org/abs/2410.05854v1),  [pdf](http://arxiv.org/pdf/2410.05854v1)

**Tags**: cs.DC 



### CItruS: Chunked Instruction-aware State Eviction for Long Sequence   Modeling
**Authors**: Yu Bai, Xiyuan Zou, Heyan Huang, Sanxing Chen, Marc-Antoine Rondeau, Yang Gao, Jackie Chi Kit Cheung

**Updated**: 2024-10-08T04:25:41Z

**Summary**: Long sequence modeling has gained broad interest as large language models (LLMs) continue to advance. Recent research has identified that a large portion of hidden states within the key-value caches of Transformer models can be discarded (also termed evicted) without affecting the perplexity performance in generating long sequences. However, we show that these methods, despite preserving perplexity performance, often drop information that is important for solving downstream tasks, a problem which we call information neglect. To address this issue, we introduce Chunked Instruction-aware State Eviction (CItruS), a novel modeling technique that integrates the attention preferences useful for a downstream task into the eviction process of hidden states. In addition, we design a method for chunked sequence processing to further improve efficiency. Our training-free method exhibits superior performance on long sequence comprehension and retrieval tasks over several strong baselines under the same memory budget, while preserving language modeling perplexity. The code and data have been released at https://github.com/ybai-nlp/CItruS.

**Link**: [arxiv](http://arxiv.org/abs/2406.12018v2),  [pdf](http://arxiv.org/pdf/2406.12018v2)

**Tags**: cs.CL 



### PrefixQuant: Static Quantization Beats Dynamic through Prefixed Outliers   in LLMs
**Authors**: Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo

**Updated**: 2024-10-07T17:59:35Z

**Summary**: Quantization is essential for deploying Large Language Models (LLMs) by enhancing memory efficiency and inference speed. Existing methods for activation quantization mainly address channel-wise outliers, often neglecting token-wise outliers, leading to reliance on costly per-token dynamic quantization. To address this, we introduce PrefixQuant, a novel technique that isolates outlier tokens offline without re-training. Specifically, PrefixQuant identifies high-frequency outlier tokens and prefixes them in the KV cache, preventing the generation of outlier tokens during inference and simplifying quantization. To our knowledge, PrefixQuant is the first to enable efficient per-tensor static quantization to outperform expensive per-token dynamic quantization. For instance, in W4A4KV4 (4- bit weight, 4-bit activation, and 4-bit KV cache) Llama-3-8B, PrefixQuant with per-tensor static quantization achieves a 7.43 WikiText2 perplexity and 71.08% average accuracy on 5 common-sense reasoning tasks, outperforming previous per-token dynamic quantization methods like QuaRot with 0.98 perplexity improvement and +5.98 points accuracy. Additionally, the inference speed of W4A4 quantized models using PrefixQuant is 1.60x to 2.81x faster than FP16 models and exceeds QuaRot models by 1.2x to 1.3x. Our code is available at \url{https://github.com/ChenMnZ/PrefixQuant}.

**Link**: [arxiv](http://arxiv.org/abs/2410.05265v1),  [pdf](http://arxiv.org/pdf/2410.05265v1)

**Tags**: cs.LG cs.CL 



### Stateful Large Language Model Serving with Pensieve
**Authors**: Lingfan Yu, Jinkun Lin, Jinyang Li

**Updated**: 2024-10-07T17:21:57Z

**Summary**: Large Language Models (LLMs) are wildly popular today and it is important to serve them efficiently. Existing LLM serving systems are stateless across requests. Consequently, when LLMs are used in the common setting of multi-turn conversations, a growing log of the conversation history must be processed alongside any request by the serving system at each turn, resulting in repeated processing.   In this paper, we design $Pensieve$, a system optimized for multi-turn conversation LLM serving. $Pensieve$ maintains the conversation state across requests by caching previously processed history to avoid duplicate processing. $Pensieve$'s multi-tier caching strategy can utilize both GPU and CPU memory to efficiently store and retrieve cached data. $Pensieve$ also generalizes the recent PagedAttention kernel to support attention between multiple input tokens with a GPU cache spread over non-contiguous memory. Our evaluation shows that $Pensieve$ can achieve $1.14$-$3.0\times$ the throughput of vLLM and TensorRT-LLM and significantly reduce latency.

**Link**: [arxiv](http://arxiv.org/abs/2312.05516v3),  [pdf](http://arxiv.org/pdf/2312.05516v3)

**Tags**: cs.LG cs.DC 



### KV-Compress: Paged KV-Cache Compression with Variable Compression Rates   per Attention Head
**Authors**: Isaac Rehg

**Updated**: 2024-10-07T15:07:09Z

**Summary**: Context lengths of Large Language Models (LLMs) have exploded in recent years, with 128k-token context becoming a standard and million-token context becoming a reality. Efficiently supporting long-context inference remains challenging as the memory that must be allocated in key-value (KV) cache for a generation scales with its context length, limiting the number of long-context requests that can be served concurrently under a given memory budget. KV cache compression can mitigate this issue by removing under-utilized KVs from each attention head's cache and reducing its memory footprint. Higher theoretical compression rates can be achieved when the number of removed KVs varies across attention heads, but application of such a strategy within existing inference frameworks adds fragmentation and cannot realize the theoretical compression rates in physical memory. We introduce KV-Compress, a novel compression method that evicts contiguous KV blocks within a PagedAttention framework, reducing the memory footprint of the KV cache proportionally to this theoretical compression rate. Our method achieves state-of-the-art performance on LongBench for both Mistral-7B-Instruct-v0.2 and Llama-3.1-8B-Instruct while lowering the total number of compressed KVs by 4x compared with prior methods. Evaluations on Llama-3.1-8B-Instruct and Llama-3.1-70B-Instruct-FP8 achieve compression rates up to 8x with negligible impact on performance, and up to 64x while retaining over 90% of full-cache performance for all but three of the suite's subsets. We benchmark an integration of our method with vLLM that increases total throughput by up to 5.18x by enabling larger decoding batches.

**Link**: [arxiv](http://arxiv.org/abs/2410.00161v2),  [pdf](http://arxiv.org/pdf/2410.00161v2)

**Tags**: cs.CL 



### TidalDecode: Fast and Accurate LLM Decoding with Position Persistent   Sparse Attention
**Authors**: Lijie Yang, Zhihao Zhang, Zhuofu Chen, Zikun Li, Zhihao Jia

**Updated**: 2024-10-07T14:30:27Z

**Summary**: Large language models (LLMs) have driven significant advancements across diverse NLP tasks, with long-context models gaining prominence for handling extended inputs. However, the expanding key-value (KV) cache size required by Transformer architectures intensifies the memory constraints, particularly during the decoding phase, creating a significant bottleneck. Existing sparse attention mechanisms designed to address this bottleneck have two limitations: (1) they often fail to reliably identify the most relevant tokens for attention, and (2) they overlook the spatial coherence of token selection across consecutive Transformer layers, which can lead to performance degradation and substantial overhead in token selection. This paper introduces TidalDecode, a simple yet effective algorithm and system for fast and accurate LLM decoding through position persistent sparse attention. TidalDecode leverages the spatial coherence of tokens selected by existing sparse attention methods and introduces a few token selection layers that perform full attention to identify the tokens with the highest attention scores, while all other layers perform sparse attention with the pre-selected tokens. This design enables TidalDecode to substantially reduce the overhead of token selection for sparse attention without sacrificing the quality of the generated results. Evaluation on a diverse set of LLMs and tasks shows that TidalDecode closely matches the generative performance of full attention methods while reducing the LLM decoding latency by up to 2.1x.

**Link**: [arxiv](http://arxiv.org/abs/2410.05076v1),  [pdf](http://arxiv.org/pdf/2410.05076v1)

**Tags**: cs.LG cs.AI cs.CL 



### Extended Functional Representation Lemma: A Tool For Privacy, Semantic   Representation, Caching, and Compression Design
**Authors**: Amirreza Zamani, Mikael Skoglund

**Updated**: 2024-10-07T13:33:23Z

**Summary**: This paper provides an overview of a problem in information-theoretic privacy mechanism design, addressing two scenarios in which private data is either observable or hidden. In each scenario, different privacy measures are used, including bounded mutual information and two types of per-letter privacy constraints. Considering the first scenario, an agent observes useful data that is correlated with private data, and wants to disclose the useful information to a user. Due to the privacy concerns, direct disclosure is prohibited. Hence, a privacy mechanism is designed to generate disclosed data which maximizes the revealed information about the useful data while satisfying a privacy constraint. In the second scenario, the agent has additionally access to the private data. We discuss how the Functional Representation Lemma, the Strong Functional Representation Lemma, and their extended versions are useful for designing low-complexity privacy mechanisms that achieve optimal privacy-utility trade-offs under certain constraints. Furthermore, another privacy design problem is presented where part of the private attribute is more private than the remaining part. Finally, we provide applications including semantic communications, caching and delivery, and compression designs, where the approach can be applied.

**Link**: [arxiv](http://arxiv.org/abs/2410.05033v1),  [pdf](http://arxiv.org/pdf/2410.05033v1)

**Tags**: cs.IT math.IT 



### Fast State Restoration in LLM Serving with HCache
**Authors**: Shiwei Gao, Youmin Chen, Jiwu Shu

**Updated**: 2024-10-07T13:03:45Z

**Summary**: The growing complexity of LLM usage today, e.g., multi-round conversation and retrieval-augmented generation (RAG), makes contextual states (i.e., KV cache) reusable across user requests. Given the capacity constraints of GPU memory, only a limited number of contexts can be cached on GPU for reusing. Existing inference systems typically evict part of the KV cache and restore it by recomputing it from the original tokens or offloading it to host storage for later retrieval, both of which introduce substantial computational or I/O overheads. We propose HCache, a novel LLM state restoration method. Its key idea is to restore LLM states from intermediate activations and thus utilize computational and I/O resources with low overhead. We enhance HCache with two techniques, including i) a bubble-free restoration scheduler that integrates resource-complementary methods to optimize the balance between computation and IO tasks; and ii) a chunk-based storage manager to address the layout mismatch issue (i.e., layer-before-token saving versus token-before-layer restoration). Our evaluations, conducted using real-world tasks, show that HCache reduces the TTFT by up to 1.93X compared to KV offload while consuming 1.92-2.40X less storage space; compared to token recomputation, HCache achieves up to 5.73X reduction in TTFT.

**Link**: [arxiv](http://arxiv.org/abs/2410.05004v1),  [pdf](http://arxiv.org/pdf/2410.05004v1)

**Tags**: cs.DC 



### SpinQuant: LLM quantization with learned rotations
**Authors**: Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, Tijmen Blankevoort

**Updated**: 2024-10-07T01:27:59Z

**Summary**: Post-training quantization (PTQ) techniques applied to weights, activations, and the KV cache greatly reduce memory usage, latency, and power consumption of Large Language Models (LLMs), but may lead to large quantization errors when outliers are present. Rotating activation or weight matrices helps remove outliers and benefits quantization. In this work, we identify a collection of applicable rotation parameterizations that lead to identical outputs in full-precision Transformer architectures while enhancing quantization accuracy. In addition, we find that some random rotations lead to much better quantization than others, with an up to 13 points difference in downstream zero-shot reasoning performance. As a result, we propose SpinQuant, a novel approach that incorporates learned rotation matrices for optimal quantized network accuracy. With 4-bit quantization of weight, activation, and KV-cache, SpinQuant narrows the accuracy gap on zero-shot reasoning tasks with full precision to merely 2.9 points on the LLaMA-2 7B model, surpassing LLM-QAT by 19.1 points and SmoothQuant by 25.0 points. Furthermore, SpinQuant also outperforms concurrent work QuaRot, which applies random rotations to remove outliers. In particular, for LLaMA-3 8B models that are hard to quantize, SpinQuant reduces the gap to full precision by up to 45.1% relative to QuaRot.

**Link**: [arxiv](http://arxiv.org/abs/2405.16406v3),  [pdf](http://arxiv.org/pdf/2405.16406v3)

**Tags**: cs.LG cs.AI cs.CL cs.CV 



### Self-compensating Light Calorimetry with Liquid Argon Time Projection   Chamber for GeV Neutrino Physics
**Authors**: Xuyang Ning, Wei Shi, Chao Zhang, Ciro Riccio, Jay Hyun Jo

**Updated**: 2024-10-06T19:36:34Z

**Summary**: Liquid Argon Time Projection Chamber (LArTPC) is an exceptional dual calorimeter capable of estimating the energy of incident particles through both the ionization charge and the scintillation light. Our studies show that due to the mechanisms of charge recombination and light generation involved in the energy dissipation in liquid argon, light calorimetry in LArTPCs is inherently self-compensating: the missing energy in the hadronic component is compensated for by the extra recombination luminescence compared to the electromagnetic component. Good compensation of the electron-to-hadron response ratio (e/h) around unity can be achieved across a broad range of drift electric fields from 0.2 to 1.8 kV/cm.This inherent self-compensation enhances the appeal of light calorimetry in LArTPCs, complementing the well-established charge calorimetry. Using GeV neutrinos as a case study, we show that light calorimetry can achieve an energy resolution comparable to the more sophisticated charge imaging calorimetry. The synergy between light and charge calorimetry offers a novel approach to evaluating and mitigating systematic uncertainties in energy measurements with LArTPCs.

**Link**: [arxiv](http://arxiv.org/abs/2410.04603v1),  [pdf](http://arxiv.org/pdf/2410.04603v1)

**Tags**: physics.ins-det hep-ex 



### Lazy Qubit Reordering for Accelerating Parallel State-Vector-based   Quantum Circuit Simulation
**Authors**: Yusuke Teranishi, Shoma Hiraoka, Wataru Mizukami, Masao Okita, Fumihiko Ino

**Updated**: 2024-10-05T18:20:37Z

**Summary**: This paper proposes two quantum operation scheduling methods for accelerating parallel state-vector-based quantum circuit simulation using multiple graphics processing units (GPUs). The proposed methods reduce all-to-all communication caused by qubit reordering (QR), which can dominate the overhead of parallel simulation. Our approach eliminates redundant QRs by introducing intentional delays in QR communications such that multiple QRs can be aggregated into a single QR. The delays are carefully introduced based on the principles of time-space tiling, or a cache optimization technique for classical computers, which we use to arrange the execution order of quantum operations. Moreover, we present an extended scheduling method for the hierarchical interconnection of GPU cluster systems to avoid slow inter-node communication. We develop these methods tailored for two primary procedures in variational quantum eigensolver (VQE) simulation: quantum state update (QSU) and expectation value computation (EVC). Experimental validation on 32-GPU executions demonstrates acceleration in QSU and EVC -- up to 54$\times$ and 606$\times$, respectively -- compared to existing methods. Moreover, our extended scheduling method further reduced communication time by up to 15\% in a two-layered interconnected cluster system. Our approach is useful for any quantum circuit simulations, including QSU and/or EVC.

**Link**: [arxiv](http://arxiv.org/abs/2410.04252v1),  [pdf](http://arxiv.org/pdf/2410.04252v1)

**Tags**: quant-ph cs.DC 



### SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving   Model Transformation
**Authors**: Aurick Qiao, Zhewei Yao, Samyam Rajbhandari, Yuxiong He

**Updated**: 2024-10-04T22:45:26Z

**Summary**: LLM inference for popular enterprise use cases, such as summarization, RAG, and code-generation, typically observes orders of magnitude longer prompt lengths than generation lengths. This characteristic leads to high cost of prefill and increased response latency. In this paper, we present SwiftKV, a novel model transformation and distillation procedure specifically designed to reduce the time and cost of processing prompt tokens while preserving high quality of generated tokens. SwiftKV combines three key mechanisms: i) SingleInputKV, which prefills later layers' KV cache using a much earlier layer's output, allowing prompt tokens to skip much of the model computation, ii) AcrossKV, which merges the KV caches of neighboring layers to reduce the memory footprint and support larger batch size for higher throughput, and iii) a knowledge-preserving distillation procedure that can adapt existing LLMs for SwiftKV with minimal accuracy impact and low compute and data requirement. For Llama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50% and the memory requirement of the KV cache by 62.5% while incurring minimum quality degradation across a wide range of tasks. In the end-to-end inference serving using an optimized vLLM implementation, SwiftKV realizes up to 2x higher aggregate throughput and 60% lower time per output token. It can achieve a staggering 560 TFlops/GPU of normalized inference throughput, which translates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100 GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2410.03960v1),  [pdf](http://arxiv.org/pdf/2410.03960v1)

**Tags**: cs.LG cs.AI cs.CL 



### LLMProxy: Reducing Cost to Access Large Language Models
**Authors**: Noah Martin, Abdullah Bin Faisal, Hiba Eltigani, Rukhshan Haroon, Swaminathan Lamelas, Fahad Dogar

**Updated**: 2024-10-04T15:23:28Z

**Summary**: In this paper, we make a case for a proxy for large language models which has explicit support for cost-saving optimizations. We design LLMProxy, which supports three key optimizations: model selection, context management, and caching. These optimizations present tradeoffs in terms of cost, inference time, and response quality, which applications can navigate through our high level, bidirectional interface. As a case study, we implement a WhatsApp-based Q&A service that uses LLMProxy to provide a rich set of features to the users. This service is deployed on a small scale (100+ users) leveraging the cloud; it has been operational for 15+ weeks and users have asked 1400+ questions so far. We report on the experiences of running this service as well as microbenchmark the specific benefits of the various cost-optimizations we present in this paper.

**Link**: [arxiv](http://arxiv.org/abs/2410.11857v1),  [pdf](http://arxiv.org/pdf/2410.11857v1)

**Tags**: cs.DC cs.LG 



### HarmoniCa: Harmonizing Training and Inference for Better Feature Cache   in Diffusion Transformer Acceleration
**Authors**: Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jun Zhang

**Updated**: 2024-10-04T10:14:17Z

**Summary**: Diffusion Transformers (DiTs) have gained prominence for outstanding scalability and extraordinary performance in generative tasks. However, their considerable inference costs impede practical deployment. The feature cache mechanism, which involves storing and retrieving redundant computations across timesteps, holds promise for reducing per-step inference time in diffusion models. Most existing caching methods for DiT are manually designed. Although the learning-based approach attempts to optimize strategies adaptively, it suffers from discrepancies between training and inference, which hampers both the performance and acceleration ratio. Upon detailed analysis, we pinpoint that these discrepancies primarily stem from two aspects: (1) Prior Timestep Disregard, where training ignores the effect of cache usage at earlier timesteps, and (2) Objective Mismatch, where the training target (align predicted noise in each timestep) deviates from the goal of inference (generate the high-quality image). To alleviate these discrepancies, we propose HarmoniCa, a novel method that Harmonizes training and inference with a novel learning-based Caching framework built upon Step-Wise Denoising Training (SDT) and Image Error Proxy-Guided Objective (IEPO). Compared to the traditional training paradigm, the newly proposed SDT maintains the continuity of the denoising process, enabling the model to leverage information from prior timesteps during training, similar to the way it operates during inference. Furthermore, we design IEPO, which integrates an efficient proxy mechanism to approximate the final image error caused by reusing the cached feature. Therefore, IEPO helps balance final image quality and cache utilization, resolving the issue of training that only considers the impact of cache usage on the predicted output at each timestep.

**Link**: [arxiv](http://arxiv.org/abs/2410.01723v2),  [pdf](http://arxiv.org/pdf/2410.01723v2)

**Tags**: cs.CV 



### Unleashing the Potential of the Diffusion Model in Few-shot Semantic   Segmentation
**Authors**: Muzhi Zhu, Yang Liu, Zekai Luo, Chenchen Jing, Hao Chen, Guangkai Xu, Xinlong Wang, Chunhua Shen

**Updated**: 2024-10-04T07:54:58Z

**Summary**: The Diffusion Model has not only garnered noteworthy achievements in the realm of image generation but has also demonstrated its potential as an effective pretraining method utilizing unlabeled data. Drawing from the extensive potential unveiled by the Diffusion Model in both semantic correspondence and open vocabulary segmentation, our work initiates an investigation into employing the Latent Diffusion Model for Few-shot Semantic Segmentation. Recently, inspired by the in-context learning ability of large language models, Few-shot Semantic Segmentation has evolved into In-context Segmentation tasks, morphing into a crucial element in assessing generalist segmentation models. In this context, we concentrate on Few-shot Semantic Segmentation, establishing a solid foundation for the future development of a Diffusion-based generalist model for segmentation. Our initial focus lies in understanding how to facilitate interaction between the query image and the support image, resulting in the proposal of a KV fusion method within the self-attention framework. Subsequently, we delve deeper into optimizing the infusion of information from the support mask and simultaneously re-evaluating how to provide reasonable supervision from the query mask. Based on our analysis, we establish a simple and effective framework named DiffewS, maximally retaining the original Latent Diffusion Model's generative framework and effectively utilizing the pre-training prior. Experimental results demonstrate that our method significantly outperforms the previous SOTA models in multiple settings.

**Link**: [arxiv](http://arxiv.org/abs/2410.02369v2),  [pdf](http://arxiv.org/pdf/2410.02369v2)

**Tags**: cs.CV 



### Prefixing Attention Sinks can Mitigate Activation Outliers for Large   Language Model Quantization
**Authors**: Seungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, Jaeho Lee

**Updated**: 2024-10-04T06:26:20Z

**Summary**: Despite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to find a set of key-value cache, coined CushionCache, which mitigates outliers in subsequent tokens when inserted as a prefix. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods. We thoroughly evaluate our method over a wide range of models and benchmarks and find that it significantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method.

**Link**: [arxiv](http://arxiv.org/abs/2406.12016v2),  [pdf](http://arxiv.org/pdf/2406.12016v2)

**Tags**: cs.LG cs.CL 



### LoRC: Low-Rank Compression for LLMs KV Cache with a Progressive   Compression Strategy
**Authors**: Rongzhi Zhang, Kuang Wang, Liyuan Liu, Shuohang Wang, Hao Cheng, Chao Zhang, Yelong Shen

**Updated**: 2024-10-04T03:10:53Z

**Summary**: The Key-Value (KV) cache is a crucial component in serving transformer-based autoregressive large language models (LLMs), enabling faster inference by storing previously computed KV vectors. However, its memory consumption scales linearly with sequence length and batch size, posing a significant bottleneck in LLM deployment. Existing approaches to mitigate this issue include: (1) efficient attention variants integrated in upcycling stages, which requires extensive parameter tuning thus unsuitable for pre-trained LLMs; (2) KV cache compression at test time, primarily through token eviction policies, which often overlook inter-layer dependencies and can be task-specific.   This paper introduces an orthogonal approach to KV cache compression. We propose a low-rank approximation of KV weight matrices, allowing for plug-in integration with existing transformer-based LLMs without model retraining. To effectively compress KV cache at the weight level, we adjust for layerwise sensitivity and introduce a progressive compression strategy, which is supported by our theoretical analysis on how compression errors accumulate in deep networks. Our method is designed to function without model tuning in upcycling stages or task-specific profiling in test stages. Extensive experiments with LLaMA models ranging from 8B to 70B parameters across various tasks show that our approach significantly reduces the GPU memory footprint while maintaining performance.

**Link**: [arxiv](http://arxiv.org/abs/2410.03111v1),  [pdf](http://arxiv.org/pdf/2410.03111v1)

**Tags**: cs.LG cs.AI cs.CL I.2 



### UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large   Language Model Inference
**Authors**: Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Lingpeng Kong, Ngai Wong

**Updated**: 2024-10-04T02:32:36Z

**Summary**: Deploying large language models (LLMs) is challenging due to their high memory and computational demands, especially during long-context inference. While key-value (KV) caching accelerates inference by reusing previously computed keys and values, it also introduces significant memory overhead. Existing KV cache compression methods such as eviction and merging typically compress the KV cache after it is generated and overlook the eviction of hidden states, failing to improve the speed of the prefilling stage. Additionally, applying a uniform compression rate across different attention heads can harm crucial retrieval heads in needle-in-a-haystack tasks due to excessive compression. In this paper, we propose UNComp, an uncertainty-aware compression scheme that leverages matrix entropy to estimate model uncertainty across layers and heads at the token sequence level. By grouping layers and heads based on their uncertainty, UNComp adaptively compresses both the hidden states and the KV cache. Our method achieves a 1.6x speedup in the prefilling stage and reduces the KV cache to 4.74% of its original size, resulting in a 6.4x increase in throughput and a 1.4x speedup in inference with only a 1.41% performance loss. Remarkably, in needle-in-a-haystack tasks, UNComp outperforms the full-size KV cache even when compressed to 9.38% of its original size. Our approach offers an efficient, training-free Grouped-Query Attention paradigm that can be seamlessly integrated into existing KV cache schemes.

**Link**: [arxiv](http://arxiv.org/abs/2410.03090v1),  [pdf](http://arxiv.org/pdf/2410.03090v1)

**Tags**: cs.CL cs.LG 



### Compute Or Load KV Cache? Why Not Both?
**Authors**: Shuowei Jin, Xueshen Liu, Qingzhao Zhang, Z. Morley Mao

**Updated**: 2024-10-04T01:11:09Z

**Summary**: Recent advancements in Large Language Models (LLMs) have significantly increased context window sizes, enabling sophisticated applications but also introducing substantial computational overheads, particularly computing key-value (KV) cache in the prefill stage. Prefix caching has emerged to save GPU power in this scenario, which saves KV cache at disks and reuse them across multiple queries. However, traditional prefix caching mechanisms often suffer from substantial latency because the speed of loading KV cache from disks to GPU memory is bottlenecked by the throughput of I/O devices. To optimize the latency of long-context prefill, we propose Cake, a novel KV cache loader, which employs a bidirectional parallelized KV cache generation strategy. Upon receiving a prefill task, Cake simultaneously and dynamically loads saved KV cache from prefix cache locations and computes KV cache on local GPUs, maximizing the utilization of available computation and I/O bandwidth resources. Additionally, Cake automatically adapts to diverse system statuses without manual parameter. tuning. In experiments on various prompt datasets, GPUs, and I/O devices, Cake offers up to 68.1% Time To First Token (TTFT) reduction compare with compute-only method and 94.6% TTFT reduction compare with I/O-only method.

**Link**: [arxiv](http://arxiv.org/abs/2410.03065v1),  [pdf](http://arxiv.org/pdf/2410.03065v1)

**Tags**: cs.LG 



### DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured   LLM Inference
**Authors**: Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke Wang, Tao Lin

**Updated**: 2024-10-03T22:17:01Z

**Summary**: Large language models (LLMs) are increasingly employed for complex tasks that process multiple generation calls in a tree structure with shared prefixes of tokens, including few-shot prompting, multi-step reasoning, speculative decoding, etc. However, existing inference systems for tree-based applications are inefficient due to improper partitioning of queries and KV cache during attention calculation. This leads to two main issues: (1) a lack of memory access (IO) reuse for KV cache of shared prefixes, and (2) poor load balancing.As a result, there is redundant KV cache IO between GPU global memory and shared memory, along with low GPU utilization. To address these challenges, we propose DeFT(Decoding with Flash Tree-Attention), a hardware-efficient attention algorithm with prefix-aware and load-balanced KV cache partitions. DeFT reduces the number of read/write operations of KV cache during attention calculation through KV-Guided Grouping, a method that avoids repeatedly loading KV cache of shared prefixes in attention computation. Additionally, we propose Flattened Tree KV Splitting, a mechanism that ensures even distribution of the KV cache across partitions with little computation redundancy, enhancing GPU utilization during attention computations. By reducing 73-99 KV cache IO and nearly 100 IO for partial results during attention calculation, DeFT achieves up to 2.52/3.82x speedup in the end-to-end/attention latency across three practical tree-based workloads compared to state-of-the-art attention algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2404.00242v3),  [pdf](http://arxiv.org/pdf/2404.00242v3)

**Tags**: cs.CL cs.AI 



### GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering
**Authors**: Jiaye Wu, Saeed Hadadan, Geng Lin, Matthias Zwicker, David Jacobs, Roni Sengupta

**Updated**: 2024-10-03T22:11:19Z

**Summary**: In this paper, we present GaNI, a Global and Near-field Illumination-aware neural inverse rendering technique that can reconstruct geometry, albedo, and roughness parameters from images of a scene captured with co-located light and camera. Existing inverse rendering techniques with co-located light-camera focus on single objects only, without modeling global illumination and near-field lighting more prominent in scenes with multiple objects. We introduce a system that solves this problem in two stages; we first reconstruct the geometry powered by neural volumetric rendering NeuS, followed by inverse neural radiosity that uses the previously predicted geometry to estimate albedo and roughness. However, such a naive combination fails and we propose multiple technical contributions that enable this two-stage approach. We observe that NeuS fails to handle near-field illumination and strong specular reflections from the flashlight in a scene. We propose to implicitly model the effects of near-field illumination and introduce a surface angle loss function to handle specular reflections. Similarly, we observe that invNeRad assumes constant illumination throughout the capture and cannot handle moving flashlights during capture. We propose a light position-aware radiance cache network and additional smoothness priors on roughness to reconstruct reflectance. Experimental evaluation on synthetic and real data shows that our method outperforms the existing co-located light-camera-based inverse rendering techniques. Our approach produces significantly better reflectance and slightly better geometry than capture strategies that do not require a dark room.

**Link**: [arxiv](http://arxiv.org/abs/2403.15651v2),  [pdf](http://arxiv.org/pdf/2403.15651v2)

**Tags**: cs.CV 



### ReLIC: A Recipe for 64k Steps of In-Context Reinforcement Learning for   Embodied AI
**Authors**: Ahmad Elawady, Gunjan Chhablani, Ram Ramrakhya, Karmesh Yadav, Dhruv Batra, Zsolt Kira, Andrew Szot

**Updated**: 2024-10-03T17:58:11Z

**Summary**: Intelligent embodied agents need to quickly adapt to new scenarios by integrating long histories of experience into decision-making. For instance, a robot in an unfamiliar house initially wouldn't know the locations of objects needed for tasks and might perform inefficiently. However, as it gathers more experience, it should learn the layout of its environment and remember where objects are, allowing it to complete new tasks more efficiently. To enable such rapid adaptation to new tasks, we present ReLIC, a new approach for in-context reinforcement learning (RL) for embodied agents. With ReLIC, agents are capable of adapting to new environments using 64,000 steps of in-context experience with full attention while being trained through self-generated experience via RL. We achieve this by proposing a novel policy update scheme for on-policy RL called "partial updates'' as well as a Sink-KV mechanism that enables effective utilization of a long observation history for embodied agents. Our method outperforms a variety of meta-RL baselines in adapting to unseen houses in an embodied multi-object navigation task. In addition, we find that ReLIC is capable of few-shot imitation learning despite never being trained with expert demonstrations. We also provide a comprehensive analysis of ReLIC, highlighting that the combination of large-scale RL training, the proposed partial updates scheme, and the Sink-KV are essential for effective in-context learning. The code for ReLIC and all our experiments is at https://github.com/aielawady/relic

**Link**: [arxiv](http://arxiv.org/abs/2410.02751v1),  [pdf](http://arxiv.org/pdf/2410.02751v1)

**Tags**: cs.LG 



### Preble: Efficient Distributed Prompt Scheduling for LLM Serving
**Authors**: Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, Yiying Zhang

**Updated**: 2024-10-03T17:50:33Z

**Summary**: Prompts to large language models (LLMs) have evolved beyond simple user questions. For LLMs to solve complex problems, today's practices are to include domain-specific instructions, illustration of tool usages, and/or long context such as textbook chapters in prompts. As such, many parts of prompts are repetitive across requests. Recent works propose to cache and reuse KV state of prompts. However, they are all confined to a single-GPU optimization, while production LLM serving systems are distributed by nature.   This paper proposes Preble, the first distributed LLM serving platform that targets and optimizes for prompt sharing. We designed a distributed scheduling system that co-optimizes KV state reuse and computation load-balancing with a new scheduling algorithm and a hierarchical scheduling mechanism. Our evaluation of Preble with real workloads and request arrival patterns on two open-source LLMs shows that Preble outperforms the SOTA serving systems by 1.5X to 14.5X on average latency and 2X to 10X on p99 latency.

**Link**: [arxiv](http://arxiv.org/abs/2407.00023v2),  [pdf](http://arxiv.org/pdf/2407.00023v2)

**Tags**: cs.DC cs.LG 



### Disaggregated Memory with SmartNIC Offloading: a Case Study on Graph   Processing
**Authors**: Jacob Wahlgren, Gabin Schieffer, Maya Gokhale, Roger Pearce, Ivy Peng

**Updated**: 2024-10-03T15:41:31Z

**Summary**: Disaggregated memory breaks the boundary of monolithic servers to enable memory provisioning on demand. Using network-attached memory to provide memory expansion for memory-intensive applications on compute nodes can improve the overall memory utilization on a cluster and reduce the total cost of ownership. However, current software solutions for leveraging network-attached memory must consume resources on the compute node for memory management tasks. Emerging off-path smartNICs provide general-purpose programmability at low-cost low-power cores. This work provides a general architecture design that enables network-attached memory and offloading tasks onto off-path programmable SmartNIC. We provide a prototype implementation called SODA on Nvidia BlueField DPU. SODA adapts communication paths and data transfer alternatives, pipelines data movement stages, and enables customizable data caching and prefetching optimizations. We evaluate SODA in five representative graph applications on real-world graphs. Our results show that SODA can achieve up to 7.9x speedup compared to node-local SSD and reduce network traffic by 42% compared to disaggregated memory without SmartNIC offloading at similar or better performance.

**Link**: [arxiv](http://arxiv.org/abs/2410.02599v1),  [pdf](http://arxiv.org/pdf/2410.02599v1)

**Tags**: cs.DC 



### Learning from Offline Foundation Features with Tensor Augmentations
**Authors**: Emir Konuk, Christos Matsoukas, Moein Sorkhei, Phitchapha Lertsiravaramet, Kevin Smith

**Updated**: 2024-10-03T14:35:35Z

**Summary**: We introduce Learning from Offline Foundation Features with Tensor Augmentations (LOFF-TA), an efficient training scheme designed to harness the capabilities of foundation models in limited resource settings where their direct development is not feasible. LOFF-TA involves training a compact classifier on cached feature embeddings from a frozen foundation model, resulting in up to $37\times$ faster training and up to $26\times$ reduced GPU memory usage. Because the embeddings of augmented images would be too numerous to store, yet the augmentation process is essential for training, we propose to apply tensor augmentations to the cached embeddings of the original non-augmented images. LOFF-TA makes it possible to leverage the power of foundation models, regardless of their size, in settings with limited computational capacity. Moreover, LOFF-TA can be used to apply foundation models to high-resolution images without increasing compute. In certain scenarios, we find that training with LOFF-TA yields better results than directly fine-tuning the foundation model.

**Link**: [arxiv](http://arxiv.org/abs/2410.02527v1),  [pdf](http://arxiv.org/pdf/2410.02527v1)

**Tags**: cs.CV 



## Keyword: LLM Inference 
 ### HyperspectralViTs: Fast and Accurate methane detection on-board   satellites
**Authors**: Vt Rika, Andrew Markham

**Updated**: 2024-10-22T17:59:55Z

**Summary**: On-board processing of hyperspectral data with machine learning models would enable unprecedented amount of autonomy for a wide range of tasks, for example methane detection or mineral identification. Methane is the second most important greenhouse gas contributor to climate change, and it's automated detection on-board of satellites using machine learning models would allow for early warning system and could enable new capabilities such as automated scheduling inside constellations of satellites. Classical methods for methane detection suffer from high false positive rates and previous deep learning models exhibit prohibitive computational requirements. We propose fast and accurate machine learning architectures which support end-to-end training with data of high spectral dimension. We evaluate our models on two tasks related to hyperspectral data processing - methane leak detection and mineral identification. With our proposed general architectures, we improve the F1 score of the previous methane detection state-of-the-art models by more than 27% on a newly created synthetic dataset and by almost 13% on the previously released large benchmark dataset. We also demonstrate that training models on the synthetic dataset improves performance of models finetuned on the dataset of real events by 6.9% in F1 score in contrast with training from scratch. On a newly created dataset for mineral identification, our models provide 3.5% improvement in the F1 score in contrast to the default versions of the models. With our proposed models we improve the inference speed by 85.19% in contrast to previous classical and deep learning approaches by removing the dependency on classically computed features. Namely, one capture from the EMIT sensor can be processed in only 30 seconds on a realistic proxy hardware used on the ION-SCV 004 satellite.

**Link**: [arxiv](http://arxiv.org/abs/2410.17248v1),  [pdf](http://arxiv.org/pdf/2410.17248v1)

**Tags**: cs.AI 



### PyramidDrop: Accelerating Your Large Vision-Language Models via Pyramid   Visual Redundancy Reduction
**Authors**: Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, Dahua Lin

**Updated**: 2024-10-22T17:59:53Z

**Summary**: In large vision-language models (LVLMs), images serve as inputs that carry a wealth of information. As the idiom "A picture is worth a thousand words" implies, representing a single image in current LVLMs can require hundreds or even thousands of tokens. This results in significant computational costs, which grow quadratically as input image resolution increases, thereby severely impacting the efficiency of both training and inference. Previous approaches have attempted to reduce the number of image tokens either before or within the early layers of LVLMs. However, these strategies inevitably result in the loss of crucial image information, ultimately diminishing model performance. To address this challenge, we conduct an empirical study revealing that all visual tokens are necessary for LVLMs in the shallow layers, and token redundancy progressively increases in the deeper layers of the model. To this end, we propose PyramidDrop, a visual redundancy reduction strategy for LVLMs to boost their efficiency in both training and inference with neglectable performance loss. Specifically, we partition the LVLM into several stages and drop part of the image tokens at the end of each stage with a pre-defined ratio, creating pyramid-like visual tokens across model layers. The dropping is based on a lightweight similarity calculation with a negligible time overhead. Extensive experiments demonstrate that PyramidDrop can achieve a 40% training time and 55% inference FLOPs acceleration of LLaVA-NeXT with comparable performance. Besides, the PyramidDrop could also serve as a plug-and-play strategy for inference acceleration without training, with better performance and lower inference cost than counterparts. We hope that the insights and approach introduced by PyramidDrop will inspire future research to further investigate the role of image tokens in LVLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17247v1),  [pdf](http://arxiv.org/pdf/2410.17247v1)

**Tags**: cs.CV cs.CL 



### Towards Reliable Evaluation of Behavior Steering Interventions in LLMs
**Authors**: Itamar Pres, Laura Ruis, Ekdeep Singh Lubana, David Krueger

**Updated**: 2024-10-22T17:59:39Z

**Summary**: Representation engineering methods have recently shown promise for enabling efficient steering of model behavior. However, evaluation pipelines for these methods have primarily relied on subjective demonstrations, instead of quantitative, objective metrics. We aim to take a step towards addressing this issue by advocating for four properties missing from current evaluations: (i) contexts sufficiently similar to downstream tasks should be used for assessing intervention quality; (ii) model likelihoods should be accounted for; (iii) evaluations should allow for standardized comparisons across different target behaviors; and (iv) baseline comparisons should be offered. We introduce an evaluation pipeline grounded in these criteria, offering both a quantitative and visual analysis of how effectively a given method works. We use this pipeline to evaluate two representation engineering methods on how effectively they can steer behaviors such as truthfulness and corrigibility, finding that some interventions are less effective than previously reported.

**Link**: [arxiv](http://arxiv.org/abs/2410.17245v1),  [pdf](http://arxiv.org/pdf/2410.17245v1)

**Tags**: cs.AI cs.CL 



### LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias
**Authors**: Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, Zexiang Xu

**Updated**: 2024-10-22T17:58:28Z

**Summary**: We propose the Large View Synthesis Model (LVSM), a novel transformer-based approach for scalable and generalizable novel view synthesis from sparse-view inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which encodes input image tokens into a fixed number of 1D latent tokens, functioning as a fully learned scene representation, and decodes novel-view images from them; and (2) a decoder-only LVSM, which directly maps input images to novel-view outputs, completely eliminating intermediate scene representations. Both models bypass the 3D inductive biases used in previous methods -- from 3D representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar projections, plane sweeps) -- addressing novel view synthesis with a fully data-driven approach. While the encoder-decoder model offers faster inference due to its independent latent representation, the decoder-only LVSM achieves superior quality, scalability, and zero-shot generalization, outperforming previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive evaluations across multiple datasets demonstrate that both LVSM variants achieve state-of-the-art novel view synthesis quality. Notably, our models surpass all previous methods even with reduced computational resources (1-2 GPUs). Please see our website for more details: https://haian-jin.github.io/projects/LVSM/ .

**Link**: [arxiv](http://arxiv.org/abs/2410.17242v1),  [pdf](http://arxiv.org/pdf/2410.17242v1)

**Tags**: cs.CV cs.GR cs.LG 



### SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning
**Authors**: Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, Chenglin Wu

**Updated**: 2024-10-22T17:56:08Z

**Summary**: Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-Search Enhanced LLM Agents (SELA), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space. This novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges.

**Link**: [arxiv](http://arxiv.org/abs/2410.17238v1),  [pdf](http://arxiv.org/pdf/2410.17238v1)

**Tags**: cs.AI cs.CL cs.LG cs.SE 



### Large Language Models Empowered Personalized Web Agents
**Authors**: Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, Tat-Seng Chua

**Updated**: 2024-10-22T17:54:45Z

**Summary**: Web agents have emerged as a promising direction to automate Web task completion based on user instructions, significantly enhancing user experience. Recently, Web agents have evolved from traditional agents to Large Language Models (LLMs)-based Web agents. Despite their success, existing LLM-based Web agents overlook the importance of personalized data (e.g., user profiles and historical Web behaviors) in assisting the understanding of users' personalized instructions and executing customized actions. To overcome the limitation, we first formulate the task of LLM-empowered personalized Web agents, which integrate personalized data and user instructions to personalize instruction comprehension and action execution. To address the absence of a comprehensive evaluation benchmark, we construct a Personalized Web Agent Benchmark (PersonalWAB), featuring user instructions, personalized user data, Web functions, and two evaluation paradigms across three personalized Web tasks. Moreover, we propose a Personalized User Memory-enhanced Alignment (PUMA) framework to adapt LLMs to the personalized Web agent task. PUMA utilizes a memory bank with a task-specific retrieval strategy to filter relevant historical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for personalized action execution through fine-tuning and direct preference optimization. Extensive experiments validate the superiority of PUMA over existing Web agents on PersonalWAB.

**Link**: [arxiv](http://arxiv.org/abs/2410.17236v1),  [pdf](http://arxiv.org/pdf/2410.17236v1)

**Tags**: cs.CL cs.AI cs.IR 



### Fine-Tuning Large Language Models to Appropriately Abstain with Semantic   Entropy
**Authors**: Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, Kunal Handa, Yarin Gal

**Updated**: 2024-10-22T17:54:03Z

**Summary**: Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation strategies. While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses. To address these limitations, we propose fine-tuning using semantic entropy, an uncertainty measure derived from introspection into the model which does not require external labels. We demonstrate that our approach matches or outperforms models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.

**Link**: [arxiv](http://arxiv.org/abs/2410.17234v1),  [pdf](http://arxiv.org/pdf/2410.17234v1)

**Tags**: cs.CL cs.LG 



### Few-shot In-Context Preference Learning Using Large Language Models
**Authors**: Chao Yu, Hong Lu, Jiaxuan Gao, Qixin Tan, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky

**Updated**: 2024-10-22T17:53:34Z

**Summary**: Designing reward functions is a core component of reinforcement learning but can be challenging for truly complex behavior. Reinforcement Learning from Human Feedback (RLHF) has been used to alleviate this challenge by replacing a hand-coded reward function with a reward function learned from preferences. However, it can be exceedingly inefficient to learn these rewards as they are often learned tabula rasa. We investigate whether Large Language Models (LLMs) can reduce this query inefficiency by converting an iterative series of human preferences into code representing the rewards. We propose In-Context Preference Learning (ICPL), a method that uses the grounding of an LLM to accelerate learning reward functions from preferences. ICPL takes the environment context and task description, synthesizes a set of reward functions, and then repeatedly updates the reward functions using human rankings of videos of the resultant policies. Using synthetic preferences, we demonstrate that ICPL is orders of magnitude more efficient than RLHF and is even competitive with methods that use ground-truth reward functions instead of preferences. Finally, we perform a series of human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop. Additional information and videos are provided at https://sites.google.com/view/few-shot-icpl/home.

**Link**: [arxiv](http://arxiv.org/abs/2410.17233v1),  [pdf](http://arxiv.org/pdf/2410.17233v1)

**Tags**: cs.AI cs.LG 



### Context-aware Prompt Tuning: Advancing In-Context Learning with   Adversarial Methods
**Authors**: Tsachi Blau, Moshe Kimhi, Yonatan Belinkov, Alexander Bronstein, Chaim Baskin

**Updated**: 2024-10-22T17:45:47Z

**Summary**: Fine-tuning Large Language Models (LLMs) typically involves updating at least a few billions of parameters. A more parameter-efficient approach is Prompt Tuning (PT), which updates only a few learnable tokens, and differently, In-Context Learning (ICL) adapts the model to a new task by simply including examples in the input without any training. When applying optimization-based methods, such as fine-tuning and PT for few-shot learning, the model is specifically adapted to the small set of training examples, whereas ICL leaves the model unchanged. This distinction makes traditional learning methods more prone to overfitting; in contrast, ICL is less sensitive to the few-shot scenario. While ICL is not prone to overfitting, it does not fully extract the information that exists in the training examples. This work introduces Context-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and adversarial attacks. We build on the ICL strategy of concatenating examples before the input, but we extend this by PT-like learning, refining the context embedding through iterative optimization to extract deeper insights from the training examples. We carefully modify specific context tokens, considering the unique structure of input and output formats. Inspired by adversarial attacks, we adjust the input based on the labels present in the context, focusing on minimizing, rather than maximizing, the loss. Moreover, we apply a projected gradient descent algorithm to keep token embeddings close to their original values, under the assumption that the user-provided data is inherently valuable. Our method has been shown to achieve superior accuracy across multiple classification tasks using various LLM models.

**Link**: [arxiv](http://arxiv.org/abs/2410.17222v1),  [pdf](http://arxiv.org/pdf/2410.17222v1)

**Tags**: cs.CL 



### MiniPLM: Knowledge Distillation for Pre-Training Language Models
**Authors**: Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, Minlie Huang

**Updated**: 2024-10-22T17:40:32Z

**Summary**: Knowledge distillation (KD) is widely used to train small, high-performing student language models (LMs) using large teacher LMs. While effective in fine-tuning, KD during pre-training faces challenges in efficiency, flexibility, and effectiveness. Existing methods either incur high computational costs due to online teacher inference, require tokenization matching between teacher and student LMs, or risk losing the difficulty and diversity of the teacher-generated training data. To address these issues, we propose MiniPLM, a KD framework for pre-training LMs by refining the training data distribution with the teacher's knowledge. For efficiency, MiniPLM performs offline teacher LM inference, allowing KD for multiple student LMs without adding training-time costs. For flexibility, MiniPLM operates solely on the training corpus, enabling KD across model families. For effectiveness, MiniPLM leverages the differences between large and small LMs to enhance the difficulty and diversity of the training data, helping student LMs acquire versatile and sophisticated knowledge. Extensive experiments demonstrate that MiniPLM boosts the student LMs' performance on 9 widely used downstream tasks, improves the language modeling capabilities, and reduces pre-training computation. The benefit of MiniPLM extends to large pre-training scales, evidenced by the extrapolation of the scaling curves. Further analysis reveals that MiniPLM supports KD across model families and enhances the utilization of pre-training data. Our model, code, and data are available at https://github.com/thu-coai/MiniPLM.

**Link**: [arxiv](http://arxiv.org/abs/2410.17215v1),  [pdf](http://arxiv.org/pdf/2410.17215v1)

**Tags**: cs.CL 



### Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh   through Large Language Modeling
**Authors**: Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam, Mahathir Mohammad Bappy

**Updated**: 2024-10-22T17:34:59Z

**Summary**: Purpose: Bangladesh's legal system struggles with major challenges like delays, complexity, high costs, and millions of unresolved cases, which deter many from pursuing legal action due to lack of knowledge or financial constraints. This research seeks to develop a specialized Large Language Model (LLM) to assist in the Bangladeshi legal system. Methods: We created UKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and scraping data on various legal acts. We fine-tuned the GPT-2 model on this dataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance in English. Results: The model was rigorously evaluated using semantic assessments, including case studies supported by expert opinions. The evaluation provided promising results, demonstrating the potential for the model to assist in legal matters within Bangladesh. Conclusion: Our work represents the first structured effort toward building an AI-based legal assistant for Bangladesh. While the results are encouraging, further refinements are necessary to improve the model's accuracy, credibility, and safety. This is a significant step toward creating a legal AI capable of serving the needs of a population of 180 million.

**Link**: [arxiv](http://arxiv.org/abs/2410.17210v1),  [pdf](http://arxiv.org/pdf/2410.17210v1)

**Tags**: cs.CL cs.AI cs.CY 



### ACPBench: Reasoning about Action, Change, and Planning
**Authors**: Harsha Kokel, Michael Katz, Kavitha Srinivas, Shirin Sohrabi

**Updated**: 2024-10-22T17:16:17Z

**Summary**: There is an increasing body of work using Large Language Models (LLMs) as agents for orchestrating workflows and making decisions in domains that require planning and multi-step reasoning. As a result, it is imperative to evaluate LLMs on core skills required for planning. In this work, we present ACPBench, a benchmark for evaluating the reasoning tasks in the field of planning. The benchmark consists of 7 reasoning tasks over 13 planning domains. The collection is constructed from planning domains described in a formal language. This allows us to synthesize problems with provably correct solutions across many tasks and domains. Further, it allows us the luxury of scale without additional human effort, i.e., many additional problems can be created automatically. Our extensive evaluation of 22 LLMs and OpenAI o1 reasoning models highlights the significant gap in the reasoning capability of the LLMs. Our findings with OpenAI o1, a multi-turn reasoning model, reveal significant gains in performance on multiple-choice questions, yet surprisingly, no notable progress is made on boolean questions.   The ACPBench collection is available at https://ibm.github.io/ACPBench.

**Link**: [arxiv](http://arxiv.org/abs/2410.05669v2),  [pdf](http://arxiv.org/pdf/2410.05669v2)

**Tags**: cs.AI 



### VoiceBench: Benchmarking LLM-Based Voice Assistants
**Authors**: Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li

**Updated**: 2024-10-22T17:15:20Z

**Summary**: Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.

**Link**: [arxiv](http://arxiv.org/abs/2410.17196v1),  [pdf](http://arxiv.org/pdf/2410.17196v1)

**Tags**: cs.CL cs.AI cs.SD eess.AS 



### Non-myopic Generation of Language Model for Reasoning and Planning
**Authors**: Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong

**Updated**: 2024-10-23T07:02:09Z

**Summary**: Large Language Models have demonstrated remarkable abilities in reasoning and planning by breaking down complex problems into sequential steps. Despite their success in various domains like mathematical problem-solving and coding, LLMs face challenges in ensuring reliable and optimal planning due to their inherent myopic nature of autoregressive decoding. This paper revisits LLM reasoning from an optimal-control perspective, proposing a novel method, Predictive-Decoding, that leverages Model Predictive Control to enhance planning accuracy. By re-weighting LLM distributions based on foresight trajectories, Predictive-Decoding aims to mitigate early errors and promote non-myopic planning. Our experiments show significant improvements in a wide range of tasks for math, coding, and agents. Furthermore, Predictive-Decoding demonstrates computational efficiency, outperforming search baselines with reduced computational resources. This study provides insights into optimizing LLM planning capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2410.17195v2),  [pdf](http://arxiv.org/pdf/2410.17195v2)

**Tags**: cs.AI cs.CL 



### Representation Shattering in Transformers: A Synthetic Study with   Knowledge Editing
**Authors**: Kento Nishi, Maya Okawa, Rahul Ramesh, Mikail Khona, Ekdeep Singh Lubana, Hidenori Tanaka

**Updated**: 2024-10-22T17:13:34Z

**Summary**: Knowledge Editing (KE) algorithms alter models' internal weights to perform targeted updates to incorrect, outdated, or otherwise unwanted factual associations. In order to better define the possibilities and limitations of these approaches, recent work has shown that applying KE can adversely affect models' factual recall accuracy and diminish their general reasoning abilities. While these studies give broad insights into the potential harms of KE algorithms, e.g., via performance evaluations on benchmarks, we argue little is understood as to why such destructive failures occur. Is it possible KE methods distort representations of concepts beyond the targeted fact, hence hampering abilities at broad? If so, what is the extent of this distortion? To take a step towards addressing such questions, we define a novel synthetic task wherein a Transformer is trained from scratch to internalize a ``structured'' knowledge graph. The structure enforces relationships between entities of the graph, such that editing a factual association has "trickling effects" on other entities in the graph (e.g., altering X's parent is Y to Z affects who X's siblings' parent is). Through evaluations of edited models and analysis of extracted representations, we show that KE inadvertently affects representations of entities beyond the targeted one, distorting relevant structures that allow a model to infer unseen knowledge about an entity. We call this phenomenon representation shattering and demonstrate that it results in degradation of factual recall and reasoning performance more broadly. To corroborate our findings in a more naturalistic setup, we perform preliminary experiments with a pretrained GPT-2-XL model and reproduce the representation shattering effect therein as well. Overall, our work yields a precise mechanistic hypothesis to explain why KE has adverse effects on model capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2410.17194v1),  [pdf](http://arxiv.org/pdf/2410.17194v1)

**Tags**: cs.LG 



### Can Large Language Models Identify Authorship?
**Authors**: Baixiang Huang, Canyu Chen, Kai Shu

**Updated**: 2024-10-22T17:07:14Z

**Summary**: The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated an exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis remains under-explored. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing explanations into their decision making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis.

**Link**: [arxiv](http://arxiv.org/abs/2403.08213v2),  [pdf](http://arxiv.org/pdf/2403.08213v2)

**Tags**: cs.CL 



### The Impact of Large Language Models in Academia: from Writing to   Speaking
**Authors**: Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou

**Updated**: 2024-10-22T17:06:17Z

**Summary**: Large language models (LLMs) are increasingly impacting human society, particularly in textual information. Based on more than 30,000 papers and 1,000 presentations from machine learning conferences, we examined and compared the words used in writing and speaking, representing the first large-scale study of how LLMs influence the two main modes of verbal communication and expression within the same group of people. Our empirical results show that LLM-style words such as "significant" have been used more frequently in abstracts and oral presentations. The impact on speaking is beginning to emerge and is likely to grow in the future, calling attention to the implicit influence and ripple effect of LLMs on human society.

**Link**: [arxiv](http://arxiv.org/abs/2409.13686v2),  [pdf](http://arxiv.org/pdf/2409.13686v2)

**Tags**: cs.CL cs.AI cs.CY cs.DL cs.LG 



### Levels of AI Agents: from Rules to Large Language Models
**Authors**: Yu Huang

**Updated**: 2024-10-22T17:05:17Z

**Summary**: AI agents are defined as artificial entities to perceive the environment, make decisions and take actions. Inspired by the 6 levels of autonomous driving by Society of Automotive Engineers, the AI agents are also categorized based on utilities and strongness, as the following levels: L0, no AI, with tools taking into account perception plus actions; L1, using rule-based AI; L2, making rule-based AI replaced by IL/RL-based AI, with additional reasoning & decision making; L3, applying LLM-based AI instead of IL/RL-based AI, additionally setting up memory & reflection; L4, based on L3, facilitating autonomous learning & generalization; L5, based on L4, appending personality of emotion and character and collaborative behavior with multi-agents.

**Link**: [arxiv](http://arxiv.org/abs/2405.06643v2),  [pdf](http://arxiv.org/pdf/2405.06643v2)

**Tags**: cs.CL 



### LLMs left, right, and center: Assessing GPT's capabilities to label   political bias from web domains
**Authors**: Raphael Hernandes, Giulio Corsi

**Updated**: 2024-10-22T16:59:12Z

**Summary**: This research investigates whether OpenAI's GPT-4, a state-of-the-art large language model, can accurately classify the political bias of news sources based solely on their URLs. Given the subjective nature of political labels, third-party bias ratings like those from Ad Fontes Media, AllSides, and Media Bias/Fact Check (MBFC) are often used in research to analyze news source diversity. This study aims to determine if GPT-4 can replicate these human ratings on a seven-degree scale ("far-left" to "far-right"). The analysis compares GPT-4's classifications against MBFC's, and controls for website popularity using Open PageRank scores. Findings reveal a high correlation ($\text{Spearman's } \rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and MBFC's ratings, indicating the model's potential reliability. However, GPT-4 abstained from classifying approximately $\frac{2}{3}$ of the dataset. It is more likely to abstain from rating unpopular websites, which also suffer from less accurate assessments. The LLM tends to avoid classifying sources that MBFC considers to be centrist, resulting in more polarized outputs. Finally, this analysis shows a slight leftward skew in GPT's classifications compared to MBFC's. Therefore, while this paper suggests that while GPT-4 can be a scalable, cost-effective tool for political bias classification of news websites, its use should be as a complement to human judgment to mitigate biases.

**Link**: [arxiv](http://arxiv.org/abs/2407.14344v2),  [pdf](http://arxiv.org/pdf/2407.14344v2)

**Tags**: cs.CL cs.AI cs.CY 



### Prototype Stochastic Gravitational Wave Background Recovery in the LISA   Global Fit Residual
**Authors**: Robert Rosati, Tyson B. Littenberg

**Updated**: 2024-10-22T16:57:31Z

**Summary**: The Laser Interferometer Space Antenna (LISA) mission poses a difficult parameter estimation challenge: the sources will be so dense in both time and frequency that they all must be fit simultaneously in a `global fit'. Successful tests of global fit efforts on synthetic datasets have been recently reported, recovering extra-galactic black hole mergers and galactic binaries, including the $\mathtt{GLASS}$ pipeline in arXiv:2301.03673. Injected stochastic sources, however, have so far been absent in these datasets. In this work we report our development of a stochastic search pipeline ready for inclusion in future tests of the global fit, capable of detecting or placing limits on a wide variety of possible cosmologically- and astrophysically-inspired SGWBs. The code uses short-time Fourier transforms (STFTs) to allow for inference despite the non-stationarity of the noise. We quote results using both purely synthetic confusion noise and two $\mathtt{GLASS}$ residuals, and quantify the impact of the residuals' non-gaussianity on injected signal recovery and on setting upper limits. We find that, if not properly mitigated, non-gaussianities can preclude setting accurate SGWB upper limits and lead to false detections. We also stress that the narrow-band non-gaussianities we find do not affect all sources equally, and many narrower-band, cosmologically-inspired SGWBs are more sensitive to non-gaussianity than others.

**Link**: [arxiv](http://arxiv.org/abs/2410.17180v1),  [pdf](http://arxiv.org/pdf/2410.17180v1)

**Tags**: gr-qc 



### Remote Timing Attacks on Efficient Language Model Inference
**Authors**: Nicholas Carlini, Milad Nasr

**Updated**: 2024-10-22T16:51:36Z

**Summary**: Scaling up language models has significantly increased their capabilities. But larger models are slower models, and so there is now an extensive body of work (e.g., speculative sampling or parallel decoding) that improves the (average case) efficiency of language model generation. But these techniques introduce data-dependent timing characteristics. We show it is possible to exploit these timing differences to mount a timing attack. By monitoring the (encrypted) network traffic between a victim user and a remote language model, we can learn information about the content of messages by noting when responses are faster or slower. With complete black-box access, on open source systems we show how it is possible to learn the topic of a user's conversation (e.g., medical advice vs. coding assistance) with 90%+ precision, and on production systems like OpenAI's ChatGPT and Anthropic's Claude we can distinguish between specific messages or infer the user's language. We further show that an active adversary can leverage a boosting attack to recover PII placed in messages (e.g., phone numbers or credit card numbers) for open source systems. We conclude with potential defenses and directions for future work.

**Link**: [arxiv](http://arxiv.org/abs/2410.17175v1),  [pdf](http://arxiv.org/pdf/2410.17175v1)

**Tags**: cs.CR cs.LG 



### Self-calibration for Language Model Quantization and Pruning
**Authors**: Miles Williams, George Chrysostomou, Nikolaos Aletras

**Updated**: 2024-10-22T16:50:00Z

**Summary**: Quantization and pruning are fundamental approaches for model compression, enabling efficient inference for language models. In a post-training setting, state-of-the-art quantization and pruning methods require calibration data, a small set of unlabeled examples. Conventionally, randomly sampled web text is used, aiming to reflect the model training data. However, this poses two key problems: (1) unrepresentative calibration examples can harm model performance, and (2) organizations increasingly avoid releasing model training data. In this paper, we propose self-calibration as a solution. Our approach requires no external data, instead leveraging the model itself to generate synthetic calibration data as a better approximation of the pre-training data distribution. We extensively compare the performance of self-calibration with several baselines, across a variety of models, compression methods, and tasks. Our approach proves consistently competitive in maximizing downstream task performance, frequently outperforming even using real data.

**Link**: [arxiv](http://arxiv.org/abs/2410.17170v1),  [pdf](http://arxiv.org/pdf/2410.17170v1)

**Tags**: cs.CL 



### Incorporating waveform calibration error in gravitational-wave modeling   and inference for SEOBNRv4
**Authors**: Ritesh Bachhar, Michael Prrer, Stephen R. Green

**Updated**: 2024-10-22T16:49:40Z

**Summary**: As gravitational wave (GW) detector networks continue to improve in sensitivity, the demand on the accuracy of waveform models which predict the GW signals from compact binary coalescences is becoming more stringent. At high signal-to-noise ratios (SNRs) discrepancies between waveform models and the true solutions of Einstein's equations can introduce significant systematic biases in parameter estimation (PE). These biases affect the inferred astrophysical properties, including matter effects, and can also lead to erroneous claims of deviations from general relativity, impacting the interpretation of astrophysical populations and cosmological parameters. While efforts to address these biases have focused on developing more precise models, we explore an alternative strategy to account for uncertainties in waveform models, particularly from calibrating an effective-one-body (EOB) model against numerical relativity (NR) data. We introduce an efficient method for modeling and marginalizing over waveform uncertainty in the SEOBNRv4 model, which captures the dominant $(2,2)$ mode for non-precessing quasi-circular binary black holes (BBHs). Our approach uses Gaussian process regression (GPR) to model amplitude and phase deviations in the Fourier domain. This method mitigates systematic biases in PE and increases posterior variance by incorporating a broader distribution of waveforms, consistent with previous findings. This study emphasizes the importance of incorporating waveform uncertainties in GW data analysis and presents a novel, practical framework to include these uncertainties in Bayesian PE for EOB models, with broad applicability.

**Link**: [arxiv](http://arxiv.org/abs/2410.17168v1),  [pdf](http://arxiv.org/pdf/2410.17168v1)

**Tags**: gr-qc 



### Towards Map-Agnostic Policies for Adaptive Informative Path Planning
**Authors**: Julius Rckin, David Morilla-Cabello, Cyrill Stachniss, Eduardo Montijano, Marija Popovi

**Updated**: 2024-10-22T16:43:21Z

**Summary**: Robots are frequently tasked to gather relevant sensor data in unknown terrains. A key challenge for classical path planning algorithms used for autonomous information gathering is adaptively replanning paths online as the terrain is explored given limited onboard compute resources. Recently, learning-based approaches emerged that train planning policies offline and enable computationally efficient online replanning performing policy inference. These approaches are designed and trained for terrain monitoring missions assuming a single specific map representation, which limits their applicability to different terrains. To address these issues, we propose a novel formulation of the adaptive informative path planning problem unified across different map representations, enabling training and deploying planning policies in a larger variety of monitoring missions. Experimental results validate that our novel formulation easily integrates with classical non-learning-based planning approaches while maintaining their performance. Our trained planning policy performs similarly to state-of-the-art map-specifically trained policies. We validate our learned policy on unseen real-world terrain datasets.

**Link**: [arxiv](http://arxiv.org/abs/2410.17166v1),  [pdf](http://arxiv.org/pdf/2410.17166v1)

**Tags**: cs.RO 



### A Bayesian Perspective on the Maximum Score Problem
**Authors**: Christopher D. Walker

**Updated**: 2024-10-22T16:29:36Z

**Summary**: This paper presents a Bayesian inference framework for a linear index threshold-crossing binary choice model that satisfies a median independence restriction. The key idea is that the model is observationally equivalent to a probit model with nonparametric heteroskedasticity. Consequently, Gibbs sampling techniques from Albert and Chib (1993) and Chib and Greenberg (2013) lead to a computationally attractive Bayesian inference procedure in which a Gaussian process forms a conditionally conjugate prior for the natural logarithm of the skedastic function.

**Link**: [arxiv](http://arxiv.org/abs/2410.17153v1),  [pdf](http://arxiv.org/pdf/2410.17153v1)

**Tags**: econ.EM 



### Improving Pinterest Search Relevance Using Large Language Models
**Authors**: Han Wang, Mukuntha Narayanan Sundararaman, Onur Gungor, Yu Xu, Krishna Kamath, Rakesh Chalasani, Kurchi Subhra Hazra, Jinfeng Rao

**Updated**: 2024-10-22T16:29:33Z

**Summary**: To improve relevance scoring on Pinterest Search, we integrate Large Language Models (LLMs) into our search relevance model, leveraging carefully designed text representations to predict the relevance of Pins effectively. Our approach uses search queries alongside content representations that include captions extracted from a generative visual language model. These are further enriched with link-based text data, historically high-quality engaged queries, user-curated boards, Pin titles and Pin descriptions, creating robust models for predicting search relevance. We use a semi-supervised learning approach to efficiently scale up the amount of training data, expanding beyond the expensive human labeled data available. By utilizing multilingual LLMs, our system extends training data to include unseen languages and domains, despite initial data and annotator expertise being confined to English. Furthermore, we distill from the LLM-based model into real-time servable model architectures and features. We provide comprehensive offline experimental validation for our proposed techniques and demonstrate the gains achieved through the final deployed system at scale.

**Link**: [arxiv](http://arxiv.org/abs/2410.17152v1),  [pdf](http://arxiv.org/pdf/2410.17152v1)

**Tags**: cs.IR cs.CL 



### Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and   Tool Knowledge Bases
**Authors**: Elias Lumer, Vamse Kumar Subbiah, James A. Burke, Pradeep Honaganahalli Basavaraju, Austin Huber

**Updated**: 2024-10-22T16:27:12Z

**Summary**: Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks like secure database interactions and multi-agent code development. However, scaling tool capacity beyond agent reasoning or model limits remains a challenge. In this paper, we address these challenges by introducing Toolshed Knowledge Bases, a tool knowledge base (vector database) designed to store enhanced tool representations and optimize tool selection for large-scale tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a novel ensemble of tool-applied advanced retrieval-augmented generation (RAG) techniques across the pre-retrieval, intra-retrieval, and post-retrieval phases, without requiring model fine-tuning. During pre-retrieval, tool documents are enhanced with key information and stored in the Toolshed Knowledge Base. Intra-retrieval focuses on query planning and transformation to increase retrieval accuracy. Post-retrieval refines the retrieved tool documents and enables self-reflection. Furthermore, by varying both the total number of tools (tool-M) an Agent has access to and the tool selection threshold (top-k), we address trade-offs between retrieval accuracy, agent performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools benchmark datasets, respectively (Recall@5).

**Link**: [arxiv](http://arxiv.org/abs/2410.14594v2),  [pdf](http://arxiv.org/pdf/2410.14594v2)

**Tags**: cs.CL 



### LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances   Model Merging
**Authors**: Ke Wang, Nikolaos Dimitriadis, Alessandro Favero, Guillermo Ortiz-Jimenez, Francois Fleuret, Pascal Frossard

**Updated**: 2024-10-22T16:26:05Z

**Summary**: Large pre-trained models exhibit impressive zero-shot performance across diverse tasks, but fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks. To address this challenge, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. We further extend this approach to multi-task model merging scenarios, where layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Importantly, our method is simple to implement and complementary to many existing techniques.

**Link**: [arxiv](http://arxiv.org/abs/2410.17146v1),  [pdf](http://arxiv.org/pdf/2410.17146v1)

**Tags**: cs.LG cs.CV 



### Can General-Purpose Large Language Models Generalize to English-Thai   Machine Translation ?
**Authors**: Jirat Chiaranaipanich, Naiyarat Hanmatheekuna, Jitkapat Sawatphol, Krittamate Tiankanon, Jiramet Kinchagawat, Amrest Chinkamol, Parinthapat Pengpun, Piyalitt Ittichaiwong, Peerat Limkonchotiwat

**Updated**: 2024-10-22T16:26:03Z

**Summary**: Large language models (LLMs) perform well on common tasks but struggle with generalization in low-resource and low-computation settings. We examine this limitation by testing various LLMs and specialized translation models on English-Thai machine translation and code-switching datasets. Our findings reveal that under more strict computational constraints, such as 4-bit quantization, LLMs fail to translate effectively. In contrast, specialized models, with comparable or lower computational requirements, consistently outperform LLMs. This underscores the importance of specialized models for maintaining performance under resource constraints.

**Link**: [arxiv](http://arxiv.org/abs/2410.17145v1),  [pdf](http://arxiv.org/pdf/2410.17145v1)

**Tags**: cs.CL cs.AI cs.LG 



### Towards Automated Penetration Testing: Introducing LLM Benchmark,   Analysis, and Improvements
**Authors**: Isamu Isozaki, Manil Shrestha, Rick Console, Edward Kim

**Updated**: 2024-10-22T16:18:41Z

**Summary**: Hacking poses a significant threat to cybersecurity, inflicting billions of dollars in damages annually. To mitigate these risks, ethical hacking, or penetration testing, is employed to identify vulnerabilities in systems and networks. Recent advancements in large language models (LLMs) have shown potential across various domains, including cybersecurity. However, there is currently no comprehensive, open, end-to-end automated penetration testing benchmark to drive progress and evaluate the capabilities of these models in security contexts. This paper introduces a novel open benchmark for LLM-based automated penetration testing, addressing this critical gap. We first evaluate the performance of LLMs, including GPT-4o and Llama 3.1-405B, using the state-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1 demonstrates an edge over GPT-4o, both models currently fall short of performing fully automated, end-to-end penetration testing. Next, we advance the state-of-the-art and present ablation studies that provide insights into improving the PentestGPT tool. Our research illuminates the challenges LLMs face in each aspect of Pentesting, e.g. enumeration, exploitation, and privilege escalation. This work contributes to the growing body of knowledge on AI-assisted cybersecurity and lays the foundation for future research in automated penetration testing using large language models.

**Link**: [arxiv](http://arxiv.org/abs/2410.17141v1),  [pdf](http://arxiv.org/pdf/2410.17141v1)

**Tags**: cs.CR cs.AI 



### Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary   Resolution
**Authors**: Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao

**Updated**: 2024-10-22T16:17:13Z

**Summary**: Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These design features enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.

**Link**: [arxiv](http://arxiv.org/abs/2409.12961v2),  [pdf](http://arxiv.org/pdf/2409.12961v2)

**Tags**: cs.CV 



### PAPILLON: PrivAcy Preservation from Internet-based and Local Language   MOdel ENsembles
**Authors**: Li Siyan, Vethavikashini Chithrra Raghuram, Omar Khattab, Julia Hirschberg, Zhou Yu

**Updated**: 2024-10-22T16:00:26Z

**Summary**: Users can divulge sensitive information to proprietary LLM providers, raising significant privacy concerns. While open-source models, hosted locally on the user's machine, alleviate some concerns, models that users can host locally are often less capable than proprietary frontier models. Toward preserving user privacy while retaining the best quality, we propose Privacy-Conscious Delegation, a novel task for chaining API-based and local models. We utilize recent public collections of user-LLM interactions to construct a natural benchmark called PUPA, which contains personally identifiable information (PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM pipeline that uses prompt optimization to address a simpler version of our task. Our best pipeline maintains high response quality for 85.5% of user queries while restricting privacy leakage to only 7.5%. We still leave a large margin to the generation quality of proprietary LLMs for future work. Our data and code will be available at https://github.com/siyan-sylvia-li/PAPILLON.

**Link**: [arxiv](http://arxiv.org/abs/2410.17127v1),  [pdf](http://arxiv.org/pdf/2410.17127v1)

**Tags**: cs.CR cs.CL 



### Exploring RL-based LLM Training for Formal Language Tasks with   Programmed Rewards
**Authors**: Alexander G. Padula, Dennis J. N. J. Soemers

**Updated**: 2024-10-22T15:59:58Z

**Summary**: Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning from Human Feedback to align large language models (LLMs) with downstream tasks. This paper investigates the feasibility of using PPO for direct reinforcement learning (RL) from explicitly programmed reward signals, as opposed to indirect learning from human feedback via an intermediary reward model. We focus on tasks expressed through formal languages, such as mathematics and programming, where explicit reward functions can be programmed to automatically assess the quality of generated outputs. We apply this approach to a sentiment alignment task, a simple arithmetic task, and a more complex game synthesis task. The sentiment alignment task replicates prior research and serves to validate our experimental setup. Our results show that pure RL-based training for the two formal language tasks is challenging, with success being limited even for the simple arithmetic task. We propose a novel batch-entropy regularization term to aid exploration, although training is not yet entirely stable. Our findings suggest that direct RL training of LLMs may be more suitable for relatively minor changes, such as alignment, than for learning new tasks altogether, even if an informative reward signal can be expressed programmatically.

**Link**: [arxiv](http://arxiv.org/abs/2410.17126v1),  [pdf](http://arxiv.org/pdf/2410.17126v1)

**Tags**: cs.CL cs.AI cs.LG 



### Learning Load Balancing with GNN in MPTCP-Enabled Heterogeneous Networks
**Authors**: Han Ji, Xiping Wu, Zhihong Zeng, Chen Chen

**Updated**: 2024-10-22T15:49:53Z

**Summary**: Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks are a promising paradigm of heterogeneous network (HetNet), attributed to the complementary physical properties of optical spectra and radio frequency. However, the current development of such HetNets is mostly bottlenecked by the existing transmission control protocol (TCP), which restricts the user equipment (UE) to connecting one access point (AP) at a time. While the ongoing investigation on multipath TCP (MPTCP) can bring significant benefits, it complicates the network topology of HetNets, making the existing load balancing (LB) learning models less effective. Driven by this, we propose a graph neural network (GNN)-based model to tackle the LB problem for MPTCP-enabled HetNets, which results in a partial mesh topology. Such a topology can be modeled as a graph, with the channel state information and data rate requirement embedded as node features, while the LB solutions are deemed as edge labels. Compared to the conventional deep neural network (DNN), the proposed GNN-based model exhibits two key strengths: i) it can better interpret a complex network topology; and ii) it can handle various numbers of APs and UEs with a single trained model. Simulation results show that against the traditional optimisation method, the proposed learning model can achieve near-optimal throughput within a gap of 11.5%, while reducing the inference time by 4 orders of magnitude. In contrast to the DNN model, the new method can improve the network throughput by up to 21.7%, at a similar inference time level.

**Link**: [arxiv](http://arxiv.org/abs/2410.17118v1),  [pdf](http://arxiv.org/pdf/2410.17118v1)

**Tags**: cs.LG cs.SY eess.SY 



### DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency   Management
**Authors**: Mohannad Alhanahnah, Yazan Boshmaf

**Updated**: 2024-10-22T15:41:22Z

**Summary**: In the era of Large Language Models (LLMs) with their advanced capabilities, a unique opportunity arises to develop LLM-based digital assistant tools that can support software developers by facilitating comprehensive reasoning about software dependencies and open-source libraries before importing them. This reasoning process is daunting, mandating multiple specialized tools and dedicated expertise, each focusing on distinct aspects (e.g., security analysis tools may overlook design flaws such as circular dependencies, which hinder software maintainability). Creating a significant bottleneck in the software development lifecycle. In this paper, we introduce DepsRAG, a multi-agent framework designed to assist developers in reasoning about software dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG) that includes both direct and transitive dependencies. Developers can interact with DepsRAG through a conversational interface, posing queries about the dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance these queries by retrieving relevant information from the KG as well as external sources, such as the Web and vulnerability databases, thus demonstrating its adaptability to novel scenarios. DepsRAG incorporates a Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three multi-step reasoning tasks, observing a threefold increase in accuracy with the integration of the Critic-Agent mechanism. DepsRAG demo and implementation are available: https://github.com/Mohannadcse/DepsRAG.

**Link**: [arxiv](http://arxiv.org/abs/2405.20455v5),  [pdf](http://arxiv.org/pdf/2405.20455v5)

**Tags**: cs.SE 



### Enhancing Answer Attribution for Faithful Text Generation with Large   Language Models
**Authors**: Juraj Vladika, Luca Mlln, Florian Matthes

**Updated**: 2024-10-22T15:37:46Z

**Summary**: The increasing popularity of Large Language Models (LLMs) in recent years has changed the way users interact with and pose questions to AI-based conversational systems. An essential aspect for increasing the trustworthiness of generated LLM answers is the ability to trace the individual claims from responses back to relevant sources that support them, the process known as answer attribution. While recent work has started exploring the task of answer attribution in LLMs, some challenges still remain. In this work, we first perform a case study analyzing the effectiveness of existing answer attribution methods, with a focus on subtasks of answer segmentation and evidence retrieval. Based on the observed shortcomings, we propose new methods for producing more independent and contextualized claims for better retrieval and attribution. The new methods are evaluated and shown to improve the performance of answer attribution components. We end with a discussion and outline of future directions for the task.

**Link**: [arxiv](http://arxiv.org/abs/2410.17112v1),  [pdf](http://arxiv.org/pdf/2410.17112v1)

**Tags**: cs.CL cs.IR 



### Credal Bayesian Deep Learning
**Authors**: Michele Caprio, Souradeep Dutta, Kuk Jin Jang, Vivian Lin, Radoslav Ivanov, Oleg Sokolsky, Insup Lee

**Updated**: 2024-10-22T15:36:34Z

**Summary**: Uncertainty quantification and robustness to distribution shifts are important goals in machine learning and artificial intelligence. Although Bayesian Neural Networks (BNNs) allow for uncertainty in the predictions to be assessed, different sources of predictive uncertainty cannot be distinguished properly. We present Credal Bayesian Deep Learning (CBDL). Heuristically, CBDL allows to train an (uncountably) infinite ensemble of BNNs, using only finitely many elements. This is possible thanks to prior and likelihood finitely generated credal sets (FGCSs), a concept from the imprecise probability literature. Intuitively, convex combinations of a finite collection of prior-likelihood pairs are able to represent infinitely many such pairs. After training, CBDL outputs a set of posteriors on the parameters of the neural network. At inference time, such posterior set is used to derive a set of predictive distributions that is in turn utilized to distinguish between (predictive) aleatoric and epistemic uncertainties, and to quantify them. The predictive set also produces either (i) a collection of outputs enjoying desirable probabilistic guarantees, or (ii) the single output that is deemed the best, that is, the one having the highest predictive lower probability -- another imprecise-probabilistic concept. CBDL is more robust than single BNNs to prior and likelihood misspecification, and to distribution shift. We show that CBDL is better at quantifying and disentangling different types of (predictive) uncertainties than single BNNs and ensemble of BNNs. In addition, we apply CBDL to two case studies to demonstrate its downstream tasks capabilities: one, for motion prediction in autonomous driving scenarios, and two, to model blood glucose and insulin dynamics for artificial pancreas control. We show that CBDL performs better when compared to an ensemble of BNNs baseline.

**Link**: [arxiv](http://arxiv.org/abs/2302.09656v5),  [pdf](http://arxiv.org/pdf/2302.09656v5)

**Tags**: cs.LG stat.ML Primary: 68T37, Secondary: 68T05, 68W25 



### The FLAMINGO project: Baryon effects on the matter power spectrum
**Authors**: Matthieu Schaller, Joop Schaye, Roi Kugel, Jeger C. Broxterman, Marcel P. van Daalen

**Updated**: 2024-10-22T15:33:43Z

**Summary**: The effect of baryon physics associated with galaxy formation onto the large-scale matter distribution of the Universe is a key uncertainty in the theoretical modelling required for the interpretation of Stage IV cosmology surveys. We use the FLAMINGO suite of simulations to study the baryon response due to galaxy formation of the total matter power spectrum. We find that it is only well converged for simulation volumes in excess of $(200~Mpc)^3$. We report results for simulations of varying feedback intensity, which either match the X-ray inferred gas fractions in clusters and the z=0 stellar mass function, or shifted versions of the data, as well as for different implementations of AGN feedback. We package our results in the form of a Gaussian process emulator which can rapidly reproduce all the simulations' predictions to better than 1% up to the comoving wavenumber $k = 10~h/Mpc$ and up to z=2 for all the feedback models present in the FLAMINGO suite. We find that the response becomes stronger, the range of scales affected increases, and the position of the minimum of the response moves to smaller scales as the redshift decreases. We find that lower gas fractions in groups and clusters lead to a stronger response and that the use of collimated jets instead of thermally driven winds for AGN feedback enhances the effect. Lowering the stellar masses at fixed cluster gas fractions also increases the magnitude of the response. We find only a small (1% at $k<10~h/Mpc$) dependence of our results on the background cosmology, but a wider range of cosmology variations will be needed to confirm this result. The response we obtain for our strongest feedback models is compatible with some of the recent analyses combining weak lensing with external data. Such a response is, however, in strong tension with the X-ray inferred gas fractions in clusters used to calibrate the FLAMINGO model.

**Link**: [arxiv](http://arxiv.org/abs/2410.17109v1),  [pdf](http://arxiv.org/pdf/2410.17109v1)

**Tags**: astro-ph.CO 



### General Seemingly Unrelated Local Projections
**Authors**: Florian Huber, Christian Matthes, Michael Pfarrhofer

**Updated**: 2024-10-22T15:30:01Z

**Summary**: We provide a framework for efficiently estimating impulse response functions with Local Projections (LPs). Our approach offers a Bayesian treatment for LPs with Instrumental Variables, accommodating multiple shocks and instruments per shock, accounts for autocorrelation in multi-step forecasts by jointly modeling all LPs as a seemingly unrelated system of equations, defines a flexible yet parsimonious joint prior for impulse responses based on a Gaussian Process, allows for joint inference about the entire vector of impulse responses, and uses all available data across horizons by imputing missing values.

**Link**: [arxiv](http://arxiv.org/abs/2410.17105v1),  [pdf](http://arxiv.org/pdf/2410.17105v1)

**Tags**: econ.EM stat.AP 



### Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations
**Authors**: Jiyi Li

**Updated**: 2024-10-22T15:22:58Z

**Summary**: The quality is a crucial issue for crowd annotations. Answer aggregation is an important type of solution. The aggregated answers estimated from multiple crowd answers to the same instance are the eventually collected annotations, rather than the individual crowd answers themselves. Recently, the capability of Large Language Models (LLMs) on data annotation tasks has attracted interest from researchers. Most of the existing studies mainly focus on the average performance of individual crowd workers; several recent works studied the scenarios of aggregation on categorical labels and LLMs used as label creators. However, the scenario of aggregation on text answers and the role of LLMs as aggregators are not yet well-studied. In this paper, we investigate the capability of LLMs as aggregators in the scenario of close-ended crowd text answer aggregation. We propose a human-LLM hybrid text answer aggregation method with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We make the experiments based on public crowdsourcing datasets. The results show the effectiveness of our approach based on the collaboration of crowd workers and LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17099v1),  [pdf](http://arxiv.org/pdf/2410.17099v1)

**Tags**: cs.CL cs.HC cs.LG 



### Do LLMs "know" internally when they follow instructions?
**Authors**: Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Shirley Ren, Udhay Nallasamy, Andy Miller, Kwan Ho Ryan Chan, Jaya Narain

**Updated**: 2024-10-22T15:20:00Z

**Summary**: Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs' internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs' instruction-following, paving the way for reliable LLM agents.

**Link**: [arxiv](http://arxiv.org/abs/2410.14516v2),  [pdf](http://arxiv.org/pdf/2410.14516v2)

**Tags**: cs.AI cs.CL 



### CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk   Generation for Large Language Model Applications
**Authors**: Chaoran Chen, Daodao Zhou, Yanfang Ye, Toby Jia-jun Li, Yaxing Yao

**Updated**: 2024-10-22T15:17:08Z

**Summary**: The rise of end-user applications powered by large language models (LLMs), including both conversational interfaces and add-ons to existing graphical user interfaces (GUIs), introduces new privacy challenges. However, many users remain unaware of the risks. This paper explores methods to increase user awareness of privacy risks associated with LLMs in end-user applications. We conducted five co-design workshops to uncover user privacy concerns and their demand for contextual privacy information within LLMs. Based on these insights, we developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation), a just-in-time contextual assistant designed to help users identify sensitive information, summarize relevant privacy policies, and highlight potential risks when sharing information with LLMs. We evaluated the usability and usefulness of CLEAR across in two example domains: ChatGPT and the Gemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and improves user understanding of data practices and privacy risks. We also discussed LLM's duality in posing and mitigating privacy risks, offering design and policy implications.

**Link**: [arxiv](http://arxiv.org/abs/2410.13387v2),  [pdf](http://arxiv.org/pdf/2410.13387v2)

**Tags**: cs.HC 



### Do LLMs estimate uncertainty well in instruction-following?
**Authors**: Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain

**Updated**: 2024-10-22T15:16:14Z

**Summary**: Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions. Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.

**Link**: [arxiv](http://arxiv.org/abs/2410.14582v2),  [pdf](http://arxiv.org/pdf/2410.14582v2)

**Tags**: cs.AI cs.CL 



### AppPoet: Large Language Model based Android malware detection via   multi-view prompt engineering
**Authors**: Wenxiang Zhao, Juntao Wu, Zhaoyi Meng

**Updated**: 2024-10-22T15:12:37Z

**Summary**: Due to the vast array of Android applications, their multifarious functions and intricate behavioral semantics, attackers can adopt various tactics to conceal their genuine attack intentions within legitimate functions. However, numerous learning-based methods suffer from a limitation in mining behavioral semantic information, thus impeding the accuracy and efficiency of Android malware detection. Besides, the majority of existing learning-based methods are weakly interpretive and fail to furnish researchers with effective and readable detection reports. Inspired by the success of the Large Language Models (LLMs) in natural language understanding, we propose AppPoet, a LLM-assisted multi-view system for Android malware detection. Firstly, AppPoet employs a static method to comprehensively collect application features and formulate various observation views. Then, using our carefully crafted multi-view prompt templates, it guides the LLM to generate function descriptions and behavioral summaries for each view, enabling deep semantic analysis of the views. Finally, we collaboratively fuse the multi-view information to efficiently and accurately detect malware through a deep neural network (DNN) classifier and then generate the human-readable diagnostic reports. Experimental results demonstrate that our method achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline methods. Furthermore, the case study evaluates the effectiveness of our generated diagnostic reports.

**Link**: [arxiv](http://arxiv.org/abs/2404.18816v3),  [pdf](http://arxiv.org/pdf/2404.18816v3)

**Tags**: cs.CR cs.SE 



### One Thousand and One Pairs: A "novel" challenge for long-context   language models
**Authors**: Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, Mohit Iyyer

**Updated**: 2024-10-22T15:09:58Z

**Summary**: Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.

**Link**: [arxiv](http://arxiv.org/abs/2406.16264v3),  [pdf](http://arxiv.org/pdf/2406.16264v3)

**Tags**: cs.CL cs.AI 



### Carbon and Iron Deficiencies in Quiescent Galaxies at z=1-3 from   JWST-SUSPENSE: Implications for the Formation Histories of Massive Galaxies
**Authors**: Aliza G. Beverage, Martje Slob, Mariska Kriek, Charlie Conroy, Guillermo Barro, Rachel Bezanson, Gabriel Brammer, Chloe M. Cheng, Anna de Graaff, Natascha M. Frster Schreiber, Marijn Franx, Brian Lorenz, Pavel E. Mancera Pia, Danilo Marchesini, Adam Muzzin, Andrew B. Newman, Sedona H. Price, Alice E. Shapley, Mauro Stefanon, Katherine A. Suess, Pieter van Dokkum, David Weinberg, Daniel R. Weisz

**Updated**: 2024-10-22T15:08:44Z

**Summary**: We present the stellar metallicities and multi-element abundances (C, Mg, Si, Ca, Ti, Cr, and Fe) of 15 massive (log $M/M_\odot=10.2-11.2$) quiescent galaxies at z=1-3, derived from ultradeep JWST-SUSPENSE spectra. Compared to quiescent galaxies at z~0, these galaxies exhibit a deficiency of 0.26$\pm0.04$ dex in [C/H], 0.16$\pm0.03$ dex in [Fe/H], and 0.07$\pm0.04$ dex in [Mg/H], implying rapid formation and quenching before significant enrichment from asymptotic giant branch stars and Type Ia supernovae. Additionally, we find that galaxies forming at higher redshift consistently show higher [Mg/Fe] and lower [Fe/H] and [Mg/H], regardless of their observed redshift. The evolution in [Fe/H] and [C/H] is therefore primarily driven by lower-redshift samples naturally including galaxies with longer star-formation timescales. In contrast, the lower [Mg/H] likely reflects earlier-forming galaxies expelling larger gas reservoirs during their quenching phase. Consequently, the mass-metallicity relation, primarily reflecting [Mg/H], is somewhat lower at z=1-3 compared to the lower redshift relation. Finally, we compare our results to standard stellar population modeling approaches employing solar abundance patterns and non-parametric star-formation histories (using Prospector). Our SSP-equivalent ages agree with the mass-weighted ages from Prospector, while the metallicities disagree significantly. Nonetheless, the metallicities better reflect [Fe/H] than total [Z/H]. We also find that star-formation timescales inferred from elemental abundances are significantly shorter than those from Prospector, and we discuss the resulting implications for the early formation of massive galaxies.

**Link**: [arxiv](http://arxiv.org/abs/2407.02556v2),  [pdf](http://arxiv.org/pdf/2407.02556v2)

**Tags**: astro-ph.GA 



### SysBench: Can Large Language Models Follow System Messages?
**Authors**: Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, Bin Cui

**Updated**: 2024-10-22T15:07:35Z

**Summary**: Large Language Models (LLMs) have become instrumental across various applications, with the customization of these models to specific scenarios becoming increasingly critical. System message, a fundamental component of LLMs, is consist of carefully crafted instructions that guide the behavior of model to meet intended goals. Despite the recognized potential of system messages to optimize AI-driven solutions, there is a notable absence of a comprehensive benchmark for evaluating how well LLMs follow system messages. To fill this gap, we introduce SysBench, a benchmark that systematically analyzes system message following ability in terms of three limitations of existing LLMs: constraint violation, instruction misjudgement and multi-turn instability. Specifically, we manually construct evaluation dataset based on six prevalent types of constraints, including 500 tailor-designed system messages and multi-turn user conversations covering various interaction relationships. Additionally, we develop a comprehensive evaluation protocol to measure model performance. Finally, we conduct extensive evaluation across various existing LLMs, measuring their ability to follow specified constraints given in system messages. The results highlight both the strengths and weaknesses of existing models, offering key insights and directions for future research. The open source library SysBench is available at https://github.com/PKU-Baichuan-MLSystemLab/SysBench.

**Link**: [arxiv](http://arxiv.org/abs/2408.10943v2),  [pdf](http://arxiv.org/pdf/2408.10943v2)

**Tags**: cs.CL 



### FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI   Training Clusters
**Authors**: Hasibul Jamil, Abdul Alim, Laurent Schares, Pavlos Maniotis, Liran Schour, Ali Sydney, Abdullah Kayi, Tevfik Kosar, Bengi Karacali

**Updated**: 2024-10-22T14:56:50Z

**Summary**: The increasing complexity of AI workloads, especially distributed Large Language Model (LLM) training, places significant strain on the networking infrastructure of parallel data centers and supercomputing systems. While Equal-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths, hash collisions often lead to imbalanced network resource utilization and performance bottlenecks. This paper presents FlowTracer, a tool designed to analyze network path utilization and evaluate different routing strategies. FlowTracer aids in debugging network inefficiencies by providing detailed visibility into traffic distribution and helping to identify the root causes of performance degradation, such as issues caused by hash collisions. By offering flow-level insights, FlowTracer enables system operators to optimize routing, reduce congestion, and improve the performance of distributed AI workloads. We use a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to demonstrate how FlowTracer can be used to compare the flow imbalances of ECMP routing against a statically configured network. The example showcases a 30% reduction in imbalance, as measured by a new metric we introduce.

**Link**: [arxiv](http://arxiv.org/abs/2410.17078v1),  [pdf](http://arxiv.org/pdf/2410.17078v1)

**Tags**: cs.NI cs.DC 



### Optimal Design for Reward Modeling in RLHF
**Authors**: Antoine Scheid, Etienne Boursier, Alain Durmus, Michael I. Jordan, Pierre Mnard, Eric Moulines, Michal Valko

**Updated**: 2024-10-22T14:36:44Z

**Summary**: Reinforcement Learning from Human Feedback (RLHF) has become a popular approach to align language models (LMs) with human preferences. This method involves collecting a large dataset of human pairwise preferences across various text generations and using it to infer (implicitly or explicitly) a reward model. Numerous methods have been proposed to learn the reward model and align a LM with it. However, the costly process of collecting human preferences has received little attention and could benefit from theoretical insights. This paper addresses this issue and aims to formalize the reward training model in RLHF. We frame the selection of an effective dataset as a simple regret minimization task, using a linear contextual dueling bandit method. Given the potentially large number of arms, this approach is more coherent than the best-arm identification setting. We then propose an offline framework for solving this problem. Under appropriate assumptions - linearity of the reward model in the embedding space, and boundedness of the reward parameter - we derive bounds on the simple regret. Finally, we provide a lower bound that matches our upper bound up to constant and logarithmic terms. To our knowledge, this is the first theoretical contribution in this area to provide an offline approach as well as worst-case guarantees.

**Link**: [arxiv](http://arxiv.org/abs/2410.17055v1),  [pdf](http://arxiv.org/pdf/2410.17055v1)

**Tags**: cs.LG stat.ML 



### UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs
**Authors**: Yash Sinha, Murari Mandal, Mohan Kankanhalli

**Updated**: 2024-10-22T14:30:03Z

**Summary**: The key components of machine learning are data samples for training, model for learning patterns, and loss function for optimizing accuracy. Analogously, unlearning can potentially be achieved through anti-data samples (or anti-samples), unlearning method, and reversed loss function. While prior research has explored unlearning methods and reversed loss functions, the potential of anti-samples remains largely untapped. In this paper, we introduce UnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language models (LLMs). Our contributions are threefold; first, we propose a novel concept of anti-sample-induced unlearning; second, we generate anti-samples by leveraging misleading rationales, which help reverse learned associations and accelerate the unlearning process; and third, we enable fine-grained targeted unlearning, allowing for the selective removal of specific associations without impacting related knowledge - something not achievable by previous works. Results demonstrate that anti-samples offer an efficient, targeted unlearning strategy for LLMs, opening new avenues for privacy-preserving machine learning and model modification.

**Link**: [arxiv](http://arxiv.org/abs/2410.17050v1),  [pdf](http://arxiv.org/pdf/2410.17050v1)

**Tags**: cs.LG cs.AI cs.CL 



### Optimizing Mixture-of-Experts Inference Time Combining Model Deployment   and Communication Scheduling
**Authors**: Jialong Li, Shreyansh Tripathi, Lakshay Rastogi, Yiming Lei, Rui Pan, Yiting Xia

**Updated**: 2024-10-22T14:19:29Z

**Summary**: As machine learning models scale in size and complexity, their computational requirements become a significant barrier. Mixture-of-Experts (MoE) models alleviate this issue by selectively activating relevant experts. Despite this, MoE models are hindered by high communication overhead from all-to-all operations, low GPU utilization due to the synchronous communication constraint, and complications from heterogeneous GPU environments.   This paper presents Aurora, which optimizes both model deployment and all-to-all communication scheduling to address these challenges in MoE inference. Aurora achieves minimal communication times by strategically ordering token transmissions in all-to-all communications. It improves GPU utilization by colocating experts from different models on the same device, avoiding the limitations of synchronous all-to-all communication. We analyze Aurora's optimization strategies theoretically across four common GPU cluster settings: exclusive vs. colocated models on GPUs, and homogeneous vs. heterogeneous GPUs. Aurora provides optimal solutions for three cases, and for the remaining NP-hard scenario, it offers a polynomial-time sub-optimal solution with only a 1.07x degradation from the optimal.   Aurora is the first approach to minimize MoE inference time via optimal model deployment and communication scheduling across various scenarios. Evaluations demonstrate that Aurora significantly accelerates inference, achieving speedups of up to 2.38x in homogeneous clusters and 3.54x in heterogeneous environments. Moreover, Aurora enhances GPU utilization by up to 1.5x compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2410.17043v1),  [pdf](http://arxiv.org/pdf/2410.17043v1)

**Tags**: cs.LG cs.NI 



### Arabic Dataset for LLM Safeguard Evaluation
**Authors**: Yasser Ashraf, Yuxia Wang, Bin Gu, Preslav Nakov, Timothy Baldwin

**Updated**: 2024-10-22T14:12:43Z

**Summary**: The growing use of large language models (LLMs) has raised concerns regarding their safety. While many studies have focused on English, the safety of LLMs in Arabic, with its linguistic and cultural complexities, remains under-explored. Here, we aim to bridge this gap. In particular, we present an Arab-region-specific safety evaluation dataset consisting of 5,799 questions, including direct attacks, indirect attacks, and harmless requests with sensitive words, adapted to reflect the socio-cultural context of the Arab world. To uncover the impact of different stances in handling sensitive and controversial topics, we propose a dual-perspective evaluation framework. It assesses the LLM responses from both governmental and opposition viewpoints. Experiments over five leading Arabic-centric and multilingual LLMs reveal substantial disparities in their safety performance. This reinforces the need for culturally specific datasets to ensure the responsible deployment of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17040v1),  [pdf](http://arxiv.org/pdf/2410.17040v1)

**Tags**: cs.CL 



### Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by   Simulating Documents in the Wild via Low-level Perturbations
**Authors**: Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, Jong C. Park

**Updated**: 2024-10-22T14:07:57Z

**Summary**: The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.

**Link**: [arxiv](http://arxiv.org/abs/2404.13948v2),  [pdf](http://arxiv.org/pdf/2404.13948v2)

**Tags**: cs.CL 



### DIRI: Adversarial Patient Reidentification with Large Language Models   for Evaluating Clinical Text Anonymization
**Authors**: John X. Morris, Thomas R. Campion, Sri Laasya Nutheti, Yifan Peng, Akhil Raj, Ramin Zabih, Curtis L. Cole

**Updated**: 2024-10-22T14:06:31Z

**Summary**: Sharing protected health information (PHI) is critical for furthering biomedical research. Before data can be distributed, practitioners often perform deidentification to remove any PHI contained in the text. Contemporary deidentification methods are evaluated on highly saturated datasets (tools achieve near-perfect accuracy) which may not reflect the full variability or complexity of real-world clinical text and annotating them is resource intensive, which is a barrier to real-world applications. To address this gap, we developed an adversarial approach using a large language model (LLM) to re-identify the patient corresponding to a redacted clinical note and evaluated the performance with a novel De-Identification/Re-Identification (DIRI) method. Our method uses a large language model to reidentify the patient corresponding to a redacted clinical note. We demonstrate our method on medical data from Weill Cornell Medicine anonymized with three deidentification tools: rule-based Philter and two deep-learning-based models, BiLSTM-CRF and ClinicalBERT. Although ClinicalBERT was the most effective, masking all identified PII, our tool still reidentified 9% of clinical notes Our study highlights significant weaknesses in current deidentification technologies while providing a tool for iterative development and improvement.

**Link**: [arxiv](http://arxiv.org/abs/2410.17035v1),  [pdf](http://arxiv.org/pdf/2410.17035v1)

**Tags**: cs.CL 



### GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks
**Authors**: Shuyang Hou, Zhangxiao Shen, Anqi Zhao, Jianyuan Liang, Zhipeng Gui, Xuefeng Guan, Rui Li, Huayi Wu

**Updated**: 2024-10-22T13:57:55Z

**Summary**: The increasing demand for spatiotemporal data and modeling tasks in geosciences has made geospatial code generation technology a critical factor in enhancing productivity. Although large language models (LLMs) have demonstrated potential in code generation tasks, they often encounter issues such as refusal to code or hallucination in geospatial code generation due to a lack of domain-specific knowledge and code corpora. To address these challenges, this paper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along with the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and LoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first LLM focused on geospatial code generation, fine-tuned from Code Llama-7B. Furthermore, we establish a comprehensive geospatial code evaluation framework, incorporating option matching, expert validation, and prompt engineering scoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the GeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms other models in multiple-choice accuracy by 9.1% to 32.1%, in code summarization ability by 1.7% to 25.4%, and in code generation capability by 1.2% to 25.1%. This paper provides a solution and empirical validation for enhancing LLMs' performance in geospatial code generation, extends the boundaries of domain-specific model applications, and offers valuable insights into unlocking their potential in geospatial code generation.

**Link**: [arxiv](http://arxiv.org/abs/2410.17031v1),  [pdf](http://arxiv.org/pdf/2410.17031v1)

**Tags**: cs.SE cs.AI 



### Moonshine: Speech Recognition for Live Transcription and Voice Commands
**Authors**: Nat Jeffries, Evan King, Manjunath Kudlur, Guy Nicholson, James Wang, Pete Warden

**Updated**: 2024-10-22T13:55:26Z

**Summary**: This paper introduces Moonshine, a family of speech recognition models optimized for live transcription and voice command processing. Moonshine is based on an encoder-decoder transformer architecture and employs Rotary Position Embedding (RoPE) instead of traditional absolute position embeddings. The model is trained on speech segments of various lengths, but without using zero-padding, leading to greater efficiency for the encoder during inference time. When benchmarked against OpenAI's Whisper tiny-en, Moonshine Tiny demonstrates a 5x reduction in compute requirements for transcribing a 10-second speech segment while incurring no increase in word error rates across standard evaluation datasets. These results highlight Moonshine's potential for real-time and resource-constrained applications.

**Link**: [arxiv](http://arxiv.org/abs/2410.15608v2),  [pdf](http://arxiv.org/pdf/2410.15608v2)

**Tags**: cs.SD cs.CL cs.LG eess.AS 



### SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop   Question Answering Based on Finite State Machine
**Authors**: Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui

**Updated**: 2024-10-22T13:47:38Z

**Summary**: Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.

**Link**: [arxiv](http://arxiv.org/abs/2410.17021v1),  [pdf](http://arxiv.org/pdf/2410.17021v1)

**Tags**: cs.CL 



### LFME: A Simple Framework for Learning from Multiple Experts in Domain   Generalization
**Authors**: Liang Chen, Yong Zhang, Yibing Song, Zhiqiang Shen, Lingqiao Liu

**Updated**: 2024-10-22T13:44:10Z

**Summary**: Domain generalization (DG) methods aim to maintain good performance in an unseen target domain by using training data from multiple source domains. While success on certain occasions are observed, enhancing the baseline across most scenarios remains challenging. This work introduces a simple yet effective framework, dubbed learning from multiple experts (LFME), that aims to make the target model an expert in all source domains to improve DG. Specifically, besides learning the target model used in inference, LFME will also train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. Delving deep into the framework, we reveal that the introduced logit regularization term implicitly provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Extensive experiments on benchmarks from different DG tasks demonstrate that LFME is consistently beneficial to the baseline and can achieve comparable performance to existing arts. Code is available at~\url{https://github.com/liangchen527/LFME}.

**Link**: [arxiv](http://arxiv.org/abs/2410.17020v1),  [pdf](http://arxiv.org/pdf/2410.17020v1)

**Tags**: cs.LG cs.CV 



### On-Device LLMs for SMEs: Challenges and Opportunities
**Authors**: Jeremy Stephen Gabriel Yee, Pai Chet Ng, Zhengkui Wang, Ian McLoughlin, Aik Beng Ng, Simon See

**Updated**: 2024-10-22T13:40:18Z

**Summary**: This paper presents a systematic review of the infrastructure requirements for deploying Large Language Models (LLMs) on-device within the context of small and medium-sized enterprises (SMEs), focusing on both hardware and software perspectives. From the hardware viewpoint, we discuss the utilization of processing units like GPUs and TPUs, efficient memory and storage solutions, and strategies for effective deployment, addressing the challenges of limited computational resources typical in SME settings. From the software perspective, we explore framework compatibility, operating system optimization, and the use of specialized libraries tailored for resource-constrained environments. The review is structured to first identify the unique challenges faced by SMEs in deploying LLMs on-device, followed by an exploration of the opportunities that both hardware innovations and software adaptations offer to overcome these obstacles. Such a structured review provides practical insights, contributing significantly to the community by enhancing the technological resilience of SMEs in integrating LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.16070v2),  [pdf](http://arxiv.org/pdf/2410.16070v2)

**Tags**: cs.AI cs.CL 68T07 I.2 



### Exploring Forgetting in Large Language Model Pre-Training
**Authors**: Chonghua Liao, Ruobing Xie, Xingwu Sun, Haowen Sun, Zhanhui Kang

**Updated**: 2024-10-22T13:39:47Z

**Summary**: Catastrophic forgetting remains a formidable obstacle to building an omniscient model in large language models (LLMs). Despite the pioneering research on task-level forgetting in LLM fine-tuning, there is scant focus on forgetting during pre-training. We systematically explored the existence and measurement of forgetting in pre-training, questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention. Based on our revised assessment of forgetting metrics, we explored low-cost, straightforward methods to mitigate forgetting during the pre-training phase. Further, we carefully analyzed the learning curves, offering insights into the dynamics of forgetting. Extensive evaluations and analyses on forgetting of pre-training could facilitate future research on LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17018v1),  [pdf](http://arxiv.org/pdf/2410.17018v1)

**Tags**: cs.CL 



### Thinking Forward: Memory-Efficient Federated Finetuning of Language   Models
**Authors**: Kunjal Panchal, Nisarg Parikh, Sunav Choudhary, Lijun Zhang, Yuriy Brun, Hui Guan

**Updated**: 2024-10-22T13:32:59Z

**Summary**: Finetuning large language models (LLMs) in federated learning (FL) settings has become increasingly important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can significantly reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. In this paper, we introduce Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using forward-mode AD that are closer estimations of the true gradients. Spry achieves a low memory footprint, high accuracy, and fast convergence. We formally prove that the global gradients in Spry are unbiased estimators of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, Spry reduces the memory footprint during training by 1.4-7.1x in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. Spry reduces the convergence time by 1.2-20.3x and achieves 5.2-13.5% higher accuracy against zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory consumption of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible FL deployments on commodity edge devices. Our source code is available at https://github.com/Astuary/Spry.

**Link**: [arxiv](http://arxiv.org/abs/2405.15551v2),  [pdf](http://arxiv.org/pdf/2405.15551v2)

**Tags**: cs.LG 



### Joint Point Cloud Upsampling and Cleaning with Octree-based CNNs
**Authors**: Jihe Li, Bo Pang, Peng-Shuai Wang

**Updated**: 2024-10-22T13:23:05Z

**Summary**: Recovering dense and uniformly distributed point clouds from sparse or noisy data remains a significant challenge. Recently, great progress has been made on these tasks, but usually at the cost of increasingly intricate modules or complicated network architectures, leading to long inference time and huge resource consumption. Instead, we embrace simplicity and present a simple yet efficient method for jointly upsampling and cleaning point clouds. Our method leverages an off-the-shelf octree-based 3D U-Net (OUNet) with minor modifications, enabling the upsampling and cleaning tasks within a single network. Our network directly processes each input point cloud as a whole instead of processing each point cloud patch as in previous works, which significantly eases the implementation and brings at least 47 times faster inference. Extensive experiments demonstrate that our method achieves state-of-the-art performances under huge efficiency advantages on a series of benchmarks. We expect our method to serve simple baselines and inspire researchers to rethink the method design on point cloud upsampling and cleaning.

**Link**: [arxiv](http://arxiv.org/abs/2410.17001v1),  [pdf](http://arxiv.org/pdf/2410.17001v1)

**Tags**: cs.CV 



### Constraining Accreted Neutron Star Crust Shallow Heating with the   Inferred Depth of Carbon Ignition in X-ray Superbursts
**Authors**: Zach Meisel

**Updated**: 2024-10-22T13:17:34Z

**Summary**: Evidence has accumulated for an as-yet unaccounted for source of heat located at shallow depths within the accreted neutron star crust. However, the nature of this heat source is unknown. I demonstrate that the inferred depth of carbon ignition in X-ray superbursts can be used as an additional constraint for the magnitude and depth of shallow heating. The inferred shallow heating properties are relatively insensitive to the assumed crust composition and carbon fusion reaction rate. For low accretion rates, the results are weakly dependent on the duration of the accretion outburst, so long as accretion has ensued for enough time to replace the ocean down to the superburst ignition depth. For accretion rates at the Eddington rate, results show a stronger dependence on the outburst duration. Consistent with earlier work, it is shown that urca cooling does not impact the calculated superburst ignition depth unless there is some proximity in depth between the heating and cooling sources.

**Link**: [arxiv](http://arxiv.org/abs/2208.03347v2),  [pdf](http://arxiv.org/pdf/2208.03347v2)

**Tags**: astro-ph.HE 



### An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and   Geometric Reasoning Skills Using Computer Graphics Questions
**Authors**: Tony Haoran Feng, Paul Denny, Burkhard C. Wnsche, Andrew Luxton-Reilly, Jacqueline Whalley

**Updated**: 2024-10-22T13:12:47Z

**Summary**: CG (Computer Graphics) is a popular field of CS (Computer Science), but many students find this topic difficult due to it requiring a large number of skills, such as mathematics, programming, geometric reasoning, and creativity. Over the past few years, researchers have investigated ways to harness the power of GenAI (Generative Artificial Intelligence) to improve teaching. In CS, much of the research has focused on introductory computing. A recent study evaluating the performance of an LLM (Large Language Model), GPT-4 (text-only), on CG questions, indicated poor performance and reliance on detailed descriptions of image content, which often required considerable insight from the user to return reasonable results. So far, no studies have investigated the abilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG questions and how these abilities can be used to improve teaching.   In this study, we construct two datasets of CG questions requiring varying degrees of visual perception skills and geometric reasoning skills, and evaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find that although GPT-4o exhibits great potential in solving questions with visual information independently, major limitations still exist to the accuracy and quality of the generated results. We propose several novel approaches for CG educators to incorporate GenAI into CG teaching despite these limitations. We hope that our guidelines further encourage learning and engagement in CG classrooms.

**Link**: [arxiv](http://arxiv.org/abs/2410.16991v1),  [pdf](http://arxiv.org/pdf/2410.16991v1)

**Tags**: cs.AI cs.GR I.2.7; I.3.0; K.3.2 



### LLM Gesticulator: Leveraging Large Language Models for Scalable and   Controllable Co-Speech Gesture Synthesis
**Authors**: Haozhou Pang, Tianwei Ding, Lanshan He, Ming Tao, Lu Zhang, Qi Gan

**Updated**: 2024-10-22T13:08:02Z

**Summary**: In this work, we present LLM Gesticulator, an LLM-based audio-driven co-speech gesture generation framework that synthesizes full-body animations that are rhythmically aligned with the input audio while exhibiting natural movements and editability. Compared to previous work, our model demonstrates substantial scalability. As the size of the backbone LLM model increases, our framework shows proportional improvements in evaluation metrics (a.k.a. scaling law). Our method also exhibits strong controllability where the content, style of the generated gestures can be controlled by text prompt. To the best of our knowledge, LLM gesticulator is the first work that use LLM on the co-speech generation task. Evaluation with existing objective metrics and user studies indicate that our framework outperforms prior works.

**Link**: [arxiv](http://arxiv.org/abs/2410.10851v2),  [pdf](http://arxiv.org/pdf/2410.10851v2)

**Tags**: cs.GR cs.AI cs.CL cs.LG cs.SD eess.AS 



### Constraints on sterile neutrinos and the cosmological tensions
**Authors**: Supriya Pan, Osamu Seto, Tomo Takahashi, Yo Toda

**Updated**: 2024-10-22T13:07:45Z

**Summary**: We investigate cosmological bounds on sterile neutrino masses in the light of the Hubble and $S_8$ tensions. We argue that non-zero masses for sterile neutrinos are inferred at 2$\sigma$ level in some extended models such as varying dark energy equation of state, when a direct measurement of the Hubble constant $H_0$ and weak lensing measurement of dark energy survey (DES) are taken into account. Furthermore, the Hubble and $S_8$ tensions are also reduced in such a framework. We also consider the case where a non-flat Universe is allowed and show that a slightly open Universe may be favored in models with sterile neutrinos in the context of the cosmological tensions.

**Link**: [arxiv](http://arxiv.org/abs/2312.15435v2),  [pdf](http://arxiv.org/pdf/2312.15435v2)

**Tags**: astro-ph.CO hep-ph 



### Order Matters: Exploring Order Sensitivity in Multimodal Large Language   Models
**Authors**: Zhijie Tan, Xu Chu, Weiping Li, Tong Mo

**Updated**: 2024-10-22T13:05:11Z

**Summary**: Multimodal Large Language Models (MLLMs) utilize multimodal contexts consisting of text, images, or videos to solve various multimodal tasks. However, we find that changing the order of multimodal input can cause the model's performance to fluctuate between advanced performance and random guessing. This phenomenon exists in both single-modality (text-only or image-only) and mixed-modality (image-text-pair) contexts. Furthermore, we demonstrate that popular MLLMs pay special attention to certain multimodal context positions, particularly the beginning and end. Leveraging this special attention, we place key video frames and important image/text content in special positions within the context and submit them to the MLLM for inference. This method results in average performance gains of 14.7% for video-caption matching and 17.8% for visual question answering tasks. Additionally, we propose a new metric, Position-Invariant Accuracy (PIA), to address order bias in MLLM evaluation. Our research findings contribute to a better understanding of Multi-Modal In-Context Learning (MMICL) and provide practical strategies for enhancing MLLM performance without increasing computational costs.

**Link**: [arxiv](http://arxiv.org/abs/2410.16983v1),  [pdf](http://arxiv.org/pdf/2410.16983v1)

**Tags**: cs.AI 



### Publishing Neural Networks in Drug Discovery Might Compromise Training   Data Privacy
**Authors**: Fabian P. Krger, Johan stman, Lewis Mervin, Igor V. Tetko, Ola Engkvist

**Updated**: 2024-10-22T12:55:02Z

**Summary**: This study investigates the risks of exposing confidential chemical structures when machine learning models trained on these structures are made publicly available. We use membership inference attacks, a common method to assess privacy that is largely unexplored in the context of drug discovery, to examine neural networks for molecular property prediction in a black-box setting. Our results reveal significant privacy risks across all evaluated datasets and neural network architectures. Combining multiple attacks increases these risks. Molecules from minority classes, often the most valuable in drug discovery, are particularly vulnerable. We also found that representing molecules as graphs and using message-passing neural networks may mitigate these risks. We provide a framework to assess privacy risks of classification models and molecular representations. Our findings highlight the need for careful consideration when sharing neural networks trained on proprietary chemical structures, informing organisations and researchers about the trade-offs between data confidentiality and model openness.

**Link**: [arxiv](http://arxiv.org/abs/2410.16975v1),  [pdf](http://arxiv.org/pdf/2410.16975v1)

**Tags**: cs.CR cs.LG 



### RecPrompt: A Self-tuning Prompting Framework for News Recommendation   Using Large Language Models
**Authors**: Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Neil Hurley, Aonghus Lawlor, Ruihai Dong, Irene Li

**Updated**: 2024-10-22T12:40:25Z

**Summary**: News recommendations heavily rely on Natural Language Processing (NLP) methods to analyze, understand, and categorize content, enabling personalized suggestions based on user interests and reading behaviors. Large Language Models (LLMs) like GPT-4 have shown promising performance in understanding natural language. However, the extent of their applicability to news recommendation systems remains to be validated. This paper introduces RecPrompt, the first self-tuning prompting framework for news recommendation, leveraging the capabilities of LLMs to perform complex news recommendation tasks. This framework incorporates a news recommender and a prompt optimizer that applies an iterative bootstrapping process to enhance recommendations through automatic prompt engineering. Extensive experimental results with 400 users show that RecPrompt can achieve an improvement of 3.36% in AUC, 10.49% in MRR, 9.64% in nDCG@5, and 6.20% in nDCG@10 compared to deep neural models. Additionally, we introduce TopicScore, a novel metric to assess explainability by evaluating LLM's ability to summarize topics of interest for users. The results show LLM's effectiveness in accurately identifying topics of interest and delivering comprehensive topic-based explanations.

**Link**: [arxiv](http://arxiv.org/abs/2312.10463v4),  [pdf](http://arxiv.org/pdf/2312.10463v4)

**Tags**: cs.IR 



### Targeted Separation and Convergence with Kernel Discrepancies
**Authors**: Alessandro Barp, Carl-Johann Simon-Gabriel, Mark Girolami, Lester Mackey

**Updated**: 2024-10-22T12:38:35Z

**Summary**: Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our results for hypothesis testing, measuring and improving sample quality, and sampling with Stein variational gradient descent.

**Link**: [arxiv](http://arxiv.org/abs/2209.12835v4),  [pdf](http://arxiv.org/pdf/2209.12835v4)

**Tags**: stat.ML cs.LG math.ST stat.TH 



### Towards Real Zero-Shot Camouflaged Object Segmentation without   Camouflaged Annotations
**Authors**: Cheng Lei, Jie Fan, Xinran Li, Tianzhu Xiang, Ao Li, Ce Zhu, Le Zhang

**Updated**: 2024-10-22T12:33:38Z

**Summary**: Camouflaged Object Segmentation (COS) faces significant challenges due to the scarcity of annotated data, where meticulous pixel-level annotation is both labor-intensive and costly, primarily due to the intricate object-background boundaries. Addressing the core question, "Can COS be effectively achieved in a zero-shot manner without manual annotations for any camouflaged object?" we affirmatively respond and introduce a robust zero-shot COS framework. This framework leverages the inherent local pattern bias of COS and employs a broad semantic feature space derived from salient object segmentation (SOS) for efficient zero-shot transfer. We incorporate an Masked Image Modeling (MIM) based image encoder optimized for Parameter-Efficient Fine-Tuning (PEFT), a Multimodal Large Language Model (M-LLM), and a Multi-scale Fine-grained Alignment (MFA) mechanism. The MIM pre-trained image encoder focuses on capturing essential low-level features, while the M-LLM generates caption embeddings processed alongside these visual cues. These embeddings are precisely aligned using MFA, enabling our framework to accurately interpret and navigate complex semantic contexts. To optimize operational efficiency, we introduce a learnable codebook that represents the M-LLM during inference, significantly reducing computational overhead. Our framework demonstrates its versatility and efficacy through rigorous experimentation, achieving state-of-the-art performance in zero-shot COS with $F_{\beta}^w$ scores of 72.9\% on CAMO and 71.7\% on COD10K. By removing the M-LLM during inference, we achieve an inference speed comparable to that of traditional end-to-end models, reaching 18.1 FPS. Code: https://github.com/R-LEI360725/ZSCOS-CaMF

**Link**: [arxiv](http://arxiv.org/abs/2410.16953v1),  [pdf](http://arxiv.org/pdf/2410.16953v1)

**Tags**: cs.CV 



### Estimating predictability of depinning dynamics by machine learning
**Authors**: Valtteri Haavisto, Marcin Mikowski, Lasse Laurson

**Updated**: 2024-10-22T12:32:55Z

**Summary**: Predicting the future behaviour of complex systems exhibiting critical-like dynamics is often considered to be an intrinsically hard task. Here, we study the predictability of the depinning dynamics of elastic interfaces in random media driven by a slowly increasing external force, a paradigmatic complex system exhibiting critical avalanche dynamics linked to a continuous non-equilibrium depinning phase transition. To this end, we train a variety of machine learning models to infer the mapping from features of the initial relaxed line shape and the random pinning landscape to predict the sample-dependent staircase-like force-displacement curve that emerges from the depinning process. Even if for a given realization of the quenched random medium the dynamics are in principle deterministic, we find that there is an exponential decay of the predictability with the displacement of the line, quantifying how the system forgets its initial state as it nears the depinning transition from below. Our analysis on how the related displacement scale depends on the system size and the dimensionality of the input descriptor reveals that the onset of the depinning phase transition gives rise to fundamental limits to predictability.

**Link**: [arxiv](http://arxiv.org/abs/2312.11030v2),  [pdf](http://arxiv.org/pdf/2312.11030v2)

**Tags**: cond-mat.stat-mech cond-mat.dis-nn 



### Rethinking Complex Queries on Knowledge Graphs with Neural Link   Predictors
**Authors**: Hang Yin, Zihao Wang, Yangqiu Song

**Updated**: 2024-10-22T12:28:13Z

**Summary**: Reasoning on knowledge graphs is a challenging task because it utilizes observed information to predict the missing one. Particularly, answering complex queries based on first-order logic is one of the crucial tasks to verify learning to reason abilities for generalization and composition. Recently, the prevailing method is query embedding which learns the embedding of a set of entities and treats logic operations as set operations and has shown great empirical success. Though there has been much research following the same formulation, many of its claims lack a formal and systematic inspection. In this paper, we rethink this formulation and justify many of the previous claims by characterizing the scope of queries investigated previously and precisely identifying the gap between its formulation and its goal, as well as providing complexity analysis for the currently investigated queries. Moreover, we develop a new dataset containing ten new types of queries with features that have never been considered and therefore can provide a thorough investigation of complex queries. Finally, we propose a new neural-symbolic method, Fuzzy Inference with Truth value (FIT), where we equip the neural link predictors with fuzzy logic theory to support end-to-end learning using complex queries with provable reasoning capability. Empirical results show that our method outperforms previous methods significantly in the new dataset and also surpasses previous methods in the existing dataset at the same time.

**Link**: [arxiv](http://arxiv.org/abs/2304.07063v4),  [pdf](http://arxiv.org/pdf/2304.07063v4)

**Tags**: cs.AI cs.DB cs.LG cs.LO 



### Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In
**Authors**: Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor

**Updated**: 2024-10-22T12:24:41Z

**Summary**: Following the advancement of large language models (LLMs), the development of LLM-based autonomous agents has become increasingly prevalent. As a result, the need to understand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straightforward yet effective method we refer to as the foot-in-the-door attack. Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as basic calculations) can significantly increase the likelihood of the agent performing subsequent malicious actions. Our results show that once a ReAct agents thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions. Consequently, even random, harmless requests can establish a foot-in-the-door, allowing an attacker to embed malicious instructions into the agents thought process, making it more susceptible to harmful directives. To mitigate this vulnerability, we propose implementing a simple reflection mechanism that prompts the agent to reassess the safety of its actions during execution, which can help reduce the success of such attacks.

**Link**: [arxiv](http://arxiv.org/abs/2410.16950v1),  [pdf](http://arxiv.org/pdf/2410.16950v1)

**Tags**: cs.CR cs.AI 



### Self-Evolving Multi-Agent Collaboration Networks for Software   Development
**Authors**: Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, Siheng Chen

**Updated**: 2024-10-22T12:20:23Z

**Summary**: LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network. To extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose rSDE-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness. Our experiments show that: i) The automatic requirement-aware evaluation in rSDE-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark. ii) EvoMAC outperforms previous SOTA methods on both the software-level rSDE-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities. The benchmark can be downloaded at https://yuzhu-cai.github.io/rSDE-Bench/.

**Link**: [arxiv](http://arxiv.org/abs/2410.16946v1),  [pdf](http://arxiv.org/pdf/2410.16946v1)

**Tags**: cs.SE cs.AI cs.MA 



### DiP-GO: A Diffusion Pruner via Few-step Gradient Optimization
**Authors**: Haowei Zhu, Dehua Tang, Ji Liu, Mingjie Lu, Jintu Zheng, Jinzhang Peng, Dong Li, Yu Wang, Fan Jiang, Lu Tian, Spandan Tiwari, Ashish Sirasao, Jun-Hai Yong, Bin Wang, Emad Barsoum

**Updated**: 2024-10-22T12:18:24Z

**Summary**: Diffusion models have achieved remarkable progress in the field of image generation due to their outstanding capabilities. However, these models require substantial computing resources because of the multi-step denoising process during inference. While traditional pruning methods have been employed to optimize these models, the retraining process necessitates large-scale training datasets and extensive computational costs to maintain generalization ability, making it neither convenient nor efficient. Recent studies attempt to utilize the similarity of features across adjacent denoising stages to reduce computational costs through simple and static strategies. However, these strategies cannot fully harness the potential of the similar feature patterns across adjacent timesteps. In this work, we propose a novel pruning method that derives an efficient diffusion model via a more intelligent and differentiable pruner. At the core of our approach is casting the model pruning process into a SubNet search process. Specifically, we first introduce a SuperNet based on standard diffusion via adding some backup connections built upon the similar features. We then construct a plugin pruner network and design optimization losses to identify redundant computation. Finally, our method can identify an optimal SubNet through few-step gradient optimization and a simple post-processing procedure. We conduct extensive experiments on various diffusion models including Stable Diffusion series and DiTs. Our DiP-GO approach achieves 4.4 x speedup for SD-1.5 without any loss of accuracy, significantly outperforming the previous state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2410.16942v1),  [pdf](http://arxiv.org/pdf/2410.16942v1)

**Tags**: cs.CV 



### Inferring system parameters from the bursts of the accretion-powered   pulsar IGR J17498-2921
**Authors**: D. K. Galloway, A. J. Goodwin, T. Hilder, L. Waterson, M. Cupk

**Updated**: 2024-10-22T12:03:17Z

**Summary**: Thermonuclear (type-I) bursts exhibit properties that depend both on the local surface conditions of the neutron stars on which they ignite, as well as the physical parameters of the host binary system. However, constraining the system parameters requires a comprehensive method to compare the observed bursts to simulations. We have further developed the beansp code for this purpose and analysed the bursts observed from IGR J17498-2921, a 401-Hz accretion-powered pulsar, discovered during it's 2011 outburst. We find good agreement with a model having H-deficient fuel with X = 0.15 +/- 0.4, and CNO metallicity Z=0.0014^{+0.0004}_{-0.0003}, about a tenth of the solar value. The model has the system at a distance of 5.7^{+0.6}_{-0.5} kpc, with a massive (approx. 2 M_sun) neutron star and a likely inclination of 60 deg. We also re-analysed the data from the 2002 outburst of the accretion-powered millisecond pulsar SAX J1808.4-3658. For that system we find a substantially closer distance than previously inferred, at 2.7 +/- 0.3 kpc, likely driven by a larger degree of burst emission anisotropy. The other system parameters are largely consistent with the previous analysis. We briefly discuss the implications for the evolution of these two systems.

**Link**: [arxiv](http://arxiv.org/abs/2403.16471v2),  [pdf](http://arxiv.org/pdf/2403.16471v2)

**Tags**: astro-ph.HE 



### Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities   Using Only Forward Passes
**Authors**: Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen

**Updated**: 2024-10-22T12:00:58Z

**Summary**: Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.

**Link**: [arxiv](http://arxiv.org/abs/2410.16930v1),  [pdf](http://arxiv.org/pdf/2410.16930v1)

**Tags**: cs.CL cs.AI 



### Revealing Hidden Bias in AI: Lessons from Large Language Models
**Authors**: Django Beatty, Kritsada Masanthia, Teepakorn Kaphol, Niphan Sethi

**Updated**: 2024-10-22T11:58:54Z

**Summary**: As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest overall bias. Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications. This study underscores the importance of careful LLM selection and suggests best practices for minimizing bias in AI applications, promoting fairness and inclusivity.

**Link**: [arxiv](http://arxiv.org/abs/2410.16927v1),  [pdf](http://arxiv.org/pdf/2410.16927v1)

**Tags**: cs.AI cs.CY I.2.7; K.4.1 



### Pyramid Vector Quantization for LLMs
**Authors**: Tycho F. A. van der Ouderaa, Maximilian L. Croci, Agrin Hilmkil, James Hensman

**Updated**: 2024-10-22T11:57:32Z

**Summary**: Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\% accuracy on downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2410.16926v1),  [pdf](http://arxiv.org/pdf/2410.16926v1)

**Tags**: cs.LG 



### SleepCoT: A Lightweight Personalized Sleep Health Model via   Chain-of-Thought Distillation
**Authors**: Huimin Zheng, Xiaofeng Xing, Xiangmin Xu

**Updated**: 2024-10-22T11:56:34Z

**Summary**: We present a novel approach to personalized sleep health management using few-shot Chain-of-Thought (CoT) distillation, enabling small-scale language models (> 2B parameters) to rival the performance of large language models (LLMs) in specialized health domains. Our method simultaneously distills problem-solving strategies, long-tail expert knowledge, and personalized recommendation capabilities from larger models into more efficient, compact models. Unlike existing systems, our approach offers three key functionalities: generating personalized sleep health recommendations, supporting user-specific follow-up inquiries, and providing responses to domain-specific knowledge questions. We focus on sleep health due to its measurability via wearable devices and its impact on overall well-being. Our experimental setup, involving GPT-4o for data synthesis, Qwen-max for instruction set creation, and Qwen2.5 1.5B for model distillation, demonstrates significant improvements over baseline small-scale models in penalization, reasoning, and knowledge application. Experiments using 100 simulated sleep reports and 1,000 domain-specific questions shows our model achieves comparable performance to larger models while maintaining efficiency for real-world deployment. This research not only advances AI-driven health management but also provides a novel approach to leveraging LLM capabilities in resource-constrained environments, potentially enhancing the accessibility of personalized healthcare solutions.

**Link**: [arxiv](http://arxiv.org/abs/2410.16924v1),  [pdf](http://arxiv.org/pdf/2410.16924v1)

**Tags**: cs.AI 



### EnvBridge: Bridging Diverse Environments with Cross-Environment   Knowledge Transfer for Embodied AI
**Authors**: Tomoyuki Kagaya, Yuxuan Lou, Thong Jing Yuan, Subramanian Lakshmi, Jayashree Karlekar, Sugiri Pranata, Natsuki Murakami, Akira Kinose, Koki Oguri, Felix Wick, Yang You

**Updated**: 2024-10-22T11:52:22Z

**Summary**: In recent years, Large Language Models (LLMs) have demonstrated high reasoning capabilities, drawing attention for their applications as agents in various decision-making processes. One notably promising application of LLM agents is robotic manipulation. Recent research has shown that LLMs can generate text planning or control code for robots, providing substantial flexibility and interaction capabilities. However, these methods still face challenges in terms of flexibility and applicability across different environments, limiting their ability to adapt autonomously. Current approaches typically fall into two categories: those relying on environment-specific policy training, which restricts their transferability, and those generating code actions based on fixed prompts, which leads to diminished performance when confronted with new environments. These limitations significantly constrain the generalizability of agents in robotic manipulation. To address these limitations, we propose a novel method called EnvBridge. This approach involves the retention and transfer of successful robot control codes from source environments to target environments. EnvBridge enhances the agent's adaptability and performance across diverse settings by leveraging insights from multiple environments. Notably, our approach alleviates environmental constraints, offering a more flexible and generalizable solution for robotic manipulation tasks. We validated the effectiveness of our method using robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments demonstrate that LLM agents can successfully leverage diverse knowledge sources to solve complex tasks. Consequently, our approach significantly enhances the adaptability and robustness of robotic manipulation agents in planning across diverse environments.

**Link**: [arxiv](http://arxiv.org/abs/2410.16919v1),  [pdf](http://arxiv.org/pdf/2410.16919v1)

**Tags**: cs.RO cs.AI cs.CL cs.LG 



### VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large   Language Models
**Authors**: Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, Mao Yang

**Updated**: 2024-10-22T11:47:04Z

**Summary**: Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables.   In this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B, $4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of $0.79$-$1.5\%$ on LLaMA-2, $1\%$ on Mistral-7B, $11$-$22\%$ on LLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\%$ of the quantization algorithm execution time, resulting in a $1.6$-$1.8\times$ increase in inference throughput compared to SOTA.

**Link**: [arxiv](http://arxiv.org/abs/2409.17066v2),  [pdf](http://arxiv.org/pdf/2409.17066v2)

**Tags**: cs.AI 



### Mimicking Better by Matching the Approximate Action Distribution
**Authors**: Joo A. Cndido Ramos, Lionel Blond, Naoya Takeishi, Alexandros Kalousis

**Updated**: 2024-10-22T11:33:36Z

**Summary**: In this paper, we introduce MAAD, a novel, sample-efficient on-policy algorithm for Imitation Learning from Observations. MAAD utilizes a surrogate reward signal, which can be derived from various sources such as adversarial games, trajectory matching objectives, or optimal transport criteria. To compensate for the non-availability of expert actions, we rely on an inverse dynamics model that infers plausible actions distribution given the expert's state-state transitions; we regularize the imitator's policy by aligning it to the inferred action distribution. MAAD leads to significantly improved sample efficiency and stability. We demonstrate its effectiveness in a number of MuJoCo environments, both int the OpenAI Gym and the DeepMind Control Suite. We show that it requires considerable fewer interactions to achieve expert performance, outperforming current state-of-the-art on-policy methods. Remarkably, MAAD often stands out as the sole method capable of attaining expert performance levels, underscoring its simplicity and efficacy.

**Link**: [arxiv](http://arxiv.org/abs/2306.09805v3),  [pdf](http://arxiv.org/pdf/2306.09805v3)

**Tags**: cs.LG 



### The Radcliffe Wave as traced by young open clusters: Stellar parameters,   activity indicators, and abundances of solar-type members of eight young   clusters
**Authors**: J. Alonso-Santiago, A. Frasca, A. Bragaglia, G. Catanzaro, X. Fu, G. Andreuzzi, L. Magrini, S. Lucatello, A. Vallenari, M. Jian

**Updated**: 2024-10-22T11:01:29Z

**Summary**: The Radcliffe Wave has only recently been recognised as a about 3 kpc long coherent gas structure encompassing most of the star forming regions in the solar vicinity. Since its discovery, it has been mainly studied from the perspective of dynamics, but a detailed chemical study is necessary to understand its nature and the composition of the natal clouds that gave rise to it. In this paper we used some of the connected young open clusters (age $\lesssim$ 100 Myr) as tracers of the molecular clouds. We performed high-resolution spectroscopy with GIARPS at the TNG of 53 stars that are bona fide members of seven clusters located at different positions along the Radcliffe Wave. We provide radial velocities and atmospheric parameters for all of them. For a subsample consisting of 41 FGK stars we also studied the chromospheric activity and the content of Li, from which we inferred the age of the parent clusters. These values agree with the evolutionary ages reported in the literature. For these FGK stars we determined the chemical abundances for 25 species. Pleiades, ASCC 16 and NGC 7058 exhibit a solar metallicity while Melotte 20, ASCC 19, NGC 2232, and Roslund 6 show a slightly subsolar value ($\approx-$0.1 dex). On average, the clusters show a chemical composition compatible with that of the Sun, especially for $\alpha$- and Fe-peak elements. Neutron-capture elements, on the other hand, present a slight overabundance of about 0.2 dex, specially barium. Finally, considering also ASCC 123, studied by our group in a previous research, we infer a correlation between the chemical composition and the age or position of the clusters along the Wave, demonstrating their physical connection within an inhomogeneous mixing scenario.

**Link**: [arxiv](http://arxiv.org/abs/2410.14373v2),  [pdf](http://arxiv.org/pdf/2410.14373v2)

**Tags**: astro-ph.GA 



### PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs
**Authors**: Joo Pedro Fernandes Torres, Catherine Mulligan, Joaquim Jorge, Catarina Moreira

**Updated**: 2024-10-22T10:56:35Z

**Summary**: The growing volume of academic publications poses significant challenges for researchers conducting timely and accurate Systematic Literature Reviews, particularly in fast-evolving fields like artificial intelligence. This growth of academic literature also makes it increasingly difficult for lay people to access scientific knowledge effectively, meaning academic literature is often misrepresented in the popular press and, more broadly, in society. Traditional SLR methods are labor-intensive and error-prone, and they struggle to keep up with the rapid pace of new research. To address these issues, we developed \textit{PROMPTHEUS}: an AI-driven pipeline solution that automates the SLR process using Large Language Models. We aimed to enhance efficiency by reducing the manual workload while maintaining the precision and coherence required for comprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR process, including systematic search, data extraction, topic modeling using BERTopic, and summarization with transformer models. Evaluations conducted across five research domains demonstrate that PROMPTHEUS reduces review time, achieves high precision, and provides coherent topic organization, offering a scalable and effective solution for conducting literature reviews in an increasingly crowded research landscape. In addition, such tools may reduce the increasing mistrust in science by making summarization more accessible to laypeople.   The code for this project can be found on the GitHub repository at https://github.com/joaopftorres/PROMPTHEUS.git

**Link**: [arxiv](http://arxiv.org/abs/2410.15978v2),  [pdf](http://arxiv.org/pdf/2410.15978v2)

**Tags**: cs.AI 



### GLBench: A Comprehensive Benchmark for Graph with Large Language Models
**Authors**: Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, Jia Li

**Updated**: 2024-10-22T10:54:15Z

**Summary**: The emergence of large language models (LLMs) has revolutionized the way we interact with graphs, leading to a new paradigm called GraphLLM. Despite the rapid development of GraphLLM methods in recent years, the progress and understanding of this field remain unclear due to the lack of a benchmark with consistent experimental protocols. To bridge this gap, we introduce GLBench, the first comprehensive benchmark for evaluating GraphLLM methods in both supervised and zero-shot scenarios. GLBench provides a fair and thorough evaluation of different categories of GraphLLM methods, along with traditional baselines such as graph neural networks. Through extensive experiments on a collection of real-world datasets with consistent data processing and splitting strategies, we have uncovered several key findings. Firstly, GraphLLM methods outperform traditional baselines in supervised settings, with LLM-as-enhancers showing the most robust performance. However, using LLMs as predictors is less effective and often leads to uncontrollable output issues. We also notice that no clear scaling laws exist for current GraphLLM methods. In addition, both structures and semantics are crucial for effective zero-shot transfer, and our proposed simple baseline can even outperform several models tailored for zero-shot scenarios. The data and code of the benchmark can be found at https://github.com/NineAbyss/GLBench.

**Link**: [arxiv](http://arxiv.org/abs/2407.07457v3),  [pdf](http://arxiv.org/pdf/2407.07457v3)

**Tags**: cs.LG cs.CL 



### Dynamic Planning for LLM-based Graphical User Interface Automation
**Authors**: Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, Min Zhang

**Updated**: 2024-10-22T10:47:13Z

**Summary**: The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history.We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7% (34.66% $\rightarrow$ 47.36%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.

**Link**: [arxiv](http://arxiv.org/abs/2410.00467v2),  [pdf](http://arxiv.org/pdf/2410.00467v2)

**Tags**: cs.AI cs.HC 



### Unsupervised Time Series Anomaly Prediction with Importance-based   Generative Contrastive Learning
**Authors**: Kai Zhao, Zhihao Zhuang, Chenjuan Guo, Hao Miao, Yunyao Cheng, Bin Yang

**Updated**: 2024-10-22T10:46:36Z

**Summary**: Time series anomaly prediction plays an essential role in many real-world scenarios, such as environmental prevention and prompt maintenance of cyber-physical systems. However, existing time series anomaly prediction methods mainly require supervised training with plenty of manually labeled data, which are difficult to obtain in practice. Besides, unseen anomalies can occur during inference, which could differ from the labeled training data and make these models fail to predict such new anomalies. In this paper, we study a novel problem of unsupervised time series anomaly prediction. We provide a theoretical analysis and propose Importance-based Generative Contrastive Learning (IGCL) to address the aforementioned problems. IGCL distinguishes between normal and anomaly precursors, which are generated by our anomaly precursor pattern generation module. To address the efficiency issues caused by the potential complex anomaly precursor combinations, we propose a memory bank with importance-based scores to adaptively store representative anomaly precursors and generate more complicated anomaly precursors. Extensive experiments on seven benchmark datasets show our method outperforms state-of-the-art baselines on unsupervised time series anomaly prediction problems.

**Link**: [arxiv](http://arxiv.org/abs/2410.16888v1),  [pdf](http://arxiv.org/pdf/2410.16888v1)

**Tags**: cs.LG 



### Network Inversion for Training-Like Data Reconstruction
**Authors**: Pirzada Suhail, Amit Sethi

**Updated**: 2024-10-22T10:42:08Z

**Summary**: Machine Learning models are often trained on proprietary and private data that cannot be shared, though the trained models themselves are distributed openly assuming that sharing model weights is privacy preserving, as training data is not expected to be inferred from the model weights. In this paper, we present Training-Like Data Reconstruction (TLDR), a network inversion-based approach to reconstruct training-like data from trained models. To begin with, we introduce a comprehensive network inversion technique that learns the input space corresponding to different classes in the classifier using a single conditioned generator. While inversion may typically return random and arbitrary input images for a given output label, we modify the inversion process to incentivize the generator to reconstruct training-like data by exploiting key properties of the classifier with respect to the training data along with some prior knowledge about the images. To validate our approach, we conduct empirical evaluations on multiple standard vision classification datasets, thereby highlighting the potential privacy risks involved in sharing machine learning models.

**Link**: [arxiv](http://arxiv.org/abs/2410.16884v1),  [pdf](http://arxiv.org/pdf/2410.16884v1)

**Tags**: cs.CV 



### Correcting for Selection Biases in the Determination of the Hubble   Constant from Time-Delay Cosmography
**Authors**: Tian Li, Thomas E. Collett, Philip J. Marshall, Sydney Erickson, Wolfgang Enzi, Lindsay Oldham, Daniel Ballard

**Updated**: 2024-10-22T10:40:28Z

**Summary**: The time delay between multiple images of strongly lensed quasars has been used to infer the Hubble constant. The primary systematic uncertainty for time-delay cosmography is the mass-sheet transform (MST), which preserves the lensing observables while altering the inferred $H_0$. The TDCOSMO collaboration used velocity dispersion measurements of lensed quasars and lensed galaxies to infer that mass sheets are present, which decrease the inferred $H_0$ by 8$\%$. Here, we test the assumption that the density profiles of galaxy-galaxy and galaxy-quasar lenses are the same. We use a composite star-plus-dark-matter mass profile for the parent deflector population and model the selection function for galaxy-galaxy and galaxy-quasar lenses. We find that a power-law density profile with an MST is a good approximation to a two-component mass profile around the Einstein radius, but we find that galaxy-galaxy lenses have systematically higher mass-sheet components than galaxy-quasar lenses. For individual systems, $\lambda_\mathrm{int}$ correlates with the ratio of the half-light radius and Einstein radius of the lens. By propagating these results through the TDCOSMO methodology, we find that $H_0$ is lowered by a further $\sim$3\%. Using the velocity dispersions from \citet{slacs9} and our fiducial model for selection biases, we infer $H_0 = 66\pm4 \ \mathrm{(stat)} \pm 1 \ \mathrm{(model \ sys)} \pm 2 \ \mathrm{(measurement \ sys)} \ \mathrm{km} \ \mathrm{s}^{-1} \ \mathrm{Mpc}^{-1}$ for the TDCOSMO plus SLACS dataset. The first residual systematic error is due to plausible alternative choices in modeling the selection function, and the second is an estimate of the remaining systematic error in the measurement of velocity dispersions for SLACS lenses. Accurate time-delay cosmography requires precise velocity dispersion measurements and accurate calibration of selection biases.

**Link**: [arxiv](http://arxiv.org/abs/2410.16171v2),  [pdf](http://arxiv.org/pdf/2410.16171v2)

**Tags**: astro-ph.CO 



### Large Language Model-based Augmentation for Imbalanced Node   Classification on Text-Attributed Graphs
**Authors**: Leyao Wang, Yu Wang, Bo Ni, Yuying Zhao, Tyler Derr

**Updated**: 2024-10-22T10:36:15Z

**Summary**: Node classification on graphs frequently encounters the challenge of class imbalance, leading to biased performance and posing significant risks in real-world applications. Although several data-centric solutions have been proposed, none of them focus on Text-Attributed Graphs (TAGs), and therefore overlook the potential of leveraging the rich semantics encoded in textual features for boosting the classification of minority nodes. Given this crucial gap, we investigate the possibility of augmenting graph data in the text space, leveraging the textual generation power of Large Language Models (LLMs) to handle imbalanced node classification on TAGs. Specifically, we propose a novel approach called LA-TAG (LLM-based Augmentation on Text-Attributed Graphs), which prompts LLMs to generate synthetic texts based on existing node texts in the graph. Furthermore, to integrate these synthetic text-attributed nodes into the graph, we introduce a text-based link predictor to connect the synthesized nodes with the existing nodes. Our experiments across multiple datasets and evaluation metrics show that our framework significantly outperforms traditional non-textual-based data augmentation strategies and specific node imbalance solutions. This highlights the promise of using LLMs to resolve imbalance issues on TAGs.

**Link**: [arxiv](http://arxiv.org/abs/2410.16882v1),  [pdf](http://arxiv.org/pdf/2410.16882v1)

**Tags**: cs.AI cs.LG cs.SI 



### Stacking Your Transformers: A Closer Look at Model Growth for Efficient   LLM Pre-Training
**Authors**: Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, Jie Fu

**Updated**: 2024-10-22T10:31:59Z

**Summary**: LLMs are computationally expensive to pre-train due to their large scale. Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored. This work identifies three critical $\underline{\textit{O}}$bstacles: ($\textit{O}$1) lack of comprehensive evaluation, ($\textit{O}$2) untested viability for scaling, and ($\textit{O}$3) lack of empirical guidelines. To tackle $\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting. Our findings reveal that a depthwise stacking operator, called $G_{\text{stack}}$, exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\text{stack}}$ to address $\textit{O}$2 and $\textit{O}$3. For $\textit{O}$2 (untested scalability), our study shows that $G_{\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens. For example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\text{stack}}$ model converges to the same loss with 194B tokens, resulting in a 54.6\% speedup. We further address $\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\text{stack}}$, making it practical in general LLM pre-training. We also provide in-depth discussions and comprehensive ablation studies of $G_{\text{stack}}$. Our code and pre-trained model are available at https://llm-stacking.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2405.15319v2),  [pdf](http://arxiv.org/pdf/2405.15319v2)

**Tags**: cs.CL cs.AI 



### Insights from the Usage of the Ansible Lightspeed Code Completion   Service
**Authors**: Priyam Sahoo, Saurabh Pujar, Ganesh Nalawade, Richard Gebhardt, Louis Mandel, Luca Buratti

**Updated**: 2024-10-22T10:30:19Z

**Summary**: The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.   In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.   To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context.

**Link**: [arxiv](http://arxiv.org/abs/2402.17442v4),  [pdf](http://arxiv.org/pdf/2402.17442v4)

**Tags**: cs.SE cs.AI cs.PL 



### Federated Causal Inference: Multi-Centric ATE Estimation beyond   Meta-Analysis
**Authors**: Rmi Khellaf, Aurlien Bellet, Julie Josse

**Updated**: 2024-10-22T10:19:17Z

**Summary**: We study Federated Causal Inference, an approach to estimate treatment effects from decentralized data across centers. We compare three classes of Average Treatment Effect (ATE) estimators derived from the Plug-in G-Formula, ranging from simple meta-analysis to one-shot and multi-shot federated learning, the latter leveraging the full data to learn the outcome model (albeit requiring more communication). Focusing on Randomized Controlled Trials (RCTs), we derive the asymptotic variance of these estimators for linear models. Our results provide practical guidance on selecting the appropriate estimator for various scenarios, including heterogeneity in sample sizes, covariate distributions, treatment assignment schemes, and center effects. We validate these findings with a simulation study.

**Link**: [arxiv](http://arxiv.org/abs/2410.16870v1),  [pdf](http://arxiv.org/pdf/2410.16870v1)

**Tags**: stat.ML cs.LG math.ST stat.TH 



### Good Parenting is all you need -- Multi-agentic LLM Hallucination   Mitigation
**Authors**: Ted Kwartler, Matthew Berman, Alan Aqrawi

**Updated**: 2024-10-22T10:12:00Z

**Summary**: This study explores the ability of Large Language Model (LLM) agents to detect and correct hallucinations in AI-generated content. A primary agent was tasked with creating a blog about a fictional Danish artist named Flipfloppidy, which was then reviewed by another agent for factual inaccuracies. Most LLMs hallucinated the existence of this artist. Across 4,900 test runs involving various combinations of primary and reviewing agents, advanced AI models such as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in identifying hallucinations and successfully revised outputs in 85% to 100% of cases following feedback. These findings underscore the potential of advanced AI models to significantly enhance the accuracy and reliability of generated content, providing a promising approach to improving AI workflow orchestration.

**Link**: [arxiv](http://arxiv.org/abs/2410.14262v2),  [pdf](http://arxiv.org/pdf/2410.14262v2)

**Tags**: cs.CR cs.CL 



### Accounting for Numerical-Relativity Calibration Uncertainty in   Gravitational-Wave Modeling and Inference
**Authors**: Lorenzo Pompili, Alessandra Buonanno, Michael Prrer

**Updated**: 2024-10-22T09:52:20Z

**Summary**: The increasing sensitivity of current and upcoming gravitational-wave (GW) detectors poses stringent requirements on the accuracy of the GW models used for data analysis. If these requirements are not met, systematic errors could dominate over statistical uncertainties, hindering our ability to extract astrophysical and cosmological information, and conduct precise tests of General Relativity. In this work, we present a novel method to mitigate waveform-systematic errors, by incorporating and marginalizing over waveform-uncertainty estimates, which are modeled as probability distributions for the numerical-relativity calibration parameters of effective-one-body waveform models. By analyzing simulated GW signals of loud ``golden'' binary--black-hole systems, we show that our method significantly reduces biases in the recovered parameters, highlighting its potential to improve the robustness of GW parameter estimation with upcoming observing runs and next-generation ground-based facilities, such as the Einstein Telescope and Cosmic Explorer.

**Link**: [arxiv](http://arxiv.org/abs/2410.16859v1),  [pdf](http://arxiv.org/pdf/2410.16859v1)

**Tags**: gr-qc 



### CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian   Product Routing in Mixture-of-Experts
**Authors**: Zhenpeng Su, Xing Wu, Zijia Lin, Yizhe Xiong, Minxuan Lv, Guangyuan Ma, Hui Chen, Songlin Hu, Guiguang Ding

**Updated**: 2024-10-22T09:37:45Z

**Summary**: Large language models (LLM) have been attracting much attention from the community recently, due to their remarkable performance in all kinds of downstream tasks. According to the well-known scaling law, scaling up a dense LLM enhances its capabilities, but also significantly increases the computational complexity. Mixture-of-Experts (MoE) models address that by allowing the model size to grow without substantially raising training or inference costs. Yet MoE models face challenges regarding knowledge sharing among experts, making their performance somehow sensitive to routing accuracy. To tackle that, previous works introduced shared experts and combined their outputs with those of the top $K$ routed experts in an ``addition'' manner. In this paper, inspired by collective matrix factorization to learn shared knowledge among data, we propose CartesianMoE, which implements more effective knowledge sharing among experts in more like a ``multiplication'' manner. Extensive experimental results indicate that CartesianMoE outperforms previous MoE models for building LLMs, in terms of both perplexity and downstream task performance. And we also find that CartesianMoE achieves better expert routing robustness.

**Link**: [arxiv](http://arxiv.org/abs/2410.16077v2),  [pdf](http://arxiv.org/pdf/2410.16077v2)

**Tags**: cs.LG cs.CL 



### ETHIC: Evaluating Large Language Models on Long-Context Tasks with High   Information Coverage
**Authors**: Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang

**Updated**: 2024-10-22T09:35:42Z

**Summary**: Recent advancements in large language models (LLM) capable of processing extremely long texts highlight the need for a dedicated evaluation benchmark to assess their long-context capabilities. However, existing methods, like the needle-in-a-haystack test, do not effectively assess whether these models fully utilize contextual information, raising concerns about the reliability of current evaluation techniques. To thoroughly examine the effectiveness of existing benchmarks, we introduce a new metric called information coverage (IC), which quantifies the proportion of the input context necessary for answering queries. Our findings indicate that current benchmarks exhibit low IC; although the input context may be extensive, the actual usable context is often limited. To address this, we present ETHIC, a novel benchmark designed to assess LLMs' ability to leverage the entire context. Our benchmark comprises 2,648 test instances spanning four long-context tasks with high IC scores in the domains of books, debates, medicine, and law. Our evaluations reveal significant performance drops in contemporary LLMs, highlighting a critical challenge in managing long contexts. Our benchmark is available at https://github.com/dmis-lab/ETHIC.

**Link**: [arxiv](http://arxiv.org/abs/2410.16848v1),  [pdf](http://arxiv.org/pdf/2410.16848v1)

**Tags**: cs.CL 



### Critical Phase Transition in Large Language Models
**Authors**: Kai Nakaishi, Yoshihiko Nishikawa, Koji Hukushima

**Updated**: 2024-10-22T09:32:17Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive performance. To understand their behaviors, we need to consider the fact that LLMs sometimes show qualitative changes. The natural world also presents such changes called phase transitions, which are defined by singular, divergent statistical quantities. Therefore, an intriguing question is whether qualitative changes in LLMs are phase transitions. In this work, we have conducted extensive analysis on texts generated by LLMs and suggested that a phase transition occurs in LLMs when varying the temperature parameter. Specifically, statistical quantities have divergent properties just at the point between the low-temperature regime, where LLMs generate sentences with clear repetitive structures, and the high-temperature regime, where generated sentences are often incomprehensible. In addition, critical behaviors near the phase transition point, such as a power-law decay of correlation and slow convergence toward the stationary state, are similar to those in natural languages. Our results suggest a meaningful analogy between LLMs and natural phenomena.

**Link**: [arxiv](http://arxiv.org/abs/2406.05335v2),  [pdf](http://arxiv.org/pdf/2406.05335v2)

**Tags**: cond-mat.dis-nn cs.LG 



## Keyword: LLM Deployment 
 ### Towards Reliable Evaluation of Behavior Steering Interventions in LLMs
**Authors**: Itamar Pres, Laura Ruis, Ekdeep Singh Lubana, David Krueger

**Updated**: 2024-10-22T17:59:39Z

**Summary**: Representation engineering methods have recently shown promise for enabling efficient steering of model behavior. However, evaluation pipelines for these methods have primarily relied on subjective demonstrations, instead of quantitative, objective metrics. We aim to take a step towards addressing this issue by advocating for four properties missing from current evaluations: (i) contexts sufficiently similar to downstream tasks should be used for assessing intervention quality; (ii) model likelihoods should be accounted for; (iii) evaluations should allow for standardized comparisons across different target behaviors; and (iv) baseline comparisons should be offered. We introduce an evaluation pipeline grounded in these criteria, offering both a quantitative and visual analysis of how effectively a given method works. We use this pipeline to evaluate two representation engineering methods on how effectively they can steer behaviors such as truthfulness and corrigibility, finding that some interventions are less effective than previously reported.

**Link**: [arxiv](http://arxiv.org/abs/2410.17245v1),  [pdf](http://arxiv.org/pdf/2410.17245v1)

**Tags**: cs.AI cs.CL 



### SELA: Tree-Search Enhanced LLM Agents for Automated Machine Learning
**Authors**: Yizhou Chi, Yizhang Lin, Sirui Hong, Duyi Pan, Yaying Fei, Guanghao Mei, Bangbang Liu, Tianqi Pang, Jacky Kwok, Ceyao Zhang, Bang Liu, Chenglin Wu

**Updated**: 2024-10-22T17:56:08Z

**Summary**: Automated Machine Learning (AutoML) approaches encompass traditional methods that optimize fixed pipelines for model selection and ensembling, as well as newer LLM-based frameworks that autonomously build pipelines. While LLM-based agents have shown promise in automating machine learning tasks, they often generate low-diversity and suboptimal code, even after multiple iterations. To overcome these limitations, we introduce Tree-Search Enhanced LLM Agents (SELA), an innovative agent-based system that leverages Monte Carlo Tree Search (MCTS) to optimize the AutoML process. By representing pipeline configurations as trees, our framework enables agents to conduct experiments intelligently and iteratively refine their strategies, facilitating a more effective exploration of the machine learning solution space. This novel approach allows SELA to discover optimal pathways based on experimental feedback, improving the overall quality of the solutions. In an extensive evaluation across 20 machine learning datasets, we compare the performance of traditional and agent-based AutoML methods, demonstrating that SELA achieves a win rate of 65% to 80% against each baseline across all datasets. These results underscore the significant potential of agent-based strategies in AutoML, offering a fresh perspective on tackling complex machine learning challenges.

**Link**: [arxiv](http://arxiv.org/abs/2410.17238v1),  [pdf](http://arxiv.org/pdf/2410.17238v1)

**Tags**: cs.AI cs.CL cs.LG cs.SE 



### Large Language Models Empowered Personalized Web Agents
**Authors**: Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, Tat-Seng Chua

**Updated**: 2024-10-22T17:54:45Z

**Summary**: Web agents have emerged as a promising direction to automate Web task completion based on user instructions, significantly enhancing user experience. Recently, Web agents have evolved from traditional agents to Large Language Models (LLMs)-based Web agents. Despite their success, existing LLM-based Web agents overlook the importance of personalized data (e.g., user profiles and historical Web behaviors) in assisting the understanding of users' personalized instructions and executing customized actions. To overcome the limitation, we first formulate the task of LLM-empowered personalized Web agents, which integrate personalized data and user instructions to personalize instruction comprehension and action execution. To address the absence of a comprehensive evaluation benchmark, we construct a Personalized Web Agent Benchmark (PersonalWAB), featuring user instructions, personalized user data, Web functions, and two evaluation paradigms across three personalized Web tasks. Moreover, we propose a Personalized User Memory-enhanced Alignment (PUMA) framework to adapt LLMs to the personalized Web agent task. PUMA utilizes a memory bank with a task-specific retrieval strategy to filter relevant historical Web behaviors. Based on the behaviors, PUMA then aligns LLMs for personalized action execution through fine-tuning and direct preference optimization. Extensive experiments validate the superiority of PUMA over existing Web agents on PersonalWAB.

**Link**: [arxiv](http://arxiv.org/abs/2410.17236v1),  [pdf](http://arxiv.org/pdf/2410.17236v1)

**Tags**: cs.CL cs.AI cs.IR 



### Fine-Tuning Large Language Models to Appropriately Abstain with Semantic   Entropy
**Authors**: Benedict Aaron Tjandra, Muhammed Razzak, Jannik Kossen, Kunal Handa, Yarin Gal

**Updated**: 2024-10-22T17:54:03Z

**Summary**: Large Language Models (LLMs) are known to hallucinate, whereby they generate plausible but inaccurate text. This phenomenon poses significant risks in critical applications, such as medicine or law, necessitating robust hallucination mitigation strategies. While recent works have proposed fine-tuning methods to teach LLMs to abstain from answering questions beyond their knowledge or capabilities, these methods rely on the existence of ground-truth labels or are limited to short-form responses. To address these limitations, we propose fine-tuning using semantic entropy, an uncertainty measure derived from introspection into the model which does not require external labels. We demonstrate that our approach matches or outperforms models fine-tuned using prior work and achieves strong performance for both short and long-form generations on a range of datasets.

**Link**: [arxiv](http://arxiv.org/abs/2410.17234v1),  [pdf](http://arxiv.org/pdf/2410.17234v1)

**Tags**: cs.CL cs.LG 



### Few-shot In-Context Preference Learning Using Large Language Models
**Authors**: Chao Yu, Hong Lu, Jiaxuan Gao, Qixin Tan, Xinting Yang, Yu Wang, Yi Wu, Eugene Vinitsky

**Updated**: 2024-10-22T17:53:34Z

**Summary**: Designing reward functions is a core component of reinforcement learning but can be challenging for truly complex behavior. Reinforcement Learning from Human Feedback (RLHF) has been used to alleviate this challenge by replacing a hand-coded reward function with a reward function learned from preferences. However, it can be exceedingly inefficient to learn these rewards as they are often learned tabula rasa. We investigate whether Large Language Models (LLMs) can reduce this query inefficiency by converting an iterative series of human preferences into code representing the rewards. We propose In-Context Preference Learning (ICPL), a method that uses the grounding of an LLM to accelerate learning reward functions from preferences. ICPL takes the environment context and task description, synthesizes a set of reward functions, and then repeatedly updates the reward functions using human rankings of videos of the resultant policies. Using synthetic preferences, we demonstrate that ICPL is orders of magnitude more efficient than RLHF and is even competitive with methods that use ground-truth reward functions instead of preferences. Finally, we perform a series of human preference-learning trials and observe that ICPL extends beyond synthetic settings and can work effectively with humans-in-the-loop. Additional information and videos are provided at https://sites.google.com/view/few-shot-icpl/home.

**Link**: [arxiv](http://arxiv.org/abs/2410.17233v1),  [pdf](http://arxiv.org/pdf/2410.17233v1)

**Tags**: cs.AI cs.LG 



### Context-aware Prompt Tuning: Advancing In-Context Learning with   Adversarial Methods
**Authors**: Tsachi Blau, Moshe Kimhi, Yonatan Belinkov, Alexander Bronstein, Chaim Baskin

**Updated**: 2024-10-22T17:45:47Z

**Summary**: Fine-tuning Large Language Models (LLMs) typically involves updating at least a few billions of parameters. A more parameter-efficient approach is Prompt Tuning (PT), which updates only a few learnable tokens, and differently, In-Context Learning (ICL) adapts the model to a new task by simply including examples in the input without any training. When applying optimization-based methods, such as fine-tuning and PT for few-shot learning, the model is specifically adapted to the small set of training examples, whereas ICL leaves the model unchanged. This distinction makes traditional learning methods more prone to overfitting; in contrast, ICL is less sensitive to the few-shot scenario. While ICL is not prone to overfitting, it does not fully extract the information that exists in the training examples. This work introduces Context-aware Prompt Tuning (CPT), a method inspired by ICL, PT, and adversarial attacks. We build on the ICL strategy of concatenating examples before the input, but we extend this by PT-like learning, refining the context embedding through iterative optimization to extract deeper insights from the training examples. We carefully modify specific context tokens, considering the unique structure of input and output formats. Inspired by adversarial attacks, we adjust the input based on the labels present in the context, focusing on minimizing, rather than maximizing, the loss. Moreover, we apply a projected gradient descent algorithm to keep token embeddings close to their original values, under the assumption that the user-provided data is inherently valuable. Our method has been shown to achieve superior accuracy across multiple classification tasks using various LLM models.

**Link**: [arxiv](http://arxiv.org/abs/2410.17222v1),  [pdf](http://arxiv.org/pdf/2410.17222v1)

**Tags**: cs.CL 



### Exploring Possibilities of AI-Powered Legal Assistance in Bangladesh   through Large Language Modeling
**Authors**: Azmine Toushik Wasi, Wahid Faisal, Mst Rafia Islam, Mahathir Mohammad Bappy

**Updated**: 2024-10-22T17:34:59Z

**Summary**: Purpose: Bangladesh's legal system struggles with major challenges like delays, complexity, high costs, and millions of unresolved cases, which deter many from pursuing legal action due to lack of knowledge or financial constraints. This research seeks to develop a specialized Large Language Model (LLM) to assist in the Bangladeshi legal system. Methods: We created UKIL-DB-EN, an English corpus of Bangladeshi legal documents, by collecting and scraping data on various legal acts. We fine-tuned the GPT-2 model on this dataset to develop GPT2-UKIL-EN, an LLM focused on providing legal assistance in English. Results: The model was rigorously evaluated using semantic assessments, including case studies supported by expert opinions. The evaluation provided promising results, demonstrating the potential for the model to assist in legal matters within Bangladesh. Conclusion: Our work represents the first structured effort toward building an AI-based legal assistant for Bangladesh. While the results are encouraging, further refinements are necessary to improve the model's accuracy, credibility, and safety. This is a significant step toward creating a legal AI capable of serving the needs of a population of 180 million.

**Link**: [arxiv](http://arxiv.org/abs/2410.17210v1),  [pdf](http://arxiv.org/pdf/2410.17210v1)

**Tags**: cs.CL cs.AI cs.CY 



### EPContrast: Effective Point-level Contrastive Learning for Large-scale   Point Cloud Understanding
**Authors**: Zhiyi Pan, Guoqing Liu, Wei Gao, Thomas H. Li

**Updated**: 2024-10-22T17:27:16Z

**Summary**: The acquisition of inductive bias through point-level contrastive learning holds paramount significance in point cloud pre-training. However, the square growth in computational requirements with the scale of the point cloud poses a substantial impediment to the practical deployment and execution. To address this challenge, this paper proposes an Effective Point-level Contrastive Learning method for large-scale point cloud understanding dubbed \textbf{EPContrast}, which consists of AGContrast and ChannelContrast. In practice, AGContrast constructs positive and negative pairs based on asymmetric granularity embedding, while ChannelContrast imposes contrastive supervision between channel feature maps. EPContrast offers point-level contrastive loss while concurrently mitigating the computational resource burden. The efficacy of EPContrast is substantiated through comprehensive validation on S3DIS and ScanNetV2, encompassing tasks such as semantic segmentation, instance segmentation, and object detection. In addition, rich ablation experiments demonstrate remarkable bias induction capabilities under label-efficient and one-epoch training settings.

**Link**: [arxiv](http://arxiv.org/abs/2410.17207v1),  [pdf](http://arxiv.org/pdf/2410.17207v1)

**Tags**: cs.CV 



### ACPBench: Reasoning about Action, Change, and Planning
**Authors**: Harsha Kokel, Michael Katz, Kavitha Srinivas, Shirin Sohrabi

**Updated**: 2024-10-22T17:16:17Z

**Summary**: There is an increasing body of work using Large Language Models (LLMs) as agents for orchestrating workflows and making decisions in domains that require planning and multi-step reasoning. As a result, it is imperative to evaluate LLMs on core skills required for planning. In this work, we present ACPBench, a benchmark for evaluating the reasoning tasks in the field of planning. The benchmark consists of 7 reasoning tasks over 13 planning domains. The collection is constructed from planning domains described in a formal language. This allows us to synthesize problems with provably correct solutions across many tasks and domains. Further, it allows us the luxury of scale without additional human effort, i.e., many additional problems can be created automatically. Our extensive evaluation of 22 LLMs and OpenAI o1 reasoning models highlights the significant gap in the reasoning capability of the LLMs. Our findings with OpenAI o1, a multi-turn reasoning model, reveal significant gains in performance on multiple-choice questions, yet surprisingly, no notable progress is made on boolean questions.   The ACPBench collection is available at https://ibm.github.io/ACPBench.

**Link**: [arxiv](http://arxiv.org/abs/2410.05669v2),  [pdf](http://arxiv.org/pdf/2410.05669v2)

**Tags**: cs.AI 



### VoiceBench: Benchmarking LLM-Based Voice Assistants
**Authors**: Yiming Chen, Xianghu Yue, Chen Zhang, Xiaoxue Gao, Robby T. Tan, Haizhou Li

**Updated**: 2024-10-22T17:15:20Z

**Summary**: Building on the success of large language models (LLMs), recent advancements such as GPT-4o have enabled real-time speech interactions through LLM-based voice assistants, offering a significantly improved user experience compared to traditional text-based interactions. However, the absence of benchmarks designed to evaluate these speech interaction capabilities has hindered progress of LLM-based voice assistants development. Current evaluations focus primarily on automatic speech recognition (ASR) or general knowledge evaluation with clean speeches, neglecting the more intricate, real-world scenarios that involve diverse speaker characteristics, environmental and content factors. To address this, we introduce VoiceBench, the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants. VoiceBench also includes both real and synthetic spoken instructions that incorporate the above three key real-world variations. Extensive experiments reveal the limitations of current LLM-based voice assistant models and offer valuable insights for future research and development in this field.

**Link**: [arxiv](http://arxiv.org/abs/2410.17196v1),  [pdf](http://arxiv.org/pdf/2410.17196v1)

**Tags**: cs.CL cs.AI cs.SD eess.AS 



### Non-myopic Generation of Language Model for Reasoning and Planning
**Authors**: Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, Lingpeng Kong

**Updated**: 2024-10-23T07:02:09Z

**Summary**: Large Language Models have demonstrated remarkable abilities in reasoning and planning by breaking down complex problems into sequential steps. Despite their success in various domains like mathematical problem-solving and coding, LLMs face challenges in ensuring reliable and optimal planning due to their inherent myopic nature of autoregressive decoding. This paper revisits LLM reasoning from an optimal-control perspective, proposing a novel method, Predictive-Decoding, that leverages Model Predictive Control to enhance planning accuracy. By re-weighting LLM distributions based on foresight trajectories, Predictive-Decoding aims to mitigate early errors and promote non-myopic planning. Our experiments show significant improvements in a wide range of tasks for math, coding, and agents. Furthermore, Predictive-Decoding demonstrates computational efficiency, outperforming search baselines with reduced computational resources. This study provides insights into optimizing LLM planning capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2410.17195v2),  [pdf](http://arxiv.org/pdf/2410.17195v2)

**Tags**: cs.AI cs.CL 



### Minimum-Violation Temporal Logic Planning for Heterogeneous Robots under   Robot Skill Failures
**Authors**: Samarth Kalluraya, Beichen Zhou, Yiannis Kantaros

**Updated**: 2024-10-22T17:09:28Z

**Summary**: In this paper, we consider teams of robots with heterogeneous skills (e.g., sensing and manipulation) tasked with collaborative missions described by Linear Temporal Logic (LTL) formulas. These LTL-encoded tasks require robots to apply their skills to specific regions and objects in a temporal and logical order. While existing temporal logic planning algorithms can synthesize correct-by-construction paths, they typically lack reactivity to unexpected failures of robot skills, which can compromise mission performance. This paper addresses this challenge by proposing a reactive LTL planning algorithm that adapts to unexpected failures during deployment. Specifically, the proposed algorithm reassigns sub-tasks to robots based on their functioning skills and locally revises team plans to accommodate these new assignments and ensure mission completion. The main novelty of the proposed algorithm is its ability to handle cases where mission completion becomes impossible due to limited functioning robots. Instead of reporting mission failure, the algorithm strategically prioritizes the most crucial sub-tasks and locally revises the team's plans, as per user-specified priorities, to minimize mission violations. We provide theoretical conditions under which the proposed framework computes the minimum violation task reassignments and team plans. We provide numerical and hardware experiments to demonstrate the efficiency of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2410.17188v1),  [pdf](http://arxiv.org/pdf/2410.17188v1)

**Tags**: cs.RO 



### Can Large Language Models Identify Authorship?
**Authors**: Baixiang Huang, Canyu Chen, Kai Shu

**Updated**: 2024-10-22T17:07:14Z

**Summary**: The ability to accurately identify authorship is crucial for verifying content authenticity and mitigating misinformation. Large Language Models (LLMs) have demonstrated an exceptional capacity for reasoning and problem-solving. However, their potential in authorship analysis remains under-explored. Traditional studies have depended on hand-crafted stylistic features, whereas state-of-the-art approaches leverage text embeddings from pre-trained language models. These methods, which typically require fine-tuning on labeled data, often suffer from performance degradation in cross-domain applications and provide limited explainability. This work seeks to address three research questions: (1) Can LLMs perform zero-shot, end-to-end authorship verification effectively? (2) Are LLMs capable of accurately attributing authorship among multiple candidates authors (e.g., 10 and 20)? (3) Can LLMs provide explainability in authorship analysis, particularly through the role of linguistic features? Moreover, we investigate the integration of explicit linguistic features to guide LLMs in their reasoning processes. Our assessment demonstrates LLMs' proficiency in both tasks without the need for domain-specific fine-tuning, providing explanations into their decision making via a detailed analysis of linguistic features. This establishes a new benchmark for future research on LLM-based authorship analysis.

**Link**: [arxiv](http://arxiv.org/abs/2403.08213v2),  [pdf](http://arxiv.org/pdf/2403.08213v2)

**Tags**: cs.CL 



### The Impact of Large Language Models in Academia: from Writing to   Speaking
**Authors**: Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou

**Updated**: 2024-10-22T17:06:17Z

**Summary**: Large language models (LLMs) are increasingly impacting human society, particularly in textual information. Based on more than 30,000 papers and 1,000 presentations from machine learning conferences, we examined and compared the words used in writing and speaking, representing the first large-scale study of how LLMs influence the two main modes of verbal communication and expression within the same group of people. Our empirical results show that LLM-style words such as "significant" have been used more frequently in abstracts and oral presentations. The impact on speaking is beginning to emerge and is likely to grow in the future, calling attention to the implicit influence and ripple effect of LLMs on human society.

**Link**: [arxiv](http://arxiv.org/abs/2409.13686v2),  [pdf](http://arxiv.org/pdf/2409.13686v2)

**Tags**: cs.CL cs.AI cs.CY cs.DL cs.LG 



### Levels of AI Agents: from Rules to Large Language Models
**Authors**: Yu Huang

**Updated**: 2024-10-22T17:05:17Z

**Summary**: AI agents are defined as artificial entities to perceive the environment, make decisions and take actions. Inspired by the 6 levels of autonomous driving by Society of Automotive Engineers, the AI agents are also categorized based on utilities and strongness, as the following levels: L0, no AI, with tools taking into account perception plus actions; L1, using rule-based AI; L2, making rule-based AI replaced by IL/RL-based AI, with additional reasoning & decision making; L3, applying LLM-based AI instead of IL/RL-based AI, additionally setting up memory & reflection; L4, based on L3, facilitating autonomous learning & generalization; L5, based on L4, appending personality of emotion and character and collaborative behavior with multi-agents.

**Link**: [arxiv](http://arxiv.org/abs/2405.06643v2),  [pdf](http://arxiv.org/pdf/2405.06643v2)

**Tags**: cs.CL 



### LLMs left, right, and center: Assessing GPT's capabilities to label   political bias from web domains
**Authors**: Raphael Hernandes, Giulio Corsi

**Updated**: 2024-10-22T16:59:12Z

**Summary**: This research investigates whether OpenAI's GPT-4, a state-of-the-art large language model, can accurately classify the political bias of news sources based solely on their URLs. Given the subjective nature of political labels, third-party bias ratings like those from Ad Fontes Media, AllSides, and Media Bias/Fact Check (MBFC) are often used in research to analyze news source diversity. This study aims to determine if GPT-4 can replicate these human ratings on a seven-degree scale ("far-left" to "far-right"). The analysis compares GPT-4's classifications against MBFC's, and controls for website popularity using Open PageRank scores. Findings reveal a high correlation ($\text{Spearman's } \rho = .89$, $n = 5,877$, $p < 0.001$) between GPT-4's and MBFC's ratings, indicating the model's potential reliability. However, GPT-4 abstained from classifying approximately $\frac{2}{3}$ of the dataset. It is more likely to abstain from rating unpopular websites, which also suffer from less accurate assessments. The LLM tends to avoid classifying sources that MBFC considers to be centrist, resulting in more polarized outputs. Finally, this analysis shows a slight leftward skew in GPT's classifications compared to MBFC's. Therefore, while this paper suggests that while GPT-4 can be a scalable, cost-effective tool for political bias classification of news websites, its use should be as a complement to human judgment to mitigate biases.

**Link**: [arxiv](http://arxiv.org/abs/2407.14344v2),  [pdf](http://arxiv.org/pdf/2407.14344v2)

**Tags**: cs.CL cs.AI cs.CY 



### Improving Pinterest Search Relevance Using Large Language Models
**Authors**: Han Wang, Mukuntha Narayanan Sundararaman, Onur Gungor, Yu Xu, Krishna Kamath, Rakesh Chalasani, Kurchi Subhra Hazra, Jinfeng Rao

**Updated**: 2024-10-22T16:29:33Z

**Summary**: To improve relevance scoring on Pinterest Search, we integrate Large Language Models (LLMs) into our search relevance model, leveraging carefully designed text representations to predict the relevance of Pins effectively. Our approach uses search queries alongside content representations that include captions extracted from a generative visual language model. These are further enriched with link-based text data, historically high-quality engaged queries, user-curated boards, Pin titles and Pin descriptions, creating robust models for predicting search relevance. We use a semi-supervised learning approach to efficiently scale up the amount of training data, expanding beyond the expensive human labeled data available. By utilizing multilingual LLMs, our system extends training data to include unseen languages and domains, despite initial data and annotator expertise being confined to English. Furthermore, we distill from the LLM-based model into real-time servable model architectures and features. We provide comprehensive offline experimental validation for our proposed techniques and demonstrate the gains achieved through the final deployed system at scale.

**Link**: [arxiv](http://arxiv.org/abs/2410.17152v1),  [pdf](http://arxiv.org/pdf/2410.17152v1)

**Tags**: cs.IR cs.CL 



### Toolshed: Scale Tool-Equipped Agents with Advanced RAG-Tool Fusion and   Tool Knowledge Bases
**Authors**: Elias Lumer, Vamse Kumar Subbiah, James A. Burke, Pradeep Honaganahalli Basavaraju, Austin Huber

**Updated**: 2024-10-22T16:27:12Z

**Summary**: Recent advancements in tool-equipped Agents (LLMs) have enabled complex tasks like secure database interactions and multi-agent code development. However, scaling tool capacity beyond agent reasoning or model limits remains a challenge. In this paper, we address these challenges by introducing Toolshed Knowledge Bases, a tool knowledge base (vector database) designed to store enhanced tool representations and optimize tool selection for large-scale tool-equipped Agents. Additionally, we propose Advanced RAG-Tool Fusion, a novel ensemble of tool-applied advanced retrieval-augmented generation (RAG) techniques across the pre-retrieval, intra-retrieval, and post-retrieval phases, without requiring model fine-tuning. During pre-retrieval, tool documents are enhanced with key information and stored in the Toolshed Knowledge Base. Intra-retrieval focuses on query planning and transformation to increase retrieval accuracy. Post-retrieval refines the retrieved tool documents and enables self-reflection. Furthermore, by varying both the total number of tools (tool-M) an Agent has access to and the tool selection threshold (top-k), we address trade-offs between retrieval accuracy, agent performance, and token cost. Our approach achieves 46%, 56%, and 47% absolute improvements on the ToolE single-tool, ToolE multi-tool and Seal-Tools benchmark datasets, respectively (Recall@5).

**Link**: [arxiv](http://arxiv.org/abs/2410.14594v2),  [pdf](http://arxiv.org/pdf/2410.14594v2)

**Tags**: cs.CL 



### LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances   Model Merging
**Authors**: Ke Wang, Nikolaos Dimitriadis, Alessandro Favero, Guillermo Ortiz-Jimenez, Francois Fleuret, Pascal Frossard

**Updated**: 2024-10-22T16:26:05Z

**Summary**: Large pre-trained models exhibit impressive zero-shot performance across diverse tasks, but fine-tuning often leads to catastrophic forgetting, where improvements on a target domain degrade generalization on other tasks. To address this challenge, we introduce LiNeS, Layer-increasing Network Scaling, a post-training editing technique designed to preserve pre-trained generalization while enhancing fine-tuned task performance. LiNeS scales parameter updates linearly based on their layer depth within the network, maintaining shallow layers close to their pre-trained values to preserve general features while allowing deeper layers to retain task-specific representations. We further extend this approach to multi-task model merging scenarios, where layer-wise scaling of merged parameters reduces negative task interference. LiNeS demonstrates significant improvements in both single-task and multi-task settings across various benchmarks in vision and natural language processing. It mitigates forgetting, enhances out-of-distribution generalization, integrates seamlessly with existing multi-task model merging baselines improving their performance across benchmarks and model sizes, and can boost generalization when merging LLM policies aligned with different rewards via RLHF. Importantly, our method is simple to implement and complementary to many existing techniques.

**Link**: [arxiv](http://arxiv.org/abs/2410.17146v1),  [pdf](http://arxiv.org/pdf/2410.17146v1)

**Tags**: cs.LG cs.CV 



### Can General-Purpose Large Language Models Generalize to English-Thai   Machine Translation ?
**Authors**: Jirat Chiaranaipanich, Naiyarat Hanmatheekuna, Jitkapat Sawatphol, Krittamate Tiankanon, Jiramet Kinchagawat, Amrest Chinkamol, Parinthapat Pengpun, Piyalitt Ittichaiwong, Peerat Limkonchotiwat

**Updated**: 2024-10-22T16:26:03Z

**Summary**: Large language models (LLMs) perform well on common tasks but struggle with generalization in low-resource and low-computation settings. We examine this limitation by testing various LLMs and specialized translation models on English-Thai machine translation and code-switching datasets. Our findings reveal that under more strict computational constraints, such as 4-bit quantization, LLMs fail to translate effectively. In contrast, specialized models, with comparable or lower computational requirements, consistently outperform LLMs. This underscores the importance of specialized models for maintaining performance under resource constraints.

**Link**: [arxiv](http://arxiv.org/abs/2410.17145v1),  [pdf](http://arxiv.org/pdf/2410.17145v1)

**Tags**: cs.CL cs.AI cs.LG 



### YOLO-TS: Real-Time Traffic Sign Detection with Enhanced Accuracy Using   Optimized Receptive Fields and Anchor-Free Fusion
**Authors**: Junzhou Chen, Heqiang Huang, Ronghui Zhang, Nengchao Lyu, Yanyong Guo, Hong-Ning Dai, Hong Yan

**Updated**: 2024-10-22T16:19:55Z

**Summary**: Ensuring safety in both autonomous driving and advanced driver-assistance systems (ADAS) depends critically on the efficient deployment of traffic sign recognition technology. While current methods show effectiveness, they often compromise between speed and accuracy. To address this issue, we present a novel real-time and efficient road sign detection network, YOLO-TS. This network significantly improves performance by optimizing the receptive fields of multi-scale feature maps to align more closely with the size distribution of traffic signs in various datasets. Moreover, our innovative feature-fusion strategy, leveraging the flexibility of Anchor-Free methods, allows for multi-scale object detection on a high-resolution feature map abundant in contextual information, achieving remarkable enhancements in both accuracy and speed. To mitigate the adverse effects of the grid pattern caused by dilated convolutions on the detection of smaller objects, we have devised a unique module that not only mitigates this grid effect but also widens the receptive field to encompass an extensive range of spatial contextual information, thus boosting the efficiency of information usage. Evaluation on challenging public datasets, TT100K and CCTSDB2021, demonstrates that YOLO-TS surpasses existing state-of-the-art methods in terms of both accuracy and speed. The code for our method will be available.

**Link**: [arxiv](http://arxiv.org/abs/2410.17144v1),  [pdf](http://arxiv.org/pdf/2410.17144v1)

**Tags**: cs.CV 



### Towards Automated Penetration Testing: Introducing LLM Benchmark,   Analysis, and Improvements
**Authors**: Isamu Isozaki, Manil Shrestha, Rick Console, Edward Kim

**Updated**: 2024-10-22T16:18:41Z

**Summary**: Hacking poses a significant threat to cybersecurity, inflicting billions of dollars in damages annually. To mitigate these risks, ethical hacking, or penetration testing, is employed to identify vulnerabilities in systems and networks. Recent advancements in large language models (LLMs) have shown potential across various domains, including cybersecurity. However, there is currently no comprehensive, open, end-to-end automated penetration testing benchmark to drive progress and evaluate the capabilities of these models in security contexts. This paper introduces a novel open benchmark for LLM-based automated penetration testing, addressing this critical gap. We first evaluate the performance of LLMs, including GPT-4o and Llama 3.1-405B, using the state-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1 demonstrates an edge over GPT-4o, both models currently fall short of performing fully automated, end-to-end penetration testing. Next, we advance the state-of-the-art and present ablation studies that provide insights into improving the PentestGPT tool. Our research illuminates the challenges LLMs face in each aspect of Pentesting, e.g. enumeration, exploitation, and privilege escalation. This work contributes to the growing body of knowledge on AI-assisted cybersecurity and lays the foundation for future research in automated penetration testing using large language models.

**Link**: [arxiv](http://arxiv.org/abs/2410.17141v1),  [pdf](http://arxiv.org/pdf/2410.17141v1)

**Tags**: cs.CR cs.AI 



### Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary   Resolution
**Authors**: Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, Yongming Rao

**Updated**: 2024-10-22T16:17:13Z

**Summary**: Visual data comes in various forms, ranging from small icons of just a few pixels to long videos spanning hours. Existing multi-modal LLMs usually standardize these diverse visual inputs to a fixed resolution for visual encoders and yield similar numbers of tokens for LLMs. This approach is non-optimal for multimodal understanding and inefficient for processing inputs with long and short visual contents. To solve the problem, we propose Oryx, a unified multimodal architecture for the spatial-temporal understanding of images, videos, and multi-view 3D scenes. Oryx offers an on-demand solution to seamlessly and efficiently process visual inputs with arbitrary spatial sizes and temporal lengths through two core innovations: 1) a pre-trained OryxViT model that can encode images at any resolution into LLM-friendly visual representations; 2) a dynamic compressor module that supports 1x to 16x compression on visual tokens by request. These design features enable Oryx to accommodate extremely long visual contexts, such as videos, with lower resolution and high compression while maintaining high recognition precision for tasks like document understanding with native resolution and no compression. Beyond the architectural improvements, enhanced data curation and specialized training on long-context retrieval and spatial-aware data help Oryx achieve strong capabilities in image, video, and 3D multimodal understanding simultaneously. Our work is open-sourced at https://github.com/Oryx-mllm/Oryx.

**Link**: [arxiv](http://arxiv.org/abs/2409.12961v2),  [pdf](http://arxiv.org/pdf/2409.12961v2)

**Tags**: cs.CV 



### PAPILLON: PrivAcy Preservation from Internet-based and Local Language   MOdel ENsembles
**Authors**: Li Siyan, Vethavikashini Chithrra Raghuram, Omar Khattab, Julia Hirschberg, Zhou Yu

**Updated**: 2024-10-22T16:00:26Z

**Summary**: Users can divulge sensitive information to proprietary LLM providers, raising significant privacy concerns. While open-source models, hosted locally on the user's machine, alleviate some concerns, models that users can host locally are often less capable than proprietary frontier models. Toward preserving user privacy while retaining the best quality, we propose Privacy-Conscious Delegation, a novel task for chaining API-based and local models. We utilize recent public collections of user-LLM interactions to construct a natural benchmark called PUPA, which contains personally identifiable information (PII). To study potential approaches, we devise PAPILLON, a multi-stage LLM pipeline that uses prompt optimization to address a simpler version of our task. Our best pipeline maintains high response quality for 85.5% of user queries while restricting privacy leakage to only 7.5%. We still leave a large margin to the generation quality of proprietary LLMs for future work. Our data and code will be available at https://github.com/siyan-sylvia-li/PAPILLON.

**Link**: [arxiv](http://arxiv.org/abs/2410.17127v1),  [pdf](http://arxiv.org/pdf/2410.17127v1)

**Tags**: cs.CR cs.CL 



### Exploring RL-based LLM Training for Formal Language Tasks with   Programmed Rewards
**Authors**: Alexander G. Padula, Dennis J. N. J. Soemers

**Updated**: 2024-10-22T15:59:58Z

**Summary**: Proximal Policy Optimization (PPO) is commonly used in Reinforcement Learning from Human Feedback to align large language models (LLMs) with downstream tasks. This paper investigates the feasibility of using PPO for direct reinforcement learning (RL) from explicitly programmed reward signals, as opposed to indirect learning from human feedback via an intermediary reward model. We focus on tasks expressed through formal languages, such as mathematics and programming, where explicit reward functions can be programmed to automatically assess the quality of generated outputs. We apply this approach to a sentiment alignment task, a simple arithmetic task, and a more complex game synthesis task. The sentiment alignment task replicates prior research and serves to validate our experimental setup. Our results show that pure RL-based training for the two formal language tasks is challenging, with success being limited even for the simple arithmetic task. We propose a novel batch-entropy regularization term to aid exploration, although training is not yet entirely stable. Our findings suggest that direct RL training of LLMs may be more suitable for relatively minor changes, such as alignment, than for learning new tasks altogether, even if an informative reward signal can be expressed programmatically.

**Link**: [arxiv](http://arxiv.org/abs/2410.17126v1),  [pdf](http://arxiv.org/pdf/2410.17126v1)

**Tags**: cs.CL cs.AI cs.LG 



### DepsRAG: Towards Agentic Reasoning and Planning for Software Dependency   Management
**Authors**: Mohannad Alhanahnah, Yazan Boshmaf

**Updated**: 2024-10-22T15:41:22Z

**Summary**: In the era of Large Language Models (LLMs) with their advanced capabilities, a unique opportunity arises to develop LLM-based digital assistant tools that can support software developers by facilitating comprehensive reasoning about software dependencies and open-source libraries before importing them. This reasoning process is daunting, mandating multiple specialized tools and dedicated expertise, each focusing on distinct aspects (e.g., security analysis tools may overlook design flaws such as circular dependencies, which hinder software maintainability). Creating a significant bottleneck in the software development lifecycle. In this paper, we introduce DepsRAG, a multi-agent framework designed to assist developers in reasoning about software dependencies. DepsRAG first constructs a comprehensive Knowledge Graph (KG) that includes both direct and transitive dependencies. Developers can interact with DepsRAG through a conversational interface, posing queries about the dependencies. DepsRAG employs Retrieval-Augmented Generation (RAG) to enhance these queries by retrieving relevant information from the KG as well as external sources, such as the Web and vulnerability databases, thus demonstrating its adaptability to novel scenarios. DepsRAG incorporates a Critic-Agent feedback loop to ensure the accuracy and clarity of LLM-generated responses. We evaluated DepsRAG using GPT-4-Turbo and Llama-3 on three multi-step reasoning tasks, observing a threefold increase in accuracy with the integration of the Critic-Agent mechanism. DepsRAG demo and implementation are available: https://github.com/Mohannadcse/DepsRAG.

**Link**: [arxiv](http://arxiv.org/abs/2405.20455v5),  [pdf](http://arxiv.org/pdf/2405.20455v5)

**Tags**: cs.SE 



### Enhancing Answer Attribution for Faithful Text Generation with Large   Language Models
**Authors**: Juraj Vladika, Luca Mlln, Florian Matthes

**Updated**: 2024-10-22T15:37:46Z

**Summary**: The increasing popularity of Large Language Models (LLMs) in recent years has changed the way users interact with and pose questions to AI-based conversational systems. An essential aspect for increasing the trustworthiness of generated LLM answers is the ability to trace the individual claims from responses back to relevant sources that support them, the process known as answer attribution. While recent work has started exploring the task of answer attribution in LLMs, some challenges still remain. In this work, we first perform a case study analyzing the effectiveness of existing answer attribution methods, with a focus on subtasks of answer segmentation and evidence retrieval. Based on the observed shortcomings, we propose new methods for producing more independent and contextualized claims for better retrieval and attribution. The new methods are evaluated and shown to improve the performance of answer attribution components. We end with a discussion and outline of future directions for the task.

**Link**: [arxiv](http://arxiv.org/abs/2410.17112v1),  [pdf](http://arxiv.org/pdf/2410.17112v1)

**Tags**: cs.CL cs.IR 



### Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations
**Authors**: Jiyi Li

**Updated**: 2024-10-22T15:22:58Z

**Summary**: The quality is a crucial issue for crowd annotations. Answer aggregation is an important type of solution. The aggregated answers estimated from multiple crowd answers to the same instance are the eventually collected annotations, rather than the individual crowd answers themselves. Recently, the capability of Large Language Models (LLMs) on data annotation tasks has attracted interest from researchers. Most of the existing studies mainly focus on the average performance of individual crowd workers; several recent works studied the scenarios of aggregation on categorical labels and LLMs used as label creators. However, the scenario of aggregation on text answers and the role of LLMs as aggregators are not yet well-studied. In this paper, we investigate the capability of LLMs as aggregators in the scenario of close-ended crowd text answer aggregation. We propose a human-LLM hybrid text answer aggregation method with a Creator-Aggregator Multi-Stage (CAMS) crowdsourcing framework. We make the experiments based on public crowdsourcing datasets. The results show the effectiveness of our approach based on the collaboration of crowd workers and LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17099v1),  [pdf](http://arxiv.org/pdf/2410.17099v1)

**Tags**: cs.CL cs.HC cs.LG 



### Do LLMs "know" internally when they follow instructions?
**Authors**: Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Shirley Ren, Udhay Nallasamy, Andy Miller, Kwan Ho Ryan Chan, Jaya Narain

**Updated**: 2024-10-22T15:20:00Z

**Summary**: Instruction-following is crucial for building AI agents with large language models (LLMs), as these models must adhere strictly to user-provided constraints and guidelines. However, LLMs often fail to follow even simple and clear instructions. To improve instruction-following behavior and prevent undesirable outputs, a deeper understanding of how LLMs' internal states relate to these outcomes is required. Our analysis of LLM internal states reveal a dimension in the input embedding space linked to successful instruction-following. We demonstrate that modifying representations along this dimension improves instruction-following success rates compared to random changes, without compromising response quality. Further investigation reveals that this dimension is more closely related to the phrasing of prompts rather than the inherent difficulty of the task or instructions. This discovery also suggests explanations for why LLMs sometimes fail to follow clear instructions and why prompt engineering is often effective, even when the content remains largely unchanged. This work provides insight into the internal workings of LLMs' instruction-following, paving the way for reliable LLM agents.

**Link**: [arxiv](http://arxiv.org/abs/2410.14516v2),  [pdf](http://arxiv.org/pdf/2410.14516v2)

**Tags**: cs.AI cs.CL 



### CLEAR: Towards Contextual LLM-Empowered Privacy Policy Analysis and Risk   Generation for Large Language Model Applications
**Authors**: Chaoran Chen, Daodao Zhou, Yanfang Ye, Toby Jia-jun Li, Yaxing Yao

**Updated**: 2024-10-22T15:17:08Z

**Summary**: The rise of end-user applications powered by large language models (LLMs), including both conversational interfaces and add-ons to existing graphical user interfaces (GUIs), introduces new privacy challenges. However, many users remain unaware of the risks. This paper explores methods to increase user awareness of privacy risks associated with LLMs in end-user applications. We conducted five co-design workshops to uncover user privacy concerns and their demand for contextual privacy information within LLMs. Based on these insights, we developed CLEAR (Contextual LLM-Empowered Privacy Policy Analysis and Risk Generation), a just-in-time contextual assistant designed to help users identify sensitive information, summarize relevant privacy policies, and highlight potential risks when sharing information with LLMs. We evaluated the usability and usefulness of CLEAR across in two example domains: ChatGPT and the Gemini plugin in Gmail. Our findings demonstrated that CLEAR is easy to use and improves user understanding of data practices and privacy risks. We also discussed LLM's duality in posing and mitigating privacy risks, offering design and policy implications.

**Link**: [arxiv](http://arxiv.org/abs/2410.13387v2),  [pdf](http://arxiv.org/pdf/2410.13387v2)

**Tags**: cs.HC 



### Do LLMs estimate uncertainty well in instruction-following?
**Authors**: Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain

**Updated**: 2024-10-22T15:16:14Z

**Summary**: Large language models (LLMs) could be valuable personal AI agents across various domains, provided they can precisely follow user instructions. However, recent studies have shown significant limitations in LLMs' instruction-following capabilities, raising concerns about their reliability in high-stakes applications. Accurately estimating LLMs' uncertainty in adhering to instructions is critical to mitigating deployment risks. We present, to our knowledge, the first systematic evaluation of the uncertainty estimation abilities of LLMs in the context of instruction-following. Our study identifies key challenges with existing instruction-following benchmarks, where multiple factors are entangled with uncertainty stems from instruction-following, complicating the isolation and comparison across methods and models. To address these issues, we introduce a controlled evaluation setup with two benchmark versions of data, enabling a comprehensive comparison of uncertainty estimation methods under various conditions. Our findings show that existing uncertainty methods struggle, particularly when models make subtle errors in instruction following. While internal model states provide some improvement, they remain inadequate in more complex scenarios. The insights from our controlled evaluation setups provide a crucial understanding of LLMs' limitations and potential for uncertainty estimation in instruction-following tasks, paving the way for more trustworthy AI agents.

**Link**: [arxiv](http://arxiv.org/abs/2410.14582v2),  [pdf](http://arxiv.org/pdf/2410.14582v2)

**Tags**: cs.AI cs.CL 



### AppPoet: Large Language Model based Android malware detection via   multi-view prompt engineering
**Authors**: Wenxiang Zhao, Juntao Wu, Zhaoyi Meng

**Updated**: 2024-10-22T15:12:37Z

**Summary**: Due to the vast array of Android applications, their multifarious functions and intricate behavioral semantics, attackers can adopt various tactics to conceal their genuine attack intentions within legitimate functions. However, numerous learning-based methods suffer from a limitation in mining behavioral semantic information, thus impeding the accuracy and efficiency of Android malware detection. Besides, the majority of existing learning-based methods are weakly interpretive and fail to furnish researchers with effective and readable detection reports. Inspired by the success of the Large Language Models (LLMs) in natural language understanding, we propose AppPoet, a LLM-assisted multi-view system for Android malware detection. Firstly, AppPoet employs a static method to comprehensively collect application features and formulate various observation views. Then, using our carefully crafted multi-view prompt templates, it guides the LLM to generate function descriptions and behavioral summaries for each view, enabling deep semantic analysis of the views. Finally, we collaboratively fuse the multi-view information to efficiently and accurately detect malware through a deep neural network (DNN) classifier and then generate the human-readable diagnostic reports. Experimental results demonstrate that our method achieves a detection accuracy of 97.15% and an F1 score of 97.21%, which is superior to the baseline methods. Furthermore, the case study evaluates the effectiveness of our generated diagnostic reports.

**Link**: [arxiv](http://arxiv.org/abs/2404.18816v3),  [pdf](http://arxiv.org/pdf/2404.18816v3)

**Tags**: cs.CR cs.SE 



### One Thousand and One Pairs: A "novel" challenge for long-context   language models
**Authors**: Marzena Karpinska, Katherine Thai, Kyle Lo, Tanya Goyal, Mohit Iyyer

**Updated**: 2024-10-22T15:09:58Z

**Summary**: Synthetic long-context LLM benchmarks (e.g., "needle-in-the-haystack") test only surface-level retrieval capabilities, but how well can long-context LLMs retrieve, synthesize, and reason over information across book-length inputs? We address this question by creating NoCha, a dataset of 1,001 minimally different pairs of true and false claims about 67 recently-published English fictional books, written by human readers of those books. In contrast to existing long-context benchmarks, our annotators confirm that the largest share of pairs in NoCha require global reasoning over the entire book to verify. Our experiments show that while human readers easily perform this task, it is enormously challenging for all ten long-context LLMs that we evaluate: no open-weight model performs above random chance (despite their strong performance on synthetic benchmarks), while GPT-4o achieves the highest accuracy at 55.8%. Further analysis reveals that (1) on average, models perform much better on pairs that require only sentence-level retrieval vs. global reasoning; (2) model-generated explanations for their decisions are often inaccurate even for correctly-labeled claims; and (3) models perform substantially worse on speculative fiction books that contain extensive world-building. The methodology proposed in NoCha allows for the evolution of the benchmark dataset and the easy analysis of future models.

**Link**: [arxiv](http://arxiv.org/abs/2406.16264v3),  [pdf](http://arxiv.org/pdf/2406.16264v3)

**Tags**: cs.CL cs.AI 



### SysBench: Can Large Language Models Follow System Messages?
**Authors**: Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, Bin Cui

**Updated**: 2024-10-22T15:07:35Z

**Summary**: Large Language Models (LLMs) have become instrumental across various applications, with the customization of these models to specific scenarios becoming increasingly critical. System message, a fundamental component of LLMs, is consist of carefully crafted instructions that guide the behavior of model to meet intended goals. Despite the recognized potential of system messages to optimize AI-driven solutions, there is a notable absence of a comprehensive benchmark for evaluating how well LLMs follow system messages. To fill this gap, we introduce SysBench, a benchmark that systematically analyzes system message following ability in terms of three limitations of existing LLMs: constraint violation, instruction misjudgement and multi-turn instability. Specifically, we manually construct evaluation dataset based on six prevalent types of constraints, including 500 tailor-designed system messages and multi-turn user conversations covering various interaction relationships. Additionally, we develop a comprehensive evaluation protocol to measure model performance. Finally, we conduct extensive evaluation across various existing LLMs, measuring their ability to follow specified constraints given in system messages. The results highlight both the strengths and weaknesses of existing models, offering key insights and directions for future research. The open source library SysBench is available at https://github.com/PKU-Baichuan-MLSystemLab/SysBench.

**Link**: [arxiv](http://arxiv.org/abs/2408.10943v2),  [pdf](http://arxiv.org/pdf/2408.10943v2)

**Tags**: cs.CL 



### FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI   Training Clusters
**Authors**: Hasibul Jamil, Abdul Alim, Laurent Schares, Pavlos Maniotis, Liran Schour, Ali Sydney, Abdullah Kayi, Tevfik Kosar, Bengi Karacali

**Updated**: 2024-10-22T14:56:50Z

**Summary**: The increasing complexity of AI workloads, especially distributed Large Language Model (LLM) training, places significant strain on the networking infrastructure of parallel data centers and supercomputing systems. While Equal-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths, hash collisions often lead to imbalanced network resource utilization and performance bottlenecks. This paper presents FlowTracer, a tool designed to analyze network path utilization and evaluate different routing strategies. FlowTracer aids in debugging network inefficiencies by providing detailed visibility into traffic distribution and helping to identify the root causes of performance degradation, such as issues caused by hash collisions. By offering flow-level insights, FlowTracer enables system operators to optimize routing, reduce congestion, and improve the performance of distributed AI workloads. We use a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to demonstrate how FlowTracer can be used to compare the flow imbalances of ECMP routing against a statically configured network. The example showcases a 30% reduction in imbalance, as measured by a new metric we introduce.

**Link**: [arxiv](http://arxiv.org/abs/2410.17078v1),  [pdf](http://arxiv.org/pdf/2410.17078v1)

**Tags**: cs.NI cs.DC 



### UnStar: Unlearning with Self-Taught Anti-Sample Reasoning for LLMs
**Authors**: Yash Sinha, Murari Mandal, Mohan Kankanhalli

**Updated**: 2024-10-22T14:30:03Z

**Summary**: The key components of machine learning are data samples for training, model for learning patterns, and loss function for optimizing accuracy. Analogously, unlearning can potentially be achieved through anti-data samples (or anti-samples), unlearning method, and reversed loss function. While prior research has explored unlearning methods and reversed loss functions, the potential of anti-samples remains largely untapped. In this paper, we introduce UnSTAR: Unlearning with Self-Taught Anti-Sample Reasoning for large language models (LLMs). Our contributions are threefold; first, we propose a novel concept of anti-sample-induced unlearning; second, we generate anti-samples by leveraging misleading rationales, which help reverse learned associations and accelerate the unlearning process; and third, we enable fine-grained targeted unlearning, allowing for the selective removal of specific associations without impacting related knowledge - something not achievable by previous works. Results demonstrate that anti-samples offer an efficient, targeted unlearning strategy for LLMs, opening new avenues for privacy-preserving machine learning and model modification.

**Link**: [arxiv](http://arxiv.org/abs/2410.17050v1),  [pdf](http://arxiv.org/pdf/2410.17050v1)

**Tags**: cs.LG cs.AI cs.CL 



### Optimizing Mixture-of-Experts Inference Time Combining Model Deployment   and Communication Scheduling
**Authors**: Jialong Li, Shreyansh Tripathi, Lakshay Rastogi, Yiming Lei, Rui Pan, Yiting Xia

**Updated**: 2024-10-22T14:19:29Z

**Summary**: As machine learning models scale in size and complexity, their computational requirements become a significant barrier. Mixture-of-Experts (MoE) models alleviate this issue by selectively activating relevant experts. Despite this, MoE models are hindered by high communication overhead from all-to-all operations, low GPU utilization due to the synchronous communication constraint, and complications from heterogeneous GPU environments.   This paper presents Aurora, which optimizes both model deployment and all-to-all communication scheduling to address these challenges in MoE inference. Aurora achieves minimal communication times by strategically ordering token transmissions in all-to-all communications. It improves GPU utilization by colocating experts from different models on the same device, avoiding the limitations of synchronous all-to-all communication. We analyze Aurora's optimization strategies theoretically across four common GPU cluster settings: exclusive vs. colocated models on GPUs, and homogeneous vs. heterogeneous GPUs. Aurora provides optimal solutions for three cases, and for the remaining NP-hard scenario, it offers a polynomial-time sub-optimal solution with only a 1.07x degradation from the optimal.   Aurora is the first approach to minimize MoE inference time via optimal model deployment and communication scheduling across various scenarios. Evaluations demonstrate that Aurora significantly accelerates inference, achieving speedups of up to 2.38x in homogeneous clusters and 3.54x in heterogeneous environments. Moreover, Aurora enhances GPU utilization by up to 1.5x compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2410.17043v1),  [pdf](http://arxiv.org/pdf/2410.17043v1)

**Tags**: cs.LG cs.NI 



### Arabic Dataset for LLM Safeguard Evaluation
**Authors**: Yasser Ashraf, Yuxia Wang, Bin Gu, Preslav Nakov, Timothy Baldwin

**Updated**: 2024-10-22T14:12:43Z

**Summary**: The growing use of large language models (LLMs) has raised concerns regarding their safety. While many studies have focused on English, the safety of LLMs in Arabic, with its linguistic and cultural complexities, remains under-explored. Here, we aim to bridge this gap. In particular, we present an Arab-region-specific safety evaluation dataset consisting of 5,799 questions, including direct attacks, indirect attacks, and harmless requests with sensitive words, adapted to reflect the socio-cultural context of the Arab world. To uncover the impact of different stances in handling sensitive and controversial topics, we propose a dual-perspective evaluation framework. It assesses the LLM responses from both governmental and opposition viewpoints. Experiments over five leading Arabic-centric and multilingual LLMs reveal substantial disparities in their safety performance. This reinforces the need for culturally specific datasets to ensure the responsible deployment of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17040v1),  [pdf](http://arxiv.org/pdf/2410.17040v1)

**Tags**: cs.CL 



### Developing a Thailand solar irradiance map using Himawari-8 satellite   imageries and deep learning models
**Authors**: Suwichaya Suwanwimolkul, Natanon Tongamrak, Nuttamon Thungka, Naebboon Hoonchareon, Jitkomut Songsiri

**Updated**: 2024-10-22T14:09:10Z

**Summary**: This paper presents an online platform that shows Thailand's solar irradiance map every 30 minutes. It is available at https://www.cusolarforecast.com. The methodology for estimating global horizontal irradiance (GHI) across Thailand relies on cloud index extracted from Himawari-8 satellite imagery, Ineichen clear-sky model with locally-tuned Linke turbidity, and machine learning models. The methods take clear-sky irradiance, cloud index, re-analyzed GHI and temperature data from the MERRA-2 database, and date-time as inputs for GHI estimation models, including LightGBM, LSTM, Informer, and Transformer. These are benchmarked with the estimate from a commercial service X by evaluating 15-minute ground GHI data from 53 ground stations over 1.5 years from 2022-2023. The results show that the four models have competitive performances and outperform the service X. The best model is LightGBM, with an MAE of 78.58 W/sqm and RMSE of 118.97 W/sqm. Obtaining re-analyzed MERRA-2 data for Thailand is not economically feasible for deployment. When removing these features, the Informer model has a winning performance of 78.67 W/sqm in MAE. The obtained performance aligns with existing literature by taking the climate zone and time granularity of data into consideration. As the map shows an estimate of GHI over 93,000 grids with a frequent update, the paper also describes a computational framework for displaying the entire map. It tests the runtime performance of deep learning models in the GHI estimation process.

**Link**: [arxiv](http://arxiv.org/abs/2409.16320v2),  [pdf](http://arxiv.org/pdf/2409.16320v2)

**Tags**: physics.ao-ph cs.AI cs.CV cs.LG 



### Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by   Simulating Documents in the Wild via Low-level Perturbations
**Authors**: Sukmin Cho, Soyeong Jeong, Jeongyeon Seo, Taeho Hwang, Jong C. Park

**Updated**: 2024-10-22T14:07:57Z

**Summary**: The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.

**Link**: [arxiv](http://arxiv.org/abs/2404.13948v2),  [pdf](http://arxiv.org/pdf/2404.13948v2)

**Tags**: cs.CL 



### DIRI: Adversarial Patient Reidentification with Large Language Models   for Evaluating Clinical Text Anonymization
**Authors**: John X. Morris, Thomas R. Campion, Sri Laasya Nutheti, Yifan Peng, Akhil Raj, Ramin Zabih, Curtis L. Cole

**Updated**: 2024-10-22T14:06:31Z

**Summary**: Sharing protected health information (PHI) is critical for furthering biomedical research. Before data can be distributed, practitioners often perform deidentification to remove any PHI contained in the text. Contemporary deidentification methods are evaluated on highly saturated datasets (tools achieve near-perfect accuracy) which may not reflect the full variability or complexity of real-world clinical text and annotating them is resource intensive, which is a barrier to real-world applications. To address this gap, we developed an adversarial approach using a large language model (LLM) to re-identify the patient corresponding to a redacted clinical note and evaluated the performance with a novel De-Identification/Re-Identification (DIRI) method. Our method uses a large language model to reidentify the patient corresponding to a redacted clinical note. We demonstrate our method on medical data from Weill Cornell Medicine anonymized with three deidentification tools: rule-based Philter and two deep-learning-based models, BiLSTM-CRF and ClinicalBERT. Although ClinicalBERT was the most effective, masking all identified PII, our tool still reidentified 9% of clinical notes Our study highlights significant weaknesses in current deidentification technologies while providing a tool for iterative development and improvement.

**Link**: [arxiv](http://arxiv.org/abs/2410.17035v1),  [pdf](http://arxiv.org/pdf/2410.17035v1)

**Tags**: cs.CL 



### GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks
**Authors**: Shuyang Hou, Zhangxiao Shen, Anqi Zhao, Jianyuan Liang, Zhipeng Gui, Xuefeng Guan, Rui Li, Huayi Wu

**Updated**: 2024-10-22T13:57:55Z

**Summary**: The increasing demand for spatiotemporal data and modeling tasks in geosciences has made geospatial code generation technology a critical factor in enhancing productivity. Although large language models (LLMs) have demonstrated potential in code generation tasks, they often encounter issues such as refusal to code or hallucination in geospatial code generation due to a lack of domain-specific knowledge and code corpora. To address these challenges, this paper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along with the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and LoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first LLM focused on geospatial code generation, fine-tuned from Code Llama-7B. Furthermore, we establish a comprehensive geospatial code evaluation framework, incorporating option matching, expert validation, and prompt engineering scoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the GeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms other models in multiple-choice accuracy by 9.1% to 32.1%, in code summarization ability by 1.7% to 25.4%, and in code generation capability by 1.2% to 25.1%. This paper provides a solution and empirical validation for enhancing LLMs' performance in geospatial code generation, extends the boundaries of domain-specific model applications, and offers valuable insights into unlocking their potential in geospatial code generation.

**Link**: [arxiv](http://arxiv.org/abs/2410.17031v1),  [pdf](http://arxiv.org/pdf/2410.17031v1)

**Tags**: cs.SE cs.AI 



### SG-FSM: A Self-Guiding Zero-Shot Prompting Paradigm for Multi-Hop   Question Answering Based on Finite State Machine
**Authors**: Xiaochen Wang, Junqing He, Liang Chen, Reza Haf Zhe Yang, Yiru Wang, Xiangdi Meng, Kunhao Pan, Zhifang Sui

**Updated**: 2024-10-22T13:47:38Z

**Summary**: Large Language Models with chain-of-thought prompting, such as OpenAI-o1, have shown impressive capabilities in natural language inference tasks. However, Multi-hop Question Answering (MHQA) remains challenging for many existing models due to issues like hallucination, error propagation, and limited context length. To address these challenges and enhance LLMs' performance on MHQA, we propose the Self-Guiding prompting Finite State Machine (SG-FSM), designed to strengthen multi-hop reasoning abilities. Unlike traditional chain-of-thought methods, SG-FSM tackles MHQA by iteratively breaking down complex questions into sub-questions, correcting itself to improve accuracy. It processes one sub-question at a time, dynamically deciding the next step based on the current context and results, functioning much like an automaton. Experiments across various benchmarks demonstrate the effectiveness of our approach, outperforming strong baselines on challenging datasets such as Musique. SG-FSM reduces hallucination, enabling recovery of the correct final answer despite intermediate errors. It also improves adherence to specified output formats, simplifying evaluation significantly.

**Link**: [arxiv](http://arxiv.org/abs/2410.17021v1),  [pdf](http://arxiv.org/pdf/2410.17021v1)

**Tags**: cs.CL 



### On-Device LLMs for SMEs: Challenges and Opportunities
**Authors**: Jeremy Stephen Gabriel Yee, Pai Chet Ng, Zhengkui Wang, Ian McLoughlin, Aik Beng Ng, Simon See

**Updated**: 2024-10-22T13:40:18Z

**Summary**: This paper presents a systematic review of the infrastructure requirements for deploying Large Language Models (LLMs) on-device within the context of small and medium-sized enterprises (SMEs), focusing on both hardware and software perspectives. From the hardware viewpoint, we discuss the utilization of processing units like GPUs and TPUs, efficient memory and storage solutions, and strategies for effective deployment, addressing the challenges of limited computational resources typical in SME settings. From the software perspective, we explore framework compatibility, operating system optimization, and the use of specialized libraries tailored for resource-constrained environments. The review is structured to first identify the unique challenges faced by SMEs in deploying LLMs on-device, followed by an exploration of the opportunities that both hardware innovations and software adaptations offer to overcome these obstacles. Such a structured review provides practical insights, contributing significantly to the community by enhancing the technological resilience of SMEs in integrating LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.16070v2),  [pdf](http://arxiv.org/pdf/2410.16070v2)

**Tags**: cs.AI cs.CL 68T07 I.2 



### Exploring Forgetting in Large Language Model Pre-Training
**Authors**: Chonghua Liao, Ruobing Xie, Xingwu Sun, Haowen Sun, Zhanhui Kang

**Updated**: 2024-10-22T13:39:47Z

**Summary**: Catastrophic forgetting remains a formidable obstacle to building an omniscient model in large language models (LLMs). Despite the pioneering research on task-level forgetting in LLM fine-tuning, there is scant focus on forgetting during pre-training. We systematically explored the existence and measurement of forgetting in pre-training, questioning traditional metrics such as perplexity (PPL) and introducing new metrics to better detect entity memory retention. Based on our revised assessment of forgetting metrics, we explored low-cost, straightforward methods to mitigate forgetting during the pre-training phase. Further, we carefully analyzed the learning curves, offering insights into the dynamics of forgetting. Extensive evaluations and analyses on forgetting of pre-training could facilitate future research on LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2410.17018v1),  [pdf](http://arxiv.org/pdf/2410.17018v1)

**Tags**: cs.CL 



### PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory   Planner
**Authors**: Kota Kondo, Claudius T. Tewari, Andrea Tagliabue, Jesus Tordesillas, Parker C. Lusk, Jonathan P. How

**Updated**: 2024-10-22T13:35:07Z

**Summary**: In decentralized multiagent trajectory planners, agents need to communicate and exchange their positions to generate collision-free trajectories. However, due to localization errors/uncertainties, trajectory deconfliction can fail even if trajectories are perfectly shared between agents. To address this issue, we first present PARM and PARM*, perception-aware, decentralized, asynchronous multiagent trajectory planners that enable a team of agents to navigate uncertain environments while deconflicting trajectories and avoiding obstacles using perception information. PARM* differs from PARM as it is less conservative, using more computation to find closer-to-optimal solutions. While these methods achieve state-of-the-art performance, they suffer from high computational costs as they need to solve large optimization problems onboard, making it difficult for agents to replan at high rates. To overcome this challenge, we present our second key contribution, PRIMER, a learning-based planner trained with imitation learning (IL) using PARM* as the expert demonstrator. PRIMER leverages the low computational requirements at deployment of neural networks and achieves a computation speed up to 5500 times faster than optimization-based approaches.

**Link**: [arxiv](http://arxiv.org/abs/2406.10060v2),  [pdf](http://arxiv.org/pdf/2406.10060v2)

**Tags**: cs.RO cs.LG 



### Thinking Forward: Memory-Efficient Federated Finetuning of Language   Models
**Authors**: Kunjal Panchal, Nisarg Parikh, Sunav Choudhary, Lijun Zhang, Yuriy Brun, Hui Guan

**Updated**: 2024-10-22T13:32:59Z

**Summary**: Finetuning large language models (LLMs) in federated learning (FL) settings has become increasingly important as it allows resource-constrained devices to finetune a model using private data. However, finetuning LLMs using backpropagation requires excessive memory (especially from intermediate activations) for resource-constrained devices. While Forward-mode Auto-Differentiation (AD) can significantly reduce memory footprint from activations, we observe that directly applying it to LLM finetuning results in slow convergence and poor accuracy. In this paper, we introduce Spry, an FL algorithm that splits trainable weights of an LLM among participating clients, such that each client computes gradients using forward-mode AD that are closer estimations of the true gradients. Spry achieves a low memory footprint, high accuracy, and fast convergence. We formally prove that the global gradients in Spry are unbiased estimators of true global gradients for homogeneous data distributions across clients, while heterogeneity increases bias of the estimates. We also derive Spry's convergence rate, showing that the gradients decrease inversely proportional to the number of FL rounds, indicating the convergence up to the limits of heterogeneity. Empirically, Spry reduces the memory footprint during training by 1.4-7.1x in contrast to backpropagation, while reaching comparable accuracy, across a wide range of language tasks, models, and FL settings. Spry reduces the convergence time by 1.2-20.3x and achieves 5.2-13.5% higher accuracy against zero-order methods. When finetuning Llama2-7B with LoRA, compared to the peak memory consumption of 33.9GB of backpropagation, Spry only consumes 6.2GB of peak memory. For OPT13B, the reduction is from 76.5GB to 10.8GB. Spry makes feasible previously impossible FL deployments on commodity edge devices. Our source code is available at https://github.com/Astuary/Spry.

**Link**: [arxiv](http://arxiv.org/abs/2405.15551v2),  [pdf](http://arxiv.org/pdf/2405.15551v2)

**Tags**: cs.LG 



### Variational autoencoders stabilise TCN performance when classifying   weakly labelled bioacoustics data
**Authors**: Laia Garrob Fonollosa, Douglas Gillespie, Lina Stankovic, Vladimir Stankovic, Luke Rendell

**Updated**: 2024-10-22T13:25:59Z

**Summary**: Passive acoustic monitoring (PAM) data is often weakly labelled, audited at the scale of detection presence or absence on timescales of minutes to hours. Moreover, this data exhibits great variability from one deployment to the next, due to differences in ambient noise and the signals across sources and geographies. This study proposes a two-step solution to leverage weakly annotated data for training Deep Learning (DL) detection models. Our case study involves binary classification of the presence/absence of sperm whale (\textit{Physeter macrocephalus}) click trains in 4-minute-long recordings from a dataset comprising diverse sources and deployment conditions to maximise generalisability. We tested methods for extracting acoustic features from lengthy audio segments and integrated Temporal Convolutional Networks (TCNs) trained on the extracted features for sequence classification. For feature extraction, we introduced a new approach using Variational AutoEncoders (VAEs) to extract information from both waveforms and spectrograms, which eliminates the necessity for manual threshold setting or time-consuming strong labelling. For classification, TCNs were trained separately on sequences of either VAE embeddings or handpicked acoustic features extracted from the waveform and spectrogram representations using classical methods, to compare the efficacy of the two approaches. The TCN demonstrated robust classification capabilities on a validation set, achieving accuracies exceeding 85\% when applied to 4-minute acoustic recordings. Notably, TCNs trained on handpicked acoustic features exhibited greater variability in performance across recordings from diverse deployment conditions, whereas those trained on VAEs showed a more consistent performance, highlighting the robust transferability of VAEs for feature extraction across different deployment conditions.

**Link**: [arxiv](http://arxiv.org/abs/2410.17006v1),  [pdf](http://arxiv.org/pdf/2410.17006v1)

**Tags**: cs.SD eess.AS q-bio.QM 



### An Eye for an AI: Evaluating GPT-4o's Visual Perception Skills and   Geometric Reasoning Skills Using Computer Graphics Questions
**Authors**: Tony Haoran Feng, Paul Denny, Burkhard C. Wnsche, Andrew Luxton-Reilly, Jacqueline Whalley

**Updated**: 2024-10-22T13:12:47Z

**Summary**: CG (Computer Graphics) is a popular field of CS (Computer Science), but many students find this topic difficult due to it requiring a large number of skills, such as mathematics, programming, geometric reasoning, and creativity. Over the past few years, researchers have investigated ways to harness the power of GenAI (Generative Artificial Intelligence) to improve teaching. In CS, much of the research has focused on introductory computing. A recent study evaluating the performance of an LLM (Large Language Model), GPT-4 (text-only), on CG questions, indicated poor performance and reliance on detailed descriptions of image content, which often required considerable insight from the user to return reasonable results. So far, no studies have investigated the abilities of LMMs (Large Multimodal Models), or multimodal LLMs, to solve CG questions and how these abilities can be used to improve teaching.   In this study, we construct two datasets of CG questions requiring varying degrees of visual perception skills and geometric reasoning skills, and evaluate the current state-of-the-art LMM, GPT-4o, on the two datasets. We find that although GPT-4o exhibits great potential in solving questions with visual information independently, major limitations still exist to the accuracy and quality of the generated results. We propose several novel approaches for CG educators to incorporate GenAI into CG teaching despite these limitations. We hope that our guidelines further encourage learning and engagement in CG classrooms.

**Link**: [arxiv](http://arxiv.org/abs/2410.16991v1),  [pdf](http://arxiv.org/pdf/2410.16991v1)

**Tags**: cs.AI cs.GR I.2.7; I.3.0; K.3.2 



### LLM Gesticulator: Leveraging Large Language Models for Scalable and   Controllable Co-Speech Gesture Synthesis
**Authors**: Haozhou Pang, Tianwei Ding, Lanshan He, Ming Tao, Lu Zhang, Qi Gan

**Updated**: 2024-10-22T13:08:02Z

**Summary**: In this work, we present LLM Gesticulator, an LLM-based audio-driven co-speech gesture generation framework that synthesizes full-body animations that are rhythmically aligned with the input audio while exhibiting natural movements and editability. Compared to previous work, our model demonstrates substantial scalability. As the size of the backbone LLM model increases, our framework shows proportional improvements in evaluation metrics (a.k.a. scaling law). Our method also exhibits strong controllability where the content, style of the generated gestures can be controlled by text prompt. To the best of our knowledge, LLM gesticulator is the first work that use LLM on the co-speech generation task. Evaluation with existing objective metrics and user studies indicate that our framework outperforms prior works.

**Link**: [arxiv](http://arxiv.org/abs/2410.10851v2),  [pdf](http://arxiv.org/pdf/2410.10851v2)

**Tags**: cs.GR cs.AI cs.CL cs.LG cs.SD eess.AS 



### RecPrompt: A Self-tuning Prompting Framework for News Recommendation   Using Large Language Models
**Authors**: Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Neil Hurley, Aonghus Lawlor, Ruihai Dong, Irene Li

**Updated**: 2024-10-22T12:40:25Z

**Summary**: News recommendations heavily rely on Natural Language Processing (NLP) methods to analyze, understand, and categorize content, enabling personalized suggestions based on user interests and reading behaviors. Large Language Models (LLMs) like GPT-4 have shown promising performance in understanding natural language. However, the extent of their applicability to news recommendation systems remains to be validated. This paper introduces RecPrompt, the first self-tuning prompting framework for news recommendation, leveraging the capabilities of LLMs to perform complex news recommendation tasks. This framework incorporates a news recommender and a prompt optimizer that applies an iterative bootstrapping process to enhance recommendations through automatic prompt engineering. Extensive experimental results with 400 users show that RecPrompt can achieve an improvement of 3.36% in AUC, 10.49% in MRR, 9.64% in nDCG@5, and 6.20% in nDCG@10 compared to deep neural models. Additionally, we introduce TopicScore, a novel metric to assess explainability by evaluating LLM's ability to summarize topics of interest for users. The results show LLM's effectiveness in accurately identifying topics of interest and delivering comprehensive topic-based explanations.

**Link**: [arxiv](http://arxiv.org/abs/2312.10463v4),  [pdf](http://arxiv.org/pdf/2312.10463v4)

**Tags**: cs.IR 



### Breaking ReAct Agents: Foot-in-the-Door Attack Will Get You In
**Authors**: Itay Nakash, George Kour, Guy Uziel, Ateret Anaby-Tavor

**Updated**: 2024-10-22T12:24:41Z

**Summary**: Following the advancement of large language models (LLMs), the development of LLM-based autonomous agents has become increasingly prevalent. As a result, the need to understand the security vulnerabilities of these agents has become a critical task. We examine how ReAct agents can be exploited using a straightforward yet effective method we refer to as the foot-in-the-door attack. Our experiments show that indirect prompt injection attacks, prompted by harmless and unrelated requests (such as basic calculations) can significantly increase the likelihood of the agent performing subsequent malicious actions. Our results show that once a ReAct agents thought includes a specific tool or action, the likelihood of executing this tool in the subsequent steps increases significantly, as the agent seldom re-evaluates its actions. Consequently, even random, harmless requests can establish a foot-in-the-door, allowing an attacker to embed malicious instructions into the agents thought process, making it more susceptible to harmful directives. To mitigate this vulnerability, we propose implementing a simple reflection mechanism that prompts the agent to reassess the safety of its actions during execution, which can help reduce the success of such attacks.

**Link**: [arxiv](http://arxiv.org/abs/2410.16950v1),  [pdf](http://arxiv.org/pdf/2410.16950v1)

**Tags**: cs.CR cs.AI 



### Self-Evolving Multi-Agent Collaboration Networks for Software   Development
**Authors**: Yue Hu, Yuzhu Cai, Yaxin Du, Xinyu Zhu, Xiangrui Liu, Zijie Yu, Yuchen Hou, Shuo Tang, Siheng Chen

**Updated**: 2024-10-22T12:20:23Z

**Summary**: LLM-driven multi-agent collaboration (MAC) systems have demonstrated impressive capabilities in automatic software development at the function level. However, their heavy reliance on human design limits their adaptability to the diverse demands of real-world software development. To address this limitation, we introduce EvoMAC, a novel self-evolving paradigm for MAC networks. Inspired by traditional neural network training, EvoMAC obtains text-based environmental feedback by verifying the MAC network's output against a target proxy and leverages a novel textual backpropagation to update the network. To extend coding capabilities beyond function-level tasks to more challenging software-level development, we further propose rSDE-Bench, a requirement-oriented software development benchmark, which features complex and diverse software requirements along with automatic evaluation of requirement correctness. Our experiments show that: i) The automatic requirement-aware evaluation in rSDE-Bench closely aligns with human evaluations, validating its reliability as a software-level coding benchmark. ii) EvoMAC outperforms previous SOTA methods on both the software-level rSDE-Bench and the function-level HumanEval benchmarks, reflecting its superior coding capabilities. The benchmark can be downloaded at https://yuzhu-cai.github.io/rSDE-Bench/.

**Link**: [arxiv](http://arxiv.org/abs/2410.16946v1),  [pdf](http://arxiv.org/pdf/2410.16946v1)

**Tags**: cs.SE cs.AI cs.MA 



### Math Neurosurgery: Isolating Language Models' Math Reasoning Abilities   Using Only Forward Passes
**Authors**: Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen

**Updated**: 2024-10-22T12:00:58Z

**Summary**: Math reasoning is a highly active area of Large Language Model (LLM) research because it is a hallmark of artificial intelligence. However, few works have explored how math reasoning is encoded within LLM parameters and if it is a skill that can be isolated within a model. Doing so could allow targeted intervention to improve math performance without altering non-math behavior and foster understanding of how models encode math reasoning. We introduce Math Neurosurgery (MathNeuro), a method for isolating math-specific parameters in LLMs using only forward passes. MathNeuro builds on existing work by using weights and activations to calculate parameter importance, but isolates math-specific parameters by removing those important for general language tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning ability without destroying its general language ability. Scaling these parameters by a small constant improves a pretrained or instruction-tuned LLM's performance by 4-17% on GSM8K while leaving non-math behavior unaltered. MathNeuro is also data efficient: most of its effectiveness holds when identifying math-specific parameters using a single sample. MathNeuro highlights the potential for future work to intervene on math-specific parameters.

**Link**: [arxiv](http://arxiv.org/abs/2410.16930v1),  [pdf](http://arxiv.org/pdf/2410.16930v1)

**Tags**: cs.CL cs.AI 



### Revealing Hidden Bias in AI: Lessons from Large Language Models
**Authors**: Django Beatty, Kritsada Masanthia, Teepakorn Kaphol, Niphan Sethi

**Updated**: 2024-10-22T11:58:54Z

**Summary**: As large language models (LLMs) become integral to recruitment processes, concerns about AI-induced bias have intensified. This study examines biases in candidate interview reports generated by Claude 3.5 Sonnet, GPT-4o, Gemini 1.5, and Llama 3.1 405B, focusing on characteristics such as gender, race, and age. We evaluate the effectiveness of LLM-based anonymization in reducing these biases. Findings indicate that while anonymization reduces certain biases, particularly gender bias, the degree of effectiveness varies across models and bias types. Notably, Llama 3.1 405B exhibited the lowest overall bias. Moreover, our methodology of comparing anonymized and non-anonymized data reveals a novel approach to assessing inherent biases in LLMs beyond recruitment applications. This study underscores the importance of careful LLM selection and suggests best practices for minimizing bias in AI applications, promoting fairness and inclusivity.

**Link**: [arxiv](http://arxiv.org/abs/2410.16927v1),  [pdf](http://arxiv.org/pdf/2410.16927v1)

**Tags**: cs.AI cs.CY I.2.7; K.4.1 



### Pyramid Vector Quantization for LLMs
**Authors**: Tycho F. A. van der Ouderaa, Maximilian L. Croci, Agrin Hilmkil, James Hensman

**Updated**: 2024-10-22T11:57:32Z

**Summary**: Recent works on compression of large language models (LLM) using quantization considered reparameterizing the architecture such that weights are distributed on the sphere. This demonstratively improves the ability to quantize by increasing the mathematical notion of coherence, resulting in fewer weight outliers without affecting the network output. In this work, we aim to further exploit this spherical geometry of the weights when performing quantization by considering Pyramid Vector Quantization (PVQ) for large language models. Arranging points evenly on the sphere is notoriously difficult, especially in high dimensions, and in case approximate solutions exists, representing points explicitly in a codebook is typically not feasible due to its additional memory cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting points onto the 1-sphere, which allows for efficient encoding and decoding without requiring an explicit codebook in memory. To obtain a practical algorithm, we propose to combine PVQ with scale quantization for which we derive theoretically optimal quantizations, under empirically verified assumptions. Further, we extend pyramid vector quantization to use Hessian information to minimize quantization error under expected feature activations, instead of only relying on weight magnitudes. Experimentally, we achieves state-of-the-art quantization performance with pareto-optimal trade-off between performance and bits per weight and bits per activation, compared to compared methods. On weight-only, we find that we can quantize a Llama-3 70B model to 3.25 bits per weight and retain 98\% accuracy on downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2410.16926v1),  [pdf](http://arxiv.org/pdf/2410.16926v1)

**Tags**: cs.LG 



### SleepCoT: A Lightweight Personalized Sleep Health Model via   Chain-of-Thought Distillation
**Authors**: Huimin Zheng, Xiaofeng Xing, Xiangmin Xu

**Updated**: 2024-10-22T11:56:34Z

**Summary**: We present a novel approach to personalized sleep health management using few-shot Chain-of-Thought (CoT) distillation, enabling small-scale language models (> 2B parameters) to rival the performance of large language models (LLMs) in specialized health domains. Our method simultaneously distills problem-solving strategies, long-tail expert knowledge, and personalized recommendation capabilities from larger models into more efficient, compact models. Unlike existing systems, our approach offers three key functionalities: generating personalized sleep health recommendations, supporting user-specific follow-up inquiries, and providing responses to domain-specific knowledge questions. We focus on sleep health due to its measurability via wearable devices and its impact on overall well-being. Our experimental setup, involving GPT-4o for data synthesis, Qwen-max for instruction set creation, and Qwen2.5 1.5B for model distillation, demonstrates significant improvements over baseline small-scale models in penalization, reasoning, and knowledge application. Experiments using 100 simulated sleep reports and 1,000 domain-specific questions shows our model achieves comparable performance to larger models while maintaining efficiency for real-world deployment. This research not only advances AI-driven health management but also provides a novel approach to leveraging LLM capabilities in resource-constrained environments, potentially enhancing the accessibility of personalized healthcare solutions.

**Link**: [arxiv](http://arxiv.org/abs/2410.16924v1),  [pdf](http://arxiv.org/pdf/2410.16924v1)

**Tags**: cs.AI 



### EnvBridge: Bridging Diverse Environments with Cross-Environment   Knowledge Transfer for Embodied AI
**Authors**: Tomoyuki Kagaya, Yuxuan Lou, Thong Jing Yuan, Subramanian Lakshmi, Jayashree Karlekar, Sugiri Pranata, Natsuki Murakami, Akira Kinose, Koki Oguri, Felix Wick, Yang You

**Updated**: 2024-10-22T11:52:22Z

**Summary**: In recent years, Large Language Models (LLMs) have demonstrated high reasoning capabilities, drawing attention for their applications as agents in various decision-making processes. One notably promising application of LLM agents is robotic manipulation. Recent research has shown that LLMs can generate text planning or control code for robots, providing substantial flexibility and interaction capabilities. However, these methods still face challenges in terms of flexibility and applicability across different environments, limiting their ability to adapt autonomously. Current approaches typically fall into two categories: those relying on environment-specific policy training, which restricts their transferability, and those generating code actions based on fixed prompts, which leads to diminished performance when confronted with new environments. These limitations significantly constrain the generalizability of agents in robotic manipulation. To address these limitations, we propose a novel method called EnvBridge. This approach involves the retention and transfer of successful robot control codes from source environments to target environments. EnvBridge enhances the agent's adaptability and performance across diverse settings by leveraging insights from multiple environments. Notably, our approach alleviates environmental constraints, offering a more flexible and generalizable solution for robotic manipulation tasks. We validated the effectiveness of our method using robotic manipulation benchmarks: RLBench, MetaWorld, and CALVIN. Our experiments demonstrate that LLM agents can successfully leverage diverse knowledge sources to solve complex tasks. Consequently, our approach significantly enhances the adaptability and robustness of robotic manipulation agents in planning across diverse environments.

**Link**: [arxiv](http://arxiv.org/abs/2410.16919v1),  [pdf](http://arxiv.org/pdf/2410.16919v1)

**Tags**: cs.RO cs.AI cs.CL cs.LG 



### VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large   Language Models
**Authors**: Yifei Liu, Jicheng Wen, Yang Wang, Shengyu Ye, Li Lyna Zhang, Ting Cao, Cheng Li, Mao Yang

**Updated**: 2024-10-22T11:47:04Z

**Summary**: Scaling model size significantly challenges the deployment and inference of Large Language Models (LLMs). Due to the redundancy in LLM weights, recent research has focused on pushing weight-only quantization to extremely low-bit (even down to 2 bits). It reduces memory requirements, optimizes storage costs, and decreases memory bandwidth needs during inference. However, due to numerical representation limitations, traditional scalar-based weight quantization struggles to achieve such extreme low-bit. Recent research on Vector Quantization (VQ) for LLMs has demonstrated the potential for extremely low-bit model quantization by compressing vectors into indices using lookup tables.   In this paper, we introduce Vector Post-Training Quantization (VPTQ) for extremely low-bit quantization of LLMs. We use Second-Order Optimization to formulate the LLM VQ problem and guide our quantization algorithm design by solving the optimization. We further refine the weights using Channel-Independent Second-Order Optimization for a granular VQ. In addition, by decomposing the optimization problem, we propose a brief and effective codebook initialization algorithm. We also extend VPTQ to support residual and outlier quantization, which enhances model accuracy and further compresses the model. Our experimental results show that VPTQ reduces model quantization perplexity by $0.01$-$0.34$ on LLaMA-2, $0.38$-$0.68$ on Mistral-7B, $4.41$-$7.34$ on LLaMA-3 over SOTA at 2-bit, with an average accuracy improvement of $0.79$-$1.5\%$ on LLaMA-2, $1\%$ on Mistral-7B, $11$-$22\%$ on LLaMA-3 on QA tasks on average. We only utilize $10.4$-$18.6\%$ of the quantization algorithm execution time, resulting in a $1.6$-$1.8\times$ increase in inference throughput compared to SOTA.

**Link**: [arxiv](http://arxiv.org/abs/2409.17066v2),  [pdf](http://arxiv.org/pdf/2409.17066v2)

**Tags**: cs.AI 



### Precise Ranging: Modeling Bias and Variance of Double-Sided Two-Way   Ranging with TDoA Extraction under Multipath and NLOS Effects
**Authors**: Patrick Rathje, Olaf Landsiedel

**Updated**: 2024-10-22T11:23:09Z

**Summary**: Location-based services such as autonomous vehicles, drones, and indoor positioning require precise and scalable distance estimates. The bias and variance of range estimators inherently influence the resulting localization quality. In this work, we revisit the well-established Double-Sided Two-Way-Ranging (DS-TWR) protocol and the extraction of timing differences (DS-TDoA) at devices overhearing DS-TWR. Under non-line-of-sight (NLOS) and multipath effects, we analytically derive their bias and variance. Our proposed model reveals that DS-TWR retains half the variance than anticipated while DS-TDoA comprises roughly a five-fold increase in variance. We conduct numerical simulations and experimental deployments using Ultra-Wideband (UWB) devices in a public testbed. Our results confirm the adequacy of our model, providing centimeter-accurate predictions based on the underlying timestamping noise with a median $R^2$ score of 77% (30% IQR). We find that both DS-TWR and DS-TDoA exhibit reduced variance when response times are symmetric. Our experimental results further show that double-sided variants exhibit less error and variance compared to Carrier Frequency Offset (CFO)-based single-sided methods.

**Link**: [arxiv](http://arxiv.org/abs/2410.12826v2),  [pdf](http://arxiv.org/pdf/2410.12826v2)

**Tags**: eess.SP 



### PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs
**Authors**: Joo Pedro Fernandes Torres, Catherine Mulligan, Joaquim Jorge, Catarina Moreira

**Updated**: 2024-10-22T10:56:35Z

**Summary**: The growing volume of academic publications poses significant challenges for researchers conducting timely and accurate Systematic Literature Reviews, particularly in fast-evolving fields like artificial intelligence. This growth of academic literature also makes it increasingly difficult for lay people to access scientific knowledge effectively, meaning academic literature is often misrepresented in the popular press and, more broadly, in society. Traditional SLR methods are labor-intensive and error-prone, and they struggle to keep up with the rapid pace of new research. To address these issues, we developed \textit{PROMPTHEUS}: an AI-driven pipeline solution that automates the SLR process using Large Language Models. We aimed to enhance efficiency by reducing the manual workload while maintaining the precision and coherence required for comprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR process, including systematic search, data extraction, topic modeling using BERTopic, and summarization with transformer models. Evaluations conducted across five research domains demonstrate that PROMPTHEUS reduces review time, achieves high precision, and provides coherent topic organization, offering a scalable and effective solution for conducting literature reviews in an increasingly crowded research landscape. In addition, such tools may reduce the increasing mistrust in science by making summarization more accessible to laypeople.   The code for this project can be found on the GitHub repository at https://github.com/joaopftorres/PROMPTHEUS.git

**Link**: [arxiv](http://arxiv.org/abs/2410.15978v2),  [pdf](http://arxiv.org/pdf/2410.15978v2)

**Tags**: cs.AI 



### GLBench: A Comprehensive Benchmark for Graph with Large Language Models
**Authors**: Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, Jia Li

**Updated**: 2024-10-22T10:54:15Z

**Summary**: The emergence of large language models (LLMs) has revolutionized the way we interact with graphs, leading to a new paradigm called GraphLLM. Despite the rapid development of GraphLLM methods in recent years, the progress and understanding of this field remain unclear due to the lack of a benchmark with consistent experimental protocols. To bridge this gap, we introduce GLBench, the first comprehensive benchmark for evaluating GraphLLM methods in both supervised and zero-shot scenarios. GLBench provides a fair and thorough evaluation of different categories of GraphLLM methods, along with traditional baselines such as graph neural networks. Through extensive experiments on a collection of real-world datasets with consistent data processing and splitting strategies, we have uncovered several key findings. Firstly, GraphLLM methods outperform traditional baselines in supervised settings, with LLM-as-enhancers showing the most robust performance. However, using LLMs as predictors is less effective and often leads to uncontrollable output issues. We also notice that no clear scaling laws exist for current GraphLLM methods. In addition, both structures and semantics are crucial for effective zero-shot transfer, and our proposed simple baseline can even outperform several models tailored for zero-shot scenarios. The data and code of the benchmark can be found at https://github.com/NineAbyss/GLBench.

**Link**: [arxiv](http://arxiv.org/abs/2407.07457v3),  [pdf](http://arxiv.org/pdf/2407.07457v3)

**Tags**: cs.LG cs.CL 



### Dynamic Planning for LLM-based Graphical User Interface Automation
**Authors**: Shaoqing Zhang, Zhuosheng Zhang, Kehai Chen, Xinbei Ma, Muyun Yang, Tiejun Zhao, Min Zhang

**Updated**: 2024-10-22T10:47:13Z

**Summary**: The advent of large language models (LLMs) has spurred considerable interest in advancing autonomous LLMs-based agents, particularly in intriguing applications within smartphone graphical user interfaces (GUIs). When presented with a task goal, these agents typically emulate human actions within a GUI environment until the task is completed. However, a key challenge lies in devising effective plans to guide action prediction in GUI tasks, though planning have been widely recognized as effective for decomposing complex tasks into a series of steps. Specifically, given the dynamic nature of environmental GUIs following action execution, it is crucial to dynamically adapt plans based on environmental feedback and action history.We show that the widely-used ReAct approach fails due to the excessively long historical dialogues. To address this challenge, we propose a novel approach called Dynamic Planning of Thoughts (D-PoT) for LLM-based GUI agents.D-PoT involves the dynamic adjustment of planning based on the environmental feedback and execution history. Experimental results reveal that the proposed D-PoT significantly surpassed the strong GPT-4V baseline by +12.7% (34.66% $\rightarrow$ 47.36%) in accuracy. The analysis highlights the generality of dynamic planning in different backbone LLMs, as well as the benefits in mitigating hallucinations and adapting to unseen tasks. Code is available at https://github.com/sqzhang-lazy/D-PoT.

**Link**: [arxiv](http://arxiv.org/abs/2410.00467v2),  [pdf](http://arxiv.org/pdf/2410.00467v2)

**Tags**: cs.AI cs.HC 



### Large Language Model-based Augmentation for Imbalanced Node   Classification on Text-Attributed Graphs
**Authors**: Leyao Wang, Yu Wang, Bo Ni, Yuying Zhao, Tyler Derr

**Updated**: 2024-10-22T10:36:15Z

**Summary**: Node classification on graphs frequently encounters the challenge of class imbalance, leading to biased performance and posing significant risks in real-world applications. Although several data-centric solutions have been proposed, none of them focus on Text-Attributed Graphs (TAGs), and therefore overlook the potential of leveraging the rich semantics encoded in textual features for boosting the classification of minority nodes. Given this crucial gap, we investigate the possibility of augmenting graph data in the text space, leveraging the textual generation power of Large Language Models (LLMs) to handle imbalanced node classification on TAGs. Specifically, we propose a novel approach called LA-TAG (LLM-based Augmentation on Text-Attributed Graphs), which prompts LLMs to generate synthetic texts based on existing node texts in the graph. Furthermore, to integrate these synthetic text-attributed nodes into the graph, we introduce a text-based link predictor to connect the synthesized nodes with the existing nodes. Our experiments across multiple datasets and evaluation metrics show that our framework significantly outperforms traditional non-textual-based data augmentation strategies and specific node imbalance solutions. This highlights the promise of using LLMs to resolve imbalance issues on TAGs.

**Link**: [arxiv](http://arxiv.org/abs/2410.16882v1),  [pdf](http://arxiv.org/pdf/2410.16882v1)

**Tags**: cs.AI cs.LG cs.SI 



### Stacking Your Transformers: A Closer Look at Model Growth for Efficient   LLM Pre-Training
**Authors**: Wenyu Du, Tongxu Luo, Zihan Qiu, Zeyu Huang, Yikang Shen, Reynold Cheng, Yike Guo, Jie Fu

**Updated**: 2024-10-22T10:31:59Z

**Summary**: LLMs are computationally expensive to pre-train due to their large scale. Model growth emerges as a promising approach by leveraging smaller models to accelerate the training of larger ones. However, the viability of these model growth methods in efficient LLM pre-training remains underexplored. This work identifies three critical $\underline{\textit{O}}$bstacles: ($\textit{O}$1) lack of comprehensive evaluation, ($\textit{O}$2) untested viability for scaling, and ($\textit{O}$3) lack of empirical guidelines. To tackle $\textit{O}$1, we summarize existing approaches into four atomic growth operators and systematically evaluate them in a standardized LLM pre-training setting. Our findings reveal that a depthwise stacking operator, called $G_{\text{stack}}$, exhibits remarkable acceleration in training, leading to decreased loss and improved overall performance on eight standard NLP benchmarks compared to strong baselines. Motivated by these promising results, we conduct extensive experiments to delve deeper into $G_{\text{stack}}$ to address $\textit{O}$2 and $\textit{O}$3. For $\textit{O}$2 (untested scalability), our study shows that $G_{\text{stack}}$ is scalable and consistently performs well, with experiments up to 7B LLMs after growth and pre-training LLMs with 750B tokens. For example, compared to a conventionally trained 7B model using 300B tokens, our $G_{\text{stack}}$ model converges to the same loss with 194B tokens, resulting in a 54.6\% speedup. We further address $\textit{O}$3 (lack of empirical guidelines) by formalizing guidelines to determine growth timing and growth factor for $G_{\text{stack}}$, making it practical in general LLM pre-training. We also provide in-depth discussions and comprehensive ablation studies of $G_{\text{stack}}$. Our code and pre-trained model are available at https://llm-stacking.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2405.15319v2),  [pdf](http://arxiv.org/pdf/2405.15319v2)

**Tags**: cs.CL cs.AI 



### Insights from the Usage of the Ansible Lightspeed Code Completion   Service
**Authors**: Priyam Sahoo, Saurabh Pujar, Ganesh Nalawade, Richard Gebhardt, Louis Mandel, Luca Buratti

**Updated**: 2024-10-22T10:30:19Z

**Summary**: The availability of Large Language Models (LLMs) which can generate code, has made it possible to create tools that improve developer productivity. Integrated development environments or IDEs which developers use to write software are often used as an interface to interact with LLMs. Although many such tools have been released, almost all of them focus on general-purpose programming languages. Domain-specific languages, such as those crucial for Information Technology (IT) automation, have not received much attention. Ansible is one such YAML-based IT automation-specific language. Ansible Lightspeed is an LLM-based service designed explicitly to generate Ansible YAML, given natural language prompt.   In this paper, we present the design and implementation of the Ansible Lightspeed service. We then evaluate its utility to developers using diverse indicators, including extended utilization, analysis of user edited suggestions, as well as user sentiments analysis. The evaluation is based on data collected for 10,696 real users including 3,910 returning users. The code for Ansible Lightspeed service and the analysis framework is made available for others to use.   To our knowledge, our study is the first to involve thousands of users of code assistants for domain-specific languages. We are also the first code completion tool to present N-Day user retention figures, which is 13.66% on Day 30. We propose an improved version of user acceptance rate, called Strong Acceptance rate, where a suggestion is considered accepted only if less than 50% of it is edited and these edits do not change critical parts of the suggestion. By focusing on Ansible, Lightspeed is able to achieve a strong acceptance rate of 49.08% for multi-line Ansible task suggestions. With our findings we provide insights into the effectiveness of small, dedicated models in a domain-specific context.

**Link**: [arxiv](http://arxiv.org/abs/2402.17442v4),  [pdf](http://arxiv.org/pdf/2402.17442v4)

**Tags**: cs.SE cs.AI cs.PL 



### Good Parenting is all you need -- Multi-agentic LLM Hallucination   Mitigation
**Authors**: Ted Kwartler, Matthew Berman, Alan Aqrawi

**Updated**: 2024-10-22T10:12:00Z

**Summary**: This study explores the ability of Large Language Model (LLM) agents to detect and correct hallucinations in AI-generated content. A primary agent was tasked with creating a blog about a fictional Danish artist named Flipfloppidy, which was then reviewed by another agent for factual inaccuracies. Most LLMs hallucinated the existence of this artist. Across 4,900 test runs involving various combinations of primary and reviewing agents, advanced AI models such as Llama3-70b and GPT-4 variants demonstrated near-perfect accuracy in identifying hallucinations and successfully revised outputs in 85% to 100% of cases following feedback. These findings underscore the potential of advanced AI models to significantly enhance the accuracy and reliability of generated content, providing a promising approach to improving AI workflow orchestration.

**Link**: [arxiv](http://arxiv.org/abs/2410.14262v2),  [pdf](http://arxiv.org/pdf/2410.14262v2)

**Tags**: cs.CR cs.CL 



### A Framework for Holistic KLD-based Waveform Design for   Multi-User-Multi-Target ISAC Systems
**Authors**: Yousef Kloob, Mohammad Al-Jarrah, Emad Alsusa

**Updated**: 2024-10-22T09:41:46Z

**Summary**: This paper introduces a novel framework that leverages the Kullback-Leibler Divergence (KLD) metric to analyse and optimise performance trade-offs in integrated sensing and communication (ISAC) systems. We consider a multiple-input multiple-output (MIMO) base station that simultaneously serves communication user equipments (UEs) and detects multiple targets using a shared antenna deployment. We apply this framework to two widely used communication beamforming techniques, maximum ratio transmission (MRT) and zero-forcing (ZF), to assess their impact on the radar subsystem's performance. Additionally, two optimisation problems are formulated: the first optimises the radar subsystem's KLD under communication constraints, and the second focuses on communication waveform KLD optimisation with constraints on the radar KLD. These problems are solved using a projected gradient method with adaptive penalties for the radar waveforms and a gradient-assisted interior point method (IPM) for the communication waveforms. Through theoretical derivations and extensive simulations, we demonstrate that our KLD approach effectively characterises and optimises the performance trade-offs between sensing and communication in ISAC systems. The results show significant improvements in both radar detection and communication performance when compared to traditional MRT and ZF beamforming, and the identity covariance design for radar subsystems. These findings promote a more holistic design and optimisation of ISAC for next-generation wireless networks and demonstrate the advantages of KLD-based optimisation in balancing the performance of both sensing and communication.

**Link**: [arxiv](http://arxiv.org/abs/2409.20245v2),  [pdf](http://arxiv.org/pdf/2409.20245v2)

**Tags**: eess.SP 



### CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian   Product Routing in Mixture-of-Experts
**Authors**: Zhenpeng Su, Xing Wu, Zijia Lin, Yizhe Xiong, Minxuan Lv, Guangyuan Ma, Hui Chen, Songlin Hu, Guiguang Ding

**Updated**: 2024-10-22T09:37:45Z

**Summary**: Large language models (LLM) have been attracting much attention from the community recently, due to their remarkable performance in all kinds of downstream tasks. According to the well-known scaling law, scaling up a dense LLM enhances its capabilities, but also significantly increases the computational complexity. Mixture-of-Experts (MoE) models address that by allowing the model size to grow without substantially raising training or inference costs. Yet MoE models face challenges regarding knowledge sharing among experts, making their performance somehow sensitive to routing accuracy. To tackle that, previous works introduced shared experts and combined their outputs with those of the top $K$ routed experts in an ``addition'' manner. In this paper, inspired by collective matrix factorization to learn shared knowledge among data, we propose CartesianMoE, which implements more effective knowledge sharing among experts in more like a ``multiplication'' manner. Extensive experimental results indicate that CartesianMoE outperforms previous MoE models for building LLMs, in terms of both perplexity and downstream task performance. And we also find that CartesianMoE achieves better expert routing robustness.

**Link**: [arxiv](http://arxiv.org/abs/2410.16077v2),  [pdf](http://arxiv.org/pdf/2410.16077v2)

**Tags**: cs.LG cs.CL 



### ETHIC: Evaluating Large Language Models on Long-Context Tasks with High   Information Coverage
**Authors**: Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Minju Song, Hyunjae Kim, Jaewoo Kang

**Updated**: 2024-10-22T09:35:42Z

**Summary**: Recent advancements in large language models (LLM) capable of processing extremely long texts highlight the need for a dedicated evaluation benchmark to assess their long-context capabilities. However, existing methods, like the needle-in-a-haystack test, do not effectively assess whether these models fully utilize contextual information, raising concerns about the reliability of current evaluation techniques. To thoroughly examine the effectiveness of existing benchmarks, we introduce a new metric called information coverage (IC), which quantifies the proportion of the input context necessary for answering queries. Our findings indicate that current benchmarks exhibit low IC; although the input context may be extensive, the actual usable context is often limited. To address this, we present ETHIC, a novel benchmark designed to assess LLMs' ability to leverage the entire context. Our benchmark comprises 2,648 test instances spanning four long-context tasks with high IC scores in the domains of books, debates, medicine, and law. Our evaluations reveal significant performance drops in contemporary LLMs, highlighting a critical challenge in managing long contexts. Our benchmark is available at https://github.com/dmis-lab/ETHIC.

**Link**: [arxiv](http://arxiv.org/abs/2410.16848v1),  [pdf](http://arxiv.org/pdf/2410.16848v1)

**Tags**: cs.CL 



### Critical Phase Transition in Large Language Models
**Authors**: Kai Nakaishi, Yoshihiko Nishikawa, Koji Hukushima

**Updated**: 2024-10-22T09:32:17Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive performance. To understand their behaviors, we need to consider the fact that LLMs sometimes show qualitative changes. The natural world also presents such changes called phase transitions, which are defined by singular, divergent statistical quantities. Therefore, an intriguing question is whether qualitative changes in LLMs are phase transitions. In this work, we have conducted extensive analysis on texts generated by LLMs and suggested that a phase transition occurs in LLMs when varying the temperature parameter. Specifically, statistical quantities have divergent properties just at the point between the low-temperature regime, where LLMs generate sentences with clear repetitive structures, and the high-temperature regime, where generated sentences are often incomprehensible. In addition, critical behaviors near the phase transition point, such as a power-law decay of correlation and slow convergence toward the stationary state, are similar to those in natural languages. Our results suggest a meaningful analogy between LLMs and natural phenomena.

**Link**: [arxiv](http://arxiv.org/abs/2406.05335v2),  [pdf](http://arxiv.org/pdf/2406.05335v2)

**Tags**: cond-mat.dis-nn cs.LG 



### Knowledge Distillation-Based Model Extraction Attack using GAN-based   Private Counterfactual Explanations
**Authors**: Fatima Ezzeddine, Omran Ayoub, Silvia Giordano

**Updated**: 2024-10-22T09:31:49Z

**Summary**: In recent years, there has been a notable increase in the deployment of machine learning (ML) models as services (MLaaS) across diverse production software applications. In parallel, explainable AI (XAI) continues to evolve, addressing the necessity for transparency and trustworthiness in ML models. XAI techniques aim to enhance the transparency of ML models by providing insights, in terms of model's explanations, into their decision-making process. Simultaneously, some MLaaS platforms now offer explanations alongside the ML prediction outputs. This setup has elevated concerns regarding vulnerabilities in MLaaS, particularly in relation to privacy leakage attacks such as model extraction attacks (MEA). This is due to the fact that explanations can unveil insights about the inner workings of the model which could be exploited by malicious users. In this work, we focus on investigating how model explanations, particularly counterfactual explanations (CFs), can be exploited for performing MEA within the MLaaS platform. We also delve into assessing the effectiveness of incorporating differential privacy (DP) as a mitigation strategy. To this end, we first propose a novel approach for MEA based on Knowledge Distillation (KD) to enhance the efficiency of extracting a substitute model of a target model exploiting CFs, without any knowledge about the training data distribution by the attacker. Then, we advise an approach for training CF generators incorporating DP to generate private CFs. We conduct thorough experimental evaluations on real-world datasets and demonstrate that our proposed KD-based MEA can yield a high-fidelity substitute model with a reduced number of queries with respect to baseline approaches. Furthermore, our findings reveal that including a privacy layer can allow mitigating the MEA. However, on the account of the quality of CFs, impacts the performance of the explanations.

**Link**: [arxiv](http://arxiv.org/abs/2404.03348v2),  [pdf](http://arxiv.org/pdf/2404.03348v2)

**Tags**: cs.LG cs.AI cs.CR cs.CY 



### ERABAL: Enhancing Role-Playing Agents through Boundary-Aware Learning
**Authors**: Yihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, Kun Gai

**Updated**: 2024-10-22T09:00:19Z

**Summary**: Role-playing is an emerging application in the field of Human-Computer Interaction (HCI), primarily implemented through the alignment training of a large language model (LLM) with assigned characters. Despite significant progress, role-playing agents (RPLAs) still struggle with maintaining role-consistency across conversations, particularly when confronted with boundary queries subtly related to character attributes. In this paper, we present ERABAL, a framework aimed at enhancing RPLAs' role-playing capabilities through boundary-aware learning. ERABAL encompasses a generation pipeline for role-specific dialogues and a concomitant methodology for alignment training. Through comprehensive evaluations, we demonstrate that ERABAL is both efficient and effective. By training with significantly fewer dialogues than those used in leading approaches, ERABAL achieves notable improvements across WikiRoleEval, CharacterEval, and the role-playing subset of MT-Bench compared to the generalist baseline models. Our code and datasets will be made publicly available to support further research.

**Link**: [arxiv](http://arxiv.org/abs/2409.14710v2),  [pdf](http://arxiv.org/pdf/2409.14710v2)

**Tags**: cs.CL cs.AI 



### PerspectiveNet: Multi-View Perception for Dynamic Scene Understanding
**Authors**: Vinh Nguyen

**Updated**: 2024-10-22T08:57:17Z

**Summary**: Generating detailed descriptions from multiple cameras and viewpoints is challenging due to the complex and inconsistent nature of visual data. In this paper, we introduce PerspectiveNet, a lightweight yet efficient model for generating long descriptions across multiple camera views. Our approach utilizes a vision encoder, a compact connector module to convert visual features into a fixed-size tensor, and large language models (LLMs) to harness the strong natural language generation capabilities of LLMs. The connector module is designed with three main goals: mapping visual features onto LLM embeddings, emphasizing key information needed for description generation, and producing a fixed-size feature matrix. Additionally, we augment our solution with a secondary task, the correct frame sequence detection, enabling the model to search for the correct sequence of frames to generate descriptions. Finally, we integrate the connector module, the secondary task, the LLM, and a visual feature extraction model into a single architecture, which is trained for the Traffic Safety Description and Analysis task. This task requires generating detailed, fine-grained descriptions of events from multiple cameras and viewpoints. The resulting model is lightweight, ensuring efficient training and inference, while remaining highly effective.

**Link**: [arxiv](http://arxiv.org/abs/2410.16824v1),  [pdf](http://arxiv.org/pdf/2410.16824v1)

**Tags**: cs.CV cs.AI 



### HAF-RM: A Hybrid Alignment Framework for Reward Model Training
**Authors**: Shujun Liu, Xiaoyu Shen, Yuhang Lai, Siyuan Wang, Shengbin Yue, Zengfeng Huang, Xuanjing Huang, Zhongyu Wei

**Updated**: 2024-10-22T08:53:02Z

**Summary**: The reward model has become increasingly important in alignment, assessment, and data construction for large language models (LLMs). Most existing researchers focus on enhancing reward models through data improvements, following the conventional training framework for reward models that directly optimizes the predicted rewards. In this paper, we propose a hybrid alignment framework HaF-RM for reward model training by introducing an additional constraint on token-level policy probabilities in addition to the reward score. It can simultaneously supervise the internal preference model at the token level and optimize the mapping layer of the reward model at the sequence level. Theoretical justifications and experiment results on five datasets show the validity and effectiveness of our proposed hybrid framework for training a high-quality reward model. By decoupling the reward modeling procedure and incorporating hybrid supervision, our HaF-RM framework offers a principled and effective approach to enhancing the performance and alignment of reward models, a critical component in the responsible development of powerful language models. We release our code at https://haf-rm.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2407.04185v3),  [pdf](http://arxiv.org/pdf/2407.04185v3)

**Tags**: cs.CL 



### Bridging Search and Recommendation in Generative Retrieval: Does One   Task Help the Other?
**Authors**: Gustavo Penha, Ali Vardasbi, Enrico Palumbo, Marco de Nadai, Hugues Bouchard

**Updated**: 2024-10-22T08:49:43Z

**Summary**: Generative retrieval for search and recommendation is a promising paradigm for retrieving items, offering an alternative to traditional methods that depend on external indexes and nearest-neighbor searches. Instead, generative models directly associate inputs with item IDs. Given the breakthroughs of Large Language Models (LLMs), these generative systems can play a crucial role in centralizing a variety of Information Retrieval (IR) tasks in a single model that performs tasks such as query understanding, retrieval, recommendation, explanation, re-ranking, and response generation. Despite the growing interest in such a unified generative approach for IR systems, the advantages of using a single, multi-task model over multiple specialized models are not well established in the literature. This paper investigates whether and when such a unified approach can outperform task-specific models in the IR tasks of search and recommendation, broadly co-existing in multiple industrial online platforms, such as Spotify, YouTube, and Netflix. Previous work shows that (1) the latent representations of items learned by generative recommenders are biased towards popularity, and (2) content-based and collaborative-filtering-based information can improve an item's representations. Motivated by this, our study is guided by two hypotheses: [H1] the joint training regularizes the estimation of each item's popularity, and [H2] the joint training regularizes the item's latent representations, where search captures content-based aspects of an item and recommendation captures collaborative-filtering aspects. Our extensive experiments with both simulated and real-world data support both [H1] and [H2] as key contributors to the effectiveness improvements observed in the unified search and recommendation generative models over the single-task approaches.

**Link**: [arxiv](http://arxiv.org/abs/2410.16823v1),  [pdf](http://arxiv.org/pdf/2410.16823v1)

**Tags**: cs.IR 



### Can Large Language Models Act as Ensembler for Multi-GNNs?
**Authors**: Hanqi Duan, Yao Cheng, Jianxiang Yu, Xiang Li

**Updated**: 2024-10-22T08:48:52Z

**Summary**: Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, GNNs lack the inherent semantic understanding capability of rich textual nodesattributes, limiting their effectiveness in applications. On the other hand, we empirically observe that for existing GNN models, no one can consistently outperforms others across diverse datasets. In this paper, we study whether LLMs can act as an ensembler for multi-GNNs and propose the LensGNN model. The model first aligns multiple GNNs, mapping the representations of different GNNs into the same space. Then, through LoRA fine-tuning, it aligns the space between the GNN and the LLM, injecting graph tokens and textual information into LLMs. This allows LensGNN to integrate multiple GNNs and leverage LLM's strengths, resulting in better performance. Experimental results show that LensGNN outperforms existing models. This research advances text-attributed graph ensemble learning by providing a robust, superior solution for integrating semantic and structural information. We provide our code and data here: https://anonymous.4open.science/r/EnsemGNN-E267/.

**Link**: [arxiv](http://arxiv.org/abs/2410.16822v1),  [pdf](http://arxiv.org/pdf/2410.16822v1)

**Tags**: cs.AI 



### Combining Ontological Knowledge and Large Language Model for   User-Friendly Service Robots
**Authors**: Haru Nakajima, Jun Miura

**Updated**: 2024-10-22T08:32:01Z

**Summary**: Lifestyle support through robotics is an increasingly promising field, with expectations for robots to take over or assist with chores like floor cleaning, table setting and clearing, and fetching items. The growth of AI, particularly foundation models, such as large language models (LLMs) and visual language models (VLMs), is significantly shaping this sector. LLMs, by facilitating natural interactions and providing vast general knowledge, are proving invaluable for robotic tasks. This paper zeroes in on the benefits of LLMs for "bring-me" tasks, where robots fetch specific items for users, often based on vague instructions. Our previous efforts utilized an ontology extended to handle environmental data to decipher such vagueness, but faced limitations when unresolvable ambiguities required user intervention for clarity. Here, we enhance our approach by integrating LLMs for providing additional commonsense knowledge, pairing it with ontological data to mitigate the issue of hallucinations and reduce the need for user queries, thus improving system usability. We present a system that merges these knowledge bases and assess its efficacy on "bring-me" tasks, aiming to provide a more seamless and efficient robotic assistance experience.

**Link**: [arxiv](http://arxiv.org/abs/2410.16804v1),  [pdf](http://arxiv.org/pdf/2410.16804v1)

**Tags**: cs.RO 



### Binarized Diffusion Model for Image Super-Resolution
**Authors**: Zheng Chen, Haotong Qin, Yong Guo, Xiongfei Su, Xin Yuan, Linghe Kong, Yulun Zhang

**Updated**: 2024-10-22T08:28:13Z

**Summary**: Advanced diffusion models (DMs) perform impressively in image super-resolution (SR), but the high memory and computational costs hinder their deployment. Binarization, an ultra-compression algorithm, offers the potential for effectively accelerating DMs. Nonetheless, due to the model structure and the multi-step iterative attribute of DMs, existing binarization methods result in significant performance degradation. In this paper, we introduce a novel binarized diffusion model, BI-DiffSR, for image SR. First, for the model structure, we design a UNet architecture optimized for binarization. We propose the consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to maintain dimension consistent and facilitate the full-precision information transfer. Meanwhile, we design the channel-shuffle-fusion (CS-Fusion) to enhance feature fusion in skip connection. Second, for the activation difference across timestep, we design the timestep-aware redistribution (TaR) and activation function (TaA). The TaR and TaA dynamically adjust the distribution of activations based on different timesteps, improving the flexibility and representation alability of the binarized module. Comprehensive experiments demonstrate that our BI-DiffSR outperforms existing binarization methods. Code is released at: https://github.com/zhengchen1999/BI-DiffSR.

**Link**: [arxiv](http://arxiv.org/abs/2406.05723v2),  [pdf](http://arxiv.org/pdf/2406.05723v2)

**Tags**: cs.CV 



### Controlled Low-Rank Adaptation with Subspace Regularization for   Continued Training on Large Language Models
**Authors**: Yuheng Lu, Bingshuo Qian, Caixia Yuan, Huixing Jiang, Xiaojie Wang

**Updated**: 2024-10-22T08:27:23Z

**Summary**: Large language models (LLMs) exhibit remarkable capabilities in natural language processing but face catastrophic forgetting when learning new tasks, where adaptation to a new domain leads to a substantial decline in performance on previous tasks. In this paper, we propose Controlled LoRA (CLoRA), a subspace regularization method on LoRA structure. Aiming to reduce the scale of output change while introduce minimal constraint on model capacity, CLoRA imposes constraint on the direction of updating matrix null space. Experimental results on commonly used LLM finetuning tasks reveal that CLoRA significantly outperforms existing LoRA subsequent methods on both in-domain and outdomain evaluations, highlighting the superority of CLoRA as a effective parameter-efficient finetuning method with catastrophic forgetting mitigating. Further investigation for model parameters indicates that CLoRA effectively balances the trade-off between model capacity and degree of forgetting.

**Link**: [arxiv](http://arxiv.org/abs/2410.16801v1),  [pdf](http://arxiv.org/pdf/2410.16801v1)

**Tags**: cs.CL cs.AI 



### Mining Glitch Tokens in Large Language Models via Gradient-based   Discrete Optimization
**Authors**: Zihui Wu, Haichang Gao, Ping Wang, Shudong Zhang, Zhaoxiang Liu, Shiguo Lian

**Updated**: 2024-10-22T08:22:46Z

**Summary**: Glitch tokens in Large Language Models (LLMs) can trigger unpredictable behaviors, compromising model reliability and safety. Existing detection methods often rely on manual observation to infer the prior distribution of glitch tokens, which is inefficient and lacks adaptability across diverse model architectures. To address these limitations, we introduce GlitchMiner, a gradient-based discrete optimization framework designed for efficient glitch token detection in LLMs. GlitchMiner leverages an entropy-based loss function to quantify the uncertainty in model predictions and integrates first-order Taylor approximation with a local search strategy to effectively explore the token space. Our evaluation across various mainstream LLM architectures demonstrates that GlitchMiner surpasses existing methods in both detection precision and adaptability. In comparison to the previous state-of-the-art, GlitchMiner achieves an average improvement of 19.07% in precision@1000 for glitch token detection. By enabling efficient detection of glitch tokens, GlitchMiner provides a valuable tool for assessing and mitigating potential vulnerabilities in LLMs, contributing to their overall security.

**Link**: [arxiv](http://arxiv.org/abs/2410.15052v2),  [pdf](http://arxiv.org/pdf/2410.15052v2)

**Tags**: cs.AI 



### Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced   Extrapolation in LLMs
**Authors**: Xin Ma, Yang Liu, Jingjing Liu, Xiaoxu Ma

**Updated**: 2024-10-22T08:00:00Z

**Summary**: Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths. In this work, we conduct a theoretical analysis to better understand why No Position Encoding (NoPE) fails outside its effective range, as well as examining the power of Position Encoding (PE) in this context. Our findings reveal that with meticulous weave position, PE can indeed be extended beyond effective range. Our theorems establish that LLMs equipped with weave PE can achieve improved extrapolation performance without additional cost. Furthermore, we introduce a novel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based triangular attention matrix and applies Stair PE to manage the final chunk. This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed. Extensive experiments validate the effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable solution to enhancing LLMs applicative reach.

**Link**: [arxiv](http://arxiv.org/abs/2410.15859v2),  [pdf](http://arxiv.org/pdf/2410.15859v2)

**Tags**: cs.LG cs.AI 



### Beyond Retrieval: Generating Narratives in Conversational Recommender   Systems
**Authors**: Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi

**Updated**: 2024-10-22T07:53:41Z

**Summary**: The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.   First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.

**Link**: [arxiv](http://arxiv.org/abs/2410.16780v1),  [pdf](http://arxiv.org/pdf/2410.16780v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Dynamic Intelligence Assessment: Benchmarking LLMs on the Road to AGI   with a Focus on Model Confidence
**Authors**: Norbert Tihanyi, Tamas Bisztray, Richard A. Dubniczky, Rebeka Toth, Bertalan Borsos, Bilel Cherif, Mohamed Amine Ferrag, Lajos Muzsai, Ridhi Jain, Ryan Marinelli, Lucas C. Cordeiro, Merouane Debbah

**Updated**: 2024-10-22T07:46:35Z

**Summary**: As machine intelligence evolves, the need to test and compare the problem-solving abilities of different AI models grows. However, current benchmarks are often overly simplistic, allowing models to perform uniformly well, making it difficult to distinguish their capabilities. Additionally, benchmarks typically rely on static question-answer pairs, which models might memorize or guess. To address these limitations, we introduce the Dynamic Intelligence Assessment (DIA), a novel methodology for testing AI models using dynamic question templates and improved metrics across multiple disciplines such as mathematics, cryptography, cybersecurity, and computer science. The accompanying DIA-Bench dataset, which includes 150 diverse and challenging task templates with mutable parameters, is presented in various formats such as text, PDFs, compiled binaries, and visual puzzles. Our framework introduces four new metrics to assess a model's reliability and confidence across multiple attempts. These metrics revealed that even simple questions are frequently answered incorrectly when posed in varying forms, highlighting significant gaps in models' reliability. Notably, models like GPT-4o tended to overestimate their mathematical abilities, while ChatGPT-4o demonstrated better decision-making and performance through effective tool usage. We evaluated eight state-of-the-art large language models (LLMs) using DIA-Bench, showing that current models struggle with complex tasks and often display unexpectedly low confidence, even with simpler questions. The DIA framework sets a new standard for assessing not only problem-solving but also a model's adaptive intelligence and ability to assess its own limitations. The dataset is publicly available on our project's website.

**Link**: [arxiv](http://arxiv.org/abs/2410.15490v2),  [pdf](http://arxiv.org/pdf/2410.15490v2)

**Tags**: cs.AI cs.MA 



### Context-Aware LLM Translation System Using Conversation Summarization   and Dialogue History
**Authors**: Mingi Sung, Seungmin Lee, Jiwon Kim, Sejoon Kim

**Updated**: 2024-10-22T07:45:18Z

**Summary**: Translating conversational text, particularly in customer support contexts, presents unique challenges due to its informal and unstructured nature. We propose a context-aware LLM translation system that leverages conversation summarization and dialogue history to enhance translation quality for the English-Korean language pair. Our approach incorporates the two most recent dialogues as raw data and a summary of earlier conversations to manage context length effectively. We demonstrate that this method significantly improves translation accuracy, maintaining coherence and consistency across conversations. This system offers a practical solution for customer support translation tasks, addressing the complexities of conversational text.

**Link**: [arxiv](http://arxiv.org/abs/2410.16775v1),  [pdf](http://arxiv.org/pdf/2410.16775v1)

**Tags**: cs.CL 



### ETF: An Entity Tracing Framework for Hallucination Detection in Code   Summaries
**Authors**: Kishan Maharaj, Vitobha Munigala, Srikanth G. Tamilselvam, Prince Kumar, Sayandeep Sen, Palani Kodeswaran, Abhijit Mishra, Pushpak Bhattacharyya

**Updated**: 2024-10-22T07:19:40Z

**Summary**: Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarization. However, LLMs are prone to hallucination-outputs that stray from intended meanings. Detecting hallucinations in code summarization is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset with $\sim$10K samples, curated specifically for hallucination detection in code summarization. We further propose a novel Entity Tracing Framework (ETF) that a) utilizes static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the effectiveness of the framework, leading to a 0.73 F1 score. This approach provides an interpretable method for detecting hallucinations by grounding entities, allowing us to evaluate summary accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2410.14748v2),  [pdf](http://arxiv.org/pdf/2410.14748v2)

**Tags**: cs.SE cs.AI cs.CL 



### LASER: Script Execution by Autonomous Agents for On-demand Traffic   Simulation
**Authors**: Hao Gao, Jingyue Wang, Wenyang Fang, Jingwei Xu, Yunpeng Huang, Taolue Chen, Xiaoxing Ma

**Updated**: 2024-10-22T07:14:11Z

**Summary**: Autonomous Driving Systems (ADS) require diverse and safety-critical traffic scenarios for effective training and testing, but the existing data generation methods struggle to provide flexibility and scalability. We propose LASER, a novel frame-work that leverage large language models (LLMs) to conduct traffic simulations based on natural language inputs. The framework operates in two stages: it first generates scripts from user-provided descriptions and then executes them using autonomous agents in real time. Validated in the CARLA simulator, LASER successfully generates complex, on-demand driving scenarios, significantly improving ADS training and testing data generation.

**Link**: [arxiv](http://arxiv.org/abs/2410.16197v2),  [pdf](http://arxiv.org/pdf/2410.16197v2)

**Tags**: cs.RO cs.MA 



### Language Model Alignment in Multilingual Trolley Problems
**Authors**: Zhijing Jin, Max Kleiman-Weiner, Giorgio Piatti, Sydney Levine, Jiarui Liu, Fernando Gonzalez, Francesco Ortu, Andrs Strausz, Mrinmaya Sachan, Rada Mihalcea, Yejin Choi, Bernhard Schlkopf

**Updated**: 2024-10-22T06:48:54Z

**Summary**: We evaluate the moral alignment of large language models (LLMs) with human preferences in multilingual trolley problems. Building on the Moral Machine experiment, which captures over 40 million human judgments across 200+ countries, we develop a cross-lingual corpus of moral dilemma vignettes in over 100 languages called MultiTP. This dataset enables the assessment of LLMs' decision-making processes in diverse linguistic contexts. Our analysis explores the alignment of 19 different LLMs with human judgments, capturing preferences across six moral dimensions: species, gender, fitness, status, age, and the number of lives involved. By correlating these preferences with the demographic distribution of language speakers and examining the consistency of LLM responses to various prompt paraphrasings, our findings provide insights into cross-lingual and ethical biases of LLMs and their intersection. We discover significant variance in alignment across languages, challenging the assumption of uniform moral reasoning in AI systems and highlighting the importance of incorporating diverse perspectives in AI ethics. The results underscore the need for further research on the integration of multilingual dimensions in responsible AI research to ensure fair and equitable AI interactions worldwide. Our code and data are at https://github.com/causalNLP/moralmachine

**Link**: [arxiv](http://arxiv.org/abs/2407.02273v4),  [pdf](http://arxiv.org/pdf/2407.02273v4)

**Tags**: cs.CL 



### UCFE: A User-Centric Financial Expertise Benchmark for Large Language   Models
**Authors**: Yuzhe Yang, Yifei Zhang, Yan Hu, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang, Haining Wang, Qianqian Xie, Jimin Huang, Honghai Yu, Benyou Wang

**Updated**: 2024-10-22T06:47:43Z

**Summary**: This paper introduces the UCFE: User-Centric Financial Expertise benchmark, an innovative framework designed to evaluate the ability of large language models (LLMs) to handle complex real-world financial tasks. UCFE benchmark adopts a hybrid approach that combines human expert evaluations with dynamic, task-specific interactions to simulate the complexities of evolving financial scenarios. Firstly, we conducted a user study involving 804 participants, collecting their feedback on financial tasks. Secondly, based on this feedback, we created our dataset that encompasses a wide range of user intents and interactions. This dataset serves as the foundation for benchmarking 12 LLM services using the LLM-as-Judge methodology. Our results show a significant alignment between benchmark scores and human preferences, with a Pearson correlation coefficient of 0.78, confirming the effectiveness of the UCFE dataset and our evaluation approach. UCFE benchmark not only reveals the potential of LLMs in the financial sector but also provides a robust framework for assessing their performance and user satisfaction. The benchmark dataset and evaluation code are available.

**Link**: [arxiv](http://arxiv.org/abs/2410.14059v2),  [pdf](http://arxiv.org/pdf/2410.14059v2)

**Tags**: q-fin.CP cs.CE cs.CL 



### Experiences with Sub-Arctic Sensor Network Deployment and Feasibility of   Geothermal Energy Harvesting
**Authors**: Priyesh Pappinisseri Puluckul, Maarten Weyn

**Updated**: 2024-10-22T06:47:21Z

**Summary**: This paper discusses the experiences gained from designing, deploying and maintaining low-power wireless sensor networks in three geothermally active remote locations in Iceland. The purpose of deploying the network was to collect soil temperature data and investigate the impact of global warming on (sub)Arctic climate and subsequent carbon release. Functional networks from three sites with no direct access to power and the internet have been providing researchers with insight into the warming impacts since 2021. The network employs low-power wireless sensor nodes equipped with DASH7 communication protocol, providing real-time data and remote access to sensors and instruments deployed in the field. In addition to discussing the architecture and deployment of the network, we conduct a primary analysis using models and methods to demonstrate the feasibility of harvesting energy from the temperature gradient between geothermally active soil and air.

**Link**: [arxiv](http://arxiv.org/abs/2407.04594v2),  [pdf](http://arxiv.org/pdf/2407.04594v2)

**Tags**: cs.NI 



### LLM-Assisted Red Teaming of Diffusion Models through "Failures Are   Fated, But Can Be Faded"
**Authors**: Som Sagar, Aditya Taparia, Ransalu Senanayake

**Updated**: 2024-10-22T06:46:09Z

**Summary**: In large deep neural networks that seem to perform surprisingly well on many tasks, we also observe a few failures related to accuracy, social biases, and alignment with human values, among others. Therefore, before deploying these models, it is crucial to characterize this failure landscape for engineers to debug or audit models. Nevertheless, it is infeasible to exhaustively test for all possible combinations of factors that could lead to a model's failure. In this paper, we improve the "Failures are fated, but can be faded" framework (arXiv:2406.07145)--a post-hoc method to explore and construct the failure landscape in pre-trained generative models--with a variety of deep reinforcement learning algorithms, screening tests, and LLM-based rewards and state generation. With the aid of limited human feedback, we then demonstrate how to restructure the failure landscape to be more desirable by moving away from the discovered failure modes. We empirically demonstrate the effectiveness of the proposed method on diffusion models. We also highlight the strengths and weaknesses of each algorithm in identifying failure modes.

**Link**: [arxiv](http://arxiv.org/abs/2410.16738v1),  [pdf](http://arxiv.org/pdf/2410.16738v1)

**Tags**: cs.LG 



### Forewarned is Forearmed: Leveraging LLMs for Data Synthesis through   Failure-Inducing Exploration
**Authors**: Qintong Li, Jiahui Gao, Sheng Wang, Renjie Pi, Xueliang Zhao, Chuan Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong

**Updated**: 2024-10-22T06:43:28Z

**Summary**: Large language models (LLMs) have significantly benefited from training on diverse, high-quality task-specific data, leading to impressive performance across a range of downstream applications. Current methods often rely on human-annotated data or predefined task templates to direct powerful LLMs in synthesizing task-relevant data for effective model training. However, this dependence on manually designed components may constrain the scope of generated data, potentially overlooking critical edge cases or novel scenarios that could challenge the model. In this paper, we present a novel approach, ReverseGen, designed to automatically generate effective training samples that expose the weaknesses of LLMs. Specifically, we introduce a dedicated proposer trained to produce queries that lead target models to generate unsatisfactory responses. These failure-inducing queries are then used to construct training data, helping to address the models' shortcomings and improve overall performance. Our approach is flexible and can be applied to models of various scales (3B, 7B, and 8B). We evaluate ReverseGen on three key applications (safety, honesty, and math), demonstrating that our generated data is both highly effective and diverse. Models fine-tuned with ReverseGen-generated data consistently outperform those trained on human-annotated or general model-generated data, offering a new perspective on data synthesis for task-specific LLM enhancement.

**Link**: [arxiv](http://arxiv.org/abs/2410.16736v1),  [pdf](http://arxiv.org/pdf/2410.16736v1)

**Tags**: cs.CL 



### COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for   Aligning Large Language Models to Online Communities
**Authors**: Zihao He, Minh Duc Chu, Rebecca Dorn, Siyi Guo, Kristina Lerman

**Updated**: 2024-10-22T06:38:07Z

**Summary**: Social scientists use surveys to probe the opinions and beliefs of populations, but these methods are slow, costly, and prone to biases. Recent advances in large language models (LLMs) enable the creating of computational representations or "digital twins" of populations that generate human-like responses mimicking the population's language, styles, and attitudes. We introduce Community-Cross-Instruct, an unsupervised framework for aligning LLMs to online communities to elicit their beliefs. Given a corpus of a community's online discussions, Community-Cross-Instruct automatically generates instruction-output pairs by an advanced LLM to (1) finetune a foundational LLM to faithfully represent that community, and (2) evaluate the alignment of the finetuned model to the community. We demonstrate the method's utility in accurately representing political and diet communities on Reddit. Unlike prior methods requiring human-authored instructions, Community-Cross-Instruct generates instructions in a fully unsupervised manner, enhancing scalability and generalization across domains. This work enables cost-effective and automated surveying of diverse online communities.

**Link**: [arxiv](http://arxiv.org/abs/2406.12074v3),  [pdf](http://arxiv.org/pdf/2406.12074v3)

**Tags**: cs.CL 



### Granularity Matters in Long-Tail Learning
**Authors**: Shizhen Zhao, Xin Wen, Jiahui Liu, Chuofan Ma, Chunfeng Yuan, Xiaojuan Qi

**Updated**: 2024-10-22T06:35:13Z

**Summary**: Balancing training on long-tail data distributions remains a long-standing challenge in deep learning. While methods such as re-weighting and re-sampling help alleviate the imbalance issue, limited sample diversity continues to hinder models from learning robust and generalizable feature representations, particularly for tail classes. In contrast to existing methods, we offer a novel perspective on long-tail learning, inspired by an observation: datasets with finer granularity tend to be less affected by data imbalance. In this paper, we investigate this phenomenon through both quantitative and qualitative studies, showing that increased granularity enhances the generalization of learned features in tail categories. Motivated by these findings, we propose a method to increase dataset granularity through category extrapolation. Specifically, we introduce open-set auxiliary classes that are visually similar to existing ones, aiming to enhance representation learning for both head and tail classes. This forms the core contribution and insight of our approach. To automate the curation of auxiliary data, we leverage large language models (LLMs) as knowledge bases to search for auxiliary categories and retrieve relevant images through web crawling. To prevent the overwhelming presence of auxiliary classes from disrupting training, we introduce a neighbor-silencing loss that encourages the model to focus on class discrimination within the target dataset. During inference, the classifier weights for auxiliary categories are masked out, leaving only the target class weights for use. Extensive experiments and ablation studies on three standard long-tail benchmarks demonstrate the effectiveness of our approach, notably outperforming strong baseline methods that use the same amount of data. The code will be made publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2410.15980v2),  [pdf](http://arxiv.org/pdf/2410.15980v2)

**Tags**: cs.CV 



### A Text is Worth Several Tokens: Text Embedding from LLMs Secretly Aligns   Well with The Key Tokens
**Authors**: Zhijie Nie, Richong Zhang, Zhanyu Wu

**Updated**: 2024-10-22T06:32:10Z

**Summary**: Text embeddings from large language models (LLMs) have achieved excellent results in tasks such as information retrieval, semantic textual similarity, etc. In this work, we show an interesting finding: when feeding a text into the embedding LLMs, the obtained text embedding will be able to be aligned with the key tokens in the input text. We first fully analyze this phenomenon on eight embedding LLMs and show that this phenomenon is universal and is not affected by model architecture, training strategy, and embedding method. With a deeper analysis, we then find that the main change in embedding space between the embedding LLMs and their original generative LLMs is in the first principal component. By adjusting the first principal component, we can align text embedding with the key tokens. Finally, we give several examples to demonstrate the vast application potential of this finding: (1) we propose a simple and practical sparse retrieval method based on the aligned tokens, which can achieve 80\% of the dense retrieval effect of the same model while reducing the computation significantly; (2) we show that our findings provide a fresh perspective to help understand fuzzy concepts (e.g., semantic relatedness vs. semantic similarity) and emerging technologies (e.g., instruction-following embedding) in this field.

**Link**: [arxiv](http://arxiv.org/abs/2406.17378v2),  [pdf](http://arxiv.org/pdf/2406.17378v2)

**Tags**: cs.CL cs.IR 



### Progressive Compositionality In Text-to-Image Generative Models
**Authors**: Xu Han, Linghao Jin, Xiaofeng Liu, Paul Pu Liang

**Updated**: 2024-10-22T05:59:29Z

**Summary**: Despite the impressive text-to-image (T2I) synthesis capabilities of diffusion models, they often struggle to understand compositional relationships between objects and attributes, especially in complex settings. Existing solutions have tackled these challenges by optimizing the cross-attention mechanism or learning from the caption pairs with minimal semantic changes. However, can we generate high-quality complex contrastive images that diffusion models can directly discriminate based on visual representations? In this work, we leverage large-language models (LLMs) to compose realistic, complex scenarios and harness Visual-Question Answering (VQA) systems alongside diffusion models to automatically curate a contrastive dataset, ConPair, consisting of 15k pairs of high-quality contrastive images. These pairs feature minimal visual discrepancies and cover a wide range of attribute categories, especially complex and natural scenarios. To learn effectively from these error cases, i.e., hard negative images, we propose EvoGen, a new multi-stage curriculum for contrastive learning of diffusion models. Through extensive experiments across a wide range of compositional scenarios, we showcase the effectiveness of our proposed framework on compositional T2I benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2410.16719v1),  [pdf](http://arxiv.org/pdf/2410.16719v1)

**Tags**: cs.CV cs.LG 



### Magnetic Preference Optimization: Achieving Last-iterate Convergence for   Language Models Alignment
**Authors**: Mingzhi Wang, Chengdong Ma, Qizhi Chen, Linjian Meng, Yang Han, Jiancong Xiao, Zhaowei Zhang, Jing Huo, Weijie J. Su, Yaodong Yang

**Updated**: 2024-10-22T05:51:34Z

**Summary**: Self-play methods have demonstrated remarkable success in enhancing model capabilities across various domains. In the context of Reinforcement Learning from Human Feedback (RLHF), self-play not only boosts Large Language Model (LLM) performance but also overcomes the limitations of traditional Bradley-Terry (BT) model assumptions by finding the Nash equilibrium (NE) of a preference-based, two-player constant-sum game. However, existing methods either guarantee only average-iterate convergence, incurring high storage and inference costs, or converge to the NE of a regularized game, failing to accurately reflect true human preferences. In this paper, we introduce Magnetic Preference Optimization (MPO), a novel approach capable of achieving last-iterate convergence to the NE of the original game, effectively overcoming the limitations of existing methods. Building upon Magnetic Mirror Descent (MMD), MPO attains a linear convergence rate, making it particularly suitable for fine-tuning LLMs. To ensure our algorithm is both theoretically sound and practically viable, we present a simple yet effective implementation that adapts the theoretical insights to the RLHF setting. Empirical results demonstrate that MPO can significantly enhance the performance of LLMs, highlighting the potential of self-play methods in alignment.

**Link**: [arxiv](http://arxiv.org/abs/2410.16714v1),  [pdf](http://arxiv.org/pdf/2410.16714v1)

**Tags**: cs.CL 



### Position Engineering: Boosting Large Language Models through Positional   Information Manipulation
**Authors**: Zhiyuan He, Huiqiang Jiang, Zilong Wang, Yuqing Yang, Luna Qiu, Lili Qiu

**Updated**: 2024-10-22T05:45:46Z

**Summary**: The performance of large language models (LLMs) is significantly influenced by the quality of the prompts provided. In response, researchers have developed enormous prompt engineering strategies aimed at modifying the prompt text to enhance task performance. In this paper, we introduce a novel technique termed position engineering, which offers a more efficient way to guide large language models. Unlike prompt engineering, which requires substantial effort to modify the text provided to LLMs, position engineering merely involves altering the positional information in the prompt without modifying the text itself. We have evaluated position engineering in two widely-used LLM scenarios: retrieval-augmented generation (RAG) and in-context learning (ICL). Our findings show that position engineering substantially improves upon the baseline in both cases. Position engineering thus represents a promising new strategy for exploiting the capabilities of large language models.

**Link**: [arxiv](http://arxiv.org/abs/2404.11216v2),  [pdf](http://arxiv.org/pdf/2404.11216v2)

**Tags**: cs.CL cs.AI cs.LG 



### PHAnToM: Persona-based Prompting Has An Effect on Theory-of-Mind   Reasoning in Large Language Models
**Authors**: Fiona Anting Tan, Gerard Christopher Yeo, Kokil Jaidka, Fanyou Wu, Weijie Xu, Vinija Jain, Aman Chadha, Yang Liu, See-Kiong Ng

**Updated**: 2024-10-22T05:44:04Z

**Summary**: The use of LLMs in natural language reasoning has shown mixed results, sometimes rivaling or even surpassing human performance in simpler classification tasks while struggling with social-cognitive reasoning, a domain where humans naturally excel. These differences have been attributed to many factors, such as variations in prompting and the specific LLMs used. However, no reasons appear conclusive, and no clear mechanisms have been established in prior work. In this study, we empirically evaluate how role-playing prompting influences Theory-of-Mind (ToM) reasoning capabilities. Grounding our rsearch in psychological theory, we propose the mechanism that, beyond the inherent variance in the complexity of reasoning tasks, performance differences arise because of socially-motivated prompting differences. In an era where prompt engineering with role-play is a typical approach to adapt LLMs to new contexts, our research advocates caution as models that adopt specific personas might potentially result in errors in social-cognitive reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2403.02246v3),  [pdf](http://arxiv.org/pdf/2403.02246v3)

**Tags**: cs.CL 



### Atomic Fact Decomposition Helps Attributed Question Answering
**Authors**: Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Xiaoli Li, Ru Li, Jeff Z. Pan

**Updated**: 2024-10-22T05:25:54Z

**Summary**: Attributed Question Answering (AQA) aims to provide both a trustworthy answer and a reliable attribution report for a given question. Retrieval is a widely adopted approach, including two general paradigms: Retrieval-Then-Read (RTR) and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown remarkable proficiency, prompting growing interest in AQA among researchers. However, RTR-based AQA often suffers from irrelevant knowledge and rapidly changing information, even when LLMs are adopted, while post-hoc retrieval-based AQA struggles with comprehending long-form answers with complex logic, and precisely identifying the content needing revision and preserving the original intent. To tackle these problems, this paper proposes an Atomic fact decomposition-based Retrieval and Editing (ARE) framework, which decomposes the generated long-form answers into molecular clauses and atomic facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are fine-tuned using a well-constructed dataset, generated from large scale Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from a given set of entities and transforming the result into coherent long-form text. Subsequently, ARE leverages a search engine to retrieve evidences related to atomic facts, inputting these evidences into an LLM-based verifier to determine whether the facts require expansion for re-retrieval or editing. Furthermore, the edited facts are backtracked into the original answer, with evidence aggregated based on the relationship between molecular clauses and atomic facts. Extensive evaluations demonstrate the superior performance of our proposed method over the state-of-the-arts on several datasets, with an additionally proposed new metric $Attr_{p}$ for evaluating the precision of evidence attribution.

**Link**: [arxiv](http://arxiv.org/abs/2410.16708v1),  [pdf](http://arxiv.org/pdf/2410.16708v1)

**Tags**: cs.CL 



