# Arxiv Results
## Keyword: kv cache 
 ### KaVa: Latent Reasoning via Compressed KV-Cache Distillation
**Authors**: Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi

**Updated**: 2025-10-02T17:59:51Z

**Summary**: Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.02312v1),  [pdf](http://arxiv.org/pdf/2510.02312v1)

**Tags**: cs.LG 



### KAIROS: Unified Training for Universal Non-Autoregressive Time Series   Forecasting
**Authors**: Kuiye Ding, Fanda Fan, Zheya Wang, Hongxiao Li, Yifan Wang, Lei Wang, Chunjie Luo, Jianfeng Zhan

**Updated**: 2025-10-02T14:50:50Z

**Summary**: In the World Wide Web, reliable time series forecasts provide the forward-looking signals that drive resource planning, cache placement, and anomaly response, enabling platforms to operate efficiently as user behavior and content distributions evolve. Compared with other domains, time series forecasting for Web applications requires much faster responsiveness to support real-time decision making. We present KAIROS, a non-autoregressive time series forecasting framework that directly models segment-level multi-peak distributions. Unlike autoregressive approaches, KAIROS avoids error accumulation and achieves just-in-time inference, while improving over existing non-autoregressive models that collapse to over-smoothed predictions. Trained on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization on six widely used benchmarks, delivering forecasting performance comparable to state-of-the-art foundation models with similar scale, at a fraction of their inference cost. Beyond empirical results, KAIROS highlights the importance of non-autoregressive design as a scalable paradigm for foundation models in time series.

**Link**: [arxiv](http://arxiv.org/abs/2510.02084v1),  [pdf](http://arxiv.org/pdf/2510.02084v1)

**Tags**: cs.LG cs.AI 



### DiCache: Let Diffusion Model Determine Its Own Cache
**Authors**: Jiazi Bu, Pengyang Ling, Yujie Zhou, Yibin Wang, Yuhang Zang, Dahua Lin, Jiaqi Wang

**Updated**: 2025-10-02T14:42:41Z

**Summary**: Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: "When to cache" and "How to use cache", typically relying on predefined empirical laws or dataset-level priors to determine caching timings and adopting handcrafted rules for multi-step cache utilization. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail to cope with diverse samples. In this paper, a strong sample-specific correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of deep-layer features. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain an on-the-fly indicator for the caching error in real time, enabling the model to dynamically customize the caching schedule for each sample. (2) Dynamic Cache Trajectory Alignment adaptively approximates the deep-layer feature output from multi-step historical caches based on the shallow-layer feature trajectory, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved fidelity over state-of-the-art approaches on various leading diffusion models including WAN 2.1, HunyuanVideo and Flux.

**Link**: [arxiv](http://arxiv.org/abs/2508.17356v2),  [pdf](http://arxiv.org/pdf/2508.17356v2)

**Tags**: cs.CV 



### QSpec: Speculative Decoding with Complementary Quantization Schemes
**Authors**: Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu

**Updated**: 2025-10-02T14:09:03Z

**Summary**: Quantization is widely adopted to accelerate inference and reduce memory consumption in large language models (LLMs). While activation-weight joint quantization enables efficient low-precision decoding, it suffers from substantial performance degradation on multi-step reasoning tasks. We propose QSpec, a novel quantization paradigm that decouples efficiency from quality by integrating two complementary schemes via speculative decoding: low-precision joint quantization for fast drafting and high-precision weight-only quantization for accurate verification. QSpec reuses both weights and KV cache across stages, enabling near-zero-cost switching without retraining or auxiliary models. Compared to high-precision baselines, QSpec achieves up to 1.64x speedup without quality degradation, and outperforms state-of-the-art speculative decoding methods by up to 1.55x in batched settings. Furthermore, QSpec supports plug-and-play deployment and generalizes well across model scales, quantization methods, and workloads. These properties make QSpec a practical and scalable solution for high-fidelity quantized LLM serving under memory-constrained scenarios. Our code is available at https://github.com/hku-netexplo-lab/QSpec.

**Link**: [arxiv](http://arxiv.org/abs/2410.11305v3),  [pdf](http://arxiv.org/pdf/2410.11305v3)

**Tags**: cs.LG cs.AI 



### Study of the $^{20}$Ne($p,Î³$)$^{21}$Na reaction at LUNA
**Authors**: A. Caciolli

**Updated**: 2025-10-02T10:49:54Z

**Summary**: The NeNa-MgAl cycles are involved in the synthesis of Ne, Na, Mg, and Al isotopes. The $^{20}$Ne($p,\gamma$)$^{21}$Na (Q = 2431.68 keV) reaction is the first and slowest reaction of the NeNa cycle and it controls the speed at which the entire cycle proceeds. At the state of the art, the uncertainty on the 20Ne(p,{\gamma})21Na reaction rate affects the production of the elements in the NeNa cycle. In particular, in the temperature range from 0.1 GK to 1 GK, the rate is dominated by the 366 keV resonance corresponding to the excited state of EX = 2797.5 keV and by the direct capture component. The present study focus on the study of the 366 keV resonance and the direct capture below 400 keV. At LUNA (Laboratory for Underground Nuclear Astrophysics) the $^{20}$Ne($p,\gamma$)$^{21}$Na reaction has been measured using the intense proton beam delivered by the LUNA 400 kV accelerator and a windowless differential-pumping gas target. The products of the reaction are detected with two high-purity germanium detectors. The experimental details and preliminary results on the 366 keV resonance and on the direct capture component at very low energies will be shown, together with their possible impact on the $^{20}$Ne($p,\gamma$)$^{21}$Na reaction rate.

**Link**: [arxiv](http://arxiv.org/abs/2510.01884v1),  [pdf](http://arxiv.org/pdf/2510.01884v1)

**Tags**: nucl-ex 



### PiCa: Parameter-Efficient Fine-Tuning with Column Space Projection
**Authors**: Junseo Hwang, Wonguk Cho, Taesup Kim

**Updated**: 2025-10-02T04:11:07Z

**Summary**: Fine-tuning large foundation models is essential for building expert models tailored to specialized tasks and domains, but fully updating billions of parameters is computationally prohibitive. Reducing the number of trainable parameters using parameter-efficient fine-tuning is therefore crucial not only to reduce training costs but also to mitigate storage, caching, and serving overheads during deployment. Prior works, such as Singular Vectors-guided Fine-Tuning, have shown that exploiting the geometry of pre-trained weights can significantly improve parameter-efficiency, but they lack a solid theoretical foundation. In this paper, we introduce Parameter-efficient Fine-tuning with Column Space Projection (PiCa), a novel theoretically grounded PEFT method. We prove that projecting gradients onto the principal column space of pre-trained weights provides an effective inductive bias for adaptation and further enhance parameter efficiency through a novel weight-sharing strategy. Across diverse NLP and vision tasks, PiCa consistently outperforms state-of-the-art baselines under comparable or smaller parameter budgets, demonstrating both theoretical rigor and practical effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2505.20211v2),  [pdf](http://arxiv.org/pdf/2505.20211v2)

**Tags**: cs.LG cs.AI 



### Faster LLM Inference using DBMS-Inspired Preemption and Cache   Replacement Policies
**Authors**: Kyoungmin Kim, Jiacheng Li, Kijae Hong, Anastasia Ailamaki

**Updated**: 2025-10-01T20:30:18Z

**Summary**: LLMs are increasingly used world-wide from daily tasks to agentic systems and data analytics, requiring significant GPU resources. LLM inference systems, however, are slow compared to database systems, and inference performance and mechanism have been often regarded as a black box, limiting the expansion of the use of LLMs inside databases and other performance-critical applications. This paper first analyzes the LLM inference performance and focuses on a data management issue inside LLM inference. We find that inference systems lack an adequate resource cost model and optimization strategy to schedule requests with their intermediate results in a cache reside in GPU memory when executing multiple concurrent inference requests. We adapt classic database techniques by building cost models for concurrent inference requests and a new cache replacement policy tailored for LLM inference, which can substantially save GPU costs.

**Link**: [arxiv](http://arxiv.org/abs/2411.07447v4),  [pdf](http://arxiv.org/pdf/2411.07447v4)

**Tags**: cs.PF cs.AI 



### StreamAgent: Towards Anticipatory Agents for Streaming Video   Understanding
**Authors**: Haolin Yang, Feilong Tang, Linxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Boqian Wang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak

**Updated**: 2025-10-01T19:06:10Z

**Summary**: Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2508.01875v2),  [pdf](http://arxiv.org/pdf/2508.01875v2)

**Tags**: cs.CV 



### Autoregressive Adversarial Post-Training for Real-Time Interactive Video   Generation
**Authors**: Shanchuan Lin, Ceyuan Yang, Hao He, Jianwen Jiang, Yuxi Ren, Xin Xia, Yang Zhao, Xuefeng Xiao, Lu Jiang

**Updated**: 2025-10-01T18:55:20Z

**Summary**: Existing large-scale video generation models are computationally intensive, preventing adoption in real-time and interactive applications. In this work, we propose autoregressive adversarial post-training (AAPT) to transform a pre-trained latent video diffusion model into a real-time, interactive video generator. Our model autoregressively generates a latent frame at a time using a single neural function evaluation (1NFE). The model can stream the result to the user in real time and receive interactive responses as controls to generate the next latent frame. Unlike existing approaches, our method explores adversarial training as an effective paradigm for autoregressive generation. This not only allows us to design an architecture that is more efficient for one-step generation while fully utilizing the KV cache, but also enables training the model in a student-forcing manner that proves to be effective in reducing error accumulation during long video generation. Our experiments demonstrate that our 8B model achieves real-time, 24fps, streaming video generation at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to a minute long (1440 frames). Visit our research website at https://seaweed-apt.com/2

**Link**: [arxiv](http://arxiv.org/abs/2506.09350v2),  [pdf](http://arxiv.org/pdf/2506.09350v2)

**Tags**: cs.CV cs.AI cs.LG 



### HiSpec: Hierarchical Speculative Decoding for LLMs
**Authors**: Avinash Kumar, Sujay Sanghavi, Poulami Das

**Updated**: 2025-10-01T18:04:14Z

**Summary**: Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is $4\times$ slower than token generation when a 3B model speculates for a 70B target model), but most prior works focus only on accelerating drafting. $\textit{``Intermediate"}$ verification reduces verification time by discarding inaccurate draft tokens early, but existing methods incur substantial training overheads in incorporating the intermediate verifier, increase the memory footprint to orchestrate the intermediate verification step, and compromise accuracy by relying on approximate heuristics.   We propose $\underline{\textit{Hi}}\textit{erarchical }\underline{\textit{Spec}}\textit{ulative Decoding (HiSpec)}$, a framework for high-throughput speculative decoding that exploits $\textit{early-exit (EE) models}$ for low-overhead intermediate verification. EE models allow tokens to exit early by skipping layer traversal and are explicitly trained so that hidden states at selected layers can be interpreted, making them uniquely suited for intermediate verification without drastically increasing compute and memory overheads. To improve resource-efficiency even further, we design a methodology that enables HiSpec to re-use key-value caches and hidden states between the draft, intermediate verifier, and target models. To maintain accuracy, HiSpec periodically validates the draft tokens accepted by the intermediate verifier against the target model. Our evaluations using various representative benchmarks and models show that HiSpec improves throughput by 1.28$\times$ on average and by up to 2.01$\times$ compared to the baseline single-layer speculation without compromising accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2510.01336v1),  [pdf](http://arxiv.org/pdf/2510.01336v1)

**Tags**: cs.CL cs.AI cs.LG 



### InfVSR: Breaking Length Limits of Generic Video Super-Resolution
**Authors**: Ziqing Zhang, Kai Liu, Zheng Chen, Xi Li, Yucong Chen, Bingnan Duan, Linghe Kong, Yulun Zhang

**Updated**: 2025-10-01T14:21:45Z

**Summary**: Real-world videos often extend over thousands of frames. Existing video super-resolution (VSR) approaches, however, face two persistent challenges when processing long sequences: (1) inefficiency due to the heavy cost of multi-step denoising for full-length sequences; and (2) poor scalability hindered by temporal decomposition that causes artifacts and discontinuities. To break these limits, we propose InfVSR, which novelly reformulates VSR as an autoregressive-one-step-diffusion paradigm. This enables streaming inference while fully leveraging pre-trained video diffusion priors. First, we adapt the pre-trained DiT into a causal structure, maintaining both local and global coherence via rolling KV-cache and joint visual guidance. Second, we distill the diffusion process into a single step efficiently, with patch-wise pixel supervision and cross-chunk distribution matching. Together, these designs enable efficient and scalable VSR for unbounded-length videos. To fill the gap in long-form video evaluation, we build a new benchmark tailored for extended sequences and further introduce semantic-level metrics to comprehensively assess temporal consistency. Our method pushes the frontier of long-form VSR, achieves state-of-the-art quality with enhanced semantic consistency, and delivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will be available at https://github.com/Kai-Liu001/InfVSR.

**Link**: [arxiv](http://arxiv.org/abs/2510.00948v1),  [pdf](http://arxiv.org/pdf/2510.00948v1)

**Tags**: cs.CV 



### AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block   Size
**Authors**: Guanxi Lu, Hao Mark Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan

**Updated**: 2025-10-01T11:26:36Z

**Summary**: Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26432v2),  [pdf](http://arxiv.org/pdf/2509.26432v2)

**Tags**: cs.LG cs.AI 



### Expected Attention: KV Cache Compression by Estimating Attention from   Future Queries Distribution
**Authors**: Alessio Devoto, Maximilian Jeblick, Simon JÃ©gou

**Updated**: 2025-10-01T08:12:14Z

**Summary**: Memory consumption of the Key-Value (KV) cache represents a major bottleneck for efficient large language model inference. While attention-score-based KV cache pruning shows promise, it faces critical practical limitations: attention scores from future tokens are unavailable during compression, and modern implementations like Flash Attention do not materialize the full attention matrix, making past scores inaccessible. To overcome these challenges, we introduce $\textbf{Expected Attention, a training-free compression method}$ that estimates KV pairs importance by predicting how future queries will attend to them. Our approach leverages the distributional properties of LLM activations to compute expected attention scores in closed form for each KV pair. These scores enable principled ranking and pruning of KV pairs with minimal impact on the residual stream, achieving effective compression without performance degradation. Importantly, our method operates seamlessly across both prefilling and decoding phases, consistently outperforming state-of-the-art baselines in both scenarios. Finally, $\textbf{we release KVPress, a comprehensive library to enable researchers to implement and benchmark KV cache compression methods, already including more than 20 techniques}$.

**Link**: [arxiv](http://arxiv.org/abs/2510.00636v1),  [pdf](http://arxiv.org/pdf/2510.00636v1)

**Tags**: cs.AI cs.CL 



### Panorama: Fast-Track Nearest Neighbors
**Authors**: Vansh Ramani, Alexis Schlomer, Akash Nayar, Panagiotis Karras, Sayan Ranu, Jignesh M. Patel

**Updated**: 2025-10-01T06:38:45Z

**Summary**: Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99\% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90\% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\times$ end-to-end speedup with no recall loss.

**Link**: [arxiv](http://arxiv.org/abs/2510.00566v1),  [pdf](http://arxiv.org/pdf/2510.00566v1)

**Tags**: cs.LG cs.AI cs.DB 



### GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness
**Authors**: Kung-Hsiang Huang, Haoyi Qiu, Yutong Dai, Caiming Xiong, Chien-Sheng Wu

**Updated**: 2025-10-01T05:37:54Z

**Summary**: Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.00536v1),  [pdf](http://arxiv.org/pdf/2510.00536v1)

**Tags**: cs.CL 



### DeepSearch: Overcome the Bottleneck of Reinforcement Learning with   Verifiable Rewards via Monte Carlo Tree Search
**Authors**: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi

**Updated**: 2025-10-01T05:09:42Z

**Summary**: Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.

**Link**: [arxiv](http://arxiv.org/abs/2509.25454v2),  [pdf](http://arxiv.org/pdf/2509.25454v2)

**Tags**: cs.AI cs.CL 



### ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning   Models
**Authors**: Akshat Ramachandran, Marina Neseem, Charbel Sakr, Rangharajan Venkatesan, Brucek Khailany, Tushar Krishna

**Updated**: 2025-10-01T04:09:02Z

**Summary**: The long-output context generation of large reasoning models enables extended chain of thought (CoT) but also drives rapid growth of the key-value (KV) cache, quickly overwhelming GPU memory. To address this challenge, we propose ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on the observation that attention sparsity reveals distinct thought types with varying importance within the CoT. It applies a hybrid quantization-eviction strategy, assigning token precision by thought importance and progressively evicting tokens from less critical thoughts as reasoning trajectories evolve. Furthermore, to implement ThinKV, we design a kernel that extends PagedAttention to enable efficient reuse of evicted tokens' memory slots, eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show that ThinKV achieves near-lossless accuracy with less than 5% of the original KV cache, while improving performance with up to 5.8x higher inference throughput over state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2510.01290v1),  [pdf](http://arxiv.org/pdf/2510.01290v1)

**Tags**: cs.LG 



### Detailed Derivation of the Scalar Explicit Expressions Governing the   Electric Field, Current Density, and Volumetric Power Density in the Four   Types of Linear Divergent MHD Channels Under a Unidirectional Applied   Magnetic Field
**Authors**: Osama A. Marzouk

**Updated**: 2025-10-01T02:56:59Z

**Summary**: The current study belongs to the field of applied mathematics in plasma physics and electric power, where mathematical analysis of the algebraic equations governing the electric field vector, and the electric-current density field vector within a Magnetohydrodynamic (MHD) linear two-dimensional divergent supersonic channel is utilized to derive analytical expressions for these important fields, as well as closed-form equations for the volumetric power density (output electric power per unit volume of the plasma channel). The expressions presented here describe analytically the operation of the MHD channel as an electric power source within an Open-Cycle Magnetohydrodynamic (OCMHD) generator. The four common types of the MHD linear channels are covered here: namely, (1) continuous-electrode Faraday channel, (2) linear Hall channel, (3) segmented-electrode Faraday channel, and (4) diagonal-electrode channel. The mathematical results, their detailed derivation, and the companion graphical illustrations aid in making a proper decision regarding which channel type is the most suitable for a given application.Under typical operational conditions of 5 S/m plasma electric conductivity, 5 T magnetic field, and 2,000 m/s plasma speed, as well as an optimized load factor of 0.5, we estimate the following numerical values (unsigned magnitudes) for the continuous-electrode Faraday channel (with a Hall parameter of 1): useful electric field (across the external electric load): 5 kV/m, useful electric current-density (between the terminal electrodes within the channel): 12.5 kA/m2 , volumetric power density (dissipated by the load per unit volume of plasma): 62.5 MW/m3 , and electric efficiency (for the electric field or voltage): 50%. For the Halllinear channel (with a Hall parameter of 5), these quantitative performance values become25 kV/m, 4.808 kA/m2, 120.19 MW/m3, and 46.30%.

**Link**: [arxiv](http://arxiv.org/abs/2510.01289v1),  [pdf](http://arxiv.org/pdf/2510.01289v1)

**Tags**: physics.plasm-ph 00A79, 03H10 



### Free Draft-and-Verification: Toward Lossless Parallel Decoding for   Diffusion Large Language Models
**Authors**: Shutong Wu, Jiawei Zhang

**Updated**: 2025-09-30T21:28:04Z

**Summary**: Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous "reversal curse" or learning under data-constrained scenarios. However, this bidirectional nature also brings an obstacle that DLLMs are not inherently compatible with KV Cache, and consequently, the inference efficiency is not competitive compared with autoregressive models. Taking advantage of their inherent capability of multi-token prediction, existing parallel decoding algorithms can speed up the DLLM inference, but at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (Freedave), a novel fast sampling algorithm tailored for DLLMs that achieves lossless parallel decoding. Specifically, we propose a pipeline of parallel-decoded candidate generation and verification, which is guaranteed to reproduce the same sequence generated by static sampling, without introducing extra model forward calls. By applying Freedave, the throughput of DLLMs can be boosted up to $2.8\times$ without performance degradation on math reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.00294v1),  [pdf](http://arxiv.org/pdf/2510.00294v1)

**Tags**: cs.LG cs.AI 



### The Pitfalls of KV Cache Compression
**Authors**: Alex Chen, Renato Geh, Aditya Grover, Guy Van den Broeck, Daniel Israel

**Updated**: 2025-09-30T19:55:26Z

**Summary**: KV cache compression promises increased throughput and efficiency with negligible loss in performance. While the gains in throughput are indisputable and recent literature has indeed shown minimal degradation on particular benchmarks, in general the consequences of compression in realistic scenarios such as multi-instruction prompting have been insufficiently studied. In this paper, we identify several pitfalls practitioners should be aware of when deploying KV cache compressed LLMs. Importantly, we show that certain instructions degrade much more rapidly with compression, effectively causing them to be completely ignored by the LLM. As a practical example of that, we highlight system prompt leakage as a case study, empirically showing the impact of compression on leakage and general instruction following. We show several factors that play a role in prompt leakage: compression method, instruction order, and KV eviction bias. We then propose simple changes to KV cache eviction policies that can reduce the impact of these factors and improve the overall performance in multi-instruction tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.00231v1),  [pdf](http://arxiv.org/pdf/2510.00231v1)

**Tags**: cs.LG cs.AI 



### Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals   Long-Range Dependency Pitfalls
**Authors**: Xiaoyan Bai, Itamar Pres, Yuntian Deng, Chenhao Tan, Stuart Shieber, Fernanda ViÃ©gas, Martin Wattenberg, Andrew Lee

**Updated**: 2025-09-30T19:03:26Z

**Summary**: Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.

**Link**: [arxiv](http://arxiv.org/abs/2510.00184v1),  [pdf](http://arxiv.org/pdf/2510.00184v1)

**Tags**: cs.LG cs.AI 



### TASP: Topology-aware Sequence Parallelism
**Authors**: Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang

**Updated**: 2025-09-30T17:15:27Z

**Summary**: Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.

**Link**: [arxiv](http://arxiv.org/abs/2509.26541v1),  [pdf](http://arxiv.org/pdf/2509.26541v1)

**Tags**: cs.LG cs.DC 



### LoLA: Low-Rank Linear Attention With Sparse Caching
**Authors**: Luke McDermott, Robert W. Heath Jr., Rahul Parhi

**Updated**: 2025-09-30T16:42:50Z

**Summary**: The per-token cost of transformer inference scales with context length, preventing its application to lifelong in-context learning. Linear attention is an efficient alternative that maintains a constant memory footprint, even on infinite context lengths. While this is a potential candidate for lifelong learning, it falls short in memory capacity. In this paper, we propose LoLA, a training-free augmentation to linear attention that boosts associative recall. LoLA distributes past key-value pairs from context into three memory systems: (i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize pairs in a sparse, global cache; and (iii) generic pairs in the recurrent hidden state of linear attention. We show through ablations that our self-recall error metric is crucial to efficiently manage long-term associative memories. On pass-key retrieval tasks, LoLA improves the base model's performance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller cache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B and 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2505.23666v2),  [pdf](http://arxiv.org/pdf/2505.23666v2)

**Tags**: cs.CL cs.LG 



### SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study
**Authors**: Yang Xiang, Fernando GarcÃ­a-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings

**Updated**: 2025-09-30T15:44:29Z

**Summary**: This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.

**Link**: [arxiv](http://arxiv.org/abs/2508.18250v3),  [pdf](http://arxiv.org/pdf/2508.18250v3)

**Tags**: cs.ET 



### Fast-dLLM v2: Efficient Block-Diffusion LLM
**Authors**: Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-09-30T14:40:18Z

**Summary**: Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.

**Link**: [arxiv](http://arxiv.org/abs/2509.26328v1),  [pdf](http://arxiv.org/pdf/2509.26328v1)

**Tags**: cs.CL 



### Scaling RL to Long Videos
**Authors**: Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han

**Updated**: 2025-09-30T14:13:20Z

**Summary**: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).

**Link**: [arxiv](http://arxiv.org/abs/2507.07966v4),  [pdf](http://arxiv.org/pdf/2507.07966v4)

**Tags**: cs.CV cs.AI cs.CL 



### FastCoder: Accelerating Repository-level Code Generation via Efficient   Retrieval and Verification
**Authors**: Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Jia Li, Lin Shi

**Updated**: 2025-09-30T09:10:26Z

**Summary**: Code generation is a latency-sensitive task that demands high timeliness. However, with the growing interest and inherent difficulty in repository-level code generation, most existing code generation studies focus on improving the correctness of generated code while overlooking the inference efficiency, which is substantially affected by the overhead during LLM generation. Although there has been work on accelerating LLM inference, these approaches are not tailored to the specific characteristics of code generation; instead, they treat code the same as natural language sequences and ignore its unique syntax and semantic characteristics, which are also crucial for improving efficiency. Consequently, these approaches exhibit limited effectiveness in code generation tasks, particularly for repository-level scenarios with considerable complexity and difficulty. To alleviate this issue, following draft-verification paradigm, we propose FastCoder, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without compromising the quality of the output. FastCoder constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences. Moreover, FastCoder reduces the retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and a context- and LLM preference-aware cache. Experimental results show that FastCoder can reach up to 2.53x and 2.54x speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to 88%. FastCoder can also be integrated with existing correctness-focused code generation approaches to accelerate the LLM generation process, and reach a speedup exceeding 2.6x.

**Link**: [arxiv](http://arxiv.org/abs/2502.17139v2),  [pdf](http://arxiv.org/pdf/2502.17139v2)

**Tags**: cs.AI cs.SE 



### KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction
**Authors**: Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song

**Updated**: 2025-09-30T02:51:05Z

**Summary**: Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by $3$-$4\times$ and FlashAttention decoding latency by approximately $2\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.23416v2),  [pdf](http://arxiv.org/pdf/2505.23416v2)

**Tags**: cs.DB cs.LG 



### dVLA: Diffusion Vision-Language-Action Model with Multimodal   Chain-of-Thought
**Authors**: Junjie Wen, Minjie Zhu, Jiaming Liu, Zhiyuan Liu, Yicun Yang, Linfeng Zhang, Shanghang Zhang, Yichen Zhu, Yi Xu

**Updated**: 2025-09-30T02:36:11Z

**Summary**: Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.

**Link**: [arxiv](http://arxiv.org/abs/2509.25681v1),  [pdf](http://arxiv.org/pdf/2509.25681v1)

**Tags**: cs.RO cs.CV 



### FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers
**Authors**: Liang Qiao, Yue Dai, Yeqi Huang, Hongyu Kan, Jun Shi, Hong An

**Updated**: 2025-09-29T18:57:14Z

**Summary**: Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional capabilities in visual synthesis, yet their deployment remains constrained by substantial computational demands. To alleviate this bottleneck, many sparsity-based acceleration methods have been proposed. However, their diverse sparsity patterns often require customized kernels for high-performance inference, limiting universality. We propose FlashOmni, a unified sparse attention engine compatible with arbitrary DiT architectures. FlashOmni introduces flexible sparse symbols to standardize the representation of a wide range of sparsity strategies, such as feature caching and block-sparse skipping. This unified abstraction enables the execution of diverse sparse computations within a single attention kernel. In addition, FlashOmni designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and further improve efficiency. Experiments demonstrate that FlashOmni delivers near-linear, closely matching the sparsity ratio speedup (1:1) in attention and GEMM-$Q$, and achieves 2.5$\times$-3.8$\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of the theoretical limit). Applied with a multi-granularity sparsity strategy, it enables the Hunyuan model (33K) to achieve about 1.5$\times$ end-to-end acceleration without degrading visual quality.

**Link**: [arxiv](http://arxiv.org/abs/2509.25401v1),  [pdf](http://arxiv.org/pdf/2509.25401v1)

**Tags**: cs.LG cs.AI cs.PF 



### Learning to Parallel: Accelerating Diffusion Large Language Models via   Adaptive Parallel Decoding
**Authors**: Wenrui Bao, Zhiben Chen, Dan Xu, Yuzhang Shang

**Updated**: 2025-09-29T17:59:54Z

**Summary**: Autoregressive decoding in large language models (LLMs) requires $\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token generation through iterative denoising. However, current parallel decoding strategies rely on fixed, input-agnostic heuristics (e.g., confidence thresholds), which fail to adapt to input-specific characteristics, resulting in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work, we explore a more flexible and dynamic approach to parallel decoding. We propose Learning to Parallel Decode (Learn2PD), a framework that trains a lightweight and adaptive filter model to predict, for each token position, whether the current prediction matches the final output. This learned filter approximates an oracle parallel decoding strategy that unmasks tokens only when correctly predicted. Importantly, the filter model is learned in a post-training manner, requiring only a small amount of computation to optimize it (minute-level GPU time). Additionally, we introduce End-of-Text Prediction (EoTP) to detect decoding completion at the end of sequence, avoiding redundant decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that our method achieves up to 22.58$\times$ speedup without any performance drop, and up to 57.51$\times$ when combined with KV-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2509.25188v1),  [pdf](http://arxiv.org/pdf/2509.25188v1)

**Tags**: cs.CL 



### Context-Driven Performance Modeling for Causal Inference Operators on   Neural Processing Units
**Authors**: Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna

**Updated**: 2025-09-29T17:55:43Z

**Summary**: The proliferation of large language models (LLMs) has driven demand for long context inference on resource constrained edge devices. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to the architectural mismatch: quadratic complexity of standard attention mechanisms conflicts with memory and compute patterns of edge accelerators. This paper presents a comprehensive performance analysis of various causal inference operators on a modern NPU. We benchmark standard quadratic attention against several sub-quadratic alternatives, including structured state-space and linear attention models. Our analysis reveals that while sub-quadratic methods offer superior scalability, they introduce distinct computational bottlenecks on the NPU's specialized execution units. We identify that quadratic attention becomes severely memory-bound, suffering from cache inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast, sub-quadratic models can become compute-bound on programmable vector cores. These findings provide critical insights for the co-design of hardware-aware models and optimization strategies to enable on-device AI inference with long-contexts.

**Link**: [arxiv](http://arxiv.org/abs/2509.25155v1),  [pdf](http://arxiv.org/pdf/2509.25155v1)

**Tags**: cs.DC cs.LG 



### METok: Multi-Stage Event-based Token Compression for Efficient Long   Video Understanding
**Authors**: Mengyue Wang, Shuo Chen, Kristian Kersting, Volker Tresp, Yunpu Ma

**Updated**: 2025-09-29T15:20:29Z

**Summary**: Recent advances in Video Large Language Models (VLLMs) have significantly enhanced their ability to understand video content. Nonetheless, processing long videos remains challenging due to high computational demands and the redundancy present in the visual data. In this work, we propose METok, a training-free, Multi-stage Event-based Token compression framework designed to accelerate VLLMs' inference while preserving accuracy. METok progressively eliminates redundant visual tokens across three critical stages: (1) event-aware compression during vision encoding, (2) hierarchical token pruning in the prefilling stage based on semantic alignment and event importance, and (3) a decoding-stage KV Cache optimization that further reduces memory consumption. Our experiments on diverse video benchmarks demonstrate that METok achieves an optimal trade-off between efficiency and accuracy by dynamically selecting informative visual tokens. For instance, equipping LongVA-7B with METok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all while maintaining comparable or even superior accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2506.02850v2),  [pdf](http://arxiv.org/pdf/2506.02850v2)

**Tags**: cs.CV 



### Not All Models Suit Expert Offloading: On Local Routing Consistency of   Mixture-of-Expert Models
**Authors**: Jingcong Liang, Siyuan Wang, Miren Tian, Yitong Li, Duyu Tang, Zhongyu Wei

**Updated**: 2025-09-29T15:15:49Z

**Summary**: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .

**Link**: [arxiv](http://arxiv.org/abs/2505.16056v2),  [pdf](http://arxiv.org/pdf/2505.16056v2)

**Tags**: cs.LG cs.AI 



### SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts   via Token-Level LSH Matching
**Authors**: Xinye Zhao, Spyridon Mastorakis

**Updated**: 2025-09-29T14:16:13Z

**Summary**: As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2509.24832v1),  [pdf](http://arxiv.org/pdf/2509.24832v1)

**Tags**: cs.CL cs.AI 



### Vision Function Layer in Multimodal LLMs
**Authors**: Cheng Shi, Yizhou Yu, Sibei Yang

**Updated**: 2025-09-29T13:45:35Z

**Summary**: This study identifies that visual-related functional decoding is distributed across different decoder layers in Multimodal Large Language Models (MLLMs). Typically, each function, such as counting, grounding, or OCR recognition, narrows down to two or three layers, which we define as Vision Function Layers (VFL). Additionally, the depth and its order of different VFLs exhibits a consistent pattern across different MLLMs, which is well-aligned with human behaviors (e.g., recognition occurs first, followed by counting, and then grounding). These findings are derived from Visual Token Swapping, our novel analytical framework that modifies targeted KV cache entries to precisely elucidate layer-specific functions during decoding. Furthermore, these insights offer substantial utility in tailoring MLLMs for real-world downstream applications. For instance, when LoRA training is selectively applied to VFLs whose functions align with the training data, VFL-LoRA not only outperform full-LoRA but also prevent out-of-domain function forgetting. Moreover, by analyzing the performance differential on training data when particular VFLs are ablated, VFL-select automatically classifies data by function, enabling highly efficient data selection to directly bolster corresponding capabilities. Consequently, VFL-select surpasses human experts in data selection, and achieves 98% of full-data performance with only 20% of the original dataset. This study delivers deeper comprehension of MLLM visual processing, fostering the creation of more efficient, interpretable, and robust models.

**Link**: [arxiv](http://arxiv.org/abs/2509.24791v1),  [pdf](http://arxiv.org/pdf/2509.24791v1)

**Tags**: cs.CV 



### SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long   Sequences
**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

**Updated**: 2025-09-29T12:34:50Z

**Summary**: Speculative decoding is a widely used technique for accelerating inference in large language models (LLMs), but its performance degrades as input length grows, with significant drops even at moderate lengths. Yet, this early degradation has remained largely underexplored. We introduce SpecExtend, a drop-in enhancement that improves speculative decoding on long sequences without additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention to accelerate prefill and verification steps. To improve both draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that leverages the target model's attention scores to dynamically select relevant context for the smaller draft model. Extensive evaluations show that SpecExtend accelerates speculative decoding by up to 2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while preserving the short-input performance of state-of-the-art frameworks. Our code is available at https://github.com/jycha98/SpecExtend .

**Link**: [arxiv](http://arxiv.org/abs/2505.20776v3),  [pdf](http://arxiv.org/pdf/2505.20776v3)

**Tags**: cs.CL cs.AI cs.LG I.2.7; C.4 



### SANA-Video: Efficient Video Generation with Block Linear Diffusion   Transformer
**Authors**: Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-09-29T12:28:09Z

**Summary**: We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.24695v1),  [pdf](http://arxiv.org/pdf/2509.24695v1)

**Tags**: cs.CV cs.AI 



### SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in   Long-Context LLM Serving
**Authors**: Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng

**Updated**: 2025-09-29T11:35:55Z

**Summary**: Serving long-context LLMs is costly because attention computation grows linearly with context length. Dynamic sparse attention algorithms (DSAs) mitigate this by attending only to the key-value (KV) cache of critical tokens. However, with DSAs, the main performance bottleneck shifts from HBM bandwidth to HBM capacity: KV caches for unselected tokens must remain in HBM for low-latency decoding, constraining parallel batch size and stalling further throughput gains. Offloading these underutilized KV caches to DRAM could free HBM capacity, allowing larger parallel batch sizes. Yet, achieving such hierarchical HBM-DRAM storage raises new challenges, including fragmented KV cache access, HBM cache contention, and high HBM demands of hybrid batching, that remain unresolved in prior work.   This paper proposes SparseServe, an LLM serving system that unlocks the parallel potential of DSAs through efficient hierarchical HBM-DRAM management. SparseServe introduces three key innovations to address the challenges mentioned above: (1) fragmentation-aware KV cache transfer, which accelerates HBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted saving (FlashD2H); (2) working-set-aware batch size control that adjusts batch sizes based on real-time working set estimation to minimize HBM cache thrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a single layer, enabling efficient execution even for long prompts. Extensive experimental results demonstrate that SparseServe achieves up to 9.26x lower mean time-to-first-token (TTFT) latency and up to 3.14x higher token generation throughput compared to state-of-the-art LLM serving systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.24626v1),  [pdf](http://arxiv.org/pdf/2509.24626v1)

**Tags**: cs.DC 



### Q-REACH: Quantum information Repetition, Error Analysis and Correction   using Caching Network
**Authors**: Karl C. Linne, Yuanyuan Li, Debashri Roy, Kaushik Chowdhury

**Updated**: 2025-09-29T07:54:44Z

**Summary**: Quantum repeaters incorporating quantum memory play a pivotal role in mitigating loss in transmitted quantum information (photons) due to link attenuation over a long-distance quantum communication network. However, limited availability of available storage in such quantum repeaters and the impact on the time spent within the memory unit presents a trade-off between quantum information fidelity (a metric that quantifies the degree of similarity between a pair of quantum states) and qubit transmission rate. Thus, effective management of storage time for qubits becomes a key consideration in multi-hop quantum networks. To address these challenges, we propose Q-REACH, which leverages queuing theory in caching networks to tune qubit transmission rate while considering fidelity as the cost metric. Our contributions in this work include (i) utilizing a method of repetition that encodes and broadcasts multiple qubits through different quantum paths, (ii) analytically estimating the time spent by these emitted qubits as a function of the number of paths and repeaters, as well as memory units within a repeater, and (iii) formulating optimization problem that leverages this analysis to correct the transmitted logic qubit and select the optimum repetition rate at the transmitter.

**Link**: [arxiv](http://arxiv.org/abs/2509.24407v1),  [pdf](http://arxiv.org/pdf/2509.24407v1)

**Tags**: quant-ph 



### SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV   Caching
**Authors**: Yuxuan Zhu, Ali Falahati, David H. Yang, Mohammad Mohammadi Amiri

**Updated**: 2025-09-29T05:12:51Z

**Summary**: Large language models face significant computational and memory challenges when processing long contexts. During inference, efficient management of the key-value (KV) cache, which stores intermediate activations for autoregressive generation, is critical to reducing memory overhead and improving computational efficiency. Traditional token-level efficient KV caching methods overlook semantic information, treating tokens independently without considering their semantic relationships. Meanwhile, existing semantic-preserving KV cache management approaches often suffer from substantial memory usage and high time-to-first-token. To address these limitations, we propose SentenceKV, a novel sentence-level semantic KV caching approach designed to enhance inference efficiency while preserving semantic coherence. During prefilling, SentenceKV groups tokens based on sentence-level semantic similarity, compressing sentence representations into concise semantic vectors stored directly on the GPU, while individual KV pairs are offloaded to CPU. During decoding, SentenceKV generates tokens by selectively retrieving semantically relevant sentence-level KV entries, leveraging the semantic similarity between the prefilling-stage semantic vectors and decoding-stage queries. This ensures efficient and contextually accurate predictions, minimizing the loading of redundant or irrelevant data into GPU memory and significantly reducing memory overhead while maintaining stable inference latency, even for extremely long contexts. Extensive evaluations on benchmarks including PG-19, LongBench, and Needle-In-A-Haystack demonstrate that SentenceKV significantly outperforms state-of-the-art methods in both efficiency and memory usage, without compromising model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.00970v2),  [pdf](http://arxiv.org/pdf/2504.00970v2)

**Tags**: cs.CL cs.AI cs.LG 



### Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language   Models
**Authors**: Keda Tao, Haoxuan You, Yang Sui, Can Qin, Huan Wang

**Updated**: 2025-09-29T02:46:45Z

**Summary**: Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, the key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2503.16257v2),  [pdf](http://arxiv.org/pdf/2503.16257v2)

**Tags**: cs.CV 



### BladderFormer: A Streaming Transformer for Real-Time Urological State   Monitoring
**Authors**: Chengwei Zhou, Steve Majerus, Gourav Datta

**Updated**: 2025-09-29T01:52:10Z

**Summary**: Bladder pressure monitoring systems are increasingly vital in diagnosing and managing urinary tract dysfunction. Existing solutions rely heavily on hand-crafted features and shallow classifiers, limiting their adaptability to complex signal dynamics. We propose a one-layer streaming transformer model for real-time classification of bladder pressure states, operating on wavelet-transformed representations of raw time-series data. Our model incorporates temporal multi-head self-attention and state caching, enabling efficient online inference with high adaptability. Trained on a dataset of 91 patients with 20,000-80,000 samples each, our method demonstrates improved accuracy, higher energy- and latency-efficiency. Implementation considerations for edge deployment on low-power hardware, such as edge graphical processing units (GPU) and micro-controllers, are also discussed.

**Link**: [arxiv](http://arxiv.org/abs/2509.24178v1),  [pdf](http://arxiv.org/pdf/2509.24178v1)

**Tags**: eess.SP 



### CORRECT: COndensed eRror RECognition via knowledge Transfer in   multi-agent systems
**Authors**: Yifan Yu, Moyan Li, Shaoyuan Xu, Jinmiao Fu, Xinhai Hou, Fan Lai, Bryan Wang

**Updated**: 2025-09-28T21:47:20Z

**Summary**: Multi-agent systems (MAS) are increasingly capable of tackling complex real-world tasks, yet their reliance on inter-agent coordination, tool use, and long-horizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, a large-scale dataset of over 2,000 annotated trajectories collected through a novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition.

**Link**: [arxiv](http://arxiv.org/abs/2509.24088v1),  [pdf](http://arxiv.org/pdf/2509.24088v1)

**Tags**: cs.MA 



### Sequential Diffusion Language Models
**Authors**: Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, Yanting Zhang, Yuqiang Li, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang

**Updated**: 2025-09-28T17:59:15Z

**Summary**: Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM

**Link**: [arxiv](http://arxiv.org/abs/2509.24007v1),  [pdf](http://arxiv.org/pdf/2509.24007v1)

**Tags**: cs.CL cs.LG 



### HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in   Vision-Language Models
**Authors**: Zhinan Xie, Peisong Wang, Jian Cheng

**Updated**: 2025-09-28T15:05:21Z

**Summary**: Speculative decoding is an effective approach for accelerating inference in Large Language models (LLMs), but its adaptation to Vision-Language models (VLMs) remains challenging for additional visual tokens in multimodal inputs. First, owing to the fact that the drafter and the target VLM may derived from different families, the semantic representations of visual tokens in the target VLM are misaligned with those in the drafter, introducing bias into the KV-cache during the prefill stage. Second, the large number of visual tokens substantially slows down the drafter's self-attention during the decoding stage. We propose Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models (HiViS), an explicit-implicit input decomposition framework that alleviates the above inefficiency. All visual tokens are removed from the drafter's input, retaining only textual tokens as explicit inputs, while directly reusing the target VLM's corresponding last-layer hidden states as implicit visual information without additional processing. To train the drafter efficiently, we introduces multi-step self-feedback training strategy with dynamic data selection and sequential embedding supervision to simulate reasoning during training. Our approach compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the target VLM's input, while maintaining lossless generation quality. Extensive experiments across diverse models and tasks demonstrate up to 2.65x speedup, confirming the effectiveness of HiViS in accelerating VLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2509.23928v1),  [pdf](http://arxiv.org/pdf/2509.23928v1)

**Tags**: cs.LG cs.AI 



### SALM: A Multi-Agent Framework for Language Model-Driven Social Network   Simulation
**Authors**: Gaurav Koley

**Updated**: 2025-09-28T08:32:26Z

**Summary**: Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.

**Link**: [arxiv](http://arxiv.org/abs/2505.09081v2),  [pdf](http://arxiv.org/pdf/2505.09081v2)

**Tags**: cs.SI cs.AI cs.MA 



### VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration
**Authors**: Han Hu, Zhuoran Zheng, Liang Li, Chen Lyu

**Updated**: 2025-09-28T03:12:43Z

**Summary**: Recent Mamba-based image restoration methods have achieved promising results but remain   limited by fixed scanning patterns and inefficient feature utilization. Conventional Mamba   architectures rely on predetermined paths that cannot adapt to diverse degradations, constraining   both restoration performance and computational efficiency. To overcome these limitations, we   propose VAMamba, a Visual Adaptive Mamba framework with two key innovations. First,   QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha   FIFO cache that stores historical representations. Similarity between current LoRA-adapted and   cached features guides intelligent fusion, enabling dynamic reuse while effectively controlling   memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning. A   Vision Transformer generates score maps to estimate pixel importance, and a greedy strategy de termines optimal forward and backward scanning paths. These learned trajectories replace rigid   patterns, enabling SS2D to perform targeted feature extraction. The integration of QCLAM and   GPS-SS2D allows VAMamba to adaptively focus on degraded regions while maintaining high   computational efficiency. Extensive experiments across diverse restoration tasks demonstrate   that VAMamba consistently outperforms existing approaches in both restoration quality and   efficiency, establishing new benchmarks for adaptive image restoration. Our code is available   at https://github.com/WaterHQH/VAMamba.

**Link**: [arxiv](http://arxiv.org/abs/2509.23601v1),  [pdf](http://arxiv.org/pdf/2509.23601v1)

**Tags**: cs.CV 



### READER: Retrieval-Assisted Drafter for Efficient LLM Inference
**Authors**: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Stanislav Ilyushin, Sultan Isali, Vasily Kalugin, Nuriza Aitassova, Fei Yi, Weidi Zeng

**Updated**: 2025-09-27T20:13:25Z

**Summary**: Autoregressive Language Models instantiate a factorized likelihood over token sequences, yet their strictly sequential decoding process imposes an intrinsic lower bound on inference latency. This bottleneck has emerged as a central obstacle to the scalable deployment of large-scale generative models. Existing acceleration techniques partially mitigate token-level latency by relying on auxiliary draft models or introducing an additional training phase, but fail to address the dominant memory and communication costs. We present READER, a provably lossless speculative decoding framework that bypasses the training of the auxiliary draft model. READER formalizes speculative decoding as a stochastic tree construction problem and exploits the empirical redundancy structure of natural language to generate high-probability candidate continuations. Our method revisits the problem of constructing draft trees, establishing substantial statistical improvements over stochastic draft-tree methods and providing a complexity-theoretic analysis that characterizes the optimality frontier of speculative decoding under bounded computation and memory resources. Beyond the single-sequence regime traditionally considered in prior work, we introduce a memory-optimal key-value cache-serving strategy that guarantees amortized sublinear overhead in the batch dimension, allowing READER to scale to realistic inference workloads. Comprehensive experiments demonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to 5.92x on batched inference, consistently surpassing prior speculative decoding baselines, while preserving exact output equivalence, with even more pronounced gains in retrieval-augmented generation pipelines. Our results close a key gap between theoretical parallelism limits and practical LLM inference, suggesting a new standard for efficient deployment.

**Link**: [arxiv](http://arxiv.org/abs/2508.09072v2),  [pdf](http://arxiv.org/pdf/2508.09072v2)

**Tags**: cs.CL 



### A Near-Cache Architectural Framework for Cryptographic Computing
**Authors**: Jingyao Zhang, Elaheh Sadredini

**Updated**: 2025-09-27T08:15:17Z

**Summary**: Recent advancements in post-quantum cryptographic algorithms have led to their standardization by the National Institute of Standards and Technology (NIST) to safeguard information security in the post-quantum era. These algorithms, however, employ public keys and signatures that are 3 to 9$\times$ longer than those used in pre-quantum cryptography, resulting in significant performance and energy efficiency overheads. A critical bottleneck identified in our analysis is the cache bandwidth. This limitation motivates the adoption of on-chip in-/near-cache computing, a computing paradigm that offers high-performance, exceptional energy efficiency, and flexibility to accelerate post-quantum cryptographic algorithms. Our analysis of existing works reveals challenges in integrating in-/near-cache computing into modern computer systems and performance limitations due to external bandwidth limitation, highlighting the need for innovative solutions that can seamlessly integrate into existing systems without performance and energy efficiency issues. In this paper, we introduce a near-cache-slice computing paradigm with support of customization and virtual address, named Crypto-Near-Cache (CNC), designed to accelerate post-quantum cryptographic algorithms and other applications. By placing SRAM arrays with bitline computing capability near cache slices, high internal bandwidth and short data movement are achieved with native support of virtual addressing. An ISA extension to facilitate CNC is also proposed, with detailed discussion on the implementation aspects of the core/cache datapath.

**Link**: [arxiv](http://arxiv.org/abs/2509.23179v1),  [pdf](http://arxiv.org/pdf/2509.23179v1)

**Tags**: cs.AR cs.CR 



### Runtime Adaptive Pruning for LLM Inference
**Authors**: Huanrong Liu, Chunlin Tian, Xuyang Wei, Qingbiao Li, Li Li

**Updated**: 2025-09-27T07:41:38Z

**Summary**: Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. Compression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests. To address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. Specifically, RAP dynamically tracks the evolving ratio between model parameters and KV-cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter -light attention layers dominate KV-cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state. Extensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.

**Link**: [arxiv](http://arxiv.org/abs/2505.17138v4),  [pdf](http://arxiv.org/pdf/2505.17138v4)

**Tags**: cs.LG cs.AI 



### d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching
**Authors**: Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, Xu Yang

**Updated**: 2025-09-27T04:07:23Z

**Summary**: Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.

**Link**: [arxiv](http://arxiv.org/abs/2509.23094v1),  [pdf](http://arxiv.org/pdf/2509.23094v1)

**Tags**: cs.CL 



### ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline   Calibration
**Authors**: Xianglong Yan, Zhiteng Li, Tianao Zhang, Haotong Qin, Linghe Kong, Yulun Zhang, Xiaokang Yang

**Updated**: 2025-09-27T03:37:40Z

**Summary**: Large language models (LLMs) have demonstrated remarkable performance, but their long-context reasoning remains constrained by the excessive memory required for the Key-Value (KV) cache. This makes KV cache compression a critical step toward efficient long-context inference. Recent methods have explored low-rank techniques to reduce the hidden size of the KV cache. However, they neglect the distinct roles and varying importance of Keys and Values, leading to significant performance drops under high compression. To address this, we propose ReCalKV, a post-training low-rank KV cache compression approach with tailored strategies for Keys and Values. For Keys, we propose Head-wise Similarity aware Reordering (HSR), which clusters structurally similar heads into groups, enabling more accurate low-rank approximation via grouped SVD. For Values, we propose Offline Value Calibration (OVC), which efficiently calibrates the value projection matrix using calibration data without training, ensuring an accurate representation of contextual information. Extensive experiments show that ReCalKV consistently outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. The code and models will be available at:https://github.com/XIANGLONGYAN/ReCalKV.

**Link**: [arxiv](http://arxiv.org/abs/2505.24357v3),  [pdf](http://arxiv.org/pdf/2505.24357v3)

**Tags**: cs.LG cs.AI 



### vCache: Verified Semantic Prompt Caching
**Authors**: Luis Gaspar Schroeder, Aditya Desai, Alejandro Cuadron, Kyle Chu, Shu Liu, Mark Zhao, Stephan Krusche, Alfons Kemper, Ion Stoica, Matei Zaharia, Joseph E. Gonzalez

**Updated**: 2025-09-26T21:40:58Z

**Summary**: Semantic caches return cached responses for semantically similar prompts to reduce LLM inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, can result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines. We release the vCache implementation and three benchmarks to support future research.

**Link**: [arxiv](http://arxiv.org/abs/2502.03771v4),  [pdf](http://arxiv.org/pdf/2502.03771v4)

**Tags**: cs.LG cs.CL 



### On KV-Poisson Structure and related invariants
**Authors**: Prosper Rosaire Mama Assandje, Herguey Mopeng, Joseph Dongho

**Updated**: 2025-09-26T19:40:33Z

**Summary**: We propose an deepened analysis of KV-Poisson structures of on IR^2. We present their classification their properties an their possible applications in different domains. We prove that these structure give rise to a new Cohomological invariant. We explicitly compute the Cohomological groups of some of these structures.

**Link**: [arxiv](http://arxiv.org/abs/2509.22875v1),  [pdf](http://arxiv.org/pdf/2509.22875v1)

**Tags**: math.DG 



### KV Cache Steering for Controlling Frozen LLMs
**Authors**: Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, James R. Glass, Cees G. M. Snoek, Yuki M. Asano

**Updated**: 2025-09-26T17:59:54Z

**Summary**: We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach constructs steering vectors from reasoning traces, obtained either from teacher models (e.g., GPT-4o) or existing human annotations, that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Additional experiments show that the method also scales to larger models and yields further gains on challenging datasets such as GPQA and MATH. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of inference latency, hyperparameter stability, and ease of integration with existing inference APIs. Beyond mere reasoning induction, we show that cache steering enables controllable transfer of reasoning styles (e.g., stepwise, causal, analogical), making it a practical tool for behavior-level guidance of language models.

**Link**: [arxiv](http://arxiv.org/abs/2507.08799v2),  [pdf](http://arxiv.org/pdf/2507.08799v2)

**Tags**: cs.CL cs.AI 



### LongLive: Real-time Interactive Long Video Generation
**Authors**: Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen

**Updated**: 2025-09-26T17:48:24Z

**Summary**: We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.

**Link**: [arxiv](http://arxiv.org/abs/2509.22622v1),  [pdf](http://arxiv.org/pdf/2509.22622v1)

**Tags**: cs.CV 



### JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory   for Vision-Language Navigation
**Authors**: Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei

**Updated**: 2025-09-26T16:29:37Z

**Summary**: Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2509.22548v1),  [pdf](http://arxiv.org/pdf/2509.22548v1)

**Tags**: cs.CV cs.RO 



### TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent   and Explainable Digital Assessments
**Authors**: Rakesh Thakur, Shivaansh Kaushik, Gauri Chopra, Harsh Rohilla

**Updated**: 2025-09-26T16:00:36Z

**Summary**: This paper introduces TrueGradeAI, an AI-driven digital examination framework designed to overcome the shortcomings of traditional paper-based assessments, including excessive paper usage, logistical complexity, grading delays, and evaluator bias. The system preserves natural handwriting by capturing stylus input on secure tablets and applying transformer-based optical character recognition for transcription. Evaluation is conducted through a retrieval-augmented pipeline that integrates faculty solutions, cache layers, and external references, enabling a large language model to assign scores with explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems that primarily digitize responses, TrueGradeAI advances the field by incorporating explainable automation, bias mitigation, and auditable grading trails. By uniting handwriting preservation with scalable and transparent evaluation, the framework reduces environmental costs, accelerates feedback cycles, and progressively builds a reusable knowledge base, while actively working to mitigate grading bias and ensure fairness in assessment.

**Link**: [arxiv](http://arxiv.org/abs/2509.22516v1),  [pdf](http://arxiv.org/pdf/2509.22516v1)

**Tags**: cs.AI cs.LG 



### AxLLM: accelerator architecture for large language models with   computation reuse capability
**Authors**: Soroush Ahadi, Mehdi Modarressi, Masoud Daneshtalab

**Updated**: 2025-09-26T15:54:50Z

**Summary**: Large language models demand massive computational power and memory resources, posing significant challenges for efficient deployment. While quantization has been widely explored to reduce model size and computation, this paper demonstrates an additional benefit: quantization increases parameter locality, creating opportunities for computation reuse. Building on this insight, we propose AxLLM, a hardware accelerator architecture designed for quantized models. Axllm introduces a novel redundancy elimination technique that caches and reuses multiplication results for repeated weight values, substantially reducing redundant operations. The architecture features dual multiply and reuse pipelines, efficiently supporting both base models and LoRA fine-tuned models without altering parameters, retraining, or requiring offline preprocessing. Experimental results show that AxLLM achieves up to 90% reduction in computations, delivering 28% lower energy consumption and a 1.7x speedup over baseline execution. These results highlight Axllm as a scalable and efficient solution for accelerating LLMs on specialized hardware.

**Link**: [arxiv](http://arxiv.org/abs/2509.22512v1),  [pdf](http://arxiv.org/pdf/2509.22512v1)

**Tags**: cs.AR n/a 



### Organ dose optimization for a point-of-care forearm X-ray   photon-counting CT
**Authors**: Pierre-Antoine Rodesch, AnaÃ¯s Viry, Mouad Khorsi, Fabio Becce, JÃ©rÃ´me Damet, LucÃ­a Gallego Manzano

**Updated**: 2025-09-26T15:35:05Z

**Summary**: Background: Spectral shaping is a computed tomography (CT) dose optimization technique that adjusts source voltage and filtration to reduce patient radiation exposure without compromising image quality. Traditionally, radiation dose has been assessed using the computed tomography dose index (CTDI). However, emerging dosimetric approaches aim to enable patient-specific evaluations by estimating organ absorbed doses, providing a more accurate representation of the biological impact. This study investigates spectral shaping for an extremity photon-counting detector (PCD) CT, through organ absorbed dose estimation and image quality evaluation. Method: Monte Carlo simulations were conducted to evaluate various combinations of source voltage and filtration. Tube voltage ranged from 80 to 140 kV, combined with three distinct filtration material and thicknesses. Simulations included three stages: a standardized phantom for CTDI assessment, an adult forearm phantom for organ dose measurement, and an image quality phantom for evaluation of an advanced image quality metric: the detectability index. Results: In a wrist PCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation dose by up to 50%. This reduction is achieved while maintaining the same detectability index value as the standard 120 kV protocol. However, the optimal filtration depends on the organ targeted for dose reduction, as bone and skin benefit from opposing filtration approaches. While CTDI provides a useful initial estimate, it may lead to suboptimal optimization compared to organ-specific dose evaluation. Conclusions: Patient-specific dosimetry based on organ absorbed dose estimation offers a more accurate framework for optimizing CT protocols through spectral shaping than conventional CTDI-based approaches.

**Link**: [arxiv](http://arxiv.org/abs/2509.22488v1),  [pdf](http://arxiv.org/pdf/2509.22488v1)

**Tags**: physics.med-ph 



### Bottlenecked Transformers: Periodic KV Cache Consolidation for   Generalised Reasoning
**Authors**: Adnan Oomerjee, Zafeirios Fountas, Haitham Bou-Ammar, Jun Wang

**Updated**: 2025-09-26T14:35:04Z

**Summary**: Transformer LLMs have been shown to exhibit strong reasoning ability that scales with inference-time compute, most prominently through token-space "thinking" chains of thought. A growing line of work pushes extra computation into the model's latent space, which we term Auxiliary Latent-Space Computation (ALSC). Existing ALSC methods largely fall into three buckets: (i) token-mediated latent rollouts, (ii) residual/activation steering, and (iii) memory (KV) compression. An underexplored alternative is memory consolidation/reconsolidation, two processes in the brain that are responsible for stabilising newly formed memory traces, and, upon recall, transiently rendering established traces plastic such they can integrate new contextual information before restabilising. In Transformer LLMs, this can be seen as analogous to performing in-place rewrites of new KV segments, and rewrites of recalled past segments. In this work, we give a theoretical justification as to why memory (re)consolidation via KV cache rewrites is beneficial for improved reasoning. We do this through the lens of Information Bottleneck (IB) theory, which posits that model generalisation emerges from an optimal balance between input information compression and retention of predictive information in latent representations. We then introduce the Bottlenecked Transformer, which augments a backbone LLM with a Cache Processor, an auxiliary Transformer that performs periodic, non-causal, in-place KV rewrites at newline-delimited reasoning step boundaries. The Processor consolidates recently written KV entries and reconsolidates a small, top-k attention-selected set of prior entries. We evaluate our Bottlenecked Transformer architecture on math reasoning benchmarks. Our model sees consistent performance gains over vanilla Transformers and pause-token augmented baselines, with gains of up to +6.6pp for selected tasks/backbones.

**Link**: [arxiv](http://arxiv.org/abs/2505.16950v3),  [pdf](http://arxiv.org/pdf/2505.16950v3)

**Tags**: cs.LG cs.AI cs.IT math.IT 



### RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion   Transformer
**Authors**: Wangbo Zhao, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Pengfei Zhou, Kai Wang, Bohan Zhuang, Zhangyang Wang, Fan Wang, Yang You

**Updated**: 2025-09-26T13:20:52Z

**Summary**: Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model's distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2509.22323v1),  [pdf](http://arxiv.org/pdf/2509.22323v1)

**Tags**: cs.CV 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-09-26T10:00:54Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty, and only those elements are processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a cache-friendlier priority queue algorithm that avoids accessing auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, and animation. Moreover, thanks to numerous low-level optimizations, Spineless Traversal is competitive across the whole spectrum of incremental layout workloads. Spineless Traversal is faster than the standard approach on 83.0% of 2216 benchmarks, with a mean speedup of 1.80x concentrated in the most latency-critical interactions.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v8),  [pdf](http://arxiv.org/pdf/2411.10659v8)

**Tags**: cs.PL 



### Persistent Autoregressive Mapping with Traffic Rules for Autonomous   Driving
**Authors**: Shiyi Liang, Xinyuan Chang, Changjie Wu, Huiyuan Yan, Yifan Bai, Xinran Liu, Hang Zhang, Yujian Yuan, Shuang Zeng, Mu Xu, Xing Wei

**Updated**: 2025-09-26T09:33:36Z

**Summary**: Safe autonomous driving requires both accurate HD map construction and persistent awareness of traffic rules, even when their associated signs are no longer visible. However, existing methods either focus solely on geometric elements or treat rules as temporary classifications, failing to capture their persistent effectiveness across extended driving sequences. In this paper, we present PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel framework that performs autoregressive co-construction of lane vectors and traffic rules from visual observations. Our approach introduces two key mechanisms: Map-Rule Co-Construction for processing driving scenes in temporal segments, and Map-Rule Cache for maintaining rule consistency across these segments. To properly evaluate continuous and consistent map generation, we develop MapDRv2, featuring improved lane geometry annotations. Extensive experiments demonstrate that PAMR achieves superior performance in joint vector-rule mapping tasks, while maintaining persistent rule effectiveness throughout extended driving sequences.

**Link**: [arxiv](http://arxiv.org/abs/2509.22756v1),  [pdf](http://arxiv.org/pdf/2509.22756v1)

**Tags**: cs.RO cs.AI 



### LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for   Multi-Turn Dialogues
**Authors**: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan

**Updated**: 2025-09-26T07:14:44Z

**Summary**: Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. As a result, these models cannot accurately identify and prioritize the most relevant context, leading to degraded response quality. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a new benchmark with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.13681v2),  [pdf](http://arxiv.org/pdf/2507.13681v2)

**Tags**: cs.CL cs.AI 



### Taming Flow-based I2V Models for Creative Video Editing
**Authors**: Xianghao Kong, Hansheng Chen, Yuwei Guo, Lvmin Zhang, Gordon Wetzstein, Maneesh Agrawala, Anyi Rao

**Updated**: 2025-09-26T05:57:04Z

**Summary**: Although image editing techniques have advanced significantly, video editing, which aims to manipulate videos according to user intent, remains an emerging challenge. Most existing image-conditioned video editing methods either require inversion with model-specific design or need extensive optimization, limiting their capability of leveraging up-to-date image-to-video (I2V) models to transfer the editing capability of image editing models to the video domain. To this end, we propose IF-V2V, an Inversion-Free method that can adapt off-the-shelf flow-matching-based I2V models for video editing without significant computational overhead. To circumvent inversion, we devise Vector Field Rectification with Sample Deviation to incorporate information from the source video into the denoising process by introducing a deviation term into the denoising vector field. To further ensure consistency with the source video in a model-agnostic way, we introduce Structure-and-Motion-Preserving Initialization to generate motion-aware temporally correlated noise with structural information embedded. We also present a Deviation Caching mechanism to minimize the additional computational cost for denoising vector rectification without significantly impacting editing quality. Evaluations demonstrate that our method achieves superior editing quality and consistency over existing approaches, offering a lightweight plug-and-play solution to realize visual creativity.

**Link**: [arxiv](http://arxiv.org/abs/2509.21917v1),  [pdf](http://arxiv.org/pdf/2509.21917v1)

**Tags**: cs.CV cs.MM 



### 2.34 kV \b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with   sub-micron fin width
**Authors**: Chinmoy Nath Saha, Saurav Roy, Yizheng Liu, Carl Peterson, Sriram Krishnamoorthy

**Updated**: 2025-09-26T04:32:56Z

**Summary**: In this letter, we present a kilovolt-class \b{eta}-Ga2O3 vertical trench Schottky barrier diode with a field plate incorporating narrow fin width (Wfin) structures of sub-micron dimensions. We used a nanolaminate dielectric comprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric and for field plate edge termination. Both Wfin of 200 nm and 500 nm demonstrate excellent on-state performance with specific on-resistance (Ron,sp) of 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist planarization and etch-back process was employed to expose the top of the fins for Schottky contact formation, eliminating critical lithographic alignment challenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV with very low leakage currents before catastrophic breakdown. The measured breakdown voltage is limited by dielectric breakdown at the trench bottom corner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD simulation shows a reduced electric field at the surface of the metal-semiconductor junction due to the RESURF effect, resulting in very low reverse leakage before breakdown. The parallel plane electric field in the \b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using accurately extracted drift layer doping profile from high voltage CV measurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current spreading) was calculated. Enhanced RESURF by integration of high-k dielectrics with self-aligned photoresist planarization, offers a promising pathway towards high figure of merit, low leakage high-performance vertical devices.

**Link**: [arxiv](http://arxiv.org/abs/2509.21857v1),  [pdf](http://arxiv.org/pdf/2509.21857v1)

**Tags**: physics.app-ph 



### DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for   Autonomous Travel Planning Agents
**Authors**: Yansong Ning, Rui Liu, Jun Wang, Kai Chen, Wei Li, Jun Fang, Kan Zheng, Naiqiang Tan, Hao Liu

**Updated**: 2025-09-26T04:03:52Z

**Summary**: Travel planning (TP) agent has recently worked as an emerging building block to interact with external tools and resources for travel itinerary generation, ensuring enjoyable user experience. Despite its benefits, existing studies rely on hand craft prompt and fixed agent workflow, hindering more flexible and autonomous TP agent. This paper proposes DeepTravel, an end to end agentic reinforcement learning framework for building autonomous travel planning agent, capable of autonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi step reasoning. To achieve this, we first construct a robust sandbox environment by caching transportation, accommodation and POI data, facilitating TP agent training without being constrained by real world APIs limitations (e.g., inconsistent outputs). Moreover, we develop a hierarchical reward modeling system, where a trajectory level verifier first checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and then the turn level verifier further validate itinerary detail consistency with tool responses, enabling efficient and precise reward service. Finally, we propose the reply augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity. We deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.21842v1),  [pdf](http://arxiv.org/pdf/2509.21842v1)

**Tags**: cs.AI 



### LiteGS: A High-performance Framework to Train 3DGS in Subminutes via   System and Algorithm Codesign
**Authors**: Kaimin Liao, Hua Wang, Zhi Chen, Luchao Wang, Yaohua Tang

**Updated**: 2025-09-26T03:24:20Z

**Summary**: 3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D representation. However, it still suffers from high training cost. This paper introduces LiteGS, a high performance framework that systematically optimizes the 3DGS training pipeline from multiple aspects. At the low-level computation layer, we design a ``warp-based raster'' associated with two hardware-aware optimizations to significantly reduce gradient reduction overhead. At the mid-level data management layer, we introduce dynamic spatial sorting based on Morton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and improve data locality, therefore reducing cache misses. At the top-level algorithm layer, we establish a new robust densification criterion based on the variance of the opacity gradient, paired with a more stable opacity control mechanism, to achieve more precise parameter growth. Experimental results demonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x with comparable or superior quality and surpasses the current SOTA in lightweight models by up to 1.4x speedup. For high-quality reconstruction tasks, LiteGS sets a new accuracy record and decreases the training time by an order of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2503.01199v3),  [pdf](http://arxiv.org/pdf/2503.01199v3)

**Tags**: cs.CV 



### DOTA: Distributional Test-Time Adaptation of Vision-Language Models
**Authors**: Zongbo Han, Jialong Yang, Guangyu Wang, Junfan Li, Qianli Xu, Mike Zheng Shou, Changqing Zhang

**Updated**: 2025-09-26T03:17:15Z

**Summary**: Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable performance across a wide range of tasks. However, deploying these models can be unreliable when significant distribution gaps exist between training and test data, while fine-tuning for diverse scenarios is often costly. Cache-based test-time adapters offer an efficient alternative by storing representative test samples to guide subsequent classifications. Yet, these methods typically employ naive cache management with limited capacity, leading to severe catastrophic forgetting when samples are inevitably dropped during updates. In this paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet effective method addressing this limitation. Crucially, instead of merely memorizing individual test samples, DOTA continuously estimates the underlying distribution of the test data stream. Test-time posterior probabilities are then computed using these dynamically estimated distributions via Bayes' theorem for adaptation. This distribution-centric approach enables the model to continually learn and adapt to the deployment environment. Extensive experiments validate that DOTA significantly mitigates forgetting and achieves state-of-the-art performance compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.19375v3),  [pdf](http://arxiv.org/pdf/2409.19375v3)

**Tags**: cs.LG cs.AI cs.CL cs.CV cs.HC 



### OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's   Rule
**Authors**: Yuxuan Zhu, David H. Yang, Mohammad Mohammadi Amiri, Keerthiram Murugesan, Tejaswini Pedapati, Pin-Yu Chen

**Updated**: 2025-09-25T21:42:27Z

**Summary**: The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation. This bottleneck is substantial; for instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of 4 requires approximately 16GB for its KV cache, a size exceeding the model's weights. While KV-cache compression via low-rank projection is a promising direction, existing methods rely on a static, offline-learned subspace that performs poorly under data distribution shifts. To overcome these limitations, we introduce OjaKV, a novel framework that integrates a strategic hybrid storage policy with online subspace adaptation. First, OjaKV recognizes that not all tokens are equally important for compression; it preserves the crucial first and most recent tokens in full-rank, maintaining high-fidelity anchors for attention. Second, for the vast majority of intermediate tokens, it applies low-rank compression by incrementally adapting the projection basis using Oja's algorithm for online principal component analysis. This adaptation involves a comprehensive update during prompt prefilling and lightweight periodic updates during decoding, ensuring the subspace remains aligned with the evolving context. Crucially, our framework is fully compatible with modern attention modules like FlashAttention. Experiments demonstrate that OjaKV maintains or even improves zero-shot accuracy at high compression ratios. In particular, OjaKV achieves its strongest gains on very long-context benchmarks that require complex reasoning, highlighting the importance of online subspace adaptation in dynamically tracking context shifts. These results establish our hybrid framework as a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2509.21623v1),  [pdf](http://arxiv.org/pdf/2509.21623v1)

**Tags**: cs.CL cs.AI cs.LG 



### Enhanced Generative Machine Listener
**Authors**: Vishnu Raj, Gouthaman KV, Shiv Gehlot, Lars Villemoes, Arijit Biswas

**Updated**: 2025-09-25T19:29:25Z

**Summary**: We present GMLv2, a reference-based model designed for the prediction of subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta distribution-based loss to model the listener ratings and incorporates additional neural audio coding (NAC) subjective datasets to extend its generalization and applicability. Extensive evaluations on diverse testset demonstrate that proposed GMLv2 consistently outperforms widely used metrics, such as PEAQ and ViSQOL, both in terms of correlation with subjective scores and in reliably predicting these scores across diverse content types and codec configurations. Consequently, GMLv2 offers a scalable and automated framework for perceptual audio quality evaluation, poised to accelerate research and development in modern audio coding technologies.

**Link**: [arxiv](http://arxiv.org/abs/2509.21463v1),  [pdf](http://arxiv.org/pdf/2509.21463v1)

**Tags**: eess.AS cs.AI cs.LG 



### Autoregressive Image Generation with Randomized Parallel Decoding
**Authors**: Haopeng Li, Jinyue Yang, Guoqi Li, Huan Wang

**Updated**: 2025-09-25T13:55:44Z

**Summary**: We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale.

**Link**: [arxiv](http://arxiv.org/abs/2503.10568v2),  [pdf](http://arxiv.org/pdf/2503.10568v2)

**Tags**: cs.CV 



### WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP
**Authors**: Moshe Kimhi, Erez Koifman, Ehud Rivlin, Eli Schwartz, Chaim Baskin

**Updated**: 2025-09-25T13:39:16Z

**Summary**: We introduce WAVECLIP, a single unified model for adaptive resolution inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces standard patch embeddings with a multi-level wavelet decomposition, enabling the model to process images coarse to fine while naturally supporting multiple resolutions within the same model. At inference time, the model begins with low resolution tokens and refines only when needed, using key-value caching and causal cross-level attention to reuse computation, effectively introducing to the model only new information when needed. We evaluate WAVECLIP in zero-shot classification, demonstrating that a simple confidence-based gating mechanism enables adaptive early exits. This allows users to dynamically choose a compute-accuracy trade-off using a single deployed model. Our approach requires only lightweight distillation from a frozen CLIP teacher and achieves competitive accuracy with significant computational savings.

**Link**: [arxiv](http://arxiv.org/abs/2509.21153v1),  [pdf](http://arxiv.org/pdf/2509.21153v1)

**Tags**: cs.CV cs.AI cs.MM 



### InComeS: Integrating Compression and Selection Mechanisms into LLMs for   Efficient Model Editing
**Authors**: Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Wai Lam

**Updated**: 2025-09-25T13:15:45Z

**Summary**: Although existing model editing methods perform well in recalling exact edit facts, they often struggle in complex scenarios that require deeper semantic understanding rather than mere knowledge regurgitation. Leveraging the strong contextual reasoning abilities of large language models (LLMs), in-context learning (ICL) becomes a promising editing method by comprehending edit information through context encoding. However, this method is constrained by the limited context window of LLMs, leading to degraded performance and efficiency as the number of edits increases. To overcome this limitation, we propose InComeS, a flexible framework that enhances LLMs' ability to process editing contexts through explicit compression and selection mechanisms. Specifically, InComeS compresses each editing context into the key-value (KV) cache of a special gist token, enabling efficient handling of multiple edits without being restricted by the model's context window. Furthermore, specialized cross-attention modules are added to dynamically select the most relevant information from the gist pools, enabling adaptive and effective utilization of edit information. We conduct experiments on diverse model editing benchmarks with various editing formats, and the results demonstrate the effectiveness and efficiency of our method.

**Link**: [arxiv](http://arxiv.org/abs/2505.22156v2),  [pdf](http://arxiv.org/pdf/2505.22156v2)

**Tags**: cs.CL 



### EpiCache: Episodic KV Cache Management for Long Conversational Question   Answering
**Authors**: Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho

**Updated**: 2025-09-25T10:24:14Z

**Summary**: Modern large language models (LLMs) extend context lengths to up to millions of tokens, enabling AI assistants to generate coherent and personalized responses grounded in long conversational histories. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly becomes the bottleneck in resource-constrained environments. An active line of research for reducing memory bottleneck is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting the KV cache after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to failure cases in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.

**Link**: [arxiv](http://arxiv.org/abs/2509.17396v2),  [pdf](http://arxiv.org/pdf/2509.17396v2)

**Tags**: cs.CL 



### Toward Robust and Efficient ML-Based GPU Caching for Modern Inference
**Authors**: Peng Chen, Jiaji Zhang, Hailiang Zhao, Yirong Zhang, Jiahong Yu, Xueyan Tang, Yixuan Wang, Hao Li, Jianping Zou, Gang Xiong, Kingsum Chow, Shuibing He, Shuiguang Deng

**Updated**: 2025-09-25T10:23:50Z

**Summary**: In modern GPU inference, cache efficiency remains a major bottleneck. In recommendation models, embedding hit rates largely determine throughput, while in large language models, KV-cache misses substantially increase time-to-first-token (TTFT). Heuristic policies such as \textsc{LRU} often struggle under structured access patterns. Learning-based approaches are promising, but in practice face two major limitations: they degrade sharply when predictions are inaccurate, or they gain little even with accurate predictions due to conservative designs. Some also incur high overhead, further limiting practicality.   We present \textsc{LCR}, a practical framework for learning-based GPU caching that delivers performance gains while ensuring robustness and efficiency. Its core algorithm, \textsc{LARU}, enhances \textsc{LRU} with machine-learned predictions and dynamically adapts to prediction accuracy through online error estimation. When predictions are accurate, \textsc{LARU} achieves near-optimal performance. With inaccurate predictions, it degrades gracefully to near-\textsc{LRU} performance. With \textsc{LCR}, we bridge the gap between empirical progress and theoretical advances in learning-based caching.   Experiments show that \textsc{LCR} delivers consistent gains under realistic conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\% and reduces P99 TTFT by up to 28.3\%, outperforming widely used inference systems. Even under poor predictions, its performance remains stable, demonstrating practical robustness.

**Link**: [arxiv](http://arxiv.org/abs/2509.20979v1),  [pdf](http://arxiv.org/pdf/2509.20979v1)

**Tags**: cs.LG 



### Robustifying Learning-Augmented Caching Efficiently without Compromising   1-Consistency
**Authors**: Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng

**Updated**: 2025-09-25T09:49:59Z

**Summary**: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce significant computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_k + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only $O(1)$ additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.16242v5),  [pdf](http://arxiv.org/pdf/2507.16242v5)

**Tags**: cs.DS cs.LG 



### ILRe: Intermediate Layer Retrieval for Context Compression in Causal   Language Models
**Authors**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li

**Updated**: 2025-09-25T03:30:06Z

**Summary**: Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to a few tenths of that required for the full context, but also delivers performance comparable to or superior to the full-context setup in long-context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

**Link**: [arxiv](http://arxiv.org/abs/2508.17892v2),  [pdf](http://arxiv.org/pdf/2508.17892v2)

**Tags**: cs.CL cs.LG 



### HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO   Serving and Fast Scaling
**Authors**: Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-09-25T03:00:22Z

**Summary**: Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to 19.39$\times$. These optimizations allow the system to achieve up to 4.44$\times$ higher SLO attainment, 65.82% lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.15919v2),  [pdf](http://arxiv.org/pdf/2508.15919v2)

**Tags**: cs.DC cs.AI 



### DELM: a Python toolkit for Data Extraction with Language Models
**Authors**: Eric Fithian, Kirill Skobelev

**Updated**: 2025-09-24T23:47:55Z

**Summary**: Large Language Models (LLMs) have become powerful tools for annotating unstructured data. However, most existing workflows rely on ad hoc scripts, making reproducibility, robustness, and systematic evaluation difficult. To address these challenges, we introduce DELM (Data Extraction with Language Models), an open-source Python toolkit designed for rapid experimental iteration of LLM-based data extraction pipelines and for quantifying the trade-offs between them. DELM minimizes boilerplate code and offers a modular framework with structured outputs, built-in validation, flexible data-loading and scoring strategies, and efficient batch processing. It also includes robust support for working with LLM APIs, featuring retry logic, result caching, detailed cost tracking, and comprehensive configuration management. We showcase DELM's capabilities through two case studies: one featuring a novel prompt optimization algorithm, and another illustrating how DELM quantifies trade-offs between cost and coverage when selecting keywords to decide which paragraphs to pass to an LLM. DELM is available at \href{https://github.com/Center-for-Applied-AI/delm}{\texttt{github.com/Center-for-Applied-AI/delm}}.

**Link**: [arxiv](http://arxiv.org/abs/2509.20617v1),  [pdf](http://arxiv.org/pdf/2509.20617v1)

**Tags**: cs.IR 



### UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from   an Uncertainty-Aware Perspective
**Authors**: Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Min Yang, Lingpeng Kong, Ngai Wong

**Updated**: 2025-09-24T16:56:17Z

**Summary**: Deploying large language models (LLMs) for long-context inference remains challenging due to their substantial memory and computational demands. While techniques such as Key-Value (KV) cache compression are designed to reduce memory usage, they often neglect the structured sparsity inherent in the relationship between hidden states and their corresponding KV cache. In this work, we explore the role of uncertainty as a potential indicator of sparsity within LLMs. We propose UNComp, an uncertainty-aware framework that leverages truncated matrix entropy to identify areas of low information content, thereby revealing sparsity patterns that can be used for adaptive compression. Unlike traditional methods that apply uniform compression, UNComp dynamically adjusts its approach to compression, guided by uncertainty measures that reflect the importance of various model components. Our analysis shows that sparsity patterns, when derived from uncertainty estimates, can be exploited to reveal special long-range dependencies, such as retrieval heads and retrieval layers. This perspective not only enhances our understanding of how compression can be optimized but also provides new insights into the inherent sparsity of LLMs during long-context inference. By focusing on uncertainty to analyze the sparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the original, achieves a 6% prefill speedup, and improves throughput by 6.4x - not only delivering strong lossless compression performance, but also validating the effectiveness of the underlying theoretical tool. We release the code at https://github.com/menik1126/UNComp.

**Link**: [arxiv](http://arxiv.org/abs/2410.03090v2),  [pdf](http://arxiv.org/pdf/2410.03090v2)

**Tags**: cs.CL cs.LG 



### Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient   LLM Inference
**Authors**: Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang

**Updated**: 2025-09-24T03:15:37Z

**Summary**: Efficiently processing the dynamics of requests, especially the context length variance, is important in Large Language Model (LLM) serving scenarios. However, there is an intrinsic trade-off: while leveraging parallelism strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to accommodate larger context lengths, it inevitably results in degraded overall throughput. In this paper, we propose Cross-Instance Parallelism Transformation (Gyges), which adaptively adjusts the parallelism strategies of running instances to align with the dynamics of incoming requests. We design (1) a page-friendly, header-centric layout to accelerate KV cache transformations; (2) dedicated weight padding to accelerate model weight transformations; and (3) a transformation-aware scheduler to cooperatively schedule requests and parallelism transformations, optimizing the overall performance. Evaluations using real-world traces show that Gyges improves throughput by 1.75x-6.57x compared to state-of-the-art solutions.

**Link**: [arxiv](http://arxiv.org/abs/2509.19729v1),  [pdf](http://arxiv.org/pdf/2509.19729v1)

**Tags**: cs.DC 



### LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale   Architectures
**Authors**: Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore

**Updated**: 2025-09-24T01:32:55Z

**Summary**: Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on three exascale machines -- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS Alps supercomputer, for the three potentials.

**Link**: [arxiv](http://arxiv.org/abs/2508.13523v2),  [pdf](http://arxiv.org/pdf/2508.13523v2)

**Tags**: cs.DC cs.PF physics.comp-ph C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2 



### CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn   Interactions with a Single On-Device LLM
**Authors**: Juntae Lee, Jihwan Bang, Seunghan Yang, Simyung Chang

**Updated**: 2025-09-24T01:20:47Z

**Summary**: We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which is a novel execution system for efficient sub-task handling in multi-turn interactions with a single on-device large language model (LLM). As LLMs become increasingly capable, a single model is expected to handle diverse sub-tasks that more effectively and comprehensively support answering user requests. Naive approach reprocesses the entire conversation context when switching between main and sub-tasks (e.g., query rewriting, summarization), incurring significant computational overhead. CIFLEX mitigates this overhead by reusing the key-value (KV) cache from the main task and injecting only task-specific instructions into isolated side paths. After sub-task execution, the model rolls back to the main path via cached context, thereby avoiding redundant prefill computation. To support sub-task selection, we also develop a hierarchical classification strategy tailored for small-scale models, decomposing multi-choice decisions into binary ones. Experiments show that CIFLEX significantly reduces computational costs without degrading task performance, enabling scalable and efficient multi-task dialogue on-device.

**Link**: [arxiv](http://arxiv.org/abs/2510.01239v1),  [pdf](http://arxiv.org/pdf/2510.01239v1)

**Tags**: cs.CL 



### Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method   for Multi-Agent Systems
**Authors**: Danilo Trombino, Vincenzo Pecorella, Alessandro de Giulii, Davide Tresoldi

**Updated**: 2025-09-23T21:46:38Z

**Summary**: Multi-agent systems (MAS) are increasingly tasked with solving complex, knowledge-intensive problems where effective agent orchestration is critical. Conventional orchestration methods rely on static agent descriptions, which often become outdated or incomplete. This limitation leads to inefficient task routing, particularly in dynamic environments where agent capabilities continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a novel approach that augments static descriptions with dynamic, privacy-preserving relevance signals derived from each agent's internal knowledge base (KB). In the proposed framework, when static descriptions are insufficient for a clear routing decision, the orchestrator prompts the subagents in parallel. Each agent then assesses the task's relevance against its private KB, returning a lightweight ACK signal without exposing the underlying data. These collected signals populate a shared semantic cache, providing dynamic indicators of agent suitability for future queries. By combining this novel mechanism with static descriptions, our method achieves more accurate and adaptive task routing preserving agent autonomy and data confidentiality. Benchmarks show that our KBA Orchestration significantly outperforms static description-driven methods in routing precision and overall system efficiency, making it suitable for large-scale systems that require higher accuracy than standard description-driven routing.

**Link**: [arxiv](http://arxiv.org/abs/2509.19599v1),  [pdf](http://arxiv.org/pdf/2509.19599v1)

**Tags**: cs.MA cs.AI 



### From Slow Bidirectional to Fast Autoregressive Video Diffusion Models
**Authors**: Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang

**Updated**: 2025-09-23T21:08:03Z

**Summary**: Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner.

**Link**: [arxiv](http://arxiv.org/abs/2412.07772v4),  [pdf](http://arxiv.org/pdf/2412.07772v4)

**Tags**: cs.CV 



### Decentralized Learning Strategies for Estimation Error Minimization with   Graph Neural Networks
**Authors**: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti

**Updated**: 2025-09-23T20:25:15Z

**Summary**: We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.

**Link**: [arxiv](http://arxiv.org/abs/2404.03227v3),  [pdf](http://arxiv.org/pdf/2404.03227v3)

**Tags**: eess.SP cs.LG 



### Automated Insertion of Flushes and Fences for Persistency
**Authors**: Yutong Guo, Weiyu Luo, Brian Demsky

**Updated**: 2025-09-23T18:14:21Z

**Summary**: CXL shared memory and persistent memory allow the contents of memory to persist beyond crashes. Stores to persistent or CXL memory are typically not immediately made persistent; developers must manually flush the corresponding cache lines to force the data to be written to the underlying storage. Correctly using flush and fence operations is known to be challenging. While state-of-the-art tools can find missing flush instructions, they often require bug-revealing test cases. No existing tools can ensure the absence of missing flush bugs.   In this paper, we present PMRobust, a compiler that automatically inserts flush and fence operations to ensure that code using persistent memory is free from missing flush and fence bugs. PMRobust employs a novel static analysis with optimizations that target newly allocated objects. We have evaluated PMRobust on persistent memory libraries and several persistent memory data structures and measured a geometric mean overhead of 0.26% relative to the original benchmarks with hand-placed flush and fence operations.

**Link**: [arxiv](http://arxiv.org/abs/2509.19459v1),  [pdf](http://arxiv.org/pdf/2509.19459v1)

**Tags**: cs.SE cs.PL 



### CompLLM: Compression for Long Context Q&A
**Authors**: Gabriele Berton, Jayakrishnan Unnikrishnan, Son Tran, Mubarak Shah

**Updated**: 2025-09-23T16:49:43Z

**Summary**: Large Language Models (LLMs) face significant computational challenges when processing long contexts due to the quadratic complexity of self-attention. While soft context compression methods, which map input text to smaller latent representations, have shown promise, their real-world adoption is limited. Existing techniques typically compress the context as a single unit, which leads to quadratic compression complexity and an inability to reuse computations across queries with overlapping contexts. In this work, we introduce CompLLM, a soft compression technique designed for practical deployment. Instead of processing the context holistically, CompLLM divides it into segments and compresses each one independently. This simple design choice yields three critical properties: efficiency, as the compression step scales linearly with the context length; scalability, enabling models trained on short sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and reusability, allowing compressed segments to be cached and reused across different queries. Our experiments show that with a 2x compression rate, at high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance comparable to that obtained with the uncompressed context, and even surpasses it on very long sequences, demonstrating its effectiveness and practical utility.

**Link**: [arxiv](http://arxiv.org/abs/2509.19228v1),  [pdf](http://arxiv.org/pdf/2509.19228v1)

**Tags**: cs.CL 



### 3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes   Equations with Applications to Geodynamics
**Authors**: Marcel Ferrari, Cyrill PÃ¼ntener, Alexander Sotoudeh, Niklas Viebig

**Updated**: 2025-09-23T14:25:13Z

**Summary**: We present the design, implementation, and evaluation of optimized matrix-free stencil kernels for multigrid smoothing in the incompressible Stokes equations with variable viscosity, motivated by geophysical flow problems. We investigate five smoother variants derived from different optimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked fused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a strategy that applies local iterations on overlapping tiles to improve cache reuse. To ensure correctness, we introduce an energy-based residual norm that balances velocity and pressure contributions, and validate all implementations using a high-contrast sinker benchmark representative of realistic geodynamic numerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of the ALPS supercomputer demonstrates that all smoothers scale well within a single NUMA domain, but the RAS-Jacobi smoother consistently achieves the best performance at higher core counts. It sustains over 90% weak-scaling efficiency up to 64 cores and delivers up to a threefold speedup compared to the C++ Jacobi baseline, owing to improved cache reuse and reduced memory traffic. These results show that temporal blocking, already employed in distributed-memory solvers to reduce communication, can also provide substantial benefits at the socket and NUMA level. This work highlights the importance of cache-aware stencil design for harnessing modern heterogeneous architectures and lays the groundwork for extending RAS-type temporal blocking strategies to three-dimensional problems and GPU accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2509.19061v1),  [pdf](http://arxiv.org/pdf/2509.19061v1)

**Tags**: physics.comp-ph cs.NA math.NA 65F08, 65N55, 65N22, 76M20 G.1.8; F.2.1; D.1.3; C.1.4 



### Obelix: Mitigating Side-Channels Through Dynamic Obfuscation
**Authors**: Jan Wichelmann, Anja Rabich, Anna P"atschke, Thomas Eisenbarth

**Updated**: 2025-09-23T12:32:51Z

**Summary**: Trusted execution environments (TEEs) offer hardware-assisted means to protect code and data. However, as shown in numerous results over the years, attackers can use side-channels to leak data access patterns and even single-step the code. While the vendors are slowly introducing hardware-based countermeasures for some attacks, others will stay unaddressed. This makes a software-level countermeasure desirable, but current available solutions only address very specific attack vectors or have a narrow leakage model.   In this work, we take a holistic view at the vulnerabilities of TEEs and design a tool named Obelix, which is the first to protect both code and data against a wide range of TEE attacks, from cache attacks over single-stepping to ciphertext side-channels. We analyze the practically achievable precision of state-of-the-art single-stepping tools, and present an algorithm which uses that knowledge to divide a program into uniform code blocks, that are indistinguishable for a strong attacker. By storing these blocks and the program data in oblivious RAM, the attacker cannot follow execution, effectively protecting both secret code and data. We describe how we automate our approach to make it available for developers who are unfamiliar with side-channels. As an obfuscation tool, Obelix comes with a considerable performance overhead, but compensates this with strong security guarantees and easy applicability without requiring any expert knowledge.

**Link**: [arxiv](http://arxiv.org/abs/2509.18909v1),  [pdf](http://arxiv.org/pdf/2509.18909v1)

**Tags**: cs.CR 



### PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference
**Authors**: Hao Zhang, Mengsi Lyu, Zhuo Chen, Xingrun Xing, Yulong Ao, Yonghua Lin

**Updated**: 2025-09-23T08:31:26Z

**Summary**: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the same (default) settings, our method achieves improved performance and faster inference, along with a 4.95$\times$ reduction in data transmission bandwidth consumption.

**Link**: [arxiv](http://arxiv.org/abs/2509.04467v3),  [pdf](http://arxiv.org/pdf/2509.04467v3)

**Tags**: cs.CL cs.AI 



### Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and   Flash
**Authors**: Fucheng Jia, Zewen Wu, Shiqi Jiang, Huiqiang Jiang, Qianxi Zhang, Yuqing Yang, Yunxin Liu, Ju Ren, Deyu Zhang, Ting Cao

**Updated**: 2025-09-23T08:24:07Z

**Summary**: Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.08378v2),  [pdf](http://arxiv.org/pdf/2504.08378v2)

**Tags**: cs.LG 



### Static Estimation of Reuse Profiles for Arrays in Nested Loops
**Authors**: Abdur Razzak, Atanu Barai, Nandakishore Santhi, Abdel-Hameed A. Badawy

**Updated**: 2025-09-23T06:10:20Z

**Summary**: Efficient memory access patterns play a crucial role in determining the overall performance of applications by exploiting temporal and spatial locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is a widely used metric to quantify temporal locality, measuring the distance between consecutive accesses to the same memory location. Traditionally, calculating RDH requires program execution and memory trace collection to obtain dynamic memory access behavior. This trace collection is often time-consuming, resource-intensive, and unsuitable for early-stage optimization or large-scale applications. Static prediction, on the other hand, offers a significant speedup in estimating RDH and cache hit rates. However, these approaches lack accuracy, since the predictions come without running the program and knowing the complete memory access pattern, more specifically when arrays are used inside nested loops. This paper presents a novel static analysis framework for predicting the reuse profiles of array references in programs with nested loop structures, without requiring any runtime information. By analyzing loop bounds, access patterns in smaller problem sizes, and predictive equations, our method predicts access patterns of arrays and estimates reuse distances and cache hit rate at compile time. This paper extends our previous study by incorporating more analysis and improving prediction by addressing previously unhandled reuse patterns. We evaluate our technique against a widely accepted traditional trace-driven profiling tool, Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our static predictor achieves comparable accuracy while offering orders-of-magnitude improvement in the analysis speed. This work offers a practical alternative to dynamic reuse profiling and paves the way for integration into compilers and static performance modeling tools.

**Link**: [arxiv](http://arxiv.org/abs/2509.18684v1),  [pdf](http://arxiv.org/pdf/2509.18684v1)

**Tags**: cs.PF 



### CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases
**Authors**: Yeonwoo Jeong, Hyunji Cho, Kyuri Park, Youngjae Kim, Sungyong Park

**Updated**: 2025-09-23T05:39:47Z

**Summary**: Embedding models capture both semantic and syntactic structures of queries, often mapping different queries to similar regions in vector space. This results in non-uniform cluster access patterns in modern disk-based vector databases. While existing approaches optimize individual queries, they overlook the impact of cluster access patterns, failing to account for the locality effects of queries that access similar clusters. This oversight increases cache miss penalty. To minimize the cache miss penalty, we propose CALL, a context-aware query grouping mechanism that organizes queries based on shared cluster access patterns. Additionally, CALL incorporates a group-aware prefetching method to minimize cache misses during transitions between query groups and latency-aware cluster loading. Experimental results show that CALL reduces the 99th percentile tail latency by up to 33% while consistently maintaining a higher cache hit ratio, substantially reducing search latency.

**Link**: [arxiv](http://arxiv.org/abs/2509.18670v1),  [pdf](http://arxiv.org/pdf/2509.18670v1)

**Tags**: cs.DB 



### VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic   Vision-Language Planning for Zero-Shot Transfer in Robot Navigation
**Authors**: Neel P. Bhatt, Yunhao Yang, Rohan Siva, Pranay Samineni, Daniel Milan, Zhangyang Wang, Ufuk Topcu

**Updated**: 2025-09-23T03:23:03Z

**Summary**: Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2509.18592v1),  [pdf](http://arxiv.org/pdf/2509.18592v1)

**Tags**: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY 



### Foresight: Adaptive Layer Reuse for Accelerated and High-Quality   Text-to-Video Generation
**Authors**: Muhammad Adnan, Nithesh Kurella, Akhil Arunkumar, Prashant J. Nair

**Updated**: 2025-09-22T19:20:33Z

**Summary**: Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.   We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to \latencyimprv end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.

**Link**: [arxiv](http://arxiv.org/abs/2506.00329v2),  [pdf](http://arxiv.org/pdf/2506.00329v2)

**Tags**: cs.LG cs.AI cs.CV 



### Speculate Deep and Accurate: Lossless and Training-Free Acceleration for   Offloaded LLMs via Substitute Speculative Decoding
**Authors**: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu

**Updated**: 2025-09-22T19:08:57Z

**Summary**: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

**Link**: [arxiv](http://arxiv.org/abs/2509.18344v1),  [pdf](http://arxiv.org/pdf/2509.18344v1)

**Tags**: cs.CL 



## Keyword: LLM Inference 
 ### KaVa: Latent Reasoning via Compressed KV-Cache Distillation
**Authors**: Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi

**Updated**: 2025-10-02T17:59:51Z

**Summary**: Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.02312v1),  [pdf](http://arxiv.org/pdf/2510.02312v1)

**Tags**: cs.LG 



### Inferring Dynamic Physical Properties from Video Foundation Models
**Authors**: Guanqi Zhan, Xianzheng Ma, Weidi Xie, Andrew Zisserman

**Updated**: 2025-10-02T17:59:50Z

**Summary**: We study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dynamic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, consisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mechanism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strategies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.

**Link**: [arxiv](http://arxiv.org/abs/2510.02311v1),  [pdf](http://arxiv.org/pdf/2510.02311v1)

**Tags**: cs.CV cs.LG 



### Drawing Conclusions from Draws: Rethinking Preference Semantics in   Arena-Style LLM Evaluation
**Authors**: Raphael Tang, Crystina Zhang, Wenyan Li, Carmen Lai, Pontus Stenetorp, Yao Lu

**Updated**: 2025-10-02T17:59:41Z

**Summary**: In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the "battle" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates.

**Link**: [arxiv](http://arxiv.org/abs/2510.02306v1),  [pdf](http://arxiv.org/pdf/2510.02306v1)

**Tags**: cs.CL 



### Equilibrium Matching: Generative Modeling with Implicit Energy-Based   Models
**Authors**: Runqian Wang, Yilun Du

**Updated**: 2025-10-02T17:59:06Z

**Summary**: We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.02300v1),  [pdf](http://arxiv.org/pdf/2510.02300v1)

**Tags**: cs.LG cs.AI cs.CV 



### Test-Time Anchoring for Discrete Diffusion Posterior Sampling
**Authors**: Litu Rout, Andreas Lugmayr, Yasamin Jafarian, Srivatsan Varadharajan, Constantine Caramanis, Sanjay Shakkottai, Ira Kemelmacher-Shlizerman

**Updated**: 2025-10-02T17:58:37Z

**Summary**: We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations -- quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing.

**Link**: [arxiv](http://arxiv.org/abs/2510.02291v1),  [pdf](http://arxiv.org/pdf/2510.02291v1)

**Tags**: cs.LG cs.CV stat.ML 



### LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code   Analysis
**Authors**: Benedikt Fein, Florian ObermÃ¼ller, Gordon Fraser

**Updated**: 2025-10-02T17:57:32Z

**Summary**: Large language models (LLMs) have become an essential tool to support developers using traditional text-based programming languages, but the graphical notation of the block-based Scratch programming environment inhibits the use of LLMs. To overcome this limitation, we propose the LitterBox+ framework that extends the Scratch static code analysis tool LitterBox with the generative abilities of LLMs. By converting block-based code to a textual representation suitable for LLMs, LitterBox+ allows users to query LLMs about their programs, about quality issues reported by LitterBox, and it allows generating code fixes. Besides offering a programmatic API for these functionalities, LitterBox+ also extends the Scratch user interface to make these functionalities available directly in the environment familiar to learners. The framework is designed to be easily extensible with other prompts, LLM providers, and new features combining the program analysis capabilities of LitterBox with the generative features of LLMs. We provide a screencast demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

**Link**: [arxiv](http://arxiv.org/abs/2509.12021v2),  [pdf](http://arxiv.org/pdf/2509.12021v2)

**Tags**: cs.SE 



### Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming   Attacks
**Authors**: Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth

**Updated**: 2025-10-02T17:57:05Z

**Summary**: Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.

**Link**: [arxiv](http://arxiv.org/abs/2510.02286v1),  [pdf](http://arxiv.org/pdf/2510.02286v1)

**Tags**: cs.LG cs.AI cs.CL 



### Learning to Generate Object Interactions with Physics-Guided Video   Diffusion
**Authors**: David Romero, Ariana Bermudez, Hao Li, Fabio Pizzati, Ivan Laptev

**Updated**: 2025-10-02T17:56:46Z

**Summary**: Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2510.02284v1),  [pdf](http://arxiv.org/pdf/2510.02284v1)

**Tags**: cs.CV cs.AI cs.LG 



### Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods   for Natural Language Generation
**Authors**: Mykyta Ielanskyi, Kajetan Schweighofer, Lukas Aichberger, Sepp Hochreiter

**Updated**: 2025-10-02T17:54:09Z

**Summary**: Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings.

**Link**: [arxiv](http://arxiv.org/abs/2510.02279v1),  [pdf](http://arxiv.org/pdf/2510.02279v1)

**Tags**: cs.LG cs.AI 



### InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in   Tool-Augmented Agents
**Authors**: Yaxin Du, Yuanshuo Zhang, Xiyuan Yang, Yifan Zhou, Cheng Wang, Gongyi Zou, Xianghe Pang, Wenhao Wang, Menglan Chen, Shuo Tang, Zhiyu Li, Siheng Chen

**Updated**: 2025-10-02T17:48:03Z

**Summary**: Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools -- and more importantly, whether they can integrate them with general-purpose search to solve complex tasks. Therefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality. Experiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling.

**Link**: [arxiv](http://arxiv.org/abs/2510.02271v1),  [pdf](http://arxiv.org/pdf/2510.02271v1)

**Tags**: cs.CL cs.AI 



### microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for   Fine-Grained Image Classification
**Authors**: Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan

**Updated**: 2025-10-02T17:47:39Z

**Summary**: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at https://github.com/sathiiii/microCLIP.

**Link**: [arxiv](http://arxiv.org/abs/2510.02270v1),  [pdf](http://arxiv.org/pdf/2510.02270v1)

**Tags**: cs.CV cs.AI 



### Do You Know Where Your Camera Is? View-Invariant Policy Learning with   Camera Conditioning
**Authors**: Tianchong Jiang, Jingtian Ji, Xiangshan Tan, Jiading Fang, Anand Bhattad, Vitor Guizilini, Matthew R. Walter

**Updated**: 2025-10-02T17:47:06Z

**Summary**: We study view-invariant imitation learning by explicitly conditioning policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we show that conditioning on extrinsics significantly improves generalization across viewpoints for standard behavior cloning policies, including ACT, Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic viewpoint shifts, we introduce six manipulation tasks in RoboSuite and ManiSkill that pair "fixed" and "randomized" scene variants, decoupling background cues from camera pose. Our analysis reveals that policies without extrinsics often infer camera pose using visual cues from static backgrounds in fixed scenes; this shortcut collapses when workspace geometry or camera placement shifts. Conditioning on extrinsics restores performance and yields robust RGB-only control without depth. We release the tasks, demonstrations, and code at https://ripl.github.io/know_your_camera/ .

**Link**: [arxiv](http://arxiv.org/abs/2510.02268v1),  [pdf](http://arxiv.org/pdf/2510.02268v1)

**Tags**: cs.RO cs.CV 



### RLAD: Training LLMs to Discover Abstractions for Solving Reasoning   Problems
**Authors**: Yuxiao Qu, Anikait Singh, Yoonho Lee, Amrith Setlur, Ruslan Salakhutdinov, Chelsea Finn, Aviral Kumar

**Updated**: 2025-10-02T17:44:23Z

**Summary**: Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement "algorithmic procedures" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.

**Link**: [arxiv](http://arxiv.org/abs/2510.02263v1),  [pdf](http://arxiv.org/pdf/2510.02263v1)

**Tags**: cs.AI cs.CL cs.LG 



### SAGAbg III: Environmental Stellar Mass Functions, Self-Quenching, and   the Stellar-to-Halo Mass Relation in the Dwarf Galaxy Regime
**Authors**: Erin Kado-Fong, Yao-Yuan Mao, Yasmeen Asali, Marla Geha, Risa H. Wechsler, Mithi A. C. de los Reyes, Yunchong Wang, Ethan O. Nadler, Nitya Kallivayalil, Erik J. Tollerud, Benjamin Weiner

**Updated**: 2025-10-02T17:43:05Z

**Summary**: Recent efforts have extended our view of the number and properties of satellite galaxies beyond the Local Group firmly down to $\rm M_\star\sim 10^6 M_\odot$. A similarly complete view of the field dwarf population has lagged behind. Using the background galaxies sample from the Satellites Around Galactic Analogs (SAGA) Survey at $z<0.05$, we take inventory of the dwarf population down to $\rm M_\star \sim 5\times10^6 M_\odot$ using three metrics: the stellar mass function (SMF) as function of environment, the stellar-to-halo mass relation (SHMR) of dwarf galaxies inferred via abundance matching, and the quenched fraction of highly isolated dwarfs. We find that the low-mass SMF shape shows minimal environmental dependence, with the field dwarf SMF described by a low-mass power-law index of $\alpha_1=-1.44\pm0.09$ down to $\rm M_\star \sim 5\times10^6 M_\odot$, and that the quenched fraction of isolated dwarfs drops monotonically to $f_{q} \sim 10^{-3}$ at $\rm M_\star \sim \rm 10^{8.5} M_\odot$. Though slightly steeper than estimates from \HI{} kinematic measures, our inferred SHMR agrees with literature measurements of satellite systems, consistent with minimal environmental dependence of the SHMR in the probed mass range. Finally, although most contemporary cosmological simulations against which we compare accurately predict the \sagalocal{} SHMR, we find that big-box cosmological simulations largely over-predict isolated galaxy quenched fractions via a turnaround in $f_q(\rm M_\star)$ at $\rm 10^8\lesssim M_\star/M_\odot\lesssim 10^9$, underscoring the complexities in disentangling the drivers of galaxy formation and the need for systematic multidimensional observations of the dwarf population across environments.

**Link**: [arxiv](http://arxiv.org/abs/2509.20444v2),  [pdf](http://arxiv.org/pdf/2509.20444v2)

**Tags**: astro-ph.GA 



### From Frames to Clips: Efficient Key Clip Selection for Long-Form Video   Understanding
**Authors**: Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler

**Updated**: 2025-10-02T17:43:01Z

**Summary**: Video Large Language Models (VLMs) have achieved remarkable results on a variety of vision language tasks, yet their practical use is limited by the "needle in a haystack" problem: the massive number of visual tokens produced from raw video frames exhausts the model's context window. Existing solutions alleviate this issue by selecting a sparse set of frames, thereby reducing token count, but such frame-wise selection discards essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity. In this work we systematically explore the impact of temporal information and demonstrate that extending selection from isolated key frames to key clips, which are short, temporally coherent segments, improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we propose an adaptive resolution strategy that dynamically balances spatial resolution and clip length, ensuring a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling Video LLMs to real world video understanding applications. Project webpage is available at https://guangyusun.com/f2c .

**Link**: [arxiv](http://arxiv.org/abs/2510.02262v1),  [pdf](http://arxiv.org/pdf/2510.02262v1)

**Tags**: cs.CV 



### VITA: Vision-to-Action Flow Matching Policy
**Authors**: Dechen Gao, Boqi Zhao, Andrew Lee, Ian Chuang, Hanchu Zhou, Hang Wang, Zhe Zhao, Junshan Zhang, Iman Soltani

**Updated**: 2025-10-02T17:42:30Z

**Summary**: Conventional flow matching and diffusion-based policies sample through iterative denoising from standard noise distributions (e.g., Gaussian), and require conditioning mechanisms to incorporate visual information during the generative process, incurring substantial time and memory overhead. To reduce the complexity, we develop VITA(VIsion-To-Action policy), a noise-free and conditioning-free policy learning framework that directly maps visual representations to latent actions using flow matching. VITA treats latent visual representations as the source of the flow, thus eliminating the need of conditioning. As expected, bridging vision and action is challenging, because actions are lower-dimensional, less structured, and sparser than visual representations; moreover, flow matching requires the source and target to have the same dimensionality. To overcome this, we introduce an action autoencoder that maps raw actions into a structured latent space aligned with visual latents, trained jointly with flow matching. To further prevent latent space collapse, we propose flow latent decoding, which anchors the latent generation process by backpropagating the action reconstruction loss through the flow matching ODE (ordinary differential equations) solving steps. We evaluate VITA on 8 simulation and 2 real-world tasks from ALOHA and Robomimic. VITA outperforms or matches state-of-the-art generative policies, while achieving 1.5-2.3x faster inference compared to conventional methods with conditioning. Project page: https://ucd-dare.github.io/VITA/

**Link**: [arxiv](http://arxiv.org/abs/2507.13231v2),  [pdf](http://arxiv.org/pdf/2507.13231v2)

**Tags**: cs.CV cs.AI cs.RO 



### Mapping the Cloud-Driven Atmospheric Dynamics & Chemistry of an Isolated   Exoplanet Analog with Harmonic Signatures
**Authors**: Michael K. Plummer, Francis P. Cocchini, Peter A. Kearns, Allison McCarthy, Ãtienne Artigau, Nicolas B. Cowan, Roman Akhmetshyn, Johanna Vos, Evert Nasedkin, Channon Visscher, BjÃ¶rn Benneke, RenÃ© Doyon, Stanimir A. Metchev, Jason F. Rowe, Genaro SuÃ¡rez

**Updated**: 2025-10-02T17:42:26Z

**Summary**: Young planetary-mass objects and brown dwarfs near the L/T spectral transition exhibit enhanced spectrophotometric variability over field brown dwarfs. Patchy clouds, auroral processes, stratospheric hot spots, and complex carbon chemistry have all been proposed as potential sources of this variability. Using time-resolved, low-to-mid-resolution spectroscopy collected with the JWST/NIRISS and NIRSpec instruments, we apply harmonic analysis to SIMP J0136, a highly variable, young, isolated planetary-mass object. Odd harmonics (k=3) at pressure levels (> 1 bar) corresponding to iron and forsterite cloud formation suggest North/South hemispheric asymmetry in the cloudy, and likely equatorial, regions. We use the inferred harmonics, along with 1-D substellar atmospheric models, to map the flux variability by atmospheric pressure level. These vertical maps demonstrate robust interaction between deep convective weather layers and the overlying stratified and radiative atmosphere. We identify distinct time-varying structures in the near-infrared that we interpret as planetary-scale wave (e.g., Rossby or Kelvin)-associated cloud modulation. We detect variability in water (S/N = 14.0), carbon monoxide (S/N = 13.0), and methane (S/N = 14.9) molecular signatures. Forsterite cloud modulation is anti-correlated with overlying carbon monoxide and water abundances and correlated with deep methane absorption, suggesting complex interaction between cloud formation, atmospheric chemistry, and temperature structure. Furthermore, we identify distinct harmonic behavior between methane and carbon monoxide absorption bands, providing evidence for time-resolved disequilibrium carbon chemistry. At the lowest pressures (< 100 mbar), we find that the mapped methane lines transition from absorption to emission, supporting evidence of high-altitude auroral heating via electron precipitation.

**Link**: [arxiv](http://arxiv.org/abs/2510.02260v1),  [pdf](http://arxiv.org/pdf/2510.02260v1)

**Tags**: astro-ph.EP astro-ph.SR physics.ao-ph physics.geo-ph 



### Transformers Discover Molecular Structure Without Graph Priors
**Authors**: Tobias Kreiman, Yutong Bai, Fadi Atieh, Elizabeth Weaver, Eric Qu, Aditi S. Krishnapriyan

**Updated**: 2025-10-02T17:42:10Z

**Summary**: Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning, particularly for molecular property prediction and machine learning interatomic potentials (MLIPs). GNNs perform message passing on predefined graphs often induced by a fixed radius cutoff or k-nearest neighbor scheme. While this design aligns with the locality present in many molecular tasks, a hard-coded graph can limit expressivity due to the fixed receptive field and slows down inference with sparse graph operations. In this work, we investigate whether pure, unmodified Transformers trained directly on Cartesian coordinates$\unicode{x2013}$without predefined graphs or physical priors$\unicode{x2013}$can approximate molecular energies and forces. As a starting point for our analysis, we demonstrate how to train a Transformer to competitive energy and force mean absolute errors under a matched training compute budget, relative to a state-of-the-art equivariant GNN on the OMol25 dataset. We discover that the Transformer learns physically consistent patterns$\unicode{x2013}$such as attention weights that decay inversely with interatomic distance$\unicode{x2013}$and flexibly adapts them across different molecular environments due to the absence of hard-coded biases. The use of a standard Transformer also unlocks predictable improvements with respect to scaling training resources, consistent with empirical scaling laws observed in other domains. Our results demonstrate that many favorable properties of GNNs can emerge adaptively in Transformers, challenging the necessity of hard-coded graph inductive biases and pointing toward standardized, scalable architectures for molecular modeling.

**Link**: [arxiv](http://arxiv.org/abs/2510.02259v1),  [pdf](http://arxiv.org/pdf/2510.02259v1)

**Tags**: cs.LG cond-mat.mtrl-sci physics.chem-ph q-bio.BM 



### Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case   Study on English-Korean Code-Switching
**Authors**: Seoyeon Kim, Huiseo Kim, Chanjun Park, Jinyoung Yeo, Dongha Lee

**Updated**: 2025-10-02T17:40:50Z

**Summary**: Recent large language models (LLMs) demonstrate multilingual abilities, yet they are English-centric due to dominance of English in training corpora. The limited resource for low-resource languages remains a crucial challenge. Code-switching (CS), a phenomenon where multilingual speakers alternate between languages in a discourse, can convey subtle cultural and linguistic nuances that can be otherwise lost in translation and elicits language-specific knowledge in human communications. In light of this, we investigate whether code-switching can activate, or identify and leverage knowledge for reasoning when LLMs solve low-resource language tasks. To facilitate the research, we first present EnKoQA, a synthetic English-Korean CS question-answering dataset. We provide comprehensive analysis on a variety of multilingual LLMs by subdividing activation process into knowledge identification and knowledge leveraging. Our results demonstrate that compared to English text, CS can faithfully activate knowledge inside LLMs especially on language-specific domains, suggesting the potential of code-switching on low-resource language tasks.

**Link**: [arxiv](http://arxiv.org/abs/2410.18436v4),  [pdf](http://arxiv.org/pdf/2410.18436v4)

**Tags**: cs.CL 



### Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative   Entropy Regulation
**Authors**: Tianyi Jiang, Yi Bin, Yujuan Ding, Kainian Zhu, Fei Ma, Jingkuan Song, Heng Tao Shen

**Updated**: 2025-10-02T17:36:50Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities on complex problems using long Chain-of-Thought (CoT) reasoning. However, they often suffer from overthinking, meaning generating unnecessarily lengthy reasoning steps for simpler problems. This issue may degrade the efficiency of the models and make them difficult to adapt the reasoning depth to the complexity of problems. To address this, we introduce a novel metric Token Entropy Cumulative Average (TECA), which measures the extent of exploration throughout the reasoning process. We further propose a novel reasoning paradigm -- Explore Briefly, Then Decide -- with an associated Cumulative Entropy Regulation (CER) mechanism. This paradigm leverages TECA to help the model dynamically determine the optimal point to conclude its thought process and provide a final answer, thus achieving efficient reasoning. Experimental results across diverse mathematical benchmarks show that our approach substantially mitigates overthinking without sacrificing problem-solving ability. With our thinking paradigm, the average response length decreases by up to 71% on simpler datasets, demonstrating the effectiveness of our method in creating a more efficient and adaptive reasoning process.

**Link**: [arxiv](http://arxiv.org/abs/2510.02249v1),  [pdf](http://arxiv.org/pdf/2510.02249v1)

**Tags**: cs.CL cs.AI cs.LG 



### Probabilistic Reasoning with LLMs for k-anonymity Estimation
**Authors**: Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu

**Updated**: 2025-10-02T17:36:23Z

**Summary**: Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average.

**Link**: [arxiv](http://arxiv.org/abs/2503.09674v4),  [pdf](http://arxiv.org/pdf/2503.09674v4)

**Tags**: cs.CL cs.LG 



### AccurateRAG: A Framework for Building Accurate Retrieval-Augmented   Question-Answering Applications
**Authors**: Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Van-Cuong Pham, Hoang Ngo, Dat Quoc Nguyen

**Updated**: 2025-10-02T17:30:08Z

**Summary**: We introduce AccurateRAG -- a novel framework for constructing high-performance question-answering applications based on retrieval-augmented generation (RAG). Our framework offers a pipeline for development efficiency with tools for raw dataset processing, fine-tuning data generation, text embedding & LLM fine-tuning, output evaluation, and building RAG systems locally. Experimental results show that our framework outperforms previous strong baselines and obtains new state-of-the-art question-answering performance on benchmark datasets.

**Link**: [arxiv](http://arxiv.org/abs/2510.02243v1),  [pdf](http://arxiv.org/pdf/2510.02243v1)

**Tags**: cs.CL 



### Transfer of Stability from the Classical to the Fractional Anisotropic   CalderÃ³n Problem
**Authors**: Hendrik Baers, Angkana RÃ¼land

**Updated**: 2025-10-02T17:29:53Z

**Summary**: We discuss two spectral fractional anisotropic Calder\'on problems with source-to-solution measurements and their quantitative relation to the classical Calder\'on problem. Firstly, we consider the anistropic fractional Calder\'on problem from [FGKU25]. In this setting, we quantify the relation between the local and nonlocal Calder\'on problems which had been deduced in [R25] and provide an associated stability estimate. As a consequence, any stability result which holds on the level of the local problem with source-to-solution data has a direct nonlocal analogue (up to a logarithmic loss). Secondly, we introduce and discuss the fractional Calder\'on problem with source-to-solution measurements for the spectral fractional Dirichlet Laplacian on open, bounded, connected, Lipschitz sets on $\mathbb{R}^n$. Also in this context, we provide a qualitative and quantitative transfer of uniqueness from the local to the nonlocal setting. As a consequence, we infer the first stability results for the principal part for a fractional Calder\'on type problem for which no reduction of Liouville type is known. Our arguments rely on quantitative unique continuation arguments. As a result of independent interest, we also prove a quantitative relation between source-to-solution and Dirichlet-to-Neumann measurements for the classical Calder\'on problem.

**Link**: [arxiv](http://arxiv.org/abs/2510.02242v1),  [pdf](http://arxiv.org/pdf/2510.02242v1)

**Tags**: math.AP 



### Study on LLMs for Promptagator-Style Dense Retriever Training
**Authors**: Daniel Gwon, Nour Jedidi, Jimmy Lin

**Updated**: 2025-10-02T17:29:51Z

**Summary**: Promptagator demonstrated that Large Language Models (LLMs) with few-shot prompts can be used as task-specific query generators for fine-tuning domain-specialized dense retrieval models. However, the original Promptagator approach relied on proprietary and large-scale LLMs which users may not have access to or may be prohibited from using with sensitive data. In this work, we study the impact of open-source LLMs at accessible scales ($\leq$14B parameters) as an alternative. Our results demonstrate that open-source LLMs as small as 3B parameters can serve as effective Promptagator-style query generators. We hope our work will inform practitioners with reliable alternatives for synthetic data generation and give insights to maximize fine-tuning results for domain-specific applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.02241v1),  [pdf](http://arxiv.org/pdf/2510.02241v1)

**Tags**: cs.IR cs.CL 



### AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features
**Authors**: Xudong Zhu, Mohammad Mahdi Khalili, Zhihui Zhu

**Updated**: 2025-10-02T17:28:55Z

**Summary**: Sparse autoencoders (SAEs) have emerged as powerful techniques for interpretability of large language models (LLMs), aiming to decompose hidden states into meaningful semantic features. While several SAE variants have been proposed, there remains no principled framework to derive SAEs from the original dictionary learning formulation. In this work, we introduce such a framework by unrolling the proximal gradient method for sparse coding. We show that a single-step update naturally recovers common SAE variants, including ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation of existing SAEs: their sparsity-inducing regularizers enforce non-negativity, preventing a single feature from representing bidirectional concepts (e.g., male vs. female). This structural constraint fragments semantic axes into separate, redundant features, limiting representational completeness. To address this issue, we propose AbsTopK SAE, a new variant derived from the $\ell_0$ sparsity constraint that applies hard thresholding over the largest-magnitude activations. By preserving both positive and negative activations, AbsTopK uncovers richer, bidirectional conceptual representations. Comprehensive experiments across four LLMs and seven probing and steering tasks show that AbsTopK improves reconstruction fidelity, enhances interpretability, and enables single features to encode contrasting concepts. Remarkably, AbsTopK matches or even surpasses the Difference-in-Mean method, a supervised approach that requires labeled data for each concept and has been shown in prior work to outperform SAEs.

**Link**: [arxiv](http://arxiv.org/abs/2510.00404v2),  [pdf](http://arxiv.org/pdf/2510.00404v2)

**Tags**: cs.LG cs.AI cs.CL 



### Adapting Large Language Models for Character-based Augmentative and   Alternative Communication
**Authors**: Dylan Gaines, Keith Vertanen

**Updated**: 2025-10-02T17:20:32Z

**Summary**: Users of Augmentative and Alternative Communication (AAC) may write letter-by-letter via an interface that uses a character language model. However, most state-of-the-art large pretrained language models predict subword tokens of variable length. We investigate how to practically use such models to make accurate and efficient character predictions. Our algorithm for producing character predictions from a subword large language model (LLM) provides more accurate predictions than using a classification layer, a byte-level LLM, or an n-gram model. Additionally, we investigate a domain adaptation procedure based on a large dataset of sentences we curated based on scoring how useful each sentence might be for spoken or written AAC communication. We find our procedure further improves model performance on simple, conversational text.

**Link**: [arxiv](http://arxiv.org/abs/2501.10582v3),  [pdf](http://arxiv.org/pdf/2501.10582v3)

**Tags**: cs.CL cs.HC 



### The Reasoning Boundary Paradox: How Reinforcement Learning Constrains   Language Models
**Authors**: Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan

**Updated**: 2025-10-02T17:17:27Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.

**Link**: [arxiv](http://arxiv.org/abs/2510.02230v1),  [pdf](http://arxiv.org/pdf/2510.02230v1)

**Tags**: cs.AI cs.CL cs.CV 



### xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity
**Authors**: Maximilian Beck, Kajetan Schweighofer, Sebastian BÃ¶ck, Sebastian Lehner, Sepp Hochreiter

**Updated**: 2025-10-02T17:14:34Z

**Summary**: Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow.

**Link**: [arxiv](http://arxiv.org/abs/2510.02228v1),  [pdf](http://arxiv.org/pdf/2510.02228v1)

**Tags**: cs.LG 



### More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration
**Authors**: Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Hengtao Shen

**Updated**: 2025-10-02T17:14:00Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.

**Link**: [arxiv](http://arxiv.org/abs/2510.02227v1),  [pdf](http://arxiv.org/pdf/2510.02227v1)

**Tags**: cs.CL cs.AI cs.LG 



### TempoControl: Temporal Attention Guidance for Text-to-Video Models
**Authors**: Shira Schiber, Ofir Lindenbaum, Idan Schwartz

**Updated**: 2025-10-02T17:13:35Z

**Summary**: Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.02226v1),  [pdf](http://arxiv.org/pdf/2510.02226v1)

**Tags**: cs.CV cs.AI cs.LG 



### Collaborative Edge Inference via Semantic Grouping under Wireless   Channel Constraints
**Authors**: Mateus P. Mota, Mattia Merluzzi, Emilio Calvanese Strinati

**Updated**: 2025-10-02T17:06:13Z

**Summary**: In this paper, we study the framework of collaborative inference, or edge ensembles. This framework enables multiple edge devices to improve classification accuracy by exchanging intermediate features rather than raw observations. However, efficient communication strategies are essential to balance accuracy and bandwidth limitations. Building upon a key-query mechanism for selective information exchange, this work extends collaborative inference by studying the impact of channel noise in feature communication, the choice of intermediate collaboration points, and the communication-accuracy trade-off across tasks. By analyzing how different collaboration points affect performance and exploring communication pruning, we show that it is possible to optimize accuracy while minimizing resource usage. We show that the intermediate collaboration approach is robust to channel errors and that the query transmission needs a higher degree of reliability than the data transmission itself.

**Link**: [arxiv](http://arxiv.org/abs/2510.02222v1),  [pdf](http://arxiv.org/pdf/2510.02222v1)

**Tags**: cs.IT eess.SP math.IT 



### Contrastive Retrieval Heads Improve Attention-Based Re-Ranking
**Authors**: Linh Tran, Yulong Li, Radu Florian, Wei Sun

**Updated**: 2025-10-02T17:03:09Z

**Summary**: The strong zero-shot and long-context capabilities of recent Large Language Models (LLMs) have paved the way for highly effective re-ranking systems. Attention-based re-rankers leverage attention weights from transformer heads to produce relevance scores, but not all heads are created equally: many contribute noise and redundancy, thus limiting performance. To address this, we introduce CoRe heads, a small set of retrieval heads identified via a contrastive scoring metric that explicitly rewards high attention heads that correlate with relevant documents, while downplaying nodes with higher attention that correlate with irrelevant documents. This relative ranking criterion isolates the most discriminative heads for re-ranking and yields a state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs show that aggregated signals from CoRe heads, constituting less than 1% of all heads, substantially improve re-ranking accuracy over strong baselines. We further find that CoRe heads are concentrated in middle layers, and pruning the computation of final 50% of model layers preserves accuracy while significantly reducing inference time and memory usage.

**Link**: [arxiv](http://arxiv.org/abs/2510.02219v1),  [pdf](http://arxiv.org/pdf/2510.02219v1)

**Tags**: cs.IR 



### Training Hybrid Deep Quantum Neural Network for Efficient Reinforcement   Learning
**Authors**: Jie Luo, Jeremy Kulcsar, Xueyin Chen, Giulio Giaconi, Georgios Korpas

**Updated**: 2025-10-02T17:02:26Z

**Summary**: Quantum circuits embed data in a Hilbert space whose dimensionality grows exponentially with the number of qubits, allowing even shallow parameterised quantum circuits (PQCs) to represent highly-correlated probability distributions that are costly for classical networks to capture. Reinforcement-learning (RL) agents, which must reason over long-horizon, continuous-control tasks, stand to benefit from this expressive quantum feature space, but only if the quantum layers can be trained jointly with the surrounding deep-neural components. Current gradient-estimation techniques (e.g., parameter-shift rule) make such hybrid training impractical for realistic RL workloads, because every gradient step requires a prohibitive number of circuit evaluations and thus erodes the potential quantum advantage. We introduce qtDNN, a tangential surrogate that locally approximates a PQC with a small differentiable network trained on-the-fly from the same minibatch. Embedding qtDNN inside the computation graph yields scalable batch gradients while keeping the original quantum layer for inference. Building on qtDNN we design hDQNN-TD3, a hybrid deep quantum neural network for continuous-control reinforcement learning based on the TD3 architecture, which matches or exceeds state-of-the-art classical performance on popular benchmarks. The method opens a path toward applying hybrid quantum models to large-scale RL and other gradient-intensive machine-learning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2503.09119v6),  [pdf](http://arxiv.org/pdf/2503.09119v6)

**Tags**: quant-ph 



### DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via   Reinforcement Learning
**Authors**: Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus

**Updated**: 2025-10-02T16:57:24Z

**Summary**: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.02212v1),  [pdf](http://arxiv.org/pdf/2510.02212v1)

**Tags**: cs.LG cs.AI 



### DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph   Completion across General and Biomedical Domains
**Authors**: Yongkang Xiao, Sinian Zhang, Yi Dai, Huixue Zhou, Jue Hou, Jie Ding, Rui Zhang

**Updated**: 2025-10-02T16:56:36Z

**Summary**: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.

**Link**: [arxiv](http://arxiv.org/abs/2506.00708v2),  [pdf](http://arxiv.org/pdf/2506.00708v2)

**Tags**: cs.AI cs.CL cs.LG 



### StockBench: Can LLM Agents Trade Stocks Profitably In Real-world   Markets?
**Authors**: Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li

**Updated**: 2025-10-02T16:54:57Z

**Summary**: Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.

**Link**: [arxiv](http://arxiv.org/abs/2510.02209v1),  [pdf](http://arxiv.org/pdf/2510.02209v1)

**Tags**: cs.LG cs.CL 



### Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling
**Authors**: Daniel Gallo FernÃ¡ndez

**Updated**: 2025-10-02T16:52:45Z

**Summary**: Sequence-to-sequence models have become central in Artificial Intelligence, particularly following the introduction of the transformer architecture. While initially developed for Natural Language Processing, these models have demonstrated utility across domains, including Computer Vision. Such models require mechanisms to exchange information along the time dimension, typically using recurrent or self-attention layers. However, self-attention scales quadratically with sequence length, limiting its practicality for very long sequences.   We introduce Poolformer, a sequence-to-sequence model that replaces self-attention with recurrent layers and incorporates pooling operations to reduce sequence length. Poolformer is defined recursively using SkipBlocks, which contain residual blocks, a down-pooling layer, a nested SkipBlock, an up-pooling layer, and additional residual blocks. We conduct extensive experiments to support our architectural choices.   Our results show that pooling greatly accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. Our experiments also suggest that long-range dependencies are handled by deep layers, while shallow layers take care of short-term features.   Evaluated on raw audio, which naturally features long sequence lengths, Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba. Future directions include applications to text and vision, as well as multi-modal scenarios, where a Poolformer-based LLM could effectively process dense representations of images and videos.

**Link**: [arxiv](http://arxiv.org/abs/2510.02206v1),  [pdf](http://arxiv.org/pdf/2510.02206v1)

**Tags**: cs.LG 



### Understanding the Origins of Super-Puff Planets: A New Mass-Loss Regime   Coupled to Planetary Evolution
**Authors**: Yao Tang, Jonathan J. Fortney, Ruth Murray-Clay, Madelyne Broome

**Updated**: 2025-10-02T16:50:16Z

**Summary**: Super-puffs are a class of low-mass, large-radius planets that have challenged planet formation and evolution models. Their high inferred H/He mass fractions, required to explain their physical sizes, would lead to rapid atmospheric escape, raising questions about their long-term retention. Recent modeling work indicates that low-mass planets typically require 50\% less H/He mass to match their observed radius, due to significant roles of the radiative atmosphere and interior heating from the rock/iron core. Here, through a new quantitative analysis of XUV-driven escape in sub-Neptunes, we find that previous studies overestimated mass loss, as scaling laws in low-gravity regimes deviate greatly from the widely used energy-limited regime. We define a new regime, thermal-energy-mediated photoevaporation (TEMP), in which thermal energy conversion critically sets the mass-loss rate. These effects make super-puffs more resilient to mass loss than previously thought. We develop a coupled evolution model integrating this updated thermal evolution framework with a 1D hydrodynamic photoevaporation model. Applying this novel, joint model to observed super-puffs and young low-density planets, we find that their masses, radii and transit pressures align with predictions assuming either a clear or hazy atmosphere. This indicates that super-puffs have undergone a combination of boil-off and photoevaporative mass loss, with boil-off dominating the process. Our results indicate that low-density planets typically possess both a thick convective envelope and substantial radiative atmosphere, which contribute to their large radii. For this to occur, these planets must have intermediate masses of 5-10$M_\oplus$ and receive stellar insolation $\lesssim 30F_\oplus$, favoring FG-type stars over M-dwarfs.

**Link**: [arxiv](http://arxiv.org/abs/2510.02201v1),  [pdf](http://arxiv.org/pdf/2510.02201v1)

**Tags**: astro-ph.EP 



### ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge   Graph Exploration Utilities
**Authors**: Felix Brei, Lorenz BÃ¼hmann, Johannes Frey, Daniel Gerber, Lars-Peter Meyer, Claus Stadler, Kirill Bulert

**Updated**: 2025-10-02T16:49:27Z

**Summary**: Interacting with knowledge graphs can be a daunting task for people without a background in computer science since the query language that is used (SPARQL) has a high barrier of entry. Large language models (LLMs) can lower that barrier by providing support in the form of Text2SPARQL translation. In this paper we introduce a generalized method based on SPINACH, an LLM backed agent that translates natural language questions to SPARQL queries not in a single shot, but as an iterative process of exploration and execution. We describe the overall architecture and reasoning behind our design decisions, and also conduct a thorough analysis of the agent behavior to gain insights into future areas for targeted improvements. This work was motivated by the Text2SPARQL challenge, a challenge that was held to facilitate improvements in the Text2SPARQL domain.

**Link**: [arxiv](http://arxiv.org/abs/2510.02200v1),  [pdf](http://arxiv.org/pdf/2510.02200v1)

**Tags**: cs.CL cs.AI 



### On Code-Induced Reasoning in LLMs
**Authors**: Abdul Waheed, Zhen Wu, Carolyn RosÃ©, Daphne Ippolito

**Updated**: 2025-10-02T16:45:24Z

**Summary**: Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2509.21499v2),  [pdf](http://arxiv.org/pdf/2509.21499v2)

**Tags**: cs.CL cs.PL 



### UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language   Models
**Authors**: Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie

**Updated**: 2025-10-02T16:43:33Z

**Summary**: Large Language Models (LLMs) have achieved remarkable progress across a wide range of tasks, but remain vulnerable to safety risks such as harmful content generation and jailbreak attacks. Existing safety techniques -- including external guardrails, inference-time guidance, and post-training alignment -- each face limitations in balancing safety, utility, and controllability. In this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM safety through safety-aware upcycling. Our approach first identifies safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE) structure, where the router acts as a soft guardrail that selectively activates original MLPs and added safety experts. We further introduce a two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities. To enable flexible control at inference time, we introduce a safety temperature mechanism, allowing dynamic adjustment of the trade-off between safety and utility. Experiments across multiple benchmarks, base model, and model scales demonstrate that UpSafe$^\circ$C achieves robust safety improvements against harmful and jailbreak inputs, while maintaining competitive performance on general tasks. Moreover, analysis shows that safety temperature provides fine-grained inference-time control that achieves the Pareto-optimal frontier between utility and safety. Our results highlight a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.

**Link**: [arxiv](http://arxiv.org/abs/2510.02194v1),  [pdf](http://arxiv.org/pdf/2510.02194v1)

**Tags**: cs.AI cs.CR cs.LG 



### Joint Channel and Semantic-aware Grouping for Effective Collaborative   Edge Inference
**Authors**: Mateus P. Mota, Mattia Merluzzi, Emilio Calvanese Strinati

**Updated**: 2025-10-02T16:40:21Z

**Summary**: We focus on collaborative edge inference over wireless, which enables multiple devices to cooperate to improve inference performance in the presence of corrupted data. Exploiting a key-query mechanism for selective information exchange (or, group formation for collaboration), we recall the effect of wireless channel impairments in feature communication. We argue and show that a disjoint approach, which only considers either the semantic relevance or channel state between devices, performs poorly, especially in harsh propagation conditions. Based on these findings, we propose a joint approach that takes into account semantic information relevance and channel states when grouping devices for collaboration, by making the general attention weights dependent of the channel information. Numerical simulations show the superiority of the joint approach against local inference on corrupted data, as well as compared to collaborative inference with disjoint decisions that either consider application or physical layer parameters when forming groups.

**Link**: [arxiv](http://arxiv.org/abs/2510.02191v1),  [pdf](http://arxiv.org/pdf/2510.02191v1)

**Tags**: cs.IT eess.SP math.IT 



### GeoPurify: A Data-Efficient Geometric Distillation Framework for   Open-Vocabulary 3D Segmentation
**Authors**: Weijia Dou, Xu Zhang, Yi Bin, Jian Liu, Bo Peng, Guoqing Wang, Yang Yang, Heng Tao Shen

**Updated**: 2025-10-02T16:37:56Z

**Summary**: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to 3D semantic segmentation expose a persistent trade-off. Directly projecting 2D features into 3D yields noisy and fragmented predictions, whereas enforcing geometric coherence necessitates costly training pipelines and large-scale annotated 3D data. We argue that this limitation stems from the dominant segmentation-and-matching paradigm, which fails to reconcile 2D semantics with 3D geometric structure. The geometric cues are not eliminated during the 2D-to-3D transfer but remain latent within the noisy and view-aggregated features. To exploit this property, we propose GeoPurify that applies a small Student Affinity Network to purify 2D VLM-generated 3D point features using geometric priors distilled from a 3D self-supervised teacher model. During inference, we devise a Geometry-Guided Pooling module to further denoise the point cloud and ensure the semantic and structural consistency. Benefiting from latent geometric information and the learned affinity network, GeoPurify effectively mitigates the trade-off and achieves superior data efficiency. Extensive experiments on major 3D benchmarks demonstrate that GeoPurify achieves or surpasses state-of-the-art performance while utilizing only about 1.5% of the training data. Our codes and checkpoints are available at [https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

**Link**: [arxiv](http://arxiv.org/abs/2510.02186v1),  [pdf](http://arxiv.org/pdf/2510.02186v1)

**Tags**: cs.CV cs.LG 



### FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen   Using Agentic AI
**Authors**: Paschal C. Amusuo, Dongge Liu, Ricardo Andres Calvo Mendez, Jonathan Metzman, Oliver Chang, James C. Davis

**Updated**: 2025-10-02T16:36:56Z

**Summary**: Fuzz testing has become a cornerstone technique for identifying software bugs and security vulnerabilities, with broad adoption in both industry and open-source communities. Directly fuzzing a function requires fuzz drivers, which translate random fuzzer inputs into valid arguments for the target function. Given the cost and expertise required to manually develop fuzz drivers, methods exist that leverage program analysis and Large Language Models to automatically generate these drivers. However, the generated fuzz drivers frequently lead to false positive crashes, especially in functions highly structured input and complex state requirements. This problem is especially crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as reporting false positive crashes to maintainers impede trust in both the system and the team.   This paper presents two AI-driven strategies to reduce false positives in OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First, constraint-based fuzz driver generation proactively enforces constraints on a function's inputs and state to guide driver creation. Second, context-based crash validation reactively analyzes function callers to determine whether reported crashes are feasible from program entry points. Using 1,500 benchmark functions from OSS-Fuzz, we show that these strategies reduce spurious crashes by up to 8%, cut reported crashes by more than half, and demonstrate that frontier LLMs can serve as reliable program analysis agents. Our results highlight the promise and challenges of integrating AI into large-scale fuzzing pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2510.02185v1),  [pdf](http://arxiv.org/pdf/2510.02185v1)

**Tags**: cs.SE cs.CR cs.MA D.2.4; F.3.1 



### Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward
**Authors**: Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques

**Updated**: 2025-10-02T16:36:08Z

**Summary**: Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.

**Link**: [arxiv](http://arxiv.org/abs/2504.03206v3),  [pdf](http://arxiv.org/pdf/2504.03206v3)

**Tags**: cs.CL cs.AI 



### Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex   with Mutual Information-Guided Diffusion
**Authors**: Yule Wang, Joseph Yu, Chengrui Li, Weihan Li, Anqi Wu

**Updated**: 2025-10-02T16:33:40Z

**Summary**: Understanding how neural populations in higher visual areas encode object-centered visual information remains a central challenge in computational neuroscience. Prior works have investigated representational alignment between artificial neural networks and the visual cortex. Nevertheless, these findings are indirect and offer limited insights to the structure of neural populations themselves. Similarly, decoding-based methods have quantified semantic features from neural populations but have not uncovered their underlying organizations. This leaves open a scientific question: "how feature-specific visual information is distributed across neural populations in higher visual areas, and whether it is organized into structured, semantically meaningful subspaces." To tackle this problem, we present MIG-Vis, a method that leverages the generative power of diffusion models to visualize and validate the visual-semantic attributes encoded in neural latent subspaces. Our method first uses a variational autoencoder to infer a group-wise disentangled neural latent subspace from neural populations. Subsequently, we propose a mutual information (MI)-guided diffusion synthesis procedure to visualize the specific visual-semantic features encoded by each latent group. We validate MIG-Vis on multi-session neural spiking datasets from the inferior temporal (IT) cortex of two macaques. The synthesized results demonstrate that our method identifies neural latent groups with clear semantic selectivity to diverse visual features, including object pose, inter-category transformations, and intra-class content. These findings provide direct, interpretable evidence of structured semantic representation in the higher visual cortex and advance our understanding of its encoding principles.

**Link**: [arxiv](http://arxiv.org/abs/2510.02182v1),  [pdf](http://arxiv.org/pdf/2510.02182v1)

**Tags**: q-bio.NC cs.CV cs.LG 



### DisCo-Layout: Disentangling and Coordinating Semantic and Physical   Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis
**Authors**: Jialin Gao, Donghao Zhou, Mingjian Liang, Lihao Liu, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng

**Updated**: 2025-10-02T16:30:37Z

**Summary**: 3D indoor layout synthesis is crucial for creating virtual environments. Traditional methods struggle with generalization due to fixed datasets. While recent LLM and VLM-based approaches offer improved semantic richness, they often lack robust and flexible refinement, resulting in suboptimal layouts. We develop DisCo-Layout, a novel framework that disentangles and coordinates physical and semantic refinement. For independent refinement, our Semantic Refinement Tool (SRT) corrects abstract object relationships, while the Physical Refinement Tool (PRT) resolves concrete spatial issues via a grid-matching algorithm. For collaborative refinement, a multi-agent framework intelligently orchestrates these tools, featuring a planner for placement rules, a designer for initial layouts, and an evaluator for assessment. Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating realistic, coherent, and generalizable 3D indoor layouts. Our code will be publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2510.02178v1),  [pdf](http://arxiv.org/pdf/2510.02178v1)

**Tags**: cs.RO cs.CV 



### Learning to Reason for Hallucination Span Detection
**Authors**: Hsuan Su, Ting-Yao Hu, Hema Swetha Koppula, Kundan Krishna, Hadi Pouransari, Cheng-Yu Hsieh, Cem Koc, Joseph Yitan Cheng, Oncel Tuzel, Raviteja Vemulapalli

**Updated**: 2025-10-02T16:24:28Z

**Summary**: Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.

**Link**: [arxiv](http://arxiv.org/abs/2510.02173v1),  [pdf](http://arxiv.org/pdf/2510.02173v1)

**Tags**: cs.CL cs.AI cs.LG 



### Superficial Safety Alignment Hypothesis
**Authors**: Jianwei Li, Jung-Eun Kim

**Updated**: 2025-10-02T16:15:20Z

**Summary**: As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe responses is a pressing need. Previous studies on alignment have largely focused on general instruction-following but have often overlooked the distinct properties of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment teaches an otherwise unsafe model to choose the correct reasoning direction - fulfill or refuse users' requests - interpreted as an implicit binary classification task. Through SSAH, we hypothesize that only a few essential components can establish safety guardrails in LLMs. We successfully identify four types of attribute-critical components: Safety Critical Unit (SCU), Utility Critical Unit (UCU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Similarly, we show that leveraging redundant units in the pre-trained model as an "alignment budget" can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated.

**Link**: [arxiv](http://arxiv.org/abs/2410.10862v2),  [pdf](http://arxiv.org/pdf/2410.10862v2)

**Tags**: cs.CL cs.AI cs.CR cs.CY cs.LG 



### CodeSense: a Real-World Benchmark and Dataset for Code Semantic   Reasoning
**Authors**: Monoshi Kumar Roy, Simin Chen, Benjamin Steenhoek, Jinjun Peng, Gail Kaiser, Baishakhi Ray, Wei Le

**Updated**: 2025-10-02T16:10:36Z

**Summary**: Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2506.00750v2),  [pdf](http://arxiv.org/pdf/2506.00750v2)

**Tags**: cs.SE cs.AI 



### Agentic Reasoning and Refinement through Semantic Interaction
**Authors**: Xuxin Tang, Rehema Abulikemu, Eric Krokos, Kirsten Whitley, Xuan Wang, Chris North

**Updated**: 2025-10-02T16:08:51Z

**Summary**: Sensemaking report writing often requires multiple refinements in the iterative process. While Large Language Models (LLMs) have shown promise in generating initial reports based on human visual workspace representations, they struggle to precisely incorporate sequential semantic interactions during the refinement process. We introduce VIS-ReAct, a framework that reasons about newly-added semantic interactions in visual workspaces to steer the LLM for report refinement.   VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning, followed by an LLM refinement agent that updates reports accordingly. Through case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM analysis) on targeted refinement, semantic fidelity, and transparent inference. Results demonstrate that VIS-ReAct better handles various interaction types and granularities while enhancing the transparency of human-LLM collaboration.

**Link**: [arxiv](http://arxiv.org/abs/2510.02157v1),  [pdf](http://arxiv.org/pdf/2510.02157v1)

**Tags**: cs.HC 



### Formation of over-massive black holes in high-redshift disk galaxies via   globular cluster accretion
**Authors**: Hidenobu Yajima

**Updated**: 2025-10-02T16:05:48Z

**Summary**: Recent observations with the James Webb Space Telescope (JWST) have suggested the existence of over-massive black holes (OMBHs) in high-redshift galaxies. In this paper, we propose a new mechanism for the formation of OMBHs, based on the accretion of globular clusters (GCs) in compact disk galaxies. We derive the conditions under which OMBHs can form, focusing on key parameters such as halo mass, redshift, and halo spin parameter. Our results show that at redshift $z = 10$, a halo with mass $10^{11}~M_{\odot}$ and a spin parameter of $\sim 0.02$ can form a black hole of $2.3 \times 10^{8}~M_{\odot}$ through GC migration and accretion via tidal disruption events (TDEs). The resulting black hole-to-stellar mass ratio can reach $\sim 0.1$, corresponding to the fraction of GC mass accreted onto the black hole. This mechanism thus provides a plausible explanation for the OMBHs observed by JWST. Furthermore, by combining our model with the halo mass function and the spin-parameter distribution, we construct black hole mass functions that reproduce the number densities of the massive BH candidates UHZ1 and GHZ9 at $z \approx 10$, as well as the abundances of BHs with masses $\gtrsim 10^{8}~\rm{M_\odot}$ at $z \approx 5$ inferred from JWST observations. However, our model overpredicts the abundance of BHs with masses $ < 10^{8}~\rm{M_\odot}$, suggesting that moderately massive, inactive BHs are more frequent.

**Link**: [arxiv](http://arxiv.org/abs/2507.22863v2),  [pdf](http://arxiv.org/pdf/2507.22863v2)

**Tags**: astro-ph.GA 



### Multivariate distributional modeling of low, moderate, and large   intensities without threshold selection steps
**Authors**: Carlo Gaetan, Philippe Naveau

**Updated**: 2025-10-02T16:04:28Z

**Summary**: In fields such as hydrology and climatology, modelling the entire distribution of positive data is essential, as stakeholders require insights into the full range of values, from low to extreme. Traditional approaches often segment the distribution into separate regions, which introduces subjectivity and limits coherence. This is especially true when dealing with multivariate data.   In line with multivariate extreme value theory, this paper presents a unified, threshold-free framework for modelling marginal behaviours and dependence structures based on an extended generalized Pareto distribution (EGPD). We propose decomposing multivariate data into radial and angular components. The radial component is modelled using a semi-parametric EGPD and the angular distribution is permitted to vary conditionally. This approach allows for sufficiently flexible dependence modelling.   The hierarchical structure of the model facilitates the inference process. First, we combine classical maximum likelihood estimation (MLE) methods with semi-parametric approaches based on Bernstein polynomials to estimate the distribution of the radial component. Then, we use multivariate regression techniques to estimate the angular component's parameters.   The model is evaluated through synthetic simulations and applied to hydrological datasets to exemplify its capacity to capture heavy-tailed marginals and complex multivariate dependencies without threshold specification.

**Link**: [arxiv](http://arxiv.org/abs/2510.02152v1),  [pdf](http://arxiv.org/pdf/2510.02152v1)

**Tags**: stat.ME stat.AP 



### When Disagreements Elicit Robustness: Investigating Self-Repair   Capabilities under LLM Multi-Agent Disagreements
**Authors**: Tianjie Ju, Bowen Wang, Hao Fei, Mong-Li Lee, Wynne Hsu, Yun Li, Qianren Wang, Pengzhou Cheng, Zongru Wu, Haodong Zhao, Zhuosheng Zhang, Gongshen Liu

**Updated**: 2025-10-02T15:55:21Z

**Summary**: Recent advances in Large Language Models (LLMs) have upgraded them from sophisticated text generators to autonomous agents capable of cooperation and tool use in multi-agent systems (MAS). However, it remains unclear how disagreements shape collective decision-making. In this paper, we revisit the role of disagreement and argue that general, partially overlapping disagreements prevent premature consensus and expand the explored solution space, while disagreements on task-critical steps can derail collaboration depending on the topology of solution paths. We investigate two collaborative settings with distinct path structures: collaborative reasoning (CounterFact, MQuAKE-cf), which typically follows a single evidential chain, whereas collaborative programming (HumanEval, GAIA) often adopts multiple valid implementations. Disagreements are instantiated as general heterogeneity among agents and as task-critical counterfactual knowledge edits injected into context or parameters. Experiments reveal that general disagreements consistently improve success by encouraging complementary exploration. By contrast, task-critical disagreements substantially reduce success on single-path reasoning, yet have a limited impact on programming, where agents can choose alternative solutions. Trace analyses show that MAS frequently bypasses the edited facts in programming but rarely does so in reasoning, revealing an emergent self-repair capability that depends on solution-path rather than scale alone. Our code is available at https://github.com/wbw625/MultiAgentRobustness.

**Link**: [arxiv](http://arxiv.org/abs/2502.15153v2),  [pdf](http://arxiv.org/pdf/2502.15153v2)

**Tags**: cs.CL 



### MissionHD: Hyperdimensional Refinement of Distribution-Deficient   Reasoning Graphs for Video Anomaly Detection
**Authors**: Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Nathaniel D. Bastian, Mohsen Imani

**Updated**: 2025-10-02T15:55:20Z

**Summary**: LLM-generated reasoning graphs, referred to as mission-specific graphs (MSGs), are increasingly used for video anomaly detection (VAD) and recognition (VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity and lack large-scale datasets for pre-training, which makes existing graph structure refinement (GSR) methods ineffective. To address this challenge, we propose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that leverages hyperdimensional computing (HDC) to optimize decodable graph representations without relying on structural-distribution learning. Building on this paradigm, we introduce MissionHD, an HDC framework that encodes graphs with constrained graph-neural operations, aligns them directly with downstream task loss, and decodes refined structures. Experiments on VAD/VAR benchmarks demonstrate that MissionHD-refined graphs consistently improve performance, establishing HDC-GSR as an effective pre-processing step for structured reasoning in video anomaly tasks.

**Link**: [arxiv](http://arxiv.org/abs/2508.14746v3),  [pdf](http://arxiv.org/pdf/2508.14746v3)

**Tags**: cs.LG 



### The Disparate Impacts of Speculative Decoding
**Authors**: Jameson Sandler, Ahmet ÃstÃ¼n, Marco Romanelli, Sara Hooker, Ferdinando Fioretto

**Updated**: 2025-10-02T15:38:57Z

**Summary**: The practice of speculative decoding, whereby inference is probabilistically supported by a smaller, cheaper, ``drafter'' model, has become a standard technique for systematically reducing the decoding time of large language models. This paper conducts an analysis of speculative decoding through the lens of its potential disparate speed-up rates across tasks. Crucially, the paper shows that speed-up gained from speculative decoding is not uniformly distributed across tasks, consistently diminishing for under-fit, and often underrepresented tasks. To better understand this phenomenon, we derive an analysis to quantify this observed ``unfairness'' and draw attention to the factors that motivate such disparate speed-ups to emerge. Further, guided by these insights, the paper proposes a mitigation strategy designed to reduce speed-up disparities and validates the approach across several model pairs, revealing on average a 12% improvement in our fairness metric.

**Link**: [arxiv](http://arxiv.org/abs/2510.02128v1),  [pdf](http://arxiv.org/pdf/2510.02128v1)

**Tags**: cs.CL cs.AI 



### Identifying Subgroup and Context Effects in Conjoint Experiments
**Authors**: Steven Wang, Isys Johnson, Jessica Grogan, Lalit Jain, Atri Rudra, Kyle Hunt, Kenneth Joseph

**Updated**: 2025-10-02T15:34:41Z

**Summary**: Conjoint experiments have become central to survey research in political science and related fields because they allow researchers to study preferences across multiple attributes simultaneously. Beyond estimating main effects, scholars increasingly analyze heterogeneity through subgroup analysis and contextual variables, raising methodological challenges in detecting and interpreting interaction effects. Statistical power constraints, common in survey experiments, further complicate this task. This paper addresses the question: how can both main and interaction effects be reliably inferred in conjoint studies? We contribute in two ways. First, we conduct a systematic evaluation of leading approaches, including post-hoc corrections, sparse regression methods, and Bayesian models, across simulation regimes that vary sparsity, noise, and data availability. Second, we propose a novel black-box inference framework that leverages machine learning to recover main and interaction effects in conjoint experiments. Our approach balances computational efficiency with accuracy, providing a practical tool for researchers studying heterogeneous effects.

**Link**: [arxiv](http://arxiv.org/abs/2510.02123v1),  [pdf](http://arxiv.org/pdf/2510.02123v1)

**Tags**: stat.ME 



### Eccentric binary black holes: A new framework for numerical relativity   waveform surrogates
**Authors**: Peter James Nee, Adhrit Ravichandran, Scott E. Field, Tousif Islam, Harald P. Pfeiffer, Vijay Varma, Michael Boyle, Andrea Ceja, Noora Ghadiri, Lawrence E. Kidder, Prayush Kumar, Akash Maurya, Marlo Morales, Antoni Ramos-Buades, Abhishek Ravishankar, Katie Rink, Hannes R. RÃ¼ter, Mark A. Scheel, Md Arif Shaikh, Daniel Tellez

**Updated**: 2025-10-02T15:29:23Z

**Summary**: Mounting evidence indicates that some of the gravitational wave signals observed by the LIGO/Virgo/KAGRA observatories might arise from eccentric compact object binaries, increasing the urgency for accurate waveform models for such systems. While for non-eccentric binaries, surrogate models are efficient and accurate, the additional features due to eccentricity have posed a challenge. In this letter, we present a novel method for decomposing eccentric numerical relativity waveforms which makes them amenable to surrogate modelling techniques. We parameterize the inspiral in the radial phase domain, factoring out eccentricity-induced dephasing and thus enhancing compressibility and accuracy. This is combined with a second surrogate for the merger-ringdown in the time-domain and a novel technique to take advantage of the approximate periodicity with radial oscillations during the inspiral. We apply this procedure to the $(2,2)$ mode for non-spinning black hole binaries, and demonstrate that the resulting surrogate, NRSurE_q4NoSpin_22, is able to faithfully reproduce the underlying numerical relativity waveforms, with maximum mismatches of $5\times10^{-4}$ and median mismatches of $2\times10^{-5}$. This technique paves the way for high-accuracy parameter estimation with eccentric models, a key ingredient for astrophysical inference and tests of general relativity.

**Link**: [arxiv](http://arxiv.org/abs/2510.00106v2),  [pdf](http://arxiv.org/pdf/2510.00106v2)

**Tags**: gr-qc 



### REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking
**Authors**: Pinhuan Wang, Zhiqiu Xia, Chunhua Liao, Feiyi Wang, Hang Liu

**Updated**: 2025-10-02T15:20:02Z

**Summary**: Large Language Models (LLMs) have shown strong capabilities in document re-ranking, a key component in modern Information Retrieval (IR) systems. However, existing LLM-based approaches face notable limitations, including ranking uncertainty, unstable top-k recovery, and high token cost due to token-intensive prompting. To effectively address these limitations, we propose REALM, an uncertainty-aware re-ranking framework that models LLM-derived relevance as Gaussian distributions and refines them through recursive Bayesian updates. By explicitly capturing uncertainty and minimizing redundant queries, REALM achieves better rankings more efficiently. Experimental results demonstrate that our REALM surpasses state-of-the-art re-rankers while significantly reducing token usage and latency, improving NDCG@10 by 0.7-11.9 and simultaneously reducing the number of LLM inferences by 23.4-84.4%, promoting it as the next-generation re-ranker for modern IR systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.18379v2),  [pdf](http://arxiv.org/pdf/2508.18379v2)

**Tags**: cs.IR 



### Scaling Laws for Optimal Data Mixtures
**Authors**: Mustafa Shukor, Louis Bethune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, Pierre Ablin

**Updated**: 2025-10-02T15:18:02Z

**Summary**: Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. We propose a systematic method to determine the optimal data mixture for any target domain using scaling laws. Our approach accurately predicts the loss of a model of size $N$ trained with $D$ tokens and a specific domain weight vector $h$. We validate the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. We further show that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget ($N$,$D$), providing a principled alternative to costly trial-and-error methods.

**Link**: [arxiv](http://arxiv.org/abs/2507.09404v2),  [pdf](http://arxiv.org/pdf/2507.09404v2)

**Tags**: cs.LG 



### On Predictability of Reinforcement Learning Dynamics for Large Language   Models
**Authors**: Yuchen Cai, Ding Cao, Xin Xu, Zijun Yao, Yuqing Huang, Zhenyu Tan, Benyi Zhang, Guiquan Liu, Junfeng Fang

**Updated**: 2025-10-02T15:16:51Z

**Summary**: Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \textgreater 96\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.00553v2),  [pdf](http://arxiv.org/pdf/2510.00553v2)

**Tags**: cs.LG cs.AI 



### Unlocking Symbol-Level Precoding Efficiency Through Tensor Equivariant   Neural Network
**Authors**: Jinshuo Zhang, Yafei Wang, Xinping Yi, Wenjin Wang, Shi Jin, Symeon Chatzinotas, BjÃ¶rn Ottersten

**Updated**: 2025-10-02T15:15:50Z

**Summary**: Although symbol-level precoding (SLP) based on constructive interference (CI) exploitation offers performance gains, its high complexity remains a bottleneck. This paper addresses this challenge with an end-to-end deep learning (DL) framework with low inference complexity that leverages the structure of the optimal SLP solution in the closed-form and its inherent tensor equivariance (TE), where TE denotes that a permutation of the input induces the corresponding permutation of the output. Building upon the computationally efficient model-based formulations, as well as their known closed-form solutions, we analyze their relationship with linear precoding (LP) and investigate the corresponding optimality condition. We then construct a mapping from the problem formulation to the solution and prove its TE, based on which the designed networks reveal a specific parameter-sharing pattern that delivers low computational complexity and strong generalization. Leveraging these, we propose the backbone of the framework with an attention-based TE module, achieving linear computational complexity. Furthermore, we demonstrate that such a framework is also applicable to imperfect CSI scenarios, where we design a TE-based network to map the CSI, statistics, and symbols to auxiliary variables. Simulation results show that the proposed framework captures substantial performance gains of optimal SLP, while achieving an approximately 80-times speedup over conventional methods and maintaining strong generalization across user numbers and symbol block lengths.

**Link**: [arxiv](http://arxiv.org/abs/2510.02108v1),  [pdf](http://arxiv.org/pdf/2510.02108v1)

**Tags**: eess.SP cs.AI 



### LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot   Grasping with Ambiguous Instructions
**Authors**: Yunhan Lin, Wenqi Wu, Zhijie Zhang, Huasong Min

**Updated**: 2025-10-02T15:10:26Z

**Summary**: The existing language-driven grasping methods struggle to fully handle ambiguous instructions containing implicit intents. To tackle this challenge, we propose LangGrasp, a novel language-interactive robotic grasping framework. The framework integrates fine-tuned large language models (LLMs) to leverage their robust commonsense understanding and environmental perception capabilities, thereby deducing implicit intents from linguistic instructions and clarifying task requirements along with target manipulation objects. Furthermore, our designed point cloud localization module, guided by 2D part segmentation, enables partial point cloud localization in scenes, thereby extending grasping operations from coarse-grained object-level to fine-grained part-level manipulation. Experimental results show that the LangGrasp framework accurately resolves implicit intents in ambiguous instructions, identifying critical operations and target information that are unstated yet essential for task completion. Additionally, it dynamically selects optimal grasping poses by integrating environmental information. This enables high-precision grasping from object-level to part-level manipulation, significantly enhancing the adaptability and task execution efficiency of robots in unstructured environments. More information and code are available here: https://github.com/wu467/LangGrasp.

**Link**: [arxiv](http://arxiv.org/abs/2510.02104v1),  [pdf](http://arxiv.org/pdf/2510.02104v1)

**Tags**: cs.RO 



### Neural Network Parameter-optimization of Gaussian pmDAGs
**Authors**: Mehrzad Saremi

**Updated**: 2025-10-02T15:06:38Z

**Summary**: Finding the parameters of a latent variable causal model is central to causal inference and causal identification. In this article, we show that existing graphical structures that are used in causal inference are not stable under marginalization of Gaussian Bayesian networks, and present a graphical structure that faithfully represent margins of Gaussian Bayesian networks. We present the first duality between parameter optimization of a latent variable model and training a feed-forward neural network in the parameter space of the assumed family of distributions. Based on this observation, we develop an algorithm for parameter optimization of these graphical structures based on a given observational distribution. Then, we provide conditions for causal effect identifiability in the Gaussian setting. We propose an meta-algorithm that checks whether a causal effect is identifiable or not. Moreover, we lay a grounding for generalizing the duality between a neural network and a causal model from the Gaussian to other distributions.

**Link**: [arxiv](http://arxiv.org/abs/2309.14073v4),  [pdf](http://arxiv.org/pdf/2309.14073v4)

**Tags**: stat.ML cs.AI cs.LG math.PR 



### Multiplier-free In-Memory Vector-Matrix Multiplication Using Distributed   Arithmetic
**Authors**: Felix Zeller, John Reuben, Dietmar Fey

**Updated**: 2025-10-02T15:06:00Z

**Summary**: Vector-Matrix Multiplication (VMM) is the fundamental and frequently required computation in inference of Neural Networks (NN). Due to the large data movement required during inference, VMM can benefit greatly from in-memory computing. However, ADC/DACs required for in-memory VMM consume significant power and area. `Distributed Arithmetic (DA)', a technique in computer architecture prevalent in 1980s was used to achieve inner product or dot product of two vectors without using a hard-wired multiplier when one of the vectors is a constant. In this work, we extend the DA technique to multiply an input vector with a constant matrix. By storing the sum of the weights in memory, DA achieves VMM using shift-and-add circuits in the periphery of ReRAM memory. We verify functional and also estimate non-functional properties (latency, energy, area) by performing transistor-level simulations. Using energy-efficient sensing and fine grained pipelining, our approach achieves 4.5 x less latency and 12 x less energy than VMM performed in memory conventionally by bit slicing. Furthermore, DA completely eliminated the need for power-hungry ADCs which are the main source of area and energy consumption in the current VMM implementations in memory.

**Link**: [arxiv](http://arxiv.org/abs/2510.02099v1),  [pdf](http://arxiv.org/pdf/2510.02099v1)

**Tags**: cs.AR cs.NE B.3; B.7; I.4 



### Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and   Reasoning
**Authors**: Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu

**Updated**: 2025-10-02T14:57:13Z

**Summary**: Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.

**Link**: [arxiv](http://arxiv.org/abs/2510.02091v1),  [pdf](http://arxiv.org/pdf/2510.02091v1)

**Tags**: cs.AI 



### Break the ID-Language Barrier: An Adaption Framework for LLM-based   Sequential Recommendation
**Authors**: Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang

**Updated**: 2025-10-02T14:53:04Z

**Summary**: The recent breakthrough of large language models (LLMs) in natural language processing has sparked exploration in recommendation systems, however, their limited domain-specific knowledge remains a critical bottleneck. Specifically, LLMs lack key pieces of information crucial for sequential recommendations, such as user behavior patterns. To address this critical gap, we propose IDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich in domain-specific knowledge, into LLMs to improve recommendation accuracy. IDLE-Adapter acts as a bridge, transforming sparse user-item interaction data into dense, LLM-compatible representations through a Pre-trained ID Sequential Model, Dimensionality Alignment, Layer-wise Embedding Refinement, and Layer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates remarkable flexibility by seamlessly integrating ID embeddings from diverse ID-based sequential models and LLM architectures. Extensive experiments across various datasets demonstrate the superiority of IDLE-Adapter, achieving over 10\% and 20\% improvements in HitRate@5 and NDCG@5 metrics, respectively, compared to state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.18262v3),  [pdf](http://arxiv.org/pdf/2411.18262v3)

**Tags**: cs.IR cs.LG 



### KAIROS: Unified Training for Universal Non-Autoregressive Time Series   Forecasting
**Authors**: Kuiye Ding, Fanda Fan, Zheya Wang, Hongxiao Li, Yifan Wang, Lei Wang, Chunjie Luo, Jianfeng Zhan

**Updated**: 2025-10-02T14:50:50Z

**Summary**: In the World Wide Web, reliable time series forecasts provide the forward-looking signals that drive resource planning, cache placement, and anomaly response, enabling platforms to operate efficiently as user behavior and content distributions evolve. Compared with other domains, time series forecasting for Web applications requires much faster responsiveness to support real-time decision making. We present KAIROS, a non-autoregressive time series forecasting framework that directly models segment-level multi-peak distributions. Unlike autoregressive approaches, KAIROS avoids error accumulation and achieves just-in-time inference, while improving over existing non-autoregressive models that collapse to over-smoothed predictions. Trained on the large-scale corpus, KAIROS demonstrates strong zero-shot generalization on six widely used benchmarks, delivering forecasting performance comparable to state-of-the-art foundation models with similar scale, at a fraction of their inference cost. Beyond empirical results, KAIROS highlights the importance of non-autoregressive design as a scalable paradigm for foundation models in time series.

**Link**: [arxiv](http://arxiv.org/abs/2510.02084v1),  [pdf](http://arxiv.org/pdf/2510.02084v1)

**Tags**: cs.LG cs.AI 



### Fine-Tuning Flow Matching via Maximum Likelihood Estimation of   Reconstructions
**Authors**: Zhaoyi Li, Jingtao Ding, Yong Li, Shihua Li

**Updated**: 2025-10-02T14:49:47Z

**Summary**: Flow Matching (FM) algorithm achieves remarkable results in generative tasks especially in robotic manipulation. Building upon the foundations of diffusion models, the simulation-free paradigm of FM enables simple and efficient training, but inherently introduces a train-inference gap. Specifically, we cannot assess the model's output during the training phase. In contrast, other generative models including Variational Autoencoder (VAE), Normalizing Flow and Generative Adversarial Networks (GANs) directly optimize on the reconstruction loss. Such a gap is particularly evident in scenarios that demand high precision, such as robotic manipulation. Moreover, we show that FM's over-pursuit of straight predefined paths may introduce some serious problems such as stiffness into the system. These motivate us to fine-tune FM via Maximum Likelihood Estimation of reconstructions - an approach made feasible by FM's underlying smooth ODE formulation, in contrast to the stochastic differential equations (SDEs) used in diffusion models. This paper first theoretically analyzes the relation between training loss and inference error in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood Estimation of reconstructions, which includes both straightforward fine-tuning and residual-based fine-tuning approaches. Furthermore, through specifically designed architectures, the residual-based fine-tuning can incorporate the contraction property into the model, which is crucial for the model's robustness and interpretability. Experimental results in image generation and robotic manipulation verify that our method reliably improves the inference performance of FM.

**Link**: [arxiv](http://arxiv.org/abs/2510.02081v1),  [pdf](http://arxiv.org/pdf/2510.02081v1)

**Tags**: cs.LG 



### TrimTokenator: Towards Adaptive Visual Token Pruning for Large   Multimodal Models
**Authors**: Hao Zhang, Mengsi Lyu, Chenrui He, Yulong Ao, Yonghua Lin

**Updated**: 2025-10-02T14:46:26Z

**Summary**: Large Multimodal Models (LMMs) have achieved significant success across various tasks. These models usually encode visual inputs into dense token sequences, which are then concatenated with textual tokens and jointly processed by a language model. However, the increased token count substantially raises computational and memory costs during inference. Token pruning has emerged as a promising approach to address this issue. Existing token pruning methods often rely on costly calibration or suboptimal importance metrics, leading to redundant retained tokens. In this paper, we analyze the redundancy differences between visual and textual tokens and propose pruning exclusively on visual tokens. Based on this, we propose a visual token pruning strategy that explicitly preserves both cross-modal alignment and intra-modal informational diversity. We introduce a mutual information-based token pruning strategy that removes visual tokens semantically misaligned with textual tokens, effectively preserving the alignment between the visual and textual modalities. To further improve the representational quality of the retained tokens, we additionally prune redundant visual tokens by maximizing the expected pairwise distances in the embedding space, which is solved efficiently with a greedy algorithm. Extensive experiments demonstrate that our method maintains strong performance while reducing tokens by 88.9% on models such as LLaVA-1.5-7B and LLaVA-NEXT-7B, resulting in a 56.7% improvement in inference speed.

**Link**: [arxiv](http://arxiv.org/abs/2509.00320v2),  [pdf](http://arxiv.org/pdf/2509.00320v2)

**Tags**: cs.CV 



### Inferring Optical Tissue Properties from Photoplethysmography using   Hybrid Amortized Inference
**Authors**: Jens Behrmann, Maria R. Cervera, Antoine Wehenkel, Andrew C. Miller, Albert Cerussi, Pranay Jain, Vivek Venugopal, Shijie Yan, Guillermo Sapiro, Luca Pegolotti, JÃ¶rn-Henrik Jacobsen

**Updated**: 2025-10-02T14:36:02Z

**Summary**: Smart wearables enable continuous tracking of established biomarkers such as heart rate, heart rate variability, and blood oxygen saturation via photoplethysmography (PPG). Beyond these metrics, PPG waveforms contain richer physiological information, as recent deep learning (DL) studies demonstrate. However, DL models often rely on features with unclear physiological meaning, creating a tension between predictive power, clinical interpretability, and sensor design. We address this gap by introducing PPGen, a biophysical model that relates PPG signals to interpretable physiological and optical parameters. Building on PPGen, we propose hybrid amortized inference (HAI), enabling fast, robust, and scalable estimation of relevant physiological parameters from PPG signals while correcting for model misspecification. In extensive in-silico experiments, we show that HAI can accurately infer physiological parameters under diverse noise and sensor conditions. Our results illustrate a path toward PPG models that retain the fidelity needed for DL-based features while supporting clinical interpretation and informed hardware design.

**Link**: [arxiv](http://arxiv.org/abs/2510.02073v1),  [pdf](http://arxiv.org/pdf/2510.02073v1)

**Tags**: cs.LG physics.bio-ph stat.ML 



### Adaptive Kernel Selection for Stein Variational Gradient Descent
**Authors**: Moritz Melcher, Simon Weissmann, Ashia C. Wilson, Jakob Zech

**Updated**: 2025-10-02T14:33:57Z

**Summary**: A central challenge in Bayesian inference is efficiently approximating posterior distributions. Stein Variational Gradient Descent (SVGD) is a popular variational inference method which transports a set of particles to approximate a target distribution. The SVGD dynamics are governed by a reproducing kernel Hilbert space (RKHS) and are highly sensitive to the choice of the kernel function, which directly influences both convergence and approximation quality. The commonly used median heuristic offers a simple approach for setting kernel bandwidths but lacks flexibility and often performs poorly, particularly in high-dimensional settings. In this work, we propose an alternative strategy for adaptively choosing kernel parameters over an abstract family of kernels. Recent convergence analyses based on the kernelized Stein discrepancy (KSD) suggest that optimizing the kernel parameters by maximizing the KSD can improve performance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a method that alternates between updating the particles via SVGD and adaptively tuning kernel bandwidths through gradient ascent on the KSD. We provide a simplified theoretical analysis that extends existing results on minimizing the KSD for fixed kernels to our adaptive setting, showing convergence properties for the maximal KSD over our kernel class. Our empirical results further support this intuition: Ad-SVGD consistently outperforms standard heuristics in a variety of tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.02067v1),  [pdf](http://arxiv.org/pdf/2510.02067v1)

**Tags**: stat.ML cs.LG 



### MSRepaint: Multiple Sclerosis Repaint with Conditional Denoising   Diffusion Implicit Model for Bidirectional Lesion Filling and Synthesis
**Authors**: Jinwei Zhang, Lianrui Zuo, Yihao Liu, Hang Zhang, Samuel W. Remedios, Bennett A. Landman, Peter A. Calabresi, Shiv Saidha, Scott D. Newsome, Dzung L. Pham, Jerry L. Prince, Ellen M. Mowry, Aaron Carass

**Updated**: 2025-10-02T14:30:46Z

**Summary**: In multiple sclerosis, lesions interfere with automated magnetic resonance imaging analyses such as brain parcellation and deformable registration, while lesion segmentation models are hindered by the limited availability of annotated training data. To address both issues, we propose MSRepaint, a unified diffusion-based generative model for bidirectional lesion filling and synthesis that restores anatomical continuity for downstream analyses and augments segmentation through realistic data generation. MSRepaint conditions on spatial lesion masks for voxel-level control, incorporates contrast dropout to handle missing inputs, integrates a repainting mechanism to preserve surrounding anatomy during lesion filling and synthesis, and employs a multi-view DDIM inversion and fusion pipeline for 3D consistency with fast inference. Extensive evaluations demonstrate the effectiveness of MSRepaint across multiple tasks. For lesion filling, we evaluate both the accuracy within the filled regions and the impact on downstream tasks including brain parcellation and deformable registration. MSRepaint outperforms the traditional lesion filling methods FSL and NiftySeg, and achieves accuracy on par with FastSurfer-LIT, a recent diffusion model-based inpainting method, while offering over 20 times faster inference. For lesion synthesis, state-of-the-art MS lesion segmentation models trained on MSRepaint-synthesized data outperform those trained on CarveMix-synthesized data or real ISBI challenge training data across multiple benchmarks, including the MICCAI 2016 and UMCL datasets. Additionally, we demonstrate that MSRepaint's unified bidirectional filling and synthesis capability, with full spatial control over lesion appearance, enables high-fidelity simulation of lesion evolution in longitudinal MS progression.

**Link**: [arxiv](http://arxiv.org/abs/2510.02063v1),  [pdf](http://arxiv.org/pdf/2510.02063v1)

**Tags**: eess.IV 



### ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly   Detection
**Authors**: Sanghyu Yoon, Dongmin Kim, Suhee Yoon, Ye Seul Sim, Seungdong Yoa, Hye-Seung Cho, Soonyoung Lee, Hankook Lee, Woohyung Lim

**Updated**: 2025-10-02T14:28:45Z

**Summary**: In tabular anomaly detection (AD), textual semantics often carry critical signals, as the definition of an anomaly is closely tied to domain-specific context. However, existing benchmarks provide only raw data points without semantic context, overlooking rich textual metadata such as feature descriptions and domain knowledge that experts rely on in practice. This limitation restricts research flexibility and prevents models from fully leveraging domain knowledge for detection. ReTabAD addresses this gap by restoring textual semantics to enable context-aware tabular AD research. We provide (1) 20 carefully curated tabular datasets enriched with structured textual metadata, together with implementations of state-of-the-art AD algorithms including classical, deep learning, and LLM-based approaches, and (2) a zero-shot LLM framework that leverages semantic context without task-specific training, establishing a strong baseline for future research. Furthermore, this work provides insights into the role and utility of textual metadata in AD through experiments and analysis. Results show that semantic context improves detection performance and enhances interpretability by supporting domain-aware reasoning. These findings establish ReTabAD as a benchmark for systematic exploration of context-aware AD.

**Link**: [arxiv](http://arxiv.org/abs/2510.02060v1),  [pdf](http://arxiv.org/pdf/2510.02060v1)

**Tags**: cs.AI cs.LG 



### Adaptive Heterogeneous Mixtures of Normalising Flows for Robust   Variational Inference
**Authors**: Benjamin Wiriyapong, Oktay KarakuÅ, Kirill Sidorov

**Updated**: 2025-10-02T14:25:29Z

**Summary**: Normalising-flow variational inference (VI) can approximate complex posteriors, yet single-flow models often behave inconsistently across qualitatively different distributions. We propose Adaptive Mixture Flow Variational Inference (AMF-VI), a heterogeneous mixture of complementary flows (MAF, RealNVP, RBIG) trained in two stages: (i) sequential expert training of individual flows, and (ii) adaptive global weight estimation via likelihood-driven updates, without per-sample gating or architectural changes. Evaluated on six canonical posterior families of banana, X-shape, two-moons, rings, a bimodal, and a five-mode mixture, AMF-VI achieves consistently lower negative log-likelihood than each single-flow baseline and delivers stable gains in transport metrics (Wasserstein-2) and maximum mean discrepancy (MDD), indicating improved robustness across shapes and modalities. The procedure is efficient and architecture-agnostic, incurring minimal overhead relative to standard flow training, and demonstrates that adaptive mixtures of diverse flows provide a reliable route to robust VI across diverse posterior families whilst preserving each expert's inductive bias.

**Link**: [arxiv](http://arxiv.org/abs/2510.02056v1),  [pdf](http://arxiv.org/pdf/2510.02056v1)

**Tags**: cs.LG stat.ML 



### LLM-Enhanced, Data-Driven Personalized and Equitable Clinician   Scheduling: A Predict-then-Optimize Approach
**Authors**: Anjali Jha, Wanqing Chen, Maxim Eckmann, Ian Stockwell, Jianwu Wang, Kai Sun

**Updated**: 2025-10-02T14:22:20Z

**Summary**: Clinician scheduling remains a persistent challenge due to limited clinical resources and fluctuating demands. This complexity is especially acute in large academic anesthesiology departments as physicians balance responsibilities across multiple clinical sites with conflicting priorities. Further, scheduling must account for individual clinical and lifestyle preferences to ensure job satisfaction and well-being. Traditional approaches, often based on statistical or rule-based optimization models, rely on structured data and explicit domain knowledge. However, these methods often overlook unstructured information, e.g., free-text notes from routinely administered clinician well-being surveys and scheduling platforms. These notes may reveal implicit and underutilized clinical resources. Neglecting such information can lead to misaligned schedules, increased burnout, overlooked staffing flexibility, and suboptimal utilization of available resources. To address this gap, we propose a predict-then-optimize framework that integrates classification-based clinician availability predictions with a mixed-integer programming schedule optimization model. Large language models (LLMs) are employed to extract actionable preferences and implicit constraints from unstructured schedule notes, enhancing the reliability of availability predictions. These predictions then inform the schedule optimization considering four objectives: first, ensuring clinical full-time equivalent compliance, second, reducing workload imbalances by enforcing equitable proportions of shift types, third, maximizing clinician availability for assigned shifts, and fourth, schedule consistency. By combining the interpretive power of LLMs with the rigor of mathematical optimization, our framework provides a robust, data-driven solution that enhances operational efficiency while supporting equity and clinician well-being.

**Link**: [arxiv](http://arxiv.org/abs/2510.02047v1),  [pdf](http://arxiv.org/pdf/2510.02047v1)

**Tags**: math.OC cs.CE cs.SY eess.SY 



### Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming   Tool Usage
**Authors**: Siddhant Arora, Haidar Khan, Kai Sun, Xin Luna Dong, Sajal Choudhary, Seungwhan Moon, Xinyuan Zhang, Adithya Sagar, Surya Teja Appini, Kaushik Patnaik, Sanat Sharma, Shinji Watanabe, Anuj Kumar, Ahmed Aly, Yue Liu, Florian Metze, Zhaojiang Lin

**Updated**: 2025-10-02T14:18:20Z

**Summary**: End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR-LLM-TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (Streaming RAG), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to issue tool calls during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both accuracy and responsiveness. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our streaming RAG approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our streaming RAG approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.

**Link**: [arxiv](http://arxiv.org/abs/2510.02044v1),  [pdf](http://arxiv.org/pdf/2510.02044v1)

**Tags**: cs.CL cs.SD eess.AS 



### Large Language Models Inference Engines based on Spiking Neural Networks
**Authors**: Adarsha Balaji, Sandeep Madireddy

**Updated**: 2025-10-02T14:15:41Z

**Summary**: Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computationally challenging as the time and space complexity has a quadratic relation to the input sequence length. Several efforts exploring efficient computational paradigms and model architectures to address these limitations have been made. In this work, we explore spiking neural networks (SNNs) to design transformer models. A challenge in training large-scale SNNs, using existing surrogate learning methods is inefficient and time-consuming. On the other hand, techniques to convert existing transformer-based models to their SNN equivalent are not scalable, as achieving optimal performance comes at the cost of a large number of spike time-steps, i.e. increased latency. To address this, we propose NeurTransformer, a methodology for designing transformer-based SNN for inference using a supervised fine-tuning approach with existing conversion methods. The proposed methodology works by: (1) replacing the self-attention mechanism with a spike-based self-attention (SSA), (2) converting the feed-forward block of the trained transformer model to its equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate learning algorithms. We benchmark the proposed methodology and demonstrate its accuracy and scalability using three variants of the GPT-2 model of increasing model size. We observe that the converted GPT-2 small models demonstrate a 5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we demonstrate the energy efficiency of the SSA block compared to the ASA block and show between 64.71% and 85.28% reductions in estimated energy consumption when implementing the self-attention mechanism on a digital hardware.

**Link**: [arxiv](http://arxiv.org/abs/2510.00133v2),  [pdf](http://arxiv.org/pdf/2510.00133v2)

**Tags**: cs.LG 



### Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation   and Confidence Awareness
**Authors**: Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li

**Updated**: 2025-10-02T14:15:09Z

**Summary**: Machine unlearning techniques aim to mitigate unintended memorization in large language models (LLMs). However, existing approaches predominantly focus on the explicit removal of isolated facts, often overlooking latent inferential dependencies and the non-deterministic nature of knowledge within LLMs. Consequently, facts presumed forgotten may persist implicitly through correlated information. To address these challenges, we propose a knowledge unlearning evaluation framework that more accurately captures the implicit structure of real-world knowledge by representing relevant factual contexts as knowledge graphs with associated confidence scores. We further develop an inference-based evaluation protocol leveraging powerful LLMs as judges; these judges reason over the extracted knowledge subgraph to determine unlearning success. Our LLM judges utilize carefully designed prompts and are calibrated against human evaluations to ensure their trustworthiness and stability. Extensive experiments on our newly constructed benchmark demonstrate that our framework provides a more realistic and rigorous assessment of unlearning performance. Moreover, our findings reveal that current evaluation strategies tend to overestimate unlearning effectiveness. Our code is publicly available at https://github.com/Graph-COM/Knowledge_Unlearning.git.

**Link**: [arxiv](http://arxiv.org/abs/2506.05735v3),  [pdf](http://arxiv.org/pdf/2506.05735v3)

**Tags**: cs.CL cs.LG 



### Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with   Receptive-Field-Aware Attention Weighting
**Authors**: Han Zhang, Yu Lu, Liyun Zhang, Dian Ding, Dinghua Zhao, Yi-Chao Chen, Ye Wu, Guangtao Xue

**Updated**: 2025-10-02T14:13:37Z

**Summary**: Understanding the emotions in a dialogue usually requires external knowledge to accurately understand the contents. As the LLMs become more and more powerful, we do not want to settle on the limited ability of the pre-trained language model. However, the LLMs either can only process text modality or are too expensive to process the multimedia information. We aim to utilize both the power of LLMs and the supplementary features from the multimedia modalities. In this paper, we present a framework, Lantern, that can improve the performance of a certain vanilla model by prompting large language models with receptive-field-aware attention weighting. This framework trained a multi-task vanilla model to produce probabilities of emotion classes and dimension scores. These predictions are fed into the LLMs as references to adjust the predicted probabilities of each emotion class with its external knowledge and contextual understanding. We slice the dialogue into different receptive fields, and each sample is included in exactly t receptive fields. Finally, the predictions of LLMs are merged with a receptive-field-aware attention-driven weighting module. In the experiments, vanilla models CORECT and SDT are deployed in Lantern with GPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way settings demonstrated that the Lantern can significantly improve the performance of current vanilla models by up to 1.23% and 1.80%.

**Link**: [arxiv](http://arxiv.org/abs/2411.17674v2),  [pdf](http://arxiv.org/pdf/2411.17674v2)

**Tags**: cs.CL 



### Investigating ReLoRA: Effects on the Learning Dynamics of Small Language   Models
**Authors**: Yuval Weiss, David Demitri Africa, Paula Buttery, Richard Diehl Martinez

**Updated**: 2025-10-02T14:09:44Z

**Summary**: Parameter-efficient methods like LoRA have revolutionised large language model (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly merging and reinitialising low-rank adapters, increasing cumulative rank while keeping updates cheap. This aligns well with observations that high-capacity models learn through locally low-rank trajectories that expand over time. By contrast, recent work suggests that small language models (SLMs) exhibit rank deficiencies and under-utilise their available dimensionality. This raises a natural question: can ReLoRA's rank-expanding update rule \textit{steer} SLMs toward healthier learning dynamics, mitigating rank bottlenecks in a capacity-constrained regime? We argue SLMs are an ideal testbed: they train quickly, enable controlled ablations, and make rank phenomena more measurable. We present the first systematic study of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and learning dynamics. Across loss, Paloma perplexity, and BLiMP, we find that ReLoRA underperforms full-rank training, with gaps widening at larger scales. Analysis of proportional effective rank and condition numbers shows that ReLoRA amplifies existing rank deficiencies and induces ill-conditioned updates early in training. Our results suggest that while ReLoRA's merge-and-restart strategy can expand ranks in larger models, it does not straightforwardly translate to capacity-limited SLMs, motivating adaptive-rank or hybrid-rank approaches for low-compute pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2509.12960v2),  [pdf](http://arxiv.org/pdf/2509.12960v2)

**Tags**: cs.CL cs.AI 



### QSpec: Speculative Decoding with Complementary Quantization Schemes
**Authors**: Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu

**Updated**: 2025-10-02T14:09:03Z

**Summary**: Quantization is widely adopted to accelerate inference and reduce memory consumption in large language models (LLMs). While activation-weight joint quantization enables efficient low-precision decoding, it suffers from substantial performance degradation on multi-step reasoning tasks. We propose QSpec, a novel quantization paradigm that decouples efficiency from quality by integrating two complementary schemes via speculative decoding: low-precision joint quantization for fast drafting and high-precision weight-only quantization for accurate verification. QSpec reuses both weights and KV cache across stages, enabling near-zero-cost switching without retraining or auxiliary models. Compared to high-precision baselines, QSpec achieves up to 1.64x speedup without quality degradation, and outperforms state-of-the-art speculative decoding methods by up to 1.55x in batched settings. Furthermore, QSpec supports plug-and-play deployment and generalizes well across model scales, quantization methods, and workloads. These properties make QSpec a practical and scalable solution for high-fidelity quantized LLM serving under memory-constrained scenarios. Our code is available at https://github.com/hku-netexplo-lab/QSpec.

**Link**: [arxiv](http://arxiv.org/abs/2410.11305v3),  [pdf](http://arxiv.org/pdf/2410.11305v3)

**Tags**: cs.LG cs.AI 



### Flexible Feature Distillation for Large Language Models
**Authors**: Khouloud Saadi, Di Wang

**Updated**: 2025-10-02T14:05:51Z

**Summary**: Knowledge distillation (KD) has become a cornerstone for compressing large language models (LLMs). However, existing LLM-KD methods have primarily focused on logit-based approaches, which achieve good performance but overlook the rich internal representations of LLMs. Feature-level KD could leverage this structure to provide complementary benefits, yet it remains underexplored because current feature-KD approaches typically assume identical teacher-student hidden sizes, a restrictive and unrealistic assumption. A common workaround is to train a linear projector to align their feature spaces; however, this introduces additional parameters, distorts teacher embeddings, and often degrades downstream performance, especially in generative tasks. We propose Flex-KD, a parameter-free framework for task-driven feature distillation for LLMs. Instead of projecting the entire teacher representation, Flex-KD uses gradient-based scores to identify the most task-relevant dimensions of the teacher's hidden states and distills only this subspace into the student. This ensures that the student's limited capacity is allocated to informative components, while avoiding projector-induced distortion and extra parameters. Flex-KD integrates seamlessly with existing KD pipelines and supports differing teacher-student hidden sizes. Extensive experiments across both classification and generative tasks, i.e., instruction-following and summarization, show that Flex-KD consistently boosts student performance, achieving up to a 3.75 percent performance gain over the linear projection baseline.

**Link**: [arxiv](http://arxiv.org/abs/2507.10155v2),  [pdf](http://arxiv.org/pdf/2507.10155v2)

**Tags**: cs.CL 



### Is the $3S$-$2D$ mixing strong for the charmonia $Ï(4040)$ and   $Ï(4160)$?
**Authors**: Zi-Long Man, Si-Qiang Luo, Xiang Liu

**Updated**: 2025-10-02T14:04:34Z

**Summary**: In this work, we revisit the $3S$-$2D$ mixing scheme for the charmonia $\psi(4040)$ and $\psi(4160)$. We introduce a coupled-channel mechanism-distinct from the tensor-force contribution in potential models, which alone is insufficient to induce significant mixing-to describe the mixing between these states. Our analysis yields mixing angles of $\theta_1=7^\circ$ and $\theta_2=10^\circ$, inconsistent with the larger angle inferred from experimental data, such as the di-lectronic widths of the $\psi(4040)$ and $\psi(4160)$. We discuss possible origins of this discrepancy and emphasize the need for future experiments to resolve it. Precise measurements of the resonance parameters and di-lectronic decay widths, via both inclusive and exclusive processes, will be crucial in clarifying this issue.

**Link**: [arxiv](http://arxiv.org/abs/2507.18536v2),  [pdf](http://arxiv.org/pdf/2507.18536v2)

**Tags**: hep-ph hep-ex 



### CrediBench: Building Web-Scale Network Datasets for Information   Integrity
**Authors**: Emma Kondrup, Sebastian Sabry, Hussein Abdallah, Zachary Yang, James Zhou, Kellin Pelrine, Jean-FranÃ§ois Godbout, Michael M. Bronstein, Reihaneh Rabbany, Shenyang Huang

**Updated**: 2025-10-02T14:03:57Z

**Summary**: Online misinformation poses an escalating threat, amplified by the Internet's open nature and increasingly capable LLMs that generate persuasive yet deceptive content. Existing misinformation detection methods typically focus on either textual content or network structure in isolation, failing to leverage the rich, dynamic interplay between website content and hyperlink relationships that characterizes real-world misinformation ecosystems. We introduce CrediBench: a large-scale data processing pipeline for constructing temporal web graphs that jointly model textual content and hyperlink structure for misinformation detection. Unlike prior work, our approach captures the dynamic evolution of general misinformation domains, including changes in both content and inter-site references over time. Our processed one-month snapshot extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, representing the largest web graph dataset made publicly available for misinformation research to date. From our experiments on this graph snapshot, we demonstrate the strength of both structural and webpage content signals for learning credibility scores, which measure source reliability. The pipeline and experimentation code are all available here, and the dataset is in this folder.

**Link**: [arxiv](http://arxiv.org/abs/2509.23340v3),  [pdf](http://arxiv.org/pdf/2509.23340v3)

**Tags**: cs.SI cs.DC cs.LG 



### Charting the Landscape of African NLP: Mapping Progress and Shaping the   Road Ahead
**Authors**: Jesujoba O. Alabi, Michael A. Hedderich, David Ifeoluwa Adelani, Dietrich Klakow

**Updated**: 2025-10-02T14:01:14Z

**Summary**: With over 2,000 languages and potentially millions of speakers, Africa represents one of the richest linguistic regions in the world. Yet, this diversity is scarcely reflected in state-of-the-art natural language processing (NLP) systems and large language models (LLMs), which predominantly support a narrow set of high-resource languages. This exclusion not only limits the reach and utility of modern NLP technologies but also risks widening the digital divide across linguistic communities. Nevertheless, NLP research on African languages is active and growing. In recent years, there has been a surge of interest in this area, driven by several factors-including the creation of multilingual language resources, the rise of community-led initiatives, and increased support through funding programs. In this survey, we analyze 884 research papers on NLP for African languages published over the past five years, offering a comprehensive overview of recent progress across core tasks. We identify key trends shaping the field and conclude by outlining promising directions to foster more inclusive and sustainable NLP research for African languages.

**Link**: [arxiv](http://arxiv.org/abs/2505.21315v3),  [pdf](http://arxiv.org/pdf/2505.21315v3)

**Tags**: cs.CL 



### GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries
**Authors**: Shuyang Hou, Haoyue Jiao, Ziqi Liu, Lutong Xie, Guanyu Chen, Shaowen Wu, Xuefeng Guan, Huayi Wu

**Updated**: 2025-10-02T13:58:56Z

**Summary**: Large language models (LLMs) have shown strong performance in natural language to SQL (NL2SQL) tasks within general databases. However, extending to GeoSQL introduces additional complexity from spatial data types, function invocation, and coordinate systems, which greatly increases generation and execution difficulty. Existing benchmarks mainly target general SQL, and a systematic evaluation framework for GeoSQL is still lacking. To fill this gap, we present GeoSQL-Eval, the first end-to-end automated evaluation framework for PostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing LLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task categories-conceptual understanding, syntax-level SQL generation, and schema retrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic databases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model, covering four cognitive dimensions, five capability levels, and twenty task types to establish a comprehensive process from knowledge acquisition and syntax generation to semantic alignment, execution accuracy, and robustness. We evaluate 24 representative models across six categories and apply the entropy weight method with statistical analyses to uncover performance differences, common error patterns, and resource usage. Finally, we release a public GeoSQL-Eval leaderboard platform for continuous testing and global comparison. This work extends the NL2GeoSQL paradigm and provides a standardized, interpretable, and extensible framework for evaluating LLMs in spatial database contexts, offering valuable references for geospatial information science and related applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.25264v2),  [pdf](http://arxiv.org/pdf/2509.25264v2)

**Tags**: cs.DB cs.AI cs.LG cs.SE 



### Style Over Story: A Process-Oriented Study of Authorial Creativity in   Large Language Models
**Authors**: Donghoon Jung, Jiwoo Choi, Songeun Chae, Seohyon Jung

**Updated**: 2025-10-02T13:57:14Z

**Summary**: Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity.

**Link**: [arxiv](http://arxiv.org/abs/2510.02025v1),  [pdf](http://arxiv.org/pdf/2510.02025v1)

**Tags**: cs.CL 



### Design and Application of Multimodal Large Language Model Based System   for End to End Automation of Accident Dataset Generation
**Authors**: MD Thamed Bin Zaman Chowdhury, Moazzem Hossain

**Updated**: 2025-10-02T13:50:05Z

**Summary**: Road traffic accidents remain a major public safety and socio-economic issue in developing countries like Bangladesh. Existing accident data collection is largely manual, fragmented, and unreliable, resulting in underreporting and inconsistent records. This research proposes a fully automated system using Large Language Models (LLMs) and web scraping techniques to address these challenges. The pipeline consists of four components: automated web scraping code generation, news collection from online sources, accident news classification with structured data extraction, and duplicate removal. The system uses the multimodal generative LLM Gemini-2.0-Flash for seamless automation. The code generation module classifies webpages into pagination, dynamic, or infinite scrolling categories and generates suitable Python scripts for scraping. LLMs also classify and extract key accident information such as date, time, location, fatalities, injuries, road type, vehicle types, and pedestrian involvement. A deduplication algorithm ensures data integrity by removing duplicate reports. The system scraped 14 major Bangladeshi news sites over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news articles and identifying 705 unique accidents. The code generation module achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the highest number of accidents (80), fatalities (70), and injuries (115), followed by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning (8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also developed with usage instructions. This study demonstrates the viability of an LLM-powered, scalable system for accurate, low-effort accident data collection, providing a foundation for data-driven road safety policymaking in Bangladesh.

**Link**: [arxiv](http://arxiv.org/abs/2505.00015v2),  [pdf](http://arxiv.org/pdf/2505.00015v2)

**Tags**: cs.CL 



### Erdos-Turan photonic Ising machines with record-high coupling resolution
**Authors**: Huaqiang Li, Guangfeng Wang, Erez Hasman, Bo Wang, Xianfeng Chen

**Updated**: 2025-10-02T13:19:01Z

**Summary**: Ising machines have emerged as promising platforms for efficiently tackling a wide range of combinatorial optimization problems relevant to resource allocation, statistical inference and deep learning, yet their practical utility is fundamentally constrained by the coarse resolution of spin-spin couplings (Jij). Current implementations, relying on direct modulation of physical parameters, achieve at most 256 discrete coupling levels, which severely hinder the faithfully modeling of arbitrary real-valued interactions in realistic applications. Here we present a novel photonic Ising machine that encodes spins in random lattices while programming couplings in the momentum space of light. By introducing the Sidon set-a mathematical structure ensuring pairwise difference uniqueness - and employing the Erdos-Turan bound, we establish an optical framework in which each spin pair can be assigned a unique Jij. This approach decouples the resolution limit from hardware modulation to the spatial precision in the momentum space of light. Experimentally, we demonstrate a record-high coupling resolution of 7,038 on a simple photonic platform, surpassing previous Ising machines. Our results highlight the power of uniting discrete mathematics with momentum-space photonics, paving the way toward scalable Ising machines capable of faithfully modeling real-world optimization problems.

**Link**: [arxiv](http://arxiv.org/abs/2510.01998v1),  [pdf](http://arxiv.org/pdf/2510.01998v1)

**Tags**: physics.optics math-ph math.MP 



### LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and   Target
**Authors**: Md Arid Hasan, Firoj Alam, Md Fahad Hossain, Usman Naseem, Syed Ishtiaque Ahmed

**Updated**: 2025-10-02T13:17:11Z

**Summary**: Online social media platforms are central to everyday communication and information seeking. While these platforms serve positive purposes, they also provide fertile ground for the spread of hate speech, offensive language, and bullying content targeting individuals, organizations, and communities. Such content undermines safety, participation, and equity online. Reliable detection systems are therefore needed, especially for low-resource languages where moderation tools are limited. In Bangla, prior work has contributed resources and models, but most are single-task (e.g., binary hate/offense) with limited coverage of multi-facet signals (type, severity, target). We address these gaps by introducing the first multi-task Bangla hate-speech dataset, BanglaMultiHate, one of the largest manually annotated corpus to date. Building on this resource, we conduct a comprehensive, controlled comparison spanning classical baselines, monolingual pretrained models, and LLMs under zero-shot prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs are competitive with BanglaBERT, culturally and linguistically grounded pretraining remains critical for robust performance. Together, our dataset and findings establish a stronger benchmark for developing culturally aligned moderation tools in low-resource contexts. For reproducibility, we will release the dataset and all related scripts.

**Link**: [arxiv](http://arxiv.org/abs/2510.01995v1),  [pdf](http://arxiv.org/pdf/2510.01995v1)

**Tags**: cs.CL 68T50 F.2.2; I.2.7 



### Clarifying Semantics of In-Context Examples for Unit Test Generation
**Authors**: Chen Yang, Lin Yang, Ziqi Wang, Dong Wang, Jianyi Zhou, Junjie Chen

**Updated**: 2025-10-02T13:15:40Z

**Summary**: Recent advances in large language models (LLMs) have enabled promising performance in unit test generation through in-context learning (ICL). However, the quality of in-context examples significantly influences the effectiveness of generated tests-poorly structured or semantically unclear test examples often lead to suboptimal outputs. In this paper, we propose CLAST, a novel technique that systematically refines unit tests to improve their semantic clarity, thereby enhancing their utility as in-context examples. The approach decomposes complex tests into logically clearer ones and improves semantic clarity through a combination of program analysis and LLM-based rewriting. We evaluated CLAST on four open-source and three industrial projects. The results demonstrate that CLAST largely outperforms UTgen, the state-of-the-art refinement technique, in both preserving test effectiveness and enhancing semantic clarity. Specifically, CLAST fully retains the original effectiveness of unit tests, while UTgen reduces compilation success rate (CSR), pass rate (PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%, 35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user study preferred the semantic clarity of CLAST-refined tests. Notably, incorporating CLAST-refined tests as examples effectively improves ICL-based unit test generation approaches such as RAGGen and TELPA, resulting in an average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for generated tests, compared to incorporating UTgen-refined tests. The insights from the follow-up user study not only reinforce CLAST's potential impact in software testing practice but also illuminate avenues for future research.

**Link**: [arxiv](http://arxiv.org/abs/2510.01994v1),  [pdf](http://arxiv.org/pdf/2510.01994v1)

**Tags**: cs.SE cs.AI 



### The Hidden Costs of Translation Accuracy: Distillation, Quantization,   and Environmental Impact
**Authors**: Dhaathri Vijay, Anandaswarup Vadapalli

**Updated**: 2025-10-02T13:15:14Z

**Summary**: The rapid expansion of large language models (LLMs) has heightened concerns about their computational and environmental costs. This study investigates the trade-offs between translation quality and efficiency by comparing full-scale, distilled, and quantized models using machine translation as a case study. We evaluated performance on the Flores+ benchmark and through human judgments of conversational translations in French, Hindi, and Kannada. Our analysis revealed that the full 3.3B FP32 model, while achieving the highest BLEU scores, incurred the largest environmental footprint (~ 0.007-0.008 kg CO2 per run). The distilled 600M FP32 model reduced inference time by 71-78% and carbon emissions by 63-65% compared with the full model, with only minimal reductions in BLEU scores. Human evaluations further showed that even aggressive quantization (INT4) preserved high levels of accuracy and fluency, with differences between models generally minor. These findings demonstrate that model compression strategies can substantially reduce computational demands and environmental impact while maintaining competitive translation quality, though trade-offs are more pronounced in low-resource settings. We argue for evaluation frameworks that integrate efficiency and sustainability alongside accuracy as central dimensions of progress in NLP.

**Link**: [arxiv](http://arxiv.org/abs/2509.23990v2),  [pdf](http://arxiv.org/pdf/2509.23990v2)

**Tags**: cs.CL cs.AI 



### 4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing
**Authors**: Lei Liu, Can Wang, Zhenghao Chen, Dong Xu

**Updated**: 2025-10-02T13:13:19Z

**Summary**: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2510.01991v1),  [pdf](http://arxiv.org/pdf/2510.01991v1)

**Tags**: cs.CV 



### LEXam: Benchmarking Legal Reasoning on 340 Law Exams
**Authors**: Yu Fan, Jingwei Ni, Jakob Merane, Yang Tian, Yoan HermstrÃ¼wer, Yinya Huang, Mubashara Akhtar, Etienne Salimbeni, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus

**Updated**: 2025-10-02T13:09:42Z

**Summary**: Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. To address this, we introduce \textsc{LEXam}, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Deploying an ensemble LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately, closely aligning with human expert assessments. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. We have open-sourced our code on \href{https://github.com/LEXam-Benchmark/LEXam}{GitHub} and released our data on \href{https://huggingface.co/datasets/LEXam-Benchmark/LEXam}{Hugging Face}. Project page: https://lexam-benchmark.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2505.12864v4),  [pdf](http://arxiv.org/pdf/2505.12864v4)

**Tags**: cs.CL cs.AI cs.LG 68T50 I.2 



### MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance
**Authors**: Xingjian Zhao, Zhe Xu, Qinyuan Cheng, Zhaoye Fei, Luozhijie Jin, Yang Wang, Hanfu Chen, Yaozhou Jiang, Qinghui Gao, Ke Chen, Ruixiao Li, Mingshu Chen, Ruiming Wang, Wenbo Zhang, Yiyang Zhang, Donghua Yu, Yang Gao, Xiaogui Yang, Yitian Gong, Yuanfan Xu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu

**Updated**: 2025-10-02T13:05:41Z

**Summary**: Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.

**Link**: [arxiv](http://arxiv.org/abs/2510.00499v2),  [pdf](http://arxiv.org/pdf/2510.00499v2)

**Tags**: cs.CL cs.AI 



### VFP: Variational Flow-Matching Policy for Multi-Modal Robot Manipulation
**Authors**: Xuanran Zhai, Qianyou Zhao, Qiaojun Yu, Ce Hao

**Updated**: 2025-10-02T13:01:54Z

**Summary**: Flow-matching-based policies have recently emerged as a promising approach for learning-based robot manipulation, offering significant acceleration in action sampling compared to diffusion-based policies. However, conventional flow-matching methods struggle with multi-modality, often collapsing to averaged or ambiguous behaviors in complex manipulation tasks. To address this, we propose the Variational Flow-Matching Policy (VFP), which introduces a variational latent prior for mode-aware action generation and effectively captures both task-level and trajectory-level multi-modality. VFP further incorporates Kantorovich Optimal Transport (K-OT) for distribution-level alignment and utilizes a Mixture-of-Experts (MoE) decoder for mode specialization and efficient inference. We comprehensively evaluate VFP on 41 simulated tasks and 3 real-robot tasks, demonstrating its effectiveness and sampling efficiency in both simulated and real-world settings. Results show that VFP achieves a 49% relative improvement in task success rate over standard flow-based baselines in simulation, and further outperforms them on real-robot tasks, while still maintaining fast inference and a compact model size. More details are available on our project page: https://sites.google.com/view/varfp/

**Link**: [arxiv](http://arxiv.org/abs/2508.01622v2),  [pdf](http://arxiv.org/pdf/2508.01622v2)

**Tags**: cs.RO cs.LG 



### The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering   for LLM Pretraining
**Authors**: Thiziri Nait Saada, Louis Bethune, Michal Klein, David Grangier, Marco Cuturi, Pierre Ablin

**Updated**: 2025-10-02T12:56:04Z

**Summary**: Large-scale models are pretrained on massive web-crawled datasets containing documents of mixed quality, making data filtering essential. A popular method is Classifier-based Quality Filtering (CQF), which trains a binary classifier to distinguish between pretraining data and a small, high-quality set. It assigns each pretraining document a quality score defined as the classifier's score and retains only the top-scoring ones. We provide an in-depth analysis of CQF. We show that while CQF improves downstream task performance, it does not necessarily enhance language modeling on the high-quality dataset. We explain this paradox by the fact that CQF implicitly filters the high-quality dataset as well. We further compare the behavior of models trained with CQF to those trained on synthetic data of increasing quality, obtained via random token permutations, and find starkly different trends. Our results challenge the view that CQF captures a meaningful notion of data quality.

**Link**: [arxiv](http://arxiv.org/abs/2510.00866v2),  [pdf](http://arxiv.org/pdf/2510.00866v2)

**Tags**: cs.LG cs.CL 



### Impact Plasma Amplification of the Ancient Mercury Magnetic Field
**Authors**: Isaac S. Narrett, Rona Oran, Yuxi Chen, Katarina MiljkoviÄ, GÃ¡bor TÃ³th, Catherine L. Johnson, Benjamin P. Weiss

**Updated**: 2025-10-02T12:48:59Z

**Summary**: Spacecraft measurements of Mercury indicate it has a core dynamo with a surface field of 200-800 nT. These data also indicate that the crust contains remanent magnetization likely produced by an ancient magnetic field. The inferred magnetization intensity is consistent with a wide range of paleofield strengths (0.2-50 uT), possibly indicating that Mercury once had a dynamo field much stronger than today. Recent modeling of ancient lunar impacts has demonstrated that plasma generated during basin-formation can transiently amplify a planetary dynamo field near the surface. Simultaneous impact-induced pressure waves can then record these fields in the form of crustal shock remanent magnetization (SRM). Here, we present impact hydrocode and magnetohydrodynamic simulations of a Caloris-size basin (~1,550 km diameter) formation event. Our results demonstrate that the ancient magnetospheric field (~0.5-0.9 uT) created by the interaction of the ancient interplanetary magnetic field (IMF) and Mercury's dynamo field can be amplified by the plasma up to ~13 uT and, via impact pressure waves, be recorded as SRM in the basin antipode. Such magnetization could produce ~5 nT crustal fields at 20-km altitude antipodal to Caloris detectable by future spacecraft like BepiColombo. Furthermore, impacts in the southern hemisphere that formed ~1,000 km diameter basins (e.g., Andal-Coleridge, Matisse-Repin, Eitkou-Milton, and Sadi-Scopus) could impart crustal magnetization in the northern hemisphere, contributing to the overall remanent field measured by MESSENGER. Overall, the impact plasma amplification process can contribute to crustal magnetization on airless bodies and should be considered when reconstructing dynamo history from crustal anomaly measurements.

**Link**: [arxiv](http://arxiv.org/abs/2510.01975v1),  [pdf](http://arxiv.org/pdf/2510.01975v1)

**Tags**: astro-ph.EP physics.space-ph 



### Fusing Foveal Fixations Using Linear Retinal Transformations and   Bayesian Experimental Design
**Authors**: Christopher K. I. Williams

**Updated**: 2025-10-02T12:29:39Z

**Summary**: Humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. In this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. This linear transformation allows us to carry out exact inference for the latent variables in factor analysis (FA) and mixtures of FA models of the scene. Further, this allows us to formulate and solve the choice of "where to look next" as a Bayesian experimental design problem using the Expected Information Gain criterion. Experiments on the Frey faces and MNIST datasets demonstrate the effectiveness of our models.

**Link**: [arxiv](http://arxiv.org/abs/2505.01249v2),  [pdf](http://arxiv.org/pdf/2505.01249v2)

**Tags**: cs.CV cs.LG 



## Keyword: LLM Deployment 
 ### KaVa: Latent Reasoning via Compressed KV-Cache Distillation
**Authors**: Anna Kuzina, Maciej Pioro, Paul N. Whatmough, Babak Ehteshami Bejnordi

**Updated**: 2025-10-02T17:59:51Z

**Summary**: Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.02312v1),  [pdf](http://arxiv.org/pdf/2510.02312v1)

**Tags**: cs.LG 



### Drawing Conclusions from Draws: Rethinking Preference Semantics in   Arena-Style LLM Evaluation
**Authors**: Raphael Tang, Crystina Zhang, Wenyan Li, Carmen Lai, Pontus Stenetorp, Yao Lu

**Updated**: 2025-10-02T17:59:41Z

**Summary**: In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the "battle" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates.

**Link**: [arxiv](http://arxiv.org/abs/2510.02306v1),  [pdf](http://arxiv.org/pdf/2510.02306v1)

**Tags**: cs.CL 



### ARMADA: Autonomous Online Failure Detection and Human Shared Control   Empower Scalable Real-world Deployment and Adaptation
**Authors**: Wenye Yu, Jun Lv, Zixi Ying, Yang Jin, Chuan Wen, Cewu Lu

**Updated**: 2025-10-02T17:59:02Z

**Summary**: Imitation learning has shown promise in learning from large-scale real-world datasets. However, pretrained policies usually perform poorly without sufficient in-domain data. Besides, human-collected demonstrations entail substantial labour and tend to encompass mixed-quality data and redundant information. As a workaround, human-in-the-loop systems gather domain-specific data for policy post-training, and exploit closed-loop policy feedback to offer informative guidance, but usually require full-time human surveillance during policy rollout. In this work, we devise ARMADA, a multi-robot deployment and adaptation system with human-in-the-loop shared control, featuring an autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA enables paralleled policy rollout and requests human intervention only when necessary, significantly reducing reliance on human supervision. Hence, ARMADA enables efficient acquisition of in-domain data, and leads to more scalable deployment and faster adaptation to new scenarios. We evaluate the performance of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on average, surpassing prior state-of-the-art failure detection approaches by over 20%. Besides, ARMADA manifests more than 4$\times$ increase in success rate and greater than 2$\times$ reduction in human intervention rate over multiple rounds of policy rollout and post-training, compared to previous human-in-the-loop learning methods.

**Link**: [arxiv](http://arxiv.org/abs/2510.02298v1),  [pdf](http://arxiv.org/pdf/2510.02298v1)

**Tags**: cs.RO 



### LitterBox+: An Extensible Framework for LLM-enhanced Scratch Static Code   Analysis
**Authors**: Benedikt Fein, Florian ObermÃ¼ller, Gordon Fraser

**Updated**: 2025-10-02T17:57:32Z

**Summary**: Large language models (LLMs) have become an essential tool to support developers using traditional text-based programming languages, but the graphical notation of the block-based Scratch programming environment inhibits the use of LLMs. To overcome this limitation, we propose the LitterBox+ framework that extends the Scratch static code analysis tool LitterBox with the generative abilities of LLMs. By converting block-based code to a textual representation suitable for LLMs, LitterBox+ allows users to query LLMs about their programs, about quality issues reported by LitterBox, and it allows generating code fixes. Besides offering a programmatic API for these functionalities, LitterBox+ also extends the Scratch user interface to make these functionalities available directly in the environment familiar to learners. The framework is designed to be easily extensible with other prompts, LLM providers, and new features combining the program analysis capabilities of LitterBox with the generative features of LLMs. We provide a screencast demonstrating the tool at https://youtu.be/RZ6E0xgrIgQ.

**Link**: [arxiv](http://arxiv.org/abs/2509.12021v2),  [pdf](http://arxiv.org/pdf/2509.12021v2)

**Tags**: cs.SE 



### Tree-based Dialogue Reinforced Policy Optimization for Red-Teaming   Attacks
**Authors**: Ruohao Guo, Afshin Oroojlooy, Roshan Sridhar, Miguel Ballesteros, Alan Ritter, Dan Roth

**Updated**: 2025-10-02T17:57:05Z

**Summary**: Despite recent rapid progress in AI safety, current large language models remain vulnerable to adversarial attacks in multi-turn interaction settings, where attackers strategically adapt their prompts across conversation turns and pose a more critical yet realistic challenge. Existing approaches that discover safety vulnerabilities either rely on manual red-teaming with human experts or employ automated methods using pre-defined templates and human-curated attack data, with most focusing on single-turn attacks. However, these methods did not explore the vast space of possible multi-turn attacks, failing to consider novel attack trajectories that emerge from complex dialogue dynamics and strategic conversation planning. This gap is particularly critical given recent findings that LLMs exhibit significantly higher vulnerability to multi-turn attacks compared to single-turn attacks. We propose DialTree-RPO, an on-policy reinforcement learning framework integrated with tree search that autonomously discovers diverse multi-turn attack strategies by treating the dialogue as a sequential decision-making problem, enabling systematic exploration without manually curated data. Through extensive experiments, our approach not only achieves more than 25.9% higher ASR across 10 target models compared to previous state-of-the-art approaches, but also effectively uncovers new attack strategies by learning optimal dialogue policies that maximize attack success across multiple turns.

**Link**: [arxiv](http://arxiv.org/abs/2510.02286v1),  [pdf](http://arxiv.org/pdf/2510.02286v1)

**Tags**: cs.LG cs.AI cs.CL 



### Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods   for Natural Language Generation
**Authors**: Mykyta Ielanskyi, Kajetan Schweighofer, Lukas Aichberger, Sepp Hochreiter

**Updated**: 2025-10-02T17:54:09Z

**Summary**: Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings.

**Link**: [arxiv](http://arxiv.org/abs/2510.02279v1),  [pdf](http://arxiv.org/pdf/2510.02279v1)

**Tags**: cs.LG cs.AI 



### Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps
**Authors**: Kyoungjun Park, Yifan Yang, Changhan Ge, Lili Qiu, Shiqi Jiang

**Updated**: 2025-10-02T17:50:22Z

**Summary**: Modeling radio frequency (RF) signal propagation is essential for understanding the environment, as RF signals offer valuable insights beyond the capabilities of RGB cameras, which are limited by the visible-light spectrum, lens coverage, and occlusions. It is also useful for supporting wireless diagnosis, deployment, and optimization. However, accurately predicting RF signals in complex environments remains a challenge due to interactions with obstacles such as absorption and reflection. We introduce Diffusion^2, a diffusion-based approach that uses 3D point clouds to model the propagation of RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves. To effectively capture RF-related features from 3D data, we present the RF-3D Encoder, which encapsulates the complexities of 3D geometry along with signal-specific details. These features undergo multi-scale embedding to simulate the actual RF signal dissemination process. Our evaluation, based on synthetic and real-world measurements, demonstrates that Diffusion^2 accurately estimates the behavior of RF signals in various frequency bands and environmental conditions, with an error margin of just 1.9 dB and 27x faster than existing methods, marking a significant advancement in the field. Refer to https://rfvision-project.github.io/ for more information.

**Link**: [arxiv](http://arxiv.org/abs/2510.02274v1),  [pdf](http://arxiv.org/pdf/2510.02274v1)

**Tags**: cs.LG 



### InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in   Tool-Augmented Agents
**Authors**: Yaxin Du, Yuanshuo Zhang, Xiyuan Yang, Yifan Zhou, Cheng Wang, Gongyi Zou, Xianghe Pang, Wenhao Wang, Menglan Chen, Shuo Tang, Zhiyu Li, Siheng Chen

**Updated**: 2025-10-02T17:48:03Z

**Summary**: Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools -- and more importantly, whether they can integrate them with general-purpose search to solve complex tasks. Therefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality. Experiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling.

**Link**: [arxiv](http://arxiv.org/abs/2510.02271v1),  [pdf](http://arxiv.org/pdf/2510.02271v1)

**Tags**: cs.CL cs.AI 



### microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for   Fine-Grained Image Classification
**Authors**: Sathira Silva, Eman Ali, Chetan Arora, Muhammad Haris Khan

**Updated**: 2025-10-02T17:47:39Z

**Summary**: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at https://github.com/sathiiii/microCLIP.

**Link**: [arxiv](http://arxiv.org/abs/2510.02270v1),  [pdf](http://arxiv.org/pdf/2510.02270v1)

**Tags**: cs.CV cs.AI 



### RLAD: Training LLMs to Discover Abstractions for Solving Reasoning   Problems
**Authors**: Yuxiao Qu, Anikait Singh, Yoonho Lee, Amrith Setlur, Ruslan Salakhutdinov, Chelsea Finn, Aviral Kumar

**Updated**: 2025-10-02T17:44:23Z

**Summary**: Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement "algorithmic procedures" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.

**Link**: [arxiv](http://arxiv.org/abs/2510.02263v1),  [pdf](http://arxiv.org/pdf/2510.02263v1)

**Tags**: cs.AI cs.CL cs.LG 



### From Frames to Clips: Efficient Key Clip Selection for Long-Form Video   Understanding
**Authors**: Guangyu Sun, Archit Singhal, Burak Uzkent, Mubarak Shah, Chen Chen, Garin Kessler

**Updated**: 2025-10-02T17:43:01Z

**Summary**: Video Large Language Models (VLMs) have achieved remarkable results on a variety of vision language tasks, yet their practical use is limited by the "needle in a haystack" problem: the massive number of visual tokens produced from raw video frames exhausts the model's context window. Existing solutions alleviate this issue by selecting a sparse set of frames, thereby reducing token count, but such frame-wise selection discards essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity. In this work we systematically explore the impact of temporal information and demonstrate that extending selection from isolated key frames to key clips, which are short, temporally coherent segments, improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we propose an adaptive resolution strategy that dynamically balances spatial resolution and clip length, ensuring a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling Video LLMs to real world video understanding applications. Project webpage is available at https://guangyusun.com/f2c .

**Link**: [arxiv](http://arxiv.org/abs/2510.02262v1),  [pdf](http://arxiv.org/pdf/2510.02262v1)

**Tags**: cs.CV 



### Can Code-Switched Texts Activate a Knowledge Switch in LLMs? A Case   Study on English-Korean Code-Switching
**Authors**: Seoyeon Kim, Huiseo Kim, Chanjun Park, Jinyoung Yeo, Dongha Lee

**Updated**: 2025-10-02T17:40:50Z

**Summary**: Recent large language models (LLMs) demonstrate multilingual abilities, yet they are English-centric due to dominance of English in training corpora. The limited resource for low-resource languages remains a crucial challenge. Code-switching (CS), a phenomenon where multilingual speakers alternate between languages in a discourse, can convey subtle cultural and linguistic nuances that can be otherwise lost in translation and elicits language-specific knowledge in human communications. In light of this, we investigate whether code-switching can activate, or identify and leverage knowledge for reasoning when LLMs solve low-resource language tasks. To facilitate the research, we first present EnKoQA, a synthetic English-Korean CS question-answering dataset. We provide comprehensive analysis on a variety of multilingual LLMs by subdividing activation process into knowledge identification and knowledge leveraging. Our results demonstrate that compared to English text, CS can faithfully activate knowledge inside LLMs especially on language-specific domains, suggesting the potential of code-switching on low-resource language tasks.

**Link**: [arxiv](http://arxiv.org/abs/2410.18436v4),  [pdf](http://arxiv.org/pdf/2410.18436v4)

**Tags**: cs.CL 



### Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative   Entropy Regulation
**Authors**: Tianyi Jiang, Yi Bin, Yujuan Ding, Kainian Zhu, Fei Ma, Jingkuan Song, Heng Tao Shen

**Updated**: 2025-10-02T17:36:50Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities on complex problems using long Chain-of-Thought (CoT) reasoning. However, they often suffer from overthinking, meaning generating unnecessarily lengthy reasoning steps for simpler problems. This issue may degrade the efficiency of the models and make them difficult to adapt the reasoning depth to the complexity of problems. To address this, we introduce a novel metric Token Entropy Cumulative Average (TECA), which measures the extent of exploration throughout the reasoning process. We further propose a novel reasoning paradigm -- Explore Briefly, Then Decide -- with an associated Cumulative Entropy Regulation (CER) mechanism. This paradigm leverages TECA to help the model dynamically determine the optimal point to conclude its thought process and provide a final answer, thus achieving efficient reasoning. Experimental results across diverse mathematical benchmarks show that our approach substantially mitigates overthinking without sacrificing problem-solving ability. With our thinking paradigm, the average response length decreases by up to 71% on simpler datasets, demonstrating the effectiveness of our method in creating a more efficient and adaptive reasoning process.

**Link**: [arxiv](http://arxiv.org/abs/2510.02249v1),  [pdf](http://arxiv.org/pdf/2510.02249v1)

**Tags**: cs.CL cs.AI cs.LG 



### Probabilistic Reasoning with LLMs for k-anonymity Estimation
**Authors**: Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu

**Updated**: 2025-10-02T17:36:23Z

**Summary**: Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average.

**Link**: [arxiv](http://arxiv.org/abs/2503.09674v4),  [pdf](http://arxiv.org/pdf/2503.09674v4)

**Tags**: cs.CL cs.LG 



### Anti-Regulatory AI: How "AI Safety" is Leveraged Against Regulatory   Oversight
**Authors**: Rui-Jie Yew, Brian Judge

**Updated**: 2025-10-02T17:35:49Z

**Summary**: AI companies increasingly develop and deploy privacy-enhancing technologies, bias-constraining measures, evaluation frameworks, and alignment techniques -- framing them as addressing concerns related to data privacy, algorithmic fairness, and AI safety. This paper examines the ulterior function of these technologies as mechanisms of legal influence. First, we examine how encryption, federated learning, and synthetic data -- presented as enhancing privacy and reducing bias -- can operate as mechanisms of avoidance with existing regulations in attempts to place data operations outside the scope of traditional regulatory frameworks. Second, we investigate how emerging AI safety practices including open-source model releases, evaluations, and alignment techniques can be used as mechanisms of change that direct regulatory focus towards industry-controlled voluntary standards and self-governance. We term this phenomenon "anti-regulatory AI" -- the deployment of ostensibly protective technologies that simultaneously shapes the terms of regulatory oversight. Our analysis additionally reveals how technologies' anti-regulatory functions are enabled through framing that legitimizes their deployment while obscuring their use as regulatory workarounds. This paper closes with a discussion of policy implications that centers on the consideration of business incentives that drive AI development and the role of technical expertise in assessing whether these technologies fulfill their purported protections.

**Link**: [arxiv](http://arxiv.org/abs/2509.22872v2),  [pdf](http://arxiv.org/pdf/2509.22872v2)

**Tags**: cs.CY 



### AccurateRAG: A Framework for Building Accurate Retrieval-Augmented   Question-Answering Applications
**Authors**: Linh The Nguyen, Chi Tran, Dung Ngoc Nguyen, Van-Cuong Pham, Hoang Ngo, Dat Quoc Nguyen

**Updated**: 2025-10-02T17:30:08Z

**Summary**: We introduce AccurateRAG -- a novel framework for constructing high-performance question-answering applications based on retrieval-augmented generation (RAG). Our framework offers a pipeline for development efficiency with tools for raw dataset processing, fine-tuning data generation, text embedding & LLM fine-tuning, output evaluation, and building RAG systems locally. Experimental results show that our framework outperforms previous strong baselines and obtains new state-of-the-art question-answering performance on benchmark datasets.

**Link**: [arxiv](http://arxiv.org/abs/2510.02243v1),  [pdf](http://arxiv.org/pdf/2510.02243v1)

**Tags**: cs.CL 



### Study on LLMs for Promptagator-Style Dense Retriever Training
**Authors**: Daniel Gwon, Nour Jedidi, Jimmy Lin

**Updated**: 2025-10-02T17:29:51Z

**Summary**: Promptagator demonstrated that Large Language Models (LLMs) with few-shot prompts can be used as task-specific query generators for fine-tuning domain-specialized dense retrieval models. However, the original Promptagator approach relied on proprietary and large-scale LLMs which users may not have access to or may be prohibited from using with sensitive data. In this work, we study the impact of open-source LLMs at accessible scales ($\leq$14B parameters) as an alternative. Our results demonstrate that open-source LLMs as small as 3B parameters can serve as effective Promptagator-style query generators. We hope our work will inform practitioners with reliable alternatives for synthetic data generation and give insights to maximize fine-tuning results for domain-specific applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.02241v1),  [pdf](http://arxiv.org/pdf/2510.02241v1)

**Tags**: cs.IR cs.CL 



### AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features
**Authors**: Xudong Zhu, Mohammad Mahdi Khalili, Zhihui Zhu

**Updated**: 2025-10-02T17:28:55Z

**Summary**: Sparse autoencoders (SAEs) have emerged as powerful techniques for interpretability of large language models (LLMs), aiming to decompose hidden states into meaningful semantic features. While several SAE variants have been proposed, there remains no principled framework to derive SAEs from the original dictionary learning formulation. In this work, we introduce such a framework by unrolling the proximal gradient method for sparse coding. We show that a single-step update naturally recovers common SAE variants, including ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation of existing SAEs: their sparsity-inducing regularizers enforce non-negativity, preventing a single feature from representing bidirectional concepts (e.g., male vs. female). This structural constraint fragments semantic axes into separate, redundant features, limiting representational completeness. To address this issue, we propose AbsTopK SAE, a new variant derived from the $\ell_0$ sparsity constraint that applies hard thresholding over the largest-magnitude activations. By preserving both positive and negative activations, AbsTopK uncovers richer, bidirectional conceptual representations. Comprehensive experiments across four LLMs and seven probing and steering tasks show that AbsTopK improves reconstruction fidelity, enhances interpretability, and enables single features to encode contrasting concepts. Remarkably, AbsTopK matches or even surpasses the Difference-in-Mean method, a supervised approach that requires labeled data for each concept and has been shown in prior work to outperform SAEs.

**Link**: [arxiv](http://arxiv.org/abs/2510.00404v2),  [pdf](http://arxiv.org/pdf/2510.00404v2)

**Tags**: cs.LG cs.AI cs.CL 



### Adapting Large Language Models for Character-based Augmentative and   Alternative Communication
**Authors**: Dylan Gaines, Keith Vertanen

**Updated**: 2025-10-02T17:20:32Z

**Summary**: Users of Augmentative and Alternative Communication (AAC) may write letter-by-letter via an interface that uses a character language model. However, most state-of-the-art large pretrained language models predict subword tokens of variable length. We investigate how to practically use such models to make accurate and efficient character predictions. Our algorithm for producing character predictions from a subword large language model (LLM) provides more accurate predictions than using a classification layer, a byte-level LLM, or an n-gram model. Additionally, we investigate a domain adaptation procedure based on a large dataset of sentences we curated based on scoring how useful each sentence might be for spoken or written AAC communication. We find our procedure further improves model performance on simple, conversational text.

**Link**: [arxiv](http://arxiv.org/abs/2501.10582v3),  [pdf](http://arxiv.org/pdf/2501.10582v3)

**Tags**: cs.CL cs.HC 



### The Reasoning Boundary Paradox: How Reinforcement Learning Constrains   Language Models
**Authors**: Phuc Minh Nguyen, Chinh D. La, Duy M. H. Nguyen, Nitesh V. Chawla, Binh T. Nguyen, Khoa D. Doan

**Updated**: 2025-10-02T17:17:27Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.

**Link**: [arxiv](http://arxiv.org/abs/2510.02230v1),  [pdf](http://arxiv.org/pdf/2510.02230v1)

**Tags**: cs.AI cs.CL cs.CV 



### xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity
**Authors**: Maximilian Beck, Kajetan Schweighofer, Sebastian BÃ¶ck, Sebastian Lehner, Sepp Hochreiter

**Updated**: 2025-10-02T17:14:34Z

**Summary**: Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow.

**Link**: [arxiv](http://arxiv.org/abs/2510.02228v1),  [pdf](http://arxiv.org/pdf/2510.02228v1)

**Tags**: cs.LG 



### More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for   Diverse Exploration
**Authors**: Xiaoyang Yuan, Yujuan Ding, Yi Bin, Wenqi Shao, Jinyu Cai, Jingkuan Song, Yang Yang, Hengtao Shen

**Updated**: 2025-10-02T17:14:00Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This "guidance-on-demand" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.

**Link**: [arxiv](http://arxiv.org/abs/2510.02227v1),  [pdf](http://arxiv.org/pdf/2510.02227v1)

**Tags**: cs.CL cs.AI cs.LG 



### Contrastive Retrieval Heads Improve Attention-Based Re-Ranking
**Authors**: Linh Tran, Yulong Li, Radu Florian, Wei Sun

**Updated**: 2025-10-02T17:03:09Z

**Summary**: The strong zero-shot and long-context capabilities of recent Large Language Models (LLMs) have paved the way for highly effective re-ranking systems. Attention-based re-rankers leverage attention weights from transformer heads to produce relevance scores, but not all heads are created equally: many contribute noise and redundancy, thus limiting performance. To address this, we introduce CoRe heads, a small set of retrieval heads identified via a contrastive scoring metric that explicitly rewards high attention heads that correlate with relevant documents, while downplaying nodes with higher attention that correlate with irrelevant documents. This relative ranking criterion isolates the most discriminative heads for re-ranking and yields a state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs show that aggregated signals from CoRe heads, constituting less than 1% of all heads, substantially improve re-ranking accuracy over strong baselines. We further find that CoRe heads are concentrated in middle layers, and pruning the computation of final 50% of model layers preserves accuracy while significantly reducing inference time and memory usage.

**Link**: [arxiv](http://arxiv.org/abs/2510.02219v1),  [pdf](http://arxiv.org/pdf/2510.02219v1)

**Tags**: cs.IR 



### DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via   Reinforcement Learning
**Authors**: Hanyang Zhao, Dawen Liang, Wenpin Tang, David Yao, Nathan Kallus

**Updated**: 2025-10-02T16:57:24Z

**Summary**: We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.02212v1),  [pdf](http://arxiv.org/pdf/2510.02212v1)

**Tags**: cs.LG cs.AI 



### DrKGC: Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph   Completion across General and Biomedical Domains
**Authors**: Yongkang Xiao, Sinian Zhang, Yi Dai, Huixue Zhou, Jue Hou, Jie Ding, Rui Zhang

**Updated**: 2025-10-02T16:56:36Z

**Summary**: Knowledge graph completion (KGC) aims to predict missing triples in knowledge graphs (KGs) by leveraging existing triples and textual information. Recently, generative large language models (LLMs) have been increasingly employed for graph tasks. However, current approaches typically encode graph context in textual form, which fails to fully exploit the potential of LLMs for perceiving and reasoning about graph structures. To address this limitation, we propose DrKGC (Dynamic Subgraph Retrieval-Augmented LLMs for Knowledge Graph Completion). DrKGC employs a flexible lightweight model training strategy to learn structural embeddings and logical rules within the KG. It then leverages a novel bottom-up graph retrieval method to extract a subgraph for each query guided by the learned rules. Finally, a graph convolutional network (GCN) adapter uses the retrieved subgraph to enhance the structural embeddings, which are then integrated into the prompt for effective LLM fine-tuning. Experimental results on two general domain benchmark datasets and two biomedical datasets demonstrate the superior performance of DrKGC. Furthermore, a realistic case study in the biomedical domain highlights its interpretability and practical utility.

**Link**: [arxiv](http://arxiv.org/abs/2506.00708v2),  [pdf](http://arxiv.org/pdf/2506.00708v2)

**Tags**: cs.AI cs.CL cs.LG 



### StockBench: Can LLM Agents Trade Stocks Profitably In Real-world   Markets?
**Authors**: Yanxu Chen, Zijun Yao, Yantao Liu, Jin Ye, Jianing Yu, Lei Hou, Juanzi Li

**Updated**: 2025-10-02T16:54:57Z

**Summary**: Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.

**Link**: [arxiv](http://arxiv.org/abs/2510.02209v1),  [pdf](http://arxiv.org/pdf/2510.02209v1)

**Tags**: cs.LG cs.CL 



### Measurement-Guided Consistency Model Sampling for Inverse Problems
**Authors**: Amirreza Tanevardi, Pooria Abbas Rad Moghadam, Sajjad Amini

**Updated**: 2025-10-02T16:53:07Z

**Summary**: Diffusion models have become powerful generative priors for solving inverse imaging problems, but their reliance on slow multi-step sampling limits practical deployment. Consistency models address this bottleneck by enabling high-quality generation in a single or only a few steps, yet their direct adaptation to inverse problems is underexplored. In this paper, we present a modified consistency sampling approach tailored for inverse problem reconstruction: the sampler's stochasticity is guided by a measurement-consistency mechanism tied to the measurement operator, which enforces fidelity to the acquired measurements while retaining the efficiency of consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom datasets demonstrate consistent improvements in perceptual and pixel-level metrics, including Fr\'echet Inception Distance, Kernel Inception Distance, peak signal-to-noise ratio, and structural similarity index measure, compared to baseline consistency sampling, yielding competitive or superior reconstructions with only a handful of steps.

**Link**: [arxiv](http://arxiv.org/abs/2510.02208v1),  [pdf](http://arxiv.org/pdf/2510.02208v1)

**Tags**: eess.IV cs.CV cs.LG 



### Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling
**Authors**: Daniel Gallo FernÃ¡ndez

**Updated**: 2025-10-02T16:52:45Z

**Summary**: Sequence-to-sequence models have become central in Artificial Intelligence, particularly following the introduction of the transformer architecture. While initially developed for Natural Language Processing, these models have demonstrated utility across domains, including Computer Vision. Such models require mechanisms to exchange information along the time dimension, typically using recurrent or self-attention layers. However, self-attention scales quadratically with sequence length, limiting its practicality for very long sequences.   We introduce Poolformer, a sequence-to-sequence model that replaces self-attention with recurrent layers and incorporates pooling operations to reduce sequence length. Poolformer is defined recursively using SkipBlocks, which contain residual blocks, a down-pooling layer, a nested SkipBlock, an up-pooling layer, and additional residual blocks. We conduct extensive experiments to support our architectural choices.   Our results show that pooling greatly accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. Our experiments also suggest that long-range dependencies are handled by deep layers, while shallow layers take care of short-term features.   Evaluated on raw audio, which naturally features long sequence lengths, Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba. Future directions include applications to text and vision, as well as multi-modal scenarios, where a Poolformer-based LLM could effectively process dense representations of images and videos.

**Link**: [arxiv](http://arxiv.org/abs/2510.02206v1),  [pdf](http://arxiv.org/pdf/2510.02206v1)

**Tags**: cs.LG 



### ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge   Graph Exploration Utilities
**Authors**: Felix Brei, Lorenz BÃ¼hmann, Johannes Frey, Daniel Gerber, Lars-Peter Meyer, Claus Stadler, Kirill Bulert

**Updated**: 2025-10-02T16:49:27Z

**Summary**: Interacting with knowledge graphs can be a daunting task for people without a background in computer science since the query language that is used (SPARQL) has a high barrier of entry. Large language models (LLMs) can lower that barrier by providing support in the form of Text2SPARQL translation. In this paper we introduce a generalized method based on SPINACH, an LLM backed agent that translates natural language questions to SPARQL queries not in a single shot, but as an iterative process of exploration and execution. We describe the overall architecture and reasoning behind our design decisions, and also conduct a thorough analysis of the agent behavior to gain insights into future areas for targeted improvements. This work was motivated by the Text2SPARQL challenge, a challenge that was held to facilitate improvements in the Text2SPARQL domain.

**Link**: [arxiv](http://arxiv.org/abs/2510.02200v1),  [pdf](http://arxiv.org/pdf/2510.02200v1)

**Tags**: cs.CL cs.AI 



### Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition:   A Machine Learning Approach for Small-Scale Farming Applications
**Authors**: Emmanuel Nsengiyumvaa, Leonard Niyitegekaa, Eric Umuhoza

**Updated**: 2025-10-02T16:45:43Z

**Summary**: Accurate livestock identification is a cornerstone of modern farming: it supports health monitoring, breeding programs, and productivity tracking. However, common pig identification methods, such as ear tags and microchips, are often unreliable, costly, target pure breeds, and thus impractical for small-scale farmers. To address this gap, we propose a noninvasive biometric identification approach that leverages uniqueness of the auricular vein patterns. To this end, we have collected 800 ear images from 20 mixed-breed pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a standard smartphone and simple back lighting. A multistage computer vision pipeline was developed to enhance vein visibility, extract structural and spatial features, and generate biometric signatures. These features were then classified using machine learning models. Support Vector Machines (SVM) achieved the highest accuracy: correctly identifying pigs with 98.12% precision across mixed-breed populations. The entire process from image processing to classification was completed in an average of 8.3 seconds, demonstrating feasibility for real-time farm deployment. We believe that by replacing fragile physical identifiers with permanent biological markers, this system provides farmers with a cost-effective and stress-free method of animal identification. More broadly, the findings confirm the practicality of auricular vein biometrics for digitizing livestock management, reinforcing its potential to extend the benefits of precision farming to resource-constrained agricultural communities.

**Link**: [arxiv](http://arxiv.org/abs/2510.02197v1),  [pdf](http://arxiv.org/pdf/2510.02197v1)

**Tags**: cs.CV cs.SE 



### On Code-Induced Reasoning in LLMs
**Authors**: Abdul Waheed, Zhen Wu, Carolyn RosÃ©, Daphne Ippolito

**Updated**: 2025-10-02T16:45:24Z

**Summary**: Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2509.21499v2),  [pdf](http://arxiv.org/pdf/2509.21499v2)

**Tags**: cs.CL cs.PL 



### UpSafe$^\circ$C: Upcycling for Controllable Safety in Large Language   Models
**Authors**: Yuhao Sun, Zhuoer Xu, Shiwen Cui, Kun Yang, Lingyun Yu, Yongdong Zhang, Hongtao Xie

**Updated**: 2025-10-02T16:43:33Z

**Summary**: Large Language Models (LLMs) have achieved remarkable progress across a wide range of tasks, but remain vulnerable to safety risks such as harmful content generation and jailbreak attacks. Existing safety techniques -- including external guardrails, inference-time guidance, and post-training alignment -- each face limitations in balancing safety, utility, and controllability. In this work, we propose UpSafe$^\circ$C, a unified framework for enhancing LLM safety through safety-aware upcycling. Our approach first identifies safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE) structure, where the router acts as a soft guardrail that selectively activates original MLPs and added safety experts. We further introduce a two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities. To enable flexible control at inference time, we introduce a safety temperature mechanism, allowing dynamic adjustment of the trade-off between safety and utility. Experiments across multiple benchmarks, base model, and model scales demonstrate that UpSafe$^\circ$C achieves robust safety improvements against harmful and jailbreak inputs, while maintaining competitive performance on general tasks. Moreover, analysis shows that safety temperature provides fine-grained inference-time control that achieves the Pareto-optimal frontier between utility and safety. Our results highlight a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.

**Link**: [arxiv](http://arxiv.org/abs/2510.02194v1),  [pdf](http://arxiv.org/pdf/2510.02194v1)

**Tags**: cs.AI cs.CR cs.LG 



### FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen   Using Agentic AI
**Authors**: Paschal C. Amusuo, Dongge Liu, Ricardo Andres Calvo Mendez, Jonathan Metzman, Oliver Chang, James C. Davis

**Updated**: 2025-10-02T16:36:56Z

**Summary**: Fuzz testing has become a cornerstone technique for identifying software bugs and security vulnerabilities, with broad adoption in both industry and open-source communities. Directly fuzzing a function requires fuzz drivers, which translate random fuzzer inputs into valid arguments for the target function. Given the cost and expertise required to manually develop fuzz drivers, methods exist that leverage program analysis and Large Language Models to automatically generate these drivers. However, the generated fuzz drivers frequently lead to false positive crashes, especially in functions highly structured input and complex state requirements. This problem is especially crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as reporting false positive crashes to maintainers impede trust in both the system and the team.   This paper presents two AI-driven strategies to reduce false positives in OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First, constraint-based fuzz driver generation proactively enforces constraints on a function's inputs and state to guide driver creation. Second, context-based crash validation reactively analyzes function callers to determine whether reported crashes are feasible from program entry points. Using 1,500 benchmark functions from OSS-Fuzz, we show that these strategies reduce spurious crashes by up to 8%, cut reported crashes by more than half, and demonstrate that frontier LLMs can serve as reliable program analysis agents. Our results highlight the promise and challenges of integrating AI into large-scale fuzzing pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2510.02185v1),  [pdf](http://arxiv.org/pdf/2510.02185v1)

**Tags**: cs.SE cs.CR cs.MA D.2.4; F.3.1 



### Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward
**Authors**: Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques

**Updated**: 2025-10-02T16:36:08Z

**Summary**: Effective conversational agents like large language models (LLMs) must personalize their interactions to adapt to user preferences, personalities, and attributes across diverse domains like education and healthcare. Current methods like Reinforcement Learning from Human Feedback (RLHF), often prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized dialogues. Existing personalization approaches typically rely on extensive user history, limiting their effectiveness for new or context-limited users. To address these limitations, we propose leveraging a user model to incorporate a curiosity-based intrinsic reward into multi-turn RLHF. This novel reward mechanism encourages the LLM agent to actively infer user traits by optimizing conversations to improve its user model's accuracy. Consequently, the agent delivers more personalized interactions by learning more about the user. We demonstrate our method's effectiveness in two distinct domains: significantly improving personalization performance in a conversational recommendation task, and personalizing conversations for different learning styles in an educational setting. We show improved generalization capabilities compared to traditional multi-turn RLHF, all while maintaining conversation quality. Our method offers a promising solution for creating more personalized, adaptive, and engaging conversational agents.

**Link**: [arxiv](http://arxiv.org/abs/2504.03206v3),  [pdf](http://arxiv.org/pdf/2504.03206v3)

**Tags**: cs.CL cs.AI 



### DisCo-Layout: Disentangling and Coordinating Semantic and Physical   Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis
**Authors**: Jialin Gao, Donghao Zhou, Mingjian Liang, Lihao Liu, Chi-Wing Fu, Xiaowei Hu, Pheng-Ann Heng

**Updated**: 2025-10-02T16:30:37Z

**Summary**: 3D indoor layout synthesis is crucial for creating virtual environments. Traditional methods struggle with generalization due to fixed datasets. While recent LLM and VLM-based approaches offer improved semantic richness, they often lack robust and flexible refinement, resulting in suboptimal layouts. We develop DisCo-Layout, a novel framework that disentangles and coordinates physical and semantic refinement. For independent refinement, our Semantic Refinement Tool (SRT) corrects abstract object relationships, while the Physical Refinement Tool (PRT) resolves concrete spatial issues via a grid-matching algorithm. For collaborative refinement, a multi-agent framework intelligently orchestrates these tools, featuring a planner for placement rules, a designer for initial layouts, and an evaluator for assessment. Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating realistic, coherent, and generalizable 3D indoor layouts. Our code will be publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2510.02178v1),  [pdf](http://arxiv.org/pdf/2510.02178v1)

**Tags**: cs.RO cs.CV 



### Learning to Reason for Hallucination Span Detection
**Authors**: Hsuan Su, Ting-Yao Hu, Hema Swetha Koppula, Kundan Krishna, Hadi Pouransari, Cheng-Yu Hsieh, Cem Koc, Joseph Yitan Cheng, Oncel Tuzel, Raviteja Vemulapalli

**Updated**: 2025-10-02T16:24:28Z

**Summary**: Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.

**Link**: [arxiv](http://arxiv.org/abs/2510.02173v1),  [pdf](http://arxiv.org/pdf/2510.02173v1)

**Tags**: cs.CL cs.AI cs.LG 



### Superficial Safety Alignment Hypothesis
**Authors**: Jianwei Li, Jung-Eun Kim

**Updated**: 2025-10-02T16:15:20Z

**Summary**: As large language models (LLMs) are overwhelmingly more and more integrated into various applications, ensuring they generate safe responses is a pressing need. Previous studies on alignment have largely focused on general instruction-following but have often overlooked the distinct properties of safety alignment, such as the brittleness of safety mechanisms. To bridge the gap, we propose the Superficial Safety Alignment Hypothesis (SSAH), which posits that safety alignment teaches an otherwise unsafe model to choose the correct reasoning direction - fulfill or refuse users' requests - interpreted as an implicit binary classification task. Through SSAH, we hypothesize that only a few essential components can establish safety guardrails in LLMs. We successfully identify four types of attribute-critical components: Safety Critical Unit (SCU), Utility Critical Unit (UCU), Complex Unit (CU), and Redundant Unit (RU). Our findings show that freezing certain safety-critical components during fine-tuning allows the model to retain its safety attributes while adapting to new tasks. Similarly, we show that leveraging redundant units in the pre-trained model as an "alignment budget" can effectively minimize the alignment tax while achieving the alignment goal. All considered, this paper concludes that the atomic functional unit for safety in LLMs is at the neuron level and underscores that safety alignment should not be complicated.

**Link**: [arxiv](http://arxiv.org/abs/2410.10862v2),  [pdf](http://arxiv.org/pdf/2410.10862v2)

**Tags**: cs.CL cs.AI cs.CR cs.CY cs.LG 



### CodeSense: a Real-World Benchmark and Dataset for Code Semantic   Reasoning
**Authors**: Monoshi Kumar Roy, Simin Chen, Benjamin Steenhoek, Jinjun Peng, Gail Kaiser, Baishakhi Ray, Wei Le

**Updated**: 2025-10-02T16:10:36Z

**Summary**: Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2506.00750v2),  [pdf](http://arxiv.org/pdf/2506.00750v2)

**Tags**: cs.SE cs.AI 



### Agentic Reasoning and Refinement through Semantic Interaction
**Authors**: Xuxin Tang, Rehema Abulikemu, Eric Krokos, Kirsten Whitley, Xuan Wang, Chris North

**Updated**: 2025-10-02T16:08:51Z

**Summary**: Sensemaking report writing often requires multiple refinements in the iterative process. While Large Language Models (LLMs) have shown promise in generating initial reports based on human visual workspace representations, they struggle to precisely incorporate sequential semantic interactions during the refinement process. We introduce VIS-ReAct, a framework that reasons about newly-added semantic interactions in visual workspaces to steer the LLM for report refinement.   VIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning, followed by an LLM refinement agent that updates reports accordingly. Through case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM analysis) on targeted refinement, semantic fidelity, and transparent inference. Results demonstrate that VIS-ReAct better handles various interaction types and granularities while enhancing the transparency of human-LLM collaboration.

**Link**: [arxiv](http://arxiv.org/abs/2510.02157v1),  [pdf](http://arxiv.org/pdf/2510.02157v1)

**Tags**: cs.HC 



### When Disagreements Elicit Robustness: Investigating Self-Repair   Capabilities under LLM Multi-Agent Disagreements
**Authors**: Tianjie Ju, Bowen Wang, Hao Fei, Mong-Li Lee, Wynne Hsu, Yun Li, Qianren Wang, Pengzhou Cheng, Zongru Wu, Haodong Zhao, Zhuosheng Zhang, Gongshen Liu

**Updated**: 2025-10-02T15:55:21Z

**Summary**: Recent advances in Large Language Models (LLMs) have upgraded them from sophisticated text generators to autonomous agents capable of cooperation and tool use in multi-agent systems (MAS). However, it remains unclear how disagreements shape collective decision-making. In this paper, we revisit the role of disagreement and argue that general, partially overlapping disagreements prevent premature consensus and expand the explored solution space, while disagreements on task-critical steps can derail collaboration depending on the topology of solution paths. We investigate two collaborative settings with distinct path structures: collaborative reasoning (CounterFact, MQuAKE-cf), which typically follows a single evidential chain, whereas collaborative programming (HumanEval, GAIA) often adopts multiple valid implementations. Disagreements are instantiated as general heterogeneity among agents and as task-critical counterfactual knowledge edits injected into context or parameters. Experiments reveal that general disagreements consistently improve success by encouraging complementary exploration. By contrast, task-critical disagreements substantially reduce success on single-path reasoning, yet have a limited impact on programming, where agents can choose alternative solutions. Trace analyses show that MAS frequently bypasses the edited facts in programming but rarely does so in reasoning, revealing an emergent self-repair capability that depends on solution-path rather than scale alone. Our code is available at https://github.com/wbw625/MultiAgentRobustness.

**Link**: [arxiv](http://arxiv.org/abs/2502.15153v2),  [pdf](http://arxiv.org/pdf/2502.15153v2)

**Tags**: cs.CL 



### MissionHD: Hyperdimensional Refinement of Distribution-Deficient   Reasoning Graphs for Video Anomaly Detection
**Authors**: Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Nathaniel D. Bastian, Mohsen Imani

**Updated**: 2025-10-02T15:55:20Z

**Summary**: LLM-generated reasoning graphs, referred to as mission-specific graphs (MSGs), are increasingly used for video anomaly detection (VAD) and recognition (VAR). These MSGs are novel artifacts: they often exhibit skewed connectivity and lack large-scale datasets for pre-training, which makes existing graph structure refinement (GSR) methods ineffective. To address this challenge, we propose HDC-constrained Graph Structure Refinement (HDC-GSR), a paradigm that leverages hyperdimensional computing (HDC) to optimize decodable graph representations without relying on structural-distribution learning. Building on this paradigm, we introduce MissionHD, an HDC framework that encodes graphs with constrained graph-neural operations, aligns them directly with downstream task loss, and decodes refined structures. Experiments on VAD/VAR benchmarks demonstrate that MissionHD-refined graphs consistently improve performance, establishing HDC-GSR as an effective pre-processing step for structured reasoning in video anomaly tasks.

**Link**: [arxiv](http://arxiv.org/abs/2508.14746v3),  [pdf](http://arxiv.org/pdf/2508.14746v3)

**Tags**: cs.LG 



### FlexDoc: Parameterized Sampling for Diverse Multilingual Synthetic   Documents for Training Document Understanding Models
**Authors**: Karan Dua, Hitesh Laxmichand Patel, Puneet Mittal, Ranjeet Gupta, Amit Agarwal, Praneet Pabolu, Srikant Panda, Hansa Meghwani, Graham Horwood, Fahad Shah

**Updated**: 2025-10-02T15:42:35Z

**Summary**: Developing document understanding models at enterprise scale requires large, diverse, and well-annotated datasets spanning a wide range of document types. However, collecting such data is prohibitively expensive due to privacy constraints, legal restrictions, and the sheer volume of manual annotation needed - costs that can scale into millions of dollars. We introduce FlexDoc, a scalable synthetic data generation framework that combines Stochastic Schemas and Parameterized Sampling to produce realistic, multilingual semi-structured documents with rich annotations. By probabilistically modeling layout patterns, visual structure, and content variability, FlexDoc enables the controlled generation of diverse document variants at scale. Experiments on Key Information Extraction (KIE) tasks demonstrate that FlexDoc-generated data improves the absolute F1 Score by up to 11% when used to augment real datasets, while reducing annotation effort by over 90% compared to traditional hard-template methods. The solution is in active deployment, where it has accelerated the development of enterprise-grade document understanding models while significantly reducing data acquisition and annotation costs.

**Link**: [arxiv](http://arxiv.org/abs/2510.02133v1),  [pdf](http://arxiv.org/pdf/2510.02133v1)

**Tags**: cs.AI cs.LG I.2.7; I.2.10; I.4.8; I.4.9 



### Beyond Outliers: A Study of Optimizers Under Quantization
**Authors**: Georgios Vlassis, Saleh Ashkboos, Alexandra Volkova, Torsten Hoefler, Dan Alistarh

**Updated**: 2025-10-02T15:34:43Z

**Summary**: As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers.

**Link**: [arxiv](http://arxiv.org/abs/2509.23500v2),  [pdf](http://arxiv.org/pdf/2509.23500v2)

**Tags**: cs.LG 



### REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking
**Authors**: Pinhuan Wang, Zhiqiu Xia, Chunhua Liao, Feiyi Wang, Hang Liu

**Updated**: 2025-10-02T15:20:02Z

**Summary**: Large Language Models (LLMs) have shown strong capabilities in document re-ranking, a key component in modern Information Retrieval (IR) systems. However, existing LLM-based approaches face notable limitations, including ranking uncertainty, unstable top-k recovery, and high token cost due to token-intensive prompting. To effectively address these limitations, we propose REALM, an uncertainty-aware re-ranking framework that models LLM-derived relevance as Gaussian distributions and refines them through recursive Bayesian updates. By explicitly capturing uncertainty and minimizing redundant queries, REALM achieves better rankings more efficiently. Experimental results demonstrate that our REALM surpasses state-of-the-art re-rankers while significantly reducing token usage and latency, improving NDCG@10 by 0.7-11.9 and simultaneously reducing the number of LLM inferences by 23.4-84.4%, promoting it as the next-generation re-ranker for modern IR systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.18379v2),  [pdf](http://arxiv.org/pdf/2508.18379v2)

**Tags**: cs.IR 



### Scaling Laws for Optimal Data Mixtures
**Authors**: Mustafa Shukor, Louis Bethune, Dan Busbridge, David Grangier, Enrico Fini, Alaaeldin El-Nouby, Pierre Ablin

**Updated**: 2025-10-02T15:18:02Z

**Summary**: Large foundation models are typically trained on data from multiple domains, with the data mixture--the proportion of each domain used--playing a critical role in model performance. The standard approach to selecting this mixture relies on trial and error, which becomes impractical for large-scale pretraining. We propose a systematic method to determine the optimal data mixture for any target domain using scaling laws. Our approach accurately predicts the loss of a model of size $N$ trained with $D$ tokens and a specific domain weight vector $h$. We validate the universality of these scaling laws by demonstrating their predictive power in three distinct and large-scale settings: large language model (LLM), native multimodal model (NMM), and large vision models (LVM) pretraining. We further show that these scaling laws can extrapolate to new data mixtures and across scales: their parameters can be accurately estimated using a few small-scale training runs, and used to estimate the performance at larger scales and unseen domain weights. The scaling laws allow to derive the optimal domain weights for any target domain under a given training budget ($N$,$D$), providing a principled alternative to costly trial-and-error methods.

**Link**: [arxiv](http://arxiv.org/abs/2507.09404v2),  [pdf](http://arxiv.org/pdf/2507.09404v2)

**Tags**: cs.LG 



### On Predictability of Reinforcement Learning Dynamics for Large Language   Models
**Authors**: Yuchen Cai, Ding Cao, Xin Xu, Zijun Yao, Yuqing Huang, Zhenyu Tan, Benyi Zhang, Guiquan Liu, Junfeng Fang

**Updated**: 2025-10-02T15:16:51Z

**Summary**: Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \textgreater 96\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.00553v2),  [pdf](http://arxiv.org/pdf/2510.00553v2)

**Tags**: cs.LG cs.AI 



### SpurBreast: A Curated Dataset for Investigating Spurious Correlations in   Real-world Breast MRI Classification
**Authors**: Jong Bum Won, Wesley De Neve, Joris Vankerschaver, Utku Ozbulak

**Updated**: 2025-10-02T15:16:20Z

**Summary**: Deep neural networks (DNNs) have demonstrated remarkable success in medical imaging, yet their real-world deployment remains challenging due to spurious correlations, where models can learn non-clinical features instead of meaningful medical patterns. Existing medical imaging datasets are not designed to systematically study this issue, largely due to restrictive licensing and limited supplementary patient data. To address this gap, we introduce SpurBreast, a curated breast MRI dataset that intentionally incorporates spurious correlations to evaluate their impact on model performance. Analyzing over 100 features involving patient, device, and imaging protocol, we identify two dominant spurious signals: magnetic field strength (a global feature influencing the entire image) and image orientation (a local feature affecting spatial alignment). Through controlled dataset splits, we demonstrate that DNNs can exploit these non-clinical signals, achieving high validation accuracy while failing to generalize to unbiased test data. Alongside these two datasets containing spurious correlations, we also provide benchmark datasets without spurious correlations, allowing researchers to systematically investigate clinically relevant and irrelevant features, uncertainty estimation, adversarial robustness, and generalization strategies. Models and datasets are available at https://github.com/utkuozbulak/spurbreast.

**Link**: [arxiv](http://arxiv.org/abs/2510.02109v1),  [pdf](http://arxiv.org/pdf/2510.02109v1)

**Tags**: eess.IV cs.AI cs.CV 



### LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot   Grasping with Ambiguous Instructions
**Authors**: Yunhan Lin, Wenqi Wu, Zhijie Zhang, Huasong Min

**Updated**: 2025-10-02T15:10:26Z

**Summary**: The existing language-driven grasping methods struggle to fully handle ambiguous instructions containing implicit intents. To tackle this challenge, we propose LangGrasp, a novel language-interactive robotic grasping framework. The framework integrates fine-tuned large language models (LLMs) to leverage their robust commonsense understanding and environmental perception capabilities, thereby deducing implicit intents from linguistic instructions and clarifying task requirements along with target manipulation objects. Furthermore, our designed point cloud localization module, guided by 2D part segmentation, enables partial point cloud localization in scenes, thereby extending grasping operations from coarse-grained object-level to fine-grained part-level manipulation. Experimental results show that the LangGrasp framework accurately resolves implicit intents in ambiguous instructions, identifying critical operations and target information that are unstated yet essential for task completion. Additionally, it dynamically selects optimal grasping poses by integrating environmental information. This enables high-precision grasping from object-level to part-level manipulation, significantly enhancing the adaptability and task execution efficiency of robots in unstructured environments. More information and code are available here: https://github.com/wu467/LangGrasp.

**Link**: [arxiv](http://arxiv.org/abs/2510.02104v1),  [pdf](http://arxiv.org/pdf/2510.02104v1)

**Tags**: cs.RO 



### Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing   Unauthorized Sensing
**Authors**: Kawon Han, Kaitao Meng, Christos Masouros

**Updated**: 2025-10-02T15:09:10Z

**Summary**: The deployment of integrated sensing and communication (ISAC) brings along unprecedented vulnerabilities to authorized sensing, necessitating the development of secure solutions. Sensing parameters are embedded within the target-reflected signal leaked to unauthorized passive radar sensing eavesdroppers (Eve), implying that they can silently extract sensory information without prior knowledge of the information data. To overcome this limitation, we propose a sensing-secure ISAC framework that ensures secure target detection and estimation for the legitimate system, while obfuscating unauthorized sensing without requiring any prior knowledge of Eve. By introducing artificial imperfections into the ambiguity function (AF) of ISAC signals, we introduce artificial targets into Eve's range profile which increase its range estimation ambiguity. In contrast, the legitimate sensing receiver (Alice) can suppress these AF artifacts using mismatched filtering, albeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM signal, a structured subcarrier power allocation scheme is designed to shape the secure autocorrelation function (ACF), inserting periodic peaks to mislead Eve's range estimation and degrade target detection performance. To quantify the sensing security, we introduce peak sidelobe level (PSL) and integrated sidelobe level (ISL) as key performance metrics. Then, we analyze the three-way trade-offs between communication, legitimate sensing, and sensing security, highlighting the impact of the proposed sensing-secure ISAC signaling on system performance. We formulate a convex optimization problem to maximize ISAC performance while guaranteeing a certain sensing security level. Numerical results validate the effectiveness of the proposed sensing-secure ISAC signaling, demonstrating its ability to degrade Eve's target estimation while preserving Alice's performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.02103v1),  [pdf](http://arxiv.org/pdf/2510.02103v1)

**Tags**: eess.SP 



### Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and   Reasoning
**Authors**: Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu

**Updated**: 2025-10-02T14:57:13Z

**Summary**: Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.

**Link**: [arxiv](http://arxiv.org/abs/2510.02091v1),  [pdf](http://arxiv.org/pdf/2510.02091v1)

**Tags**: cs.AI 



### Break the ID-Language Barrier: An Adaption Framework for LLM-based   Sequential Recommendation
**Authors**: Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang

**Updated**: 2025-10-02T14:53:04Z

**Summary**: The recent breakthrough of large language models (LLMs) in natural language processing has sparked exploration in recommendation systems, however, their limited domain-specific knowledge remains a critical bottleneck. Specifically, LLMs lack key pieces of information crucial for sequential recommendations, such as user behavior patterns. To address this critical gap, we propose IDLE-Adapter, a novel framework that integrates pre-trained ID embeddings, rich in domain-specific knowledge, into LLMs to improve recommendation accuracy. IDLE-Adapter acts as a bridge, transforming sparse user-item interaction data into dense, LLM-compatible representations through a Pre-trained ID Sequential Model, Dimensionality Alignment, Layer-wise Embedding Refinement, and Layer-wise Distribution Alignment. Furthermore, IDLE-Adapter demonstrates remarkable flexibility by seamlessly integrating ID embeddings from diverse ID-based sequential models and LLM architectures. Extensive experiments across various datasets demonstrate the superiority of IDLE-Adapter, achieving over 10\% and 20\% improvements in HitRate@5 and NDCG@5 metrics, respectively, compared to state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.18262v3),  [pdf](http://arxiv.org/pdf/2411.18262v3)

**Tags**: cs.IR cs.LG 



### ReTabAD: A Benchmark for Restoring Semantic Context in Tabular Anomaly   Detection
**Authors**: Sanghyu Yoon, Dongmin Kim, Suhee Yoon, Ye Seul Sim, Seungdong Yoa, Hye-Seung Cho, Soonyoung Lee, Hankook Lee, Woohyung Lim

**Updated**: 2025-10-02T14:28:45Z

**Summary**: In tabular anomaly detection (AD), textual semantics often carry critical signals, as the definition of an anomaly is closely tied to domain-specific context. However, existing benchmarks provide only raw data points without semantic context, overlooking rich textual metadata such as feature descriptions and domain knowledge that experts rely on in practice. This limitation restricts research flexibility and prevents models from fully leveraging domain knowledge for detection. ReTabAD addresses this gap by restoring textual semantics to enable context-aware tabular AD research. We provide (1) 20 carefully curated tabular datasets enriched with structured textual metadata, together with implementations of state-of-the-art AD algorithms including classical, deep learning, and LLM-based approaches, and (2) a zero-shot LLM framework that leverages semantic context without task-specific training, establishing a strong baseline for future research. Furthermore, this work provides insights into the role and utility of textual metadata in AD through experiments and analysis. Results show that semantic context improves detection performance and enhances interpretability by supporting domain-aware reasoning. These findings establish ReTabAD as a benchmark for systematic exploration of context-aware AD.

**Link**: [arxiv](http://arxiv.org/abs/2510.02060v1),  [pdf](http://arxiv.org/pdf/2510.02060v1)

**Tags**: cs.AI cs.LG 



### LLM-Enhanced, Data-Driven Personalized and Equitable Clinician   Scheduling: A Predict-then-Optimize Approach
**Authors**: Anjali Jha, Wanqing Chen, Maxim Eckmann, Ian Stockwell, Jianwu Wang, Kai Sun

**Updated**: 2025-10-02T14:22:20Z

**Summary**: Clinician scheduling remains a persistent challenge due to limited clinical resources and fluctuating demands. This complexity is especially acute in large academic anesthesiology departments as physicians balance responsibilities across multiple clinical sites with conflicting priorities. Further, scheduling must account for individual clinical and lifestyle preferences to ensure job satisfaction and well-being. Traditional approaches, often based on statistical or rule-based optimization models, rely on structured data and explicit domain knowledge. However, these methods often overlook unstructured information, e.g., free-text notes from routinely administered clinician well-being surveys and scheduling platforms. These notes may reveal implicit and underutilized clinical resources. Neglecting such information can lead to misaligned schedules, increased burnout, overlooked staffing flexibility, and suboptimal utilization of available resources. To address this gap, we propose a predict-then-optimize framework that integrates classification-based clinician availability predictions with a mixed-integer programming schedule optimization model. Large language models (LLMs) are employed to extract actionable preferences and implicit constraints from unstructured schedule notes, enhancing the reliability of availability predictions. These predictions then inform the schedule optimization considering four objectives: first, ensuring clinical full-time equivalent compliance, second, reducing workload imbalances by enforcing equitable proportions of shift types, third, maximizing clinician availability for assigned shifts, and fourth, schedule consistency. By combining the interpretive power of LLMs with the rigor of mathematical optimization, our framework provides a robust, data-driven solution that enhances operational efficiency while supporting equity and clinician well-being.

**Link**: [arxiv](http://arxiv.org/abs/2510.02047v1),  [pdf](http://arxiv.org/pdf/2510.02047v1)

**Tags**: math.OC cs.CE cs.SY eess.SY 



### Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming   Tool Usage
**Authors**: Siddhant Arora, Haidar Khan, Kai Sun, Xin Luna Dong, Sajal Choudhary, Seungwhan Moon, Xinyuan Zhang, Adithya Sagar, Surya Teja Appini, Kaushik Patnaik, Sanat Sharma, Shinji Watanabe, Anuj Kumar, Ahmed Aly, Yue Liu, Florian Metze, Zhaojiang Lin

**Updated**: 2025-10-02T14:18:20Z

**Summary**: End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR-LLM-TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (Streaming RAG), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to issue tool calls during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both accuracy and responsiveness. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our streaming RAG approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our streaming RAG approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.

**Link**: [arxiv](http://arxiv.org/abs/2510.02044v1),  [pdf](http://arxiv.org/pdf/2510.02044v1)

**Tags**: cs.CL cs.SD eess.AS 



### Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation   and Confidence Awareness
**Authors**: Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li

**Updated**: 2025-10-02T14:15:09Z

**Summary**: Machine unlearning techniques aim to mitigate unintended memorization in large language models (LLMs). However, existing approaches predominantly focus on the explicit removal of isolated facts, often overlooking latent inferential dependencies and the non-deterministic nature of knowledge within LLMs. Consequently, facts presumed forgotten may persist implicitly through correlated information. To address these challenges, we propose a knowledge unlearning evaluation framework that more accurately captures the implicit structure of real-world knowledge by representing relevant factual contexts as knowledge graphs with associated confidence scores. We further develop an inference-based evaluation protocol leveraging powerful LLMs as judges; these judges reason over the extracted knowledge subgraph to determine unlearning success. Our LLM judges utilize carefully designed prompts and are calibrated against human evaluations to ensure their trustworthiness and stability. Extensive experiments on our newly constructed benchmark demonstrate that our framework provides a more realistic and rigorous assessment of unlearning performance. Moreover, our findings reveal that current evaluation strategies tend to overestimate unlearning effectiveness. Our code is publicly available at https://github.com/Graph-COM/Knowledge_Unlearning.git.

**Link**: [arxiv](http://arxiv.org/abs/2506.05735v3),  [pdf](http://arxiv.org/pdf/2506.05735v3)

**Tags**: cs.CL cs.LG 



### Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with   Receptive-Field-Aware Attention Weighting
**Authors**: Han Zhang, Yu Lu, Liyun Zhang, Dian Ding, Dinghua Zhao, Yi-Chao Chen, Ye Wu, Guangtao Xue

**Updated**: 2025-10-02T14:13:37Z

**Summary**: Understanding the emotions in a dialogue usually requires external knowledge to accurately understand the contents. As the LLMs become more and more powerful, we do not want to settle on the limited ability of the pre-trained language model. However, the LLMs either can only process text modality or are too expensive to process the multimedia information. We aim to utilize both the power of LLMs and the supplementary features from the multimedia modalities. In this paper, we present a framework, Lantern, that can improve the performance of a certain vanilla model by prompting large language models with receptive-field-aware attention weighting. This framework trained a multi-task vanilla model to produce probabilities of emotion classes and dimension scores. These predictions are fed into the LLMs as references to adjust the predicted probabilities of each emotion class with its external knowledge and contextual understanding. We slice the dialogue into different receptive fields, and each sample is included in exactly t receptive fields. Finally, the predictions of LLMs are merged with a receptive-field-aware attention-driven weighting module. In the experiments, vanilla models CORECT and SDT are deployed in Lantern with GPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way settings demonstrated that the Lantern can significantly improve the performance of current vanilla models by up to 1.23% and 1.80%.

**Link**: [arxiv](http://arxiv.org/abs/2411.17674v2),  [pdf](http://arxiv.org/pdf/2411.17674v2)

**Tags**: cs.CL 



### Investigating ReLoRA: Effects on the Learning Dynamics of Small Language   Models
**Authors**: Yuval Weiss, David Demitri Africa, Paula Buttery, Richard Diehl Martinez

**Updated**: 2025-10-02T14:09:44Z

**Summary**: Parameter-efficient methods like LoRA have revolutionised large language model (LLM) fine-tuning. ReLoRA extends this idea to pretraining by repeatedly merging and reinitialising low-rank adapters, increasing cumulative rank while keeping updates cheap. This aligns well with observations that high-capacity models learn through locally low-rank trajectories that expand over time. By contrast, recent work suggests that small language models (SLMs) exhibit rank deficiencies and under-utilise their available dimensionality. This raises a natural question: can ReLoRA's rank-expanding update rule \textit{steer} SLMs toward healthier learning dynamics, mitigating rank bottlenecks in a capacity-constrained regime? We argue SLMs are an ideal testbed: they train quickly, enable controlled ablations, and make rank phenomena more measurable. We present the first systematic study of ReLoRA in SLMs (11M-66M parameters), evaluating both performance and learning dynamics. Across loss, Paloma perplexity, and BLiMP, we find that ReLoRA underperforms full-rank training, with gaps widening at larger scales. Analysis of proportional effective rank and condition numbers shows that ReLoRA amplifies existing rank deficiencies and induces ill-conditioned updates early in training. Our results suggest that while ReLoRA's merge-and-restart strategy can expand ranks in larger models, it does not straightforwardly translate to capacity-limited SLMs, motivating adaptive-rank or hybrid-rank approaches for low-compute pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2509.12960v2),  [pdf](http://arxiv.org/pdf/2509.12960v2)

**Tags**: cs.CL cs.AI 



### QSpec: Speculative Decoding with Complementary Quantization Schemes
**Authors**: Juntao Zhao, Wenhao Lu, Sheng Wang, Lingpeng Kong, Chuan Wu

**Updated**: 2025-10-02T14:09:03Z

**Summary**: Quantization is widely adopted to accelerate inference and reduce memory consumption in large language models (LLMs). While activation-weight joint quantization enables efficient low-precision decoding, it suffers from substantial performance degradation on multi-step reasoning tasks. We propose QSpec, a novel quantization paradigm that decouples efficiency from quality by integrating two complementary schemes via speculative decoding: low-precision joint quantization for fast drafting and high-precision weight-only quantization for accurate verification. QSpec reuses both weights and KV cache across stages, enabling near-zero-cost switching without retraining or auxiliary models. Compared to high-precision baselines, QSpec achieves up to 1.64x speedup without quality degradation, and outperforms state-of-the-art speculative decoding methods by up to 1.55x in batched settings. Furthermore, QSpec supports plug-and-play deployment and generalizes well across model scales, quantization methods, and workloads. These properties make QSpec a practical and scalable solution for high-fidelity quantized LLM serving under memory-constrained scenarios. Our code is available at https://github.com/hku-netexplo-lab/QSpec.

**Link**: [arxiv](http://arxiv.org/abs/2410.11305v3),  [pdf](http://arxiv.org/pdf/2410.11305v3)

**Tags**: cs.LG cs.AI 



### Flexible Feature Distillation for Large Language Models
**Authors**: Khouloud Saadi, Di Wang

**Updated**: 2025-10-02T14:05:51Z

**Summary**: Knowledge distillation (KD) has become a cornerstone for compressing large language models (LLMs). However, existing LLM-KD methods have primarily focused on logit-based approaches, which achieve good performance but overlook the rich internal representations of LLMs. Feature-level KD could leverage this structure to provide complementary benefits, yet it remains underexplored because current feature-KD approaches typically assume identical teacher-student hidden sizes, a restrictive and unrealistic assumption. A common workaround is to train a linear projector to align their feature spaces; however, this introduces additional parameters, distorts teacher embeddings, and often degrades downstream performance, especially in generative tasks. We propose Flex-KD, a parameter-free framework for task-driven feature distillation for LLMs. Instead of projecting the entire teacher representation, Flex-KD uses gradient-based scores to identify the most task-relevant dimensions of the teacher's hidden states and distills only this subspace into the student. This ensures that the student's limited capacity is allocated to informative components, while avoiding projector-induced distortion and extra parameters. Flex-KD integrates seamlessly with existing KD pipelines and supports differing teacher-student hidden sizes. Extensive experiments across both classification and generative tasks, i.e., instruction-following and summarization, show that Flex-KD consistently boosts student performance, achieving up to a 3.75 percent performance gain over the linear projection baseline.

**Link**: [arxiv](http://arxiv.org/abs/2507.10155v2),  [pdf](http://arxiv.org/pdf/2507.10155v2)

**Tags**: cs.CL 



### CrediBench: Building Web-Scale Network Datasets for Information   Integrity
**Authors**: Emma Kondrup, Sebastian Sabry, Hussein Abdallah, Zachary Yang, James Zhou, Kellin Pelrine, Jean-FranÃ§ois Godbout, Michael M. Bronstein, Reihaneh Rabbany, Shenyang Huang

**Updated**: 2025-10-02T14:03:57Z

**Summary**: Online misinformation poses an escalating threat, amplified by the Internet's open nature and increasingly capable LLMs that generate persuasive yet deceptive content. Existing misinformation detection methods typically focus on either textual content or network structure in isolation, failing to leverage the rich, dynamic interplay between website content and hyperlink relationships that characterizes real-world misinformation ecosystems. We introduce CrediBench: a large-scale data processing pipeline for constructing temporal web graphs that jointly model textual content and hyperlink structure for misinformation detection. Unlike prior work, our approach captures the dynamic evolution of general misinformation domains, including changes in both content and inter-site references over time. Our processed one-month snapshot extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, representing the largest web graph dataset made publicly available for misinformation research to date. From our experiments on this graph snapshot, we demonstrate the strength of both structural and webpage content signals for learning credibility scores, which measure source reliability. The pipeline and experimentation code are all available here, and the dataset is in this folder.

**Link**: [arxiv](http://arxiv.org/abs/2509.23340v3),  [pdf](http://arxiv.org/pdf/2509.23340v3)

**Tags**: cs.SI cs.DC cs.LG 



### Charting the Landscape of African NLP: Mapping Progress and Shaping the   Road Ahead
**Authors**: Jesujoba O. Alabi, Michael A. Hedderich, David Ifeoluwa Adelani, Dietrich Klakow

**Updated**: 2025-10-02T14:01:14Z

**Summary**: With over 2,000 languages and potentially millions of speakers, Africa represents one of the richest linguistic regions in the world. Yet, this diversity is scarcely reflected in state-of-the-art natural language processing (NLP) systems and large language models (LLMs), which predominantly support a narrow set of high-resource languages. This exclusion not only limits the reach and utility of modern NLP technologies but also risks widening the digital divide across linguistic communities. Nevertheless, NLP research on African languages is active and growing. In recent years, there has been a surge of interest in this area, driven by several factors-including the creation of multilingual language resources, the rise of community-led initiatives, and increased support through funding programs. In this survey, we analyze 884 research papers on NLP for African languages published over the past five years, offering a comprehensive overview of recent progress across core tasks. We identify key trends shaping the field and conclude by outlining promising directions to foster more inclusive and sustainable NLP research for African languages.

**Link**: [arxiv](http://arxiv.org/abs/2505.21315v3),  [pdf](http://arxiv.org/pdf/2505.21315v3)

**Tags**: cs.CL 



### GeoSQL-Eval: First Evaluation of LLMs on PostGIS-Based NL2GeoSQL Queries
**Authors**: Shuyang Hou, Haoyue Jiao, Ziqi Liu, Lutong Xie, Guanyu Chen, Shaowen Wu, Xuefeng Guan, Huayi Wu

**Updated**: 2025-10-02T13:58:56Z

**Summary**: Large language models (LLMs) have shown strong performance in natural language to SQL (NL2SQL) tasks within general databases. However, extending to GeoSQL introduces additional complexity from spatial data types, function invocation, and coordinate systems, which greatly increases generation and execution difficulty. Existing benchmarks mainly target general SQL, and a systematic evaluation framework for GeoSQL is still lacking. To fill this gap, we present GeoSQL-Eval, the first end-to-end automated evaluation framework for PostGIS query generation, together with GeoSQL-Bench, a benchmark for assessing LLM performance in NL2GeoSQL tasks. GeoSQL-Bench defines three task categories-conceptual understanding, syntax-level SQL generation, and schema retrieval-comprising 14,178 instances, 340 PostGIS functions, and 82 thematic databases. GeoSQL-Eval is grounded in Webb's Depth of Knowledge (DOK) model, covering four cognitive dimensions, five capability levels, and twenty task types to establish a comprehensive process from knowledge acquisition and syntax generation to semantic alignment, execution accuracy, and robustness. We evaluate 24 representative models across six categories and apply the entropy weight method with statistical analyses to uncover performance differences, common error patterns, and resource usage. Finally, we release a public GeoSQL-Eval leaderboard platform for continuous testing and global comparison. This work extends the NL2GeoSQL paradigm and provides a standardized, interpretable, and extensible framework for evaluating LLMs in spatial database contexts, offering valuable references for geospatial information science and related applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.25264v2),  [pdf](http://arxiv.org/pdf/2509.25264v2)

**Tags**: cs.DB cs.AI cs.LG cs.SE 



### Style Over Story: A Process-Oriented Study of Authorial Creativity in   Large Language Models
**Authors**: Donghoon Jung, Jiwoo Choi, Songeun Chae, Seohyon Jung

**Updated**: 2025-10-02T13:57:14Z

**Summary**: Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity.

**Link**: [arxiv](http://arxiv.org/abs/2510.02025v1),  [pdf](http://arxiv.org/pdf/2510.02025v1)

**Tags**: cs.CL 



### Design and Application of Multimodal Large Language Model Based System   for End to End Automation of Accident Dataset Generation
**Authors**: MD Thamed Bin Zaman Chowdhury, Moazzem Hossain

**Updated**: 2025-10-02T13:50:05Z

**Summary**: Road traffic accidents remain a major public safety and socio-economic issue in developing countries like Bangladesh. Existing accident data collection is largely manual, fragmented, and unreliable, resulting in underreporting and inconsistent records. This research proposes a fully automated system using Large Language Models (LLMs) and web scraping techniques to address these challenges. The pipeline consists of four components: automated web scraping code generation, news collection from online sources, accident news classification with structured data extraction, and duplicate removal. The system uses the multimodal generative LLM Gemini-2.0-Flash for seamless automation. The code generation module classifies webpages into pagination, dynamic, or infinite scrolling categories and generates suitable Python scripts for scraping. LLMs also classify and extract key accident information such as date, time, location, fatalities, injuries, road type, vehicle types, and pedestrian involvement. A deduplication algorithm ensures data integrity by removing duplicate reports. The system scraped 14 major Bangladeshi news sites over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news articles and identifying 705 unique accidents. The code generation module achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the highest number of accidents (80), fatalities (70), and injuries (115), followed by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning (8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also developed with usage instructions. This study demonstrates the viability of an LLM-powered, scalable system for accurate, low-effort accident data collection, providing a foundation for data-driven road safety policymaking in Bangladesh.

**Link**: [arxiv](http://arxiv.org/abs/2505.00015v2),  [pdf](http://arxiv.org/pdf/2505.00015v2)

**Tags**: cs.CL 



### Programming Distributed Collective Processes in the eXchange Calculus
**Authors**: Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Gianluca Torta, Mirko Viroli

**Updated**: 2025-10-02T13:35:00Z

**Summary**: Recent trends like the Internet of Things (IoT) suggest a vision of dense and multi-scale deployments of computing devices in nearly all kinds of environments. A prominent engineering challenge revolves around programming the collective adaptive behaviour of such computational ecosystems. This requires abstractions able to capture concepts like ensembles (dynamic groups of cooperating devices) and collective tasks (joint activities carried out by ensembles). In this work, we consider collections of devices interacting with neighbours and that execute in nearly-synchronised sense-compute-interact rounds, where the computation is given by a single program mapping sensing values and incoming messages to output and outcoming messages. To support programming whole computational collectives, we propose the abstraction of a distributed collective process, which can be used to define at once the ensemble formation logic and its collective task. We formalise the abstraction in the eXchange Calculus (XC), a core functional language based on neighbouring values (maps from neighbours to values) where state and interaction is handled through a single primitive, exchange, and provide a corresponding implementation in the FCPP language. Then, we exercise distributed collective processes using two case studies: multi-hop message propagation and distributed monitoring of spatial properties. Finally, we discuss the features of the abstraction and its suitability for different kinds of distributed computing applications.

**Link**: [arxiv](http://arxiv.org/abs/2401.11212v6),  [pdf](http://arxiv.org/pdf/2401.11212v6)

**Tags**: cs.DC cs.AI cs.MA cs.PL 



### Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing   Routing in Lightweight Image Super-Resolution
**Authors**: Junyu Wu, Jie Tang, Jie Liu, Gangshan Wu

**Updated**: 2025-10-02T13:18:43Z

**Summary**: Image Super-Resolution (SR) aims to reconstruct high-resolution images from low-resolution counterparts, but the computational complexity of deep learning-based methods often hinders practical deployment. CAMixer is the pioneering work to integrate the advantages of existing lightweight SR methods and proposes a content-aware mixer to route token mixers of varied complexities according to the difficulty of content recovery. However, several limitations remain, such as poor adaptability, coarse-grained masking and spatial inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking mechanism that identifies pure pixels and exempts them from expensive computations. PP utilizes fixed color center points to classify pixels into distinct categories, enabling fine-grained, spatially flexible masking while maintaining adaptive flexibility. Integrated into the state-of-the-art ATD-light model, PP-ATD-light achieves superior SR performance with minimal overhead, outperforming CAMixer-ATD-light in reconstruction quality and parameter efficiency when saving a similar amount of computation.

**Link**: [arxiv](http://arxiv.org/abs/2510.01997v1),  [pdf](http://arxiv.org/pdf/2510.01997v1)

**Tags**: cs.CV 



### LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and   Target
**Authors**: Md Arid Hasan, Firoj Alam, Md Fahad Hossain, Usman Naseem, Syed Ishtiaque Ahmed

**Updated**: 2025-10-02T13:17:11Z

**Summary**: Online social media platforms are central to everyday communication and information seeking. While these platforms serve positive purposes, they also provide fertile ground for the spread of hate speech, offensive language, and bullying content targeting individuals, organizations, and communities. Such content undermines safety, participation, and equity online. Reliable detection systems are therefore needed, especially for low-resource languages where moderation tools are limited. In Bangla, prior work has contributed resources and models, but most are single-task (e.g., binary hate/offense) with limited coverage of multi-facet signals (type, severity, target). We address these gaps by introducing the first multi-task Bangla hate-speech dataset, BanglaMultiHate, one of the largest manually annotated corpus to date. Building on this resource, we conduct a comprehensive, controlled comparison spanning classical baselines, monolingual pretrained models, and LLMs under zero-shot prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs are competitive with BanglaBERT, culturally and linguistically grounded pretraining remains critical for robust performance. Together, our dataset and findings establish a stronger benchmark for developing culturally aligned moderation tools in low-resource contexts. For reproducibility, we will release the dataset and all related scripts.

**Link**: [arxiv](http://arxiv.org/abs/2510.01995v1),  [pdf](http://arxiv.org/pdf/2510.01995v1)

**Tags**: cs.CL 68T50 F.2.2; I.2.7 



### Clarifying Semantics of In-Context Examples for Unit Test Generation
**Authors**: Chen Yang, Lin Yang, Ziqi Wang, Dong Wang, Jianyi Zhou, Junjie Chen

**Updated**: 2025-10-02T13:15:40Z

**Summary**: Recent advances in large language models (LLMs) have enabled promising performance in unit test generation through in-context learning (ICL). However, the quality of in-context examples significantly influences the effectiveness of generated tests-poorly structured or semantically unclear test examples often lead to suboptimal outputs. In this paper, we propose CLAST, a novel technique that systematically refines unit tests to improve their semantic clarity, thereby enhancing their utility as in-context examples. The approach decomposes complex tests into logically clearer ones and improves semantic clarity through a combination of program analysis and LLM-based rewriting. We evaluated CLAST on four open-source and three industrial projects. The results demonstrate that CLAST largely outperforms UTgen, the state-of-the-art refinement technique, in both preserving test effectiveness and enhancing semantic clarity. Specifically, CLAST fully retains the original effectiveness of unit tests, while UTgen reduces compilation success rate (CSR), pass rate (PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%, 35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user study preferred the semantic clarity of CLAST-refined tests. Notably, incorporating CLAST-refined tests as examples effectively improves ICL-based unit test generation approaches such as RAGGen and TELPA, resulting in an average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for generated tests, compared to incorporating UTgen-refined tests. The insights from the follow-up user study not only reinforce CLAST's potential impact in software testing practice but also illuminate avenues for future research.

**Link**: [arxiv](http://arxiv.org/abs/2510.01994v1),  [pdf](http://arxiv.org/pdf/2510.01994v1)

**Tags**: cs.SE cs.AI 



### The Hidden Costs of Translation Accuracy: Distillation, Quantization,   and Environmental Impact
**Authors**: Dhaathri Vijay, Anandaswarup Vadapalli

**Updated**: 2025-10-02T13:15:14Z

**Summary**: The rapid expansion of large language models (LLMs) has heightened concerns about their computational and environmental costs. This study investigates the trade-offs between translation quality and efficiency by comparing full-scale, distilled, and quantized models using machine translation as a case study. We evaluated performance on the Flores+ benchmark and through human judgments of conversational translations in French, Hindi, and Kannada. Our analysis revealed that the full 3.3B FP32 model, while achieving the highest BLEU scores, incurred the largest environmental footprint (~ 0.007-0.008 kg CO2 per run). The distilled 600M FP32 model reduced inference time by 71-78% and carbon emissions by 63-65% compared with the full model, with only minimal reductions in BLEU scores. Human evaluations further showed that even aggressive quantization (INT4) preserved high levels of accuracy and fluency, with differences between models generally minor. These findings demonstrate that model compression strategies can substantially reduce computational demands and environmental impact while maintaining competitive translation quality, though trade-offs are more pronounced in low-resource settings. We argue for evaluation frameworks that integrate efficiency and sustainability alongside accuracy as central dimensions of progress in NLP.

**Link**: [arxiv](http://arxiv.org/abs/2509.23990v2),  [pdf](http://arxiv.org/pdf/2509.23990v2)

**Tags**: cs.CL cs.AI 



### 4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing
**Authors**: Lei Liu, Can Wang, Zhenghao Chen, Dong Xu

**Updated**: 2025-10-02T13:13:19Z

**Summary**: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2510.01991v1),  [pdf](http://arxiv.org/pdf/2510.01991v1)

**Tags**: cs.CV 



### LEXam: Benchmarking Legal Reasoning on 340 Law Exams
**Authors**: Yu Fan, Jingwei Ni, Jakob Merane, Yang Tian, Yoan HermstrÃ¼wer, Yinya Huang, Mubashara Akhtar, Etienne Salimbeni, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus

**Updated**: 2025-10-02T13:09:42Z

**Summary**: Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. To address this, we introduce \textsc{LEXam}, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Deploying an ensemble LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately, closely aligning with human expert assessments. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. We have open-sourced our code on \href{https://github.com/LEXam-Benchmark/LEXam}{GitHub} and released our data on \href{https://huggingface.co/datasets/LEXam-Benchmark/LEXam}{Hugging Face}. Project page: https://lexam-benchmark.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2505.12864v4),  [pdf](http://arxiv.org/pdf/2505.12864v4)

**Tags**: cs.CL cs.AI cs.LG 68T50 I.2 



### MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance
**Authors**: Xingjian Zhao, Zhe Xu, Qinyuan Cheng, Zhaoye Fei, Luozhijie Jin, Yang Wang, Hanfu Chen, Yaozhou Jiang, Qinghui Gao, Ke Chen, Ruixiao Li, Mingshu Chen, Ruiming Wang, Wenbo Zhang, Yiyang Zhang, Donghua Yu, Yang Gao, Xiaogui Yang, Yitian Gong, Yuanfan Xu, Yaqian Zhou, Xuanjing Huang, Xipeng Qiu

**Updated**: 2025-10-02T13:05:41Z

**Summary**: Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.

**Link**: [arxiv](http://arxiv.org/abs/2510.00499v2),  [pdf](http://arxiv.org/pdf/2510.00499v2)

**Tags**: cs.CL cs.AI 



### The Data-Quality Illusion: Rethinking Classifier-Based Quality Filtering   for LLM Pretraining
**Authors**: Thiziri Nait Saada, Louis Bethune, Michal Klein, David Grangier, Marco Cuturi, Pierre Ablin

**Updated**: 2025-10-02T12:56:04Z

**Summary**: Large-scale models are pretrained on massive web-crawled datasets containing documents of mixed quality, making data filtering essential. A popular method is Classifier-based Quality Filtering (CQF), which trains a binary classifier to distinguish between pretraining data and a small, high-quality set. It assigns each pretraining document a quality score defined as the classifier's score and retains only the top-scoring ones. We provide an in-depth analysis of CQF. We show that while CQF improves downstream task performance, it does not necessarily enhance language modeling on the high-quality dataset. We explain this paradox by the fact that CQF implicitly filters the high-quality dataset as well. We further compare the behavior of models trained with CQF to those trained on synthetic data of increasing quality, obtained via random token permutations, and find starkly different trends. Our results challenge the view that CQF captures a meaningful notion of data quality.

**Link**: [arxiv](http://arxiv.org/abs/2510.00866v2),  [pdf](http://arxiv.org/pdf/2510.00866v2)

**Tags**: cs.LG cs.CL 



### ZK-WAGON: Imperceptible Watermark for Image Generation Models using   ZK-SNARKs
**Authors**: Aadarsh Anantha Ramakrishnan, Shubham Agarwal, Selvanayagam S, Kunwar Singh

**Updated**: 2025-10-02T12:39:57Z

**Summary**: As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.01967v1),  [pdf](http://arxiv.org/pdf/2510.01967v1)

**Tags**: cs.CR cs.AI cs.CV 



### More Thought, Less Accuracy? On the Dual Nature of Reasoning in   Vision-Language Models
**Authors**: Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, Jing Zhang

**Updated**: 2025-10-02T12:24:56Z

**Summary**: Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model's reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: https://xytian1008.github.io/VAPO/

**Link**: [arxiv](http://arxiv.org/abs/2509.25848v2),  [pdf](http://arxiv.org/pdf/2509.25848v2)

**Tags**: cs.CV cs.AI 



### Foundation Models for Software Engineering of Cyber-Physical Systems:   the Road Ahead
**Authors**: Chengjie Lu, Pablo Valle, Jiahui Wu, Erblin Isaku, Hassan Sartaj, Aitor Arrieta, Shaukat Ali

**Updated**: 2025-10-02T12:24:51Z

**Summary**: FMs, particularly LLMs, are increasingly used to support various software engineering activities (e.g., coding and testing). Their applications in the software engineering of CPSs are also growing. However, research in this area remains limited. Moreover, existing studies have primarily focused on LLMs-only one type of FM-leaving ample opportunities to explore others, such as vision-language models. We argue that, in addition to LLMs, other FMs utilizing different data modalities (e.g., images, audio) and multimodal models (which integrate multiple modalities) hold great potential for supporting CPS software engineering, given that these systems process diverse data types. To address this, we present a research roadmap for integrating FMs into various phases of CPS software engineering, highlighting key research opportunities and challenges for the software engineering community. Moreover, we discuss the common challenges associated with applying FMs in this context, including the correctness of FM-generated artifacts, as well as the inherent uncertainty and hallucination associated with FMs. This roadmap is intended for researchers and practitioners in CPS software engineering, providing future research directions using FMs in this domain.

**Link**: [arxiv](http://arxiv.org/abs/2504.04630v2),  [pdf](http://arxiv.org/pdf/2504.04630v2)

**Tags**: cs.SE 



### Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in   MLLMs
**Authors**: Yongyi Su, Haojie Zhang, Shijie Li, Nanqing Liu, Jingyi Liao, Junyi Pan, Yuan Liu, Xiaofen Xing, Chong Sun, Chen Li, Nancy F. Chen, Shuicheng Yan, Xulei Yang, Xun Xu

**Updated**: 2025-10-02T12:23:57Z

**Summary**: Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

**Link**: [arxiv](http://arxiv.org/abs/2510.01954v1),  [pdf](http://arxiv.org/pdf/2510.01954v1)

**Tags**: cs.CV 



### Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection
**Authors**: Tairan Huang, Yili Wang, Qiutong Li, Changlong He, Jianliang Gao

**Updated**: 2025-10-02T11:55:50Z

**Summary**: Graph fraud detection has garnered significant attention as Graph Neural Networks (GNNs) have proven effective in modeling complex relationships within multimodal data. However, existing graph fraud detection methods typically use preprocessed node embeddings and predefined graph structures to reveal fraudsters, which ignore the rich semantic cues contained in raw textual information. Although Large Language Models (LLMs) exhibit powerful capabilities in processing textual information, it remains a significant challenge to perform multimodal fusion of processed textual embeddings with graph structures. In this paper, we propose a \textbf{M}ulti-level \textbf{L}LM \textbf{E}nhanced Graph Fraud \textbf{D}etection framework called MLED. In MLED, we utilize LLMs to extract external knowledge from textual information to enhance graph fraud detection methods. To integrate LLMs with graph structure information and enhance the ability to distinguish fraudsters, we design a multi-level LLM enhanced framework including type-level enhancer and relation-level enhancer. One is to enhance the difference between the fraudsters and the benign entities, the other is to enhance the importance of the fraudsters in different relations. The experiments on four real-world datasets show that MLED achieves state-of-the-art performance in graph fraud detection as a generalized framework that can be applied to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2507.11997v2),  [pdf](http://arxiv.org/pdf/2507.11997v2)

**Tags**: cs.LG cs.AI 



### Software Engineering for Self-Adaptive Robotics: A Research Agenda
**Authors**: Hassan Sartaj, Shaukat Ali, Ana Cavalcanti, Lukas Esterle, ClÃ¡udio Gomes, Peter Gorm Larsen, Anastasios Tefas, Jim Woodcock, Houxiang Zhang

**Updated**: 2025-10-02T11:52:44Z

**Summary**: Self-adaptive robotic systems operate autonomously in dynamic and uncertain environments, requiring robust real-time monitoring and adaptive behaviour. Unlike traditional robotic software with predefined logic, self-adaptive robots exploit artificial intelligence (AI), machine learning, and model-driven engineering to adapt continuously to changing conditions, thereby ensuring reliability, safety, and optimal performance. This paper presents a research agenda for software engineering in self-adaptive robotics, structured along two dimensions. The first concerns the software engineering lifecycle, requirements, design, development, testing, and operations, tailored to the challenges of self-adaptive robotics. The second focuses on enabling technologies such as digital twins, AI-driven adaptation, and quantum computing, which support runtime monitoring, fault detection, and automated decision-making. We identify open challenges, including verifying adaptive behaviours under uncertainty, balancing trade-offs between adaptability, performance, and safety, and integrating self-adaptation frameworks like MAPE-K/MAPLE-K. By consolidating these challenges into a roadmap toward 2030, this work contributes to the foundations of trustworthy and efficient self-adaptive robotic systems capable of meeting the complexities of real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2505.19629v2),  [pdf](http://arxiv.org/pdf/2505.19629v2)

**Tags**: cs.SE cs.RO 



### Veri-R1: Toward Precise and Faithful Claim Verification via Online   Reinforcement Learning
**Authors**: Qi He, Cheng Qian, Xiusi Chen, Bingxiang He, Yi R., Fung, Heng Ji

**Updated**: 2025-10-02T11:49:48Z

**Summary**: Claim verification with large language models (LLMs) has recently attracted considerable attention, owing to their superior reasoning capabilities and transparent verification pathways compared to traditional answer-only judgments. Online claim verification requires iterative evidence retrieval and reasoning, yet existing approaches mainly rely on prompt engineering or predesigned reasoning workflows without offering a unified training paradigm to improve necessary skills. Therefore, we introduce Veri-R1, an online reinforcement learning (RL) framework that enables an LLM to interact with a search engine and to receive reward signals that explicitly shape its planning, retrieval, and reasoning behaviors. The dynamic interaction between models and retrieval systems more accurately reflects real-world verification scenarios and fosters comprehensive verification skills. Empirical results show that Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often surpassing larger-scale counterparts. Ablation studies further reveal the impact of reward components and the link between output logits and label accuracy. Our results highlight the effectiveness of online RL for precise and faithful claim verification and provide a foundation for future research. We release our code to support community progress in LLM empowered claim verification.

**Link**: [arxiv](http://arxiv.org/abs/2510.01932v1),  [pdf](http://arxiv.org/pdf/2510.01932v1)

**Tags**: cs.CL 



### Inverse Language Modeling towards Robust and Grounded LLMs
**Authors**: Davide Gabrielli, Simone Sestito, Iacopo Masi

**Updated**: 2025-10-02T11:47:18Z

**Summary**: The current landscape of defensive mechanisms for LLMs is fragmented and underdeveloped, unlike prior work on classifiers. To further promote adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a unified framework that simultaneously 1) improves the robustness of LLMs to input perturbations, and, at the same time, 2) enables native grounding by inverting model outputs to identify potentially toxic or unsafe input triggers. ILM transforms LLMs from static generators into analyzable and robust systems, potentially helping RED teaming. ILM can lay the foundation for next-generation LLMs that are not only robust and grounded but also fundamentally more controllable and trustworthy. The code is publicly available at github.com/davegabe/pag-llm.

**Link**: [arxiv](http://arxiv.org/abs/2510.01929v1),  [pdf](http://arxiv.org/pdf/2510.01929v1)

**Tags**: cs.CL 



### Enhancing Large Language Model Reasoning with Reward Models: An   Analytical Survey
**Authors**: Qiyuan Liu, Hao Xu, Xuhong Chen, Wei Chen, Yee Whye Teh, Ning Miao

**Updated**: 2025-10-02T11:42:17Z

**Summary**: Reward models (RMs) play a critical role in enhancing the reasoning performance of LLMs. For example, they can provide training signals to finetune LLMs during reinforcement learning (RL) and help select the best answer from multiple candidates during inference. In this paper, we provide a systematic introduction to RMs, along with a comprehensive survey of their applications in LLM reasoning. We first review fundamental concepts of RMs, including their architectures, training methodologies, and evaluation techniques. Then, we explore their key applications: (1) guiding generation and selecting optimal outputs during LLM inference, (2) facilitating data synthesis and iterative self-improvement for LLMs, and (3) providing training signals in RL-based finetuning. Finally, we address critical open questions regarding the selection, generalization, evaluation, and enhancement of RMs, based on existing research and our own empirical findings. Our analysis aims to provide actionable insights for the effective deployment and advancement of RMs for LLM reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.01925v1),  [pdf](http://arxiv.org/pdf/2510.01925v1)

**Tags**: cs.CL 



### To Mask or to Mirror: Human-AI Alignment in Collective Reasoning
**Authors**: Crystal Qian, Aaron Parisi, ClÃ©mentine Bouleau, Vivian Tsai, MaÃ«l Lebreton, Lucas Dixon

**Updated**: 2025-10-02T11:41:30Z

**Summary**: As large language models (LLMs) are increasingly used to model and augment collective decision-making, it is critical to examine their alignment with human social reasoning. We present an empirical framework for assessing collective alignment, in contrast to prior work on the individual level. Using the Lost at Sea social psychology task, we conduct a large-scale online experiment (N=748), randomly assigning groups to leader elections with either visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We then simulate matched LLM groups conditioned on the human data, benchmarking Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some mirror human biases; others mask these biases and attempt to compensate for them. We empirically demonstrate that human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases. Understanding how LLMs align with collective human behavior is critical to advancing socially-aligned AI, and demands dynamic benchmarks that capture the complexities of collective reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.01924v1),  [pdf](http://arxiv.org/pdf/2510.01924v1)

**Tags**: cs.AI cs.MA 



### Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase   Heuristics for 2D Bin-Packing
**Authors**: Syed Mahbubul Huq, Daniel Brito, Daniel Sikar, Chris Child, Tillman Weyde, Rajesh Mojumder

**Updated**: 2025-10-02T11:37:39Z

**Summary**: This paper presents an evaluation framework for assessing Large Language Models' (LLMs) capabilities in combinatorial optimization, specifically addressing the 2D bin-packing problem. We introduce a systematic methodology that combines LLMs with evolutionary algorithms to generate and refine heuristic solutions iteratively. Through comprehensive experiments comparing LLM generated heuristics against traditional approaches (Finite First-Fit and Hybrid First-Fit), we demonstrate that LLMs can produce more efficient solutions while requiring fewer computational resources. Our evaluation reveals that GPT-4o achieves optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78 to 0.83. This work contributes to understanding LLM evaluation in specialized domains and establishes benchmarks for assessing LLM performance in combinatorial optimization tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.22255v3),  [pdf](http://arxiv.org/pdf/2509.22255v3)

**Tags**: cs.AI 



### Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes   Correct Reasoning in Base LLMs
**Authors**: Xumeng Wen, Zihan Liu, Shun Zheng, Shengyu Ye, Zhirong Wu, Yang Wang, Zhijian Xu, Xiao Liang, Junjie Li, Ziming Miao, Jiang Bian, Mao Yang

**Updated**: 2025-10-02T11:31:47Z

**Summary**: Recent advancements in long chain-of-thought (CoT) reasoning, particularly through the Group Relative Policy Optimization algorithm used by DeepSeek-R1, have led to significant interest in the potential of Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Models (LLMs). While RLVR promises to improve reasoning by allowing models to learn from free exploration, there remains debate over whether it truly enhances reasoning abilities or simply boosts sampling efficiency. This paper systematically investigates the impact of RLVR on LLM reasoning. We revisit Pass@K experiments and demonstrate that RLVR can extend the reasoning boundary for both mathematical and coding tasks. This is supported by our introduction of a novel evaluation metric, CoT-Pass@K, which captures reasoning success by accounting for both the final answer and intermediate reasoning steps. Furthermore, we present a theoretical framework explaining RLVR's incentive mechanism, demonstrating how it can encourage correct reasoning even when rewards are based solely on answer correctness. Our analysis of RLVR's training dynamics reveals that it incentivizes correct reasoning early in the process, with substantial improvements in reasoning quality confirmed through extensive evaluations. These findings provide strong evidence of RLVR's potential to enhance LLM reasoning, offering valuable insights into its mechanisms and performance improvements.

**Link**: [arxiv](http://arxiv.org/abs/2506.14245v2),  [pdf](http://arxiv.org/pdf/2506.14245v2)

**Tags**: cs.AI cs.CL 



### Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under   Deficiencies with Iterative Refinement
**Authors**: Zhaoyan Wang, Zheng Gao, Arogya Kharel, In-Young Ko

**Updated**: 2025-10-02T11:30:51Z

**Summary**: Graph Neural Networks (GNNs) are widely adopted in Web-related applications, serving as a core technique for learning from graph-structured data, such as text-attributed graphs. Yet in real-world scenarios, such graphs exhibit deficiencies that substantially undermine GNN performance. While prior GNN-based augmentation studies have explored robustness against individual imperfections, a systematic understanding of how graph-native and Large Language Models (LLMs) enhanced methods behave under compound deficiencies is still missing. Specifically, there has been no comprehensive investigation comparing conventional approaches and recent LLM-on-graph frameworks, leaving their merits unclear. To fill this gap, we conduct the first empirical study that benchmarks these two lines of methods across diverse graph deficiencies, revealing overlooked vulnerabilities and challenging the assumption that LLM augmentation is consistently superior. Building on empirical findings, we propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement (RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is the first iterative paradigm that leverages Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations by supplying class-consistent, diverse augmentations and enforcing discriminative representations through iterative graph contrastive learning. It transforms LLM augmentation for graphs from static signal injection into dynamic refinement. Extensive experiments demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement.

**Link**: [arxiv](http://arxiv.org/abs/2510.01910v1),  [pdf](http://arxiv.org/pdf/2510.01910v1)

**Tags**: cs.LG cs.AI 



### MathArena: Evaluating LLMs on Uncontaminated Math Competitions
**Authors**: Mislav BalunoviÄ, Jasper Dekoninck, Ivo Petrov, Nikola JovanoviÄ, Martin Vechev

**Updated**: 2025-10-02T11:27:40Z

**Summary**: The rapid advancement of reasoning capabilities in large language models (LLMs) has led to notable improvements on mathematical benchmarks. However, many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely available online, making it difficult to disentangle genuine reasoning from potential memorization. Furthermore, these benchmarks do not evaluate proof-writing capabilities, which are crucial for many mathematical tasks. To address this, we introduce MathArena, a new benchmark based on the following key insight: recurring math competitions provide a stream of high-quality, challenging problems that can be used for real-time evaluation of LLMs. By evaluating models as soon as new problems are released, we effectively eliminate the risk of contamination. Using this framework, we find strong signs of contamination in AIME 2024. Nonetheless, evaluations on harder competitions, such as CMIMC 2025, demonstrate impressive reasoning capabilities in top-performing models. MathArena is also the first benchmark for proof-writing capabilities. On IMO 2025, top models achieve slightly less than 40%, demonstrating both notable progress and significant room for improvement. So far, we have evaluated over $50$ models across seven competitions, totaling $162$ problems. As an evolving benchmark, MathArena will continue to track the progress of LLMs on newly released competitions, ensuring rigorous and up-to-date evaluation of mathematical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2505.23281v2),  [pdf](http://arxiv.org/pdf/2505.23281v2)

**Tags**: cs.AI cs.CL 



### OntoURL: A Benchmark for Evaluating Large Language Models on Symbolic   Ontological Understanding, Reasoning and Learning
**Authors**: Xiao Zhang, Huiyuan Lai, Qianru Meng, Johan Bos

**Updated**: 2025-10-02T11:25:50Z

**Summary**: Large language models have demonstrated remarkable capabilities across a wide range of tasks, yet their ability to process structured symbolic knowledge remains underexplored. To address this gap, we propose a taxonomy of ontological capabilities and introduce OntoURL, the first comprehensive benchmark designed to systematically evaluate LLMs' capabilities in handling ontologies -- formal and symbolic representations of domain knowledge. Based on the proposed taxonomy, OntoURL systematically assesses three dimensions: understanding, reasoning, and learning through 15 distinct tasks comprising 57,303 questions derived from 40 ontologies across 8 domains. Experiments with 20 open-source LLMs reveal significant performance differences across models, tasks, and domains, with current LLMs showing capabilities in understanding ontological knowledge but weaknesses in reasoning and learning tasks. Further experiments with few-shot and chain-of-thought prompting illustrate how different prompting strategies affect model performance. Additionally, a human evaluation reveals that LLMs outperform humans in understanding and reasoning tasks but fall short in most learning tasks. These findings highlight both the potential and limitations of LLMs in processing symbolic knowledge and establish OntoURL as a critical benchmark for advancing the integration of LLMs with formal knowledge representations.

**Link**: [arxiv](http://arxiv.org/abs/2505.11031v3),  [pdf](http://arxiv.org/pdf/2505.11031v3)

**Tags**: cs.CL 



### Schema Generation for Large Knowledge Graphs Using Large Language Models
**Authors**: Bohui Zhang, Yuan He, Lydia Pintscher, Albert MeroÃ±o PeÃ±uela, Elena Simperl

**Updated**: 2025-10-02T11:15:14Z

**Summary**: Schemas play a vital role in ensuring data quality and supporting usability in the Semantic Web and natural language processing. Traditionally, their creation demands substantial involvement from knowledge engineers and domain experts. Leveraging the impressive capabilities of large language models (LLMs) in tasks like ontology engineering, we explore schema generation using LLMs. To bridge the resource gap, we introduce two datasets: YAGO Schema and Wikidata EntitySchema, along with novel evaluation metrics. The LLM-based pipelines utilize local and global information from knowledge graphs (KGs) to generate schemas in Shape Expressions (ShEx). Experiments demonstrate LLMs' strong potential in producing high-quality ShEx schemas, paving the way for scalable, automated schema generation for large KGs. Furthermore, our benchmark introduces a new challenge for structured generation, pushing the limits of LLMs on syntactically rich formalisms.

**Link**: [arxiv](http://arxiv.org/abs/2506.04512v2),  [pdf](http://arxiv.org/pdf/2506.04512v2)

**Tags**: cs.AI 



### Generating Difficult-to-Translate Texts
**Authors**: VilÃ©m Zouhar, Wenda Xu, Parker Riley, Juraj Juraska, Mara Finkelstein, Markus Freitag, Daniel Deutsch

**Updated**: 2025-10-02T11:06:30Z

**Summary**: Machine translation benchmarks sourced from the real world are quickly obsoleted, due to most examples being easy for state-of-the-art translation models. This limits the benchmark's ability to distinguish which model is better or to reveal models' weaknesses. Current methods for creating difficult test cases, such as subsampling or from-scratch synthesis, either fall short of identifying difficult examples or suffer from a lack of diversity and naturalness. Inspired by the iterative process of human experts probing for model failures, we propose MT-breaker, a method where a large language model iteratively refines a source text to increase its translation difficulty. The LLM iteratively queries a target machine translation model to guide its generation of difficult examples. Our approach generates examples that are more challenging for the target MT model while preserving the diversity of natural texts. While the examples are tailored to a particular machine translation model during the generation, the difficulty also transfers to other models and languages.

**Link**: [arxiv](http://arxiv.org/abs/2509.26592v2),  [pdf](http://arxiv.org/pdf/2509.26592v2)

**Tags**: cs.CL 



### A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal   Sensor Fusion
**Authors**: Fabio Montello, Ronja GÃ¼ldenring, Simone Scardapane, Lazaros Nalpantidis

**Updated**: 2025-10-02T10:51:09Z

**Summary**: Model compression is essential in the deployment of large Computer Vision models on embedded devices. However, static optimization techniques (e.g. pruning, quantization, etc.) neglect the fact that different inputs have different complexities, thus requiring different amount of computations. Dynamic Neural Networks allow to condition the number of computations to the specific input. The current literature on the topic is very extensive and fragmented. We present a comprehensive survey that synthesizes and unifies existing Dynamic Neural Networks research in the context of Computer Vision. Additionally, we provide a logical taxonomy based on which component of the network is adaptive: the output, the computation graph or the input. Furthermore, we argue that Dynamic Neural Networks are particularly beneficial in the context of Sensor Fusion for better adaptivity, noise reduction and information prioritization. We present preliminary works in this direction. We complement this survey with a curated repository listing all the surveyed papers, each with a brief summary of the solution and the code base when available: https://github.com/DTU-PAS/awesome-dynn-for-cv .

**Link**: [arxiv](http://arxiv.org/abs/2501.07451v2),  [pdf](http://arxiv.org/pdf/2501.07451v2)

**Tags**: cs.CV 68T45 I.4.0; I.2.10 



### REPAIR: Robust Editing via Progressive Adaptive Intervention and   Reintegration
**Authors**: Yisu Wang, Ming Wang, Haoyuan Song, Wenjie Huang, Chaozheng Wang, Yi Xie, Xuming Ran

**Updated**: 2025-10-02T10:35:39Z

**Summary**: Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.01879v1),  [pdf](http://arxiv.org/pdf/2510.01879v1)

**Tags**: cs.CL cs.AI 



### Randomized Gradient Subspaces for Efficient Large Language Model   Training
**Authors**: Sahar Rajabi, Nayeema Nonta, Samanvay Vajpayee, Sirisha Rambhatla

**Updated**: 2025-10-02T10:35:38Z

**Summary**: Training large language models (LLMs) is often bottlenecked by extreme memory demands, with optimizer states dominating the footprint. Recent works mitigates this cost by projecting gradients into low-dimensional subspaces using sophisticated update strategies. In this paper, we analyze the dynamics of gradient space and its underlying subspaces. We find that while a small subspace captures most gradient energy, a significant portion still resides in the residual bulk; moreover, the influence of the core subspace diminishes over time and in deeper layers. We also observe that the gradient space exhibits near-flat curvature, calling for algorithms that explicitly account for this geometry. Motivated by these insights, we introduce a suite of randomized algorithms, GrassWalk and GrassJump, which exploit subspace and achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2510.01878v1),  [pdf](http://arxiv.org/pdf/2510.01878v1)

**Tags**: cs.LG 



### TACOS: Task Agnostic COordinator of a multi-drone System
**Authors**: Alessandro Nazzari, Roberto Rubinacci, Marco Lovera

**Updated**: 2025-10-02T10:21:35Z

**Summary**: When a single pilot is responsible for managing a multi-drone system, the task demands varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real-world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system in real-world multi-drone system and conduct an ablation study to assess the contribution of each module.

**Link**: [arxiv](http://arxiv.org/abs/2510.01869v1),  [pdf](http://arxiv.org/pdf/2510.01869v1)

**Tags**: cs.RO cs.AI cs.MA 



### DreamOmni: Unified Image Generation and Editing
**Authors**: Bin Xia, Yuechen Zhang, Jingyao Li, Chengyao Wang, Yitong Wang, Xinglong Wu, Bei Yu, Jiaya Jia

**Updated**: 2025-10-02T10:10:56Z

**Summary**: Currently, the success of large language models (LLMs) illustrates that a unified multitasking approach can significantly enhance model usability, streamline deployment, and foster synergistic benefits across different tasks. However, in computer vision, while text-to-image (T2I) models have significantly improved generation quality through scaling up, their framework design did not initially consider how to unify with downstream tasks, such as various types of editing. To address this, we introduce DreamOmni, a unified model for image generation and editing. We begin by analyzing existing frameworks and the requirements of downstream tasks, proposing a unified framework that integrates both T2I models and various editing tasks. Furthermore, another key challenge is the efficient creation of high-quality editing data, particularly for instruction-based and drag-based editing. To this end, we develop a synthetic data pipeline using sticker-like elements to synthesize accurate, high-quality datasets efficiently, which enables editing data scaling up for unified model training. For training, DreamOmni jointly trains T2I generation and downstream tasks. T2I training enhances the model's understanding of specific concepts and improves generation quality, while editing training helps the model grasp the nuances of the editing task. This collaboration significantly boosts editing performance. Extensive experiments confirm the effectiveness of DreamOmni. The code and model will be released.

**Link**: [arxiv](http://arxiv.org/abs/2412.17098v2),  [pdf](http://arxiv.org/pdf/2412.17098v2)

**Tags**: cs.CV 



### Microscaling Floating Point Formats for Large Language Models
**Authors**: Marco Cococcioni, Dario Pagani, Federico Rossi

**Updated**: 2025-10-02T10:08:59Z

**Summary**: The increasing computational and memory demands of large language models (LLMs) necessitate innovative approaches to optimize resource usage without compromising performance. This paper leverages microscaling floating-point formats, a novel technique designed to address these challenges by reducing the storage and computational overhead associated with numerical representations in LLMs. Unlike traditional floating-point representations that allocate a dedicated scale for each value, microscaling employs a shared scale across a block of values, enabling compact one-byte floating-point representations while maintaining an extended dynamic range. We explore the application of microscaling in the context of 8-bit floating-point formats to significantly reduce memory footprint and computational costs. We tested several configurations of microscaling floats within the GPT-2 LLM architecture, demonstrating that microscaling data formats can achieve competitive accuracy during training and inference, proving its efficacy as a resource-efficient alternative for deploying LLMs at scale. The source code is publicly available at: https://github.com/unipi-dii-compressedarith/llm.c-sve

**Link**: [arxiv](http://arxiv.org/abs/2510.01863v1),  [pdf](http://arxiv.org/pdf/2510.01863v1)

**Tags**: cs.NE cs.LG 



### Benchmarking Foundation Models with Retrieval-Augmented Generation in   Olympic-Level Physics Problem Solving
**Authors**: Shunfeng Zheng, Yudi Zhang, Meng Fang, Zihan Zhang, Zhitan Wu, Mykola Pechenizkiy, Ling Chen

**Updated**: 2025-10-02T09:55:14Z

**Summary**: Retrieval-augmented generation (RAG) with foundation models has achieved strong performance across diverse tasks, but their capacity for expert-level reasoning-such as solving Olympiad-level physics problems-remains largely unexplored. Inspired by the way students prepare for competitions by reviewing past problems, we investigate the potential of RAG to enhance physics reasoning in foundation models. We introduce PhoPile, a high-quality multimodal dataset specifically designed for Olympiad-level physics, enabling systematic study of retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations, capturing the inherently multimodal nature of physics problem solving. Using PhoPile, we benchmark RAG-augmented foundation models, covering both large language models (LLMs) and large multimodal models (LMMs) with multiple retrievers. Our results demonstrate that integrating retrieval with physics corpora can improve model performance, while also highlighting challenges that motivate further research in retrieval-augmented physics reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.00919v2),  [pdf](http://arxiv.org/pdf/2510.00919v2)

**Tags**: cs.CL cs.AI 



### TLUE: A Tibetan Language Understanding Evaluation Benchmark
**Authors**: Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeg Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Hao Wang Xiao Feng, Yongbin Yu

**Updated**: 2025-10-02T09:49:07Z

**Summary**: Large language models have made tremendous progress in recent years, but low-resource languages, like Tibetan, remain significantly underrepresented in their evaluation. Despite Tibetan being spoken by over seven million people, it has largely been neglected in the development and assessment of large language models. To address this gap, we present a \textbf{T}ibetan \textbf{L}anguage \textbf{U}nderstanding \textbf{E}valuation Benchmark, \textbf{TLUE}, the first large-scale benchmark for measuring the proficiency of LLMs in the Tibetan language. \textbf{TLUE} comprises two major components: a comprehensive multi-task understanding benchmark spanning 5 domains and 67 subdomains, and a safety benchmark encompassing 7 subdomains. Then, we evaluate a diverse set of state-of-the-art large language models. Experimental results demonstrate that most large language models perform below the random baseline, highlighting the considerable challenges they face in Tibetan language processing. \textbf{TLUE} provides a crucial foundation for advancing future research in Tibetan language understanding and highlights the importance of promoting greater inclusivity in the development of large language models.

**Link**: [arxiv](http://arxiv.org/abs/2503.12051v5),  [pdf](http://arxiv.org/pdf/2503.12051v5)

**Tags**: cs.CL 



### Pre-Hoc Predictions in AutoML: Leveraging LLMs to Enhance Model   Selection and Benchmarking for Tabular datasets
**Authors**: Yannis Belkhiter, Seshu Tirupathi, Giulio Zizzo, Sachin Sharma, John D. Kelleher

**Updated**: 2025-10-02T09:37:12Z

**Summary**: The field of AutoML has made remarkable progress in post-hoc model selection, with libraries capable of automatically identifying the most performing models for a given dataset. Nevertheless, these methods often rely on exhaustive hyperparameter searches, where methods automatically train and test different types of models on the target dataset. Contrastingly, pre-hoc prediction emerges as a promising alternative, capable of bypassing exhaustive search through intelligent pre-selection of models. Despite its potential, pre-hoc prediction remains under-explored in the literature. This paper explores the intersection of AutoML and pre-hoc model selection by leveraging traditional models and Large Language Model (LLM) agents to reduce the search space of AutoML libraries. By relying on dataset descriptions and statistical information, we reduce the AutoML search space. Our methodology is applied to the AWS AutoGluon portfolio dataset, a state-of-the-art AutoML benchmark containing 175 tabular classification datasets available on OpenML. The proposed approach offers a shift in AutoML workflows, significantly reducing computational overhead, while still selecting the best model for the given dataset.

**Link**: [arxiv](http://arxiv.org/abs/2510.01842v1),  [pdf](http://arxiv.org/pdf/2510.01842v1)

**Tags**: cs.LG cs.AI 



### Subspace Node Pruning
**Authors**: Joshua Offergeld, Marcel van Gerven, Nasir Ahmad

**Updated**: 2025-10-02T09:34:54Z

**Summary**: Improving the efficiency of neural network inference is undeniably important in a time where commercial use of AI models increases daily. Node pruning is the art of removing computational units such as neurons, filters, attention heads, or even entire layers to significantly reduce inference time while retaining network performance. In this work, we propose the projection of unit activations to an orthogonal subspace in which there is no redundant activity and within which we may prune nodes while simultaneously recovering the impact of lost units via linear least squares. We furthermore show that the order in which units are orthogonalized can be optimized to maximally rank units by their redundancy. Finally, we leverage these orthogonal subspaces to automatically determine layer-wise pruning ratios based upon the relative scale of node activations in our subspace, equivalent to cumulative variance. Our method matches or exceeds state-of-the-art pruning results on ImageNet-trained VGG-16, ResNet-50 and DeiT models while simultaneously having up to 24x lower computational cost than alternative methods. We also demonstrate that this method can be applied in a one-shot manner to OPT LLM models, again outperforming competing methods.

**Link**: [arxiv](http://arxiv.org/abs/2405.17506v3),  [pdf](http://arxiv.org/pdf/2405.17506v3)

**Tags**: cs.LG cs.CV cs.NE 



