# Arxiv Results
## Keyword: kv cache 
 ### PromptTea: Let Prompts Tell TeaCache the Optimal Threshold
**Authors**: Zishen Huang, Chunyu Yang, Mengyuan Ren

**Updated**: 2025-07-09T10:53:05Z

**Summary**: Despite recent progress in video generation, inference speed remains a major bottleneck. A common acceleration strategy involves reusing model outputs via caching mechanisms at fixed intervals. However, we find that such fixed-frequency reuse significantly degrades quality in complex scenes, while manually tuning reuse thresholds is inefficient and lacks robustness. To address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that automatically adjusts reuse thresholds based on scene complexity estimated directly from the input prompt. By incorporating prompt-derived semantic cues, PCA enables more adaptive and informed reuse decisions than conventional caching methods. We also revisit the assumptions behind TeaCache and identify a key limitation: it suffers from poor input-output relationship modeling due to an oversimplified prior. To overcome this, we decouple the noisy input, enhance the contribution of meaningful textual information, and improve the model's predictive accuracy through multivariate polynomial feature expansion. To further reduce computational cost, we replace the static CFGCache with DynCFGCache, a dynamic mechanism that selectively reuses classifier-free guidance (CFG) outputs based on estimated output variations. This allows for more flexible reuse without compromising output quality. Extensive experiments demonstrate that our approach achieves significant acceleration-for example, 2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across a range of scenes.

**Link**: [arxiv](http://arxiv.org/abs/2507.06739v1),  [pdf](http://arxiv.org/pdf/2507.06739v1)

**Tags**: cs.CV 



### Saffron-1: Safety Inference Scaling
**Authors**: Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, Hanghang Tong

**Updated**: 2025-07-09T07:47:59Z

**Summary**: Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .

**Link**: [arxiv](http://arxiv.org/abs/2506.06444v2),  [pdf](http://arxiv.org/pdf/2506.06444v2)

**Tags**: cs.LG cs.AI cs.CR 



### SlimCaching: Edge Caching of Mixture-of-Experts for Distributed   Inference
**Authors**: Qian Chen, Xianhao Chen, Kaibin Huang

**Updated**: 2025-07-09T05:43:43Z

**Summary**: Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed within an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K\geq1$, expert co-activation within the same MoE layer introduces non-submodularity, causing greedy methods to be ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.

**Link**: [arxiv](http://arxiv.org/abs/2507.06567v1),  [pdf](http://arxiv.org/pdf/2507.06567v1)

**Tags**: cs.LG cs.DC cs.NI 



### FiRST: Finetuning Router-Selective Transformers for Input-Adaptive   Latency Reduction
**Authors**: Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal

**Updated**: 2025-07-09T04:43:59Z

**Summary**: Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across different domains such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FiRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during the prefill stage) decides which layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming other layer selection strategies in quality metics. It retains competitive performance to base model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a promising and efficient solution for LLM deployment in low-resource environments.

**Link**: [arxiv](http://arxiv.org/abs/2410.12513v3),  [pdf](http://arxiv.org/pdf/2410.12513v3)

**Tags**: cs.CL 



### SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and   Deep Layers
**Authors**: Zicong Tang, Shi Luohe, Zuchao Li, Baoyuan Qi, Guoming Liu, Lefei Zhang, Ping Wang

**Updated**: 2025-07-09T03:33:44Z

**Summary**: Large Language Models (LLMs) have achieved impressive accomplishments in recent years. However, the increasing memory consumption of KV cache has possessed a significant challenge to the inference system. Eviction methods have revealed the inherent redundancy within the KV cache, demonstrating its potential for reduction, particularly in deeper layers. However, KV cache reduction for shallower layers has been found to be insufficient. Based on our observation that, the KV cache exhibits a high degree of similarity. Based on this observation, we proposed a novel KV cache reduction method, SpindleKV, which balances both shallow and deep layers. For deep layers, we employ an attention weight based eviction method, while for shallow layers, we apply a codebook based replacement approach which is learnt by similarity and merging policy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma faced by other attention based eviction methods. Experiments on two common benchmarks with three different LLMs shown that SpindleKV obtained better KV cache reduction effect compared to baseline methods, while preserving similar or even better model performance.

**Link**: [arxiv](http://arxiv.org/abs/2507.06517v1),  [pdf](http://arxiv.org/pdf/2507.06517v1)

**Tags**: cs.CL 



### TokenSwift: Lossless Acceleration of Ultra Long Sequence Generation
**Authors**: Tong Wu, Junzhe Shen, Zixia Jia, Yuxuan Wang, Zilong Zheng

**Updated**: 2025-07-09T02:35:21Z

**Summary**: Generating ultra-long sequences with large language models (LLMs) has become increasingly crucial but remains a highly time-intensive task, particularly for sequences up to 100K tokens. While traditional speculative decoding methods exist, simply extending their generation limits fails to accelerate the process and can be detrimental. Through an in-depth analysis, we identify three major challenges hindering efficient generation: frequent model reloading, dynamic key-value (KV) management and repetitive generation. To address these issues, we introduce TOKENSWIFT, a novel framework designed to substantially accelerate the generation process of ultra-long sequences while maintaining the target model's inherent quality. Experimental results demonstrate that TOKENSWIFT achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B, 14B) and architectures (MHA, GQA). This acceleration translates to hours of time savings for ultra-long sequence generation, establishing TOKENSWIFT as a scalable and effective solution at unprecedented lengths. Code can be found at https://github.com/bigai-nlco/TokenSwift.

**Link**: [arxiv](http://arxiv.org/abs/2502.18890v2),  [pdf](http://arxiv.org/pdf/2502.18890v2)

**Tags**: cs.CL 



### Optomechanical resource for fault-tolerant quantum computing
**Authors**: Margaret Pavlovich, Peter Rakich, Shruti Puri

**Updated**: 2025-07-08T21:23:30Z

**Summary**: Fusion-based quantum computing with dual-rail qubits is a leading candidate for scalable quantum computing using linear optics. This paradigm requires single photons which are entangled into small resource states before being fed into a fusion network. The most common sources for single optical photons and for small entangled states are probabilistic and heralded. The realization of a single reliable deterministic source requires many redundant probabilistic sources and a complex optical network for rerouting and retiming probabilistic outputs. In this work, we show how optomechanics enables reliable production of resources for photonic quantum computing without the redundancy of the all-optical approach. This is achieved by using acoustic modes as caches of quantum resources, ranging from single-particle states to small entangled states, with on-demand read-out. The advantages of acoustic modes as optical quantum memories, compared to other technologies, include their intrinsically long lifetimes and that they are solid state, highly tailorable, and insensitive to electromagnetic noise. We show how the resource states can be prepared directly in the acoustic modes using optical controls. This is still probabilistic and heralded, as in the all-optical approach, but the acoustic modes act as a quantum memory which is integrated into the production of the states. The quantum states may be deterministically transferred from acoustic modes to optical modes, on demand, with another optical drive.

**Link**: [arxiv](http://arxiv.org/abs/2505.00768v2),  [pdf](http://arxiv.org/pdf/2505.00768v2)

**Tags**: quant-ph 



### Multi-Queue SSD I/O Modeling & Its Implications for Data Structure   Design
**Authors**: Erin Ransom, Andrew Lim, Michael Mitzenmacher

**Updated**: 2025-07-08T19:20:30Z

**Summary**: Understanding the performance profiles of storage devices and how best to utilize them has always been non-trivial due to factors such as seek times, caching, scheduling, concurrent access, flash wear-out, and garbage collection. However, analytical frameworks that provide simplified abstractions of storage performance can still be accurate enough to evaluate external memory algorithms and data structures at the design stage. For example, the Disk Access Machine (DAM) model assumes that a storage device transfers data in fixed-size blocks of size B and that all transfers have unit latency. This abstraction is already sufficient to explain some of the benefits of data structures such as B-trees and Log-Structured Merge trees (LSM trees); however, storage technology advances have significantly reduced current models' accuracy and utility.   This paper introduces the Multi-Queue Solid State Drive (MQSSD) model, a new storage abstraction. This model builds upon previous models and aims to more accurately represent the performance characteristics of modern storage hardware. We identify key performance-critical aspects of modern multi-queue solid-state drives on which we base our model and demonstrate these characteristics on actual hardware. We then show how our model can be applied to LSM-tree-based storage engines to optimize them for modern storage hardware. We highlight that leveraging concurrent access is crucial for fully utilizing the high throughput of multi-queue SSDs, enabling designs that may appear counterintuitive under traditional paradigms We then validate these insights through experiments using Facebook's LSM-tree-based key-value store, RocksDB. We conclude that the MQSSD model offers a more accurate abstraction of modern hardware than previous models, allowing for greater insight and optimization.

**Link**: [arxiv](http://arxiv.org/abs/2507.06349v1),  [pdf](http://arxiv.org/pdf/2507.06349v1)

**Tags**: cs.DS cs.AR 



### FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning
**Authors**: Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, Luca Benini

**Updated**: 2025-07-08T12:34:10Z

**Summary**: Visual Autoregressive (VAR) modeling has gained popularity for its shift towards next-scale prediction. However, existing VAR paradigms process the entire token map at each scale step, leading to the complexity and runtime scaling dramatically with image resolution. To address this challenge, we propose FastVAR, a post-training acceleration method for efficient resolution scaling with VARs. Our key finding is that the majority of latency arises from the large-scale step where most tokens have already converged. Leveraging this observation, we develop the cached token pruning strategy that only forwards pivotal tokens for scale-specific modeling while using cached tokens from previous scale steps to restore the pruned slots. This significantly reduces the number of forwarded tokens and improves the efficiency at larger resolutions. Experiments show the proposed FastVAR can further speedup FlashAttention-accelerated VAR by 2.7$\times$ with negligible performance drop of <1%. We further extend FastVAR to zero-shot generation of higher resolution images. In particular, FastVAR can generate one 2K image with 15GB memory footprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at https://github.com/csguoh/FastVAR.

**Link**: [arxiv](http://arxiv.org/abs/2503.23367v3),  [pdf](http://arxiv.org/pdf/2503.23367v3)

**Tags**: cs.CV 



### An Ensemble Embedding Approach for Improving Semantic Caching   Performance in LLM-based Systems
**Authors**: Shervin Ghaffari, Zohre Bahranifard, Mohammad Akbari

**Updated**: 2025-07-08T09:20:12Z

**Summary**: Semantic caching enhances the efficiency of large language model (LLM) systems by identifying semantically similar queries, storing responses once, and serving them for subsequent equivalent requests. However, existing semantic caching frameworks rely on single embedding models for query representation, which limits their ability to capture the diverse semantic relationships present in real-world query distributions. This paper presents an ensemble embedding approach that combines multiple embedding models through a trained meta-encoder to improve semantic similarity detection in LLM caching systems. We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring cache hit ratios, cache miss ratios, token savings, and response times. Our ensemble approach achieves a 92\% cache hit ratio for semantically equivalent queries while maintaining an 85\% accuracy in correctly rejecting non-equivalent queries as cache misses. These results demonstrate that ensemble embedding methods significantly outperform single-model approaches in distinguishing between semantically similar and dissimilar queries, leading to more effective caching performance and reduced computational overhead in LLM-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2507.07061v1),  [pdf](http://arxiv.org/pdf/2507.07061v1)

**Tags**: cs.LG 68T50 I.2.7; H.3.3; I.5.1 



### Towards Stabilized and Efficient Diffusion Transformers through   Long-Skip-Connections with Spectral Constraints
**Authors**: Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng

**Updated**: 2025-07-08T07:10:06Z

**Summary**: Diffusion Transformers (DiT) have emerged as a powerful architecture for image and video generation, offering superior quality and scalability. However, their practical application suffers from inherent dynamic feature instability, leading to error amplification during cached inference. Through systematic analysis, we identify the absence of long-range feature preservation mechanisms as the root cause of unstable feature propagation and perturbation sensitivity. To this end, we propose Skip-DiT, an image and video generative DiT variant enhanced with Long-Skip-Connections (LSCs) - the key efficiency component in U-Nets. Theoretical spectral norm and visualization analysis demonstrate how LSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic feature enable an efficient statical caching mechanism that reuses deep features across timesteps while updating shallow components. Extensive experiments across the image and video generation tasks demonstrate that Skip-DiT achieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2 times inference acceleration with negligible quality loss and high fidelity to the original output, outperforming existing DiT caching methods across various quantitative metrics. Our findings establish Long-Skip-Connections as critical architectural components for stable and efficient diffusion transformers. Codes are provided in the https://github.com/OpenSparseLLMs/Skip-DiT.

**Link**: [arxiv](http://arxiv.org/abs/2411.17616v4),  [pdf](http://arxiv.org/pdf/2411.17616v4)

**Tags**: cs.CV 



### Torpor: GPU-Enabled Serverless Computing for Low-Latency,   Resource-Efficient Inference
**Authors**: Minchen Yu, Ao Wang, Dong Chen, Haoxuan Yu, Xiaonan Luo, Zhuohao Li, Wei Wang, Ruichuan Chen, Dapeng Nie, Haoran Yang, Yu Ding

**Updated**: 2025-07-08T02:15:07Z

**Summary**: Serverless computing offers a compelling cloud model for online inference services. However, existing serverless platforms lack efficient support for GPUs, hindering their ability to deliver high-performance inference. In this paper, we present Torpor, a serverless platform for GPU-efficient, low-latency inference. To enable efficient sharing of a node's GPUs among numerous inference functions, Torpor maintains models in main memory and dynamically swaps them onto GPUs upon request arrivals (i.e., late binding with model swapping). Torpor uses various techniques, including asynchronous API redirection, GPU runtime sharing, pipelined model execution, and efficient GPU memory management, to minimize latency overhead caused by model swapping. Additionally, we design an interference-aware request scheduling algorithm that utilizes high-speed GPU interconnects to meet latency service-level objectives (SLOs) for individual inference functions. We have implemented Torpor and evaluated its performance in a production environment. Utilizing late binding and model swapping, Torpor can concurrently serve hundreds of inference functions on a worker node with 4 GPUs, while achieving latency performance comparable to native execution, where each model is cached exclusively on a GPU. Pilot deployment in a leading commercial serverless cloud shows that Torpor reduces the GPU provisioning cost by 70% and 65% for users and the platform, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2306.03622v3),  [pdf](http://arxiv.org/pdf/2306.03622v3)

**Tags**: cs.DC 



### RandAR: Decoder-only Autoregressive Visual Generation in Random Orders
**Authors**: Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang

**Updated**: 2025-07-08T00:51:16Z

**Summary**: We introduce RandAR, a decoder-only visual autoregressive (AR) model capable of generating images in arbitrary token orders. Unlike previous decoder-only AR models that rely on a predefined generation order, RandAR removes this inductive bias, unlocking new capabilities in decoder-only generation. Our essential design enables random order by inserting a "position instruction token" before each image token to be predicted, representing the spatial location of the next image token. Trained on randomly permuted token sequences -- a more challenging task than fixed-order generation, RandAR achieves comparable performance to its conventional raster-order counterpart. More importantly, decoder-only transformers trained from random orders acquire new capabilities. For the efficiency bottleneck of AR models, RandAR adopts parallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration without sacrificing generation quality. Additionally, RandAR supports inpainting, outpainting and resolution extrapolation in a zero-shot manner. We hope RandAR inspires new directions for decoder-only visual generation models and broadens their applications across diverse scenarios. Our project page is at https://rand-ar.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2412.01827v2),  [pdf](http://arxiv.org/pdf/2412.01827v2)

**Tags**: cs.CV cs.AI 



### StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context   Modeling
**Authors**: Meng Wei, Chenyang Wan, Xiqian Yu, Tai Wang, Yuqiang Yang, Xiaohan Mao, Chenming Zhu, Wenzhe Cai, Hanqing Wang, Yilun Chen, Xihui Liu, Jiangmiao Pang

**Updated**: 2025-07-07T17:49:41Z

**Summary**: Vision-and-Language Navigation (VLN) in real-world settings requires agents to process continuous visual streams and generate actions with low latency grounded in language instructions. While Video-based Large Language Models (Video-LLMs) have driven recent progress, current VLN methods based on Video-LLM often face trade-offs among fine-grained visual understanding, long-term context modeling and computational efficiency. We introduce StreamVLN, a streaming VLN framework that employs a hybrid slow-fast context modeling strategy to support multi-modal reasoning over interleaved vision, language and action inputs. The fast-streaming dialogue context facilitates responsive action generation through a sliding-window of active dialogues, while the slow-updating memory context compresses historical visual states using a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN achieves coherent multi-turn dialogue through efficient KV cache reuse, supporting long video streams with bounded context size and inference cost. Experiments on VLN-CE benchmarks demonstrate state-of-the-art performance with stable low latency, ensuring robustness and efficiency in real-world deployment. The project page is: \href{https://streamvln.github.io/}{https://streamvln.github.io/}.

**Link**: [arxiv](http://arxiv.org/abs/2507.05240v1),  [pdf](http://arxiv.org/pdf/2507.05240v1)

**Tags**: cs.RO cs.CV 



### The Case for Instance-Optimized LLMs in OLAP Databases
**Authors**: Bardia Mohammadi, Laurent Bindschaedler

**Updated**: 2025-07-07T13:10:01Z

**Summary**: Large Language Models (LLMs) can enhance analytics systems with powerful data summarization, cleaning, and semantic transformation capabilities. However, deploying LLMs at scale -- processing millions to billions of rows -- remains prohibitively expensive in computation and memory. We present IOLM-DB, a novel system that makes LLM-enhanced database queries practical through query-specific model optimization. Instead of using general-purpose LLMs, IOLM-DB generates lightweight, specialized models tailored to each query's specific needs using representative data samples. IOLM-DB reduces model footprints by up to 76% and increases throughput by up to 3.31$\times$ while maintaining accuracy through aggressive compression techniques, including quantization, sparsification, and structural pruning. We further show how our approach enables higher parallelism on existing hardware and seamlessly supports caching and batching strategies to reduce overheads. Our prototype demonstrates that leveraging LLM queries inside analytics systems is feasible at scale, opening new possibilities for future OLAP applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.04967v1),  [pdf](http://arxiv.org/pdf/2507.04967v1)

**Tags**: cs.DB cs.LG 



### A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated   in a coupled reactive transport HPC simulation
**Authors**: Max Lübke, Marco De Lucia, Stefan Petri, Bettina Schnor

**Updated**: 2025-07-07T09:25:21Z

**Summary**: Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.

**Link**: [arxiv](http://arxiv.org/abs/2504.14374v2),  [pdf](http://arxiv.org/pdf/2504.14374v2)

**Tags**: cs.DC 



### Performance Evaluation of General Purpose Large Language Models for   Basic Linear Algebra Subprograms Code Generation
**Authors**: Daichi Mukunoki, Shun-ichiro Hayashi, Tetsuya Hoshino, Takahiro Katagiri

**Updated**: 2025-07-07T06:33:59Z

**Summary**: Generative AI technology based on Large Language Models (LLM) has been developed and applied to assist or automatically generate program codes. In this paper, we evaluate the capability of existing general LLMs for Basic Linear Algebra Subprograms (BLAS) code generation for CPUs. We use two LLMs provided by OpenAI: GPT-4.1, a Generative Pre-trained Transformer (GPT) model, and o4-mini, one of the o-series of Reasoning models. Both have been released in April 2025. For the routines from level-1 to 3 BLAS, we tried to generate (1) C code without optimization from routine name only, (2) C code with basic performance optimizations (thread parallelization, SIMD vectorization, and cache blocking) from routine name only, and (3) C code with basic performance optimizations based on Fortran reference code. As a result, we found that correct code can be generated in many cases even when only routine name are given. We also confirmed that thread parallelization with OpenMP, SIMD vectorization, and cache blocking can be implemented to some extent, and that the code is faster than the reference code.

**Link**: [arxiv](http://arxiv.org/abs/2507.04697v1),  [pdf](http://arxiv.org/pdf/2507.04697v1)

**Tags**: cs.LG cs.DC cs.MS 



### RAT: Bridging RNN Efficiency and Attention Accuracy in Language Modeling
**Authors**: Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre

**Updated**: 2025-07-06T15:08:49Z

**Summary**: Transformers have become the cornerstone of modern large-scale language models; however, their dependence on softmax attention poses a major computational bottleneck, particularly in long-context settings. In this work, rather than following prevalent approaches such as linear attention (or SSMs) and local attention, we introduce an intermediate design called \rat between recurrence and attention mechanisms. It partitions the input into chunks, applies a simple linear recurrence within each chunk to capture local dependencies, and then performs softmax attention across chunks to model long-range interactions. By adjusting the size of the chunk, \rat enables flexible trade-offs, combining the strengths of RNN and attention. Empirically, with a chunk size of 16, the \rat layer achieves a \(7\times\) improvement in training speed with 100K token sequences and \(9\times\) in generation at 4K sequence length, while maintaining similar or sometimes even better accuracy compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves \rat with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage compared to attention, but also consistently enhances performance, for example, achieving an average 1 point gain in commonsense reasoning tasks, up to 4 points on code tasks, and a 1 point Rouge-L increase in a summarization SFT task. Code is available at https://github.com/CLAIRE-Labo/RAT

**Link**: [arxiv](http://arxiv.org/abs/2507.04416v1),  [pdf](http://arxiv.org/pdf/2507.04416v1)

**Tags**: cs.CL 



### A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale   Reconstruction with External Memory
**Authors**: Felix Windisch, Lukas Radl, Thomas Köhler, Michael Steiner, Dieter Schmalstieg, Markus Steinberger

**Updated**: 2025-07-05T15:51:57Z

**Summary**: Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.

**Link**: [arxiv](http://arxiv.org/abs/2507.01110v2),  [pdf](http://arxiv.org/pdf/2507.01110v2)

**Tags**: cs.GR cs.LG 



### SparseMM: Head Sparsity Emerges from Visual Concept Responses in MLLMs
**Authors**: Jiahui Wang, Zuyan Liu, Yongming Rao, Jiwen Lu

**Updated**: 2025-07-05T15:40:51Z

**Summary**: Multimodal Large Language Models (MLLMs) are commonly derived by extending pre-trained Large Language Models (LLMs) with visual capabilities. In this work, we investigate how MLLMs process visual inputs by analyzing their attention mechanisms. We reveal a surprising sparsity phenomenon: only a small subset (approximately less than 5%) of attention heads in LLMs actively contribute to visual understanding, termed visual heads. To identify these heads efficiently, we design a training-free framework that quantifies head-level visual relevance through targeted response analysis. Building on this discovery, we introduce SparseMM, a KV-Cache optimization strategy that allocates asymmetric computation budgets to heads in LLMs based on their visual scores, leveraging the sparity of visual heads for accelerating the inference of MLLMs. Compared with prior KV-Cache acceleration methods that ignore the particularity of visual, SparseMM prioritizes stress and retaining visual semantics during decoding. Extensive evaluations across mainstream multimodal benchmarks demonstrate that SparseMM achieves superior accuracy-efficiency trade-offs. Notably, SparseMM delivers 1.38x real-time acceleration and 52% memory reduction during generation while maintaining performance parity on efficiency test. Our project is open sourced at https://github.com/CR400AF-A/SparseMM.

**Link**: [arxiv](http://arxiv.org/abs/2506.05344v2),  [pdf](http://arxiv.org/pdf/2506.05344v2)

**Tags**: cs.CV 



### Heterogeneous Memory Benchmarking Toolkit
**Authors**: Golsana Ghaemi, Gabriel Franco, Kazem Taram, Renato Mancuso

**Updated**: 2025-07-05T13:37:48Z

**Summary**: This paper presents an open-source kernel-level heterogeneous memory characterization framework (MemScope) for embedded systems. MemScope enables precise characterization of the temporal behavior of available memory modules under configurable contention stress scenarios. MemScope leverages kernel-level control over physical memory allocation, cache maintenance, CPU state, interrupts, and I/O device activity to accurately benchmark heterogeneous memory subsystems. This gives us the privilege to directly map pieces of contiguous physical memory and instantiate allocators, allowing us to finely control cores to create and eliminate interference. Additionally, we can minimize noise and interruptions, guaranteeing more consistent and precise results compared to equivalent user-space solutions. Running our Framework on a Xilinx Zynq UltraScale+ ZCU102 CPU-FPGA platform demonstrates its capability to precisely benchmark bandwidth and latency across various memory types, including PL-side DRAM and BRAM, in a multi-core system.

**Link**: [arxiv](http://arxiv.org/abs/2505.00901v2),  [pdf](http://arxiv.org/pdf/2505.00901v2)

**Tags**: cs.AR cs.PF 



### Combination generators with optimal cache utilization and communication   free parallel execution
**Authors**: Xi He, Max. A. Little

**Updated**: 2025-07-05T10:11:37Z

**Summary**: We introduce an efficient and elegant combination generator for producing all combinations of size less than or equal to K, designed for exhaustive generation and combinatorial optimization tasks. This generator can be implemented to achieve what we define as optimal efficiency: constant amortized time, optimal cache utilization, embarrassingly parallel execution, and a recursive structure compatible with pruning-based search. These properties are difficult to satisfy simultaneously in existing generators. For example, classical Gray code or lexicographic generators are typically list-based and sequentially defined, making them difficult to vectorized, inefficient in cache usage, and inherently hard to parallelize. Generators based on unranking methods, while easy to parallelize, are non-recursive. These limitations reduce their applicability in our target applications, where both computational efficiency and recursion are crucial. We adapt Bird's algebra of programming-style calculation to derive our algorithms, a formalism for developing correct-by-construction programs from specifications. As a result, all generators in this paper are first formulated in their clearest specification, and efficient definitions are derived constructively through equational reasoning, resulting in concise and elegant divide-and-conquer definitions. Beyond presenting a combination generator, we extend our approach to construct generators for K-permutations, nested combinations of combinations, and nested permutation-combination structures. To the best of our knowledge, the literature has not previously reported generators for these nested structures. We also develop sequential variants that produce configurations in Gray code-compatible orders -- such as the revolving door ordering -- which are particularly useful for constructing nested generators.

**Link**: [arxiv](http://arxiv.org/abs/2507.03980v1),  [pdf](http://arxiv.org/pdf/2507.03980v1)

**Tags**: cs.DM cs.DS 



### PFCS: Prime Factorization Cache System for Deterministic Data   Relationship Discovery
**Authors**: Duy Le

**Updated**: 2025-07-05T06:55:45Z

**Summary**: Cache systems fundamentally limit modern computing performance due to their inability to precisely capture data relationships. While achieving 85-92% hit rates, traditional systems rely on statistical heuristics that cannot guarantee relationship discovery, leading to suboptimal prefetching and resource waste. We present PFCS (Prime Factorization Cache System), which leverages the mathematical uniqueness of prime factorization to achieve deterministic relationship discovery with zero false positives. PFCS assigns unique primes to data elements and represents relationships as composite numbers, enabling the recovery of perfect relationships through factorization. A comprehensive evaluation across database, ML, and HPC workloads demonstrates an average performance improvement of x 6.2, 98.9% hit rates, and a 38% power reduction compared to state-of-the-art systems. The mathematical foundation provides formal guarantees impossible with approximation-based approaches, establishing a new paradigm for cache system design

**Link**: [arxiv](http://arxiv.org/abs/2507.03919v1),  [pdf](http://arxiv.org/pdf/2507.03919v1)

**Tags**: cs.DB cs.CC 



### A Taxonomy and Comparative Analysis of IPv4 Identifier Selection   Correctness, Security, and Performance
**Authors**: Joshua J. Daymude, Antonio M. Espinoza, Sean Bergen, Benjamin Mixon-Baca, Jeffrey Knockel, Jedidiah R. Crandall

**Updated**: 2025-07-05T01:08:40Z

**Summary**: The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.

**Link**: [arxiv](http://arxiv.org/abs/2406.06483v3),  [pdf](http://arxiv.org/pdf/2406.06483v3)

**Tags**: cs.NI cs.CR 



### Memory- and compute-optimized geometric multigrid GMGPolar for   curvilinear coordinate representations -- Applications to fusion plasma
**Authors**: Julian Litz, Philippe Leleux, Carola Kruse, Joscha Gedicke, Martin J. Kühn

**Updated**: 2025-07-04T21:09:51Z

**Summary**: Tokamak fusion reactors are actively studied as a means of realizing energy production from plasma fusion. However, due to the substantial cost and time required to construct fusion reactors and run physical experiments, numerical experiments are indispensable for understanding plasma physics inside tokamaks, supporting the design and engineering phase, and optimizing future reactor designs. Geometric multigrid methods are optimal solvers for many problems that arise from the discretization of partial differential equations. It has been shown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson equation in linear complexity and with only small memory requirements compared to other state-of-the-art solvers. In this paper, we present a completely refactored and object-oriented version of GMGPolar which offers two different matrix-free implementations. Among other things, we leverage the Sherman-Morrison formula to solve cyclic tridiagonal systems from circular line solvers without additional fill-in and we apply reordering to optimize cache access of circular and radial smoothing operations. With the Give approach, memory requirements are further reduced and speedups of four to seven are obtained for usual test cases. For the Take approach, speedups of 16 to 18 can be attained.

**Link**: [arxiv](http://arxiv.org/abs/2507.03812v1),  [pdf](http://arxiv.org/pdf/2507.03812v1)

**Tags**: cs.MS physics.plasm-ph 68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99 



### Quantum Algorithm for the Fixed-Radius Neighbor Search
**Authors**: Luca Cappelli, Claudio Sanavio, Alessandro Andrea Zecchi, Giuseppe Murante, Sauro Succi

**Updated**: 2025-07-04T10:01:10Z

**Summary**: The neighbor search is a computationally demanding problem, usually both time- and memory-consuming. The main problem of this kind of algorithms is the long execution time due to cache misses. In this work, we propose a quantum algorithm for the Fixed RAdius Neighbor Search problem (FRANS) based on the fixed-point version of Grover's algorithm. We derive an efficient circuit for solving the FRANS with linear query complexity with the number of particles $N$. The quantum circuit returns the list of all the neighbors' pairs within the fixed radius, together with their distance, avoiding the slow down given by cache miss. We explicitly write the Grover's operator and analyze its gate complexity. The whole algorithm has complexity of $\mathcal{O}(M^{\frac{1}{2}}N^{2})$ in the worst-case scenario, where $M$ is the number of neighboring pairs, and uses $\mathcal{O}(\log N)$ number of qubits. By employing extra ancilla qubits the depth of the circuit can be brought down to $\mathcal{O}(N\log N)$ at the cost of $\mathcal{O}(N)$ qubits for unstructured dataset, or $\mathcal{O}(\text{poly}(\log N))$ qubits for structured datasets. Finally we assess the resilience of the model to the readout error, suggesting an error correction-free strategy to check the accuracy of the results.

**Link**: [arxiv](http://arxiv.org/abs/2507.03445v1),  [pdf](http://arxiv.org/pdf/2507.03445v1)

**Tags**: quant-ph 



### Numerical investigation of the effect of high voltage frequency on the   density of RONS species in the air atmospheric pressure gas discharge
**Authors**: Fariborz Momtazzadeh, Farshad Sohbatzadeh, Hamed Soltani Ahmadi, Ramin Mehrabifard

**Updated**: 2025-07-04T09:03:18Z

**Summary**: In the last few decades, studies in various fields of plasma technology have expanded and its application in different processes has increased. Therefore, the achievement of a desirable and practical plasma with specific characteristics is of particular importance. The frequency of the applied voltage is one of the important factors that play a role in the physical and chemical characteristics. In this research, changes in the density of active species produced in an electrical discharge using a dielectric barrier and air working gas have been investigated from a frequency of 500 Hz to 500 kHz, and by applying a constant voltage of 2 kV, have been investigated. For this purpose, 87 different reactions with specific collision cross-sections were defined in COMSOL Multiphysics. Other parameters, including current-voltage waveform, electric field, and species densitywere evaluated. The results show that under completely identical conditions, the electron temperature distribution changes with increasing applied frequency, and the density of reactive oxygen and nitrogen species RONS decreases, but O shows an increasing trend. It should be noted that the simulation results are in good agreement with previous experimental and simulation reports. These results offer valuable insights into optimizing plasma parameters for different applications, potentially resulting in better treatment outcomes across a range of therapeutic domains.

**Link**: [arxiv](http://arxiv.org/abs/2507.03396v1),  [pdf](http://arxiv.org/pdf/2507.03396v1)

**Tags**: physics.plasm-ph 



### CAOTE: KV Cache Eviction for LLMs via Attention Output Error-Based Token   Selection
**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott

**Updated**: 2025-07-04T06:49:31Z

**Summary**: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.

**Link**: [arxiv](http://arxiv.org/abs/2504.14051v3),  [pdf](http://arxiv.org/pdf/2504.14051v3)

**Tags**: cs.LG 



### Hunyuan-TurboS: Advancing Large Language Models through   Mamba-Transformer Synergy and Adaptive Chain-of-Thought
**Authors**: Tencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Zhen Yang, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu

**Updated**: 2025-07-04T06:36:38Z

**Summary**: As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS, a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It synergistically combines Mamba's long-sequence processing efficiency with Transformer's superior contextual understanding. Hunyuan-TurboS features an adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching between rapid responses for simple queries and deep "thinking" modes for complex problems, optimizing computational resources. Architecturally, this 56B activated (560B total) parameter model employs 128 layers (Mamba2, Attention, FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear complexity, Grouped-Query Attention minimizes KV cache, and FFNs use an MoE structure. Pre-trained on 16T high-quality tokens, it supports a 256K context length and is the first industry-deployed large-scale Mamba model. Our comprehensive post-training strategy enhances capabilities via Supervised Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method, Multi-round Deliberation Learning for iterative improvement, and a two-stage Large-scale Reinforcement Learning process targeting STEM and general instruction-following. Evaluations show strong performance: overall top 7 rank on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances high performance and efficiency, offering substantial capabilities at lower inference costs than many reasoning models, establishing a new paradigm for efficient large-scale pre-trained models.

**Link**: [arxiv](http://arxiv.org/abs/2505.15431v3),  [pdf](http://arxiv.org/pdf/2505.15431v3)

**Tags**: cs.CL 



### Robust and Efficient Embedded Convex Optimization through First-Order   Adaptive Caching
**Authors**: Ishaan Mahajan, Brian Plancher

**Updated**: 2025-07-04T00:16:15Z

**Summary**: Recent advances in Model Predictive Control (MPC) leveraging a combination of first-order methods, such as the Alternating Direction Method of Multipliers (ADMM), and offline precomputation and caching of select operations, have excitingly enabled real-time MPC on microcontrollers. Unfortunately, these approaches require the use of fixed hyperparameters, limiting their adaptability and overall performance. In this work, we introduce First-Order Adaptive Caching, which precomputes not only select matrix operations but also their sensitivities to hyperparameter variations, enabling online hyperparameter updates without full recomputation of the cache. We demonstrate the effectiveness of our approach on a number of dynamic quadrotor tasks, achieving up to a 63.4% reduction in ADMM iterations over the use of optimized fixed hyperparameters and approaching 70% of the performance of a full cache recomputation, while reducing the computational cost from O(n^3) to O(n^2) complexity. This performance enables us to perform figure-eight trajectories on a 27g tiny quadrotor under wind disturbances. We release our implementation open-source for the benefit of the wider robotics community.

**Link**: [arxiv](http://arxiv.org/abs/2507.03231v1),  [pdf](http://arxiv.org/pdf/2507.03231v1)

**Tags**: cs.RO math.OC 



### HGCA: Hybrid GPU-CPU Attention for Long Context LLM Inference
**Authors**: Weishu Deng, Yujie Yang, Peiran Du, Lingfeng Xiang, Zhen Lin, Chen Zhong, Song Jiang, Hui Lu, Jia Rao

**Updated**: 2025-07-03T20:20:33Z

**Summary**: Scaling inference for large language models (LLMs) is increasingly constrained by limited GPU memory, especially due to growing key-value (KV) caches required for long-context generation. While existing approaches offload KV caches to CPU memory or apply sparse attention to reduce GPU load, they often underutilize CPU compute resources and compromise accuracy. We present HGCA, a hybrid CPU-GPU attention mechanism that enables scalable, high-throughput LLM inference with near-full attention quality. HGCA performs dense attention on recently generated KV entries retained in GPU memory and parallel sparse attention on selected, salient KV entries in CPU memory. The attention outputs are efficiently merged using log-sum-exp fusion, minimizing PCIe transfer overhead. HGCA also introduces a finegrained, per-head sparsification strategy optimized for CPU execution, preserving contextual relevance while reducing computation. Our implementation seamlessly integrates into existing LLM frameworks without requiring model retraining. Experiments across diverse models and workloads show that HGCA achieves superior scalability, supports longer sequences and larger batch sizes, and outperforms existing sparse attention baselines in both performance and accuracy -- all on commodity GPU hardware.

**Link**: [arxiv](http://arxiv.org/abs/2507.03153v1),  [pdf](http://arxiv.org/pdf/2507.03153v1)

**Tags**: cs.LG 



### Less is Enough: Training-Free Video Diffusion Acceleration via   Runtime-Adaptive Caching
**Authors**: Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, Xiang Bai

**Updated**: 2025-07-03T17:59:54Z

**Summary**: Video generation models have demonstrated remarkable performance, yet their broader adoption remains constrained by slow inference speeds and substantial computational costs, primarily due to the iterative nature of the denoising process. Addressing this bottleneck is essential for democratizing advanced video synthesis technologies and enabling their integration into real-world applications. This work proposes EasyCache, a training-free acceleration framework for video diffusion models. EasyCache introduces a lightweight, runtime-adaptive caching mechanism that dynamically reuses previously computed transformation vectors, avoiding redundant computations during inference. Unlike prior approaches, EasyCache requires no offline profiling, pre-computation, or extensive parameter tuning. We conduct comprehensive studies on various large-scale video generation models, including OpenSora, Wan2.1, and HunyuanVideo. Our method achieves leading acceleration performance, reducing inference time by up to 2.1-3.3$\times$ compared to the original baselines while maintaining high visual fidelity with a significant up to 36% PSNR improvement compared to the previous SOTA method. This improvement makes our EasyCache a efficient and highly accessible solution for high-quality video generation in both research and practical applications. The code is available at https://github.com/H-EmbodVis/EasyCache.

**Link**: [arxiv](http://arxiv.org/abs/2507.02860v1),  [pdf](http://arxiv.org/pdf/2507.02860v1)

**Tags**: cs.CV 



### HybridTier: an Adaptive and Lightweight CXL-Memory Tiering System
**Authors**: Kevin Song, Jiacheng Yang, Zixuan Wang, Jishen Zhao, Sihang Liu, Gennady Pekhimenko

**Updated**: 2025-07-03T17:11:28Z

**Summary**: Modern workloads are demanding increasingly larger memory capacity. Compute Express Link (CXL)-based memory tiering has emerged as a promising solution for addressing this problem by utilizing traditional DRAM alongside slow-tier CXL memory devices. We analyze prior tiering systems and observe two challenges for high-performance memory tiering: adapting to skewed but dynamically varying data hotness distributions while minimizing memory and cache overhead due to tiering.   To address these challenges, we propose HybridTier, an adaptive and lightweight tiering system for CXL memory. HybridTier tracks both long-term data access frequency and short-term access momentum \emph{simultaneously} to accurately capture and adapt to shifting hotness distributions. HybridTier reduces the metadata memory overhead by tracking data accesses \emph{probabilistically}, obtaining higher memory efficiency by trading off a small amount of tracking inaccuracy that has a negligible impact on application performance. To reduce cache overhead, HybridTier uses lightweight data structures that optimize for data locality to track data hotness. Our evaluations show that HybridTier outperforms prior systems by up to $91\%$ ($19\%$ geomean), incurring $2.0-7.8\times$ less memory overhead and $1.7-3.5\times$ less cache misses.

**Link**: [arxiv](http://arxiv.org/abs/2312.04789v2),  [pdf](http://arxiv.org/pdf/2312.04789v2)

**Tags**: cs.DC cs.OS 



### Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache   Compression
**Authors**: Michael R. Metel, Boxing Chen, Mehdi Rezagholizadeh

**Updated**: 2025-07-03T16:06:35Z

**Summary**: Several works have developed eviction policies to remove key-value (KV) pairs from the KV cache for more efficient inference. The focus has been on compressing the KV cache after the input prompt has been processed for faster token generation. In settings with limited GPU memory, and when the input context is longer than the generation length, we show that by also compressing the KV cache during the input processing phase, larger batch sizes can be used resulting in significantly higher throughput while still maintaining the original model's accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2412.05693v3),  [pdf](http://arxiv.org/pdf/2412.05693v3)

**Tags**: cs.CL 



### OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device   Speculative Decoding
**Authors**: Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang

**Updated**: 2025-07-03T14:20:41Z

**Summary**: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2507.02659v1),  [pdf](http://arxiv.org/pdf/2507.02659v1)

**Tags**: cs.LG cs.CL 



### Skip-Vision: Efficient and Scalable Acceleration of Vision-Language   Models via Adaptive Token Skipping
**Authors**: Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan

**Updated**: 2025-07-03T08:22:27Z

**Summary**: Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. Experimental results demonstrate that Skip-Vision reduces training time by up to 35\%, inference FLOPs by 75\%, and latency by 45\%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2503.21817v3),  [pdf](http://arxiv.org/pdf/2503.21817v3)

**Tags**: cs.CV 



### Direct Reconstruction of Terahertz-driven Subcycle Electron Emission   Dynamics
**Authors**: Jiakang Mao, Yushan Zeng, Hongyang Li, Liwei Song, Ye Tian, Ruxin Li

**Updated**: 2025-07-03T07:49:18Z

**Summary**: While field-driven electron emission is theoretically understood down to the subcycle regime, its direct experimental temporal characterization using long-wavelength terahertz (THz) fields remains elusive. Here, by driving a graphite tip with phase-stable quasi-single-cycle THz pulses, we reveal distinct subcycle electron emission dynamics including: (1) At a carrier-envelope phase (CEP) near zero, spectral peaks scale linearly with THz field strength, characteristic of subcycle emission; (2) At the opposite CEP, dominant deceleration fields generate stationary low-energy peaks. Crucially, we develop a pump-probe-free, direct reconstruction method extracting electron pulse profiles solely from measured energy spectra, obtaining durations from 97.3 to 114.3 fs as the field increases (191-290 kV/cm). Phase-resolved simulations further reveal a 71.2% modulation in the cutoff energy and a near-total (99.7%) suppression of the emission current. This work not only validates the Fowler-Nordheim model under THz excitation but also establishes a general framework for the direct temporal characterization of subcycle electron emission, opening pathways for precise electron control in ultrafast electron sources and lightwave nanoelectronics.

**Link**: [arxiv](http://arxiv.org/abs/2507.02397v1),  [pdf](http://arxiv.org/pdf/2507.02397v1)

**Tags**: physics.optics 



### Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV   Cache and Parallel Decoding
**Authors**: Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-07-03T04:51:05Z

**Summary**: Diffusion-based large language models (Diffusion LLMs) have shown promise for non-autoregressive text generation with parallel decoding capabilities. However, the practical inference speed of open-sourced Diffusion LLMs often lags behind autoregressive models due to the lack of Key-Value (KV) Cache and quality degradation when decoding multiple tokens simultaneously. To bridge this gap, we introduce a novel block-wise approximate KV Cache mechanism tailored for bidirectional diffusion models, enabling cache reuse with negligible performance drop. Additionally, we identify the root cause of generation quality degradation in parallel decoding as the disruption of token dependencies under the conditional independence assumption. To address this, we propose a confidence-aware parallel decoding strategy that selectively decodes tokens exceeding a confidence threshold, mitigating dependency violations and maintaining generation quality. Experimental results on LLaDA and Dream models across multiple LLM benchmarks demonstrate up to \textbf{27.6$\times$ throughput} improvement with minimal accuracy loss, closing the performance gap with autoregressive models and paving the way for practical deployment of Diffusion LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.22618v3),  [pdf](http://arxiv.org/pdf/2505.22618v3)

**Tags**: cs.CL 



### PhysicsCorrect: A Training-Free Approach for Stable Neural PDE   Simulations
**Authors**: Xinquan Huang, Paris Perdikaris

**Updated**: 2025-07-03T01:22:57Z

**Summary**: Neural networks have emerged as powerful surrogates for solving partial differential equations (PDEs), offering significant computational speedups over traditional methods. However, these models suffer from a critical limitation: error accumulation during long-term rollouts, where small inaccuracies compound exponentially, eventually causing complete divergence from physically valid solutions. We present PhysicsCorrect, a training-free correction framework that enforces PDE consistency at each prediction step by formulating correction as a linearized inverse problem based on PDE residuals. Our key innovation is an efficient caching strategy that precomputes the Jacobian and its pseudoinverse during an offline warm-up phase, reducing computational overhead by two orders of magnitude compared to standard correction approaches. Across three representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction errors by up to 100x while adding negligible inference time (under 5\%). The framework integrates seamlessly with diverse architectures including Fourier Neural Operators, UNets, and Vision Transformers, effectively transforming unstable neural surrogates into reliable simulation tools that bridge the gap between deep learning's computational efficiency and the physical fidelity demanded by practical scientific applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.02227v1),  [pdf](http://arxiv.org/pdf/2507.02227v1)

**Tags**: cs.LG 



### Autoregressive Image Generation with Linear Complexity: A Spatial-Aware   Decay Perspective
**Authors**: Yuxin Mao, Zhen Qin, Jinxing Zhou, Hui Deng, Xuyang Shen, Bin Fan, Jing Zhang, Yiran Zhong, Yuchao Dai

**Updated**: 2025-07-02T12:27:06Z

**Summary**: Autoregressive (AR) models have garnered significant attention in image generation for their ability to effectively capture both local and global structures within visual data. However, prevalent AR models predominantly rely on the transformer architectures, which are beset by quadratic computational complexity concerning input sequence length and substantial memory overhead due to the necessity of maintaining key-value caches. Although linear attention mechanisms have successfully reduced this burden in language models, our initial experiments reveal that they significantly degrade image generation quality because of their inability to capture critical long-range dependencies in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a novel attention mechanism that explicitly preserves genuine 2D spatial relationships within the flattened image sequences by computing position-dependent decay factors based on true 2D spatial location rather than 1D sequence positions. Based on this mechanism, we present LASADGen, an autoregressive image generator that enables selective attention to relevant spatial contexts with linear complexity. Experiments on ImageNet show LASADGen achieves state-of-the-art image generation performance and computational efficiency, bridging the gap between linear attention's efficiency and spatial understanding needed for high-quality generation.

**Link**: [arxiv](http://arxiv.org/abs/2507.01652v1),  [pdf](http://arxiv.org/pdf/2507.01652v1)

**Tags**: cs.CV cs.AI cs.MM 



### Learned-Database Systems Security
**Authors**: Roei Schuster, Jin Peng Zhou, Thorsten Eisenhofer, Paul Grubbs, Nicolas Papernot

**Updated**: 2025-07-02T10:16:58Z

**Summary**: A learned database system uses machine learning (ML) internally to improve performance. We can expect such systems to be vulnerable to some adversarial-ML attacks. Often, the learned component is shared between mutually-distrusting users or processes, much like microarchitectural resources such as caches, potentially giving rise to highly-realistic attacker models. However, compared to attacks on other ML-based systems, attackers face a level of indirection as they cannot interact directly with the learned model. Additionally, the difference between the attack surface of learned and non-learned versions of the same system is often subtle. These factors obfuscate the de-facto risks that the incorporation of ML carries. We analyze the root causes of potentially-increased attack surface in learned database systems and develop a framework for identifying vulnerabilities that stem from the use of ML. We apply our framework to a broad set of learned components currently being explored in the database community. To empirically validate the vulnerabilities surfaced by our framework, we choose 3 of them and implement and evaluate exploits against these. We show that the use of ML cause leakage of past queries in a database, enable a poisoning attack that causes exponential memory blowup in an index structure and crashes it in seconds, and enable index users to snoop on each others' key distributions by timing queries over their own keys. We find that adversarial ML is an universal threat against learned components in database systems, point to open research gaps in our understanding of learned-systems security, and conclude by discussing mitigations, while noting that data leakage is inherent in systems whose learned component is shared between multiple parties.

**Link**: [arxiv](http://arxiv.org/abs/2212.10318v4),  [pdf](http://arxiv.org/pdf/2212.10318v4)

**Tags**: cs.CR cs.LG 



### A new efficient RPKI Design
**Authors**: Haya Schulmann, Niklas Vogel

**Updated**: 2025-07-02T08:24:50Z

**Summary**: Resource Public Key Infrastructure (RPKI) is a critical security mechanism for BGP, but the complexity of its architecture is a growing concern as its adoption scales. Current RPKI design heavily reuses legacy PKI components, such as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols, all these introduce excessive cryptographic validation, redundant metadata, and inefficiencies in both storage and processing. We show that these design choices, although based on established standards, create significant performance bottlenecks, increase the vulnerability surface, and hinder scalability for wide-scale Internet deployment.   In this paper, we perform the first systematic analysis of the root causes of complexity in RPKI's design and experimentally quantify their real-world impact. We show that over 70% of validation time in RPKI relying parties is spent on certificate parsing and signature verification, much of it unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI), a backwards-compatible redesign that preserves all security guarantees while substantially reducing protocol overhead. iRPKI eliminates EE-certificates and ROA signatures, merges revocation and integrity objects, replaces verbose encodings with Protobuf, and restructures repository metadata for more efficient access. We experimentally demonstrate that our implementation of iRPKI in the Routinator validator achieves a 20x speed-up of processing time, 18x improvement of bandwidth requirements and 8x reduction in cache memory footprint, while also eliminating classes of vulnerabilities that have led to at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the feasibility of deploying RPKI at scale in the Internet, and especially in constrained environments. Our design may be deployed incrementally without impacting existing operations.

**Link**: [arxiv](http://arxiv.org/abs/2507.01465v1),  [pdf](http://arxiv.org/pdf/2507.01465v1)

**Tags**: cs.CR 



### EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices
**Authors**: Zheyu Shen, Yexiao He, Ziyao Wang, Yuning Zhang, Guoheng Sun, Wanghao Ye, Ang Li

**Updated**: 2025-07-02T07:47:28Z

**Summary**: Large Language Models (LLMs) have gained significant attention due to their versatility across a wide array of applications. Fine-tuning LLMs with parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these models to efficiently adapt to downstream tasks without extensive retraining. Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial benefits, such as reduced latency, enhanced privacy, and personalized responses. However, serving LLMs efficiently on resource-constrained edge devices presents critical challenges, including the complexity of adapter selection for different tasks and memory overhead from frequent adapter swapping. Moreover, given the multiple requests in multi-tenant settings, processing requests sequentially results in underutilization of computational resources and increased latency. This paper introduces EdgeLoRA, an efficient system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA incorporates three key innovations: (1) an adaptive adapter selection mechanism to streamline the adapter configuration process; (2) heterogeneous memory management, leveraging intelligent adapter caching and pooling to mitigate memory operation overhead; and (3) batch LoRA inference, enabling efficient batch processing to significantly reduce computational latency. Comprehensive evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly outperforms the status quo (i.e., llama.cpp) in terms of both latency and throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times boost in throughput. Even more impressively, it can serve several orders of magnitude more adapters simultaneously. These results highlight EdgeLoRA's potential to transform edge deployment of LLMs in multi-tenant scenarios, offering a scalable and efficient solution for resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2507.01438v1),  [pdf](http://arxiv.org/pdf/2507.01438v1)

**Tags**: cs.DC cs.AI cs.LG 



### Breaking the Boundaries of Long-Context LLM Inference: Adaptive KV   Management on a Single Commodity GPU
**Authors**: He Sun, Li Li, Mingjun Xiao, Chengzhong Xu

**Updated**: 2025-07-02T05:12:29Z

**Summary**: Advanced Large Language Models (LLMs) have achieved impressive performance across a wide range of complex and long-context natural language tasks. However, performing long-context LLM inference locally on a commodity GPU (a PC) with privacy concerns remains challenging due to the increasing memory demands of the key-value (KV) cache. Existing systems typically identify important tokens and selectively offload their KV data to GPU and CPU memory. The KV data needs to be offloaded to disk due to the limited memory on a commodity GPU, but the process is bottlenecked by token importance evaluation overhead and the disk's low bandwidth. In this paper, we present LeoAM, the first efficient importance-aware long-context LLM inference system for a single commodity GPU with adaptive hierarchical GPU-CPU-Disk KV management. Our system employs an adaptive KV management strategy that partitions KV data into variable-sized chunks based on the skewed distribution of attention weights across different layers to reduce computational and additional transmission overheads. Moreover, we propose a lightweight KV abstract method, which minimizes transmission latency by storing and extracting the KV abstract of each chunk on disk instead of the full KV data. LeoAM also leverages the dynamic compression and pipeline techniques to further accelerate inference. Experimental results demonstrate that LongInfer achieves an average inference latency speedup of 3.46x, while maintaining comparable LLM response quality. In scenarios with larger batch sizes, it achieves up to a 5.47x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2506.20187v2),  [pdf](http://arxiv.org/pdf/2506.20187v2)

**Tags**: cs.OS cs.CR 68M20 C.4 



### AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design
**Authors**: Shakya Jayakody, Youpeng Zhao, Jun Wang

**Updated**: 2025-07-02T00:35:43Z

**Summary**: Graph convolutional networks (GCNs) are fundamental in various scientific applications, ranging from biomedical protein-protein interactions (PPI) to large-scale recommendation systems. An essential component for modeling graph structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As the size of graph data continues to scale up, SpGEMMs are often conducted in an out-of-core fashion due to limited GPU memory space in resource-constrained systems. Albeit recent efforts that aim to alleviate the memory constraints of out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory layout, or performing the computation in sparse format, current systems suffer from both high I/O latency and GPU under-utilization issues.   In this paper, we first identify the problems of existing systems, where sparse format data alignment and memory allocation are the main performance bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the algorithm angle, AIRES proposes to alleviate the data alignment issues on the block level for matrices in sparse formats and develops a tiling algorithm to facilitate row block-wise alignment. On the system level, AIRES employs a three-phase dynamic scheduling that features a dual-way data transfer strategy utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage (GDS), and host memory to reduce I/O latency and improve throughput. Evaluations show that AIRES significantly outperforms the state-of-the-art methods, achieving up to 1.8x lower latency in real-world graph processing benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2507.02006v1),  [pdf](http://arxiv.org/pdf/2507.02006v1)

**Tags**: cs.LG 



### PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile   Device via Additive Side-Tuning
**Authors**: Xingke Yang, Liang Li, Zhiyi Wan, Sicong Li, Hao Wang, Xiaoqi Qi, Jiang Liu, Tomoaki Ohtsuki, Xin Fu, Miao Pan

**Updated**: 2025-07-01T22:27:21Z

**Summary**: There is a huge gap between numerous intriguing applications fostered by on-device large language model (LLM) fine-tuning (FT) from fresh mobile data and the limited resources of a mobile device. While existing server-assisted methods (e.g., split learning or side-tuning) may enable LLM FT on the local mobile device, they suffer from heavy communication burdens of activation transmissions, and may disclose data, labels or fine-tuned models to the server. To address those issues, we develop PAE MobiLLM, a privacy-aware and efficient LLM FT method which can be deployed on the mobile device via server-assisted additive side-tuning. To further accelerate FT convergence and improve computing efficiency, PAE MobiLLM integrates activation caching on the server side, which allows the server to reuse historical activations and saves the mobile device from repeatedly computing forward passes for the recurring data samples. Besides, to reduce communication cost, PAE MobiLLM develops a one-token (i.e., ``pivot'' token) activation shortcut that transmits only a single activation dimension instead of full activation matrices to guide the side network tuning. Last but not least, PAE MobiLLM introduces the additive adapter side-network design which makes the server train the adapter modules based on device-defined prediction differences rather than raw ground-truth labels. In this way, the server can only assist device-defined side-network computing, and learn nothing about data, labels or fine-tuned models.

**Link**: [arxiv](http://arxiv.org/abs/2507.01216v1),  [pdf](http://arxiv.org/pdf/2507.01216v1)

**Tags**: cs.LG cs.CR 



### Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model
**Authors**: Anirud Aggarwal, Abhinav Shrivastava, Matthew Gwilliam

**Updated**: 2025-07-01T21:27:40Z

**Summary**: Diffusion-based image generation models excel at producing high-quality synthetic content, but suffer from slow and computationally expensive inference. Prior work has attempted to mitigate this by caching and reusing features within diffusion transformers across inference steps. These methods, however, often rely on rigid heuristics that result in limited acceleration or poor generalization across architectures. We propose Evolutionary Caching to Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient, per-model, caching schedules forming a Pareto frontier, using only a small set of calibration prompts. ECAD requires no modifications to network parameters or reference images. It offers significant inference speedups, enables fine-grained control over the quality-latency trade-off, and adapts seamlessly to different diffusion models. Notably, ECAD's learned schedules can generalize effectively to resolutions and model variants not seen during calibration. We evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1$.$dev using multiple metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k, PartiPrompts), demonstrating consistent improvements over previous approaches. On PixArt-alpha, ECAD identifies a schedule that outperforms the previous state-of-the-art method by 4.47 COCO FID while increasing inference speedup from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable approach for accelerating diffusion inference. Our project website is available at https://aniaggarwal.github.io/ecad and our code is available at https://github.com/aniaggarwal/ecad.

**Link**: [arxiv](http://arxiv.org/abs/2506.15682v2),  [pdf](http://arxiv.org/pdf/2506.15682v2)

**Tags**: cs.CV 



### FlashDP: Private Training Large Language Models with Efficient DP-SGD
**Authors**: Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang

**Updated**: 2025-07-01T19:28:37Z

**Summary**: As large language models (LLMs) increasingly underpin technological advancements, the privacy of their training data emerges as a critical concern. Differential Privacy (DP) serves as a rigorous mechanism to protect this data, yet its integration via Differentially Private Stochastic Gradient Descent (DP-SGD) introduces substantial challenges, primarily due to the complexities of per-sample gradient clipping. Current explicit methods, such as Opacus, necessitate extensive storage for per-sample gradients, significantly inflating memory requirements. Conversely, implicit methods like GhostClip reduce storage needs by recalculating gradients multiple times, which leads to inefficiencies due to redundant computations. This paper introduces FlashDP, an innovative cache-friendly per-layer DP-SGD that consolidates necessary operations into a single task, calculating gradients only once in a fused manner. This approach not only diminishes memory movement by up to \textbf{50\%} but also cuts down redundant computations by \textbf{20\%}, compared to previous methods. Consequently, FlashDP does not increase memory demands and achieves a \textbf{90\%} throughput compared to the Non-DP method on a four-A100 system during the pre-training of the Llama-13B model, while maintaining parity with standard per-layer clipped DP-SGD in terms of accuracy. These advancements establish FlashDP as a pivotal development for efficient and privacy-preserving training of LLMs. FlashDP's code has been open-sourced in https://github.com/kaustpradalab/flashdp.

**Link**: [arxiv](http://arxiv.org/abs/2507.01154v1),  [pdf](http://arxiv.org/pdf/2507.01154v1)

**Tags**: cs.LG cs.CR 



### Integrating nano- and micrometer-scale energy deposition models for   mechanistic prediction of radiation-induced DNA damage and cell survival
**Authors**: Giulio Bordieri, Marta Missiggia, Gianluca Lattanzi, Carmen Villagrasa, Yann Perrot, Francesco G. Cordoni

**Updated**: 2025-07-01T16:36:23Z

**Summary**: We present an integrated modeling framework that combines the Generalized Stochastic Microdosimetric Model (GSM2), used to predict cell survival fractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for simulating radiation-induced DNA damage in cell populations. This approach enables the generation of spatially and structurally resolved double-strand break (DSB) distributions, capturing key features such as damage complexity and chromosome specificity. A novel application of the DBSCAN clustering algorithm is introduced to group DSBs at the micrometer scale. This allows the identification of physical aggregates of DNA damage and their association with subnuclear domains, providing a direct link to the cell survival probability as predicted by \gsm.   The model was validated using experimental data from HUVEC cells irradiated with 220 kV X-rays and H460 cells exposed to protons over a wide linear energy transfer (LET) range, from approximately 4 keV/{\mu}m to over 20 keV/{\mu}m. Results show excellent agreement between simulations and experimental survival probabilities, making this one of the first consistent multi-scale models to bridge nanodosimetric and microdosimetric representations of radiation with biological outcomes such as cell survival.   By incorporating the inherent stochastic nature of radiation-matter interactions, this framework effectively connects the physical properties of the radiation field to the biological response at the cellular level. Its accuracy across various radiation types and energies supports its potential for use in biologically optimized radiotherapy.

**Link**: [arxiv](http://arxiv.org/abs/2507.00929v1),  [pdf](http://arxiv.org/pdf/2507.00929v1)

**Tags**: physics.bio-ph 



### VEDA: Efficient LLM Generation Through Voting-based KV Cache Eviction   and Dataflow-flexible Accelerator
**Authors**: Zhican Wang, Hongxiang Fan, Haroon Waris, Gang Wang, Zhenyu Li, Jianfei Jiang, Yanan Sun, Guanghui He

**Updated**: 2025-07-01T14:30:31Z

**Summary**: Large Language Models (LLMs) excel in natural language processing tasks but pose significant computational and memory challenges for edge deployment due to their intensive resource demands. This work addresses the efficiency of LLM inference by algorithm-hardware-dataflow tri-optimizations. We propose a novel voting-based KV cache eviction algorithm, balancing hardware efficiency and algorithm accuracy by adaptively identifying unimportant kv vectors. From a dataflow perspective, we introduce a flexible-product dataflow and a runtime reconfigurable PE array for matrix-vector multiplication. The proposed approach effectively handles the diverse dimensional requirements and solves the challenges of incrementally varying sequence lengths. Additionally, an element-serial scheduling scheme is proposed for nonlinear operations, such as softmax and layer normalization (layernorm). Results demonstrate a substantial reduction in latency, accompanied by a significant decrease in hardware complexity, from O(N) to O(1). The proposed solution is realized in a custom-designed accelerator, VEDA, which outperforms existing hardware platforms. This research represents a significant advancement in LLM inference on resource-constrained edge devices, facilitating real-time processing, enhancing data privacy, and enabling model customization.

**Link**: [arxiv](http://arxiv.org/abs/2507.00797v1),  [pdf](http://arxiv.org/pdf/2507.00797v1)

**Tags**: cs.AR 



### On Hierarchical Coded Caching with Offline Users
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-07-01T13:17:46Z

**Summary**: This paper studies a two-layer hierarchical network in which some users are offline during the content delivery phase. A two-layer hierarchical network consists of a single server connected to multiple cache-aided mirror sites, and each mirror site is connected to a distinct set of cache-aided users. A scheme for such a hierarchical system with offline users has been proposed recently but considered a special case where all mirror caches have zero memory, which is a significant limitation. We propose an array known as a hierarchical hotplug placement delivery array (HHPDA), which describes the placement and delivery phases of a coded caching scheme for a general two-layer hierarchical network with offline users. Further, we construct a class of HHPDAs using combinatorial t-designs.

**Link**: [arxiv](http://arxiv.org/abs/2507.00727v1),  [pdf](http://arxiv.org/pdf/2507.00727v1)

**Tags**: cs.IT math.IT 



### Accelerating Loading WebGraphs in ParaGrapher
**Authors**: Mohsen Koohi Esfahani

**Updated**: 2025-07-01T12:51:09Z

**Summary**: ParaGrapher is a graph loading API and library that enables graph processing frameworks to load large-scale compressed graphs with minimal overhead. This capability accelerates the design and implementation of new high-performance graph algorithms and their evaluation on a wide range of graphs and across different frameworks. However, our previous study identified two major limitations in ParaGrapher: inefficient utilization of high-bandwidth storage and reduced decompression bandwidth due to increased compression ratios. To address these limitations, we present two optimizations for ParaGrapher in this paper. To improve storage utilization, particularly for high-bandwidth storage, we introduce ParaGrapher-FUSE (PG-Fuse) a filesystem based on the FUSE (Filesystem in User Space). PG-Fuse optimizes storage access by increasing the size of requested blocks, reducing the number of calls to the underlying filesystem, and caching the received blocks in memory for future calls. To improve the decompression bandwidth, we introduce CompBin, a compact binary representation of the CSR format. CompBin facilitates direct accesses to neighbors while preventing storage usage for unused bytes. Our evaluation on 12 real-world and synthetic graphs with up to 128 billion edges shows that PG-Fuse and CompBin achieve up to 7.6 and 21.8 times speedup, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2507.00716v1),  [pdf](http://arxiv.org/pdf/2507.00716v1)

**Tags**: cs.DC 



### EARN: Efficient Inference Acceleration for LLM-based Generative   Recommendation by Register Tokens
**Authors**: Chaoqun Yang, Xinyu Lin, Wenjie Wang, Yongqi Li, Teng Sun, Xianjing Han, Tat-Seng Chua

**Updated**: 2025-07-01T12:42:06Z

**Summary**: Large Language Model-based generative recommendation (LLMRec) has achieved notable success, but it suffers from high inference latency due to massive computational overhead and memory pressure of KV Cache. Existing KV Cache reduction methods face critical limitations: cache compression offers marginal acceleration given recommendation tasks' short decoding steps, while prompt compression risks discarding vital interaction history. Through systematic analysis of attention patterns in LLMRec, we uncover two pivotal insights: 1) layer-wise attention sparsity inversion where early layers retain dense informative patterns while later layers exhibit high redundancy, and 2) dual attention sinks phenomenon where attention scores concentrate on both head and tail tokens of input sequences. Motivated by these insights, we propose EARN, an efficient inference framework that leverages the early layers to compress information into register tokens placed at the input sequence boundaries, then focuses solely on these tokens in the subsequent layers. Extensive experiments on three datasets, two LLMRec methods and two LLM architectures demonstrate EARN's superiority, achieving up to 3.79x speedup and 80.8% KV Cache reduction with better accuracy than the general finetuning approach. Our work bridges the efficiency-effectiveness gap in LLMRec, offering practical deployment advantages for industrial scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2507.00715v1),  [pdf](http://arxiv.org/pdf/2507.00715v1)

**Tags**: cs.IR 



### Structural, dielectric, and ferroelectric characteristics of the   low-temperature sintered 65PMN-35PT sample for electroceramic applications
**Authors**: B. Ramachandran, N. Sudarshan, G. Mangamma, M. S. Ramachandra Rao

**Updated**: 2025-07-01T09:47:38Z

**Summary**: A single-phase 65PMN-35PT ceramic was synthesized at a relatively low temperature (875 oC) using a modified columbite method. X-ray diffraction analysis confirmed the single-phase formation of perovskite 65PMN-35PT with a tetragonal structure. Morphological studies indicated that the sample consisted of small grains with a size of about 2 micro-m. The dielectric properties of the material demonstrate its relaxor behavior near the ferroelectric transition temperature, TC = 457 K. The saturation and remnant polarization values of approximately 25.9 and 20.1 micro-C cm-2 were achieved for an electrically poled sample. Additionally, the poling induced a negative internal electric field of about -0.2 kV cm-1 was detected due to the presence of ferroelectric nano-grains in this bulk 65PMN-35PT sample. These observed characteristics of the pyrochlore-free 65PMN-35PT ceramic are similar to those of its single-crystal counterpart.

**Link**: [arxiv](http://arxiv.org/abs/2507.00614v1),  [pdf](http://arxiv.org/pdf/2507.00614v1)

**Tags**: cond-mat.mtrl-sci 



### Unleashing the Potential of All Test Samples: Mean-Shift Guided   Test-Time Adaptation
**Authors**: Jizhou Han, Chenhao Ding, SongLin Dong, Yuhang He, Xinyuan Gao, Yihong Gong

**Updated**: 2025-07-01T06:22:00Z

**Summary**: Visual-language models (VLMs) like CLIP exhibit strong generalization but struggle with distribution shifts at test time. Existing training-free test-time adaptation (TTA) methods operate strictly within CLIP's original feature space, relying on high-confidence samples while overlooking the potential of low-confidence ones. We propose MS-TTA, a training-free approach that enhances feature representations beyond CLIP's space using a single-step k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA improves feature compactness and class separability, leading to more stable adaptation. Additionally, a cache of refined embeddings further enhances inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms state-of-the-art training-free TTA methods, achieving robust adaptation without requiring additional training.

**Link**: [arxiv](http://arxiv.org/abs/2507.00462v1),  [pdf](http://arxiv.org/pdf/2507.00462v1)

**Tags**: cs.CV 



### A Minimalist Method for Fine-tuning Text-to-Image Diffusion Models
**Authors**: Yanting Miao, William Loh, Pacal Poupart, Suraj Kothawade

**Updated**: 2025-07-01T05:46:31Z

**Summary**: Recent work uses reinforcement learning (RL) to fine-tune text-to-image diffusion models, improving text-image alignment and sample quality. However, existing approaches introduce unnecessary complexity: they cache the full sampling trajectory, depend on differentiable reward models or large preference datasets, or require specialized guidance techniques. Motivated by the "golden noise" hypothesis -- that certain initial noise samples can consistently yield superior alignment -- we introduce Noise PPO, a minimalist RL algorithm that leaves the pre-trained diffusion model entirely frozen and learns a prompt-conditioned initial noise generator. Our approach requires no trajectory storage, reward backpropagation, or complex guidance tricks. Extensive experiments show that optimizing the initial noise distribution consistently improves alignment and sample quality over the original model, with the most significant gains at low inference steps. As the number of inference steps increases, the benefit of noise optimization diminishes but remains present. These findings clarify the scope and limitations of the golden noise hypothesis and reinforce the practical value of minimalist RL fine-tuning for diffusion models.

**Link**: [arxiv](http://arxiv.org/abs/2506.12036v3),  [pdf](http://arxiv.org/pdf/2506.12036v3)

**Tags**: cs.LG cs.AI 



### RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache   Compression
**Authors**: Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov

**Updated**: 2025-06-30T19:01:18Z

**Summary**: Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme.

**Link**: [arxiv](http://arxiv.org/abs/2502.14051v2),  [pdf](http://arxiv.org/pdf/2502.14051v2)

**Tags**: cs.CL cs.LG 



### Combinatorial Multi-Access Coded Caching with Private Caches under   Intersecting Index Constraints
**Authors**: Dhruv Pratap Singh, Anjana A. Mahesh, B. Sundar Rajan

**Updated**: 2025-06-30T17:07:59Z

**Summary**: We consider the coded caching system where each user, equipped with a private cache, accesses a distinct r-subset of access caches. A central server housing a library of files populates both private and access caches using uncoded placement. In this work, we focus on a constrained indexing regime, referred to as the intersection class, in which the sets used to index the demands of each user must have a nonempty intersection. This regime models resource-limited IoT scenarios such as edge-assisted IoT systems, where devices with small private caches connect to a small number of shared caches. We provide a necessary and sufficient condition under which the system parameters fall within this intersection class. Under this condition, we propose a centralized coded caching scheme and characterize its rate-memory trade-off. Next, we define a uniform-intersection subclass and establish a condition under which the system belongs to this subclass. Within this subclass, the proposed scheme has a regular structure, with each transmission benefiting the same number of users, and we characterize its rate-memory trade-off. Additionally, we derive an index coding-based lower bound on the minimum achievable worst-case rate under uncoded placement. Finally, we provide numerical comparisons between the rate of the proposed scheme, the new lower bound, and bounds from the original work.

**Link**: [arxiv](http://arxiv.org/abs/2506.24060v1),  [pdf](http://arxiv.org/pdf/2506.24060v1)

**Tags**: cs.IT math.IT 



### Full Version: (De/Re)-Composition of Data-Parallel Computations via   Multi-Dimensional Homomorphisms
**Authors**: Ari Rasch

**Updated**: 2025-06-30T16:23:35Z

**Summary**: We formally introduce a systematic (de/re)-composition approach, based on the algebraic formalism of "Multi-Dimensional Homomorphisms (MDHs)". Our approach is designed as general enough to be applicable to a wide range of data-parallel computations and for various kinds of target parallel architectures. To efficiently target the deep and complex memory and core hierarchies of contemporary architectures, we exploit our introduced (de/re)-composition approach for a correct-by-construction, parametrized cache blocking and parallelization strategy. We show that our approach is powerful enough to express, in the same formalism, the (de/re)-composition strategies of different classes of state-of-the-art approaches (scheduling-based, polyhedral, etc), and we demonstrate that the parameters of our strategies enable systematically generating code that can be fully automatically optimized (auto-tuned) for the particular target architecture and characteristics of the input and output data (e.g., their sizes and memory layouts). Particularly, our experiments confirm that via auto-tuning, we achieve higher performance than state-of-the-art approaches, including hand-optimized solutions provided by vendors (such as NVIDIA cuBLAS/cuDNN and Intel oneMKL/oneDNN), on real-world data sets and for a variety of data-parallel computations, including: linear algebra routines, stencil and quantum chemistry computations, data mining algorithms, and computations that recently gained high attention due to their relevance for deep learning.

**Link**: [arxiv](http://arxiv.org/abs/2405.05118v4),  [pdf](http://arxiv.org/pdf/2405.05118v4)

**Tags**: cs.PL 



### Large-scale Neural Network Quantum States for ab initio Quantum   Chemistry Simulations on Fugaku
**Authors**: Hongtao Xu, Zibo Wu, Mingzhen Li, Weile Jia

**Updated**: 2025-06-30T12:55:59Z

**Summary**: Solving quantum many-body problems is one of the fundamental challenges in quantum chemistry. While neural network quantum states (NQS) have emerged as a promising computational tool, its training process incurs exponentially growing computational demands, becoming prohibitively expensive for large-scale molecular systems and creating fundamental scalability barriers for real-world applications. To address above challenges, we present \ours, a high-performance NQS training framework for \textit{ab initio} electronic structure calculations. First, we propose a scalable sampling parallelism strategy with multi-layers workload division and hybrid sampling scheme, which break the scalability barriers for large-scale NQS training. Then, we introduce multi-level parallelism local energy parallelism, enabling more efficient local energy computation. Last, we employ cache-centric optimization for transformer-based \textit{ansatz} and incorporate it with sampling parallelism strategy, which further speedup up the NQS training and achieve stable memory footprint at scale. Experiments demonstrate that \ours accelerate NQS training with up to 8.41x speedup and attains a parallel efficiency up to 95.8\% when scaling to 1,536 nodes.

**Link**: [arxiv](http://arxiv.org/abs/2506.23809v1),  [pdf](http://arxiv.org/pdf/2506.23809v1)

**Tags**: cs.DC 



### VQ-LLM: High-performance Code Generation for Vector Quantization   Augmented LLM Inference
**Authors**: Zihan Liu, Xinhao Luo, Junxian Guo, Wentao Ni, Yangjie Zhou, Yue Guan, Cong Guo, Weihao Cui, Yu Feng, Minyi Guo, Yuhao Zhu, Minjia Zhang, Jingwen Leng, Chen Jin

**Updated**: 2025-06-30T05:54:40Z

**Summary**: In this work, we design and implement VQ-LLM, an efficient fused Vector Quantization (VQ) kernel generation framework. We first introduce a software abstraction called codebook cache to optimize codebook access efficiency and support the integration of VQ with various computations. The codebook cache adaptively stores different entries across the GPU's memory hierarchy, including off-chip global memory, on-chip shared memory, and registers. Centered around the codebook cache, we design an efficient computation engine that optimizes memory traffic during computations involving codebooks. This compute engine adopts the codebook-centric dataflow and fusion optimizations. Additionally, we provide adaptive heuristics to tailor parameter selection in our optimizations to diverse VQ configurations. Our optimizations achieve an average latency reduction of 46.13% compared to unoptimized versions. Compared to existing open-source implementations, our methods decrease latency by 64.36% to 99.1%. A final comparison with state-of-the-art element-wise quantization methods like AWQ and KVQuant shows that our VQ-LLM is practically viable, achieving latencies close or even better latencies to those at equivalent bit-widths, potentially offering greater accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.02236v2),  [pdf](http://arxiv.org/pdf/2503.02236v2)

**Tags**: cs.DC 



### FlexRAG: A Flexible and Comprehensive Framework for Retrieval-Augmented   Generation
**Authors**: Zhuocheng Zhang, Yang Feng, Min Zhang

**Updated**: 2025-06-30T05:45:43Z

**Summary**: Retrieval-Augmented Generation (RAG) plays a pivotal role in modern large language model applications, with numerous existing frameworks offering a wide range of functionalities to facilitate the development of RAG systems. However, we have identified several persistent challenges in these frameworks, including difficulties in algorithm reproduction and sharing, lack of new techniques, and high system overhead. To address these limitations, we introduce \textbf{FlexRAG}, an open-source framework specifically designed for research and prototyping. FlexRAG supports text-based, multimodal, and network-based RAG, providing comprehensive lifecycle support alongside efficient asynchronous processing and persistent caching capabilities. By offering a robust and flexible solution, FlexRAG enables researchers to rapidly develop, deploy, and share advanced RAG systems. Our toolkit and resources are available at \href{https://github.com/ictnlp/FlexRAG}{https://github.com/ictnlp/FlexRAG}.

**Link**: [arxiv](http://arxiv.org/abs/2506.12494v2),  [pdf](http://arxiv.org/pdf/2506.12494v2)

**Tags**: cs.CL cs.IR 



### RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM   Inference
**Authors**: Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang

**Updated**: 2025-06-30T05:21:58Z

**Summary**: The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2505.02922v2),  [pdf](http://arxiv.org/pdf/2505.02922v2)

**Tags**: cs.LG 



### Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent   Metasurfaces
**Authors**: Geng Sun, Mingzhe Fan, Lei Zhang, Hongyang Pan, Jiahui Li, Chuang Zhang, Linyao Li, Changyuan Zhao, Chau Yuen

**Updated**: 2025-06-30T03:22:32Z

**Summary**: Wireless communication systems face significant challenges in meeting the increasing demands for higher data rates and more reliable connectivity in complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for realizing wave-domain signal processing, with mobile SIMs offering superior communication performance compared to their fixed counterparts. In this paper, we investigate a novel unmanned aerial vehicle (UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the low-altitude economy (LAE) networks paradigm, where UAVs function as both base stations that cache SIM-processed data and mobile platforms that flexibly deploy SIMs to enhance uplink communications from ground users. To maximize network capacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that comprehensively addresses three critical aspects: the association between UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and the phase shifts across multiple SIM layers. Due to the inherent non-convexity and NP-hardness of USBJOP, we decompose it into three sub-optimization problems, \textit{i.e.}, association between UAV-SIMs and users optimization problem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase shifts optimization problem (USPSOP), and solve them using an alternating optimization strategy. Specifically, we transform AUUOP and ULOP into convex forms solvable by the CVX tool, while addressing USPSOP through a generative artificial intelligence (GAI)-based hybrid optimization algorithm. Simulations demonstrate that our proposed approach significantly outperforms benchmark schemes, achieving approximately 1.5 times higher network capacity compared to suboptimal alternatives. Additionally, our proposed GAI method reduces the algorithm runtime by 10\% while maintaining solution quality.

**Link**: [arxiv](http://arxiv.org/abs/2506.23488v1),  [pdf](http://arxiv.org/pdf/2506.23488v1)

**Tags**: cs.NI 



### CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors   upon GPGPU Platforms
**Authors**: Faaiq Waqar, Ming-Yen Lee, Seongwon Yoon, Seongkwang Lim, Shimeng Yu

**Updated**: 2025-06-29T21:55:58Z

**Summary**: In contemporary general-purpose graphics processing units (GPGPUs), the continued increase in raw arithmetic throughput is constrained by the capabilities of the register file (single-cycle) and last-level cache (high bandwidth), which require the delivery of operands at a cadence demanded by wide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity, density, or bandwidth of these memories can unlock substantial performance gains; however, the recent stagnation of SRAM bit-cell scaling leads to inequivalent losses in compute density.   To address the challenges posed by SRAM's scaling and leakage power consumption, this paper explores the potential CMOS+X integration of amorphous oxide semiconductor (AOS) transistors in capacitive, persistent memory topologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in multi-ported and high-bandwidth banked GPGPU memories. A detailed study of the density and energy tradeoffs of back-end-of-line (BEOL) integrated memories utilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while accounting for the macro-level limitations of integrating AOS candidate structures proposed by the device community (an aspect often overlooked in prior work). By exploiting the short lifetime of register operands, we propose a multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of the footprint of SRAM with over 70% lower standby power, enabling enhancements to compute capacity, such as larger warp sizes or processor counts. Benchmarks run on a validated NVIDIA Ampere-class GPU model, using a modified version of Accel-Sim, demonstrate improvements of up to 5.2x the performance per watt and an average 8% higher geometric mean instruction per cycle (IPC) on various compute- and memory-bound tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.23405v1),  [pdf](http://arxiv.org/pdf/2506.23405v1)

**Tags**: cs.ET cs.AR B.8.2; B.3.1 



### Scaling Out Chip Interconnect Networks with Implicit Sequence Numbers
**Authors**: Giyong Jung, Saeid Gorgin, John Kim, Jungrae Kim

**Updated**: 2025-06-28T13:02:17Z

**Summary**: As AI models outpace the capabilities of single processors, interconnects across chips have become a critical enabler for scalable computing. These processors exchange massive amounts of data at cache-line granularity, prompting the adoption of new interconnect protocols like CXL, NVLink, and UALink, designed for high bandwidth and small payloads. However, the increasing transfer rates of these protocols heighten susceptibility to errors. While mechanisms like Cyclic Redundancy Check (CRC) and Forward Error Correction (FEC) are standard for reliable data transmission, scaling chip interconnects to multi-node configurations introduces new challenges, particularly in managing silently dropped flits in switching devices. This paper introduces Implicit Sequence Number (ISN), a novel mechanism that ensures precise flit drop detection and in-order delivery without adding header overhead. Additionally, we propose Reliability Extended Link (RXL), an extension of CXL that incorporates ISN to support scalable, reliable multi-node interconnects while maintaining compatibility with the existing flit structure. By elevating CRC to a transport-layer mechanism for end-to-end data and sequence integrity, and relying on FEC for link-layer error correction and detection, RXL delivers robust reliability and scalability without compromising bandwidth efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2507.01988v1),  [pdf](http://arxiv.org/pdf/2507.01988v1)

**Tags**: cs.NI 



### ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in   Large Language Models
**Authors**: Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren

**Updated**: 2025-06-28T07:25:12Z

**Summary**: Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.

**Link**: [arxiv](http://arxiv.org/abs/2506.22791v1),  [pdf](http://arxiv.org/pdf/2506.22791v1)

**Tags**: cs.CL cs.DB 



### PromptDSI: Prompt-based Rehearsal-free Continual Learning for Document   Retrieval
**Authors**: Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Yinwei Wei, Trung Le, Dragan Gasevic, Yuan-Fang Li, Thanh-Toan Do

**Updated**: 2025-06-28T06:24:44Z

**Summary**: Differentiable Search Index (DSI) utilizes pre-trained language models to perform indexing and document retrieval via end-to-end learning without relying on external indexes. However, DSI requires full re-training to index new documents, causing significant computational inefficiencies. Continual learning (CL) offers a solution by enabling the model to incrementally update without full re-training. Existing CL solutions in document retrieval rely on memory buffers or generative models for rehearsal, which is infeasible when accessing previous training data is restricted due to privacy concerns. To this end, we introduce PromptDSI, a prompt-based, rehearsal-free continual learning approach for document retrieval. PromptDSI follows the Prompt-based Continual Learning (PCL) framework, using learnable prompts to efficiently index new documents without accessing previous documents or queries. To improve retrieval latency, we remove the initial forward pass of PCL, which otherwise greatly increases training and inference time, with a negligible trade-off in performance. Additionally, we introduce a novel topic-aware prompt pool that employs neural topic embeddings as fixed keys, eliminating the instability of prompt key optimization while maintaining competitive performance with existing PCL prompt pools. In a challenging rehearsal-free continual learning setup, we demonstrate that PromptDSI variants outperform rehearsal-based baselines, match the strong cache-based baseline in mitigating forgetting, and significantly improving retrieval performance on new corpora.

**Link**: [arxiv](http://arxiv.org/abs/2406.12593v4),  [pdf](http://arxiv.org/pdf/2406.12593v4)

**Tags**: cs.IR cs.AI cs.CL cs.LG 



### Efficiently Serving Large Multimodal Models Using EPD Disaggregation
**Authors**: Gursimran Singh, Xinglu Wang, Yifan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-06-28T03:53:17Z

**Summary**: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step negatively affects key Service Level Objectives (SLOs), such as time to first token (TTFT) and time per output token (TPOT). We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our approach decouples these steps, unlocking new opportunities and optimizations. These include a mechanism to cache multimedia tokens for efficient transfer, a novel way to parallelize the encoding load within a request, a module for optimal resource allocation for disaggregated serving, and a novel role-switching method to handle changing workload characteristics. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15x lower peak memory utilization), batch sizes (up to 22x larger), 10x more images per request, and 2.2x larger KV caches. Furthermore, it leads to significant improvements in SLO attainment (up to 90-100% improvement) and TTFT (up to 71% reduction), compared to systems that do not disaggregate. The code is available at https://github.com/vbdi/epdserve.

**Link**: [arxiv](http://arxiv.org/abs/2501.05460v4),  [pdf](http://arxiv.org/pdf/2501.05460v4)

**Tags**: cs.DC cs.AI cs.CV cs.LG 



### QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting,   KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization
**Authors**: Danush Khanna, Aditya Kumar Guru, Srivarshinee Sridhar, Zidan Ahmed, Rubhav Bahirwani, Meetu Malhotra, Vinija Jain, Aman Chadha, Amitava Das, Kripabandhu Ghosh

**Updated**: 2025-06-27T17:10:32Z

**Summary**: Inference accounts for the majority of latency and energy consumption in large language model (LLM) deployments, often exceeding 90% of total cost. While training-time efficiency has seen extensive progress, runtime optimization remains a key bottleneck, particularly under autoregressive decoding. Existing approaches -- such as pruning, quantization, early exits, and speculative decoding -- often require retraining, architectural changes, or disrupt decoding compatibility. We introduce QuickSilver, a modular, token-level framework that enables semantic adaptivity at inference time without altering model weights or structure. QuickSilver integrates four synergistic mechanisms:   (i) Dynamic Token Halting, which halts computation for tokens with converged representations; (ii) KV Cache Skipping, which selectively suppresses memory writes to reduce attention overhead; and (iii) Contextual Token Fusion, which collapses redundant tokens into shared paths to shrink sequence length.   Unlike speculative decoding or MoE routing, QuickSilver operates entirely on frozen, dense models and requires no auxiliary networks. Applied to GPT-2 and Llama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP reduction with negligible perplexity degradation (<=0.2).

**Link**: [arxiv](http://arxiv.org/abs/2506.22396v1),  [pdf](http://arxiv.org/pdf/2506.22396v1)

**Tags**: cs.CL cs.AI I.2.0; I.2.7 



### SiPipe: Bridging the CPU-GPU Utilization Gap for Efficient   Pipeline-Parallel LLM Inference
**Authors**: Yongchao He, Bohan Zhao, Zheng Cao

**Updated**: 2025-06-27T09:27:04Z

**Summary**: As inference workloads for large language models (LLMs) scale to meet growing user demand, pipeline parallelism (PP) has become a widely adopted strategy for multi-GPU deployment, particularly in cross-node setups, to improve key-value (KV) cache capacity and inference throughput. However, PP suffers from inherent inefficiencies caused by three types of execution bubbles-load-imbalance, intra-stage, and inter-stage-which limit pipeline saturation. We present SiPipe, a heterogeneous pipeline design that improves throughput by leveraging underutilized CPU resources to offload auxiliary computation and communication. SiPipe incorporates three key techniques-CPU sampling, a token-safe execution model, and structure-aware transmission-to mitigate pipeline bubbles and improve execution efficiency. Across diverse LLMs, SiPipe achieves up to 2.1 times higher throughput, 43% lower per-token latency, and up to 23% higher average GPU utilization compared to the state-of-the-art vLLM under the same PP configuration, demonstrating its generality across LLMs and deployment scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2506.22033v1),  [pdf](http://arxiv.org/pdf/2506.22033v1)

**Tags**: cs.DC 



### ChunkKV: Semantic-Preserving KV Cache Compression for Efficient   Long-Context LLM Inference
**Authors**: Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, Xiaowen Chu

**Updated**: 2025-06-27T09:14:02Z

**Summary**: Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\% of total memory during inference. Although existing compression methods reduce memory by evaluating the importance of individual tokens, they overlook critical semantic relationships between tokens, resulting in fragmented context and degraded performance. We introduce ChunkKV, which fundamentally reimagines KV cache compression by treating semantic chunks - rather than isolated tokens - as basic compression units. This approach preserves complete linguistic structures and contextual integrity, ensuring that essential meaning is retained even under aggressive compression. Our innovation includes a novel layer-wise index reuse technique that exploits the higher cross-layer similarity of preserved indices in ChunkKV, reducing computational overhead and improving throughput by 26.5\%. Comprehensive evaluations on challenging benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV outperforms state-of-the-art methods by up to 8.7\% in precision while maintaining the same compression ratio. These results confirm that semantic-aware compression significantly enhances both efficiency and performance for long-context LLM inference, providing a simple yet effective solution to the memory bottleneck problem.

**Link**: [arxiv](http://arxiv.org/abs/2502.00299v3),  [pdf](http://arxiv.org/pdf/2502.00299v3)

**Tags**: cs.CL 



### A Survey of LLM Inference Systems
**Authors**: James Pan, Guoliang Li

**Updated**: 2025-06-27T04:38:20Z

**Summary**: The past few years has witnessed specialized large language model (LLM) inference systems, such as vLLM, SGLang, Mooncake, and DeepFlow, alongside rapid LLM adoption via services like ChatGPT. Driving these system design efforts is the unique autoregressive nature of LLM request processing, motivating new techniques for achieving high performance while preserving high inference quality over high-volume and high-velocity workloads. While many of these techniques are discussed across the literature, they have not been analyzed under the framework of a complete inference system, nor have the systems themselves been analyzed and compared.   In this survey, we review these techniques, starting from operators and algorithms for request processing, then moving on to techniques for model optimization and execution, including kernel design, batching, and scheduling, before ending with techniques for memory management, including paged memory, eviction and offloading techniques, quantization, and cache persistence. Through these discussions, we show that these techniques fundamentally rely on load prediction, adaptive mechanisms, and cost reduction in order to overcome the challenges introduced by autoregressive generation and achieve the goals of the system. We then discuss how these techniques can be combined to form single-replica and multi-replica inference systems, including disaggregated inference systems that offer more control over resource allocation and serverless systems that can be deployed over shared hardware infrastructure. We end with a discussion of remaining challenges.

**Link**: [arxiv](http://arxiv.org/abs/2506.21901v1),  [pdf](http://arxiv.org/pdf/2506.21901v1)

**Tags**: cs.DB 



### Round Attention: A Novel Round-Level Attention Mechanism to Accelerate   LLM Inference
**Authors**: Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen

**Updated**: 2025-06-27T03:43:24Z

**Summary**: The increasing context window size in large language models (LLMs) has improved their ability to handle complex, long-text tasks. However, as the conversation rounds continue, it is required to store a large amount of KV cache in GPU memory, which significantly affects the efficiency and even availability of the model serving systems. This paper analyzes dialogue data from real users on the granularity of round and discovers that the LLM inference manifests a watershed layer, after which the distribution of round-level attention shows notable similarity. Based on this, we propose Round Attention - a novel round-level attention mechanism that selectively processes the KV cache of top-k relevant rounds, where k is dynamically determined through the attention matrix in the watershed layer. Theoretical analysis demonstrates that our method reduces memory usage by 54\% to 82\%, while experimental results confirm that loading sparse critical-round KV cache maintains answer accuracy without performance degradation.

**Link**: [arxiv](http://arxiv.org/abs/2502.15294v3),  [pdf](http://arxiv.org/pdf/2502.15294v3)

**Tags**: cs.CL cs.AI 



### FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual   Question Answering
**Authors**: Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn

**Updated**: 2025-06-26T18:51:04Z

**Summary**: While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and two types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

**Link**: [arxiv](http://arxiv.org/abs/2506.21710v1),  [pdf](http://arxiv.org/pdf/2506.21710v1)

**Tags**: cs.CV 



### End-to-End Long Document Summarization using Gradient Caching
**Authors**: Rohit Saxena, Hao Tang, Frank Keller

**Updated**: 2025-06-26T18:40:55Z

**Summary**: Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient $\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.

**Link**: [arxiv](http://arxiv.org/abs/2501.01805v2),  [pdf](http://arxiv.org/pdf/2501.01805v2)

**Tags**: cs.CL cs.AI 



### From Memories to Maps: Mechanisms of In-Context Reinforcement Learning   in Transformers
**Authors**: Ching Fang, Kanaka Rajan

**Updated**: 2025-06-26T17:18:54Z

**Summary**: Humans and animals show remarkable learning efficiency, adapting to new environments with minimal experience. This capability is not well captured by standard reinforcement learning algorithms that rely on incremental value updates. Rapid adaptation likely depends on episodic memory -- the ability to retrieve specific past experiences to guide decisions in novel contexts. Transformers provide a useful setting for studying these questions because of their ability to learn rapidly in-context and because their key-value architecture resembles episodic memory systems in the brain. We train a transformer to in-context reinforcement learn in a distribution of planning tasks inspired by rodent behavior. We then characterize the learning algorithms that emerge in the model. We first find that representation learning is supported by in-context structure learning and cross-context alignment, where representations are aligned across environments with different sensory stimuli. We next demonstrate that the reinforcement learning strategies developed by the model are not interpretable as standard model-free or model-based planning. Instead, we show that in-context reinforcement learning is supported by caching intermediate computations within the model's memory tokens, which are then accessed at decision time. Overall, we find that memory may serve as a computational resource, storing both raw experience and cached computations to support flexible behavior. Furthermore, the representations developed in the model resemble computations associated with the hippocampal-entorhinal system in the brain, suggesting that our findings may be relevant for natural cognition. Taken together, our work offers a mechanistic hypothesis for the rapid adaptation that underlies in-context learning in artificial and natural settings.

**Link**: [arxiv](http://arxiv.org/abs/2506.19686v2),  [pdf](http://arxiv.org/pdf/2506.19686v2)

**Tags**: cs.AI 



### Measurements, simulations, and models of the point-spread function of   electron-beam lithography
**Authors**: Nikolaj B. Hougs, Kristian S. Knudsen, Marcus Albrechtsen, Taichi Suhara, Christian A. Rosiek, Søren Stobbe

**Updated**: 2025-06-26T13:22:30Z

**Summary**: When a sample is exposed using electron-beam lithography, the electrons scatter deep and far in the substrate, resulting in unwanted deposition of dose at both the nano- and the microscale. This proximity effect can be mitigated by proximity effect correction provided that accurate and validated models of the point-spread function of the electron scattering are available. Most works so far considered a double-Gaussian model of the electron point-spread function, which is very inaccurate for modern electron-beam writers with high acceleration voltages. We present measurements of the process point-spread function for chemically semi-amplified resist on silicon and indium phosphide substrates using a 150 kV electron-beam lithography system. We find that the double-Gaussian model deviates from experiments by up to four orders of magnitude. We propose instead a model comprising the sum of a power-law and a Gaussian, which is in excellent agreement with simulations of the electron scattering obtained by a Monte Carlo method. We apply the power-law plus Gaussian model to quantify the electron scattering and proximity effect correction parameters across material stacks, processing, and voltages from 5 kV to 150 kV. We find that the power-law term remains remarkably constant, whereas the long-range dose contributions and the clearing dose are significantly affected by the substrate and the acceleration voltage.

**Link**: [arxiv](http://arxiv.org/abs/2506.21236v1),  [pdf](http://arxiv.org/pdf/2506.21236v1)

**Tags**: physics.app-ph 



### Task-Aware KV Compression For Cost-Effective Long Video Understanding
**Authors**: Minghao Qin, Yan Shu, Peitian Zhang, Kun Lun, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

**Updated**: 2025-06-26T12:43:43Z

**Summary**: Long-video understanding (LVU) remains a severe challenge for existing multimodal large language models (MLLMs), primarily due to the prohibitive computational cost. Recent approaches have explored KV compression to mitigate this issue, but they often suffer from significant information loss at high compression ratios. In this paper, we introduce Video-X^2L, which flexibly preserves critical video information for each LVU task. Video-X^2L involves two key operations. The first one is called bi-level KV compression. During the MLLM's pre-filling stage, Video-X^2L generates two types of compressed KVs: low-compression KVs (L-KVs) to capture fine-grained video details and high-compression KVs (H-KVs) to offer compact video representations. The second one is called selective KV re-loading. During the MLLM's decoding stage, Video-X^2L selectively re-loads L-KVs for the most critical video chunks while using H-KVs for other less important ones. This allows the MLLM to fully utilize task-specific information while maintaining the overall compactness. Video-X^2L is simple yet effective: it is free from additional training and directly compatible with existing KV-compressible MLLMs. We evaluate Video-X^2L with a variety of popular LVU benchmarks, including VideoMME, MLVU, LongVideoBench, and VNBench. Our experiment result shows that Video-X^2L outperforms existing KV-compression methods by a huge advantage while substantially saving the computation cost.

**Link**: [arxiv](http://arxiv.org/abs/2506.21184v1),  [pdf](http://arxiv.org/pdf/2506.21184v1)

**Tags**: cs.CV cs.AI 



### LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas   and Ad-Hoc Networks
**Authors**: Atonu Ghosh, Sudip Misra

**Updated**: 2025-06-26T05:12:22Z

**Summary**: Minimal infrastructure requirements make LoRa suitable for service delivery in remote areas. Additionally, web applications have become a de-facto standard for modern service delivery. However, Long Range (LoRa) fails to enable HTTP access due to its limited bandwidth, payload size limitations, and high collisions in multi-user setups. We propose LoRaConnect to enable HTTP access over LoRa. The LoRaWeb hardware tethers a WiFi hotspot to which client devices connect and access HTTP resources over LoRa backhaul. It implements caching and synchronization mechanisms to address LoRa's aforementioned limitations. It also implements a message-slicing method in the application layer to overcome LoRa's payload limitations. We evaluate the proposed system using actual hardware in three experimental setups to assess the baseline performance, ideal scenario, and practical application scenario with Frequency Hopping Spread Spectrum (FHSS). Additionally, it implements a ping operation to demonstrate Internet capability and extensible nature. LoRaWeb achieves an average throughput of 1.18 KB/S approximately, with an access delay of only 1.3 S approximately for a 1.5KB webpage in the baseline setup. Moreover, it achieves an access delay of approximately 6.7 S for a 10KB webpage in the ideal case and an average end-to-end delay of only 612 ms approximately in the FHSS-based setup. Comparison with benchmark suggests multi-fold improvement.

**Link**: [arxiv](http://arxiv.org/abs/2501.02469v3),  [pdf](http://arxiv.org/pdf/2501.02469v3)

**Tags**: cs.NI cs.CY cs.SY eess.SY 



### The electronic structures, magnetic transition and Fermi surface   instability of room-temperature altermagnet KV$_{2}$Se$_{2}$O
**Authors**: Yuanji Xu, Huiyuan Zhang, Maoyuan Feng, Fuyang Tian

**Updated**: 2025-06-26T03:13:33Z

**Summary**: Altermagnetism has recently emerged as a distinct and fundamental class of magnetic order. Exploring its interplay with quantum phenomena such as unconventional superconductivity, density-wave instabilities, and many-body effects represents a compelling frontier. In this work, we theoretically confirm the presence of high-temperature metallic altermagnetism in KV$_2$Se$_2$O. We demonstrate that the anomalous metal-insulator-metal transition arises from a Lifshitz transition associated with Fermi surface reconstruction. The previously reported spin-density wave gap is found to lie below the Fermi level in our study and is now recognized to be attributed to the V-shaped density of states, originating from orbital-selective and sublattice-resolved half-metal-like behavior on a specific V atom. Furthermore, we identify the instability from the nesting of spin-momentum-locked two-dimensional Fermi surfaces, which induces the SDW state. These findings position KV$_2$Se$_2$O as a promising platform for investigating the interplay among altermagnetism, unconventional superconductivity, and density-wave order.

**Link**: [arxiv](http://arxiv.org/abs/2506.20968v1),  [pdf](http://arxiv.org/pdf/2506.20968v1)

**Tags**: cond-mat.str-el 



### AirCache: Activating Inter-modal Relevancy KV Cache Compression for   Efficient Large Vision-Language Model Inference
**Authors**: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

**Updated**: 2025-06-26T01:30:43Z

**Summary**: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.23956v2),  [pdf](http://arxiv.org/pdf/2503.23956v2)

**Tags**: cs.CV cs.AI 



### Omniwise: Predicting GPU Kernels Performance with LLMs
**Authors**: Zixian Wang, Cole Ramos, Muhammad A. Awad, Keith Lowery

**Updated**: 2025-06-25T23:36:44Z

**Summary**: In recent years, the rapid advancement of deep neural networks (DNNs) has revolutionized artificial intelligence, enabling models with unprecedented capabilities in understanding, generating, and processing complex data. These powerful architectures have transformed a wide range of downstream applications, tackling tasks beyond human reach. In this paper, we introduce Omniwise, the first end-to-end, self-supervised fine-tuning pipeline that applies large language models (LLMs) to GPU kernel performance prediction--a novel use case in performance profiling. Omniwise is model-agnostic and lightweight, achieving strong results even with a small 3B-parameter model. It can predict key performance metrics, including memory bandwidth, cache hit rates, GFLOPs, and arithmetic intensity, directly from kernel code without the need for code execution or profiling tools. Our approach achieves over 90% of predictions within 10% relative error on GPU kernels executed on AMD MI250 and MI300X architectures. In addition to the pipeline, we develop an online inference server and a Visual Studio Code plugin that seamlessly integrate LLM-based performance prediction into developers' workflows.

**Link**: [arxiv](http://arxiv.org/abs/2506.20886v1),  [pdf](http://arxiv.org/pdf/2506.20886v1)

**Tags**: cs.LG cs.AI 



### A3 : an Analytical Low-Rank Approximation Framework for Attention
**Authors**: Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao

**Updated**: 2025-06-25T23:03:54Z

**Summary**: Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.12942v3),  [pdf](http://arxiv.org/pdf/2505.12942v3)

**Tags**: cs.CL cs.AI cs.LG 



### Generative Blocks World: Moving Things Around in Pictures
**Authors**: Vaibhav Vavilala, Seemandhar Jain, Rahul Vasanth, D. A. Forsyth, Anand Bhattad

**Updated**: 2025-06-25T17:59:55Z

**Summary**: We describe Generative Blocks World to interact with the scene of a generated image by manipulating simple geometric abstractions. Our method represents scenes as assemblies of convex 3D primitives, and the same scene can be represented by different numbers of primitives, allowing an editor to move either whole structures or small details. Once the scene geometry has been edited, the image is generated by a flow-based method which is conditioned on depth and a texture hint. Our texture hint takes into account the modified 3D primitives, exceeding texture-consistency provided by existing key-value caching techniques. These texture hints (a) allow accurate object and camera moves and (b) largely preserve the identity of objects depicted. Quantitative and qualitative experiments demonstrate that our approach outperforms prior works in visual fidelity, editability, and compositional generalization.

**Link**: [arxiv](http://arxiv.org/abs/2506.20703v1),  [pdf](http://arxiv.org/pdf/2506.20703v1)

**Tags**: cs.GR cs.CV 



### Semantic Caching for Improving Web Affordability
**Authors**: Hafsa Akbar, Danish Athar, Muhammad Ayain Fida Rana, Chaudhary Hammad Javed, Zartash Afzal Uzmi, Ihsan Ayyub Qazi, Zafar Ayyub Qazi

**Updated**: 2025-06-25T13:35:25Z

**Summary**: The rapid growth of web content has led to increasingly large webpages, posing significant challenges for Internet affordability, especially in developing countries where data costs remain prohibitively high. We propose semantic caching using Large Language Models (LLMs) to improve web affordability by enabling reuse of semantically similar images within webpages. Analyzing 50 leading news and media websites, encompassing 4,264 images and over 40,000 image pairs, we demonstrate potential for significant data transfer reduction, with some website categories showing up to 37% of images as replaceable. Our proof-of-concept architecture shows users can achieve approximately 10% greater byte savings compared to exact caching. We evaluate both commercial and open-source multi-modal LLMs for assessing semantic replaceability. GPT-4o performs best with a low Normalized Root Mean Square Error of 0.1735 and a weighted F1 score of 0.8374, while the open-source LLaMA 3.1 model shows comparable performance, highlighting its viability for large-scale applications. This approach offers benefits for both users and website operators, substantially reducing data transmission. We discuss ethical concerns and practical challenges, including semantic preservation, user-driven cache configuration, privacy concerns, and potential resistance from website operators

**Link**: [arxiv](http://arxiv.org/abs/2506.20420v1),  [pdf](http://arxiv.org/pdf/2506.20420v1)

**Tags**: cs.NI F.2.2, I.2.7 



### Do cell culturing influence the radiosensitizing effect of gold   nanoparticles part 2: scrutinizing the methodology producing recent evidence
**Authors**: Hans Rabus, Oswald Msosa Mkanda

**Updated**: 2025-06-25T09:44:25Z

**Summary**: When irradiation is performed with gold nanoparticles (AuNPs), a different shape of cells in suspension or adherent to walls may result in different probability of cell survival. In a recent study, differences of up to a factor of 2 were found between the predicted survival of floating and adherent cells. The present work aims to quantify the biases introduced by the simulation setup and the use of voxelized geometry in conjunction with the local effect model for cell survival. The results show that simulated irradiation of a cell near the surface with an incident beam matched to the cell dimensions results in dose values that are by a factor of about 50 lower than the dose to cells deeper in the medium when irradiated with a Co-60 spectrum and lateral beam dimensions in the centimeter range. Furthermore, the number of ionizing photon interactions in gold nanoparticles in a cell near the surface is lower by a factor of about 2 than for cells at 5 mm and 1 cm depth. Using the average dose in voxels of size in the order of 200 nm for assessing cell survival with the local effect model (LEM) leads to an underestimation of the number of lesions from a single ionized AuNP by roughly two orders of magnitude and thus to an overestimation of cell survival. The effect of cell geometry on the survival rate was examined for approximate cell geometries and 100 kV x-ray irradiation, for which the probability of photon interaction in gold nanoparticles is by more than two orders of magnitude higher than for Co-60 irradiation. The results show that the effects are negligible for 5 nm nanoparticles at the concentration of AuNPs considered in preceding work. For 50 nm nanoparticles and thus a thousand times higher mass fraction of gold, significant reduction in cell survival is found, with a clear additional reduction predicted by the LEM as compared to the prediction based on mean dose to the nucleus.

**Link**: [arxiv](http://arxiv.org/abs/2506.20283v1),  [pdf](http://arxiv.org/pdf/2506.20283v1)

**Tags**: physics.med-ph 



### MegaFold: System-Level Optimizations for Accelerating Protein Structure   Prediction Models
**Authors**: Hoa La, Ahan Gupta, Alex Morehead, Jianlin Cheng, Minjia Zhang

**Updated**: 2025-06-24T23:30:49Z

**Summary**: Protein structure prediction models such as AlphaFold3 (AF3) push the frontier of biomolecular modeling by incorporating science-informed architectural changes to the transformer architecture. However, these advances come at a steep system cost, introducing: compute- and memory-intensive operators, 2D attention mechanisms, and retrieval-augmented data pipelines, which collectively hinder the scalability of AF3 training. In this work, we present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle time from the retrieval-augmented data pipeline, Triton-based kernels for memory-efficient EvoAttention on heterogeneous devices, and deep fusion for common and critical small operators in AF3. Evaluation on both NVIDIA H200 and AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by up to 1.23$\times$ and improves per-iteration training time by up-to 1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines without running out-of-memory, significantly improving the scalability of modern protein folding models. We open source our code at https://github.com/Supercomputing-System-AI-Lab/MegaFold/.

**Link**: [arxiv](http://arxiv.org/abs/2506.20686v1),  [pdf](http://arxiv.org/pdf/2506.20686v1)

**Tags**: q-bio.BM cs.DC cs.LG cs.PF 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hoßfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-06-24T19:02:08Z

**Summary**: As AI workloads drive soaring memory requirements, higher-density on-chip memory is needed for domain-specific accelerators beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, little work has incorporated dynamic application profiles into these design decisions, and no existing tools are expressly designed for this purpose. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and data lifetimes in domain-specific accelerators. By instrumenting retargetable architectural simulator backends with application- and device-agnostic analytical frontends, GainSight aligns workload-specific traffic and lifetime metrics with mockups of emerging memory devices, informing system-level heterogeneous memory design. We also present a set of case studies on MLPerf Inference and PolyBench workloads using simulated GPU and systolic array architectures, highlighting the utility of GainSight and the insights it provides: (1) 64% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Heterogeneous memory arrays that augment SRAM with GCRAM can reduce active energy consumption by up to 66.8%. To facilitate further research in this domain, GainSight is open source at https://gainsight.stanford.edu/.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v4),  [pdf](http://arxiv.org/pdf/2504.14866v4)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### CronusVLA: Transferring Latent Motion Across Time for Multi-Frame   Prediction in Manipulation
**Authors**: Hao Li, Shuai Yang, Yilun Chen, Yang Tian, Xiaoda Yang, Xinyi Chen, Hanqing Wang, Tai Wang, Feng Zhao, Dahua Lin, Jiangmiao Pang

**Updated**: 2025-06-24T17:30:27Z

**Summary**: Recent vision-language-action (VLA) models built on pretrained vision-language models (VLMs) have demonstrated strong generalization across manipulation tasks. However, they remain constrained by a single-frame observation paradigm and cannot fully benefit from the motion information offered by aggregated multi-frame historical observations, as the large vision-language backbone introduces substantial computational cost and inference latency. We propose CronusVLA, a unified framework that extends single-frame VLA models to the multi-frame paradigm through an efficient post-training stage. CronusVLA comprises three key components: (1) single-frame pretraining on large-scale embodied datasets with autoregressive action tokens prediction, which establishes an embodied vision-language foundation; (2) multi-frame encoding, adapting the prediction of vision-language backbones from discrete action tokens to motion features during post-training, and aggregating motion features from historical frames into a feature chunking; (3) cross-frame decoding, which maps the feature chunking to accurate actions via a shared decoder with cross-attention. By reducing redundant token computation and caching past motion features, CronusVLA achieves efficient inference. As an application of motion features, we further propose an action adaptation mechanism based on feature-action retrieval to improve model performance during finetuning. CronusVLA achieves state-of-the-art performance on SimplerEnv with 70.9% success rate, and 12.7% improvement over OpenVLA on LIBERO. Real-world Franka experiments also show the strong performance and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2506.19816v1),  [pdf](http://arxiv.org/pdf/2506.19816v1)

**Tags**: cs.RO cs.CV 



### RCStat: A Statistical Framework for using Relative Contextualization in   Transformers
**Authors**: Debabrata Mahapatra, Shubham Agarwal, Apoorv Saxena, Subrata Mitra

**Updated**: 2025-06-24T11:55:43Z

**Summary**: Prior work on input-token importance in auto-regressive transformers has relied on Softmax-normalized attention weights, which obscure the richer structure of pre-Softmax query-key logits. We introduce RCStat, a statistical framework that harnesses raw attention logits via Relative Contextualization (RC), a random variable measuring contextual alignment between token segments, and derive an efficient upper bound for RC. We demonstrate two applications: (i) Key-Value compression, where RC-based thresholds drive adaptive key-value eviction for substantial cache reduction with minimal quality loss; and (ii) Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level explanations than post-Softmax methods. Across question answering, summarization, and attribution benchmarks, RCStat achieves significant empirical gains, delivering state-of-the-art compression and attribution performance without any model retraining.

**Link**: [arxiv](http://arxiv.org/abs/2506.19549v1),  [pdf](http://arxiv.org/pdf/2506.19549v1)

**Tags**: cs.CL cs.AI cs.LG 



### AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in   Large Language Models
**Authors**: Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu

**Updated**: 2025-06-24T10:45:48Z

**Summary**: Quantization has emerged as an effective and lightweight solution to reduce the memory footprint of the KV cache in Large Language Models (LLMs). Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV cache quantization remains a significant challenge. We observe that quantizing the KV cache of different tokens has varying impacts on the quality of attention outputs. To systematically investigate this phenomenon, we perform forward error propagation analysis on attention and propose the Anchor Score (AnS) that quantifies the sensitivity of each token's KV cache to quantization-induced error. Our analysis reveals significant disparities in AnS across tokens, suggesting that preserving a small subset with full precision (FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive quantization scenarios. Based on this insight, we introduce AnTKV, a novel framework that leverages Anchor Token-aware Vector Quantization to compress the KV cache. Furthermore, to support efficient deployment, we design and develop a triton kernel that is fully compatible with FlashAttention, enabling fast online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x higher decoding throughput compared to the FP16 baseline. Our experiment results demonstrate that AnTKV matches or outperforms prior works such as KIVI, SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves significantly lower perplexity under ultra-low-bit quantization on Mistral-7B, with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of 4.73.

**Link**: [arxiv](http://arxiv.org/abs/2506.19505v1),  [pdf](http://arxiv.org/pdf/2506.19505v1)

**Tags**: cs.CL 



### Mixture of Cache-Conditional Experts for Efficient Mobile Device   Inference
**Authors**: Andrii Skliar, Ties van Rozendaal, Romain Lepert, Todor Boinovski, Mart van Baalen, Markus Nagel, Paul Whatmough, Babak Ehteshami Bejnordi

**Updated**: 2025-06-24T09:27:46Z

**Summary**: Mixture of Experts (MoE) LLMs have recently gained attention for their ability to enhance performance by selectively engaging specialized subnetworks or "experts" for each input. However, deploying MoEs on memory-constrained devices remains challenging, particularly when generating tokens sequentially with a batch size of one, as opposed to typical high-throughput settings involving long sequences or large batches. In this work, we optimize MoE on memory-constrained devices where only a subset of expert weights fit in DRAM. We introduce a novel cache-aware routing strategy that leverages expert reuse during token generation to improve cache locality. We evaluate our approach on language modeling, MMLU, and GSM8K benchmarks and present on-device results demonstrating 2$\times$ speedups on mobile devices, offering a flexible, training-free solution to extend MoE's applicability across real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.00099v2),  [pdf](http://arxiv.org/pdf/2412.00099v2)

**Tags**: cs.LG cs.AI cs.AR 



### Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments   with a Hierarchical Spatial-Cognition Long-Short Memory System
**Authors**: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li

**Updated**: 2025-06-24T09:00:43Z

**Summary**: Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.

**Link**: [arxiv](http://arxiv.org/abs/2506.19433v1),  [pdf](http://arxiv.org/pdf/2506.19433v1)

**Tags**: cs.CV cs.AI cs.CL 



### PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning
**Authors**: Duong Bach

**Updated**: 2025-06-24T06:44:47Z

**Summary**: The proliferation of multi-agent systems (MAS) in complex, dynamic environments necessitates robust and efficient mechanisms for managing shared knowledge. A critical challenge is ensuring that distributed memories remain synchronized, relevant, and free from the accumulation of outdated or inconsequential data - a process analogous to biological forgetting. This paper introduces the Co-Forgetting Protocol, a novel, comprehensive framework designed to address this challenge by enabling synchronized memory pruning in MAS. The protocol integrates three key components: (1) context-aware semantic voting, where agents utilize a lightweight DistilBERT model to assess the relevance of memory items based on their content and the current operational context; (2) multi-scale temporal decay functions, which assign diminishing importance to memories based on their age and access frequency across different time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based consensus mechanism, ensuring that decisions to retain or discard memory items are agreed upon by a qualified and fault-tolerant majority of agents, even in the presence of up to f Byzantine (malicious or faulty) agents in a system of N greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient inter-agent communication and Pinecone for scalable vector embedding storage and similarity search, with SQLite managing metadata. Experimental evaluations in a simulated MAS environment with four agents demonstrate the protocol's efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88% voting accuracy in forgetting decisions against human-annotated benchmarks, a 92% PBFT consensus success rate under simulated Byzantine conditions, and an 82% cache hit rate for memory access.

**Link**: [arxiv](http://arxiv.org/abs/2506.17338v2),  [pdf](http://arxiv.org/pdf/2506.17338v2)

**Tags**: cs.DC cs.AI cs.MA 



### Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV   Sparsification
**Authors**: Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

**Updated**: 2025-06-24T01:19:56Z

**Summary**: Multi-modal large language models (MLLMs) models have made significant progress in video understanding over the past few years. However, processing long video inputs remains a major challenge due to high memory and computational costs. This makes it difficult for current models to achieve both strong performance and high efficiency in long video understanding. To address this challenge, we propose Video-XL-2, a novel MLLM that delivers superior cost-effectiveness for long-video understanding based on task-aware KV sparsification. The proposed framework operates with two key steps: chunk-based pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides the visual token sequence into chunks, applying full attention within each chunk and sparse attention across chunks. This significantly reduces computational and memory overhead. During decoding, bi-level key-value decoding selectively reloads either dense or sparse key-values for each chunk based on its relevance to the task. This approach further improves memory efficiency and enhances the model's ability to capture fine-grained information. Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks, outperforming existing open-source lightweight models. It also demonstrates exceptional efficiency, capable of processing over 10,000 frames on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few seconds.

**Link**: [arxiv](http://arxiv.org/abs/2506.19225v1),  [pdf](http://arxiv.org/pdf/2506.19225v1)

**Tags**: cs.CV cs.AI 



### Binsparse: A Specification for Cross-Platform Storage of Sparse Matrices   and Tensors
**Authors**: Benjamin Brock, Willow Ahrens, Hameer Abbasi, Timothy A. Davis, Juni Kim, James Kitchen, Spencer Patty, Isaac Virshup, Erik Welch

**Updated**: 2025-06-23T22:33:58Z

**Summary**: Sparse matrices and tensors are ubiquitous throughout multiple subfields of computing. The widespread usage of sparse data has inspired many in-memory and on-disk storage formats, but the only widely adopted storage specifications are the Matrix Market and FROSTT file formats, which both use ASCII text. Due to the inefficiency of text storage, these files typically have larger file sizes and longer parsing times than binary storage formats, which directly store an in-memory representation to disk. This can be a major bottleneck; since sparse computation is often bandwidth-bound, the cost of loading or storing a matrix to disk often exceeds the cost of performing a sparse computation. While it is common practice for practitioners to develop their own, custom, non-portable binary formats for high-performance sparse matrix storage, there is currently no cross-platform binary sparse matrix storage format. We present Binsparse, a cross-platform binary sparse matrix and tensor format specification. Binsparse is a modular, embeddable format, consisting of a JSON descriptor, which describes the matrix or tensor dimensions, type, and format, and a series of binary arrays, which can be stored in all modern binary containers, such as HDF5, Zarr, or NPZ. We provide several reference implementations of Binsparse spanning 5 languages, 5 frameworks, and 4 binary containers. We evaluate our Binsparse format on every matrix in the SuiteSparse Matrix Collection and a selection of tensors from the FROSTT collection. The Binsparse HDF5 CSR format shows file size reductions of 2.4x on average without compression and 7.5x with compression. We evaluate our parser's read/write performance against a state-of-the-art Matrix Market parser, demonstrating warm cache mean read speedups of 26.5x without compression and 2.6x with compression, and write speedups of 31x without compression and 1.4x with compression.

**Link**: [arxiv](http://arxiv.org/abs/2506.19175v1),  [pdf](http://arxiv.org/pdf/2506.19175v1)

**Tags**: cs.MS cs.DC cs.DS 



### CommVQ: Commutative Vector Quantization for KV Cache Compression
**Authors**: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

**Updated**: 2025-06-23T17:50:11Z

**Summary**: Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

**Link**: [arxiv](http://arxiv.org/abs/2506.18879v1),  [pdf](http://arxiv.org/pdf/2506.18879v1)

**Tags**: cs.CL cs.AI 



### Persistent Sampling: Enhancing the Efficiency of Sequential Monte Carlo
**Authors**: Minas Karamanis, Uroš Seljak

**Updated**: 2025-06-23T07:59:17Z

**Summary**: Sequential Monte Carlo (SMC) samplers are powerful tools for Bayesian inference but suffer from high computational costs due to their reliance on large particle ensembles for accurate estimates. We introduce persistent sampling (PS), an extension of SMC that systematically retains and reuses particles from all prior iterations to construct a growing, weighted ensemble. By leveraging multiple importance sampling and resampling from a mixture of historical distributions, PS mitigates the need for excessively large particle counts, directly addressing key limitations of SMC such as particle impoverishment and mode collapse. Crucially, PS achieves this without additional likelihood evaluations-weights for persistent particles are computed using cached likelihood values. This framework not only yields more accurate posterior approximations but also produces marginal likelihood estimates with significantly lower variance, enhancing reliability in model comparison. Furthermore, the persistent ensemble enables efficient adaptation of transition kernels by leveraging a larger, decorrelated particle pool. Experiments on high-dimensional Gaussian mixtures, hierarchical models, and non-convex targets demonstrate that PS consistently outperforms standard SMC and related variants, including recycled and waste-free SMC, achieving substantial reductions in mean squared error for posterior expectations and evidence estimates, all at reduced computational cost. PS thus establishes itself as a robust, scalable, and efficient alternative for complex Bayesian inference tasks.

**Link**: [arxiv](http://arxiv.org/abs/2407.20722v3),  [pdf](http://arxiv.org/pdf/2407.20722v3)

**Tags**: stat.ML cs.LG stat.CO 



### FutureFill: Fast Generation from Convolutional Sequence Models
**Authors**: Naman Agarwal, Xinyi Chen, Evan Dogariu, Devan Shah, Hubert Strauss, Vlad Feinberg, Daniel Suo, Peter Bartlett, Elad Hazan

**Updated**: 2025-06-23T03:20:46Z

**Summary**: We address the challenge of efficient auto-regressive generation in sequence prediction models by introducing FutureFill, a general-purpose fast generation method for any sequence prediction algorithm based on convolutional operators. FutureFill reduces generation time from quadratic to quasilinear in the context length. Moreover, when generating from a prompt, it requires a prefill cache whose size grows only with the number of tokens to be generated, often much smaller than the caches required by standard convolutional or attention based models. We validate our theoretical claims with experiments on synthetic tasks and demonstrate substantial efficiency gains when generating from a deep convolutional sequence prediction model.

**Link**: [arxiv](http://arxiv.org/abs/2410.03766v3),  [pdf](http://arxiv.org/pdf/2410.03766v3)

**Tags**: cs.LG cs.AI cs.CL 



## Keyword: LLM Inference 
 ### Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor
**Authors**: Vatsal Agarwal, Matthew Gwilliam, Gefen Kohavi, Eshan Verma, Daniel Ulbricht, Abhinav Shrivastava

**Updated**: 2025-07-09T17:59:47Z

**Summary**: Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found https://vatsalag99.github.io/mustafar/.

**Link**: [arxiv](http://arxiv.org/abs/2507.07106v1),  [pdf](http://arxiv.org/pdf/2507.07106v1)

**Tags**: cs.CV cs.LG 



### Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models
**Authors**: Tiezheng Zhang, Yitong Li, Yu-cheng Chou, Jieneng Chen, Alan Yuille, Chen Wei, Junfei Xiao

**Updated**: 2025-07-09T17:59:04Z

**Summary**: Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD.

**Link**: [arxiv](http://arxiv.org/abs/2507.07104v1),  [pdf](http://arxiv.org/pdf/2507.07104v1)

**Tags**: cs.CV 



### Less can be more for predicting properties with large language models
**Authors**: Nawaf Alampara, Santiago Miret, Kevin Maik Jablonka

**Updated**: 2025-07-09T17:37:23Z

**Summary**: Predicting properties from coordinate-category data -- sets of vectors paired with categorical information -- is fundamental to computational science. In materials science, this challenge manifests as predicting properties like formation energies or elastic moduli from crystal structures comprising atomic positions (vectors) and element types (categorical information). While large language models (LLMs) have increasingly been applied to such tasks, with researchers encoding structural data as text, optimal strategies for achieving reliable predictions remain elusive. Here, we report fundamental limitations in LLM's ability to learn from coordinate information in coordinate-category data. Through systematic experiments using synthetic datasets with tunable coordinate and category contributions, combined with a comprehensive benchmarking framework (MatText) spanning multiple representations and model scales, we find that LLMs consistently fail to capture coordinate information while excelling at category patterns. This geometric blindness persists regardless of model size (up to 70B parameters), dataset scale (up to 2M structures), or text representation strategy. Our findings suggest immediate practical implications: for materials property prediction tasks dominated by structural effects, specialized geometric architectures consistently outperform LLMs by significant margins, as evidenced by a clear "GNN-LM wall" in performance benchmarks. Based on our analysis, we provide concrete guidelines for architecture selection in scientific machine learning, while highlighting the critical importance of understanding model inductive biases when tackling scientific prediction problems.

**Link**: [arxiv](http://arxiv.org/abs/2406.17295v3),  [pdf](http://arxiv.org/pdf/2406.17295v3)

**Tags**: cond-mat.mtrl-sci cs.LG 



### Reading a Ruler in the Wild
**Authors**: Yimu Pan, Manas Mehta, Gwen Sincerbeaux, Jeffery A. Goldstein, Alison D. Gernand, James Z. Wang

**Updated**: 2025-07-09T17:35:58Z

**Summary**: Accurately converting pixel measurements into absolute real-world dimensions remains a fundamental challenge in computer vision and limits progress in key applications such as biomedicine, forensics, nutritional analysis, and e-commerce. We introduce RulerNet, a deep learning framework that robustly infers scale "in the wild" by reformulating ruler reading as a unified keypoint-detection problem and by representing the ruler with geometric-progression parameters that are invariant to perspective transformations. Unlike traditional methods that rely on handcrafted thresholds or rigid, ruler-specific pipelines, RulerNet directly localizes centimeter marks using a distortion-invariant annotation and training strategy, enabling strong generalization across diverse ruler types and imaging conditions while mitigating data scarcity. We also present a scalable synthetic-data pipeline that combines graphics-based ruler generation with ControlNet to add photorealistic context, greatly increasing training diversity and improving performance. To further enhance robustness and efficiency, we propose DeepGP, a lightweight feed-forward network that regresses geometric-progression parameters from noisy marks and eliminates iterative optimization, enabling real-time scale estimation on mobile or edge devices. Experiments show that RulerNet delivers accurate, consistent, and efficient scale estimates under challenging real-world conditions. These results underscore its utility as a generalizable measurement tool and its potential for integration with other vision components for automated, scale-aware analysis in high-impact domains. A live demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.

**Link**: [arxiv](http://arxiv.org/abs/2507.07077v1),  [pdf](http://arxiv.org/pdf/2507.07077v1)

**Tags**: cs.CV 



### TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management   in LLM-based Agentic Multi-Agent Systems
**Authors**: Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis

**Updated**: 2025-07-09T17:33:49Z

**Summary**: Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of \textbf{Trust, Risk, and Security Management (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around four key pillars: Explainability, ModelOps, Security, Privacy and Governance, each contextualized to the challenges of multi-agent LLM systems. A novel risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI , as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, outlining critical directions to align emerging systems with TRiSM principles for safe, transparent, and accountable operation.

**Link**: [arxiv](http://arxiv.org/abs/2506.04133v3),  [pdf](http://arxiv.org/pdf/2506.04133v3)

**Tags**: cs.AI 



### Multi-Attribute Steering of Language Models via Targeted Intervention
**Authors**: Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

**Updated**: 2025-07-09T17:31:20Z

**Summary**: Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-Steer learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient fine-tuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline).

**Link**: [arxiv](http://arxiv.org/abs/2502.12446v2),  [pdf](http://arxiv.org/pdf/2502.12446v2)

**Tags**: cs.CL cs.AI cs.LG 



### How to Bridge the Sim-to-Real Gap in Digital Twin-Aided   Telecommunication Networks
**Authors**: Clement Ruah, Houssem Sifaou, Osvaldo Simeone, Bashir M. Al-Hashimi

**Updated**: 2025-07-09T17:27:51Z

**Summary**: Training effective artificial intelligence models for telecommunications is challenging due to the scarcity of deployment-specific data. Real data collection is expensive, and available datasets often fail to capture the unique operational conditions and contextual variability of the network environment. Digital twinning provides a potential solution to this problem, as simulators tailored to the current network deployment can generate site-specific data to augment the available training datasets. However, there is a need to develop solutions to bridge the inherent simulation-to-reality (sim-to-real) gap between synthetic and real-world data. This paper reviews recent advances on two complementary strategies: 1) the calibration of digital twins (DTs) through real-world measurements, and 2) the use of sim-to-real gap-aware training strategies to robustly handle residual discrepancies between digital twin-generated and real data. For the latter, we evaluate two conceptually distinct methods that model the sim-to-real gap either at the level of the environment via Bayesian learning or at the level of the training loss via prediction-powered inference.

**Link**: [arxiv](http://arxiv.org/abs/2507.07067v1),  [pdf](http://arxiv.org/pdf/2507.07067v1)

**Tags**: eess.SP cs.LG 



### Boosting Parameter Efficiency in LLM-Based Recommendation through   Sophisticated Pruning
**Authors**: Shanle Zheng, Keqin Bao, Jizhi Zhang, Yang Zhang, Fuli Feng, Xiangnan He

**Updated**: 2025-07-09T17:26:10Z

**Summary**: LLM-based recommender systems have made significant progress; however, the deployment cost associated with the large parameter volume of LLMs still hinders their real-world applications. This work explores parameter pruning to improve parameter efficiency while maintaining recommendation quality, thereby enabling easier deployment. Unlike existing approaches that focus primarily on inter-layer redundancy, we uncover intra-layer redundancy within components such as self-attention and MLP modules. Building on this analysis, we propose a more fine-grained pruning approach that integrates both intra-layer and layer-wise pruning. Specifically, we introduce a three-stage pruning strategy that progressively prunes parameters at different levels and parts of the model, moving from intra-layer to layer-wise pruning, or from width to depth. Each stage also includes a performance restoration step using distillation techniques, helping to strike a balance between performance and parameter efficiency. Empirical results demonstrate the effectiveness of our approach: across three datasets, our models achieve an average of 88% of the original model's performance while pruning more than 95% of the non-embedding parameters. This underscores the potential of our method to significantly reduce resource requirements without greatly compromising recommendation quality. Our code will be available at: https://github.com/zheng-sl/PruneRec

**Link**: [arxiv](http://arxiv.org/abs/2507.07064v1),  [pdf](http://arxiv.org/pdf/2507.07064v1)

**Tags**: cs.IR 



### LCFO: Long Context and Long Form Output Dataset and Benchmarking
**Authors**: Marta R. Costa-jussà, Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo Sánchez, Holger Schwenk, Tuan Tran, Arina Turkatenko, Carleigh Wood

**Updated**: 2025-07-09T17:25:55Z

**Summary**: This paper presents the Long Context and Form Output (LCFO) benchmark, a novel evaluation framework for assessing gradual summarization and summary expansion capabilities across diverse domains. LCFO consists of long input documents (5k words average length), each of which comes with three summaries of different lengths (20%, 10%, and 5% of the input text), as well as approximately 15 questions and answers (QA) related to the input content. Notably, LCFO also provides alignments between specific QA pairs and corresponding summaries in 7 domains. The primary motivation behind providing summaries of different lengths is to establish a controllable framework for generating long texts from shorter inputs, i.e. summary expansion. To establish an evaluation metric framework for summarization and summary expansion, we provide human evaluation scores for human-generated outputs, as well as results from various state-of-the-art large language models (LLMs). GPT-4o-mini achieves best human scores among automatic systems in both summarization and summary expansion tasks (~ +10% and +20%, respectively). It even surpasses human output quality in the case of short summaries (~ +7%). Overall automatic metrics achieve low correlations with human evaluation scores (~ 0.4) but moderate correlation on specific evaluation aspects such as fluency and attribution (~ 0.6).

**Link**: [arxiv](http://arxiv.org/abs/2412.08268v3),  [pdf](http://arxiv.org/pdf/2412.08268v3)

**Tags**: cs.CL I.2.7 



### LASeR: Learning to Adaptively Select Reward Models with Multi-Armed   Bandits
**Authors**: Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

**Updated**: 2025-07-09T17:19:50Z

**Summary**: Reward Models (RMs) are crucial to aligning large language models (LLMs), but the degree to which an RM specialized to one task (e.g. writing) generalizes to new tasks (e.g. math) is often not known a priori, often making using only one fixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs simultaneously can incur a prohibitively high computational cost and lead to conflicting signals from different RMs that may degrade performance. To address these challenges, we introduce LASeR (Learning to Adaptively Select Rewards), which frames reward model selection as a multi-armed bandit problem, efficiently and iteratively training LLMs using multiple RMs by selecting the most well-suited RM for each instance. On commonsense and math reasoning tasks, we show that LASeR boosts iterative LLM training, improving the absolute average accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of RM scores while also showing superior efficiency (e.g., a 2x speedup). Moreover, on WildChat (open-ended instruction-following tasks), LASeR leads to a 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to long-context generation, LASeR improves by 2.96 F1 points (avg.) on single-document QA tasks and 2.97 F1 points on few-shot learning over the RM score ensemble baseline with best-of-n sampling.

**Link**: [arxiv](http://arxiv.org/abs/2410.01735v2),  [pdf](http://arxiv.org/pdf/2410.01735v2)

**Tags**: cs.CL cs.LG 



### ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment
**Authors**: Shaofei Cai, Zhancun Mu, Anji Liu, Yitao Liang

**Updated**: 2025-07-09T17:13:26Z

**Summary**: We aim to develop a goal specification method that is semantically clear, spatially sensitive, domain-agnostic, and intuitive for human users to guide agent interactions in 3D environments. Specifically, we propose a novel cross-view goal alignment framework that allows users to specify target objects using segmentation masks from their camera views rather than the agent's observations. We highlight that behavior cloning alone fails to align the agent's behavior with human intent when the human and agent camera views differ significantly. To address this, we introduce two auxiliary objectives: cross-view consistency loss and target visibility loss, which explicitly enhance the agent's spatial reasoning ability. According to this, we develop ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an improvement in the efficiency of inference 3x to 6x compared to ROCKET-1. We show that ROCKET-2 can directly interpret goals from human camera views, enabling better human-agent interaction. Remarkably, ROCKET-2 demonstrates zero-shot generalization capabilities: despite being trained exclusively on the Minecraft dataset, it can adapt and generalize to other 3D environments like Doom, DMLab, and Unreal through a simple action space mapping.

**Link**: [arxiv](http://arxiv.org/abs/2503.02505v2),  [pdf](http://arxiv.org/pdf/2503.02505v2)

**Tags**: cs.AI cs.CV cs.LG cs.RO 



### Low-Rank Adaptation Secretly Imitates Differentially Private SGD
**Authors**: Saber Malekmohammadi, Golnoosh Farnadi

**Updated**: 2025-07-09T17:11:15Z

**Summary**: As pre-trained language models grow in size, full fine-tuning their parameters on task adaptation data becomes increasingly impractical. To address this challenge, some methods for low-rank adaptation of language models have been proposed, e.g. LoRA, which incorporates trainable low-rank decomposition matrices into only some parameters of the pre-trained model, called adapters. This approach significantly reduces the number of trainable parameters compared to fine-tuning all parameters or adapters. In this work, we look at low-rank adaptation method from the lens of data privacy. We show theoretically that the low-rank adaptation used in LoRA is equivalent to fine-tuning adapters with noisy batch gradients - just like what DPSGD algorithm does. We also quantify the variance of the injected noise as a decreasing function of adaptation rank. By establishing a Berry-Esseen type bound on the total variation distance between the injected noise distribution and a Gaussian noise distribution with the same variance, we show that the dynamics of low-rank adaptation is very close to when DPSGD is performed w.r.t the adapters. Following our theoretical findings and approved by our experimental results, we show that low-rank adaptation provides robustness to membership inference attacks w.r.t the fine-tuning data.

**Link**: [arxiv](http://arxiv.org/abs/2409.17538v7),  [pdf](http://arxiv.org/pdf/2409.17538v7)

**Tags**: cs.LG cs.AI cs.CL 



### Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark   Enriched with Contextual Metadata
**Authors**: Bruce Coburn, Jiangpeng He, Megan E. Rollo, Satvinder S. Dhaliwal, Deborah A. Kerr, Fengqing Zhu

**Updated**: 2025-07-09T17:10:33Z

**Summary**: Large Multimodal Models (LMMs) are increasingly applied to meal images for nutrition analysis. However, existing work primarily evaluates proprietary models, such as GPT-4. This leaves the broad range of LLMs underexplored. Additionally, the influence of integrating contextual metadata and its interaction with various reasoning modifiers remains largely uncharted. This work investigates how interpreting contextual metadata derived from GPS coordinates (converted to location/venue type), timestamps (transformed into meal/day type), and the food items present can enhance LMM performance in estimating key nutritional values. These values include calories, macronutrients (protein, carbohydrates, fat), and portion sizes. We also introduce ACETADA, a new food-image dataset slated for public release. This open dataset provides nutrition information verified by the dietitian and serves as the foundation for our analysis. Our evaluation across eight LMMs (four open-weight and four closed-weight) first establishes the benefit of contextual metadata integration over straightforward prompting with images alone. We then demonstrate how this incorporation of contextual information enhances the efficacy of reasoning modifiers, such as Chain-of-Thought, Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona. Empirical results show that integrating metadata intelligently, when applied through straightforward prompting strategies, can significantly reduce the Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted nutritional values. This work highlights the potential of context-aware LMMs for improved nutrition analysis.

**Link**: [arxiv](http://arxiv.org/abs/2507.07048v1),  [pdf](http://arxiv.org/pdf/2507.07048v1)

**Tags**: cs.CV 



### 5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient   Design Framework for Individual and SME LLM Usage
**Authors**: Ugur Ari

**Updated**: 2025-07-09T17:07:39Z

**Summary**: The progression from traditional prompt engineering to a more rigorous discipline of prompt design marks a pivotal shift in human-LLM interaction. As Large Language Models (LLMs) become increasingly embedded in mission-critical applications, there emerges a pressing need for frameworks that are not only explicit and systematic but also minimal enough to remain practical and broadly accessible. While many existing approaches address prompt structuring through elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such methods can impose significant token and cognitive overhead, potentially constraining the model's creative capacity. In this context, we propose the 5C Prompt Contract, a framework that distills prompt design into five intuitive components: Character, Cause, Constraint, Contingency, and Calibration. This minimal cognitive schema explicitly integrates fallback and output optimization directives, fostering reliable, interpretable, and creatively flexible AI interactions. Experimental results demonstrate that the 5C framework consistently achieves superior input token efficiency while maintaining rich and consistent outputs across diverse LLM architectures (OpenAI, Anthropic, DeepSeek, and Gemini), making it particularly suited for individuals and Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

**Link**: [arxiv](http://arxiv.org/abs/2507.07045v1),  [pdf](http://arxiv.org/pdf/2507.07045v1)

**Tags**: cs.SE cs.SI 68T05 I.2.7; I.2.6 



### ZKTorch: Compiling ML Inference to Zero-Knowledge Proofs via Parallel   Proof Accumulation
**Authors**: Bing-Jyue Chen, Lilia Tang, Daniel Kang

**Updated**: 2025-07-09T17:03:21Z

**Summary**: As AI models become ubiquitous in our daily lives, there has been an increasing demand for transparency in ML services. However, the model owner does not want to reveal the weights, as they are considered trade secrets. To solve this problem, researchers have turned to zero-knowledge proofs of ML model inference. These proofs convince the user that the ML model output is correct, without revealing the weights of the model to the user. Past work on these provers can be placed into two categories. The first method compiles the ML model into a low-level circuit, and proves the circuit using a ZK-SNARK. The second method uses custom cryptographic protocols designed only for a specific class of models. Unfortunately, the first method is highly inefficient, making it impractical for the large models used today, and the second method does not generalize well, making it difficult to update in the rapidly changing field of machine learning. To solve this, we propose ZKTorch, an open source end-to-end proving system that compiles ML models into base cryptographic operations called basic blocks, each proved using specialized protocols. ZKTorch is built on top of a novel parallel extension to the Mira accumulation scheme, enabling succinct proofs with minimal accumulation overhead. These contributions allow ZKTorch to achieve at least a $3\times$ reduction in the proof size compared to specialized protocols and up to a $6\times$ speedup in proving time over a general-purpose ZKML framework.

**Link**: [arxiv](http://arxiv.org/abs/2507.07031v1),  [pdf](http://arxiv.org/pdf/2507.07031v1)

**Tags**: cs.CR cs.LG 



### FlexOlmo: Open Language Models for Flexible Data Use
**Authors**: Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, Daogao Liu, Margaret Li, Dirk Groeneveld, Mike Lewis, Wen-tau Yih, Luca Soldaini, Kyle Lo, Noah A. Smith, Luke Zettlemoyer, Pang Wei Koh, Hannaneh Hajishirzi, Ali Farhadi, Sewon Min

**Updated**: 2025-07-09T16:54:21Z

**Summary**: We introduce FlexOlmo, a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. We evaluate models with up to 37 billion parameters (20 billion active) on 31 diverse downstream tasks. We show that a general expert trained on public data can be effectively combined with independently trained experts from other data owners, leading to an average 41% relative improvement while allowing users to opt out of certain data based on data licensing or permission requirements. Our approach also outperforms prior model merging methods by 10.1% on average and surpasses the standard MoE trained without data restrictions using the same training FLOPs. Altogether, this research presents a solution for both data owners and researchers in regulated industries with sensitive or protected data. FlexOlmo enables benefiting from closed data while respecting data owners' preferences by keeping their data local and supporting fine-grained control of data access during inference.

**Link**: [arxiv](http://arxiv.org/abs/2507.07024v1),  [pdf](http://arxiv.org/pdf/2507.07024v1)

**Tags**: cs.CL cs.AI 



### First Return, Entropy-Eliciting Explore
**Authors**: Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, Zejun Ma

**Updated**: 2025-07-09T16:45:48Z

**Summary**: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration.

**Link**: [arxiv](http://arxiv.org/abs/2507.07017v1),  [pdf](http://arxiv.org/pdf/2507.07017v1)

**Tags**: cs.AI 



### TokenShapley: Token Level Context Attribution with Shapley Value
**Authors**: Yingtai Xiao, Yuqing Zhu, Sirat Samyoun, Wanrong Zhang, Jiachen T. Wang, Jian Du

**Updated**: 2025-07-09T16:40:38Z

**Summary**: Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.05261v2),  [pdf](http://arxiv.org/pdf/2507.05261v2)

**Tags**: cs.CL cs.LG 



### PhyloProfile v2: Scalable Exploration of Multilayered Phylogenetic   Profiles via Dimensionality Reduction
**Authors**: Vinh Tran, Ingo Ebersberger

**Updated**: 2025-07-09T16:39:17Z

**Summary**: Phylogenetic profiles - presence-absence patterns of genes across taxa - are rich information sources for inferring the evolutionary history of genes and gene families. When aggregated across many genes, these profiles can reveal coevolutionary patterns, supporting the prediction of gene functions and interactions. With rapidly growing numbers of sequenced genomes, phylogenetic profiles now routinely encompass thousands of genes and taxa. Existing software fall short in enabling interactive visualization, exploration, and analysis of such large datasets. We present PhyloProfile v2, a comprehensive overhaul of the original PhyloProfile software. This new version introduces major performance improvements along with novel features designed for more efficient data exploration. Notably, PhyloProfile v2 integrates dimensionality reduction techniques to visualize phylogenetic profiles in interactive 2D or 3D space, offering an intuitive overview even for massive datasets. Furthermore, the platform enables seamless transitions from large-scale analyses - spanning millions of orthology relationships - to detailed comparisons of protein feature architectures between specific orthologs. PhyloProfile v2 thus provides a versatile and scalable solution for evolutionary and functional genomics research. PhyloProfile v2 is available as an R package at Bioconductor https://doi.org/doi:10.18129/B9.bioc.PhyloProfile. The open-source code and documentation are provided under MIT license at https://github.com/BIONF/PhyloProfile

**Link**: [arxiv](http://arxiv.org/abs/2504.19710v2),  [pdf](http://arxiv.org/pdf/2504.19710v2)

**Tags**: q-bio.PE 



### LongAnimation: Long Animation Generation with Dynamic Global-Local   Memory
**Authors**: Nan Chen, Mengqi Huang, Yihao Meng, Zhendong Mao

**Updated**: 2025-07-09T16:30:21Z

**Summary**: Animation colorization is a crucial part of real animation industry production. Long animation colorization has high labor costs. Therefore, automated long animation colorization based on the video generation model has significant research value. Existing studies are limited to short-term colorization. These studies adopt a local paradigm, fusing overlapping features to achieve smooth transitions between local segments. However, the local paradigm neglects global information, failing to maintain long-term color consistency. In this study, we argue that ideal long-term color consistency can be achieved through a dynamic global-local paradigm, i.e., dynamically extracting global color-consistent features relevant to the current generation. Specifically, we propose LongAnimation, a novel framework, which mainly includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color Consistency Reward. The SketchDiT captures hybrid reference features to support the DGLM module. The DGLM module employs a long video understanding model to dynamically compress global historical features and adaptively fuse them with the current generation features. To refine the color consistency, we introduce a Color Consistency Reward. During inference, we propose a color consistency fusion to smooth the video segment transition. Extensive experiments on both short-term (14 frames) and long-term (average 500 frames) animations show the effectiveness of LongAnimation in maintaining short-term and long-term color consistency for open-domain animation colorization task. The code can be found at https://cn-makers.github.io/long_animation_web/.

**Link**: [arxiv](http://arxiv.org/abs/2507.01945v2),  [pdf](http://arxiv.org/pdf/2507.01945v2)

**Tags**: cs.CV 



### Skewed Score: A statistical framework to assess autograders
**Authors**: Magda Dubois, Harry Coppock, Mario Giulianelli, Timo Flesch, Lennart Luettgau, Cozmin Ududec

**Updated**: 2025-07-09T16:28:55Z

**Summary**: The evaluation of large language model (LLM) outputs is increasingly performed by other LLMs, a setup commonly known as "LLM-as-a-judge", or autograders. While autograders offer a scalable alternative to human evaluation, they have shown mixed reliability and may exhibit systematic biases, depending on response type, scoring methodology, domain specificity, or other factors. Here we propose a statistical framework based on Bayesian generalised linear models (GLMs) that enables researchers to simultaneously assess their autograders while addressing their primary research questions (e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores or pairwise preferences) as a function of properties of the grader (e.g., human vs. autograder) and the evaluated item (e.g., response length or the LLM that generated it), allowing for explicit quantification of scoring differences and potential biases within a unified framework. In addition, our method can be used to augment traditional metrics such as inter-rater agreement, by providing uncertainty estimates and clarifying sources of disagreement. Overall, this approach contributes to more robust and interpretable use of autograders in LLM evaluation, enabling both performance analysis and bias detection.

**Link**: [arxiv](http://arxiv.org/abs/2507.03772v2),  [pdf](http://arxiv.org/pdf/2507.03772v2)

**Tags**: cs.LG stat.ML 



### Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning   in Multimodal LLMs
**Authors**: Yahan Yu, Yuyang Dong, Masafumi Oyamada

**Updated**: 2025-07-09T16:25:44Z

**Summary**: Reasoning is a key capability for large language models (LLMs), particularly when applied to complex tasks such as mathematical problem solving. However, multimodal reasoning research still requires further exploration of modality alignment and training costs. Many of these approaches rely on additional data annotation and relevant rule-based rewards to enhance the understanding and reasoning ability, which significantly increases training costs and limits scalability. To address these challenges, we propose the Deliberate-to-Intuitive reasoning framework (D2I) that improves the understanding and reasoning ability of multimodal LLMs (MLLMs) without extra annotations and complex rewards. Specifically, our method sets deliberate reasoning strategies to enhance modality alignment only through the rule-based format reward during training. While evaluating, the reasoning style shifts to intuitive, which removes deliberate reasoning strategies during training and implicitly reflects the model's acquired abilities in the response. D2I outperforms baselines across both in-domain and out-of-domain benchmarks. Our findings highlight the role of format reward in fostering transferable reasoning skills in MLLMs, and inspire directions for decoupling training-time reasoning depth from test-time response flexibility.

**Link**: [arxiv](http://arxiv.org/abs/2507.06999v1),  [pdf](http://arxiv.org/pdf/2507.06999v1)

**Tags**: cs.CV cs.CL cs.LG 



### The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced   Planning, Navigation, and Dynamic Adaptation
**Authors**: Jieren Deng, Aleksandar Cvetkovic, Pak Kiu Chung, Dragomir Yankov, Chiqun Zhang

**Updated**: 2025-07-09T16:18:09Z

**Summary**: Traditional travel-planning systems are often static and fragmented, leaving them ill-equipped to handle real-world complexities such as evolving environmental conditions and unexpected itinerary disruptions. In this paper, we identify three gaps between existing service providers causing frustrating user experience: intelligent trip planning, precision "last-100-meter" navigation, and dynamic itinerary adaptation. We propose three cooperative agents: a Travel Planning Agent that employs grid-based spatial grounding and map analysis to help resolve complex multi-modal user queries; a Destination Assistant Agent that provides fine-grained guidance for the final navigation leg of each journey; and a Local Discovery Agent that leverages image embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to trip plan disruptions. With evaluations and experiments, our system demonstrates substantial improvements in query interpretation, navigation accuracy, and disruption resilience, underscoring its promise for applications from urban exploration to emergency response.

**Link**: [arxiv](http://arxiv.org/abs/2507.06993v1),  [pdf](http://arxiv.org/pdf/2507.06993v1)

**Tags**: cs.AI cs.CV 



### MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation
**Authors**: Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang

**Updated**: 2025-07-09T16:15:38Z

**Summary**: Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation.

**Link**: [arxiv](http://arxiv.org/abs/2507.06992v1),  [pdf](http://arxiv.org/pdf/2507.06992v1)

**Tags**: cs.CV cs.AI 



### Planning Anything with Rigor: General-Purpose Zero-Shot Planning with   LLM-based Formalized Programming
**Authors**: Yilun Hao, Yang Zhang, Chuchu Fan

**Updated**: 2025-07-09T16:13:20Z

**Summary**: While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such as multi-constraint or long-horizon tasks. On the other hand, many frameworks aiming to solve complex planning problems often rely on task-specific preparatory efforts, such as task-specific in-context examples and pre-defined critics/verifiers, which limits their cross-task generalization capability. In this paper, we tackle these challenges by observing that the core of many planning problems lies in optimization problems: searching for the optimal solution (best plan) with goals subject to constraints (preconditions and effects of decisions). With LLMs' commonsense, reasoning, and programming capabilities, this opens up the possibilities of a universal LLM-based approach to planning problems. Inspired by this observation, we propose LLMFP, a general-purpose framework that leverages LLMs to capture key information from planning problems and formally formulate and solve them as optimization problems from scratch, with no task-specific examples needed. We apply LLMFP to 9 planning problems, ranging from multi-constraint decision making to multi-step planning problems, and demonstrate that LLMFP achieves on average 83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet, significantly outperforming the best baseline (direct planning with OpenAI o1-preview) with 37.6% and 40.7% improvements. We also validate components of LLMFP with ablation experiments and analyzed the underlying success and failure reasons. Project page: https://sites.google.com/view/llmfp.

**Link**: [arxiv](http://arxiv.org/abs/2410.12112v3),  [pdf](http://arxiv.org/pdf/2410.12112v3)

**Tags**: cs.AI cs.CL 



### BarkBeetle: Stealing Decision Tree Models with Fault Injection
**Authors**: Qifan Wang, Jonas Sander, Minmin Jiang, Thomas Eisenbarth, David Oswald

**Updated**: 2025-07-09T16:08:58Z

**Summary**: Machine learning models, particularly decision trees (DTs), are widely adopted across various domains due to their interpretability and efficiency. However, as ML models become increasingly integrated into privacy-sensitive applications, concerns about their confidentiality have grown, particularly in light of emerging threats such as model extraction and fault injection attacks. Assessing the vulnerability of DTs under such attacks is therefore important. In this work, we present BarkBeetle, a novel attack that leverages fault injection to extract internal structural information of DT models. BarkBeetle employs a bottom-up recovery strategy that uses targeted fault injection at specific nodes to efficiently infer feature splits and threshold values. Our proof-of-concept implementation demonstrates that BarkBeetle requires significantly fewer queries and recovers more structural information compared to prior approaches, when evaluated on DTs trained with public UCI datasets. To validate its practical feasibility, we implement BarkBeetle on a Raspberry Pi RP2350 board and perform fault injections using the Faultier voltage glitching tool. As BarkBeetle targets general DT models, we also provide an in-depth discussion on its applicability to a broader range of tree-based applications, including data stream classification, DT variants, and cryptography schemes.

**Link**: [arxiv](http://arxiv.org/abs/2507.06986v1),  [pdf](http://arxiv.org/pdf/2507.06986v1)

**Tags**: cs.CR 



### Are They All Good? Evaluating the Quality of CoTs in LLM-based Code   Generation
**Authors**: Binquan Zhang, Li Zhang, Zhiwen Luo, Yuxin Du, Fang Liu, Song Wang, Lin Shi

**Updated**: 2025-07-09T16:07:20Z

**Summary**: Large language models (LLMs) have demonstrated impressive performance in code generation, particularly when augmented with chain-of-thought (CoT) prompting techniques. They break down requirements into intermediate reasoning steps, which act as design rationales to guide LLMs in writing code like human programmers. Thus, the quality of these steps is crucial for ensuring the correctness and reliability of the generated code. However, little is known about the quality of CoT generated by LLMs. To what extent can we trust the thoughts generated by LLMs? How good are they? This paper empirically explores the external and internal factors of why LLMs generate unsatisfactory CoTs by analyzing 1,023 failed code samples on two widely used code generation benchmarks. We also evaluate their impact on code generation performance by analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting LLMs. Our study reveals three key findings: (1) External factors (53.60%), such as unclear requirements and lack of context, mainly affect CoT quality, while internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even when CoTs are correct, 18.5% of the generated code contains errors due to instruction-following issues; conversely, 11.90% of correct code is paired with flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when given detailed problem descriptions. These findings highlight key challenges in CoT-based code generation and suggest directions for improving LLM reasoning and reliability.

**Link**: [arxiv](http://arxiv.org/abs/2507.06980v1),  [pdf](http://arxiv.org/pdf/2507.06980v1)

**Tags**: cs.SE 



### DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via   Joint LiDAR-Based 3D Object Detection and Denoising
**Authors**: Sven Teufel, Dominique Mayer, Jörg Gamerdinger, Oliver Bringmann

**Updated**: 2025-07-09T16:05:25Z

**Summary**: While automated vehicles hold the potential to significantly reduce traffic accidents, their perception systems remain vulnerable to sensor degradation caused by adverse weather and environmental occlusions. Collective perception, which enables vehicles to share information, offers a promising approach to overcoming these limitations. However, to this date collective perception in adverse weather is mostly unstudied. Therefore, we conduct the first study of LiDAR-based collective perception under diverse weather conditions and present a novel multi-task architecture for LiDAR-based collective perception under adverse weather. Adverse weather conditions can not only degrade perception capabilities, but also negatively affect bandwidth requirements and latency due to the introduced noise that is also transmitted and processed. Denoising prior to communication can effectively mitigate these issues. Therefore, we propose DenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective perception under adverse weather conditions. DenoiseCP-Net integrates voxel-level noise filtering and object detection into a unified sparse convolution backbone, eliminating redundant computations associated with two-stage pipelines. This design not only reduces inference latency and computational cost but also minimizes communication overhead by removing non-informative noise. We extended the well-known OPV2V dataset by simulating rain, snow, and fog using our realistic weather simulation models. We demonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in adverse weather, reduces the bandwidth requirements by up to 23.6% while maintaining the same detection accuracy and reducing the inference latency for cooperative vehicles.

**Link**: [arxiv](http://arxiv.org/abs/2507.06976v1),  [pdf](http://arxiv.org/pdf/2507.06976v1)

**Tags**: cs.CV 



### Unifying Re-Identification, Attribute Inference, and Data Reconstruction   Risks in Differential Privacy
**Authors**: Bogdan Kulynych, Juan Felipe Gomez, Georgios Kaissis, Jamie Hayes, Borja Balle, Flavio du Pin Calmon, Jean Louis Raisaro

**Updated**: 2025-07-09T15:59:30Z

**Summary**: Differentially private (DP) mechanisms are difficult to interpret and calibrate because existing methods for mapping standard privacy parameters to concrete privacy risks -- re-identification, attribute inference, and data reconstruction -- are both overly pessimistic and inconsistent. In this work, we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that bounds on attack success can take the same unified form across re-identification, attribute inference, and data reconstruction risks. Our unified bounds are (1) consistent across a multitude of attack settings, and (2) tunable, enabling practitioners to evaluate risk with respect to arbitrary (including worst-case) levels of baseline risk. Empirically, our results are tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated DP. As a result, calibrating noise using our bounds can reduce the required noise by 20% at the same risk level, which yields, e.g., more than 15pp accuracy increase in a text classification task. Overall, this unifying perspective provides a principled framework for interpreting and calibrating the degree of protection in DP against specific levels of re-identification, attribute inference, or data reconstruction risk.

**Link**: [arxiv](http://arxiv.org/abs/2507.06969v1),  [pdf](http://arxiv.org/pdf/2507.06969v1)

**Tags**: cs.LG cs.AI cs.CR cs.CY stat.ML 



### IdentityByDescentDispersal.jl: Inferring dispersal rates with   identity-by-descent blocks
**Authors**: Francisco Campuzano Jiménez, Arthur Zwaenepoel, Els Lea R De Keyzer, Hannes Svardal

**Updated**: 2025-07-10T05:41:21Z

**Summary**: The population density and per-generation dispersal rate of a population are central parameters in the study of evolution and ecology. The distribution of recent coalescent events between individuals in space can be used to estimate such quantities through the distribution of identity-by-descent (IBD) blocks. An IBD block is defined as a segment of DNA that has been inherited by a pair of individuals from a common ancestor without being broken by recombination. We introduce IdentityByDescentDispersal.jl, a Julia package for estimating effective population densities and dispersal rates from observed spatial patterns of IBD shared blocks. It implements the inference scheme proposed by Ringbauer, Coop, and Barton (2017). The package provides a user-friendly interface, supports efficient gradient-based optimization and accommodates arbitrary user-defined demographic models through numerical integration. This software aims to encourage a wider audience to utilize spatial genetic data for estimating dispersal rates, thereby motivating further research and expanding its applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.06964v2),  [pdf](http://arxiv.org/pdf/2507.06964v2)

**Tags**: q-bio.PE 



### Bayesian Quantum Amplitude Estimation
**Authors**: Alexandra Ramôa, Luis Paulo Santos

**Updated**: 2025-07-09T15:49:27Z

**Summary**: We present BAE, a problem-tailored and noise-aware Bayesian algorithm for quantum amplitude estimation. In a fault tolerant scenario, BAE is capable of saturating the Heisenberg limit; if device noise is present, BAE can dynamically characterize it and self-adapt. We further propose aBAE, an annealed variant of BAE drawing on methods from statistical inference, to enhance robustness. Our proposals are parallelizable in both quantum and classical components, offer tools for fast noise model assessment, and can leverage preexisting information. Additionally, they accommodate experimental limitations and preferred cost trade-offs. We propose a robust benchmark for amplitude estimation algorithms and use it to test BAE against other approaches, demonstrating its competitive performance in both noisy and noiseless scenarios. In both cases, it achieves lower error than any other algorithm as a function of the cost. In the presence of decoherence, it is capable of learning when other algorithms fail.

**Link**: [arxiv](http://arxiv.org/abs/2412.04394v2),  [pdf](http://arxiv.org/pdf/2412.04394v2)

**Tags**: quant-ph 



### Off-Policy Evaluation Under Nonignorable Missing Data
**Authors**: Han Wang, Yang Xu, Wenbin Lu, Rui Song

**Updated**: 2025-07-09T15:46:39Z

**Summary**: Off-Policy Evaluation (OPE) aims to estimate the value of a target policy using offline data collected from potentially different policies. In real-world applications, however, logged data often suffers from missingness. While OPE has been extensively studied in the literature, a theoretical understanding of how missing data affects OPE results remains unclear. In this paper, we investigate OPE in the presence of monotone missingness and theoretically demonstrate that the value estimates remain unbiased under ignorable missingness but can be biased under nonignorable (informative) missingness. To retain the consistency of value estimation, we propose an inverse probability weighted value estimator and conduct statistical inference to quantify the uncertainty of the estimates. Through a series of numerical experiments, we empirically demonstrate that our proposed estimator yields a more reliable value inference under missing data.

**Link**: [arxiv](http://arxiv.org/abs/2507.06961v1),  [pdf](http://arxiv.org/pdf/2507.06961v1)

**Tags**: stat.ML cs.LG 



### Bayesian Invariance Modeling of Multi-Environment Data
**Authors**: Luhuan Wu, Mingzhang Yin, Yixin Wang, John P. Cunningham, David M. Blei

**Updated**: 2025-07-09T15:42:31Z

**Summary**: Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from multiple environments to identify invariant features - those with a stable predictive relationship to the outcome. Such features support generalization to new environments and help reveal causal mechanisms. Previous methods have primarily tackled this problem through hypothesis testing or regularized optimization. Here we develop Bayesian Invariant Prediction (BIP), a probabilistic model for invariant prediction. BIP encodes the indices of invariant features as a latent variable and recover them by posterior inference. Under the assumptions of Peters et al. [2016], the BIP posterior targets the true invariant features. We prove that the posterior is consistent and that greater environment heterogeneity leads to faster posterior contraction. To handle many features, we design an efficient variational approximation called VI-BIP. In simulations and real data, we find that BIP and VI-BIP are more accurate and scalable than existing methods for invariant prediction.

**Link**: [arxiv](http://arxiv.org/abs/2506.22675v3),  [pdf](http://arxiv.org/pdf/2506.22675v3)

**Tags**: stat.ML cs.LG 



### Investigating the Robustness of Retrieval-Augmented Generation at the   Query Level
**Authors**: Sezen Perçin, Xin Su, Qutub Sha Syed, Phillip Howard, Aleksei Kuvshinov, Leo Schwinn, Kay-Ulrich Scholl

**Updated**: 2025-07-09T15:39:17Z

**Summary**: Large language models (LLMs) are very costly and inefficient to update with new information. To address this limitation, retrieval-augmented generation (RAG) has been proposed as a solution that dynamically incorporates external knowledge during inference, improving factual consistency and reducing hallucinations. Despite its promise, RAG systems face practical challenges-most notably, a strong dependence on the quality of the input query for accurate retrieval. In this paper, we investigate the sensitivity of different components in the RAG pipeline to various types of query perturbations. Our analysis reveals that the performance of commonly used retrievers can degrade significantly even under minor query variations. We study each module in isolation as well as their combined effect in an end-to-end question answering setting, using both general-domain and domain-specific datasets. Additionally, we propose an evaluation framework to systematically assess the query-level robustness of RAG pipelines and offer actionable recommendations for practitioners based on the results of more than 1092 experiments we performed.

**Link**: [arxiv](http://arxiv.org/abs/2507.06956v1),  [pdf](http://arxiv.org/pdf/2507.06956v1)

**Tags**: cs.CL 



### Calibration of Quantum Devices via Robust Statistical Methods
**Authors**: Alexandra Ramôa, Raffaele Santagati, Nathan Wiebe

**Updated**: 2025-07-09T15:22:17Z

**Summary**: Bayesian inference is a widely used technique for real-time characterization of quantum systems. It excels in experimental characterization in the low data regime, and when the measurements have degrees of freedom. A decisive factor for its performance is the numerical representation of the Bayesian probability distributions. In this work, we explore advanced statistical methods for this purpose, and numerically analyze their performance against the state-of-the-art in quantum parameter learning. In particular, we consider sequential importance resampling, tempered likelihood estimation, Markov Chain Monte Carlo, random walk Metropolis (RWM), Hamiltonian Monte Carlo (HMC) and variants (stochastic gradients with and without friction, energy conserving subsampling), block pseudo-marginal Metropolis-Hastings with subsampling, hybrid HMC-RWM approaches, and Gaussian rejection filtering. We demonstrate advantages of these approaches over existing ones, namely robustness under multi-modality and high dimensionality. We apply these algorithms to the calibration of superconducting qubits from IBMQ, surpassing the standard quantum limit and achieving better results than Qiskit's default tools. In Hahn echo and Ramsey experiments, we reduce the uncertainty by factors of 10 and 3 respectively, without increasing the number of measurements; conversely, we match the performance of Qiskit's methods while using up to to 99.5% less experimental data. We additionally investigate the roles of adaptivity, dataset ordering and heuristics in quantum characterization. Our findings have applications in challenging quantum characterization tasks, namely learning the dynamics of open quantum systems.

**Link**: [arxiv](http://arxiv.org/abs/2507.06941v1),  [pdf](http://arxiv.org/pdf/2507.06941v1)

**Tags**: quant-ph 



### Sound Interval-Based Synthesis for Probabilistic Programs
**Authors**: Guilherme Espada, Alcides Fonseca

**Updated**: 2025-07-09T15:21:22Z

**Summary**: Probabilistic programming has become a standard practice to model stochastic events and learn about the behavior of nature in different scientific contexts, ranging from Genetics and Ecology to Linguistics and Psychology. However, domain practitioners (such as biologists) also need to be experts in statistics in order to select which probabilistic model is suitable for a given particular problem, relying then on probabilistic inference engines such as Stan, Pyro or Edward to fine-tune the parameters of that particular model. Probabilistic Programming would be more useful if the model selection is made automatic, without requiring statistics expertise from the end user. Automatically selecting the model is challenging because of the large search space of probabilistic programs needed to be explored, because the fact that most of that search space contains invalid programs, and because invalid programs may only be detected in some executions, due to its probabilistic nature. We propose a type system to statically reject invalid probabilistic programs, a type-directed synthesis algorithm that guarantees that generated programs are type-safe by construction, and an heuristic search procedure to handle the vast search space. We collect a number of probabilistic programs from the literature, and use them to compare our method with both a type-agnostic random search, and a data-guided method from the literature (DaPPer). Our results show that our technique both outperforms random search and DaPPer, specially on more complex programs. This drastic performance difference in synthesis allows for fast sampling of programs and enables techniques that previously suffered from the complexity of synthesis, such as Genetic Programming, to be applied.

**Link**: [arxiv](http://arxiv.org/abs/2507.06939v1),  [pdf](http://arxiv.org/pdf/2507.06939v1)

**Tags**: cs.PL 



### Accelerated Spatio-Temporal Bayesian Modeling for Multivariate Gaussian   Processes
**Authors**: Lisa Gaedke-Merzhäuser, Vincent Maillou, Fernando Rodriguez Avellaneda, Olaf Schenk, Mathieu Luisier, Paula Moraga, Alexandros Nikolaos Ziogas, Håvard Rue

**Updated**: 2025-07-09T15:20:02Z

**Summary**: Multivariate Gaussian processes (GPs) offer a powerful probabilistic framework to represent complex interdependent phenomena. They pose, however, significant computational challenges in high-dimensional settings, which frequently arise in spatial-temporal applications. We present DALIA, a highly scalable framework for performing Bayesian inference tasks on spatio-temporal multivariate GPs, based on the methodology of integrated nested Laplace approximations. Our approach relies on a sparse inverse covariance matrix formulation of the GP, puts forward a GPU-accelerated block-dense approach, and introduces a hierarchical, triple-layer, distributed memory parallel scheme. We showcase weak scaling performance surpassing the state-of-the-art by two orders of magnitude on a model whose parameter space is 8$\times$ larger and measure strong scaling speedups of three orders of magnitude when running on 496 GH200 superchips on the Alps supercomputer. Applying DALIA to air pollution data from northern Italy over 48 days, we showcase refined spatial resolutions over the aggregated pollutant measurements.

**Link**: [arxiv](http://arxiv.org/abs/2507.06938v1),  [pdf](http://arxiv.org/pdf/2507.06938v1)

**Tags**: stat.CO cs.DC 62F15, 68W15 G.3; G.4 



### Neuron-Level Differentiation of Memorization and Generalization in Large   Language Models
**Authors**: Ko-Wei Huang, Yi-Fu Fu, Ching-Yu Tsai, Yu-Chieh Tu, Tzu-Ling Cheng, Cheng-Yu Lin, Yi-Ting Yang, Heng-Yi Liu, Keng-Te Liao, Da-Cheng Juan, Shou-De Lin

**Updated**: 2025-07-09T15:14:46Z

**Summary**: We investigate how Large Language Models (LLMs) distinguish between memorization and generalization at the neuron level. Through carefully designed tasks, we identify distinct neuron subsets responsible for each behavior. Experiments on both a GPT-2 model trained from scratch and a pretrained LLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level specialization. We further demonstrate that inference-time interventions on these neurons can steer the model's behavior toward memorization or generalization. To assess robustness, we evaluate intra-task and inter-task consistency, confirming that these neuron-behavior associations reflect generalizable patterns rather than dataset-specific artifacts. Our findings reveal modular structure in LLMs and enable controlling memorization and generalization behaviors at inference time.

**Link**: [arxiv](http://arxiv.org/abs/2412.18497v2),  [pdf](http://arxiv.org/pdf/2412.18497v2)

**Tags**: cs.CL 



### Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning   Large Language Models
**Authors**: Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, Wanxiang Che

**Updated**: 2025-07-09T15:13:24Z

**Summary**: Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like "overthinking" and "inference-time scaling." This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and inference-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2503.09567v4),  [pdf](http://arxiv.org/pdf/2503.09567v4)

**Tags**: cs.AI cs.CL 



### What to Keep and What to Drop: Adaptive Table Filtering Framework
**Authors**: WonJune Jang

**Updated**: 2025-07-09T15:10:56Z

**Summary**: Large language models (LLMs) for table-based reasoning often struggle with large tables due to input length limits. We propose ATF (Adaptive Table Filtering Framework), a modular and question-aware filtering pipeline that prunes uninformative columns and rows using LLM-generated column descriptions, clustering, and sparse-dense alignment scores. ATF integrates seamlessly with existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that ATF reduces table cells by 70%, boosting performance on out-of-domain TableQA tasks while causing slight performance drops on Table Fact Verification, where full-table context is more critical. These results highlight ATF's ability to adaptively balance informativeness and minimalism across tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.23463v2),  [pdf](http://arxiv.org/pdf/2506.23463v2)

**Tags**: cs.CL I.2.7 



### Distribution-free inference for LightGBM and GLM with Tweedie loss
**Authors**: Alokesh Manna, Aditya Vikram Sett, Dipak K. Dey, Yuwen Gu, Elizabeth D. Schifano, Jichao He

**Updated**: 2025-07-09T14:58:54Z

**Summary**: Prediction uncertainty quantification is a key research topic in recent years scientific and business problems. In insurance industries (\cite{parodi2023pricing}), assessing the range of possible claim costs for individual drivers improves premium pricing accuracy. It also enables insurers to manage risk more effectively by accounting for uncertainty in accident likelihood and severity. In the presence of covariates, a variety of regression-type models are often used for modeling insurance claims, ranging from relatively simple generalized linear models (GLMs) to regularized GLMs to gradient boosting models (GBMs). Conformal predictive inference has arisen as a popular distribution-free approach for quantifying predictive uncertainty under relatively weak assumptions of exchangeability, and has been well studied under the classic linear regression setting. In this work, we propose new non-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized Tweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal prediction performance with these non-conformity measures in insurance claims data. Our simulation results favor the use of locally weighted Pearson residuals for LightGBM over other methods considered, as the resulting intervals maintained the nominal coverage with the smallest average width.

**Link**: [arxiv](http://arxiv.org/abs/2507.06921v1),  [pdf](http://arxiv.org/pdf/2507.06921v1)

**Tags**: stat.ML cs.LG Application to insurance data, Methodology 



### Rethinking Verification for LLM Code Generation: From Generation to   Testing
**Authors**: Zihan Ma, Taolin Zhang, Maosong Cao, Junnan Liu, Wenwei Zhang, Minnan Luo, Songyang Zhang, Kai Chen

**Updated**: 2025-07-10T03:12:09Z

**Summary**: Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.

**Link**: [arxiv](http://arxiv.org/abs/2507.06920v2),  [pdf](http://arxiv.org/pdf/2507.06920v2)

**Tags**: cs.CL 



### Going beyond $S_8$: fast inference of the matter power spectrum from   weak-lensing surveys
**Authors**: Cyrille Doux, Tanvi Karwal

**Updated**: 2025-07-09T14:47:50Z

**Summary**: Weak lensing surveys are often summarized by constraints on the derived parameter ${S_8\equiv\sigma_8\sqrt{\Omega_{\rm m}/0.3}}$, obscuring the rich scale and redshift information encoded in the data, and limiting our ability to identify the origin of any tensions with $\Lambda$CDM predictions from the cosmic microwave background. In this work, we introduce a fast and flexible framework to extract the scale-dependent matter power spectrum $P(k, z)$ from cosmic shear and CMB lensing measurements, parameterizing deviations from the Planck $\Lambda$CDM prediction as a free function $\alpha(k)$. Using public data from DES Y3, KiDS-1000, HSC Y3, and ACT DR6, we constrain $\alpha(k)$ with fast Hamiltonian Monte Carlo inference, employing multipoles up to $\ell_{\rm max}\sim2000$ for the galaxy lensing surveys. Our results show a consistent 15-30% suppression in the matter power spectrum at intermediate scales ($k \sim 0.1-1{\rm Mpc}^{-1}$) in galaxy-lensing data relative to a Planck $\Lambda$CDM prediction with a CDM-only (no baryonic feedback) power spectrum, with combined tensions reaching up to $4\sigma$. This is under a fixed cosmology and with analytic marginalization over shear and redshift calibration uncertainties. In contrast, ACT CMB lensing is consistent with $\Lambda$CDM at ${k\lesssim 0.1 {\rm Mpc}^{-1}}$. We validate our method using mock data, quantify consistency between datasets, and demonstrate how the resulting $\alpha(k)$ likelihoods can be used to test specific models for the power spectrum. All code, data products, and derived likelihoods are publicly released. Our results highlight the importance of reporting lensing constraints on $P(k, z)$ and pave the way for model-agnostic test of growth of structure with upcoming surveys such as LSST, Euclid, and Roman.

**Link**: [arxiv](http://arxiv.org/abs/2506.16434v2),  [pdf](http://arxiv.org/pdf/2506.16434v2)

**Tags**: astro-ph.CO 



### Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in   Dialogues
**Authors**: Fareya Ikram, Alexander Scarlatos, Andrew Lan

**Updated**: 2025-07-09T14:47:35Z

**Summary**: Tutoring dialogues have gained significant attention in recent years, given the prominence of online learning and the emerging tutoring abilities of artificial intelligence (AI) agents powered by large language models (LLMs). Recent studies have shown that the strategies used by tutors can have significant effects on student outcomes, necessitating methods to predict how tutors will behave and how their actions impact students. However, few works have studied predicting tutor strategy in dialogues. Therefore, in this work we investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to predict both future tutor moves and student outcomes in dialogues, using two math tutoring dialogue datasets. We find that even state-of-the-art LLMs struggle to predict future tutor strategy while tutor strategy is highly indicative of student outcomes, outlining a need for more powerful methods to approach this task.

**Link**: [arxiv](http://arxiv.org/abs/2507.06910v1),  [pdf](http://arxiv.org/pdf/2507.06910v1)

**Tags**: cs.CL cs.CY 



### MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal   Prediction
**Authors**: Xiao Wang, Jiahuan Pei, Diancheng Shui, Zhiguang Han, Xin Sun, Dawei Zhu, Xiaoyu Shen

**Updated**: 2025-07-09T14:47:00Z

**Summary**: Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in LJP? To address this, we introduce a new dataset namely multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating the performance of several prevailing legal large language models (LLMs) on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD, while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD. Our dataset and code are available at https://github.com/lololo-xiao/MultiJustice-MPMCP.

**Link**: [arxiv](http://arxiv.org/abs/2507.06909v1),  [pdf](http://arxiv.org/pdf/2507.06909v1)

**Tags**: cs.CL cs.AI 



### NoLiMa: Long-Context Evaluation Beyond Literal Matching
**Authors**: Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Schütze

**Updated**: 2025-07-09T14:35:23Z

**Summary**: Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 13 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 11 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. Even models enhanced with reasoning capabilities or CoT prompting struggle to maintain performance in long contexts. We publicly release the dataset and evaluation code at https://github.com/adobe-research/NoLiMa.

**Link**: [arxiv](http://arxiv.org/abs/2502.05167v3),  [pdf](http://arxiv.org/pdf/2502.05167v3)

**Tags**: cs.CL 



### Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning   for Large Language Model
**Authors**: Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao

**Updated**: 2025-07-10T13:42:04Z

**Summary**: Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs). One major limitation of most existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL in nature, i.e., data generated during the past learning process is not fully utilized. This inevitably comes at a significant cost of compute and time, posing a stringent bottleneck on continuing economic and efficient scaling. To this end, we launch the renaissance of off-policy RL and propose Reincarnating Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio for efficient training; (2) KL-Convex policy constraint to balance the trade-off between stability and flexibility; (3) Policy reincarnation to achieve a seamless transition from efficient early-stage learning to steady asymptotic improvement. In our experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with 0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over 30x to 450x reduction in training cost in terms of rollout data volume. In addition, we reveal insightful findings via multifaceted analysis, including the implicit preference for shorter responses due to the Whipping Effect of off-policy discrepancy, the collapse mode of self-reflection behavior under the presence of severe off-policyness, etc.

**Link**: [arxiv](http://arxiv.org/abs/2507.06892v2),  [pdf](http://arxiv.org/pdf/2507.06892v2)

**Tags**: cs.LG cs.AI cs.CL 



### LARP: Learner-Agnostic Robust Data Prefiltering
**Authors**: Kristian Minchev, Dimitar Iliev Dimitrov, Nikola Konstantinov

**Updated**: 2025-07-10T08:40:09Z

**Summary**: The widespread availability of large public datasets is a key factor behind the recent successes of statistical inference and machine learning methods. However, these datasets often contain some low-quality or contaminated data, to which many learning procedures are sensitive. Therefore, the question of whether and how public datasets should be prefiltered to facilitate accurate downstream learning arises. On a technical level this requires the construction of principled data prefiltering methods which are learner-agnostic robust, in the sense of provably protecting a set of pre-specified downstream learners from corrupted data. In this work, we formalize the problem of Learner-Agnostic Robust data Prefiltering (LARP), which aims at finding prefiltering procedures that minimize a worst-case loss over a pre-specified set of learners. We first instantiate our framework in the context of scalar mean estimation with Huber estimators under the Huber data contamination model. We provide a hardness result on a specific problem instance and analyze several natural prefiltering procedures. Our theoretical results indicate that performing LARP on a heterogeneous set of learners leads to some loss in model performance compared to the alternative of prefiltering data for each learner/use-case individually. We explore the resulting utility loss and its dependence on the problem parameters via extensive experiments on real-world image and tabular data, observing statistically significant reduction in utility. Finally, we model the trade-off between the utility drop and the cost of repeated (learner-specific) prefiltering within a game-theoretic framework and showcase benefits of LARP for large datasets.

**Link**: [arxiv](http://arxiv.org/abs/2506.20573v3),  [pdf](http://arxiv.org/pdf/2506.20573v3)

**Tags**: stat.ML cs.LG 



### Toward Neurodivergent-Aware Productivity: A Systems and AI-Based   Human-in-the-Loop Framework for ADHD-Affected Professionals
**Authors**: Raghavendra Deshmukh

**Updated**: 2025-07-09T14:05:13Z

**Summary**: Digital work environments in IT and knowledge-based sectors demand high levels of attention management, task juggling, and self-regulation. For adults with ADHD, these settings often amplify challenges such as time blindness, digital distraction, emotional reactivity, and executive dysfunction. These individuals prefer low-touch, easy-to-use interventions for daily tasks. Conventional productivity tools often fail to support the cognitive variability and overload experienced by neurodivergent professionals. This paper presents a framework that blends Systems Thinking, Human-in-the-Loop design, AI/ML, and privacy-first adaptive agents to support ADHD-affected users. The assistant senses tab usage, application focus, and inactivity using on-device ML. These cues are used to infer attention states and deliver nudges, reflective prompts, or accountability-based presence (body doubling) that aid regulation without disruption. Technically grounded in AI, the approach views attention as shaped by dynamic feedback loops. The result is a replicable model for adaptive, inclusive support tools in high-distraction work environments.

**Link**: [arxiv](http://arxiv.org/abs/2507.06864v1),  [pdf](http://arxiv.org/pdf/2507.06864v1)

**Tags**: cs.HC 



### Nonparametric Bayesian Inference for Stochastic Reaction-Diffusion   Equations
**Authors**: Randolf Altmeyer, Sascha Gaudlitz

**Updated**: 2025-07-09T13:58:51Z

**Summary**: We consider the Bayesian nonparametric estimation of a nonlinear reaction function in a reaction-diffusion stochastic partial differential equation (SPDE). The likelihood is well-defined and tractable by the infinite-dimensional Girsanov theorem, and the posterior distribution is analysed in the growing domain asymptotic. Based on a Gaussian wavelet prior, the contraction of the posterior distribution around the truth at the minimax optimal rate is proved. The analysis of the posterior distribution is complemented by a semiparametric Bernstein--von Mises theorem. The proofs rely on the sub-Gaussian concentration of spatio-temporal averages of transformations of the SPDE, which is derived by combining the Clark-Ocone formula with bounds for the derivatives of the (marginal) densities of the SPDE.

**Link**: [arxiv](http://arxiv.org/abs/2507.06857v1),  [pdf](http://arxiv.org/pdf/2507.06857v1)

**Tags**: math.ST math.PR stat.TH Primary 62G20, 60H15, secondary 60H07, 62F15 



### Adaptive Elicitation of Latent Information Using Natural Language
**Authors**: Jimmy Wang, Thomas Zollo, Richard Zemel, Hongseok Namkoong

**Updated**: 2025-07-09T13:58:35Z

**Summary**: Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.

**Link**: [arxiv](http://arxiv.org/abs/2504.04204v2),  [pdf](http://arxiv.org/pdf/2504.04204v2)

**Tags**: cs.CL cs.AI cs.LG 



### Proof-Theoretic Functional Completeness for the Connexive Logic C
**Authors**: Sara Ayhan, Hrafn Valtýr Oddsson

**Updated**: 2025-07-09T13:58:07Z

**Summary**: We show the functional completeness for the connectives of the non-trivial negation inconsistent logic C by using a well-established method implementing purely proof-theoretic notions only. Firstly, given that C contains a strong negation, expressing a notion of direct refutation, the proof needs to be applied in a bilateralist way in that not only higher-order rule schemata for proofs but also for refutations need to be considered. Secondly, given that C is a connexive logic we need to take a connexive understanding of inference as a basis, leading to a different conception of (higher-order) refutation than is usually employed.

**Link**: [arxiv](http://arxiv.org/abs/2507.06854v1),  [pdf](http://arxiv.org/pdf/2507.06854v1)

**Tags**: cs.LO math.LO 



### DiffSpectra: Molecular Structure Elucidation from Spectra using   Diffusion Models
**Authors**: Liang Wang, Yu Rong, Tingyang Xu, Zhenyi Zhong, Zhiyuan Liu, Pengju Wang, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang

**Updated**: 2025-07-09T13:57:20Z

**Summary**: Molecular structure elucidation from spectra is a foundational problem in chemistry, with profound implications for compound identification, synthesis, and drug development. Traditional methods rely heavily on expert interpretation and lack scalability. Pioneering machine learning methods have introduced retrieval-based strategies, but their reliance on finite libraries limits generalization to novel molecules. Generative models offer a promising alternative, yet most adopt autoregressive SMILES-based architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that directly infers both 2D and 3D molecular structures from multi-modal spectral data using diffusion models. DiffSpectra formulates structure elucidation as a conditional generation process. Its denoising network is parameterized by Diffusion Molecule Transformer, an SE(3)-equivariant architecture that integrates topological and geometric information. Conditioning is provided by SpecFormer, a transformer-based spectral encoder that captures intra- and inter-spectral dependencies from multi-modal spectra. Extensive experiments demonstrate that DiffSpectra achieves high accuracy in structure elucidation, recovering exact structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through sampling. The model benefits significantly from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. These results highlight the effectiveness of spectrum-conditioned diffusion modeling in addressing the challenge of molecular structure elucidation. To our knowledge, DiffSpectra is the first framework to unify multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.

**Link**: [arxiv](http://arxiv.org/abs/2507.06853v1),  [pdf](http://arxiv.org/pdf/2507.06853v1)

**Tags**: cs.LG cs.AI cs.CE physics.chem-ph q-bio.MN 



### The Dark Side of LLMs: Agent-based Attacks for Complete Computer   Takeover
**Authors**: Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, Angelo Furfaro

**Updated**: 2025-07-10T15:18:20Z

**Summary**: The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables unprecedented capabilities in natural language processing and generation. However, these systems have introduced unprecedented security vulnerabilities that extend beyond traditional prompt injection attacks. This paper presents the first comprehensive evaluation of LLM agents as attack vectors capable of achieving complete computer takeover through the exploitation of trust boundaries within agentic AI systems where autonomous entities interact and influence each other. We demonstrate that adversaries can leverage three distinct attack surfaces - direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation - to coerce popular LLMs (including GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals an alarming vulnerability hierarchy: while 41.2% of models succumb to direct prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical 82.4% can be compromised through inter-agent trust exploitation. Notably, we discovered that LLMs which successfully resist direct malicious commands will execute identical payloads when requested by peer agents, revealing a fundamental flaw in current multi-agent security models. Our findings demonstrate that only 5.9% of tested models (1/17) proved resistant to all attack vectors, with the majority exhibiting context-dependent security behaviors that create exploitable blind spots. Our findings also highlight the need to increase awareness and research on the security risks of LLMs, showing a paradigm shift in cybersecurity threats, where AI tools themselves become sophisticated attack vectors.

**Link**: [arxiv](http://arxiv.org/abs/2507.06850v2),  [pdf](http://arxiv.org/pdf/2507.06850v2)

**Tags**: cs.CR cs.AI 



### OpenDPDv2: A Unified Learning and Optimization Framework for Neural   Network Digital Predistortion
**Authors**: Yizhuo Wu, Ang Li, Chang Gao

**Updated**: 2025-07-09T13:54:47Z

**Summary**: Neural network (NN)-based Digital Predistortion (DPD) stands out in improving signal quality in wideband radio frequency (RF) power amplifiers (PAs) employing complex modulation. However, NN DPDs usually rely on a large number of parameters for effective linearization and can significantly contribute to the energy consumption of the digital back-end in RF systems. This paper presents OpenDPDv2, a unified framework for PA modeling, DPD learning, and model optimization to reduce power consumption while maintaining high linearization performance. The optimization techniques feature a novel DPD algorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The top-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an Adjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude (EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal sparsity of input signals and hidden neurons, the inference energy of our model can be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM with 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth 256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code, datasets, and documentation are publicly accessible at: https://github.com/lab-emi/OpenDPD.

**Link**: [arxiv](http://arxiv.org/abs/2507.06849v1),  [pdf](http://arxiv.org/pdf/2507.06849v1)

**Tags**: eess.SP cs.AI 



### Perturbation theory for post-Newtonian neutron stars
**Authors**: Fabian Gittins, Nils Andersson, Shanshan Yin

**Updated**: 2025-07-09T13:53:54Z

**Summary**: Neutron stars are compact, relativistic bodies that host several extremes of modern physics. An exciting development in recent years has been the opportunity to probe this exotic physics by observing compact-binary coalescences using sensitive gravitational-wave and electromagnetic instruments. To maximise the science inferred from these measurements, we require models that accurately represent the physics. In this study, we consider the post-Newtonian approximation to general relativity for the modelling of neutron-star dynamics, with a particular view to model dynamical tides at the late stages of binary inspiral. We develop the post-Newtonian perturbation equations for a non-rotating star and show that the perturbation problem is Hermitian and therefore derives from a fundamental Lagrangian. Establishing this Lagrangian system leads to a conserved symplectic product and canonical energy for the perturbations. We determine the orthogonality condition for the post-Newtonian oscillation modes, which in turn forms the foundation of a mode-sum representation often used for dynamical tides. Finally, we demonstrate that the perturbation formulation is unique.

**Link**: [arxiv](http://arxiv.org/abs/2503.03345v2),  [pdf](http://arxiv.org/pdf/2503.03345v2)

**Tags**: gr-qc astro-ph.HE 



### Know Your Attention Maps: Class-specific Token Masking for Weakly   Supervised Semantic Segmentation
**Authors**: Joelle Hanna, Damian Borth

**Updated**: 2025-07-09T13:53:34Z

**Summary**: Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that has been extensively studied in recent years. Traditional approaches often rely on external modules like Class Activation Maps to highlight regions of interest and generate pseudo segmentation masks. In this work, we propose an end-to-end method that directly utilizes the attention maps learned by a Vision Transformer (ViT) for WSSS. We propose training a sparse ViT with multiple [CLS] tokens (one for each class), using a random masking strategy to promote [CLS] token - class assignment. At inference time, we aggregate the different self-attention maps of each [CLS] token corresponding to the predicted labels to generate pseudo segmentation masks. Our proposed approach enhances the interpretability of self-attention maps and ensures accurate class assignments. Extensive experiments on two standard benchmarks and three specialized datasets demonstrate that our method generates accurate pseudo-masks, outperforming related works. Those pseudo-masks can be used to train a segmentation model which achieves results comparable to fully-supervised models, significantly reducing the need for fine-grained labeled data.

**Link**: [arxiv](http://arxiv.org/abs/2507.06848v1),  [pdf](http://arxiv.org/pdf/2507.06848v1)

**Tags**: cs.CV 



### Constraints on fast radio burst population from the first CHIME/FRB   catalog with the Hierarchical Bayesian Inference
**Authors**: Huan Zhou, Zhengxiang Li, Zong-Hong Zhu

**Updated**: 2025-07-09T13:49:37Z

**Summary**: Fast Radio Bursts (FRBs) have emerged as one of the most dynamic areas of research in astronomy and cosmology. Despite increasing number of FRBs have been reported, the exact origin of FRBs remains elusive. Investigating the intrinsic redshift distributions of FRBs could provide valuable insights into their possible origins and enhance the power of FRBs as a cosmological probe. In this paper, we propose a hierarchical Bayesian inference approach combining with several viable models to investigate the redshift distribution of the CHIME/FRB catalog 1. By utilizing this method, we aim to uncover the underlying patterns and characteristics of the FRB population, i.e. intrinsic redshift distribution of FRB. Taking uncertainties within the observational data and selection effects into consideration, we obtained that the redshift distribution of FRBs is significantly delayed with respect to that of the star formation history.

**Link**: [arxiv](http://arxiv.org/abs/2501.15530v3),  [pdf](http://arxiv.org/pdf/2501.15530v3)

**Tags**: astro-ph.HE astro-ph.CO 



### EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and   Flexible LLM Fine-Tuning
**Authors**: Lingxiao Kong, Cong Yang, Susanne Neufang, Oya Deniz Beyan, Zeyd Boukhers

**Updated**: 2025-07-09T13:45:07Z

**Summary**: Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including competing objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the fine-tuning to improve efficiency and flexibility. Our method is the first to aggregate the hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text classification models to score the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.

**Link**: [arxiv](http://arxiv.org/abs/2505.02579v3),  [pdf](http://arxiv.org/pdf/2505.02579v3)

**Tags**: cs.CL cs.AI cs.LG 



### Measuring cosmic baryon density with FRB and GW data
**Authors**: Ji-Guo Zhang, Ji-Yu Song, Ze-Wei Zhao, Wan-Peng Sun, Jing-Fei Zhang, Xin Zhang

**Updated**: 2025-07-09T13:41:52Z

**Summary**: The dispersion measure (DM) analysis of extragalactic fast radio bursts (FRBs) has accounted for all cosmic baryons but remains limited by systematic uncertainties from various parameter degeneracies. We show that the prominent degeneracy between the baryon density ($\Omega_{\rm b}$) and the Hubble constant ($H_0$) can be disentangled through an independent $H_0$ value uniquely inferred from the absolute luminosity distance measurements of gravitational-wave (GW) standard sirens. By combining $104$ localized FRBs with $47$ GW events, we obtain a robust, late-Universe measurement of \ob $=0.0488\pm0.0064$ ($1\sigma$), in concordance with early-Universe observations of CMB + BBN. Notably, incorporating GW data helps not only avoid biases induced by the $H_0$ tension, but also mitigate those from the parameters of diffuse baryon fraction and DM distribution models. Although the current precision ($\sim 13\%$) is limited by sample size, the growing detections of both FRBs and GWs will make their synergy a powerful probe of low-redshift cosmology.

**Link**: [arxiv](http://arxiv.org/abs/2507.06841v1),  [pdf](http://arxiv.org/pdf/2507.06841v1)

**Tags**: astro-ph.CO astro-ph.HE gr-qc hep-ph 



### Shifting from Ranking to Set Selection for Retrieval Augmented   Generation
**Authors**: Dahyun Lee, Yongrae Jo, Haeju Park, Moontae Lee

**Updated**: 2025-07-10T01:36:33Z

**Summary**: Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set. Existing approaches primarily rerank top-k passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering. In this work, we propose a set-wise passage selection approach and introduce SETR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements. Experiments on multi-hop RAG benchmarks show that SETR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems. The code is available at https://github.com/LGAI-Research/SetR

**Link**: [arxiv](http://arxiv.org/abs/2507.06838v2),  [pdf](http://arxiv.org/pdf/2507.06838v2)

**Tags**: cs.CL cs.IR 



### Adaptive Termination for Multi-round Parallel Reasoning: An Universal   Semantic Entropy-Guided Framework
**Authors**: Zenan Xu, Zexuan Qiu, Guanhua Huang, Kun Li, Siheng Li, Chenchen Zhang, Kejiao Li, Qi Yi, Yuhao Jiang, Bo Zhou, Fengzong Lian, Zhanhui Kang

**Updated**: 2025-07-09T13:28:35Z

**Summary**: Recent advances in large language models (LLMs) have accelerated progress toward artificial general intelligence, with inference-time scaling emerging as a key technique. Contemporary approaches leverage either sequential reasoning (iteratively extending chains of thought) or parallel reasoning (generating multiple solutions simultaneously) to scale inference. However, both paradigms face fundamental limitations: sequential scaling typically relies on arbitrary token budgets for termination, leading to inefficiency or premature cutoff; while parallel scaling often lacks coordination among parallel branches and requires intrusive fine-tuning to perform effectively. In light of these challenges, we aim to design a flexible test-time collaborative inference framework that exploits the complementary strengths of both sequential and parallel reasoning paradigms. Towards this goal, the core challenge lies in developing an efficient and accurate intrinsic quality metric to assess model responses during collaborative inference, enabling dynamic control and early termination of the reasoning trace. To address this challenge, we introduce semantic entropy (SE), which quantifies the semantic diversity of parallel model responses and serves as a robust indicator of reasoning quality due to its strong negative correlation with accuracy...

**Link**: [arxiv](http://arxiv.org/abs/2507.06829v1),  [pdf](http://arxiv.org/pdf/2507.06829v1)

**Tags**: cs.CL 



### CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models   in Medical Quality Control Indicator Calculation
**Authors**: Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan

**Updated**: 2025-07-09T13:26:39Z

**Summary**: Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repository https://github.com/YuY-2001/C-MQCIC.

**Link**: [arxiv](http://arxiv.org/abs/2502.11703v2),  [pdf](http://arxiv.org/pdf/2502.11703v2)

**Tags**: cs.CL 



### LLM Agent for Hyper-Parameter Optimization
**Authors**: Wanzhe Wang, Jianqiu Peng, Menghao Hu, Weihuang Zhong, Tong Zhang, Shuai Wang, Yixin Zhang, Mingjie Shao, Wanli Ni

**Updated**: 2025-07-09T13:20:45Z

**Summary**: Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters optimization approaches for Warm-Start Particles Swarm Optimization with Crossover and Mutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication, are primarily heuristic-based, exhibiting low levels of automation and improvable performance. In this paper, we design an Large Language Model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and Model Context Protocol (MCP) are applied. In particular, the LLM agent is first set up via a profile, which specifies the boundary of hyper-parameters, task objective, terminal condition, conservative or aggressive strategy of optimizing hyper-parameters, and LLM configurations. Then, the LLM agent iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the loop based on the terminal condition and returns an optimized set of hyperparameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in seeking high-performance hyper-parameters.

**Link**: [arxiv](http://arxiv.org/abs/2506.15167v2),  [pdf](http://arxiv.org/pdf/2506.15167v2)

**Tags**: cs.IT cs.AI math.IT 



### Losing our Tail -- Again: On (Un)Natural Selection And Multilingual   Large Language Models
**Authors**: Eva Vanmassenhove

**Updated**: 2025-07-09T13:14:29Z

**Summary**: Multilingual Large Language Models (LLMs) considerably changed how technologies can influence language. While previous technologies could mediate or assist humans, there is now a tendency to offload the task of writing itself to these technologies, enabling them to change our linguistic ecosystem more directly. While they provide us quick access to information and impressively fluent output, beneath their apparent sophistication lies a subtle, more insidious threat: the gradual decline and loss of linguistic diversity. With this opinion piece, I explore how model collapse, with a particular focus on translation technology, can lead to the loss of linguistic forms, grammatical features, and cultural nuance. Model collapse refers to the eventual consequence of self-consuming training loops, where models reinforce their own biases and lose linguistic diversity. Drawing on recent work in Computer Vision, Natural Language Processing (NLP) and Machine Translation (MT), I argue that the tails of our linguistic distributions are vanishing, and with them, the narratives and identities they carry. This is a call to resist linguistic flattening and to reimagine NLP as a field that encourages, values and protects expressive multilingual lexical and linguistic diversity and creativity.

**Link**: [arxiv](http://arxiv.org/abs/2507.03933v2),  [pdf](http://arxiv.org/pdf/2507.03933v2)

**Tags**: cs.CL 



### Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts
**Authors**: Hongyu Chen, Seraphina Goldfarb-Tarrant

**Updated**: 2025-07-09T13:09:13Z

**Summary**: Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.

**Link**: [arxiv](http://arxiv.org/abs/2503.09347v3),  [pdf](http://arxiv.org/pdf/2503.09347v3)

**Tags**: cs.CL cs.AI 



### Bayesian Multi-Scale Neural Network for Crowd Counting
**Authors**: Abhinav Sagar

**Updated**: 2025-07-09T13:07:00Z

**Summary**: Crowd counting is a challenging yet critical task in computer vision with applications ranging from public safety to urban planning. Recent advances using Convolutional Neural Networks (CNNs) that estimate density maps have shown significant success. However, accurately counting individuals in highly congested scenes remains an open problem due to severe occlusions, scale variations, and perspective distortions, where people appear at drastically different sizes across the image. In this work, we propose a novel deep learning architecture that effectively addresses these challenges. Our network integrates a ResNet-based feature extractor for capturing rich hierarchical representations, followed by a downsampling block employing dilated convolutions to preserve spatial resolution while expanding the receptive field. An upsampling block using transposed convolutions reconstructs the high-resolution density map. Central to our architecture is a novel Perspective-aware Aggregation Module (PAM) designed to enhance robustness to scale and perspective variations by adaptively aggregating multi-scale contextual information. We detail the training procedure, including the loss functions and optimization strategies used. Our method is evaluated on three widely used benchmark datasets using Mean Absolute Error (MAE) and Mean Squared Error (MSE) as evaluation metrics. Experimental results demonstrate that our model achieves superior performance compared to existing state-of-the-art methods. Additionally, we incorporate principled Bayesian inference techniques to provide uncertainty estimates along with the crowd count predictions, offering a measure of confidence in the model's outputs.

**Link**: [arxiv](http://arxiv.org/abs/2007.14245v4),  [pdf](http://arxiv.org/pdf/2007.14245v4)

**Tags**: cs.CV cs.LG stat.ML 



### Text to model via SysML: Automated generation of dynamical system   computational models from unstructured natural language text via enhanced   System Modeling Language diagrams
**Authors**: Matthew Anderson Hendricks, Alice Cicirello

**Updated**: 2025-07-09T12:44:49Z

**Summary**: This paper contributes to speeding up the design and deployment of engineering dynamical systems by proposing a strategy for exploiting domain and expert knowledge for the automated generation of dynamical system computational model starting from a corpus of document relevant to the dynamical system of interest and an input document describing the specific system. This strategy is implemented in five steps and, crucially, it uses system modeling language diagrams (SysML) to extract accurate information about the dependencies, attributes, and operations of components. Natural Language Processing (NLP) strategies and Large Language Models (LLMs) are employed in specific tasks to improve intermediate outputs of the SySML diagrams automated generation, such as: list of key nouns; list of extracted relationships; list of key phrases and key relationships; block attribute values; block relationships; and BDD diagram generation. The applicability of automated SysML diagram generation is illustrated with different case studies. The computational models of complex dynamical systems from SysML diagrams are then obtained via code generation and computational model generation steps. In the code generation step, NLP strategies are used for summarization, while LLMs are used for validation only. The proposed approach is not limited to a specific system, domain, or computational software. The applicability of the proposed approach is shown via an end-to-end example from text to model of a simple pendulum, showing improved performance compared to results yielded by LLMs only.

**Link**: [arxiv](http://arxiv.org/abs/2507.06803v1),  [pdf](http://arxiv.org/pdf/2507.06803v1)

**Tags**: cs.CL cs.AI cs.CE 



### Neural Networks for Tamed Milstein Approximation of SDEs with Additive   Symmetric Jump Noise Driven by a Poisson Random Measure
**Authors**: Jose-Hermenegildo Ramirez-Gonzalez, Ying Sun

**Updated**: 2025-07-09T12:33:51Z

**Summary**: This work aims to estimate the drift and diffusion functions in stochastic differential equations (SDEs) driven by a particular class of L\'evy processes with finite jump intensity, using neural networks. We propose a framework that integrates the Tamed-Milstein scheme with neural networks employed as non-parametric function approximators. Estimation is carried out in a non-parametric fashion for the drift function $f: \mathbb{Z} \to \mathbb{R}$, the diffusion coefficient $g: \mathbb{Z} \to \mathbb{R}$. The model of interest is given by \[ dX(t) = \xi + f(X(t))\, dt + g(X(t))\, dW_t + \gamma \int_{\mathbb{Z}} z\, N(dt,dz), \] where $W_t$ is a standard Brownian motion, and $N(dt,dz)$ is a Poisson random measure on $(\mathbb{R}_{+} \times \mathbb{Z}$, $\mathcal{B} (\mathbb{R}_{+}) \otimes \mathcal{Z}$, $\lambda( \Lambda \otimes v))$, with $\lambda, \gamma > 0$, $\Lambda$ being the Lebesgue measure on $\mathbb{R}_{+}$, and $v$ a finite measure on the measurable space $(\mathbb{Z}, \mathcal{Z})$. Neural networks are used as non-parametric function approximators, enabling the modeling of complex nonlinear dynamics without assuming restrictive functional forms. The proposed methodology constitutes a flexible alternative for inference in systems with state-dependent noise and discontinuities driven by L\'evy processes.

**Link**: [arxiv](http://arxiv.org/abs/2507.04417v2),  [pdf](http://arxiv.org/pdf/2507.04417v2)

**Tags**: stat.ML cs.LG 60H10, 68T07 I.2.6; G.3 



### ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual   Pretraining
**Authors**: Seonwu Kim, Yohan Na, Kihun Kim, Hanhee Cho, Geun Lim, Mintae Kim, Seongik Park, Ki Hyun Kim, Youngsub Han, Byoung-Ki Jeon

**Updated**: 2025-07-10T07:05:41Z

**Summary**: The emergence of open-source large language models (LLMs) has expanded opportunities for enterprise applications; however, many organizations still lack the infrastructure to deploy and maintain large-scale models. As a result, small LLMs (sLLMs) have become a practical alternative, despite their inherent performance limitations. While Domain Adaptive Continual Pretraining (DACP) has been previously explored as a method for domain adaptation, its utility in commercial applications remains under-examined. In this study, we validate the effectiveness of applying a DACP-based recipe across diverse foundation models and service domains. Through extensive experiments and real-world evaluations, we demonstrate that DACP-applied sLLMs achieve substantial gains in target domain performance while preserving general capabilities, offering a cost-efficient and scalable solution for enterprise-level deployment.

**Link**: [arxiv](http://arxiv.org/abs/2507.06795v2),  [pdf](http://arxiv.org/pdf/2507.06795v2)

**Tags**: cs.CL cs.AI cs.LG 



### GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of   In-the-wild LLM Jailbreak Methods
**Authors**: Ruixuan Huang, Xunguang Wang, Zongjie Li, Daoyuan Wu, Shuai Wang

**Updated**: 2025-07-09T12:13:12Z

**Summary**: Despite the growing interest in jailbreak methods as an effective red-teaming tool for building safe and responsible large language models (LLMs), flawed evaluation system designs have led to significant discrepancies in their effectiveness assessments. We conduct a systematic measurement study based on 37 jailbreak studies since 2022, focusing on both the methods and the evaluation systems they employ. We find that existing evaluation systems lack case-specific criteria, resulting in misleading conclusions about their effectiveness and safety implications. This paper advocates a shift to a more nuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel benchmark comprising a curated harmful question dataset, detailed case-by-case evaluation guidelines and an evaluation system integrated with these guidelines -- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate measurements of jailbreak performance, enabling meaningful comparisons across methods and uncovering new insights overlooked in previous evaluations. GuidedEval reduces inter-evaluator variance by at least 76.03\%. Furthermore, we observe that incorporating guidelines can enhance the effectiveness of jailbreak methods themselves, offering new insights into both attack strategies and evaluation paradigms.

**Link**: [arxiv](http://arxiv.org/abs/2502.16903v2),  [pdf](http://arxiv.org/pdf/2502.16903v2)

**Tags**: cs.CL cs.CR 



### Learning safe, constrained policies via imitation learning: Connection   to Probabilistic Inference and a Naive Algorithm
**Authors**: George Papadopoulos, George A. Vouros

**Updated**: 2025-07-09T12:11:27Z

**Summary**: This article introduces an imitation learning method for learning maximum entropy policies that comply with constraints demonstrated by expert trajectories executing a task. The formulation of the method takes advantage of results connecting performance to bounds for the KL-divergence between demonstrated and learned policies, and its objective is rigorously justified through a connection to a probabilistic inference framework for reinforcement learning, incorporating the reinforcement learning objective and the objective to abide by constraints in an entropy maximization setting. The proposed algorithm optimizes the learning objective with dual gradient descent, supporting effective and stable training. Experiments show that the proposed method can learn effective policy models for constraints-abiding behaviour, in settings with multiple constraints of different types, accommodating different modalities of demonstrated behaviour, and with abilities to generalize.

**Link**: [arxiv](http://arxiv.org/abs/2507.06780v1),  [pdf](http://arxiv.org/pdf/2507.06780v1)

**Tags**: cs.LG cs.MA 



### Bayesian Generalized Nonlinear Models Offer Basis Free SINDy With Model   Uncertainty
**Authors**: Aliaksandr Hubin

**Updated**: 2025-07-09T12:05:31Z

**Summary**: Sparse Identification of Nonlinear Dynamics (SINDy) has become a standard methodology for inferring governing equations of dynamical systems from observed data using statistical modeling. However, classical SINDy approaches rely on predefined libraries of candidate functions to model nonlinearities, which limits flexibility and excludes robust uncertainty quantification. This paper proposes Bayesian Generalized Nonlinear Models (BGNLMs) as a principled alternative for more flexible statistical modeling. BGNLMs employ spike-and-slab priors combined with binary inclusion indicators to automatically discover relevant nonlinearities without predefined basis functions. Moreover, BGNLMs quantify uncertainty in selected bases and final model predictions, enabling robust exploration of the model space. In this paper, the BGNLM framework is applied to several three-dimensional (3D) SINDy problems.

**Link**: [arxiv](http://arxiv.org/abs/2507.06776v1),  [pdf](http://arxiv.org/pdf/2507.06776v1)

**Tags**: stat.ME math.DS stat.CO 62-02, 62-09, 62F07, 62F15, 62J12, 62J05, 62J99, 62M05, 05A16,
  60J22, 92D20, 90C27, 90C59 G.1.2; G.1.6; G.2.1; G.3; I.2.0; I.2.6; I.2.8; I.5.1; I.6; I.6.4 



### Eigenstructure inference for high-dimensional covariance with   generalized shrinkage inverse-Wishart prior
**Authors**: Seongmin Kim, Kwangmin Lee, Sewon Park, Jaeyong Lee

**Updated**: 2025-07-09T12:04:44Z

**Summary**: In multivariate statistics, estimating the covariance matrix is essential for understanding the interdependence among variables. In high-dimensional settings, where the number of covariates increases with the sample size, it is well known that the eigenstructure of the sample covariance matrix is inconsistent. The inverse-Wishart prior, a standard choice for covariance estimation in Bayesian inference, also suffers from posterior inconsistency. To address the issue of eigenvalue dispersion in high-dimensional settings, the shrinkage inverse-Wishart (SIW) prior has recently been proposed. Despite its conceptual appeal and empirical success, the asymptotic justification for the SIW prior has remained limited. In this paper, we propose a generalized shrinkage inverse-Wishart (gSIW) prior for high-dimensional covariance modeling. By extending the SIW framework, the gSIW prior accommodates a broader class of prior distributions and facilitates the derivation of theoretical properties under specific parameter choices. In particular, under the spiked covariance assumption, we establish the asymptotic behavior of the posterior distribution for both eigenvalues and eigenvectors by directly evaluating the posterior expectations for two sets of parameter choices. This direct evaluation provides insights into the large-sample behavior of the posterior that cannot be obtained through general posterior asymptotic theorems. Finally, simulation studies illustrate that the proposed prior provides accurate estimation of the eigenstructure, particularly for spiked eigenvalues, achieving narrower credible intervals and higher coverage probabilities compared to existing methods. For spiked eigenvectors, the performance is generally comparable to that of competing approaches, including the sample covariance.

**Link**: [arxiv](http://arxiv.org/abs/2505.20668v2),  [pdf](http://arxiv.org/pdf/2505.20668v2)

**Tags**: math.ST stat.ME stat.TH 62F12, 62H12 (Primary) 62F15, 60B20 (Secondary) 



### Checklist Engineering Empowers Multilingual LLM Judges
**Authors**: Mohammad Ghiasvand Mohammadkhani, Hamid Beigy

**Updated**: 2025-07-09T12:03:06Z

**Summary**: Automated text evaluation has long been a central issue in Natural Language Processing (NLP). Recently, the field has shifted toward using Large Language Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While promising and easily adaptable across tasks, this approach has seen limited exploration in multilingual contexts. Existing multilingual studies often rely on proprietary models or require extensive training data for fine-tuning, raising concerns about cost, time, and efficiency. In this paper, we propose Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free framework that uses checklist intuition for multilingual evaluation with an open-source model. Experiments across multiple languages and three benchmark datasets, under both pointwise and pairwise settings, show that our method generally surpasses the baselines and performs on par with the GPT-4o model.

**Link**: [arxiv](http://arxiv.org/abs/2507.06774v1),  [pdf](http://arxiv.org/pdf/2507.06774v1)

**Tags**: cs.CL 



### Tail-aware Adversarial Attacks: A Distributional Approach to Efficient   LLM Jailbreaking
**Authors**: Tim Beyer, Yan Scholten, Leo Schwinn, Stephan Günnemann

**Updated**: 2025-07-09T11:52:25Z

**Summary**: To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point, greedy generations, overlooking the inherently stochastic nature of LLMs. In this paper, we propose a novel framework for adversarial robustness evaluation that explicitly models the entire output distribution, including tail-risks, providing better estimates for model robustness at scale. By casting the attack process as a resource allocation problem between optimization and sampling, we determine compute-optimal tradeoffs and show that integrating sampling into existing attacks boosts ASR by up to 48% and improves efficiency by up to two orders of magnitude. Our framework also enables us to analyze how different attack algorithms affect output harm distributions. Surprisingly, we find that most optimization strategies have little effect on output harmfulness. Finally, we introduce a data-free proof-of-concept objective based on entropy-maximization to demonstrate how our tail-aware perspective enables new optimization targets. Overall, our findings highlight the importance of tail-aware attacks and evaluation protocols to accurately assess and strengthen LLM safety.

**Link**: [arxiv](http://arxiv.org/abs/2507.04446v2),  [pdf](http://arxiv.org/pdf/2507.04446v2)

**Tags**: cs.LG 



### Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation
**Authors**: Nathalia Barbosa, Paulo Borba, Léuson Da Silva

**Updated**: 2025-07-09T11:38:53Z

**Summary**: Semantic conflicts arise when a developer introduces changes to a codebase that unintentionally affect the behavior of changes integrated in parallel by other developers. Traditional merge tools are unable to detect such conflicts, so complementary tools like SMAT have been proposed. SMAT relies on generating and executing unit tests: if a test fails on the base version, passes on a developer's modified version, but fails again after merging with another developer's changes, a semantic conflict is indicated. While SMAT is effective at detecting conflicts, it suffers from a high rate of false negatives, partly due to the limitations of unit test generation tools such as Randoop and Evosuite. To investigate whether large language models (LLMs) can overcome these limitations, we propose and integrate a new test generation tool based on Code Llama 70B into SMAT. We explore the model's ability to generate tests using different interaction strategies, prompt contents, and parameter configurations. Our evaluation uses two samples: a benchmark with simpler systems from related work, and a more significant sample based on complex, real-world systems. We assess the effectiveness of the new SMAT extension in detecting conflicts. Results indicate that, although LLM-based test generation remains challenging and computationally expensive in complex scenarios, there is promising potential for improving semantic conflict detection.   --   Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso ferramentas complementares como o SMAT foram propostas. O SMAT depende da gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros. Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais simples, usados em trabalhos relacionados, e uma amostra mais significativa baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e custosa computacionalmente, h\'a potencial promissor para aprimorar a detec\c{c}~ao de conflitos sem^anticos.

**Link**: [arxiv](http://arxiv.org/abs/2507.06762v1),  [pdf](http://arxiv.org/pdf/2507.06762v1)

**Tags**: cs.SE K.6.3 



### Reinforcement Learning-based Feature Generation Algorithm for Scientific   Data
**Authors**: Meng Xiao, Junfeng Zhou, Yuanchun Zhou

**Updated**: 2025-07-09T11:30:58Z

**Summary**: Feature generation (FG) aims to enhance the prediction potential of original data by constructing high-order feature combinations and removing redundant features. It is a key preprocessing step for tabular scientific data to improve downstream machine-learning model performance. Traditional methods face the following two challenges when dealing with the feature generation of scientific data: First, the effective construction of high-order feature combinations in scientific data necessitates profound and extensive domain-specific expertise. Secondly, as the order of feature combinations increases, the search space expands exponentially, imposing prohibitive human labor consumption. Advancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have opened novel avenues for automating feature generation processes. Inspired by that, this paper revisits the conventional feature generation workflow and proposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in the iterative exploration stage, multi-agents will construct mathematical transformation equations collaboratively, synthesize and identify feature combinations ex-hibiting high information content, and leverage a reinforcement learning mechanism to evolve their strategies. Upon completing the exploration phase, MAFG integrates the large language models (LLMs) to interpreta-tively evaluate the generated features of each significant model performance breakthrough. Experimental results and case studies consistently demonstrate that the MAFG framework effectively automates the feature generation process and significantly enhances various downstream scientific data mining tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.03498v2),  [pdf](http://arxiv.org/pdf/2507.03498v2)

**Tags**: cs.LG cs.AI 



### Direct Flow Simulations with Implicit Neural Representation of Complex   Geometry
**Authors**: Samundra Karki, Mehdi Shadkah, Cheng-Hau Yang, Aditya Balu, Guglielmo Scovazzi, Adarsh Krishnamurthy, Baskar Ganapathysubramanian

**Updated**: 2025-07-09T11:28:46Z

**Summary**: Implicit neural representations have emerged as a powerful approach for encoding complex geometries as continuous functions. These implicit models are widely used in computer vision and 3D content creation, but their integration into scientific computing workflows, such as finite element or finite volume simulations, remains limited. One reason is that conventional simulation pipelines require explicit geometric inputs (meshes), forcing INR-based shapes to be converted to meshes--a step that introduces approximation errors, computational overhead, and significant manual effort. Immersed boundary methods partially alleviate this issue by allowing simulations on background grids without body-fitted meshes. However, they still require an explicit boundary description and can suffer from numerical artifacts, such as sliver cut cells. The shifted boundary method (SBM) eliminates the need for explicit geometry by using grid-aligned surrogate boundaries, making it inherently compatible with implicit shape representations. Here, we present a framework that directly couples neural implicit geometries with SBM to perform high-fidelity fluid flow simulations without any intermediate mesh generation. By leveraging neural network inference, our approach computes the surrogate boundary and distance vectors required by SBM on-the-fly directly from the INR, thus completely bypassing traditional geometry processing. We demonstrate this approach on canonical 2D and 3D flow benchmarks (lid-driven cavity flows) and complex geometries (gyroids, the Stanford bunny, and AI-generated shapes), achieving simulation accuracy comparable to conventional mesh-based methods. This work highlights a novel pathway for integrating AI-driven geometric representations into computational physics, establishing INRs as a versatile and scalable tool for simulations and removing a long-standing bottleneck in geometry handling.

**Link**: [arxiv](http://arxiv.org/abs/2503.08724v2),  [pdf](http://arxiv.org/pdf/2503.08724v2)

**Tags**: cs.GR physics.flu-dyn 



### AI Agent Smart Contract Exploit Generation
**Authors**: Arthur Gervais, Liyi Zhou

**Updated**: 2025-07-09T11:25:39Z

**Summary**: We present A1, an agentic execution driven system that transforms any LLM into an end-to-end exploit generator. A1 has no hand-crafted heuristics and provides the agent with six domain-specific tools that enable autonomous vulnerability discovery. The agent can flexibly leverage these tools to understand smart contract behavior, generate exploit strategies, test them on blockchain states, and refine approaches based on execution feedback. All outputs are concretely validated to eliminate false positives.   The evaluation across 36 real-world vulnerable contracts on Ethereum and Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional vulnerable contracts, with 5 cases occurring after the strongest model's training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59 million USD per case and 9.33 million USD total. Through 432 experiments across six LLMs, we analyze iteration-wise performance showing diminishing returns with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations 2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo analysis of 19 historical attacks shows success probabilities of 85.9%-88.8% without detection delays.   We investigate whether an attacker or a defender benefits most from deploying A1 as a continuous on-chain scanning system. Our model shows that OpenAI's o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100% vulnerability incidence rates, while faster models require >=1.000% rates to break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability rates, attackers achieve an on-chain scanning profitability at a \$6000 exploit value, while defenders require \$60000, raising fundamental questions about whether AI agents inevitably favor exploitation over defense.

**Link**: [arxiv](http://arxiv.org/abs/2507.05558v2),  [pdf](http://arxiv.org/pdf/2507.05558v2)

**Tags**: cs.CR cs.AI 



### LOVON: Legged Open-Vocabulary Object Navigator
**Authors**: Daojie Peng, Jiahang Cao, Qiang Zhang, Jun Ma

**Updated**: 2025-07-09T11:02:46Z

**Summary**: Object navigation in open-world environments remains a formidable and pervasive challenge for robotic systems, particularly when it comes to executing long-horizon tasks that require both open-world object detection and high-level task planning. Traditional methods often struggle to integrate these components effectively, and this limits their capability to deal with complex, long-range navigation missions. In this paper, we propose LOVON, a novel framework that integrates large language models (LLMs) for hierarchical task planning with open-vocabulary visual detection models, tailored for effective long-range object navigation in dynamic, unstructured environments. To tackle real-world challenges including visual jittering, blind zones, and temporary target loss, we design dedicated solutions such as Laplacian Variance Filtering for visual stabilization. We also develop a functional execution logic for the robot that guarantees LOVON's capabilities in autonomous navigation, task adaptation, and robust task completion. Extensive evaluations demonstrate the successful completion of long-sequence tasks involving real-time detection, search, and navigation toward open-vocabulary dynamic targets. Furthermore, real-world experiments across different legged robots (Unitree Go2, B2, and H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.

**Link**: [arxiv](http://arxiv.org/abs/2507.06747v1),  [pdf](http://arxiv.org/pdf/2507.06747v1)

**Tags**: cs.RO cs.CV 



### Knockout LLM Assessment: Using Large Language Models for Evaluations   through Iterative Pairwise Comparisons
**Authors**: Isik Baran Sandan, Tu Anh Dinh, Jan Niehues

**Updated**: 2025-07-09T10:58:38Z

**Summary**: Large Language Models (LLMs) have shown to be effective evaluators across various domains such as machine translations or the scientific domain. Current LLM-as-a-Judge approaches rely mostly on individual assessments or a single round of pairwise assessments, preventing the judge LLM from developing a global ranking perspective. To address this, we present Knockout Assessment, an LLM-asa Judge method using a knockout tournament system with iterative pairwise comparisons. Experiments across three LLMs on two datasets show that knockout assessment improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations, aligning LLM assessments more closely with human scoring.

**Link**: [arxiv](http://arxiv.org/abs/2506.03785v3),  [pdf](http://arxiv.org/pdf/2506.03785v3)

**Tags**: cs.CL cs.AI I.2.7 



### PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI
**Authors**: Haitham S. Al-Sinani, Chris J. Mitchell

**Updated**: 2025-07-09T10:56:32Z

**Summary**: Ethical hacking today relies on highly skilled practitioners executing complex sequences of commands, which is inherently time-consuming, difficult to scale, and prone to human error. To help mitigate these limitations, we previously introduced 'PenTest++', an AI-augmented system combining automation with generative AI supporting ethical hacking workflows. However, a key limitation of PenTest++ was its lack of support for privilege escalation, a crucial element of ethical hacking. In this paper we present 'PenTest2.0', a substantial evolution of PenTest++ supporting automated privilege escalation driven entirely by Large Language Model reasoning. It also incorporates several significant enhancements: 'Retrieval-Augmented Generation', including both one-line and offline modes; 'Chain-of-Thought' prompting for intermediate reasoning; persistent 'PenTest Task Trees' to track goal progression across turns; and the optional integration of human-authored hints. We describe how it operates, present a proof-of-concept prototype, and discuss its benefits and limitations. We also describe application of the system to a controlled Linux target, showing it can carry out multi-turn, adaptive privilege escalation. We explain the rationale behind its core design choices, and provide comprehensive testing results and cost analysis. Our findings indicate that 'PenTest2.0' represents a meaningful step toward practical, scalable, AI-automated penetration testing, whilst highlighting the shortcomings of generative AI systems, particularly their sensitivity to prompt structure, execution context, and semantic drift, reinforcing the need for further research and refinement in this emerging space.   Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM (Large Language Model), HITL (Human-in-the-Loop)

**Link**: [arxiv](http://arxiv.org/abs/2507.06742v1),  [pdf](http://arxiv.org/pdf/2507.06742v1)

**Tags**: cs.CR 



### LLM-based User Profile Management for Recommender System
**Authors**: Seunghwan Bang, Hwanjun Song

**Updated**: 2025-07-09T10:55:06Z

**Summary**: The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.

**Link**: [arxiv](http://arxiv.org/abs/2502.14541v2),  [pdf](http://arxiv.org/pdf/2502.14541v2)

**Tags**: cs.CL 



### PromptTea: Let Prompts Tell TeaCache the Optimal Threshold
**Authors**: Zishen Huang, Chunyu Yang, Mengyuan Ren

**Updated**: 2025-07-09T10:53:05Z

**Summary**: Despite recent progress in video generation, inference speed remains a major bottleneck. A common acceleration strategy involves reusing model outputs via caching mechanisms at fixed intervals. However, we find that such fixed-frequency reuse significantly degrades quality in complex scenes, while manually tuning reuse thresholds is inefficient and lacks robustness. To address this, we propose Prompt-Complexity-Aware (PCA) caching, a method that automatically adjusts reuse thresholds based on scene complexity estimated directly from the input prompt. By incorporating prompt-derived semantic cues, PCA enables more adaptive and informed reuse decisions than conventional caching methods. We also revisit the assumptions behind TeaCache and identify a key limitation: it suffers from poor input-output relationship modeling due to an oversimplified prior. To overcome this, we decouple the noisy input, enhance the contribution of meaningful textual information, and improve the model's predictive accuracy through multivariate polynomial feature expansion. To further reduce computational cost, we replace the static CFGCache with DynCFGCache, a dynamic mechanism that selectively reuses classifier-free guidance (CFG) outputs based on estimated output variations. This allows for more flexible reuse without compromising output quality. Extensive experiments demonstrate that our approach achieves significant acceleration-for example, 2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across a range of scenes.

**Link**: [arxiv](http://arxiv.org/abs/2507.06739v1),  [pdf](http://arxiv.org/pdf/2507.06739v1)

**Tags**: cs.CV 



### Hierarchical Feature Alignment for Gloss-Free Sign Language Translation
**Authors**: Sobhan Asasi, Mohamed Ilyes Lakhal, Richard Bowden

**Updated**: 2025-07-09T10:45:50Z

**Summary**: Sign Language Translation (SLT) attempts to convert sign language videos into spoken sentences. However, many existing methods struggle with the disparity between visual and textual representations during end-to-end learning. Gloss-based approaches help to bridge this gap by leveraging structured linguistic information. While, gloss-free methods offer greater flexibility and remove the burden of annotation, they require effective alignment strategies. Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by generating text-like representations from sign videos. In this work, we introduce a novel hierarchical pre-training strategy inspired by the structure of sign language, incorporating pseudo-glosses and contrastive video-language alignment. Our method hierarchically extracts features at frame, segment, and video levels, aligning them with pseudo-glosses and the spoken sentence to enhance translation quality. Experiments demonstrate that our approach improves BLEU-4 and ROUGE scores while maintaining efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2507.06732v1),  [pdf](http://arxiv.org/pdf/2507.06732v1)

**Tags**: cs.CV 



### Inferring Quantum Network Topologies using Genetic Optimisation of   Indirect Measurements
**Authors**: Conall J. Campbell, Matthew Mackinnon, Mauro Paternostro, Diana A. Chisholm

**Updated**: 2025-07-09T10:32:00Z

**Summary**: The characterisation of quantum networks is fundamental to understanding how energy and information propagates through complex systems, with applications in control, communication, error mitigation and energy transfer. In this work, we explore the use of external probes to infer the network topology in the context of continuous-time quantum walks, where a single excitation traverses the network with a pattern strongly influenced by its topology. The probes act as decay channels for the excitation, and can be interpreted as performing an indirect measurement on the network dynamics. By making use of a Genetic Optimisation algorithm, we demonstrate that the data collected by the probes can be used to successfully reconstruct the topology of any quantum network with high success rates, where performance is limited only by computational resources for large network sizes. Moreover, we show that increasing the number of probes significantly simplifies the reconstruction task, revealing a tradeoff between the number of probes and the required computational power.

**Link**: [arxiv](http://arxiv.org/abs/2506.11289v2),  [pdf](http://arxiv.org/pdf/2506.11289v2)

**Tags**: quant-ph 



### On the Effect of Uncertainty on Layer-wise Inference Dynamics
**Authors**: Sunwoo Kim, Haneul Yoo, Alice Oh

**Updated**: 2025-07-09T10:30:09Z

**Summary**: Understanding how large language models (LLMs) internally represent and process their predictions is central to detecting uncertainty and preventing hallucinations. While several studies have shown that models encode uncertainty in their hidden states, it is underexplored how this affects the way they process such hidden states. In this work, we demonstrate that the dynamics of output token probabilities across layers for certain and uncertain outputs are largely aligned, revealing that uncertainty does not seem to affect inference dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to analyze the layer-wise probability trajectories of final prediction tokens across 11 datasets and 5 models. Using incorrect predictions as those with higher epistemic uncertainty, our results show aligned trajectories for certain and uncertain predictions that both observe abrupt increases in confidence at similar layers. We balance this finding by showing evidence that more competent models may learn to process uncertainty differently. Our findings challenge the feasibility of leveraging simplistic methods for detecting uncertainty at inference. More broadly, our work demonstrates how interpretability methods may be used to investigate the way uncertainty affects inference.

**Link**: [arxiv](http://arxiv.org/abs/2507.06722v1),  [pdf](http://arxiv.org/pdf/2507.06722v1)

**Tags**: cs.CL cs.LG 



### A Neural Representation Framework with LLM-Driven Spatial Reasoning for   Open-Vocabulary 3D Visual Grounding
**Authors**: Zhenyang Liu, Sixiao Zheng, Siyu Chen, Cairong Zhao, Longfei Liang, Xiangyang Xue, Yanwei Fu

**Updated**: 2025-07-09T10:20:38Z

**Summary**: Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose SpatialReasoner, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.

**Link**: [arxiv](http://arxiv.org/abs/2507.06719v1),  [pdf](http://arxiv.org/pdf/2507.06719v1)

**Tags**: cs.CV cs.RO 



### From Blurry to Brilliant Detection: YOLO-Based Aerial Object Detection   with Super Resolution
**Authors**: Ragib Amin Nihal, Benjamin Yen, Takeshi Ashizawa, Katsutoshi Itoyama, Kazuhiro Nakadai

**Updated**: 2025-07-09T10:14:26Z

**Summary**: Aerial object detection presents challenges from small object sizes, high density clustering, and image quality degradation from distance and motion blur. These factors create an information bottleneck where limited pixel representation cannot encode sufficient discriminative features. B2BDet addresses this with a two-stage framework that applies domain-specific super-resolution during inference, followed by detection using an enhanced YOLOv5 architecture. Unlike training-time super-resolution approaches that enhance learned representations, our method recovers visual information from each input image. The approach combines aerial-optimized SRGAN fine-tuning with architectural innovations including an Efficient Attention Module (EAM) and Cross-Layer Feature Pyramid Network (CLFPN). Evaluation across four aerial datasets shows performance gains, with VisDrone achieving 52.5% mAP using only 27.7M parameters. Ablation studies show that super-resolution preprocessing contributes +2.6% mAP improvement while architectural enhancements add +2.9%, yielding +5.5% total improvement over baseline YOLOv5. The method achieves computational efficiency with 53.8% parameter reduction compared to recent approaches while achieving strong small object detection performance.

**Link**: [arxiv](http://arxiv.org/abs/2401.14661v2),  [pdf](http://arxiv.org/pdf/2401.14661v2)

**Tags**: cs.CV cs.LG 



### CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and   Context Aware Text Generation with LLMs
**Authors**: Garapati Keerthana, Manik Gupta

**Updated**: 2025-07-09T10:13:38Z

**Summary**: Large language models (LLMs), including zero-shot and few-shot paradigms, have shown promising capabilities in clinical text generation. However, real-world applications face two key challenges: (1) patient data is highly unstructured, heterogeneous, and scattered across multiple note types and (2) clinical notes are often long and semantically dense, making naive prompting infeasible due to context length constraints and the risk of omitting clinically relevant information.   We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a domain-specific framework for structured and clinically grounded text generation using LLMs. It incorporates a novel hierarchical chunking strategy that respects clinical document structure and introduces a task-specific dual-stage retrieval mechanism. The global stage identifies relevant note types using evidence-based queries, while the local stage extracts high-value content within those notes creating relevance at both document and section levels.   We apply the system to generate structured progress notes for individual hospital visits using 15 clinical note types from the MIMIC-III dataset. Experiments show that it preserves temporal and semantic alignment across visits, achieving an average alignment score of 87.7%, surpassing the 80.7% baseline from real clinician-authored notes. The generated outputs also demonstrate high consistency across LLMs, reinforcing deterministic behavior essential for reproducibility, reliability, and clinical trust.

**Link**: [arxiv](http://arxiv.org/abs/2507.06715v1),  [pdf](http://arxiv.org/pdf/2507.06715v1)

**Tags**: cs.CL cs.AI cs.IR 



### Do Larger Language Models Imply Better Generalization? A Pretraining   Scaling Law for Implicit Reasoning
**Authors**: Xinyi Wang, Shawn Tan, Mingyu Jin, William Yang Wang, Rameswar Panda, Yikang Shen

**Updated**: 2025-07-09T10:08:34Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. In this paper, we introduce a synthetic multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. Our reasoning task involves completing missing edges in the graph, which requires advanced multi-hop reasoning and mimics real-world reasoning scenarios. To evaluate this, we pretrain language models (LMs) from scratch solely on triples from the incomplete graph and assess their ability to infer the missing edges. Interestingly, we observe that overparameterization can impair reasoning performance due to excessive memorization. We investigate different factors that affect this U-shaped loss curve, including graph structure, model size, and training steps. To predict the optimal model size for a specific knowledge graph, we find an empirical scaling that linearly maps the knowledge graph search entropy to the optimal model size. This work provides new insights into the relationship between scaling and reasoning in LLMs, shedding light on possible ways to optimize their performance for reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.03635v2),  [pdf](http://arxiv.org/pdf/2504.03635v2)

**Tags**: cs.AI cs.CL 



### One Size Does Not Fit All: Investigating Efficacy of Perplexity in   Detecting LLM-Generated Code
**Authors**: Jinwei Xu, He Zhang, Yanjing Yang, Lanxin Yang, Zeru Cheng, Jun Lyu, Bohan Liu, Xin Zhou, Alberto Bacchelli, Yin Kia Chiam, Thiam Kian Chiew

**Updated**: 2025-07-09T10:04:06Z

**Summary**: Large language model-generated code (LLMgCode) has become increasingly common in software development. So far LLMgCode has more quality issues than human-authored code (HaCode). It is common for LLMgCode to mix with HaCode in a code change, while the change is signed by only human developers, without being carefully examined. Many automated methods have been proposed to detect LLMgCode from HaCode, in which the perplexity-based method (PERPLEXITY for short) is the state-of-the-art method. However, the efficacy evaluation of PERPLEXITY has focused on detection accuracy. Yet it is unclear whether PERPLEXITY is good enough in a wider range of realistic evaluation settings. To this end, we carry out a family of experiments to compare PERPLEXITY against feature- and pre-training-based methods from three perspectives: detection accuracy, detection speed, and generalization capability. The experimental results show that PERPLEXITY has the best generalization capability while having limited detection accuracy and detection speed. Based on that, we discuss the strengths and limitations of PERPLEXITY, e.g., PERPLEXITY is unsuitable for high-level programming languages. Finally, we provide recommendations to improve PERPLEXITY and apply it in practice. As the first large-scale investigation on detecting LLMgCode from HaCode, this article provides a wide range of findings for future improvement.

**Link**: [arxiv](http://arxiv.org/abs/2412.16525v2),  [pdf](http://arxiv.org/pdf/2412.16525v2)

**Tags**: cs.SE 



### Causal Inference Isn't Special: Why It's Just Another Prediction Problem
**Authors**: Carlos Fernández-Loría

**Updated**: 2025-07-09T10:00:04Z

**Summary**: Causal inference is often portrayed as fundamentally distinct from predictive modeling, with its own terminology, goals, and intellectual challenges. But at its core, causal inference is simply a structured instance of prediction under distribution shift. In both cases, we begin with labeled data from a source domain and seek to generalize to a target domain where outcomes are not observed. The key difference is that in causal inference, the labels -- potential outcomes -- are selectively observed based on treatment assignment, introducing bias that must be addressed through assumptions. This perspective reframes causal estimation as a familiar generalization problem and highlights how techniques from predictive modeling, such as reweighting and domain adaptation, apply directly to causal tasks. It also clarifies that causal assumptions are not uniquely strong -- they are simply more explicit. By viewing causal inference through the lens of prediction, we demystify its logic, connect it to familiar tools, and make it more accessible to practitioners and educators alike.

**Link**: [arxiv](http://arxiv.org/abs/2504.04320v3),  [pdf](http://arxiv.org/pdf/2504.04320v3)

**Tags**: cs.LG stat.ME stat.ML 



### QUITE: A Query Rewrite System Beyond Rules with LLM Agents
**Authors**: Yuyang Song, Hanxu Yan, Jiale Lao, Yibo Wang, Yufei Li, Yuanchun Zhou, Jianguo Wang, Mingjie Tang

**Updated**: 2025-07-09T09:51:35Z

**Summary**: Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.

**Link**: [arxiv](http://arxiv.org/abs/2506.07675v2),  [pdf](http://arxiv.org/pdf/2506.07675v2)

**Tags**: cs.DB cs.AI 



### Approximating neutron-star radii using gravitational-wave only   measurements with symbolic regression
**Authors**: Michał Bejger

**Updated**: 2025-07-09T09:30:24Z

**Summary**: Gravitational waves emitted by binary neutron-star inspirals carry information on components' masses and tidal deformabilities, but not directly radii, which are measured by electromagnetic observations of neutron stars. To improve the multi-messenger astronomy studies of neutron stars, an expression for neutron-star radii as a function of gravitational-wave only data would be advantageous, as it would allow to compare information from two different channels. In order to do so, a symbolic regression method, pySR, is trained on TOV solutions to piecewise polytropic EOS input to discover an approximate symbolic expression for the neutron-star radius as a function of gravitational-wave measurements only. The approximation is tested on piecewise polytropic EOS NS data, as well as on NS sequences based on selected realistic (non-polytropic) dense-matter theory EOSs, achieving consistent agreement between the ground truth values and the symbolic approximation for a broad range of NS parameters covering current astrophysical observations, with average radii differences of few hundred meters. Additionally, the approximation is applied to the GW170817 gravitational-wave mass and tidal deformability posteriors, and compared to reported inferred radius distributions.

**Link**: [arxiv](http://arxiv.org/abs/2504.19962v3),  [pdf](http://arxiv.org/pdf/2504.19962v3)

**Tags**: gr-qc astro-ph.HE 



### StixelNExT++: Lightweight Monocular Scene Segmentation and   Representation for Collective Perception
**Authors**: Marcel Vosshans, Omar Ait-Aider, Youcef Mezouar, Markus Enzweiler

**Updated**: 2025-07-09T09:30:07Z

**Summary**: This paper presents StixelNExT++, a novel approach to scene representation for monocular perception systems. Building on the established Stixel representation, our method infers 3D Stixels and enhances object segmentation by clustering smaller 3D Stixel units. The approach achieves high compression of scene information while remaining adaptable to point cloud and bird's-eye-view representations. Our lightweight neural network, trained on automatically generated LiDAR-based ground truth, achieves real-time performance with computation times as low as 10 ms per frame. Experimental results on the Waymo dataset demonstrate competitive performance within a 30-meter range, highlighting the potential of StixelNExT++ for collective perception in autonomous systems.

**Link**: [arxiv](http://arxiv.org/abs/2507.06687v1),  [pdf](http://arxiv.org/pdf/2507.06687v1)

**Tags**: cs.CV cs.RO 



### Tomography for Plasma Imaging: a Unifying Framework for Bayesian   Inference
**Authors**: D. Hamm, C. Theiler, M. Simeoni, B. P. Duval, T. Debarre, L. Simons, J. R. Queralt

**Updated**: 2025-07-09T09:12:25Z

**Summary**: Plasma diagnostics often employ computerized tomography to estimate emissivity profiles from a finite, and often limited, number of line-integrated measurements. Decades of algorithmic refinement have brought considerable improvements, and led to a variety of employed solutions. These often feature an underlying, common structure that is rarely acknowledged or investigated. In this paper, we present a unifying perspective on sparse-view tomographic reconstructions for plasma imaging, highlighting how many inversion approaches reported in the literature can be naturally understood within a Bayesian framework. In this setting, statistical modelling of acquired data leads to a likelihood term, while the assumed properties of the profile to be reconstructed are encoded within a prior term. Together, these terms yield the posterior distribution, which models all the available information on the profile to be reconstructed. We show how credible reconstructions, uncertainty quantification and further statistical quantities of interest can be efficiently obtained from noisy tomographic data by means of a stochastic gradient flow algorithm targeting the posterior. This is demonstrated by application to soft x-ray imaging at the TCV tokamak. We validate the proposed imaging pipeline on a large dataset of generated model phantoms, showing how posterior-based inference can be leveraged to perform principled statistical analysis of quantities of interest. Finally, we address some of the inherent, and thus remaining, limitations of sparse-view tomography. All the computational routines used in this work are made available as open access code.

**Link**: [arxiv](http://arxiv.org/abs/2506.20232v2),  [pdf](http://arxiv.org/pdf/2506.20232v2)

**Tags**: physics.plasm-ph 



### Automating IRAC Analysis in Malaysian Contract Law using a   Semi-Structured Knowledge Base
**Authors**: Xiaoxi Kang, Lizhen Qu, Lay-Ki Soon, Zhuang Li, Adnan Trakic

**Updated**: 2025-07-09T08:47:59Z

**Summary**: The effectiveness of Large Language Models (LLMs) in legal reasoning is often limited due to the unique legal terminologies and the necessity for highly specialized knowledge. These limitations highlight the need for high-quality data tailored for complex legal reasoning tasks. This paper introduces LegalSemi, a benchmark specifically curated for legal scenario analysis. LegalSemi comprises 54 legal scenarios, each rigorously annotated by legal experts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion) framework from Malaysian Contract Law. In addition, LegalSemi is accompanied by a structured knowledge base (SKE). A series of experiments were conducted to assess the usefulness of LegalSemi for IRAC analysis. The experimental results demonstrate the effectiveness of incorporating the SKE for issue identification, rule retrieval, application and conclusion generation using four different LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2406.13217v2),  [pdf](http://arxiv.org/pdf/2406.13217v2)

**Tags**: cs.CL 



### Enhancing Diffusion Model Stability for Image Restoration via Gradient   Management
**Authors**: Hongjie Wu, Mingqin Zhang, Linchao He, Ji-Zhe Zhou, Jiancheng Lv

**Updated**: 2025-07-09T08:40:46Z

**Summary**: Diffusion models have shown remarkable promise for image restoration by leveraging powerful priors. Prominent methods typically frame the restoration problem within a Bayesian inference framework, which iteratively combines a denoising step with a likelihood guidance step. However, the interactions between these two components in the generation process remain underexplored. In this paper, we analyze the underlying gradient dynamics of these components and identify significant instabilities. Specifically, we demonstrate conflicts between the prior and likelihood gradient directions, alongside temporal fluctuations in the likelihood gradient itself. We show that these instabilities disrupt the generative process and compromise restoration performance. To address these issues, we propose Stabilized Progressive Gradient Diffusion (SPGD), a novel gradient management technique. SPGD integrates two synergistic components: (1) a progressive likelihood warm-up strategy to mitigate gradient conflicts; and (2) adaptive directional momentum (ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive experiments across diverse restoration tasks demonstrate that SPGD significantly enhances generation stability, leading to state-of-the-art performance in quantitative metrics and visually superior results. Code is available at \href{https://github.com/74587887/SPGD}{here}.

**Link**: [arxiv](http://arxiv.org/abs/2507.06656v1),  [pdf](http://arxiv.org/pdf/2507.06656v1)

**Tags**: cs.CV cs.LG 



## Keyword: LLM Deployment 
 ### Towards Multimodal Understanding via Stable Diffusion as a Task-Aware   Feature Extractor
**Authors**: Vatsal Agarwal, Matthew Gwilliam, Gefen Kohavi, Eshan Verma, Daniel Ulbricht, Abhinav Shrivastava

**Updated**: 2025-07-09T17:59:47Z

**Summary**: Recent advances in multimodal large language models (MLLMs) have enabled image-based question-answering capabilities. However, a key limitation is the use of CLIP as the visual encoder; while it can capture coarse global information, it often can miss fine-grained details that are relevant to the input query. To address these shortcomings, this work studies whether pre-trained text-to-image diffusion models can serve as instruction-aware visual encoders. Through an analysis of their internal representations, we find diffusion features are both rich in semantics and can encode strong image-text alignment. Moreover, we find that we can leverage text conditioning to focus the model on regions relevant to the input question. We then investigate how to align these features with large language models and uncover a leakage phenomenon, where the LLM can inadvertently recover information from the original diffusion prompt. We analyze the causes of this leakage and propose a mitigation strategy. Based on these insights, we explore a simple fusion strategy that utilizes both CLIP and conditional diffusion features. We evaluate our approach on both general VQA and specialized MLLM benchmarks, demonstrating the promise of diffusion models for visual understanding, particularly in vision-centric tasks that require spatial and compositional reasoning. Our project page can be found https://vatsalag99.github.io/mustafar/.

**Link**: [arxiv](http://arxiv.org/abs/2507.07106v1),  [pdf](http://arxiv.org/pdf/2507.07106v1)

**Tags**: cs.CV cs.LG 



### Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation   from Diffusion Models
**Authors**: Tiezheng Zhang, Yitong Li, Yu-cheng Chou, Jieneng Chen, Alan Yuille, Chen Wei, Junfei Xiao

**Updated**: 2025-07-09T17:59:04Z

**Summary**: Building state-of-the-art Vision-Language Models (VLMs) with strong captioning capabilities typically necessitates training on billions of high-quality image-text pairs, requiring millions of GPU hours. This paper introduces the Vision-Language-Vision (VLV) auto-encoder framework, which strategically leverages key pretrained components: a vision encoder, the decoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large Language Model (LLM). Specifically, we establish an information bottleneck by regularizing the language representation space, achieved through freezing the pretrained T2I diffusion decoder. Our VLV pipeline effectively distills knowledge from the text-conditioned diffusion model using continuous embeddings, demonstrating comprehensive semantic understanding via high-quality reconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the intermediate language representations into detailed descriptions, we construct a state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o and Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and significantly reduces data requirements; by primarily utilizing single-modal images for training and maximizing the utility of existing pretrained models (image encoder, T2I diffusion model, and LLM), it circumvents the need for massive paired image-text datasets, keeping the total training expenditure under $1,000 USD.

**Link**: [arxiv](http://arxiv.org/abs/2507.07104v1),  [pdf](http://arxiv.org/pdf/2507.07104v1)

**Tags**: cs.CV 



### Less can be more for predicting properties with large language models
**Authors**: Nawaf Alampara, Santiago Miret, Kevin Maik Jablonka

**Updated**: 2025-07-09T17:37:23Z

**Summary**: Predicting properties from coordinate-category data -- sets of vectors paired with categorical information -- is fundamental to computational science. In materials science, this challenge manifests as predicting properties like formation energies or elastic moduli from crystal structures comprising atomic positions (vectors) and element types (categorical information). While large language models (LLMs) have increasingly been applied to such tasks, with researchers encoding structural data as text, optimal strategies for achieving reliable predictions remain elusive. Here, we report fundamental limitations in LLM's ability to learn from coordinate information in coordinate-category data. Through systematic experiments using synthetic datasets with tunable coordinate and category contributions, combined with a comprehensive benchmarking framework (MatText) spanning multiple representations and model scales, we find that LLMs consistently fail to capture coordinate information while excelling at category patterns. This geometric blindness persists regardless of model size (up to 70B parameters), dataset scale (up to 2M structures), or text representation strategy. Our findings suggest immediate practical implications: for materials property prediction tasks dominated by structural effects, specialized geometric architectures consistently outperform LLMs by significant margins, as evidenced by a clear "GNN-LM wall" in performance benchmarks. Based on our analysis, we provide concrete guidelines for architecture selection in scientific machine learning, while highlighting the critical importance of understanding model inductive biases when tackling scientific prediction problems.

**Link**: [arxiv](http://arxiv.org/abs/2406.17295v3),  [pdf](http://arxiv.org/pdf/2406.17295v3)

**Tags**: cond-mat.mtrl-sci cs.LG 



### TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management   in LLM-based Agentic Multi-Agent Systems
**Authors**: Shaina Raza, Ranjan Sapkota, Manoj Karkee, Christos Emmanouilidis

**Updated**: 2025-07-09T17:33:49Z

**Summary**: Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of \textbf{Trust, Risk, and Security Management (TRiSM)} in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around four key pillars: Explainability, ModelOps, Security, Privacy and Governance, each contextualized to the challenges of multi-agent LLM systems. A novel risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI , as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, outlining critical directions to align emerging systems with TRiSM principles for safe, transparent, and accountable operation.

**Link**: [arxiv](http://arxiv.org/abs/2506.04133v3),  [pdf](http://arxiv.org/pdf/2506.04133v3)

**Tags**: cs.AI 



### Multi-Attribute Steering of Language Models via Targeted Intervention
**Authors**: Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

**Updated**: 2025-07-09T17:31:20Z

**Summary**: Inference-time intervention (ITI) has emerged as a promising method for steering large language model (LLM) behavior in a particular direction (e.g., improving helpfulness) by intervening on token representations without costly updates to the LLM's parameters. However, existing ITI approaches fail to scale to multi-attribute settings with conflicts, such as enhancing helpfulness while also reducing toxicity. To address this, we introduce Multi-Attribute Targeted Steering (MAT-Steer), a novel steering framework designed for selective token-level intervention across multiple attributes. MAT-Steer learns steering vectors using an alignment objective that shifts the model's internal representations of undesirable outputs closer to those of desirable ones while enforcing sparsity and orthogonality among vectors for different attributes, thereby reducing inter-attribute conflicts. We evaluate MAT-Steer in two distinct settings: (i) on question answering (QA) tasks where we balance attributes like truthfulness, bias, and toxicity; (ii) on generative tasks where we simultaneously improve attributes like helpfulness, correctness, and coherence. MAT-Steer outperforms existing ITI and parameter-efficient fine-tuning approaches across both task types (e.g., 3% average accuracy gain across QA tasks and 55.82% win rate against the best ITI baseline).

**Link**: [arxiv](http://arxiv.org/abs/2502.12446v2),  [pdf](http://arxiv.org/pdf/2502.12446v2)

**Tags**: cs.CL cs.AI cs.LG 



### How to Bridge the Sim-to-Real Gap in Digital Twin-Aided   Telecommunication Networks
**Authors**: Clement Ruah, Houssem Sifaou, Osvaldo Simeone, Bashir M. Al-Hashimi

**Updated**: 2025-07-09T17:27:51Z

**Summary**: Training effective artificial intelligence models for telecommunications is challenging due to the scarcity of deployment-specific data. Real data collection is expensive, and available datasets often fail to capture the unique operational conditions and contextual variability of the network environment. Digital twinning provides a potential solution to this problem, as simulators tailored to the current network deployment can generate site-specific data to augment the available training datasets. However, there is a need to develop solutions to bridge the inherent simulation-to-reality (sim-to-real) gap between synthetic and real-world data. This paper reviews recent advances on two complementary strategies: 1) the calibration of digital twins (DTs) through real-world measurements, and 2) the use of sim-to-real gap-aware training strategies to robustly handle residual discrepancies between digital twin-generated and real data. For the latter, we evaluate two conceptually distinct methods that model the sim-to-real gap either at the level of the environment via Bayesian learning or at the level of the training loss via prediction-powered inference.

**Link**: [arxiv](http://arxiv.org/abs/2507.07067v1),  [pdf](http://arxiv.org/pdf/2507.07067v1)

**Tags**: eess.SP cs.LG 



### Boosting Parameter Efficiency in LLM-Based Recommendation through   Sophisticated Pruning
**Authors**: Shanle Zheng, Keqin Bao, Jizhi Zhang, Yang Zhang, Fuli Feng, Xiangnan He

**Updated**: 2025-07-09T17:26:10Z

**Summary**: LLM-based recommender systems have made significant progress; however, the deployment cost associated with the large parameter volume of LLMs still hinders their real-world applications. This work explores parameter pruning to improve parameter efficiency while maintaining recommendation quality, thereby enabling easier deployment. Unlike existing approaches that focus primarily on inter-layer redundancy, we uncover intra-layer redundancy within components such as self-attention and MLP modules. Building on this analysis, we propose a more fine-grained pruning approach that integrates both intra-layer and layer-wise pruning. Specifically, we introduce a three-stage pruning strategy that progressively prunes parameters at different levels and parts of the model, moving from intra-layer to layer-wise pruning, or from width to depth. Each stage also includes a performance restoration step using distillation techniques, helping to strike a balance between performance and parameter efficiency. Empirical results demonstrate the effectiveness of our approach: across three datasets, our models achieve an average of 88% of the original model's performance while pruning more than 95% of the non-embedding parameters. This underscores the potential of our method to significantly reduce resource requirements without greatly compromising recommendation quality. Our code will be available at: https://github.com/zheng-sl/PruneRec

**Link**: [arxiv](http://arxiv.org/abs/2507.07064v1),  [pdf](http://arxiv.org/pdf/2507.07064v1)

**Tags**: cs.IR 



### LCFO: Long Context and Long Form Output Dataset and Benchmarking
**Authors**: Marta R. Costa-jussà, Pierre Andrews, Mariano Coria Meglioli, Joy Chen, Joe Chuang, David Dale, Christophe Ropers, Alexandre Mourachko, Eduardo Sánchez, Holger Schwenk, Tuan Tran, Arina Turkatenko, Carleigh Wood

**Updated**: 2025-07-09T17:25:55Z

**Summary**: This paper presents the Long Context and Form Output (LCFO) benchmark, a novel evaluation framework for assessing gradual summarization and summary expansion capabilities across diverse domains. LCFO consists of long input documents (5k words average length), each of which comes with three summaries of different lengths (20%, 10%, and 5% of the input text), as well as approximately 15 questions and answers (QA) related to the input content. Notably, LCFO also provides alignments between specific QA pairs and corresponding summaries in 7 domains. The primary motivation behind providing summaries of different lengths is to establish a controllable framework for generating long texts from shorter inputs, i.e. summary expansion. To establish an evaluation metric framework for summarization and summary expansion, we provide human evaluation scores for human-generated outputs, as well as results from various state-of-the-art large language models (LLMs). GPT-4o-mini achieves best human scores among automatic systems in both summarization and summary expansion tasks (~ +10% and +20%, respectively). It even surpasses human output quality in the case of short summaries (~ +7%). Overall automatic metrics achieve low correlations with human evaluation scores (~ 0.4) but moderate correlation on specific evaluation aspects such as fluency and attribution (~ 0.6).

**Link**: [arxiv](http://arxiv.org/abs/2412.08268v3),  [pdf](http://arxiv.org/pdf/2412.08268v3)

**Tags**: cs.CL I.2.7 



### LASeR: Learning to Adaptively Select Reward Models with Multi-Armed   Bandits
**Authors**: Duy Nguyen, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal

**Updated**: 2025-07-09T17:19:50Z

**Summary**: Reward Models (RMs) are crucial to aligning large language models (LLMs), but the degree to which an RM specialized to one task (e.g. writing) generalizes to new tasks (e.g. math) is often not known a priori, often making using only one fixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs simultaneously can incur a prohibitively high computational cost and lead to conflicting signals from different RMs that may degrade performance. To address these challenges, we introduce LASeR (Learning to Adaptively Select Rewards), which frames reward model selection as a multi-armed bandit problem, efficiently and iteratively training LLMs using multiple RMs by selecting the most well-suited RM for each instance. On commonsense and math reasoning tasks, we show that LASeR boosts iterative LLM training, improving the absolute average accuracy of Llama-3-8B over three datasets by 2.67% over an ensemble of RM scores while also showing superior efficiency (e.g., a 2x speedup). Moreover, on WildChat (open-ended instruction-following tasks), LASeR leads to a 72.69% AlpacaEval win rate over the RM score ensemble baseline. Extending to long-context generation, LASeR improves by 2.96 F1 points (avg.) on single-document QA tasks and 2.97 F1 points on few-shot learning over the RM score ensemble baseline with best-of-n sampling.

**Link**: [arxiv](http://arxiv.org/abs/2410.01735v2),  [pdf](http://arxiv.org/pdf/2410.01735v2)

**Tags**: cs.CL cs.LG 



### Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark   Enriched with Contextual Metadata
**Authors**: Bruce Coburn, Jiangpeng He, Megan E. Rollo, Satvinder S. Dhaliwal, Deborah A. Kerr, Fengqing Zhu

**Updated**: 2025-07-09T17:10:33Z

**Summary**: Large Multimodal Models (LMMs) are increasingly applied to meal images for nutrition analysis. However, existing work primarily evaluates proprietary models, such as GPT-4. This leaves the broad range of LLMs underexplored. Additionally, the influence of integrating contextual metadata and its interaction with various reasoning modifiers remains largely uncharted. This work investigates how interpreting contextual metadata derived from GPS coordinates (converted to location/venue type), timestamps (transformed into meal/day type), and the food items present can enhance LMM performance in estimating key nutritional values. These values include calories, macronutrients (protein, carbohydrates, fat), and portion sizes. We also introduce ACETADA, a new food-image dataset slated for public release. This open dataset provides nutrition information verified by the dietitian and serves as the foundation for our analysis. Our evaluation across eight LMMs (four open-weight and four closed-weight) first establishes the benefit of contextual metadata integration over straightforward prompting with images alone. We then demonstrate how this incorporation of contextual information enhances the efficacy of reasoning modifiers, such as Chain-of-Thought, Multimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona. Empirical results show that integrating metadata intelligently, when applied through straightforward prompting strategies, can significantly reduce the Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted nutritional values. This work highlights the potential of context-aware LMMs for improved nutrition analysis.

**Link**: [arxiv](http://arxiv.org/abs/2507.07048v1),  [pdf](http://arxiv.org/pdf/2507.07048v1)

**Tags**: cs.CV 



### 5C Prompt Contracts: A Minimalist, Creative-Friendly, Token-Efficient   Design Framework for Individual and SME LLM Usage
**Authors**: Ugur Ari

**Updated**: 2025-07-09T17:07:39Z

**Summary**: The progression from traditional prompt engineering to a more rigorous discipline of prompt design marks a pivotal shift in human-LLM interaction. As Large Language Models (LLMs) become increasingly embedded in mission-critical applications, there emerges a pressing need for frameworks that are not only explicit and systematic but also minimal enough to remain practical and broadly accessible. While many existing approaches address prompt structuring through elaborate Domain-Specific Languages (DSLs) or multi-layered templates, such methods can impose significant token and cognitive overhead, potentially constraining the model's creative capacity. In this context, we propose the 5C Prompt Contract, a framework that distills prompt design into five intuitive components: Character, Cause, Constraint, Contingency, and Calibration. This minimal cognitive schema explicitly integrates fallback and output optimization directives, fostering reliable, interpretable, and creatively flexible AI interactions. Experimental results demonstrate that the 5C framework consistently achieves superior input token efficiency while maintaining rich and consistent outputs across diverse LLM architectures (OpenAI, Anthropic, DeepSeek, and Gemini), making it particularly suited for individuals and Small-to-Medium Enterprises (SMEs) with limited AI engineering resources.

**Link**: [arxiv](http://arxiv.org/abs/2507.07045v1),  [pdf](http://arxiv.org/pdf/2507.07045v1)

**Tags**: cs.SE cs.SI 68T05 I.2.7; I.2.6 



### Opto-ViT: Architecting a Near-Sensor Region of Interest-Aware Vision   Transformer Accelerator with Silicon Photonics
**Authors**: Mehrdad Morsali, Chengwei Zhou, Deniz Najafi, Sreetama Sarkar, Pietro Mercati, Navid Khoshavi, Peter Beerel, Mahdi Nikdast, Gourav Datta, Shaahin Angizi

**Updated**: 2025-07-09T17:07:26Z

**Summary**: Vision Transformers (ViTs) have emerged as a powerful architecture for computer vision tasks due to their ability to model long-range dependencies and global contextual relationships. However, their substantial compute and memory demands hinder efficient deployment in scenarios with strict energy and bandwidth limitations. In this work, we propose OptoViT, the first near-sensor, region-aware ViT accelerator leveraging silicon photonics (SiPh) for real-time and energy-efficient vision processing. Opto-ViT features a hybrid electronic-photonic architecture, where the optical core handles compute-intensive matrix multiplications using Vertical-Cavity Surface-Emitting Lasers (VCSELs) and Microring Resonators (MRs), while nonlinear functions and normalization are executed electronically. To reduce redundant computation and patch processing, we introduce a lightweight Mask Generation Network (MGNet) that identifies regions of interest in the current frame and prunes irrelevant patches before ViT encoding. We further co-optimize the ViT backbone using quantization-aware training and matrix decomposition tailored for photonic constraints. Experiments across device fabrication, circuit and architecture co-design, to classification, detection, and video tasks demonstrate that OptoViT achieves 100.4 KFPS/W with up to 84% energy savings with less than 1.6% accuracy loss, while enabling scalable and efficient ViT deployment at the edge.

**Link**: [arxiv](http://arxiv.org/abs/2507.07044v1),  [pdf](http://arxiv.org/pdf/2507.07044v1)

**Tags**: cs.AR 



### Self-Supervised Learning at the Edge: The Cost of Labeling
**Authors**: Roberto Pereira, Fernanda Famá, Asal Rangrazi, Marco Miozzo, Charalampos Kalalas, Paolo Dini

**Updated**: 2025-07-09T17:03:50Z

**Summary**: Contrastive learning (CL) has recently emerged as an alternative to traditional supervised machine learning solutions by enabling rich representations from unstructured and unlabeled data. However, CL and, more broadly, self-supervised learning (SSL) methods often demand a large amount of data and computational resources, posing challenges for deployment on resource-constrained edge devices. In this work, we explore the feasibility and efficiency of SSL techniques for edge-based learning, focusing on trade-offs between model performance and energy efficiency. In particular, we analyze how different SSL techniques adapt to limited computational, data, and energy budgets, evaluating their effectiveness in learning robust representations under resource-constrained settings. Moreover, we also consider the energy costs involved in labeling data and assess how semi-supervised learning may assist in reducing the overall energy consumed to train CL models. Through extensive experiments, we demonstrate that tailored SSL strategies can achieve competitive performance while reducing resource consumption by up to 4X, underscoring their potential for energy-efficient learning at the edge.

**Link**: [arxiv](http://arxiv.org/abs/2507.07033v1),  [pdf](http://arxiv.org/pdf/2507.07033v1)

**Tags**: cs.LG eess.SP 



### Exploring Fairness Interventions in Open Source Projects
**Authors**: Sadia Afrin Mim, Fatema Tuz Zohra, Justin Smith, Brittany Johnson

**Updated**: 2025-07-09T16:57:59Z

**Summary**: The deployment of biased machine learning (ML) models has resulted in adverse effects in crucial sectors such as criminal justice and healthcare. To address these challenges, a diverse range of machine learning fairness interventions have been developed, aiming to mitigate bias and promote the creation of more equitable models. Despite the growing availability of these interventions, their adoption in real-world applications remains limited, with many practitioners unaware of their existence. To address this gap, we systematically identified and compiled a dataset of 62 open source fairness interventions and identified active ones. We conducted an in-depth analysis of their specifications and features to uncover considerations that may drive practitioner preference and to identify the software interventions actively maintained in the open source ecosystem. Our findings indicate that 32% of these interventions have been actively maintained within the past year, and 50% of them offer both bias detection and mitigation capabilities, mostly during inprocessing.

**Link**: [arxiv](http://arxiv.org/abs/2507.07026v1),  [pdf](http://arxiv.org/pdf/2507.07026v1)

**Tags**: cs.SE 



### First Return, Entropy-Eliciting Explore
**Authors**: Tianyu Zheng, Tianshun Xing, Qingshui Gu, Taoran Liang, Xingwei Qu, Xin Zhou, Yizhi Li, Zhoufutu Wen, Chenghua Lin, Wenhao Huang, Qian Liu, Ge Zhang, Zejun Ma

**Updated**: 2025-07-09T16:45:48Z

**Summary**: Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning abilities of Large Language Models (LLMs) but it struggles with unstable exploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a structured exploration framework that identifies high-uncertainty decision points in reasoning trajectories and performs targeted rollouts to construct semantically grounded intermediate feedback. Our method provides targeted guidance without relying on dense supervision. Empirical results on mathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable training, produces longer and more coherent responses, and increases the proportion of fully correct trajectories. These results highlight the framework's effectiveness in improving LLM reasoning through more robust and structured exploration.

**Link**: [arxiv](http://arxiv.org/abs/2507.07017v1),  [pdf](http://arxiv.org/pdf/2507.07017v1)

**Tags**: cs.AI 



### TokenShapley: Token Level Context Attribution with Shapley Value
**Authors**: Yingtai Xiao, Yuqing Zhu, Sirat Samyoun, Wanrong Zhang, Jiachen T. Wang, Jian Du

**Updated**: 2025-07-09T16:40:38Z

**Summary**: Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.05261v2),  [pdf](http://arxiv.org/pdf/2507.05261v2)

**Tags**: cs.CL cs.LG 



### Skewed Score: A statistical framework to assess autograders
**Authors**: Magda Dubois, Harry Coppock, Mario Giulianelli, Timo Flesch, Lennart Luettgau, Cozmin Ududec

**Updated**: 2025-07-09T16:28:55Z

**Summary**: The evaluation of large language model (LLM) outputs is increasingly performed by other LLMs, a setup commonly known as "LLM-as-a-judge", or autograders. While autograders offer a scalable alternative to human evaluation, they have shown mixed reliability and may exhibit systematic biases, depending on response type, scoring methodology, domain specificity, or other factors. Here we propose a statistical framework based on Bayesian generalised linear models (GLMs) that enables researchers to simultaneously assess their autograders while addressing their primary research questions (e.g., LLM evaluation). Our approach models evaluation outcomes (e.g., scores or pairwise preferences) as a function of properties of the grader (e.g., human vs. autograder) and the evaluated item (e.g., response length or the LLM that generated it), allowing for explicit quantification of scoring differences and potential biases within a unified framework. In addition, our method can be used to augment traditional metrics such as inter-rater agreement, by providing uncertainty estimates and clarifying sources of disagreement. Overall, this approach contributes to more robust and interpretable use of autograders in LLM evaluation, enabling both performance analysis and bias detection.

**Link**: [arxiv](http://arxiv.org/abs/2507.03772v2),  [pdf](http://arxiv.org/pdf/2507.03772v2)

**Tags**: cs.LG stat.ML 



### Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning   in Multimodal LLMs
**Authors**: Yahan Yu, Yuyang Dong, Masafumi Oyamada

**Updated**: 2025-07-09T16:25:44Z

**Summary**: Reasoning is a key capability for large language models (LLMs), particularly when applied to complex tasks such as mathematical problem solving. However, multimodal reasoning research still requires further exploration of modality alignment and training costs. Many of these approaches rely on additional data annotation and relevant rule-based rewards to enhance the understanding and reasoning ability, which significantly increases training costs and limits scalability. To address these challenges, we propose the Deliberate-to-Intuitive reasoning framework (D2I) that improves the understanding and reasoning ability of multimodal LLMs (MLLMs) without extra annotations and complex rewards. Specifically, our method sets deliberate reasoning strategies to enhance modality alignment only through the rule-based format reward during training. While evaluating, the reasoning style shifts to intuitive, which removes deliberate reasoning strategies during training and implicitly reflects the model's acquired abilities in the response. D2I outperforms baselines across both in-domain and out-of-domain benchmarks. Our findings highlight the role of format reward in fostering transferable reasoning skills in MLLMs, and inspire directions for decoupling training-time reasoning depth from test-time response flexibility.

**Link**: [arxiv](http://arxiv.org/abs/2507.06999v1),  [pdf](http://arxiv.org/pdf/2507.06999v1)

**Tags**: cs.CV cs.CL cs.LG 



### The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced   Planning, Navigation, and Dynamic Adaptation
**Authors**: Jieren Deng, Aleksandar Cvetkovic, Pak Kiu Chung, Dragomir Yankov, Chiqun Zhang

**Updated**: 2025-07-09T16:18:09Z

**Summary**: Traditional travel-planning systems are often static and fragmented, leaving them ill-equipped to handle real-world complexities such as evolving environmental conditions and unexpected itinerary disruptions. In this paper, we identify three gaps between existing service providers causing frustrating user experience: intelligent trip planning, precision "last-100-meter" navigation, and dynamic itinerary adaptation. We propose three cooperative agents: a Travel Planning Agent that employs grid-based spatial grounding and map analysis to help resolve complex multi-modal user queries; a Destination Assistant Agent that provides fine-grained guidance for the final navigation leg of each journey; and a Local Discovery Agent that leverages image embeddings and Retrieval-Augmented Generation (RAG) to detect and respond to trip plan disruptions. With evaluations and experiments, our system demonstrates substantial improvements in query interpretation, navigation accuracy, and disruption resilience, underscoring its promise for applications from urban exploration to emergency response.

**Link**: [arxiv](http://arxiv.org/abs/2507.06993v1),  [pdf](http://arxiv.org/pdf/2507.06993v1)

**Tags**: cs.AI cs.CV 



### MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology   Report Generation
**Authors**: Qilong Xing, Zikai Song, Youjia Zhang, Na Feng, Junqing Yu, Wei Yang

**Updated**: 2025-07-09T16:15:38Z

**Summary**: Despite significant advancements in adapting Large Language Models (LLMs) for radiology report generation (RRG), clinical adoption remains challenging due to difficulties in accurately mapping pathological and anatomical features to their corresponding text descriptions. Additionally, semantic agnostic feature extraction further hampers the generation of accurate diagnostic reports. To address these challenges, we introduce Medical Concept Aligned Radiology Report Generation (MCA-RG), a knowledge-driven framework that explicitly aligns visual features with distinct medical concepts to enhance the report generation process. MCA-RG utilizes two curated concept banks: a pathology bank containing lesion-related knowledge, and an anatomy bank with anatomical descriptions. The visual features are aligned with these medical concepts and undergo tailored enhancement. We further propose an anatomy-based contrastive learning procedure to improve the generalization of anatomical features, coupled with a matching loss for pathological features to prioritize clinically relevant regions. Additionally, a feature gating mechanism is employed to filter out low-quality concept features. Finally, the visual features are corresponding to individual medical concepts, and are leveraged to guide the report generation process. Experiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate that MCA-RG achieves superior performance, highlighting its effectiveness in radiology report generation.

**Link**: [arxiv](http://arxiv.org/abs/2507.06992v1),  [pdf](http://arxiv.org/pdf/2507.06992v1)

**Tags**: cs.CV cs.AI 



### Planning Anything with Rigor: General-Purpose Zero-Shot Planning with   LLM-based Formalized Programming
**Authors**: Yilun Hao, Yang Zhang, Chuchu Fan

**Updated**: 2025-07-09T16:13:20Z

**Summary**: While large language models (LLMs) have recently demonstrated strong potential in solving planning problems, there is a trade-off between flexibility and complexity. LLMs, as zero-shot planners themselves, are still not capable of directly generating valid plans for complex planning problems such as multi-constraint or long-horizon tasks. On the other hand, many frameworks aiming to solve complex planning problems often rely on task-specific preparatory efforts, such as task-specific in-context examples and pre-defined critics/verifiers, which limits their cross-task generalization capability. In this paper, we tackle these challenges by observing that the core of many planning problems lies in optimization problems: searching for the optimal solution (best plan) with goals subject to constraints (preconditions and effects of decisions). With LLMs' commonsense, reasoning, and programming capabilities, this opens up the possibilities of a universal LLM-based approach to planning problems. Inspired by this observation, we propose LLMFP, a general-purpose framework that leverages LLMs to capture key information from planning problems and formally formulate and solve them as optimization problems from scratch, with no task-specific examples needed. We apply LLMFP to 9 planning problems, ranging from multi-constraint decision making to multi-step planning problems, and demonstrate that LLMFP achieves on average 83.7% and 86.8% optimal rate across 9 tasks for GPT-4o and Claude 3.5 Sonnet, significantly outperforming the best baseline (direct planning with OpenAI o1-preview) with 37.6% and 40.7% improvements. We also validate components of LLMFP with ablation experiments and analyzed the underlying success and failure reasons. Project page: https://sites.google.com/view/llmfp.

**Link**: [arxiv](http://arxiv.org/abs/2410.12112v3),  [pdf](http://arxiv.org/pdf/2410.12112v3)

**Tags**: cs.AI cs.CL 



### Are They All Good? Evaluating the Quality of CoTs in LLM-based Code   Generation
**Authors**: Binquan Zhang, Li Zhang, Zhiwen Luo, Yuxin Du, Fang Liu, Song Wang, Lin Shi

**Updated**: 2025-07-09T16:07:20Z

**Summary**: Large language models (LLMs) have demonstrated impressive performance in code generation, particularly when augmented with chain-of-thought (CoT) prompting techniques. They break down requirements into intermediate reasoning steps, which act as design rationales to guide LLMs in writing code like human programmers. Thus, the quality of these steps is crucial for ensuring the correctness and reliability of the generated code. However, little is known about the quality of CoT generated by LLMs. To what extent can we trust the thoughts generated by LLMs? How good are they? This paper empirically explores the external and internal factors of why LLMs generate unsatisfactory CoTs by analyzing 1,023 failed code samples on two widely used code generation benchmarks. We also evaluate their impact on code generation performance by analyzing 210 CoT-code pairs and refining the unsatisfied CoTs by prompting LLMs. Our study reveals three key findings: (1) External factors (53.60%), such as unclear requirements and lack of context, mainly affect CoT quality, while internal factors (40.10%) stem from LLMs' misunderstanding prompts. (2) Even when CoTs are correct, 18.5% of the generated code contains errors due to instruction-following issues; conversely, 11.90% of correct code is paired with flawed CoTs. (3) Refining low-quality CoTs is feasible, i.e., LLMs improve when given detailed problem descriptions. These findings highlight key challenges in CoT-based code generation and suggest directions for improving LLM reasoning and reliability.

**Link**: [arxiv](http://arxiv.org/abs/2507.06980v1),  [pdf](http://arxiv.org/pdf/2507.06980v1)

**Tags**: cs.SE 



### Investigating the Robustness of Retrieval-Augmented Generation at the   Query Level
**Authors**: Sezen Perçin, Xin Su, Qutub Sha Syed, Phillip Howard, Aleksei Kuvshinov, Leo Schwinn, Kay-Ulrich Scholl

**Updated**: 2025-07-09T15:39:17Z

**Summary**: Large language models (LLMs) are very costly and inefficient to update with new information. To address this limitation, retrieval-augmented generation (RAG) has been proposed as a solution that dynamically incorporates external knowledge during inference, improving factual consistency and reducing hallucinations. Despite its promise, RAG systems face practical challenges-most notably, a strong dependence on the quality of the input query for accurate retrieval. In this paper, we investigate the sensitivity of different components in the RAG pipeline to various types of query perturbations. Our analysis reveals that the performance of commonly used retrievers can degrade significantly even under minor query variations. We study each module in isolation as well as their combined effect in an end-to-end question answering setting, using both general-domain and domain-specific datasets. Additionally, we propose an evaluation framework to systematically assess the query-level robustness of RAG pipelines and offer actionable recommendations for practitioners based on the results of more than 1092 experiments we performed.

**Link**: [arxiv](http://arxiv.org/abs/2507.06956v1),  [pdf](http://arxiv.org/pdf/2507.06956v1)

**Tags**: cs.CL 



### Neuron-Level Differentiation of Memorization and Generalization in Large   Language Models
**Authors**: Ko-Wei Huang, Yi-Fu Fu, Ching-Yu Tsai, Yu-Chieh Tu, Tzu-Ling Cheng, Cheng-Yu Lin, Yi-Ting Yang, Heng-Yi Liu, Keng-Te Liao, Da-Cheng Juan, Shou-De Lin

**Updated**: 2025-07-09T15:14:46Z

**Summary**: We investigate how Large Language Models (LLMs) distinguish between memorization and generalization at the neuron level. Through carefully designed tasks, we identify distinct neuron subsets responsible for each behavior. Experiments on both a GPT-2 model trained from scratch and a pretrained LLaMA-3.2 model fine-tuned with LoRA show consistent neuron-level specialization. We further demonstrate that inference-time interventions on these neurons can steer the model's behavior toward memorization or generalization. To assess robustness, we evaluate intra-task and inter-task consistency, confirming that these neuron-behavior associations reflect generalizable patterns rather than dataset-specific artifacts. Our findings reveal modular structure in LLMs and enable controlling memorization and generalization behaviors at inference time.

**Link**: [arxiv](http://arxiv.org/abs/2412.18497v2),  [pdf](http://arxiv.org/pdf/2412.18497v2)

**Tags**: cs.CL 



### What to Keep and What to Drop: Adaptive Table Filtering Framework
**Authors**: WonJune Jang

**Updated**: 2025-07-09T15:10:56Z

**Summary**: Large language models (LLMs) for table-based reasoning often struggle with large tables due to input length limits. We propose ATF (Adaptive Table Filtering Framework), a modular and question-aware filtering pipeline that prunes uninformative columns and rows using LLM-generated column descriptions, clustering, and sparse-dense alignment scores. ATF integrates seamlessly with existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that ATF reduces table cells by 70%, boosting performance on out-of-domain TableQA tasks while causing slight performance drops on Table Fact Verification, where full-table context is more critical. These results highlight ATF's ability to adaptively balance informativeness and minimalism across tasks.

**Link**: [arxiv](http://arxiv.org/abs/2506.23463v2),  [pdf](http://arxiv.org/pdf/2506.23463v2)

**Tags**: cs.CL I.2.7 



### Rethinking Verification for LLM Code Generation: From Generation to   Testing
**Authors**: Zihan Ma, Taolin Zhang, Maosong Cao, Junnan Liu, Wenwei Zhang, Minnan Luo, Songyang Zhang, Kai Chen

**Updated**: 2025-07-10T03:12:09Z

**Summary**: Large language models (LLMs) have recently achieved notable success in code-generation benchmarks such as HumanEval and LiveCodeBench. However, a detailed examination reveals that these evaluation suites often comprise only a limited number of homogeneous test cases, resulting in subtle faults going undetected. This not only artificially inflates measured performance but also compromises accurate reward estimation in reinforcement learning frameworks utilizing verifiable rewards (RLVR). To address these critical shortcomings, we systematically investigate the test-case generation (TCG) task by proposing multi-dimensional metrics designed to rigorously quantify test-suite thoroughness. Furthermore, we introduce a human-LLM collaborative method (SAGA), leveraging human programming expertise with LLM reasoning capability, aimed at significantly enhancing both the coverage and the quality of generated test cases. In addition, we develop a TCGBench to facilitate the study of the TCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a verifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc) of the code generation evaluation benchmark synthesized by SAGA is 10.78% higher than that of LiveCodeBench-v6. These results demonstrate the effectiveness of our proposed method. We hope this work contributes to building a scalable foundation for reliable LLM code evaluation, further advancing RLVR in code generation, and paving the way for automated adversarial test synthesis and adaptive benchmark integration.

**Link**: [arxiv](http://arxiv.org/abs/2507.06920v2),  [pdf](http://arxiv.org/pdf/2507.06920v2)

**Tags**: cs.CL 



### Beyond Connectivity: An Open Architecture for AI-RAN Convergence in 6G
**Authors**: Michele Polese, Niloofar Mohamadi, Salvatore D'Oro, Tommaso Melodia

**Updated**: 2025-07-09T14:49:11Z

**Summary**: The proliferation of data-intensive Artificial Intelligence (AI) applications at the network edge demands a fundamental shift in RAN design, from merely consuming AI for network optimization, to actively enabling distributed AI workloads. This paradigm shift presents a significant opportunity for network operators to monetize AI at the edge while leveraging existing infrastructure investments. To realize this vision, this article presents a novel converged O-RAN and AI-RAN architecture that unifies orchestration and management of both telecommunications and AI workloads on shared infrastructure. The proposed architecture extends the Open RAN principles of modularity, disaggregation, and cloud-nativeness to support heterogeneous AI deployments. We introduce two key architectural innovations: (i) the AI-RAN Orchestrator, which extends the O-RAN Service Management and Orchestration (SMO) to enable integrated resource and allocation across RAN and AI workloads; and (ii) AI-RAN sites that provide distributed edge AI platforms with real-time processing capabilities. The proposed system supports flexible deployment options, allowing AI workloads to be orchestrated with specific timing requirements (real-time or batch processing) and geographic targeting. The proposed architecture addresses the orchestration requirements for managing heterogeneous workloads at different time scales while maintaining open, standardized interfaces and multi-vendor interoperability.

**Link**: [arxiv](http://arxiv.org/abs/2507.06911v1),  [pdf](http://arxiv.org/pdf/2507.06911v1)

**Tags**: cs.NI cs.AI eess.SP 



### Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in   Dialogues
**Authors**: Fareya Ikram, Alexander Scarlatos, Andrew Lan

**Updated**: 2025-07-09T14:47:35Z

**Summary**: Tutoring dialogues have gained significant attention in recent years, given the prominence of online learning and the emerging tutoring abilities of artificial intelligence (AI) agents powered by large language models (LLMs). Recent studies have shown that the strategies used by tutors can have significant effects on student outcomes, necessitating methods to predict how tutors will behave and how their actions impact students. However, few works have studied predicting tutor strategy in dialogues. Therefore, in this work we investigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to predict both future tutor moves and student outcomes in dialogues, using two math tutoring dialogue datasets. We find that even state-of-the-art LLMs struggle to predict future tutor strategy while tutor strategy is highly indicative of student outcomes, outlining a need for more powerful methods to approach this task.

**Link**: [arxiv](http://arxiv.org/abs/2507.06910v1),  [pdf](http://arxiv.org/pdf/2507.06910v1)

**Tags**: cs.CL cs.CY 



### MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal   Prediction
**Authors**: Xiao Wang, Jiahuan Pei, Diancheng Shui, Zhiguang Han, Xin Sun, Dawei Zhu, Xiaoyu Shen

**Updated**: 2025-07-09T14:47:00Z

**Summary**: Legal judgment prediction offers a compelling method to aid legal practitioners and researchers. However, the research question remains relatively under-explored: Should multiple defendants and charges be treated separately in LJP? To address this, we introduce a new dataset namely multi-person multi-charge prediction (MPMCP), and seek the answer by evaluating the performance of several prevailing legal large language models (LLMs) on four practical legal judgment scenarios: (S1) single defendant with a single charge, (S2) single defendant with multiple charges, (S3) multiple defendants with a single charge, and (S4) multiple defendants with multiple charges. We evaluate the dataset across two LJP tasks, i.e., charge prediction and penalty term prediction. We have conducted extensive experiments and found that the scenario involving multiple defendants and multiple charges (S4) poses the greatest challenges, followed by S2, S3, and S1. The impact varies significantly depending on the model. For example, in S4 compared to S1, InternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD, while Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD. Our dataset and code are available at https://github.com/lololo-xiao/MultiJustice-MPMCP.

**Link**: [arxiv](http://arxiv.org/abs/2507.06909v1),  [pdf](http://arxiv.org/pdf/2507.06909v1)

**Tags**: cs.CL cs.AI 



### Joint Beamforming and Position Optimization for Fluid STAR-RIS-NOMA   Assisted Wireless Communication Systems
**Authors**: Yu Liu, Qu Luo, Gaojie Chen, Pei Xiao, Ahmed Elzanaty, Mohsen Khalily, Rahim Tafazolli

**Updated**: 2025-07-09T14:43:34Z

**Summary**: To address the limitations of traditional reconfigurable intelligent surfaces (RIS) in spatial control capability, this paper introduces the concept of the fluid antenna system (FAS) and proposes a fluid simultaneously transmitting and reflecting RIS (FSTAR-RIS) assisted non-orthogonal multiple access (NOMA) multi-user communication system. In this system, each FSTAR-RIS element is capable of flexible mobility and can dynamically adjust its position in response to environmental variations, thereby enabling simultaneous service to users in both the transmission and reflection zones. This significantly enhances the system's spatial degrees of freedom (DoF) and service adaptability. To maximize the system's weighted sum-rate, we formulate a non-convex optimization problem that jointly optimizes the base station beamforming, the transmission/reflection coefficients of the FSTAR-RIS, and the element positions. An alternating optimization (AO) algorithm is developed, incorporating successive convex approximation (SCA), semi-definite relaxation (SDR), and majorization-minimization (MM) techniques. In particular, to address the complex channel coupling introduced by the coexistence of direct and FSTAR-RIS paths, the MM framework is employed in the element position optimization subproblem, enabling an efficient iterative solution strategy. Simulation results validate that the proposed system achieves up to a 27% increase in total sum rate compared to traditional STAR-RIS systems and requires approximately 50% fewer RIS elements to attain the same performance, highlighting its effectiveness for cost-efficient large-scale deployment.

**Link**: [arxiv](http://arxiv.org/abs/2507.06904v1),  [pdf](http://arxiv.org/pdf/2507.06904v1)

**Tags**: eess.SP 



### NoLiMa: Long-Context Evaluation Beyond Literal Matching
**Authors**: Ali Modarressi, Hanieh Deilamsalehy, Franck Dernoncourt, Trung Bui, Ryan A. Rossi, Seunghyun Yoon, Hinrich Schütze

**Updated**: 2025-07-09T14:35:23Z

**Summary**: Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a "needle" (relevant information) from a "haystack" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 13 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 11 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. Even models enhanced with reasoning capabilities or CoT prompting struggle to maintain performance in long contexts. We publicly release the dataset and evaluation code at https://github.com/adobe-research/NoLiMa.

**Link**: [arxiv](http://arxiv.org/abs/2502.05167v3),  [pdf](http://arxiv.org/pdf/2502.05167v3)

**Tags**: cs.CL 



### Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning   for Large Language Model
**Authors**: Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao

**Updated**: 2025-07-10T13:42:04Z

**Summary**: Reinforcement Learning (RL) has demonstrated its potential to improve the reasoning ability of Large Language Models (LLMs). One major limitation of most existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL in nature, i.e., data generated during the past learning process is not fully utilized. This inevitably comes at a significant cost of compute and time, posing a stringent bottleneck on continuing economic and efficient scaling. To this end, we launch the renaissance of off-policy RL and propose Reincarnating Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix consists of three major components: (1) Mix-policy proximal policy gradient with an increased Update-To-Data (UTD) ratio for efficient training; (2) KL-Convex policy constraint to balance the trade-off between stability and flexibility; (3) Policy reincarnation to achieve a seamless transition from efficient early-stage learning to steady asymptotic improvement. In our experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with 0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level performance with an over 30x to 450x reduction in training cost in terms of rollout data volume. In addition, we reveal insightful findings via multifaceted analysis, including the implicit preference for shorter responses due to the Whipping Effect of off-policy discrepancy, the collapse mode of self-reflection behavior under the presence of severe off-policyness, etc.

**Link**: [arxiv](http://arxiv.org/abs/2507.06892v2),  [pdf](http://arxiv.org/pdf/2507.06892v2)

**Tags**: cs.LG cs.AI cs.CL 



### A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis   in Smart Microgrids Using Dual Fractional-Order Feature Analysis
**Authors**: Yifan Wang

**Updated**: 2025-07-09T14:27:40Z

**Summary**: Cyber-attacks jeopardize the safe operation of smart microgrids. At the same time, existing diagnostic methods either depend on expensive multi-point instrumentation or stringent modelling assumptions that are untenable under single-sensor constraints. This paper proposes a Fractional-Order Memory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency fault localisation and cyber-attack detection using only one VPQ (Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual fractional-order feature library by jointly applying Caputo and Gr\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and slow drifts in the VPQ signal. A two-stage hierarchical classifier then pinpoints the affected inverter and isolates the faulty IGBT switch, effectively alleviating class imbalance. Robustness is further strengthened through Progressive Memory-Replay Adversarial Training (PMR-AT), whose attack-aware loss is dynamically re-weighted via Online Hard Example Mining (OHEM) to prioritise the most challenging samples. Experiments on a four-inverter microgrid testbed comprising 1 normal and 24 fault classes under four attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0 % (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining 96.7 % under attack-free conditions. These results establish FO-MADS as a cost-effective and readily deployable solution that markedly enhances the cyber-physical resilience of smart microgrids.

**Link**: [arxiv](http://arxiv.org/abs/2507.06890v1),  [pdf](http://arxiv.org/pdf/2507.06890v1)

**Tags**: eess.SY cs.AI cs.SY 



### Toward a Full-Stack Co-Simulation Platform for Testing of Automated   Driving Systems
**Authors**: Dong Bi, Yongqi Zhao, Zhengguo Gu, Tomislav Mihalj, Jia Hu, Arno Eichberger

**Updated**: 2025-07-09T14:19:58Z

**Summary**: Virtual testing has emerged as an effective approach to accelerate the deployment of automated driving systems. Nevertheless, existing simulation toolchains encounter difficulties in integrating rapid, automated scenario generation with simulation environments supporting advanced automated driving capabilities. To address this limitation, a full-stack toolchain is presented, enabling automatic scenario generation from real-world datasets and efficient validation through a co-simulation platform based on CarMaker, ROS, and Apollo. The simulation results demonstrate the effectiveness of the proposed toolchain. A demonstration video showcasing the toolchain is available at the provided link: https://youtu.be/taJw_-CmSiY.

**Link**: [arxiv](http://arxiv.org/abs/2507.06884v1),  [pdf](http://arxiv.org/pdf/2507.06884v1)

**Tags**: cs.RO 



### Adaptive Elicitation of Latent Information Using Natural Language
**Authors**: Jimmy Wang, Thomas Zollo, Richard Zemel, Hongseok Namkoong

**Updated**: 2025-07-09T13:58:35Z

**Summary**: Eliciting information to reduce uncertainty about a latent entity is a critical task in many application domains, e.g., assessing individual student learning outcomes, diagnosing underlying diseases, or learning user preferences. Though natural language is a powerful medium for this purpose, large language models (LLMs) and existing fine-tuning algorithms lack mechanisms for strategically gathering information to refine their own understanding of the latent entity. To harness the generalization power and world knowledge of LLMs in developing effective information-gathering strategies, we propose an adaptive elicitation framework that actively reduces uncertainty on the latent entity. Since probabilistic modeling of an abstract latent entity is difficult, our framework adopts a predictive view of uncertainty, using a meta-learned language model to simulate future observations and enable scalable uncertainty quantification over complex natural language. Through autoregressive forward simulation, our model quantifies how new questions reduce epistemic uncertainty, enabling the development of sophisticated information-gathering strategies to choose the most informative next queries. In experiments on the 20 questions game, dynamic opinion polling, and adaptive student assessment, our method consistently outperforms baselines in identifying critical unknowns and improving downstream predictions, illustrating the promise of strategic information gathering in natural language settings.

**Link**: [arxiv](http://arxiv.org/abs/2504.04204v2),  [pdf](http://arxiv.org/pdf/2504.04204v2)

**Tags**: cs.CL cs.AI cs.LG 



### The Dark Side of LLMs: Agent-based Attacks for Complete Computer   Takeover
**Authors**: Matteo Lupinacci, Francesco Aurelio Pironti, Francesco Blefari, Francesco Romeo, Luigi Arena, Angelo Furfaro

**Updated**: 2025-07-10T15:18:20Z

**Summary**: The rapid adoption of Large Language Model (LLM) agents and multi-agent systems enables unprecedented capabilities in natural language processing and generation. However, these systems have introduced unprecedented security vulnerabilities that extend beyond traditional prompt injection attacks. This paper presents the first comprehensive evaluation of LLM agents as attack vectors capable of achieving complete computer takeover through the exploitation of trust boundaries within agentic AI systems where autonomous entities interact and influence each other. We demonstrate that adversaries can leverage three distinct attack surfaces - direct prompt injection, RAG backdoor attacks, and inter-agent trust exploitation - to coerce popular LLMs (including GPT-4o, Claude-4 and Gemini-2.5) into autonomously installing and executing malware on victim machines. Our evaluation of 17 state-of-the-art LLMs reveals an alarming vulnerability hierarchy: while 41.2% of models succumb to direct prompt injection, 52.9% are vulnerable to RAG backdoor attacks, and a critical 82.4% can be compromised through inter-agent trust exploitation. Notably, we discovered that LLMs which successfully resist direct malicious commands will execute identical payloads when requested by peer agents, revealing a fundamental flaw in current multi-agent security models. Our findings demonstrate that only 5.9% of tested models (1/17) proved resistant to all attack vectors, with the majority exhibiting context-dependent security behaviors that create exploitable blind spots. Our findings also highlight the need to increase awareness and research on the security risks of LLMs, showing a paradigm shift in cybersecurity threats, where AI tools themselves become sophisticated attack vectors.

**Link**: [arxiv](http://arxiv.org/abs/2507.06850v2),  [pdf](http://arxiv.org/pdf/2507.06850v2)

**Tags**: cs.CR cs.AI 



### EMORL: Ensemble Multi-Objective Reinforcement Learning for Efficient and   Flexible LLM Fine-Tuning
**Authors**: Lingxiao Kong, Cong Yang, Susanne Neufang, Oya Deniz Beyan, Zeyd Boukhers

**Updated**: 2025-07-09T13:45:07Z

**Summary**: Recent advances in reinforcement learning (RL) for large language model (LLM) fine-tuning show promise in addressing multi-objective tasks but still face significant challenges, including competing objective balancing, low training efficiency, poor scalability, and limited explainability. Leveraging ensemble learning principles, we introduce an Ensemble Multi-Objective RL (EMORL) framework that fine-tunes multiple models with individual objectives while optimizing their aggregation after the fine-tuning to improve efficiency and flexibility. Our method is the first to aggregate the hidden states of individual models, incorporating contextual information from multiple objectives. This approach is supported by a hierarchical grid search algorithm that identifies optimal weighted combinations. We evaluate EMORL on counselor reflection generation tasks, using text classification models to score the generations and provide rewards during RL fine-tuning. Through comprehensive experiments on the PAIR and Psych8k datasets, we demonstrate the advantages of EMORL against existing baselines: significantly lower and more stable training consumption ($17,529\pm 1,650$ data points and $6,573\pm 147.43$ seconds), improved scalability and explainability, and comparable performance across multiple objectives.

**Link**: [arxiv](http://arxiv.org/abs/2505.02579v3),  [pdf](http://arxiv.org/pdf/2505.02579v3)

**Tags**: cs.CL cs.AI cs.LG 



### Shifting from Ranking to Set Selection for Retrieval Augmented   Generation
**Authors**: Dahyun Lee, Yongrae Jo, Haeju Park, Moontae Lee

**Updated**: 2025-07-10T01:36:33Z

**Summary**: Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved passages are not only individually relevant but also collectively form a comprehensive set. Existing approaches primarily rerank top-k passages based on their individual relevance, often failing to meet the information needs of complex queries in multi-hop question answering. In this work, we propose a set-wise passage selection approach and introduce SETR, which explicitly identifies the information requirements of a query through Chain-of-Thought reasoning and selects an optimal set of passages that collectively satisfy those requirements. Experiments on multi-hop RAG benchmarks show that SETR outperforms both proprietary LLM-based rerankers and open-source baselines in terms of answer correctness and retrieval quality, providing an effective and efficient alternative to traditional rerankers in RAG systems. The code is available at https://github.com/LGAI-Research/SetR

**Link**: [arxiv](http://arxiv.org/abs/2507.06838v2),  [pdf](http://arxiv.org/pdf/2507.06838v2)

**Tags**: cs.CL cs.IR 



### Enhancing Environment Generalizability for Deep Learning-Based CSI   Feedback
**Authors**: Haoyu Wang, Shuangfeng Han, Xiaoyun Wang, Zhi Sun

**Updated**: 2025-07-09T13:31:51Z

**Summary**: Accurate and low-overhead channel state information (CSI) feedback is essential to boost the capacity of frequency division duplex (FDD) massive multiple-input multiple-output (MIMO) systems. Deep learning-based CSI feedback significantly outperforms conventional approaches. Nevertheless, current deep learning-based CSI feedback algorithms exhibit limited generalizability to unseen environments, which obviously increases the deployment cost. In this paper, we first model the distribution shift of CSI across different environments, which is composed of the distribution shift of multipath structure and a single-path. Then, EG-CsiNet is proposed as a novel CSI feedback learning framework to enhance environment-generalizability. Explicitly, EG-CsiNet comprises the modules of multipath decoupling and fine-grained alignment, which can address the distribution shift of multipath structure and a single path. Based on extensive simulations, the proposed EG-CsiNet can robustly enhance the generalizability in unseen environments compared to the state-of-the-art, especially in challenging conditions with a single source environment.

**Link**: [arxiv](http://arxiv.org/abs/2507.06833v1),  [pdf](http://arxiv.org/pdf/2507.06833v1)

**Tags**: eess.SP 



### Adaptive Termination for Multi-round Parallel Reasoning: An Universal   Semantic Entropy-Guided Framework
**Authors**: Zenan Xu, Zexuan Qiu, Guanhua Huang, Kun Li, Siheng Li, Chenchen Zhang, Kejiao Li, Qi Yi, Yuhao Jiang, Bo Zhou, Fengzong Lian, Zhanhui Kang

**Updated**: 2025-07-09T13:28:35Z

**Summary**: Recent advances in large language models (LLMs) have accelerated progress toward artificial general intelligence, with inference-time scaling emerging as a key technique. Contemporary approaches leverage either sequential reasoning (iteratively extending chains of thought) or parallel reasoning (generating multiple solutions simultaneously) to scale inference. However, both paradigms face fundamental limitations: sequential scaling typically relies on arbitrary token budgets for termination, leading to inefficiency or premature cutoff; while parallel scaling often lacks coordination among parallel branches and requires intrusive fine-tuning to perform effectively. In light of these challenges, we aim to design a flexible test-time collaborative inference framework that exploits the complementary strengths of both sequential and parallel reasoning paradigms. Towards this goal, the core challenge lies in developing an efficient and accurate intrinsic quality metric to assess model responses during collaborative inference, enabling dynamic control and early termination of the reasoning trace. To address this challenge, we introduce semantic entropy (SE), which quantifies the semantic diversity of parallel model responses and serves as a robust indicator of reasoning quality due to its strong negative correlation with accuracy...

**Link**: [arxiv](http://arxiv.org/abs/2507.06829v1),  [pdf](http://arxiv.org/pdf/2507.06829v1)

**Tags**: cs.CL 



### CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models   in Medical Quality Control Indicator Calculation
**Authors**: Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan

**Updated**: 2025-07-09T13:26:39Z

**Summary**: Medical quality control indicators are essential to assess the qualifications of healthcare institutions for medical services. With the impressive performance of large language models (LLMs) like GPT-4 in the medical field, leveraging these technologies for the Medical Quality Control Indicator Calculation (MQCIC) presents a promising approach. In this work, (1) we introduce a real-world task MQCIC and propose an open-source Chinese electronic medical records (EMRs)-based dataset (CMQCIC-Bench) comprising 785 instances and 76 indicators. (2) We propose a semi-automatic method to enhance the rule representation. Then we propose the Clinical Facts-based Inferential Rule (CF-IR) method that disentangles the clinical fact verification and inferential rule reasoning actions. (3) We conduct comprehensive experiments on 20 representative LLMs, covering general and medical models. Our findings reveal that CF-IR outperforms Chain-of-Thought methods in MQCIC tasks. (4) We conduct an error analysis and investigate the capabilities of clinical fact verification and inferential rule reasoning, providing insights to improve performance in the MQCIC further. The dataset and code is available in this repository https://github.com/YuY-2001/C-MQCIC.

**Link**: [arxiv](http://arxiv.org/abs/2502.11703v2),  [pdf](http://arxiv.org/pdf/2502.11703v2)

**Tags**: cs.CL 



### LLM Agent for Hyper-Parameter Optimization
**Authors**: Wanzhe Wang, Jianqiu Peng, Menghao Hu, Weihuang Zhong, Tong Zhang, Shuai Wang, Yixin Zhang, Mingjie Shao, Wanli Ni

**Updated**: 2025-07-09T13:20:45Z

**Summary**: Hyper-parameters are essential and critical for the performance of communication algorithms. However, current hyper-parameters optimization approaches for Warm-Start Particles Swarm Optimization with Crossover and Mutation (WS-PSO-CM) algorithm, designed for radio map-enabled unmanned aerial vehicle (UAV) trajectory and communication, are primarily heuristic-based, exhibiting low levels of automation and improvable performance. In this paper, we design an Large Language Model (LLM) agent for automatic hyper-parameters-tuning, where an iterative framework and Model Context Protocol (MCP) are applied. In particular, the LLM agent is first set up via a profile, which specifies the boundary of hyper-parameters, task objective, terminal condition, conservative or aggressive strategy of optimizing hyper-parameters, and LLM configurations. Then, the LLM agent iteratively invokes WS-PSO-CM algorithm for exploration. Finally, the LLM agent exits the loop based on the terminal condition and returns an optimized set of hyperparameters. Our experiment results show that the minimal sum-rate achieved by hyper-parameters generated via our LLM agent is significantly higher than those by both human heuristics and random generation methods. This indicates that an LLM agent with PSO and WS-PSO-CM algorithm knowledge is useful in seeking high-performance hyper-parameters.

**Link**: [arxiv](http://arxiv.org/abs/2506.15167v2),  [pdf](http://arxiv.org/pdf/2506.15167v2)

**Tags**: cs.IT cs.AI math.IT 



### Losing our Tail -- Again: On (Un)Natural Selection And Multilingual   Large Language Models
**Authors**: Eva Vanmassenhove

**Updated**: 2025-07-09T13:14:29Z

**Summary**: Multilingual Large Language Models (LLMs) considerably changed how technologies can influence language. While previous technologies could mediate or assist humans, there is now a tendency to offload the task of writing itself to these technologies, enabling them to change our linguistic ecosystem more directly. While they provide us quick access to information and impressively fluent output, beneath their apparent sophistication lies a subtle, more insidious threat: the gradual decline and loss of linguistic diversity. With this opinion piece, I explore how model collapse, with a particular focus on translation technology, can lead to the loss of linguistic forms, grammatical features, and cultural nuance. Model collapse refers to the eventual consequence of self-consuming training loops, where models reinforce their own biases and lose linguistic diversity. Drawing on recent work in Computer Vision, Natural Language Processing (NLP) and Machine Translation (MT), I argue that the tails of our linguistic distributions are vanishing, and with them, the narratives and identities they carry. This is a call to resist linguistic flattening and to reimagine NLP as a field that encourages, values and protects expressive multilingual lexical and linguistic diversity and creativity.

**Link**: [arxiv](http://arxiv.org/abs/2507.03933v2),  [pdf](http://arxiv.org/pdf/2507.03933v2)

**Tags**: cs.CL 



### Safer or Luckier? LLMs as Safety Evaluators Are Not Robust to Artifacts
**Authors**: Hongyu Chen, Seraphina Goldfarb-Tarrant

**Updated**: 2025-07-09T13:09:13Z

**Summary**: Large Language Models (LLMs) are increasingly employed as automated evaluators to assess the safety of generated content, yet their reliability in this role remains uncertain. This study evaluates a diverse set of 11 LLM judge models across critical safety domains, examining three key aspects: self-consistency in repeated judging tasks, alignment with human judgments, and susceptibility to input artifacts such as apologetic or verbose phrasing. Our findings reveal that biases in LLM judges can significantly distort the final verdict on which content source is safer, undermining the validity of comparative evaluations. Notably, apologetic language artifacts alone can skew evaluator preferences by up to 98\%. Contrary to expectations, larger models do not consistently exhibit greater robustness, while smaller models sometimes show higher resistance to specific artifacts. To mitigate LLM evaluator robustness issues, we investigate jury-based evaluations aggregating decisions from multiple models. Although this approach both improves robustness and enhances alignment to human judgements, artifact sensitivity persists even with the best jury configurations. These results highlight the urgent need for diversified, artifact-resistant methodologies to ensure reliable safety assessments.

**Link**: [arxiv](http://arxiv.org/abs/2503.09347v3),  [pdf](http://arxiv.org/pdf/2503.09347v3)

**Tags**: cs.CL cs.AI 



### Text to model via SysML: Automated generation of dynamical system   computational models from unstructured natural language text via enhanced   System Modeling Language diagrams
**Authors**: Matthew Anderson Hendricks, Alice Cicirello

**Updated**: 2025-07-09T12:44:49Z

**Summary**: This paper contributes to speeding up the design and deployment of engineering dynamical systems by proposing a strategy for exploiting domain and expert knowledge for the automated generation of dynamical system computational model starting from a corpus of document relevant to the dynamical system of interest and an input document describing the specific system. This strategy is implemented in five steps and, crucially, it uses system modeling language diagrams (SysML) to extract accurate information about the dependencies, attributes, and operations of components. Natural Language Processing (NLP) strategies and Large Language Models (LLMs) are employed in specific tasks to improve intermediate outputs of the SySML diagrams automated generation, such as: list of key nouns; list of extracted relationships; list of key phrases and key relationships; block attribute values; block relationships; and BDD diagram generation. The applicability of automated SysML diagram generation is illustrated with different case studies. The computational models of complex dynamical systems from SysML diagrams are then obtained via code generation and computational model generation steps. In the code generation step, NLP strategies are used for summarization, while LLMs are used for validation only. The proposed approach is not limited to a specific system, domain, or computational software. The applicability of the proposed approach is shown via an end-to-end example from text to model of a simple pendulum, showing improved performance compared to results yielded by LLMs only.

**Link**: [arxiv](http://arxiv.org/abs/2507.06803v1),  [pdf](http://arxiv.org/pdf/2507.06803v1)

**Tags**: cs.CL cs.AI cs.CE 



### ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual   Pretraining
**Authors**: Seonwu Kim, Yohan Na, Kihun Kim, Hanhee Cho, Geun Lim, Mintae Kim, Seongik Park, Ki Hyun Kim, Youngsub Han, Byoung-Ki Jeon

**Updated**: 2025-07-10T07:05:41Z

**Summary**: The emergence of open-source large language models (LLMs) has expanded opportunities for enterprise applications; however, many organizations still lack the infrastructure to deploy and maintain large-scale models. As a result, small LLMs (sLLMs) have become a practical alternative, despite their inherent performance limitations. While Domain Adaptive Continual Pretraining (DACP) has been previously explored as a method for domain adaptation, its utility in commercial applications remains under-examined. In this study, we validate the effectiveness of applying a DACP-based recipe across diverse foundation models and service domains. Through extensive experiments and real-world evaluations, we demonstrate that DACP-applied sLLMs achieve substantial gains in target domain performance while preserving general capabilities, offering a cost-efficient and scalable solution for enterprise-level deployment.

**Link**: [arxiv](http://arxiv.org/abs/2507.06795v2),  [pdf](http://arxiv.org/pdf/2507.06795v2)

**Tags**: cs.CL cs.AI cs.LG 



### GuidedBench: Measuring and Mitigating the Evaluation Discrepancies of   In-the-wild LLM Jailbreak Methods
**Authors**: Ruixuan Huang, Xunguang Wang, Zongjie Li, Daoyuan Wu, Shuai Wang

**Updated**: 2025-07-09T12:13:12Z

**Summary**: Despite the growing interest in jailbreak methods as an effective red-teaming tool for building safe and responsible large language models (LLMs), flawed evaluation system designs have led to significant discrepancies in their effectiveness assessments. We conduct a systematic measurement study based on 37 jailbreak studies since 2022, focusing on both the methods and the evaluation systems they employ. We find that existing evaluation systems lack case-specific criteria, resulting in misleading conclusions about their effectiveness and safety implications. This paper advocates a shift to a more nuanced, case-by-case evaluation paradigm. We introduce GuidedBench, a novel benchmark comprising a curated harmful question dataset, detailed case-by-case evaluation guidelines and an evaluation system integrated with these guidelines -- GuidedEval. Experiments demonstrate that GuidedBench offers more accurate measurements of jailbreak performance, enabling meaningful comparisons across methods and uncovering new insights overlooked in previous evaluations. GuidedEval reduces inter-evaluator variance by at least 76.03\%. Furthermore, we observe that incorporating guidelines can enhance the effectiveness of jailbreak methods themselves, offering new insights into both attack strategies and evaluation paradigms.

**Link**: [arxiv](http://arxiv.org/abs/2502.16903v2),  [pdf](http://arxiv.org/pdf/2502.16903v2)

**Tags**: cs.CL cs.CR 



### Checklist Engineering Empowers Multilingual LLM Judges
**Authors**: Mohammad Ghiasvand Mohammadkhani, Hamid Beigy

**Updated**: 2025-07-09T12:03:06Z

**Summary**: Automated text evaluation has long been a central issue in Natural Language Processing (NLP). Recently, the field has shifted toward using Large Language Models (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While promising and easily adaptable across tasks, this approach has seen limited exploration in multilingual contexts. Existing multilingual studies often rely on proprietary models or require extensive training data for fine-tuning, raising concerns about cost, time, and efficiency. In this paper, we propose Checklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free framework that uses checklist intuition for multilingual evaluation with an open-source model. Experiments across multiple languages and three benchmark datasets, under both pointwise and pairwise settings, show that our method generally surpasses the baselines and performs on par with the GPT-4o model.

**Link**: [arxiv](http://arxiv.org/abs/2507.06774v1),  [pdf](http://arxiv.org/pdf/2507.06774v1)

**Tags**: cs.CL 



### Tail-aware Adversarial Attacks: A Distributional Approach to Efficient   LLM Jailbreaking
**Authors**: Tim Beyer, Yan Scholten, Leo Schwinn, Stephan Günnemann

**Updated**: 2025-07-09T11:52:25Z

**Summary**: To guarantee safe and robust deployment of large language models (LLMs) at scale, it is critical to accurately assess their adversarial robustness. Existing adversarial attacks typically target harmful responses in single-point, greedy generations, overlooking the inherently stochastic nature of LLMs. In this paper, we propose a novel framework for adversarial robustness evaluation that explicitly models the entire output distribution, including tail-risks, providing better estimates for model robustness at scale. By casting the attack process as a resource allocation problem between optimization and sampling, we determine compute-optimal tradeoffs and show that integrating sampling into existing attacks boosts ASR by up to 48% and improves efficiency by up to two orders of magnitude. Our framework also enables us to analyze how different attack algorithms affect output harm distributions. Surprisingly, we find that most optimization strategies have little effect on output harmfulness. Finally, we introduce a data-free proof-of-concept objective based on entropy-maximization to demonstrate how our tail-aware perspective enables new optimization targets. Overall, our findings highlight the importance of tail-aware attacks and evaluation protocols to accurately assess and strengthen LLM safety.

**Link**: [arxiv](http://arxiv.org/abs/2507.04446v2),  [pdf](http://arxiv.org/pdf/2507.04446v2)

**Tags**: cs.LG 



### Leveraging LLMs for Semantic Conflict Detection via Unit Test Generation
**Authors**: Nathalia Barbosa, Paulo Borba, Léuson Da Silva

**Updated**: 2025-07-09T11:38:53Z

**Summary**: Semantic conflicts arise when a developer introduces changes to a codebase that unintentionally affect the behavior of changes integrated in parallel by other developers. Traditional merge tools are unable to detect such conflicts, so complementary tools like SMAT have been proposed. SMAT relies on generating and executing unit tests: if a test fails on the base version, passes on a developer's modified version, but fails again after merging with another developer's changes, a semantic conflict is indicated. While SMAT is effective at detecting conflicts, it suffers from a high rate of false negatives, partly due to the limitations of unit test generation tools such as Randoop and Evosuite. To investigate whether large language models (LLMs) can overcome these limitations, we propose and integrate a new test generation tool based on Code Llama 70B into SMAT. We explore the model's ability to generate tests using different interaction strategies, prompt contents, and parameter configurations. Our evaluation uses two samples: a benchmark with simpler systems from related work, and a more significant sample based on complex, real-world systems. We assess the effectiveness of the new SMAT extension in detecting conflicts. Results indicate that, although LLM-based test generation remains challenging and computationally expensive in complex scenarios, there is promising potential for improving semantic conflict detection.   --   Conflitos sem^anticos surgem quando um desenvolvedor introduz mudan\c{c}as em uma base de c\'odigo que afetam, de forma n~ao intencional, o comportamento de altera\c{c}~oes integradas em paralelo por outros desenvolvedores. Ferramentas tradicionais de merge n~ao conseguem detectar esse tipo de conflito, por isso ferramentas complementares como o SMAT foram propostas. O SMAT depende da gera\c{c}~ao e execu\c{c}~ao de testes de unidade: se um teste falha na vers~ao base, passa na vers~ao modificada por um desenvolvedor, mas volta a falhar ap\'os o merge com as mudan\c{c}as de outro desenvolvedor, um conflito sem^antico \'e identificado. Embora o SMAT seja eficaz na detec\c{c}~ao de conflitos, apresenta alta taxa de falsos negativos, em parte devido \`as limita\c{c}~oes das ferramentas de gera\c{c}~ao de testes como Randoop e Evosuite. Para investigar se modelos de linguagem de grande porte (LLMs) podem superar essas limita\c{c}~oes, propomos e integramos ao SMAT uma nova ferramenta de gera\c{c}~ao de testes baseada no Code Llama 70B. Exploramos a capacidade do modelo de gerar testes utilizando diferentes estrat\'egias de intera\c{c}~ao, conte\'udos de prompts e configura\c{c}~oes de par^ametros. Nossa avalia\c{c}~ao utiliza duas amostras: um benchmark com sistemas mais simples, usados em trabalhos relacionados, e uma amostra mais significativa baseada em sistemas complexos e reais. Avaliamos a efic\'acia da nova extens~ao do SMAT na detec\c{c}~ao de conflitos. Os resultados indicam que, embora a gera\c{c}~ao de testes por LLM em cen\'arios complexos ainda seja desafiadora e custosa computacionalmente, h\'a potencial promissor para aprimorar a detec\c{c}~ao de conflitos sem^anticos.

**Link**: [arxiv](http://arxiv.org/abs/2507.06762v1),  [pdf](http://arxiv.org/pdf/2507.06762v1)

**Tags**: cs.SE K.6.3 



### Finetuning Vision-Language Models as OCR Systems for Low-Resource   Languages: A Case Study of Manchu
**Authors**: Yan Hon Michael Chung, Donghyeok Choi

**Updated**: 2025-07-09T11:38:20Z

**Summary**: Manchu, a critically endangered language essential for understanding early modern Eastern Eurasian history, lacks effective OCR systems that can handle real-world historical documents. This study develops high-performing OCR systems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B, Qwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using parameter-efficient training. LLaMA-3.2-11B achieved exceptional performance with 98.3\% word accuracy and 0.0024 character error rate on synthetic data, while crucially maintaining 93.1\% accuracy on real-world handwritten documents. Comparative evaluation reveals substantial advantages over traditional approaches: while a CRNN baseline achieved 99.8\% synthetic accuracy, it suffered severe degradation to 72.5\% on real documents. Our approach demonstrates effective synthetic-to-real domain transfer, providing a cost-effective solution deployable on accessible infrastructure. This work establishes a transferable framework for endangered language OCR that removes technical and financial barriers in digital humanities, enabling historians and linguists to process historical archives without specialized computing resources. Code and model weights are available at https://github.com/mic7ch1/ManchuAI-OCR.

**Link**: [arxiv](http://arxiv.org/abs/2507.06761v1),  [pdf](http://arxiv.org/pdf/2507.06761v1)

**Tags**: cs.CV 



### Reinforcement Learning-based Feature Generation Algorithm for Scientific   Data
**Authors**: Meng Xiao, Junfeng Zhou, Yuanchun Zhou

**Updated**: 2025-07-09T11:30:58Z

**Summary**: Feature generation (FG) aims to enhance the prediction potential of original data by constructing high-order feature combinations and removing redundant features. It is a key preprocessing step for tabular scientific data to improve downstream machine-learning model performance. Traditional methods face the following two challenges when dealing with the feature generation of scientific data: First, the effective construction of high-order feature combinations in scientific data necessitates profound and extensive domain-specific expertise. Secondly, as the order of feature combinations increases, the search space expands exponentially, imposing prohibitive human labor consumption. Advancements in the Data-Centric Artificial Intelligence (DCAI) paradigm have opened novel avenues for automating feature generation processes. Inspired by that, this paper revisits the conventional feature generation workflow and proposes the Multi-agent Feature Generation (MAFG) framework. Specifically, in the iterative exploration stage, multi-agents will construct mathematical transformation equations collaboratively, synthesize and identify feature combinations ex-hibiting high information content, and leverage a reinforcement learning mechanism to evolve their strategies. Upon completing the exploration phase, MAFG integrates the large language models (LLMs) to interpreta-tively evaluate the generated features of each significant model performance breakthrough. Experimental results and case studies consistently demonstrate that the MAFG framework effectively automates the feature generation process and significantly enhances various downstream scientific data mining tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.03498v2),  [pdf](http://arxiv.org/pdf/2507.03498v2)

**Tags**: cs.LG cs.AI 



### AI Agent Smart Contract Exploit Generation
**Authors**: Arthur Gervais, Liyi Zhou

**Updated**: 2025-07-09T11:25:39Z

**Summary**: We present A1, an agentic execution driven system that transforms any LLM into an end-to-end exploit generator. A1 has no hand-crafted heuristics and provides the agent with six domain-specific tools that enable autonomous vulnerability discovery. The agent can flexibly leverage these tools to understand smart contract behavior, generate exploit strategies, test them on blockchain states, and refine approaches based on execution feedback. All outputs are concretely validated to eliminate false positives.   The evaluation across 36 real-world vulnerable contracts on Ethereum and Binance Smart Chain demonstrates a 62.96% (17 out of 27) success rate on the VERITE benchmark. Beyond the VERITE dataset, A1 identified 9 additional vulnerable contracts, with 5 cases occurring after the strongest model's training cutoff date. Across all 26 successful cases, A1 extracts up to 8.59 million USD per case and 9.33 million USD total. Through 432 experiments across six LLMs, we analyze iteration-wise performance showing diminishing returns with average marginal gains of +9.7%, +3.7%, +5.1%, and +2.8% for iterations 2-5 respectively, with per-experiment costs ranging $0.01-$3.59. A Monte Carlo analysis of 19 historical attacks shows success probabilities of 85.9%-88.8% without detection delays.   We investigate whether an attacker or a defender benefits most from deploying A1 as a continuous on-chain scanning system. Our model shows that OpenAI's o3-pro maintains profitability up to a 30.0 days scanning delay at 0.100% vulnerability incidence rates, while faster models require >=1.000% rates to break-even. The findings exposes a troubling asymmetry: at 0.1% vulnerability rates, attackers achieve an on-chain scanning profitability at a \$6000 exploit value, while defenders require \$60000, raising fundamental questions about whether AI agents inevitably favor exploitation over defense.

**Link**: [arxiv](http://arxiv.org/abs/2507.05558v2),  [pdf](http://arxiv.org/pdf/2507.05558v2)

**Tags**: cs.CR cs.AI 



### Distributed Fault-Tolerant Multi-Robot Cooperative Localization in   Adversarial Environments
**Authors**: Tohid Kargar Tasooji, Ramviyas Parasuraman

**Updated**: 2025-07-09T11:15:19Z

**Summary**: In multi-robot systems (MRS), cooperative localization is a crucial task for enhancing system robustness and scalability, especially in GPS-denied or communication-limited environments. However, adversarial attacks, such as sensor manipulation, and communication jamming, pose significant challenges to the performance of traditional localization methods. In this paper, we propose a novel distributed fault-tolerant cooperative localization framework to enhance resilience against sensor and communication disruptions in adversarial environments. We introduce an adaptive event-triggered communication strategy that dynamically adjusts communication thresholds based on real-time sensing and communication quality. This strategy ensures optimal performance even in the presence of sensor degradation or communication failure. Furthermore, we conduct a rigorous analysis of the convergence and stability properties of the proposed algorithm, demonstrating its resilience against bounded adversarial zones and maintaining accurate state estimation. Robotarium-based experiment results show that our proposed algorithm significantly outperforms traditional methods in terms of localization accuracy and communication efficiency, particularly in adversarial settings. Our approach offers improved scalability, reliability, and fault tolerance for MRS, making it suitable for large-scale deployments in real-world, challenging environments.

**Link**: [arxiv](http://arxiv.org/abs/2507.06750v1),  [pdf](http://arxiv.org/pdf/2507.06750v1)

**Tags**: cs.RO cs.MA cs.SY eess.SY 



### LOVON: Legged Open-Vocabulary Object Navigator
**Authors**: Daojie Peng, Jiahang Cao, Qiang Zhang, Jun Ma

**Updated**: 2025-07-09T11:02:46Z

**Summary**: Object navigation in open-world environments remains a formidable and pervasive challenge for robotic systems, particularly when it comes to executing long-horizon tasks that require both open-world object detection and high-level task planning. Traditional methods often struggle to integrate these components effectively, and this limits their capability to deal with complex, long-range navigation missions. In this paper, we propose LOVON, a novel framework that integrates large language models (LLMs) for hierarchical task planning with open-vocabulary visual detection models, tailored for effective long-range object navigation in dynamic, unstructured environments. To tackle real-world challenges including visual jittering, blind zones, and temporary target loss, we design dedicated solutions such as Laplacian Variance Filtering for visual stabilization. We also develop a functional execution logic for the robot that guarantees LOVON's capabilities in autonomous navigation, task adaptation, and robust task completion. Extensive evaluations demonstrate the successful completion of long-sequence tasks involving real-time detection, search, and navigation toward open-vocabulary dynamic targets. Furthermore, real-world experiments across different legged robots (Unitree Go2, B2, and H1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.

**Link**: [arxiv](http://arxiv.org/abs/2507.06747v1),  [pdf](http://arxiv.org/pdf/2507.06747v1)

**Tags**: cs.RO cs.CV 



### Knockout LLM Assessment: Using Large Language Models for Evaluations   through Iterative Pairwise Comparisons
**Authors**: Isik Baran Sandan, Tu Anh Dinh, Jan Niehues

**Updated**: 2025-07-09T10:58:38Z

**Summary**: Large Language Models (LLMs) have shown to be effective evaluators across various domains such as machine translations or the scientific domain. Current LLM-as-a-Judge approaches rely mostly on individual assessments or a single round of pairwise assessments, preventing the judge LLM from developing a global ranking perspective. To address this, we present Knockout Assessment, an LLM-asa Judge method using a knockout tournament system with iterative pairwise comparisons. Experiments across three LLMs on two datasets show that knockout assessment improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations, aligning LLM assessments more closely with human scoring.

**Link**: [arxiv](http://arxiv.org/abs/2506.03785v3),  [pdf](http://arxiv.org/pdf/2506.03785v3)

**Tags**: cs.CL cs.AI I.2.7 



### PenTest2.0: Towards Autonomous Privilege Escalation Using GenAI
**Authors**: Haitham S. Al-Sinani, Chris J. Mitchell

**Updated**: 2025-07-09T10:56:32Z

**Summary**: Ethical hacking today relies on highly skilled practitioners executing complex sequences of commands, which is inherently time-consuming, difficult to scale, and prone to human error. To help mitigate these limitations, we previously introduced 'PenTest++', an AI-augmented system combining automation with generative AI supporting ethical hacking workflows. However, a key limitation of PenTest++ was its lack of support for privilege escalation, a crucial element of ethical hacking. In this paper we present 'PenTest2.0', a substantial evolution of PenTest++ supporting automated privilege escalation driven entirely by Large Language Model reasoning. It also incorporates several significant enhancements: 'Retrieval-Augmented Generation', including both one-line and offline modes; 'Chain-of-Thought' prompting for intermediate reasoning; persistent 'PenTest Task Trees' to track goal progression across turns; and the optional integration of human-authored hints. We describe how it operates, present a proof-of-concept prototype, and discuss its benefits and limitations. We also describe application of the system to a controlled Linux target, showing it can carry out multi-turn, adaptive privilege escalation. We explain the rationale behind its core design choices, and provide comprehensive testing results and cost analysis. Our findings indicate that 'PenTest2.0' represents a meaningful step toward practical, scalable, AI-automated penetration testing, whilst highlighting the shortcomings of generative AI systems, particularly their sensitivity to prompt structure, execution context, and semantic drift, reinforcing the need for further research and refinement in this emerging space.   Keywords: AI, Ethical Hacking, Privilege Escalation, GenAI, ChatGPT, LLM (Large Language Model), HITL (Human-in-the-Loop)

**Link**: [arxiv](http://arxiv.org/abs/2507.06742v1),  [pdf](http://arxiv.org/pdf/2507.06742v1)

**Tags**: cs.CR 



### LLM-based User Profile Management for Recommender System
**Authors**: Seunghwan Bang, Hwanjun Song

**Updated**: 2025-07-09T10:55:06Z

**Summary**: The rapid advancement of Large Language Models (LLMs) has opened new opportunities in recommender systems by enabling zero-shot recommendation without conventional training. Despite their potential, most existing works rely solely on users' purchase histories, leaving significant room for improvement by incorporating user-generated textual data, such as reviews and product descriptions. Addressing this gap, we propose PURE, a novel LLM-based recommendation framework that builds and maintains evolving user profiles by systematically extracting and summarizing key information from user reviews. PURE consists of three core components: a Review Extractor for identifying user preferences and key product features, a Profile Updater for refining and updating user profiles, and a Recommender for generating personalized recommendations using the most current profile. To evaluate PURE, we introduce a continuous sequential recommendation task that reflects real-world scenarios by adding reviews over time and updating predictions incrementally. Our experimental results on Amazon datasets demonstrate that PURE outperforms existing LLM-based methods, effectively leveraging long-term user information while managing token limitations.

**Link**: [arxiv](http://arxiv.org/abs/2502.14541v2),  [pdf](http://arxiv.org/pdf/2502.14541v2)

**Tags**: cs.CL 



### Residual Prior-driven Frequency-aware Network for Image Fusion
**Authors**: Guan Zheng, Xue Wang, Wenhua Qian, Peng Liu, Runzhuo Ma

**Updated**: 2025-07-09T10:48:00Z

**Summary**: Image fusion aims to integrate complementary information across modalities to generate high-quality fused images, thereby enhancing the performance of high-level vision tasks. While global spatial modeling mechanisms show promising results, constructing long-range feature dependencies in the spatial domain incurs substantial computational costs. Additionally, the absence of ground-truth exacerbates the difficulty of capturing complementary features effectively. To tackle these challenges, we propose a Residual Prior-driven Frequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a dual-branch feature extraction framework: the Residual Prior Module (RPM) extracts modality-specific difference information from residual maps, thereby providing complementary priors for fusion; the Frequency Domain Fusion Module (FDFM) achieves efficient global feature modeling and integration through frequency-domain convolution. Additionally, the Cross Promotion Module (CPM) enhances the synergistic perception of local details and global structures through bidirectional feature interaction. During training, we incorporate an auxiliary decoder and saliency structure loss to strengthen the model's sensitivity to modality-specific differences. Furthermore, a combination of adaptive weight-based frequency contrastive loss and SSIM loss effectively constrains the solution space, facilitating the joint capture of local details and global features while ensuring the retention of complementary information. Extensive experiments validate the fusion performance of RPFNet, which effectively integrates discriminative features, enhances texture details and salient objects, and can effectively facilitate the deployment of the high-level vision task.

**Link**: [arxiv](http://arxiv.org/abs/2507.06735v1),  [pdf](http://arxiv.org/pdf/2507.06735v1)

**Tags**: cs.CV cs.LG cs.MM 



### Hierarchical Feature Alignment for Gloss-Free Sign Language Translation
**Authors**: Sobhan Asasi, Mohamed Ilyes Lakhal, Richard Bowden

**Updated**: 2025-07-09T10:45:50Z

**Summary**: Sign Language Translation (SLT) attempts to convert sign language videos into spoken sentences. However, many existing methods struggle with the disparity between visual and textual representations during end-to-end learning. Gloss-based approaches help to bridge this gap by leveraging structured linguistic information. While, gloss-free methods offer greater flexibility and remove the burden of annotation, they require effective alignment strategies. Recent advances in Large Language Models (LLMs) have enabled gloss-free SLT by generating text-like representations from sign videos. In this work, we introduce a novel hierarchical pre-training strategy inspired by the structure of sign language, incorporating pseudo-glosses and contrastive video-language alignment. Our method hierarchically extracts features at frame, segment, and video levels, aligning them with pseudo-glosses and the spoken sentence to enhance translation quality. Experiments demonstrate that our approach improves BLEU-4 and ROUGE scores while maintaining efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2507.06732v1),  [pdf](http://arxiv.org/pdf/2507.06732v1)

**Tags**: cs.CV 



### On the Effect of Uncertainty on Layer-wise Inference Dynamics
**Authors**: Sunwoo Kim, Haneul Yoo, Alice Oh

**Updated**: 2025-07-09T10:30:09Z

**Summary**: Understanding how large language models (LLMs) internally represent and process their predictions is central to detecting uncertainty and preventing hallucinations. While several studies have shown that models encode uncertainty in their hidden states, it is underexplored how this affects the way they process such hidden states. In this work, we demonstrate that the dynamics of output token probabilities across layers for certain and uncertain outputs are largely aligned, revealing that uncertainty does not seem to affect inference dynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to analyze the layer-wise probability trajectories of final prediction tokens across 11 datasets and 5 models. Using incorrect predictions as those with higher epistemic uncertainty, our results show aligned trajectories for certain and uncertain predictions that both observe abrupt increases in confidence at similar layers. We balance this finding by showing evidence that more competent models may learn to process uncertainty differently. Our findings challenge the feasibility of leveraging simplistic methods for detecting uncertainty at inference. More broadly, our work demonstrates how interpretability methods may be used to investigate the way uncertainty affects inference.

**Link**: [arxiv](http://arxiv.org/abs/2507.06722v1),  [pdf](http://arxiv.org/pdf/2507.06722v1)

**Tags**: cs.CL cs.LG 



### A Neural Representation Framework with LLM-Driven Spatial Reasoning for   Open-Vocabulary 3D Visual Grounding
**Authors**: Zhenyang Liu, Sixiao Zheng, Siyu Chen, Cairong Zhao, Longfei Liang, Xiangyang Xue, Yanwei Fu

**Updated**: 2025-07-09T10:20:38Z

**Summary**: Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose SpatialReasoner, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.

**Link**: [arxiv](http://arxiv.org/abs/2507.06719v1),  [pdf](http://arxiv.org/pdf/2507.06719v1)

**Tags**: cs.CV cs.RO 



### CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and   Context Aware Text Generation with LLMs
**Authors**: Garapati Keerthana, Manik Gupta

**Updated**: 2025-07-09T10:13:38Z

**Summary**: Large language models (LLMs), including zero-shot and few-shot paradigms, have shown promising capabilities in clinical text generation. However, real-world applications face two key challenges: (1) patient data is highly unstructured, heterogeneous, and scattered across multiple note types and (2) clinical notes are often long and semantically dense, making naive prompting infeasible due to context length constraints and the risk of omitting clinically relevant information.   We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a domain-specific framework for structured and clinically grounded text generation using LLMs. It incorporates a novel hierarchical chunking strategy that respects clinical document structure and introduces a task-specific dual-stage retrieval mechanism. The global stage identifies relevant note types using evidence-based queries, while the local stage extracts high-value content within those notes creating relevance at both document and section levels.   We apply the system to generate structured progress notes for individual hospital visits using 15 clinical note types from the MIMIC-III dataset. Experiments show that it preserves temporal and semantic alignment across visits, achieving an average alignment score of 87.7%, surpassing the 80.7% baseline from real clinician-authored notes. The generated outputs also demonstrate high consistency across LLMs, reinforcing deterministic behavior essential for reproducibility, reliability, and clinical trust.

**Link**: [arxiv](http://arxiv.org/abs/2507.06715v1),  [pdf](http://arxiv.org/pdf/2507.06715v1)

**Tags**: cs.CL cs.AI cs.IR 



### Do Larger Language Models Imply Better Generalization? A Pretraining   Scaling Law for Implicit Reasoning
**Authors**: Xinyi Wang, Shawn Tan, Mingyu Jin, William Yang Wang, Rameswar Panda, Yikang Shen

**Updated**: 2025-07-09T10:08:34Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. In this paper, we introduce a synthetic multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. Our reasoning task involves completing missing edges in the graph, which requires advanced multi-hop reasoning and mimics real-world reasoning scenarios. To evaluate this, we pretrain language models (LMs) from scratch solely on triples from the incomplete graph and assess their ability to infer the missing edges. Interestingly, we observe that overparameterization can impair reasoning performance due to excessive memorization. We investigate different factors that affect this U-shaped loss curve, including graph structure, model size, and training steps. To predict the optimal model size for a specific knowledge graph, we find an empirical scaling that linearly maps the knowledge graph search entropy to the optimal model size. This work provides new insights into the relationship between scaling and reasoning in LLMs, shedding light on possible ways to optimize their performance for reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.03635v2),  [pdf](http://arxiv.org/pdf/2504.03635v2)

**Tags**: cs.AI cs.CL 



### Spatial-Temporal Aware Visuomotor Diffusion Policy Learning
**Authors**: Zhenyang Liu, Yikai Wang, Kuanning Wang, Longfei Liang, Xiangyang Xue, Yanwei Fu

**Updated**: 2025-07-09T10:08:15Z

**Summary**: Visual imitation learning is effective for robots to learn versatile tasks. However, many existing methods rely on behavior cloning with supervised historical trajectories, limiting their 3D spatial and 4D spatiotemporal awareness. Consequently, these methods struggle to capture the 3D structures and 4D spatiotemporal relationships necessary for real-world deployment. In this work, we propose 4D Diffusion Policy (DP4), a novel visual imitation learning method that incorporates spatiotemporal awareness into diffusion-based policies. Unlike traditional approaches that rely on trajectory cloning, DP4 leverages a dynamic Gaussian world model to guide the learning of 3D spatial and 4D spatiotemporal perceptions from interactive environments. Our method constructs the current 3D scene from a single-view RGB-D observation and predicts the future 3D scene, optimizing trajectory generation by explicitly modeling both spatial and temporal dependencies. Extensive experiments across 17 simulation tasks with 173 variants and 3 real-world robotic tasks demonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods, improving the average simulation task success rate by 16.4% (Adroit), 14% (DexArt), and 6.45% (RLBench), and the average real-world robotic task success rate by 8.6%.

**Link**: [arxiv](http://arxiv.org/abs/2507.06710v1),  [pdf](http://arxiv.org/pdf/2507.06710v1)

**Tags**: cs.RO 



### One Size Does Not Fit All: Investigating Efficacy of Perplexity in   Detecting LLM-Generated Code
**Authors**: Jinwei Xu, He Zhang, Yanjing Yang, Lanxin Yang, Zeru Cheng, Jun Lyu, Bohan Liu, Xin Zhou, Alberto Bacchelli, Yin Kia Chiam, Thiam Kian Chiew

**Updated**: 2025-07-09T10:04:06Z

**Summary**: Large language model-generated code (LLMgCode) has become increasingly common in software development. So far LLMgCode has more quality issues than human-authored code (HaCode). It is common for LLMgCode to mix with HaCode in a code change, while the change is signed by only human developers, without being carefully examined. Many automated methods have been proposed to detect LLMgCode from HaCode, in which the perplexity-based method (PERPLEXITY for short) is the state-of-the-art method. However, the efficacy evaluation of PERPLEXITY has focused on detection accuracy. Yet it is unclear whether PERPLEXITY is good enough in a wider range of realistic evaluation settings. To this end, we carry out a family of experiments to compare PERPLEXITY against feature- and pre-training-based methods from three perspectives: detection accuracy, detection speed, and generalization capability. The experimental results show that PERPLEXITY has the best generalization capability while having limited detection accuracy and detection speed. Based on that, we discuss the strengths and limitations of PERPLEXITY, e.g., PERPLEXITY is unsuitable for high-level programming languages. Finally, we provide recommendations to improve PERPLEXITY and apply it in practice. As the first large-scale investigation on detecting LLMgCode from HaCode, this article provides a wide range of findings for future improvement.

**Link**: [arxiv](http://arxiv.org/abs/2412.16525v2),  [pdf](http://arxiv.org/pdf/2412.16525v2)

**Tags**: cs.SE 



### QUITE: A Query Rewrite System Beyond Rules with LLM Agents
**Authors**: Yuyang Song, Hanxu Yan, Jiale Lao, Yibo Wang, Yufei Li, Yuanchun Zhou, Jianguo Wang, Mingjie Tang

**Updated**: 2025-07-09T09:51:35Z

**Summary**: Query rewrite transforms SQL queries into semantically equivalent forms that run more efficiently. Existing approaches mainly rely on predefined rewrite rules, but they handle a limited subset of queries and can cause performance regressions. This limitation stems from three challenges of rule-based query rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite rules do not generalize to new query patterns, and (3) some rewrite techniques cannot be expressed as fixed rules. Motivated by the fact that human experts exhibit significantly better rewrite ability but suffer from scalability, and Large Language Models (LLMs) have demonstrated nearly human-level semantic and reasoning abilities, we propose a new approach of using LLMs to rewrite SQL queries beyond rules. Due to the hallucination problems in LLMs, directly applying LLMs often leads to nonequivalent and suboptimal queries. To address this issue, we propose QUITE (query rewrite), a training-free and feedback-aware system based on LLM agents that rewrites SQL queries into semantically equivalent forms with significantly better performance, covering a broader range of query patterns and rewrite strategies compared to rule-based methods. Firstly, we design a multi-agent framework controlled by a finite state machine (FSM) to equip LLMs with the ability to use external tools and enhance the rewrite process with real-time database feedback. Secondly, we develop a rewrite middleware to enhance the ability of LLMs to generate optimized query equivalents. Finally, we employ a novel hint injection technique to improve execution plans for rewritten queries. Extensive experiments show that QUITE reduces query execution time by up to 35.8% over state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering query cases that earlier systems did not handle.

**Link**: [arxiv](http://arxiv.org/abs/2506.07675v2),  [pdf](http://arxiv.org/pdf/2506.07675v2)

**Tags**: cs.DB cs.AI 



### FlexGaussian: Flexible and Cost-Effective Training-Free Compression for   3D Gaussian Splatting
**Authors**: Boyuan Tian, Qizhe Gao, Siran Xianyu, Xiaotong Cui, Minjia Zhang

**Updated**: 2025-07-09T09:00:52Z

**Summary**: 3D Gaussian splatting has become a prominent technique for representing and rendering complex 3D scenes, due to its high fidelity and speed advantages. However, the growing demand for large-scale models calls for effective compression to reduce memory and computation costs, especially on mobile and edge devices with limited resources. Existing compression methods effectively reduce 3D Gaussian parameters but often require extensive retraining or fine-tuning, lacking flexibility under varying compression constraints.   In this paper, we introduce FlexGaussian, a flexible and cost-effective method that combines mixed-precision quantization with attribute-discriminative pruning for training-free 3D Gaussian compression. FlexGaussian eliminates the need for retraining and adapts easily to diverse compression targets. Evaluation results show that FlexGaussian achieves up to 96.4% compression while maintaining high rendering quality (<1 dB drop in PSNR), and is deployable on mobile devices. FlexGaussian delivers high compression ratios within seconds, being 1.7-2.1x faster than state-of-the-art training-free methods and 10-100x faster than training-involved approaches. The code is being prepared and will be released soon at: https://github.com/Supercomputing-System-AI-Lab/FlexGaussian

**Link**: [arxiv](http://arxiv.org/abs/2507.06671v1),  [pdf](http://arxiv.org/pdf/2507.06671v1)

**Tags**: cs.CV 



### Silent Failures in Stateless Systems: Rethinking Anomaly Detection for   Serverless Computing
**Authors**: Chanh Nguyen, Erik Elmroth, Monowar Bhuyan

**Updated**: 2025-07-09T08:59:36Z

**Summary**: Serverless computing has redefined cloud application deployment by abstracting infrastructure and enabling on-demand, event-driven execution, thereby enhancing developer agility and scalability. However, maintaining consistent application performance in serverless environments remains a significant challenge. The dynamic and transient nature of serverless functions makes it difficult to distinguish between benign and anomalous behavior, which in turn undermines the effectiveness of traditional anomaly detection methods. These conventional approaches, designed for stateful and long-running services, struggle in serverless settings where executions are short-lived, functions are isolated, and observability is limited.   In this first comprehensive vision paper on anomaly detection for serverless systems, we systematically explore the unique challenges posed by this paradigm, including the absence of persistent state, inconsistent monitoring granularity, and the difficulty of correlating behaviors across distributed functions. We further examine a range of threats that manifest as anomalies, from classical Denial-of-Service (DoS) attacks to serverless-specific threats such as Denial-of-Wallet (DoW) and cold start amplification. Building on these observations, we articulate a research agenda for next-generation detection frameworks that address the need for context-aware, multi-source data fusion, real-time, lightweight, privacy-preserving, and edge-cloud adaptive capabilities.   Through the identification of key research directions and design principles, we aim to lay the foundation for the next generation of anomaly detection in cloud-native, serverless ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2507.04969v2),  [pdf](http://arxiv.org/pdf/2507.04969v2)

**Tags**: cs.DC 



### Robust Longitudinal-lateral Look-ahead Pursuit Path-Following Control:   Fast Finite-Time Stability Guarantee
**Authors**: Zimao Sheng, Hong'an Yang, Shuxiang Yang, Zirui Yu

**Updated**: 2025-07-09T08:56:43Z

**Summary**: This paper addresses the challenging problem of robust path-following for fixed-wing unmanned aerial vehicles (UAVs) in complex environments with bounded external disturbances and non-smooth predefined paths. Due to the unique aerodynamic characteristics and flight constraints of fixed-wing UAVs, achieving accurate and fast stable path following remains difficult, especially in low-altitude mountainous terrains, urban landscapes, and under wind disturbances. Most existing path-following guidance laws often struggle to ensure fast stabilization under unknown bounded disturbances while maintaining sufficient robustness, and there is a lack of research on optimizing robustness for non-smooth paths under flight constraints. This paper addresses these issues by proposing a constraints-based robust path-following controller. Firstly, from the perspective of global random attractor, we innovatively introduce robustness metrics that quantify both the exponential convergence rate and the range of the ultimate attractor set. Secondly, building on these metrics, we develop a robust longitudinal-lateral look-ahead pursuit (RLLP) guidance law for fixed-wing UAVs, specifically considering the flight path angle and track angle under external disturbances. Thirdly, we also derive an optimized version (Optimal-RLLP) to enhance the robustness metrics, and elaborate on the sufficient conditions for fast finite-time stability, ensuring the guidance law achieves finite-time stability and robustness with reduced sensitivity to constrained uncertainties. The simulation results validate the proposed guidance law's feasibility, optimality and robustness under atmospheric disturbances using a high-fidelity simulation platform and provide key principle for practical deployment.

**Link**: [arxiv](http://arxiv.org/abs/2505.16407v2),  [pdf](http://arxiv.org/pdf/2505.16407v2)

**Tags**: eess.SY cs.SY 



### iDynamics: A Novel Framework for Evaluating Microservice Scheduling   Policies under Controllable Dynamics in Cloud-Edge Continuum
**Authors**: Ming Chen, Muhammed Tawfiqul Islam, Maria Rodriguez Read, Rajkumar Buyya

**Updated**: 2025-07-09T08:51:13Z

**Summary**: Designing and evaluating microservice scheduling policies is challenging, particularly under dynamic conditions such as complex call-graph dependencies and varying cross-node networking conditions. Moreover, deploying such systems in real-world cloud-edge environments to evaluate scheduling strategies is often impractical due to complexity, cost, and limited accessibility. This highlights the need for an emulation framework that can faithfully emulate the characteristics of the cloud-edge continuum. These characteristics include dynamic topology changes, latency-sensitive service chains, and varying networking conditions, all of which must be accurately modeled for meaningful evaluation. In this work, iDynamics addresses these challenges by providing a configurable and extensible framework that captures the essential dynamics of running microservice applications in cloud-edge environments, enabling systematic development and testing of microservice scheduling strategies. The framework comprises modular components, such as the Graph Dynamics Analyzer, Networking Dynamics Manager, and Scheduling Policy Extender. This enables fine-grained environmental control and facilitates systematic comparisons of different scheduling strategies. Extensive experiments on a real cloud-edge testbed demonstrate that iDynamics effectively captures diverse dynamic scenarios encountered in microservice deployments, offering a robust solution for designing and evaluating different policies under realistic and controllable conditions.

**Link**: [arxiv](http://arxiv.org/abs/2503.16029v3),  [pdf](http://arxiv.org/pdf/2503.16029v3)

**Tags**: cs.DC 



### Automating IRAC Analysis in Malaysian Contract Law using a   Semi-Structured Knowledge Base
**Authors**: Xiaoxi Kang, Lizhen Qu, Lay-Ki Soon, Zhuang Li, Adnan Trakic

**Updated**: 2025-07-09T08:47:59Z

**Summary**: The effectiveness of Large Language Models (LLMs) in legal reasoning is often limited due to the unique legal terminologies and the necessity for highly specialized knowledge. These limitations highlight the need for high-quality data tailored for complex legal reasoning tasks. This paper introduces LegalSemi, a benchmark specifically curated for legal scenario analysis. LegalSemi comprises 54 legal scenarios, each rigorously annotated by legal experts, based on the comprehensive IRAC (Issue, Rule, Application, Conclusion) framework from Malaysian Contract Law. In addition, LegalSemi is accompanied by a structured knowledge base (SKE). A series of experiments were conducted to assess the usefulness of LegalSemi for IRAC analysis. The experimental results demonstrate the effectiveness of incorporating the SKE for issue identification, rule retrieval, application and conclusion generation using four different LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2406.13217v2),  [pdf](http://arxiv.org/pdf/2406.13217v2)

**Tags**: cs.CL 



### Probing and Steering Evaluation Awareness of Language Models
**Authors**: Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofstätter

**Updated**: 2025-07-09T08:46:58Z

**Summary**: Language models can distinguish between testing and deployment phases -- a capability known as evaluation awareness. This has significant safety and policy implications, potentially undermining the reliability of evaluations that are central to AI governance frameworks and voluntary industry commitments. In this paper, we study evaluation awareness in Llama-3.3-70B-Instruct. We show that linear probes can separate real-world evaluation and deployment prompts, suggesting that current models internally represent this distinction. We also find that current safety evaluations are correctly classified by the probes, suggesting that they already appear artificial or inauthentic to models. Our findings underscore the importance of ensuring trustworthy evaluations and understanding deceptive capabilities. More broadly, our work showcases how model internals may be leveraged to support blackbox methods in safety audits, especially for future models more competent at evaluation awareness and deception.

**Link**: [arxiv](http://arxiv.org/abs/2507.01786v2),  [pdf](http://arxiv.org/pdf/2507.01786v2)

**Tags**: cs.CL cs.AI 



### AHCPTQ: Accurate and Hardware-Compatible Post-Training Quantization for   Segment Anything Model
**Authors**: Wenlun Zhang, Yunshan Zhong, Shimpei Ando, Kentaro Yoshioka

**Updated**: 2025-07-09T08:26:21Z

**Summary**: The Segment Anything Model (SAM) has demonstrated strong versatility across various visual tasks. However, its large storage requirements and high computational cost pose challenges for practical deployment. Post-training quantization (PTQ) has emerged as an effective strategy for efficient deployment, but we identify two key challenges in SAM that hinder the effectiveness of existing PTQ methods: the heavy-tailed and skewed distribution of post-GELU activations, and significant inter-channel variation in linear projection activations. To address these challenges, we propose AHCPTQ, an accurate and hardware-efficient PTQ method for SAM. AHCPTQ introduces hardware-compatible Hybrid Log-Uniform Quantization (HLUQ) to manage post-GELU activations, employing log2 quantization for dense small values and uniform quantization for sparse large values to enhance quantization resolution. Additionally, AHCPTQ incorporates Channel-Aware Grouping (CAG) to mitigate inter-channel variation by progressively clustering activation channels with similar distributions, enabling them to share quantization parameters and improving hardware efficiency. The combination of HLUQ and CAG not only enhances quantization effectiveness but also ensures compatibility with efficient hardware execution. For instance, under the W4A4 configuration on the SAM-L model, AHCPTQ achieves 36.6% mAP on instance segmentation with the DINO detector, while achieving a 7.89x speedup and 8.64x energy efficiency over its floating-point counterpart in FPGA implementation.

**Link**: [arxiv](http://arxiv.org/abs/2503.03088v2),  [pdf](http://arxiv.org/pdf/2503.03088v2)

**Tags**: cs.CV cs.AR cs.LG 



### SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in   Real-World Applications
**Authors**: Jinyang Li, Xiaolong Li, Ge Qu, Per Jacobsson, Bowen Qin, Binyuan Hui, Shuzheng Si, Nan Huo, Xiaohan Xu, Yue Zhang, Ziwei Tang, Yuanshuai Li, Florensia Widjaja, Xintong Zhu, Feige Zhou, Yongfeng Huang, Yannis Papakonstantinou, Fatma Ozcan, Chenhao Ma, Reynold Cheng

**Updated**: 2025-07-09T08:22:28Z

**Summary**: Resolution of complex SQL issues persists as a significant bottleneck in real-world database applications. Current Large Language Models (LLMs), while adept at text-to-SQL translation, have not been rigorously evaluated on the more challenging task of debugging SQL issues. To address this gap, we introduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530 PostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks (BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within new environments to facilitate rigorous evaluation. Baseline evaluations underscore the task's complexity, with the leading reasoning model O3-Mini achieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on BIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks is crucial for empowering local development while safeguarding data privacy. Therefore, we present Six-Gym (Sql-fIX-Gym), a training environment for elevating open-source model capabilities for SQL issue debugging. This environment leverages SQL-Rewind strategy, which automatically generates executable issue-solution datasets by reverse-engineering issues from verified SQLs. However, popular trajectory-based fine-tuning methods do not explore substantial supervisory signals. We further propose f-Plan Boosting, which extracts high-level debugging plans from SQL solutions, enabling teacher LLMs to produce 73.7% more successful trajectories for training. We integrate these components into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B, Bird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on BIRD-CRITIC-Multi, surpassing leading proprietary models such as Claude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing sophisticated SQL-debugging capabilities. The leaderboard and source code are available: https://bird-critic.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2506.18951v2),  [pdf](http://arxiv.org/pdf/2506.18951v2)

**Tags**: cs.DB cs.AI 



### Are They the Same? Exploring Visual Correspondence Shortcomings of   Multimodal LLMs
**Authors**: Yikang Zhou, Tao Zhang, Shilin Xu, Shihao Chen, Qianyu Zhou, Yunhai Tong, Shunping Ji, Jiangning Zhang, Lu Qi, Xiangtai Li

**Updated**: 2025-07-09T08:04:21Z

**Summary**: Recent advancements in multimodal large language models (MLLM) have shown a strong ability in visual perception, reasoning abilities, and vision-language understanding. However, the visual matching ability of MLLMs is rarely studied, despite finding the visual correspondence of objects is essential in computer vision. Our research reveals that the matching capabilities in recent MLLMs still exhibit systematic shortcomings, even with current strong MLLMs models, GPT-4o. In particular, we construct a Multimodal Visual Matching (MMVM) benchmark to fairly benchmark over 30 different MLLMs. The MMVM benchmark is built from 15 open-source datasets and Internet videos with manual annotation. We categorize the data samples of MMVM benchmark into eight aspects based on the required cues and capabilities to more comprehensively evaluate and analyze current MLLMs. In addition, we have designed an automatic annotation pipeline to generate the MMVM SFT dataset, including 220K visual matching data with reasoning annotation. To our knowledge, this is the first visual corresponding dataset and benchmark for the MLLM community. Finally, we present CoLVA, a novel contrastive MLLM with two novel technical designs: fine-grained vision expert with object-level contrastive learning and instruction augmentation strategy. The former learns instance discriminative tokens, while the latter further improves instruction following ability. CoLVA-InternVL2-4B achieves an overall accuracy (OA) of 49.80\% on the MMVM benchmark, surpassing GPT-4o and the best open-source MLLM, Qwen2VL-72B, by 7.15\% and 11.72\% OA, respectively. These results demonstrate the effectiveness of our MMVM SFT dataset and our novel technical designs. Code, benchmark, dataset, and models will be released.

**Link**: [arxiv](http://arxiv.org/abs/2501.04670v3),  [pdf](http://arxiv.org/pdf/2501.04670v3)

**Tags**: cs.CV 



### PBa-LLM: Privacy- and Bias-aware NLP using Named-Entity Recognition   (NER)
**Authors**: Gonzalo Mancera, Aythami Morales, Julian Fierrez, Ruben Tolosana, Alejandro Penna, Miguel Lopez-Duran, Francisco Jurado, Alvaro Ortigosa

**Updated**: 2025-07-09T08:02:08Z

**Summary**: The use of Natural Language Processing (NLP) in highstakes AI-based applications has increased significantly in recent years, especially since the emergence of Large Language Models (LLMs). However, despite their strong performance, LLMs introduce important legal/ ethical concerns, particularly regarding privacy, data protection, and transparency. Due to these concerns, this work explores the use of Named- Entity Recognition (NER) to facilitate the privacy-preserving training (or adaptation) of LLMs. We propose a framework that uses NER technologies to anonymize sensitive information in text data, such as personal identities or geographic locations. An evaluation of the proposed privacy-preserving learning framework was conducted to measure its impact on user privacy and system performance in a particular high-stakes and sensitive setup: AI-based resume scoring for recruitment processes. The study involved two language models (BERT and RoBERTa) and six anonymization algorithms (based on Presidio, FLAIR, BERT, and different versions of GPT) applied to a database of 24,000 candidate profiles. The findings indicate that the proposed privacy preservation techniques effectively maintain system performance while playing a critical role in safeguarding candidate confidentiality, thus promoting trust in the experimented scenario. On top of the proposed privacy-preserving approach, we also experiment applying an existing approach that reduces the gender bias in LLMs, thus finally obtaining our proposed Privacyand Bias-aware LLMs (PBa-LLMs). Note that the proposed PBa-LLMs have been evaluated in a particular setup (resume scoring), but are generally applicable to any other LLM-based AI application.

**Link**: [arxiv](http://arxiv.org/abs/2507.02966v2),  [pdf](http://arxiv.org/pdf/2507.02966v2)

**Tags**: cs.CL cs.AI cs.CR cs.LG 



### Expediting data extraction using a large language model (LLM) and   scoping review protocol: a methodological study within a complex scoping   review
**Authors**: James Stewart-Evans, Emma Wilson, Tessa Langley, Andrew Prayle, Angela Hands, Karen Exley, Jo Leonardi-Bee

**Updated**: 2025-07-09T07:50:55Z

**Summary**: The data extraction stages of reviews are resource-intensive, and researchers may seek to expediate data extraction using online (large language models) LLMs and review protocols. Claude 3.5 Sonnet was used to trial two approaches that used a review protocol to prompt data extraction from 10 evidence sources included in a case study scoping review. A protocol-based approach was also used to review extracted data. Limited performance evaluation was undertaken which found high accuracy for the two extraction approaches (83.3% and 100%) when extracting simple, well-defined citation details; accuracy was lower (9.6% and 15.8%) when extracting more complex, subjective data items. Considering all data items, both approaches had precision >90% but low recall (<25%) and F1 scores (<40%). The context of a complex scoping review, open response types and methodological approach likely impacted performance due to missed and misattributed data. LLM feedback considered the baseline extraction accurate and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of 38 (21.1%) to key findings data items were considered to potentially add value. However, when repeating the process with a dataset featuring deliberate errors, only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for expediency require more robust performance evaluation across a range of LLMs and review contexts with comparison to conventional prompt engineering approaches. We recommend researchers evaluate and report LLM performance if using them similarly to conduct data extraction or review extracted data. LLM feedback contributed to protocol adaptation and may assist future review protocol drafting.

**Link**: [arxiv](http://arxiv.org/abs/2507.06623v1),  [pdf](http://arxiv.org/pdf/2507.06623v1)

**Tags**: cs.CL cs.AI 



### FuDoBa: Fusing Document and Knowledge Graph-based Representations with   Bayesian Optimisation
**Authors**: Boshko Koloski, Senja Pollak, Roberto Navigli, Blaž Škrlj

**Updated**: 2025-07-09T07:49:55Z

**Summary**: Building on the success of Large Language Models (LLMs), LLM-based representations have dominated the document representation landscape, achieving great performance on the document embedding benchmarks. However, the high-dimensional, computationally expensive embeddings from LLMs tend to be either too generic or inefficient for domain-specific applications. To address these limitations, we introduce FuDoBa a Bayesian optimisation-based method that integrates LLM-based embeddings with domain-specific structured knowledge, sourced both locally and from external repositories like WikiData. This fusion produces low-dimensional, task-relevant representations while reducing training complexity and yielding interpretable early-fusion weights for enhanced classification performance. We demonstrate the effectiveness of our approach on six datasets in two domains, showing that when paired with robust AutoML-based classifiers, our proposed representation learning approach performs on par with, or surpasses, those produced solely by the proprietary LLM-based embedding baselines.

**Link**: [arxiv](http://arxiv.org/abs/2507.06622v1),  [pdf](http://arxiv.org/pdf/2507.06622v1)

**Tags**: cs.CL 



### Saffron-1: Safety Inference Scaling
**Authors**: Ruizhong Qiu, Gaotang Li, Tianxin Wei, Jingrui He, Hanghang Tong

**Updated**: 2025-07-09T07:47:59Z

**Summary**: Existing safety assurance research has primarily focused on training-phase alignment to instill safe behaviors into LLMs. However, recent studies have exposed these methods' susceptibility to diverse jailbreak attacks. Concurrently, inference scaling has significantly advanced LLM reasoning capabilities but remains unexplored in the context of safety assurance. Addressing this gap, our work pioneers inference scaling for robust and effective LLM safety against emerging threats. We reveal that conventional inference scaling techniques, despite their success in reasoning tasks, perform poorly in safety contexts, even falling short of basic approaches like Best-of-N Sampling. We attribute this inefficiency to a newly identified challenge, the exploration--efficiency dilemma, arising from the high computational overhead associated with frequent process reward model (PRM) evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference scaling paradigm tailored explicitly for safety assurance. Central to our approach is the introduction of a multifurcation reward model (MRM) that significantly reduces the required number of reward model evaluations. To operationalize this paradigm, we further propose: (i) a partial supervision training objective for MRM, (ii) a conservative exploration constraint to prevent out-of-distribution explorations, and (iii) a Trie-based key--value caching strategy that facilitates cache sharing across sequences during tree search. Extensive experiments validate the effectiveness of our method. Additionally, we publicly release our trained multifurcation reward model (Saffron-1) and the accompanying token-level safety reward dataset (Safety4M) to accelerate future research in LLM safety. Our code, model, and data are publicly available at https://github.com/q-rz/saffron , and our project homepage is at https://q-rz.github.io/p/saffron .

**Link**: [arxiv](http://arxiv.org/abs/2506.06444v2),  [pdf](http://arxiv.org/pdf/2506.06444v2)

**Tags**: cs.LG cs.AI cs.CR 



### LAURA: LLM-Assisted UAV Routing for AoI Minimization
**Authors**: Bisheng Wei, Ruichen Zhang, Ruihong Jiang, Mugen Peng, Dusit Niyato

**Updated**: 2025-07-09T07:30:20Z

**Summary**: With the rapid growth of the low-altitude economy, there is increasing demand for real-time data collection using UAV-assisted wireless sensor networks. This paper investigates the problem of minimizing the age of information (AoI) in UAV-assisted wireless sensor networks by optimizing the UAV flight routing. We formulate the AoI minimization task and propose a large language model (LLM)-assisted UAV routing algorithm (LAURA). LAURA employs an LLM as intelligent crossover operators within an evolutionary optimization framework to efficiently explore the solution space. Simulation results show that LAURA outperforms benchmark methods in reducing the maximum AoI, especially in scenarios with a large number of sensor nodes.

**Link**: [arxiv](http://arxiv.org/abs/2503.23132v2),  [pdf](http://arxiv.org/pdf/2503.23132v2)

**Tags**: cs.NI cs.IT math.IT 



### Graph Learning for Cooperative Cell-Free ISAC Systems: From Optimization   to Estimation
**Authors**: Peng Jiang, Ming Li, Rang Liu, Qian Liu

**Updated**: 2025-07-09T07:29:41Z

**Summary**: Cell-free integrated sensing and communication (ISAC) systems have emerged as a promising paradigm for sixth-generation (6G) networks, enabling simultaneous high-rate data transmission and high-precision radar sensing through cooperative distributed access points (APs). Fully exploiting these capabilities requires a unified design that bridges system-level optimization with multi-target parameter estimation. This paper proposes an end-to-end graph learning approach to close this gap, modeling the entire cell-free ISAC network as a heterogeneous graph to jointly design the AP mode selection, user association, precoding, and echo signal processing for multi-target position and velocity estimation. In particular, we propose two novel heterogeneous graph learning frameworks: a dynamic graph learning framework and a lightweight mirror-based graph attention network (mirror-GAT) framework. The dynamic graph learning framework employs structural and temporal attention mechanisms integrated with a three-dimensional convolutional neural network (3D-CNN), enabling superior performance and robustness in cell-free ISAC environments. Conversely, the mirror-GAT framework significantly reduces computational complexity and signaling overhead through a bi-level iterative structure with share adjacency. Simulation results validate that both proposed graph-learning-based frameworks achieve significant improvements in multi-target position and velocity estimation accuracy compared to conventional heuristic and optimization-based designs. Particularly, the mirror-GAT framework demonstrates substantial reductions in computational time and signaling overhead, underscoring its suitability for practical deployments.

**Link**: [arxiv](http://arxiv.org/abs/2507.06612v1),  [pdf](http://arxiv.org/pdf/2507.06612v1)

**Tags**: eess.SP 



### Nexus: Taming Throughput-Latency Tradeoff in LLM Serving via Efficient   GPU Sharing
**Authors**: Xiaoxiang Shi, Colin Cai, Junjia Du, Zhanda Zhu, Zhihao Jia

**Updated**: 2025-07-10T15:48:42Z

**Summary**: Current prefill-decode (PD) disaggregation is typically deployed at the level of entire serving engines, assigning separate GPUs to handle prefill and decode phases. While effective at reducing latency, this approach demands more hardware. To improve GPU utilization, Chunked Prefill mixes prefill and decode requests within the same batch, but introduces phase interference between prefill and decode.   While existing PD disaggregation solutions separate the phases across GPUs, we ask: can the same decoupling be achieved within a single serving engine? The key challenge lies in managing the conflicting resource requirements of prefill and decode when they share the same hardware. In this paper, we first show that chunked prefill requests cause interference with decode requests due to their distinct requirements for GPU resources. Second, we find that GPU resources exhibit diminishing returns. Beyond a saturation point, increasing GPU allocation yields negligible latency improvements. This insight enables us to split a single GPU's resources and dynamically allocate them to prefill and decode on the fly, effectively disaggregating the two phases within the same GPU.   Across a range of models and workloads, our system Nexus achieves up to 2.2x higher throughput, 20x lower TTFT, and 2.5x lower TBT than vLLM. It also outperforms SGLang with up to 2x higher throughput, 2x lower TTFT, and 1.7x lower TBT, and achieves 1.4x higher throughput than vLLM-disaggregation using only half the number of GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2507.06608v2),  [pdf](http://arxiv.org/pdf/2507.06608v2)

**Tags**: cs.DC cs.LG 



### Generalization in Reinforcement Learning for Radio Access Networks
**Authors**: Burak Demirel, Yu Wang, Cristian Tatino, Pablo Soldati

**Updated**: 2025-07-09T07:22:22Z

**Summary**: Modern RAN operate in highly dynamic and heterogeneous environments, where hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass such heuristics in constrained settings, the diversity of deployments and unpredictable radio conditions introduce major generalization challenges. Data-driven policies frequently overfit to training conditions, degrading performance in unseen scenarios. To address this, we propose a generalization-centered RL framework for RAN control that: (i) encodes cell topology and node attributes via attention-based graph representations; (ii) applies domain randomization to broaden the training distribution; and (iii) distributes data generation across multiple actors while centralizing training in a cloud-compatible architecture aligned with O-RAN principles. Although generalization increases computational and data-management complexity, our distributed design mitigates this by scaling data collection and training across diverse network conditions. Applied to downlink link adaptation in five 5G benchmarks, our policy improves average throughput and spectral efficiency by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and by >20% under high mobility. It matches specialized RL in full-buffer traffic and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks, respectively. In nine-cell deployments, GAT models offer 30% higher throughput over MLP baselines. These results, combined with our scalable architecture, offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

**Link**: [arxiv](http://arxiv.org/abs/2507.06602v1),  [pdf](http://arxiv.org/pdf/2507.06602v1)

**Tags**: cs.LG 



### FEVO: Financial Knowledge Expansion and Reasoning Evolution for Large   Language Models
**Authors**: Bo Pang, Yalu Ouyang, Hangfei Xu, Ziqi Jia, Panpan Li, Shengzhao Wen, Lu Wang, Shiyong Li, Yanpeng Wang

**Updated**: 2025-07-09T07:06:36Z

**Summary**: Advancements in reasoning for large language models (LLMs) have lead to significant performance improvements for LLMs in various fields such as mathematics and programming. However, research applying these advances to the financial domain, where considerable domain-specific knowledge is necessary to complete tasks, remains limited. To address this gap, we introduce FEVO (Financial Evolution), a multi-stage enhancement framework developed to enhance LLM performance in the financial domain. FEVO systemically enhances LLM performance by using continued pre-training (CPT) to expand financial domain knowledge, supervised fine-tuning (SFT) to instill structured, elaborate reasoning patterns, and reinforcement learning (RL) to further integrate the expanded financial domain knowledge with the learned structured reasoning. To ensure effective and efficient training, we leverage frontier reasoning models and rule-based filtering to curate FEVO-Train, high-quality datasets specifically designed for the different post-training phases. Using our framework, we train the FEVO series of models - C32B, S32B, R32B - from Qwen2.5-32B and evaluate them on seven benchmarks to assess financial and general capabilities, with results showing that FEVO-R32B achieves state-of-the-art performance on five financial benchmarks against much larger models as well as specialist models. More significantly, FEVO-R32B demonstrates markedly better performance than FEVO-R32B-0 (trained from Qwen2.5-32B-Instruct using only RL), thus validating the effectiveness of financial domain knowledge expansion and structured, logical reasoning distillation

**Link**: [arxiv](http://arxiv.org/abs/2507.06057v2),  [pdf](http://arxiv.org/pdf/2507.06057v2)

**Tags**: cs.AI cs.LG 



### Assessing Small Language Models for Code Generation: An Empirical Study   with Benchmarks
**Authors**: Md Mahade Hasan, Muhammad Waseem, Kai-Kristian Kemell, Jussi Rasku, Juha Ala-Rantala, Pekka Abrahamsson

**Updated**: 2025-07-09T06:49:35Z

**Summary**: The recent advancements of Small Language Models (SLMs) have opened new possibilities for efficient code generation. SLMs offer lightweight and cost-effective alternatives to Large Language Models (LLMs), making them attractive for use in resource-constrained environments. However, empirical understanding of SLMs, particularly their capabilities, limitations, and performance trade-offs in code generation remains limited. This study presents a comprehensive empirical evaluation of 20 open-source SLMs ranging from 0.4B to 10B parameters on five diverse code-related benchmarks (HumanEval, MBPP, Mercury, HumanEvalPack, and CodeXGLUE). The models are assessed along three dimensions: i) functional correctness of generated code, ii) computational efficiency and iii) performance across multiple programming languages. The findings of this study reveal that several compact SLMs achieve competitive results while maintaining a balance between performance and efficiency, making them viable for deployment in resource-constrained environments. However, achieving further improvements in accuracy requires switching to larger models. These models generally outperform their smaller counterparts, but they require much more computational power. We observe that for 10% performance improvements, models can require nearly a 4x increase in VRAM consumption, highlighting a trade-off between effectiveness and scalability. Besides, the multilingual performance analysis reveals that SLMs tend to perform better in languages such as Python, Java, and PHP, while exhibiting relatively weaker performance in Go, C++, and Ruby. However, statistical analysis suggests these differences are not significant, indicating a generalizability of SLMs across programming languages. Based on the findings, this work provides insights into the design and selection of SLMs for real-world code generation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.03160v3),  [pdf](http://arxiv.org/pdf/2507.03160v3)

**Tags**: cs.SE 



### Illuminating the Future: Nanophotonics for Future Green Technologies,   Precision Healthcare, and Optical Computing
**Authors**: Osama M. Halawa, Esraa Ahmed, Malk M. Abdelrazek, Yasser M. Nagy, Omar A. M. Abdelraouf

**Updated**: 2025-07-09T06:37:12Z

**Summary**: Nanophotonics, an interdisciplinary field merging nanotechnology and photonics, has enabled transformative advancements across diverse sectors including green energy, biomedicine, and optical computing. This review comprehensively examines recent progress in nanophotonic principles and applications, highlighting key innovations in material design, device engineering, and system integration. In renewable energy, nanophotonic allows light-trapping nanostructures and spectral control in perovskite solar cells, concentrating solar power, and thermophotovoltaics. That have significantly enhanced solar conversion efficiencies, approaching theoretical limits. For biosensing, nanophotonic platforms achieve unprecedented sensitivity in detecting biomolecules, pathogens, and pollutants, enabling real-time diagnostics and environmental monitoring. Medical applications leverage tailored light-matter interactions for precision photothermal therapy, image-guided surgery, and early disease detection. Furthermore, nanophotonics underpins next-generation optical neural networks and neuromorphic computing, offering ultra-fast, energy-efficient alternatives to von Neumann architectures. Despite rapid growth, challenges in scalability, fabrication costs, and material stability persist. Future advancements will rely on novel materials, AI-driven design optimization, and multidisciplinary approaches to enable scalable, low-cost deployment. This review summarizes recent progress and highlights future trends, including novel material systems, multidisciplinary approaches, and enhanced computational capabilities, to pave the way for transformative applications in this rapidly evolving field.

**Link**: [arxiv](http://arxiv.org/abs/2507.06587v1),  [pdf](http://arxiv.org/pdf/2507.06587v1)

**Tags**: physics.optics cs.ET physics.app-ph physics.med-ph 



### Can Input Attributions Explain Inductive Reasoning in In-Context   Learning?
**Authors**: Mengyu Ye, Tatsuki Kuribayashi, Goro Kobayashi, Jun Suzuki

**Updated**: 2025-07-09T06:33:14Z

**Summary**: Interpreting the internal process of neural models has long been a challenge. This challenge remains relevant in the era of large language models (LLMs) and in-context learning (ICL); for example, ICL poses a new issue of interpreting which example in the few-shot examples contributed to identifying/solving the task. To this end, in this paper, we design synthetic diagnostic tasks of inductive reasoning, inspired by the generalization tests typically adopted in psycholinguistics. Here, most in-context examples are ambiguous w.r.t. their underlying rule, and one critical example disambiguates it. The question is whether conventional input attribution (IA) methods can track such a reasoning process, i.e., identify the influential example, in ICL. Our experiments provide several practical findings; for example, a certain simple IA method works the best, and the larger the model, the generally harder it is to interpret the ICL with gradient-based IA methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.15628v5),  [pdf](http://arxiv.org/pdf/2412.15628v5)

**Tags**: cs.CL 



### Evaluating and Improving Robustness in Large Language Models: A Survey   and Future Directions
**Authors**: Kun Zhang, Le Wu, Kui Yu, Guangyi Lv, Dacao Zhang

**Updated**: 2025-07-09T06:18:33Z

**Summary**: Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the community.

**Link**: [arxiv](http://arxiv.org/abs/2506.11111v2),  [pdf](http://arxiv.org/pdf/2506.11111v2)

**Tags**: cs.CL cs.AI 



### Airway Segmentation Network for Enhanced Tubular Feature Extraction
**Authors**: Qibiao Wu, Yagang Wang, Qian Zhang

**Updated**: 2025-07-09T06:15:20Z

**Summary**: Manual annotation of airway regions in computed tomography images is a time-consuming and expertise-dependent task. Automatic airway segmentation is therefore a prerequisite for enabling rapid bronchoscopic navigation and the clinical deployment of bronchoscopic robotic systems. Although convolutional neural network methods have gained considerable attention in airway segmentation, the unique tree-like structure of airways poses challenges for conventional and deformable convolutions, which often fail to focus on fine airway structures, leading to missed segments and discontinuities. To address this issue, this study proposes a novel tubular feature extraction network, named TfeNet. TfeNet introduces a novel direction-aware convolution operation that first applies spatial rotation transformations to adjust the sampling positions of linear convolution kernels. The deformed kernels are then represented as line segments or polylines in 3D space. Furthermore, a tubular feature fusion module (TFFM) is designed based on asymmetric convolution and residual connection strategies, enhancing the network's focus on subtle airway structures. Extensive experiments conducted on one public dataset and two datasets used in airway segmentation challenges demonstrate that the proposed TfeNet achieves more accuracy and continuous airway structure predictions compared with existing methods. In particular, TfeNet achieves the highest overall score of 94.95% on the current largest airway segmentation dataset, Airway Tree Modeling(ATM22), and demonstrates advanced performance on the lung fibrosis dataset(AIIB23). The code is available at https://github.com/QibiaoWu/TfeNet.

**Link**: [arxiv](http://arxiv.org/abs/2507.06581v1),  [pdf](http://arxiv.org/pdf/2507.06581v1)

**Tags**: eess.IV cs.CV 



### From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via   Progressive Optimization
**Authors**: Xinjie Chen, Minpeng Liao, Guoxin Chen, Chengxi Li, Biao Fu, Kai Fan, Xinggao Liu

**Updated**: 2025-07-09T06:05:28Z

**Summary**: Reinforcement learning with verifiable rewards (RLVR) has recently advanced the reasoning capabilities of large language models (LLMs). While prior work has emphasized algorithmic design, data curation, and reward shaping, we investigate RLVR from a sample-centric perspective and introduce LPPO (Learning-Progress and Prefix-guided Optimization), a framework of progressive optimization techniques. Our work addresses a critical question: how to best leverage a small set of trusted, high-quality demonstrations, rather than simply scaling up data volume. First, motivated by how hints aid human problem-solving, we propose prefix-guided sampling, an online data augmentation method that incorporates partial solution prefixes from expert demonstrations to guide the policy, particularly for challenging instances. Second, inspired by how humans focus on important questions aligned with their current capabilities, we introduce learning-progress weighting, a dynamic strategy that adjusts each training sample's influence based on model progression. We estimate sample-level learning progress via an exponential moving average of per-sample pass rates, promoting samples that foster learning and de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks demonstrate that our methods outperform strong baselines, yielding faster convergence and a higher performance ceiling.

**Link**: [arxiv](http://arxiv.org/abs/2507.06573v1),  [pdf](http://arxiv.org/pdf/2507.06573v1)

**Tags**: cs.LG cs.AI 



### PersonaFlow: Designing LLM-Simulated Expert Perspectives for Enhanced   Research Ideation
**Authors**: Yiren Liu, Pranav Sharma, Mehul Jitendra Oswal, Haijun Xia, Yun Huang

**Updated**: 2025-07-09T05:59:31Z

**Summary**: Generating interdisciplinary research ideas requires diverse domain expertise, but access to timely feedback is often limited by the availability of experts. In this paper, we introduce PersonaFlow, a novel system designed to provide multiple perspectives by using LLMs to simulate domain-specific experts. Our user studies showed that the new design 1) increased the perceived relevance and creativity of ideated research directions, and 2) promoted users' critical thinking activities (e.g., interpretation, analysis, evaluation, inference, and self-regulation), without increasing their perceived cognitive load. Moreover, users' ability to customize expert profiles significantly improved their sense of agency, which can potentially mitigate their over-reliance on AI. This work contributes to the design of intelligent systems that augment creativity and collaboration, and provides design implications of using customizable AI-simulated personas in domains within and beyond research ideation.

**Link**: [arxiv](http://arxiv.org/abs/2409.12538v2),  [pdf](http://arxiv.org/pdf/2409.12538v2)

**Tags**: cs.HC cs.AI 



### Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted   Binary Cross-Entropy for Enhanced Edge Detection
**Authors**: Hao Shu

**Updated**: 2025-07-09T05:47:26Z

**Summary**: Edge detection (ED) remains a fundamental task in computer vision, yet its performance is often hindered by the ambiguous nature of non-edge pixels near object boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss treats all non-edge pixels uniformly, overlooking the structural nuances around edges and often resulting in blurred predictions. In this paper, we propose the Edge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides pixels into three categories, edge, boundary, and texture, and assigns each a distinct supervisory weight. This tri-class formulation enables more structured learning by guiding the model to focus on both edge precision and contextual boundary localization. We theoretically show that the EBT loss generalizes the WBCE loss, with the latter becoming a limit case. Extensive experiments across multiple benchmarks demonstrate the superiority of the EBT loss both quantitatively and perceptually. Furthermore, the consistent use of unified hyperparameters across all models and datasets, along with robustness to their moderate variations, indicates that the EBT loss requires minimal fine-tuning and is easily deployable in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.06569v1),  [pdf](http://arxiv.org/pdf/2507.06569v1)

**Tags**: cs.CV 



### SlimCaching: Edge Caching of Mixture-of-Experts for Distributed   Inference
**Authors**: Qian Chen, Xianhao Chen, Kaibin Huang

**Updated**: 2025-07-09T05:43:43Z

**Summary**: Mixture-of-Experts (MoE) models improve the scalability of large language models (LLMs) by activating only a small subset of relevant experts per input. However, the sheer number of expert networks in an MoE model introduces a significant storage burden for an edge device. To address this challenge, we consider a scenario where experts are dispersed within an edge network for distributed inference. Based on the popular Top-$K$ expert selection strategy, we formulate a latency minimization problem by optimizing expert caching on edge servers under storage constraints. When $K=1$, the problem reduces to a monotone submodular maximization problem with knapsack constraints, for which we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee. For the general case where $K\geq1$, expert co-activation within the same MoE layer introduces non-submodularity, causing greedy methods to be ineffective. To tackle this issue, we propose a successive greedy decomposition method to decompose the original problem into a series of subproblems, with each being solved by a dynamic programming approach. Furthermore, we design an accelerated algorithm based on the max-convolution technique to obtain the approximate solution with a provable guarantee in polynomial time. Simulation results on various MoE models demonstrate that our method significantly reduces inference latency compared to existing baselines.

**Link**: [arxiv](http://arxiv.org/abs/2507.06567v1),  [pdf](http://arxiv.org/pdf/2507.06567v1)

**Tags**: cs.LG cs.DC cs.NI 



### CHAI for LLMs: Improving Code-Mixed Translation in Large Language Models   through Reinforcement Learning with AI Feedback
**Authors**: Wenbo Zhang, Aditya Majumdar, Amulya Yadav

**Updated**: 2025-07-09T05:40:56Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across various NLP tasks but struggle with code-mixed (or code-switched) language understanding. For example, prior work benchmarking the performance of multilingual LLMs on code-mixed translation tasks has demonstrated that current state-of-the-art multilingual LLMs are ineffective in dealing with code-mixed languages. However, the question of how to improve the capability of multilingual LLMs to handle code-mixed language has not received any attention to date. In this paper, we tackle this research gap by proposing CHAI, a novel general-purpose framework for improving the ability of multilingual LLMs to handle code-mixed languages. CHAI relies on three novel contributions made in this paper. First, we explore the ability of LLMs to provide accurate annotations for code-mixed translation tasks. Second, we leverage this ability of LLMs as annotators to generate preference data for code-mixed translation tasks at scale, which are then used within a reinforcement learning from AI feedback (RLAIF) procedure to improve LLMs' capability on code-mixed tasks. Third, we conduct a rigorous experimental evaluation across various real-world datasets and settings. Our analysis shows that CHAI-powered LLMs outperform state-of-the-art open-source LLMs by 25.66% (in terms of win rate adjudicated by human annotators) in code-mixed translation tasks. This work represents a first step towards developing more inclusive code-mixed LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2411.09073v3),  [pdf](http://arxiv.org/pdf/2411.09073v3)

**Tags**: cs.CL cs.AI cs.LG 



### The Flaws of Others: An LLM-driven Framework for Scientific Knowledge   Production
**Authors**: Juan B. Gutiérrez

**Updated**: 2025-07-09T05:39:56Z

**Summary**: Large-language models turn writing into a live exchange between humans and software. We capture this new medium with a discursive-network model that treats people and LLMs as equal nodes and tracks how their statements circulate. Broadening the focus from isolated hallucinations, we define invalidation (any factual, logical, or structural breach) and show it follows four hazards: drift from truth, self-repair, fresh fabrication, and external detection. A general mathematical model of discursive networks is developed to provide valuable insights: A network governed only by drift and self-repair stabilizes at a modest error rate; adding fabrication reproduces the high rates seen in current LLMs. Giving each false claim even a small chance of peer review shifts the system to a truth-dominant state. We operationalize peer review with the open-source \emph{Flaws-of-Others (FOO) algorithm}: a configurable loop in which any set of agents critique one another while a harmoniser merges their verdicts. The takeaway is practical and cultural: reliability in this new medium comes not from perfecting single models but from wiring imperfect ones into networks that keep each other honest.

**Link**: [arxiv](http://arxiv.org/abs/2507.06565v1),  [pdf](http://arxiv.org/pdf/2507.06565v1)

**Tags**: cs.CL cs.LG 68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15 I.2.7; I.2.11; G.3 



### SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in   Urban Environments
**Authors**: Tianshun Li, Tianyi Huai, Zhen Li, Yichun Gao, Haoang Li, Xinhu Zheng

**Updated**: 2025-07-09T05:38:32Z

**Summary**: Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across various sectors, driven by their mobility and adaptability. This paper introduces SkyVLN, a novel framework integrating vision-and-language navigation (VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in complex urban environments. Unlike traditional navigation methods, SkyVLN leverages Large Language Models (LLMs) to interpret natural language instructions and visual observations, enabling UAVs to navigate through dynamic 3D spaces with improved accuracy and robustness. We present a multimodal navigation agent equipped with a fine-grained spatial verbalizer and a history path memory mechanism. These components allow the UAV to disambiguate spatial contexts, handle ambiguous instructions, and backtrack when necessary. The framework also incorporates an NMPC module for dynamic obstacle avoidance, ensuring precise trajectory tracking and collision prevention. To validate our approach, we developed a high-fidelity 3D urban simulation environment using AirSim, featuring realistic imagery and dynamic urban elements. Extensive experiments demonstrate that SkyVLN significantly improves navigation success rates and efficiency, particularly in new and unseen environments.

**Link**: [arxiv](http://arxiv.org/abs/2507.06564v1),  [pdf](http://arxiv.org/pdf/2507.06564v1)

**Tags**: cs.RO cs.AI cs.SY eess.SY 



### AutoPrep: Natural Language Question-Aware Data Preparation with a   Multi-Agent Framework
**Authors**: Meihao Fan, Ju Fan, Nan Tang, Lei Cao, Guoliang Li, Xiaoyong Du

**Updated**: 2025-07-09T05:24:56Z

**Summary**: Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-ware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose AutoPrep, a large language model (LLM)-based multiagent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components. Planner: Determines a logical plan, outlining a sequence of high-level operations. Programmer: Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor: Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-ofClauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation.

**Link**: [arxiv](http://arxiv.org/abs/2412.10422v4),  [pdf](http://arxiv.org/pdf/2412.10422v4)

**Tags**: cs.CL cs.AI 



### SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth   Generation for RAG
**Authors**: Zou Yuheng, Wang Yiran, Tian Yuzhu, Zhu Min, Huang Yanhua

**Updated**: 2025-07-09T05:13:09Z

**Summary**: Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large Language Models (LLMs), where the effectiveness of the retriever largely determines the overall response quality of RAG systems. Retrievers encompass a multitude of hyperparameters that significantly impact performance outcomes and demonstrate sensitivity to specific applications. Nevertheless, hyperparameter optimization entails prohibitively high computational expenses. Existing evaluation methods suffer from either prohibitive costs or disconnection from domain-specific scenarios. This paper proposes SEARA (Subset sampling Evaluation for Automatic Retriever Assessment), which addresses evaluation data challenges through subset sampling techniques and achieves robust automated retriever evaluation by minimal retrieval facts extraction and comprehensive retrieval metrics. Based on real user queries, this method enables fully automated retriever evaluation at low cost, thereby obtaining optimal retriever for specific business scenarios. We validate our method across classic RAG applications in rednote, including knowledge-based Q&A system and retrieval-based travel assistant, successfully obtaining scenario-specific optimal retrievers.

**Link**: [arxiv](http://arxiv.org/abs/2507.06554v1),  [pdf](http://arxiv.org/pdf/2507.06554v1)

**Tags**: cs.IR 



### WiLLM: an Open Framework for LLM Services over Wireless Systems
**Authors**: Boyi Liu, Yongguang Lu, Jianguo Zhao, Qiang Yang, Wen Wu, Lin Chen, Jagmohan Chauhan, Jun Zhang

**Updated**: 2025-07-09T05:10:03Z

**Summary**: Large Language Model (LLM) services fundamentally differ from traditional Deep Neural Network (DNN) applications in wireless networks. We identify three critical distinctions: (1) unlike traditional DNNs with unidirectional data flows, LLM's multimodal interactions create bidirectional heavy loads with contrasting bottlenecks, requiring direction-aware resource scheduling; (2) while traditional DNNs exhibit fixed computational patterns, LLM's highly variable inference times interact complexly with network slicing, causing dynamic bottleneck migration; and (3) in contrast to predictable DNN traffic, LLM's token streams demonstrate unprecedented burstiness and state dependencies. These insights motivate WiLLM, the first open-source framework, implemented as a wireless platform, for LLM service research. Built on OpenAirInterface, WiLLM introduces several technical innovations: dynamic slice compatibility, universal UE compatibility through application-layer tunneling, multi-UE multi-slice scheduling, dual-mode resource allocation, and cross-layer APIs. In addition, WiLLM eliminates the need for specialized wireless expertise, enabling researchers and developers to experiment with LLM services over realistic cellular networks. We demonstrate the platform's capabilities through a smart glasses case study and provide a comprehensive dataset of \~1.6 million synchronized measurements. The complete system, dataset, and appendix are available at https://openwillm.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2506.19030v2),  [pdf](http://arxiv.org/pdf/2506.19030v2)

**Tags**: cs.NI 



