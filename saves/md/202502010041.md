# Arxiv Results
## Keyword: kv cache 
 ### REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and   Failure Mitigation
**Authors**: Tommaso Bonato, Abdul Kabbani, Ahmad Ghalayini, Michael Papamichael, Mohammad Dohadwala, Lukas Gianinazzi, Mikhail Khalilov, Elias Achermann, Daniele De Sensi, Torsten Hoefler

**Updated**: 2025-01-30T18:23:46Z

**Summary**: Next-generation datacenters require highly efficient network load balancing to manage the growing scale of artificial intelligence (AI) training and general datacenter traffic. Existing solutions designed for Ethernet, such as Equal Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to maintain high network utilizations as datacenter topologies (and network failures as a consequence) continue to grow. To address these limitations, we propose REPS, a lightweight decentralized per-packet adaptive load balancing algorithm designed to optimize network utilization while ensuring rapid recovery from link failures. REPS adapts to network conditions by caching good-performing paths. In case of a network failure, REPS re-routes traffic away from it in less than 100 microseconds. REPS is designed to be deployed with next-generation out-of-order transports, such as Ultra Ethernet, and introduces less than 25 bytes of per-connection state. We extensively evaluate REPS in large-scale simulations and FPGA-based NICs.

**Link**: [arxiv](http://arxiv.org/abs/2407.21625v3),  [pdf](http://arxiv.org/pdf/2407.21625v3)

**Tags**: cs.NI 



### State Stream Transformer (SST) : Emergent Metacognitive Behaviours   Through Latent State Persistence
**Authors**: Thea Aviss

**Updated**: 2025-01-30T14:03:36Z

**Summary**: We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the 'state stream' that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.18356v1),  [pdf](http://arxiv.org/pdf/2501.18356v1)

**Tags**: cs.LG cs.AI cs.CL 



### Locret: Enhancing Eviction in Long-Context LLM Inference with Trained   Retaining Heads on Consumer-Grade Devices
**Authors**: Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu

**Updated**: 2025-01-30T13:07:37Z

**Summary**: Scaling the input context length of a large language model (LLM) incurs a significant increase in computation cost and memory footprint to maintain the attention key-value (KV) cache. Existing KV cache compression methods suffer from inefficient compression strategies and limited memory reduction effects, making it difficult for LLMs to conduct long-context inference on consumer-grade devices, especially when inferring long-context stream input. Such obstacles prevent consumer-grade devices from supporting more complex applications, creating challenges for the democratization of LLMs. To overcome this, we propose Locret, the first framework to create an eviction policy compatible with chunked prefill. By evaluating the causal importance of KV cache units by learnable retaining heads, Locret enables precise eviction of cache units, facilitating efficient long-context inference. In our extensive empirical studies, Locret outperforms the recent popular and competitive approaches in terms of memory efficiency and generation quality -- Locret achieves up to 20x of KV cache compression ratio within less than 10% performance loss. Furthermore, Locret achieves 128K+ long-context inference on a single NVIDIA 4090 GPU without compromising generation quality and only costs <1 GPU hour of additional training.

**Link**: [arxiv](http://arxiv.org/abs/2410.01805v2),  [pdf](http://arxiv.org/pdf/2410.01805v2)

**Tags**: cs.CL 



### Systematic Evaluation of Randomized Cache Designs against Cache   Occupancy
**Authors**: Anirban Chakraborty, Nimish Mishra, Sayandeep Saha, Sarani Bhattacharya, Debdeep Mukhopadhyay

**Updated**: 2025-01-30T06:02:11Z

**Summary**: Randomizing the address-to-set mapping and partitioning of the cache has been shown to be an effective mechanism in designing secured caches. Several designs have been proposed on a variety of rationales: (1) randomized design, (2) randomized-and-partitioned design, and (3) psuedo-fully associative design. This work fills in a crucial gap in current literature on randomized caches: currently most randomized cache designs defend only contention-based attacks, and leave out considerations of cache occupancy. We perform a systematic evaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE, Scatter-Cache, and Sass-cache against cache occupancy wrt. both performance as well as security.   With respect to performance, we first establish that benchmarking strategies used by contemporary designs are unsuitable for a fair evaluation (because of differing cache configurations, choice of benchmarking suites, additional implementation-specific assumptions). We thus propose a uniform benchmarking strategy, which allows us to perform a fair and comparative analysis across all designs under various replacement policies. Likewise, with respect to security against cache occupancy attacks, we evaluate the cache designs against various threat assumptions: (1) covert channels, (2) process fingerprinting, and (3) AES key recovery (to the best of our knowledge, this work is the first to demonstrate full AES key recovery on a randomized cache design using cache occupancy attack). Our results establish the need to also consider cache occupancy side-channel in randomized cache design considerations.

**Link**: [arxiv](http://arxiv.org/abs/2310.05172v2),  [pdf](http://arxiv.org/pdf/2310.05172v2)

**Tags**: cs.CR cs.AR 



### Reciprocating Locks
**Authors**: Dave Dice, Alex Kogan

**Updated**: 2025-01-29T16:44:27Z

**Summary**: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.

**Link**: [arxiv](http://arxiv.org/abs/2501.02380v5),  [pdf](http://arxiv.org/pdf/2501.02380v5)

**Tags**: cs.DC D.4.1 



### vAttention: Dynamic Memory Management for Serving LLMs without   PagedAttention
**Authors**: Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, Ashish Panwar

**Updated**: 2025-01-29T04:10:41Z

**Summary**: PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads.   We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.

**Link**: [arxiv](http://arxiv.org/abs/2405.04437v3),  [pdf](http://arxiv.org/pdf/2405.04437v3)

**Tags**: cs.LG cs.OS 



### Optimizing SSD Caches for Cloud Block Storage Systems Using Machine   Learning Approaches
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2025-01-28T20:35:23Z

**Summary**: The growing demand for efficient cloud storage solutions has led to the widespread adoption of Solid-State Drives (SSDs) for caching in cloud block storage systems. The management of data writes to SSD caches plays a crucial role in improving overall system performance, reducing latency, and extending the lifespan of storage devices. A critical challenge arises from the large volume of write-only data, which significantly impacts the performance of SSD caches when handled inefficiently. Specifically, writes that have not been read for a certain period may introduce unnecessary write traffic to the SSD cache without offering substantial benefits for cache performance. This paper proposes a novel approach to mitigate this issue by leveraging machine learning techniques to dynamically optimize the write policy in cloud-based storage systems. The proposed method identifies write-only data and selectively filters it out in real-time, thereby minimizing the number of unnecessary write operations and improving the overall performance of the cache system. Experimental results demonstrate that the proposed machine learning-based policy significantly outperforms traditional approaches by reducing the number of harmful writes and optimizing cache utilization. This solution is particularly suitable for cloud environments with varying and unpredictable workloads, where traditional cache management strategies often fall short.

**Link**: [arxiv](http://arxiv.org/abs/2501.14770v2),  [pdf](http://arxiv.org/pdf/2501.14770v2)

**Tags**: cs.DC cs.LG cs.OS 



### Dynamic Adaptation in Data Storage: Real-Time Machine Learning for   Enhanced Prefetching
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2025-01-28T20:33:43Z

**Summary**: The exponential growth of data storage demands has necessitated the evolution of hierarchical storage management strategies [1]. This study explores the application of streaming machine learning [3] to revolutionize data prefetching within multi-tiered storage systems. Unlike traditional batch-trained models, streaming machine learning [5] offers adaptability, real-time insights, and computational efficiency, responding dynamically to workload variations. This work designs and validates an innovative framework that integrates streaming classification models for predicting file access patterns, specifically the next file offset. Leveraging comprehensive feature engineering and real-time evaluation over extensive production traces, the proposed methodology achieves substantial improvements in prediction accuracy, memory efficiency, and system adaptability. The results underscore the potential of streaming models in real-time storage management, setting a precedent for advanced caching and tiering strategies.

**Link**: [arxiv](http://arxiv.org/abs/2501.14771v2),  [pdf](http://arxiv.org/pdf/2501.14771v2)

**Tags**: cs.DC cs.LG cs.OS 



### Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks   Detection: A Comparative Analysis
**Authors**: Tejal Joshi, Aarya Kawalay, Anvi Jamkhande, Amit Joshi

**Updated**: 2025-01-28T18:14:43Z

**Summary**: Cache side channel attacks are a sophisticated and persistent threat that exploit vulnerabilities in modern processors to extract sensitive information. These attacks leverage weaknesses in shared computational resources, particularly the last level cache, to infer patterns in data access and execution flows, often bypassing traditional security defenses. Such attacks are especially dangerous as they can be executed remotely without requiring physical access to the victim's device. This study focuses on a specific class of these threats: fingerprinting attacks, where an adversary monitors and analyzes the behavior of co-located processes via cache side channels. This can potentially reveal confidential information, such as encryption keys or user activity patterns. A comprehensive threat model illustrates how attackers sharing computational resources with target systems exploit these side channels to compromise sensitive data. To mitigate such risks, a hybrid deep learning model is proposed for detecting cache side channel attacks. Its performance is compared with five widely used deep learning models: Multi-Layer Perceptron, Convolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term Memory, and Gated Recurrent Unit. The experimental results demonstrate that the hybrid model achieves a detection rate of up to 99.96%. These findings highlight the limitations of existing models, the need for enhanced defensive mechanisms, and directions for future research to secure sensitive data against evolving side channel threats.

**Link**: [arxiv](http://arxiv.org/abs/2501.17123v1),  [pdf](http://arxiv.org/pdf/2501.17123v1)

**Tags**: cs.CR cs.NE 



### Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications
**Authors**: Mohammad NaseriTehrani, MohammadJavad Salehi, Antti Tölli

**Updated**: 2025-01-28T16:19:24Z

**Summary**: Integrating coded caching (CC) into multiple-input multiple-output (MIMO) communications can significantly enhance the achievable degrees of freedom (DoF) in wireless networks. This paper investigates a practical cache-aided asymmetric MIMO configuration with cache ratio $\gamma$, where a server equipped with $L$ transmit antennas communicates with $K$ users, each having $G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the \emph{min-G} scheme, which treats the system as symmetric by assuming all users have the same number of antennas, equal to the smallest among them; the \emph{Grouping} scheme, which maximizes spatial multiplexing gain separately within each user subset at the cost of some global caching gain; and the \emph{Phantom} scheme, which dynamically redistributes spatial resources using virtual or ``phantom'' antennas at the users, bridging the performance gains of the min-$G$ and Grouping schemes. These strategies jointly optimize the number of users, $\Omega$, and the parallel streams decoded by each user, $\beta_k$, ensuring linear decodability for all target users. Analytical and numerical results confirm that the proposed schemes achieve significant DoF improvements across various system configurations.

**Link**: [arxiv](http://arxiv.org/abs/2501.10854v2),  [pdf](http://arxiv.org/pdf/2501.10854v2)

**Tags**: cs.IT eess.SP math.IT 



### Measuring GPU utilization one level deeper
**Authors**: Paul Elvinger, Foteini Strati, Natalie Enright Jerger, Ana Klimovic

**Updated**: 2025-01-28T12:57:53Z

**Summary**: GPU hardware is vastly underutilized. Even resource-intensive AI applications have diverse resource profiles that often leave parts of GPUs idle. While colocating applications can improve utilization, current spatial sharing systems lack performance guarantees. Providing predictable performance guarantees requires a deep understanding of how applications contend for shared GPU resources such as block schedulers, compute units, L1/L2 caches, and memory bandwidth. We propose a methodology to profile resource interference of GPU kernels across these dimensions and discuss how to build GPU schedulers that provide strict performance guarantees while colocating applications to minimize cost.

**Link**: [arxiv](http://arxiv.org/abs/2501.16909v1),  [pdf](http://arxiv.org/pdf/2501.16909v1)

**Tags**: cs.DC 



### Optimizing Smart Helper Placement for Enhanced Cache Efficiency in   F-RANs
**Authors**: Hesameddin Mokhtarzadeh, Mohammed Saif, Md. Jahangir Hossain, Julian Cheng

**Updated**: 2025-01-28T00:22:34Z

**Summary**: Smart helpers (SHs) have been proposed to improve content delivery delays and alleviate high fronthaul loads in fog radio access networks (F-RANs). They offer an alternative to deploying additional enhanced remote radio heads (RRHs), which are often infeasible due to site constraints.} The optimal placement of SHs can significantly increase the number of users they serve which leads to enhanced cache efficiency and improved content delivery delay. In this letter, we optimize SH placement within an F-RAN to maximize the cache hit rate and further reduce the content delivery latency. We model the SH cache hit rate as a function of outage probability and user density distribution. We develop a function to estimate user density distribution leveraging the radial basis functions (RBFs) method and optimize SH placement utilizing the particle swarm optimization (PSO) algorithm. \an{Our} numerical results confirm the effectiveness of the proposed approach in maximizing the \an{SH cache hit rate}, thereby improving delivery delays and fronthaul loads of the network.

**Link**: [arxiv](http://arxiv.org/abs/2501.16597v1),  [pdf](http://arxiv.org/pdf/2501.16597v1)

**Tags**: eess.SP 



### Latency Guarantees for Caching with Delayed Hits
**Authors**: Keerthana Gurushankar, Noah G. Singer, Bernardo Subercaseaux

**Updated**: 2025-01-27T22:14:43Z

**Summary**: In the classical caching problem, when a requested page is not present in the cache (i.e., a "miss"), it is assumed to travel from the backing store into the cache "before" the next request arrives. However, in many real-life applications, such as content delivery networks, this assumption is unrealistic.   The "delayed-hits" model for caching, introduced by Atre, Sherry, Wang, and Berger, accounts for the latency between a missed cache request and the corresponding arrival from the backing store. This theoretical model has two parameters: the "delay" $Z$, representing the ratio between the retrieval delay and the inter-request delay in an application, and the "cache size" $k$, as in classical caching. Classical caching corresponds to $Z=1$, whereas larger values of $Z$ model applications where retrieving missed requests is expensive. Despite the practical relevance of the delayed-hits model, its theoretical underpinnings are still poorly understood.   We present the first tight theoretical guarantee for optimizing delayed-hits caching: The "Least Recently Used" algorithm, a natural, deterministic, online algorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at most $O(Zk)$ times more latency than the (offline) optimal schedule. Our result extends to any so-called "marking" algorithm.

**Link**: [arxiv](http://arxiv.org/abs/2501.16535v1),  [pdf](http://arxiv.org/pdf/2501.16535v1)

**Tags**: cs.DS 



### SP-IMPact: A Framework for Static Partitioning Interference Mitigation   and Performance Analysis
**Authors**: Diogo Costa, Gonçalo Moreira, Afonso Oliveira, José Martins, Sandro Pinto

**Updated**: 2025-01-27T17:42:20Z

**Summary**: Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms. Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability. Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. However, configuring these techniques is complex and time-consuming. Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods. Testing all configurations is impractical and often leads to suboptimal results. Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance. Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems. They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers. To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact. By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability.

**Link**: [arxiv](http://arxiv.org/abs/2501.16245v1),  [pdf](http://arxiv.org/pdf/2501.16245v1)

**Tags**: cs.DC cs.PF cs.SY eess.SY 



### Recommenadation aided Caching using Combinatorial Multi-armed Bandits
**Authors**: Pavamana K J, Chandramani Kishore Singh

**Updated**: 2025-01-27T14:55:40Z

**Summary**: We study content caching with recommendations in a wireless network where the users are connected through a base station equipped with a finite-capacity cache. We assume a fixed set of contents with unknown user preferences and content popularities. The base station can cache a subset of the contents and can also recommend subsets of the contents to different users in order to encourage them to request the recommended contents. Recommendations, depending on their acceptability, can thus be used to increase cache hits. We first assume that the users' recommendation acceptabilities are known and formulate the cache hit optimization problem as a combinatorial multi-armed bandit (CMAB). We propose a UCB-based algorithm to decide which contents to cache and recommend and provide an upper bound on the regret of this algorithm. Subsequently, we consider a more general scenario where the users' recommendation acceptabilities are also unknown and propose another UCB-based algorithm that learns these as well. We numerically demonstrate the performance of our algorithms and compare these to state-of-the-art algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2405.00080v4),  [pdf](http://arxiv.org/pdf/2405.00080v4)

**Tags**: cs.LG cs.IR cs.NI 



### SIC-free Multicast Scheduling for Multi-antenna Coded Caching
**Authors**: MohammadJavad Sojdeh, MohammadJavad Salehi, Antti Tölli

**Updated**: 2025-01-27T14:37:24Z

**Summary**: Multi-antenna coded caching (CC) with multicast beamforming typically relies on a complex successive interference cancellation (SIC) structure to decode a superposition of multiple streams received by each user. Signal-level CC schemes require the regeneration and cancellation of interfering signals at the physical layer of each receiver, which complicates practical implementations. To address this, we propose a bit-level multicast scheduling scheme enabling linear, SIC-free decoding of parallel streams by repeatedly transmitting data terms with linearly independent coefficients. Two reference strategies and a novel sparse strategy are considered for constructing the coefficient matrix. The reference cases include the random strategy, which lacks control over matrix construction, and the equal-distant strategy, which balances users' interference and data terms equally. In contrast, the sparse strategy minimizes the number of multicast streams transmitted in parallel during each interval. This approach simplifies both the decoding process and the beamforming design by decoupling the desired data terms for each user and reducing the number of SINR constraints, respectively. To further enhance the symmetric rate, a successive projection algorithm is applied to exploit channel properties and optimize user ordering. With the coefficient matrix and optimized user ordering in place, multicast beamformers are devised to aggregate desired data from relevant multicast streams. Numerical simulations validate the effectiveness of the sparse strategy and user scheduling, demonstrating significant gains in symmetric rate.

**Link**: [arxiv](http://arxiv.org/abs/2501.11126v2),  [pdf](http://arxiv.org/pdf/2501.11126v2)

**Tags**: cs.IT math.IT 



### Random Reshuffling for Stochastic Gradient Langevin Dynamics
**Authors**: Luke Shaw, Peter A. Whalley

**Updated**: 2025-01-27T13:53:12Z

**Summary**: We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent.

**Link**: [arxiv](http://arxiv.org/abs/2501.16055v1),  [pdf](http://arxiv.org/pdf/2501.16055v1)

**Tags**: math.NA cs.NA math.PR stat.ML 65C05, 82C31, 62F15 



### PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language   Models Quantization
**Authors**: Mengzhao Chen, Yi Liu, Jiahao Wang, Yi Bin, Wenqi Shao, Ping Luo

**Updated**: 2025-01-27T13:39:25Z

**Summary**: Existing weight-activation quantization methods for Large Language Models (LLMs) primarily address channel-wise outliers but often neglect token-wise outliers, which limits the accuracy of quantized models. In this work, we propose PrefixQuant, a novel quantization method that achieves state-of-the-art performance across various precision levels (W4A4KV4 and W4A8KV4) and granularities (dynamic and static quantization) by effectively isolating token-wise outliers. First, PrefixQuant eliminates token-wise outliers by prefixing outlier tokens in the KV cache, a process that is training-free and highly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant introduces new trainable parameters for block-wise training to compensate for quantization error. Our experiments show that PrefixQuant significantly outperforms existing dynamic quantization methods, even under coarser static quantization settings. For instance, PrefixQuant achieves an average accuracy improvement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on five zero-shot reasoning tasks under dynamic and static quantization settings, respectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x prefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant. Our code is available at https://github.com/ChenMnZ/PrefixQuant.

**Link**: [arxiv](http://arxiv.org/abs/2410.05265v2),  [pdf](http://arxiv.org/pdf/2410.05265v2)

**Tags**: cs.LG cs.CL 



### Dynamic Content Caching with Waiting Costs via Restless Multi-Armed   Bandits
**Authors**: Ankita Koley, Chandramani Singh

**Updated**: 2025-01-27T06:47:20Z

**Summary**: We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the exising policies.

**Link**: [arxiv](http://arxiv.org/abs/2410.18627v3),  [pdf](http://arxiv.org/pdf/2410.18627v3)

**Tags**: cs.NI 



### Online Allocation with Multi-Class Arrivals: Group Fairness vs   Individual Welfare
**Authors**: Faraz Zargari, Hossein Nekouyan Jazi, Bo Sun, Xiaoqi Tan

**Updated**: 2025-01-27T05:02:05Z

**Summary**: We introduce and study a multi-class online resource allocation problem with group fairness guarantees. The problem involves allocating a fixed amount of resources to a sequence of agents, each belonging to a specific group. The primary objective is to ensure fairness across different groups in an online setting. We focus on three fairness notions: one based on quantity and two based on utility. To achieve fair allocations, we develop two threshold-based online algorithms, proving their optimality under two fairness notions and near-optimality for the more challenging one. Additionally, we demonstrate a fundamental trade-off between group fairness and individual welfare using a novel representative function-based approach. To address this trade-off, we propose a set-aside multi-threshold algorithm that reserves a portion of the resource to ensure fairness across groups while utilizing the remaining resource to optimize efficiency under utility-based fairness notions. This algorithm is proven to achieve the Pareto-optimal trade-off. We also demonstrate that our problem can model a wide range of real-world applications, including network caching and cloud computing, and empirically evaluate our proposed algorithms in the network caching problem using real datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.15782v1),  [pdf](http://arxiv.org/pdf/2501.15782v1)

**Tags**: cs.GT cs.DS 



### ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language   Model Born from Transformer
**Authors**: Lin Yueyu, Li Zhiyuan, Peter Yue, Liu Xiao

**Updated**: 2025-01-26T15:56:56Z

**Summary**: As is known, hybrid quadratic and subquadratic attention models in multi-head architectures have surpassed both Transformer and Linear RNN models , with these works primarily focusing on reducing KV complexity and improving efficiency. For further research on expressiveness, we introduce our series of models distilled from Qwen 2.5, based on pure native RWKV-7 attention, which aims to make RNN more expressive and demonstrates state tracking ability beyond transformers. We work with QRWK 32B based on RWKV-6 architecture, another approach that reduces the entire knowledge processing time to just 8 hours using 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the distillation process can utilize any LLM, not just Qwen, and enables knowledge transfer from larger LLMs to smaller ones with more fewer tokens. We will explain the detailed process and share our insights on building more powerful foundation models. Please note that this is an ongoing work that will be updated continuously. The model checkpoints and source code are available at \href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside}, \href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.

**Link**: [arxiv](http://arxiv.org/abs/2501.15570v1),  [pdf](http://arxiv.org/pdf/2501.15570v1)

**Tags**: cs.CL 



### Query-based versus resource-based cache strategies in tag-based browsing   systems
**Authors**: Joaquín Gayoso-Cabada, Mercedes Gómez-Albarrán, José-Luis Sierra

**Updated**: 2025-01-26T11:01:10Z

**Summary**: Tag-based browsing is a popular interaction model for navigating digital libraries. According to this model, users select descriptive tags to filter resources in the collections. Typical implementations of the model are based on inverted indexes. However, these implementations can require a considerable amount of set operations to update the browsing state. To palliate this inconven-ience, it is possible to adopt suitable cache strategies. In this paper we describe and compare two of these strategies: (i) a query-based strategy, according to which previously computed browsing states are indexed by sets of selected tags; and (ii) a resource-based strategy, according to which browsing states are in-dexed by sets of filtered resources. Our comparison focused on runtime perfor-mance, and was carried out empirically, using a real-world web-based collec-tion in the field of digital humanities. The results obtained show that the re-source-based strategy clearly outperforms the query-based one.

**Link**: [arxiv](http://arxiv.org/abs/2501.15481v1),  [pdf](http://arxiv.org/pdf/2501.15481v1)

**Tags**: cs.CL 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2025-01-26T07:29:06Z

**Summary**: Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v4),  [pdf](http://arxiv.org/pdf/2407.11550v4)

**Tags**: cs.CL cs.AI 



### Collaborative Coded Caching for Partially Connected Networks
**Authors**: Kagan Akcay, Eleftherios Lampiris, MohammadJavad Salehi, Giuseppe Caire

**Updated**: 2025-01-26T01:43:46Z

**Summary**: Coded caching leverages the differences in user cache memories to achieve gains that scale with the total cache size, alleviating network congestion due to high-quality content requests. Additionally, distributing transmitters over a wide area can mitigate the adverse effects of path loss. In this work, we consider a partially connected network where the channel between distributed transmitters (helpers) and users is modeled as a distributed MIMO Gaussian broadcast channel. We propose a novel delivery scheme consisting of two phases: partitioning and transmission. In the partitioning phase, users with identical cache profiles are partitioned into the minimum number of sets, such that users within each set can successfully decode their desired message from a joint transmission enabled by MIMO precoding. To optimally partition the users, we employ the branch and bound method. In the transmission phase, each partition is treated as a single entity, and codewords are multicast to partitions with distinct cache profiles. The proposed delivery scheme is applicable to any partially connected network, and while the partitioning is optimal, the overall delivery scheme, including transmission, is heuristic. Interestingly, simulation results show that its performance closely approximates that of the fully connected optimal solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.13298v2),  [pdf](http://arxiv.org/pdf/2501.13298v2)

**Tags**: cs.IT math.IT 



### ReInc: Scaling Training of Dynamic Graph Neural Networks
**Authors**: Mingyu Guan, Saumia Singhal, Taesoo Kim, Anand Padmanabha Iyer

**Updated**: 2025-01-25T23:16:03Z

**Summary**: Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to their applicability in diverse domains such as traffic network prediction, epidemiological forecasting, and social network analysis. In this paper, we present ReInc, a system designed to enable efficient and scalable training of DGNNs on large-scale graphs. ReInc introduces key innovations that capitalize on the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) inherent in DGNNs. By reusing intermediate results and incrementally computing aggregations across consecutive graph snapshots, ReInc significantly enhances computational efficiency. To support these optimizations, ReInc incorporates a novel two-level caching mechanism with a specialized caching policy aligned to the DGNN execution workflow. Additionally, ReInc addresses the challenges of managing structural and temporal dependencies in dynamic graphs through a new distributed training strategy. This approach eliminates communication overheads associated with accessing remote features and redistributing intermediate results. Experimental results demonstrate that ReInc achieves up to an order of magnitude speedup compared to state-of-the-art frameworks, tested across various dynamic GNN architectures and real-world graph datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.15348v1),  [pdf](http://arxiv.org/pdf/2501.15348v1)

**Tags**: cs.LG cs.DC 



### Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle
**Authors**: KVS Chaithanya, Sumesh P. Thampi

**Updated**: 2025-01-25T12:17:41Z

**Summary**: Understanding the hydrodynamics of microswimmers in viscoelastic fluids and confined environments is crucial for interpreting their behaviour in natural settings and designing synthetic microswimmers for practical applications like cargo transport. In this study, we explore the hydrodynamics of a concentric active compound particle - a model microswimmer (a squirmer) positioned at the centre of a viscoelastic fluid droplet (a model cargo) suspended in another viscoelastic medium. We consider the Oldroyd-B constitutive model to characterize the fluids and employ a perturbative approach in the Deborah number to analyze viscoelastic effects analytically, assuming a small Capillary number so that the droplet remains spherical and does not deform. We examine three cases: (i) a squirmer confined within a viscoelastic fluid droplet suspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian fluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined within a viscoelastic fluid droplet suspended in another viscoelastic fluid. Our findings reveal that the swimming speeds of the squirmer and the droplet are determined by the complex interplay of viscoelasticity, the size ratio of the droplet to the squirmer (confinement strength), and the viscosity ratio of the surrounding fluid to the droplet fluid. A critical aspect of this interaction is the positioning of stagnation points within the fluid flow, which governs the distribution of polymeric stress. This distribution, in turn, plays a crucial role in determining the influence of viscoelasticity on the squirmer's dynamics. Our analysis suggests that viscoelastic effects can either enhance or hinder the swimming speed of the squirmer when confined in a droplet, depending on the specific configuration of the system.

**Link**: [arxiv](http://arxiv.org/abs/2410.09479v2),  [pdf](http://arxiv.org/pdf/2410.09479v2)

**Tags**: physics.flu-dyn cond-mat.soft 



### The Selection Problem in Multi-Query Optimization: a Comprehensive   Survey
**Authors**: Sergey Zinchenko, Denis Ponomaryov

**Updated**: 2025-01-25T10:38:11Z

**Summary**: View materialization, index selection, and plan caching are well-known techniques for optimization of query processing in database systems. The essence of these tasks is to select and save a subset of the most useful candidates (views/indexes/plans) for reuse within given space/time budget constraints. In this paper, we propose a unified view on these selection problems. We make a detailed analysis of the root causes of their complexity and summarize techniques to address them. Our survey provides a modern classification of selection algorithms known in the literature, including the latest ones based on Machine Learning. We provide a ground for reuse of the selection techniques between different optimization scenarios and highlight challenges and promising directions in the field. Based on our analysis we derive a method to exponentially accelerate some of the state-of-the-art selection algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2412.11828v2),  [pdf](http://arxiv.org/pdf/2412.11828v2)

**Tags**: cs.DB cs.DM 



### Fully-Automated Code Generation for Efficient Computation of Sparse   Matrix Permanents on GPUs
**Authors**: Deniz Elbek, Kamer Kaya

**Updated**: 2025-01-25T08:27:26Z

**Summary**: Registers are the fastest memory components within the GPU's complex memory hierarchy, accessed by names rather than addresses. They are managed entirely by the compiler through a process called register allocation, during which the compiler attempts to cache predictable data from thread-local memory into thread-private registers. Computing the permanent of a sparse matrix poses a challenge for compilers, as optimizing this process is hindered by the unpredictable distribution of nonzero elements, which only become known at runtime. In this work, we employ fully-automated code generation to address this, producing highly optimized kernels tailored to the matrix's sparsity pattern. State-of-the-art permanent computation algorithms require each thread to store a private array, denoted x, of size n. We first propose a technique that fully stores these arrays in registers, with inclusion and exclusion kernels generated for each column. To minimize control divergence and reduce the number of unique kernels within a warp, we exploit the internal structure of Gray codes, which are also used in the state-of-the-art algorithm. Our second technique reduces register pressure by utilizing both registers and global memory and introduces a matrix ordering and partitioning strategy for greater efficiency. On synthetic matrices, this approach achieves a 31x speedup over state-of-the-art CPU implementations on 112 cores, and an 8x speedup compared to our traditional GPU implementation. For real-world matrices, these speedups are 24.9x and 4.9x.

**Link**: [arxiv](http://arxiv.org/abs/2501.15126v1),  [pdf](http://arxiv.org/pdf/2501.15126v1)

**Tags**: cs.DC cs.DM cs.NA math.NA 



### Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation   of Attention Heads
**Authors**: Xingyang He, Jie Liu, Shaowei Chen

**Updated**: 2025-01-25T07:28:13Z

**Summary**: KV cache is a widely used acceleration technique for large language models (LLMs) inference. However, its memory requirement grows rapidly with input length. Previous studies have reduced the size of KV cache by either removing the same number of unimportant tokens for all attention heads or by allocating differentiated KV cache budgets for pre-identified attention heads. However, due to the importance of attention heads varies across different tasks, the pre-identified attention heads fail to adapt effectively to various downstream tasks. To address this issue, we propose Task-KV, a method that leverages the semantic differentiation of attention heads to allocate differentiated KV cache budgets across various tasks. We demonstrate that attention heads far from the semantic center (called heterogeneous heads) make an significant contribution to task outputs and semantic understanding. In contrast, other attention heads play the role of aggregating important information and focusing reasoning. Task-KV allocates full KV cache budget to heterogeneous heads to preserve comprehensive semantic information, while reserving a small number of recent tokens and attention sinks for non-heterogeneous heads. Furthermore, we innovatively introduce middle activations to preserve key contextual information aggregated from non-heterogeneous heads. To dynamically perceive semantic differences among attention heads, we design a semantic separator to distinguish heterogeneous heads from non-heterogeneous ones based on their distances from the semantic center. Experimental results on multiple benchmarks and different model architectures demonstrate that Task-KV significantly outperforms existing baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.15113v1),  [pdf](http://arxiv.org/pdf/2501.15113v1)

**Tags**: cs.CL 



### A New Construction Structure on Coded Caching with Linear   Subpacketization: Non-Half-Sum Disjoint Packing
**Authors**: Minquan Cheng, Huimei Wei, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-25T04:21:57Z

**Summary**: Coded caching is a promising technique to effectively reduce peak traffic by using local caches and the multicast gains generated by these local caches. We prefer to design a coded caching scheme with the subpacketization $F$ and transmission load $R$ as small as possible since these are the key metrics for evaluating the implementation complexity and transmission efficiency of the scheme, respectively. However, most of the existing coded caching schemes have large subpacketizations which grow exponentially with the number of users $K$, and there are a few schemes with linear subpacketizations which have large transmission loads. In this paper, we focus on studying the linear subpacketization, i.e., $K=F$, coded caching scheme with low transmission load. Specifically, we first introduce a new combinatorial structure called non-half-sum disjoint packing (NHSDP) which can be used to generate a coded caching scheme with $K=F$. Then a class of new schemes is obtained by constructing NHSDP. Theoretical and numerical comparisons show that (i) compared to the existing schemes with linear subpacketization (to the number of users), the proposed scheme achieves a lower load; (ii) compared to some existing schemes with polynomial subpacketization, the proposed scheme can also achieve a lower load in some cases; (iii) compared to some existing schemes with exponential subpacketization, the proposed scheme has loads close to those of these schemes in some cases. Moreover, the new concept of NHSDP is closely related to the classical combinatorial structures such as cyclic difference packing (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash family (PHF). These connections indicate that NHSDP is an important combinatorial structure in the field of combinatorial design.

**Link**: [arxiv](http://arxiv.org/abs/2501.11855v2),  [pdf](http://arxiv.org/pdf/2501.11855v2)

**Tags**: cs.IT math.IT 



### AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for   Vision-Language Models
**Authors**: Zunhai Su, Wang Shen, Linge Li, Zhe Chen, Hanyu Wei, Huangqi Yu, Kehong Yuan

**Updated**: 2025-01-25T02:01:56Z

**Summary**: Vision-language models (VLMs) show remarkable performance in multimodal tasks. However, excessively long multimodal inputs lead to oversized Key-Value (KV) caches, resulting in significant memory consumption and I/O bottlenecks. Previous KV quantization methods for Large Language Models (LLMs) may alleviate these issues but overlook the attention saliency differences of multimodal tokens, resulting in suboptimal performance. In this paper, we investigate the attention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL leverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient Attention (PSA) patterns to adaptively allocate bit budgets. Moreover, achieving extremely low-bit quantization requires effectively addressing outliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to construct outlier-free KV caches, thereby reducing quantization difficulty. Evaluations of 2-bit quantization on 12 long-context and multimodal tasks demonstrate that AKVQ-VL maintains or even improves accuracy, outperforming LLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up to 3.25x larger batch sizes and 2.46x throughput.

**Link**: [arxiv](http://arxiv.org/abs/2501.15021v1),  [pdf](http://arxiv.org/pdf/2501.15021v1)

**Tags**: cs.CL 



### RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via   Outlier-Aware Adaptive Rotations
**Authors**: Zunhai Su, Zhe Chen, Wang Shen, Hanyu Wei, Linge Li, Huangqi Yu, Kehong Yuan

**Updated**: 2025-01-25T01:45:29Z

**Summary**: Key-Value (KV) cache facilitates efficient large language models (LLMs) inference by avoiding recomputation of past KVs. As the batch size and context length increase, the oversized KV caches become a significant memory bottleneck, highlighting the need for efficient compression. Existing KV quantization rely on fine-grained quantization or the retention of a significant portion of high bit-widths caches, both of which compromise compression ratio and often fail to maintain robustness at extremely low average bit-widths. In this work, we explore the potential of rotation technique for 2-bit KV quantization and propose RotateKV, which achieves accurate and robust performance through the following innovations: (i) Outlier-Aware Rotation, which utilizes channel-reordering to adapt the rotations to varying channel-wise outlier distributions without sacrificing the computational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii) Pre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position embedding (RoPE) on proposed outlier-aware rotation and further smooths outliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages the massive activations to precisely identify and protect attention sinks. RotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit quantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning and long-context capabilities, with less than 1.7\% degradation on GSM8K, outperforming existing methods even at lower average bit-widths. RotateKV also showcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch sizes, and achieves a 2.32x speedup in decoding stage.

**Link**: [arxiv](http://arxiv.org/abs/2501.16383v1),  [pdf](http://arxiv.org/pdf/2501.16383v1)

**Tags**: cs.LG cs.AI cs.CL 



### EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation
**Authors**: Yifan Yu, Yu Gan, Lillian Tsai, Nikhil Sarda, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry M. Levy, David Culler

**Updated**: 2025-01-24T19:13:12Z

**Summary**: Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 60% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge sharing among requests. However, naively caching and reusing past responses leads to large quality degradation. In this paper, we introduce EchoLM, an in-context caching system that leverages historical requests as examples to guide response generation, enabling selective offloading of requests to more efficient LLMs. However, enabling this real-time knowledge transfer leads to intricate tradeoffs between response quality, latency, and system throughput at scale. For a new request, EchoLM identifies similar, high-utility examples and efficiently prepends them to the input for better response. At scale, EchoLM adaptively routes requests to LLMs of varying capabilities, accounting for response quality and serving loads. EchoLM employs a cost-aware cache replay mechanism to improve example quality and coverage offline, maximizing cache utility and runtime efficiency. Evaluations on millions of open-source requests demonstrate that EchoLM has a throughput improvement of 1.4-5.9x while reducing latency by 28-71% without hurting response quality on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.12689v2),  [pdf](http://arxiv.org/pdf/2501.12689v2)

**Tags**: cs.LG 



### Language-Queried Target Sound Extraction Without Parallel Training Data
**Authors**: Hao Ma, Zhiyuan Peng, Xu Li, Yukai Li, Mingjie Shao, Qiuqiang Kong, Ju Liu

**Updated**: 2025-01-24T15:16:48Z

**Summary**: Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a parallel-data-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the contrastive language-audio pre-trained model (CLAP). In a vanilla parallel-data-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding, while during testing, user language queries are encoded by CLAP text encoder as the condition embedding. This vanilla approach assumes perfect alignment between text and audio embeddings, which is unrealistic. Two major challenges arise from training-testing mismatch: the persistent modality gap between text and audio and the risk of overfitting due to the exposure of rich acoustic details in target audio embedding during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and testing and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.

**Link**: [arxiv](http://arxiv.org/abs/2409.09398v2),  [pdf](http://arxiv.org/pdf/2409.09398v2)

**Tags**: eess.AS cs.SD 



### A Programming Model for Disaggregated Memory over CXL
**Authors**: Gal Assa, Lucas Bürgi, Michal Friedman, Ori Lahav

**Updated**: 2025-01-24T14:32:34Z

**Summary**: CXL (Compute Express Link) is an emerging open industry-standard interconnect between processing and memory devices that is expected to revolutionize the way systems are designed in the near future. It enables cache-coherent shared memory pools in a disaggregated fashion at unprecedented scales, allowing algorithms to interact with a variety of storage devices using simple loads and stores. Alongside unleashing unique opportunities for a wide range of applications, CXL introduces new challenges of data management and crash consistency. Alas, CXL lacks an adequate programming model, which makes reasoning about the correctness and expected behaviors of algorithms and systems on top of it nearly impossible.   In this work, we present CXL0, the first programming model for concurrent programs running on top of CXL. We propose a high-level abstraction for CXL memory accesses and formally define operational semantics on top of that abstraction. We perform initial measurements that provide practical insight into CXL0. We provide a set of general transformations that adapt concurrent algorithms to the new disruptive technology. These transformations enhance linearizable algorithms with durability under a general partial-failure model. We provide an additional transformation for algorithms designed for persistent main memory and full-system crashes. We believe that this work will serve as a stepping stone for systems design and modeling on top of CXL, and support the development of future models as software and hardware evolve.

**Link**: [arxiv](http://arxiv.org/abs/2407.16300v2),  [pdf](http://arxiv.org/pdf/2407.16300v2)

**Tags**: cs.DC cs.ET 



### Application-Aware Resource Allocation and Data Management for   MEC-assisted IoT Service Providers
**Authors**: Simone Bolettieri, Raffaele Bruno, Enzo Mingozzi

**Updated**: 2025-01-24T10:39:45Z

**Summary**: To support the growing demand for data-intensive and low-latency IoT applications, Multi-Access Edge Computing (MEC) is emerging as an effective edge-computing approach enabling the execution of delay-sensitive processing tasks close to end-users. However, most of the existing works on resource allocation and service placement in MEC systems overlook the unique characteristics of new IoT use cases. For instance, many IoT applications require the periodic execution of computing tasks on real-time data streams that originate from devices dispersed over a wide area. Thus, users requesting IoT services are typically distant from the data producers. To fill this gap, the contribution of this work is two-fold. Firstly, we propose a MEC-compliant architectural solution to support the operation of multiple IoT service providers over a common MEC platform deployment, which enables the steering and shaping of IoT data transport within the platform. Secondly, we model the problem of service placement and data management in the proposed MEC-based solution taking into account the dependencies at the data level between IoT services and sensing resources. Our model also considers that caches can be deployed on MEC hosts, to allow the sharing of the same data between different IoT services with overlapping geographical scope, and provides support for IoT services with heterogeneous QoS requirements, such as different frequencies of periodic task execution. Due to the complexity of the optimisation problem, a heuristic algorithm is proposed using linear relaxation and rounding techniques. Extensive simulation results demonstrate the efficiency of the proposed approach, especially when traffic demands generated by the service requests are not uniform.

**Link**: [arxiv](http://arxiv.org/abs/2501.14387v1),  [pdf](http://arxiv.org/pdf/2501.14387v1)

**Tags**: cs.NI 



### Joint System Latency and Data Freshness Optimization for Cache-enabled   Mobile Crowdsensing Networks
**Authors**: Kexin Shi, Yaru Fu, Yongna Guo, Fu Lee Wang, Yan Zhang

**Updated**: 2025-01-24T10:00:21Z

**Summary**: Mobile crowdsensing (MCS) networks enable large-scale data collection by leveraging the ubiquity of mobile devices. However, frequent sensing and data transmission can lead to significant resource consumption. To mitigate this issue, edge caching has been proposed as a solution for storing recently collected data. Nonetheless, this approach may compromise data freshness. In this paper, we investigate the trade-off between re-using cached task results and re-sensing tasks in cache-enabled MCS networks, aiming to minimize system latency while maintaining information freshness. To this end, we formulate a weighted delay and age of information (AoI) minimization problem, jointly optimizing sensing decisions, user selection, channel selection, task allocation, and caching strategies. The problem is a mixed-integer non-convex programming problem which is intractable. Therefore, we decompose the long-term problem into sequential one-shot sub-problems and design a framework that optimizes system latency, task sensing decision, and caching strategy subproblems. When one task is re-sensing, the one-shot problem simplifies to the system latency minimization problem, which can be solved optimally. The task sensing decision is then made by comparing the system latency and AoI. Additionally, a Bayesian update strategy is developed to manage the cached task results. Building upon this framework, we propose a lightweight and time-efficient algorithm that makes real-time decisions for the long-term optimization problem. Extensive simulation results validate the effectiveness of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2501.14367v1),  [pdf](http://arxiv.org/pdf/2501.14367v1)

**Tags**: cs.NI eess.SP 



### Locality-aware Fair Scheduling in LLM Serving
**Authors**: Shiyi Cao, Yichuan Wang, Ziming Mao, Pin-Lun Hsu, Liangsheng Yin, Tian Xia, Dacheng Li, Shu Liu, Yineng Zhang, Yang Zhou, Ying Sheng, Joseph Gonzalez, Ion Stoica

**Updated**: 2025-01-24T08:12:47Z

**Summary**: Large language model (LLM) inference workload dominates a wide variety of modern AI applications, ranging from multi-turn conversation to document analysis. Balancing fairness and efficiency is critical for managing diverse client workloads with varying prefix patterns. Unfortunately, existing fair scheduling algorithms for LLM serving, such as Virtual Token Counter (VTC), fail to take prefix locality into consideration and thus suffer from poor performance. On the other hand, locality-aware scheduling algorithms in existing LLM serving frameworks tend to maximize the prefix cache hit rate without considering fair sharing among clients.   This paper introduces the first locality-aware fair scheduling algorithm, Deficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix locality with a fairness guarantee. We also introduce a novel algorithm, Double Deficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find a balance point among fairness, locality, and load-balancing. Our extensive evaluation demonstrates the superior performance of DLPM and D$^2$LPM in ensuring fairness while maintaining high throughput (up to 2.87$\times$ higher than VTC) and low per-client (up to 7.18$\times$ lower than state-of-the-art distributed LLM serving system) latency.

**Link**: [arxiv](http://arxiv.org/abs/2501.14312v1),  [pdf](http://arxiv.org/pdf/2501.14312v1)

**Tags**: cs.DC cs.LG 



### Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement   Learning-based Model Caching and Inference Offloading
**Authors**: Minrui Xu, Dusit Niyato, Christopher G. Brinton

**Updated**: 2025-01-24T03:21:20Z

**Summary**: Large Language Models (LLMs) can perform zero-shot learning on unseen tasks and few-shot learning on complex reasoning tasks. However, resource-limited mobile edge networks struggle to support long-context LLM serving for LLM agents during multi-round interactions with users. Unlike stateless computation offloading and static service offloading in edge computing, optimizing LLM serving at edge servers is challenging because LLMs continuously learn from context which raises accuracy, latency, and resource consumption dynamics. In this paper, we propose a joint model caching and inference offloading framework that utilizes test-time deep reinforcement learning (T2DRL) to optimize deployment and execution strategies for long-context LLM serving. In this framework, we analyze the performance convergence and design an optimization problem considering the utilization of context windows in LLMs. Furthermore, the T2DRL algorithm can learn in both the training phase and the testing phase to proactively manage cached models and service requests and adapt to context changes and usage patterns during execution. To further enhance resource allocation efficiency, we propose a double Dutch auction (DDA) mechanism, which dynamically matches supply and demand while maximizing social welfare. Finally, experimental results demonstrate that the T2DRL algorithm can reduce system costs by at least 30% compared to baselines while guaranteeing the performance of LLM agents in real-world perception and reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.14205v1),  [pdf](http://arxiv.org/pdf/2501.14205v1)

**Tags**: cs.NI 



### Sigma: Differential Rescaling of Query, Key and Value for Efficient   Language Models
**Authors**: Zhenghao Lin, Zihao Tang, Xiao Liu, Yeyun Gong, Yi Cheng, Qi Chen, Hang Li, Ying Xin, Ziyue Yang, Kailai Yang, Yu Yan, Xiao Liang, Shuai Lu, Yiming Huang, Zheheng Luo, Lei Qu, Xuan Feng, Yaoxiang Wang, Yuqing Xia, Feiyang Chen, Yuting Jiang, Yasen Hu, Hao Ni, Binyang Li, Guoshuai Zhao, Jui-Hao Chiang, Zhongxin Guo, Chen Lin, Kun Kuang, Wenjie Li, Yelong Shen, Jian Jiao, Peng Cheng, Mao Yang

**Updated**: 2025-01-23T12:58:14Z

**Summary**: We introduce Sigma, an efficient large language model specialized for the system domain, empowered by a novel architecture including DiffQKV attention, and pre-trained on our meticulously collected system domain data. DiffQKV attention significantly enhances the inference efficiency of Sigma by optimizing the Query (Q), Key (K), and Value (V) components in the attention mechanism differentially, based on their varying impacts on the model performance and efficiency indicators. Specifically, we (1) conduct extensive experiments that demonstrate the model's varying sensitivity to the compression of K and V components, leading to the development of differentially compressed KV, and (2) propose augmented Q to expand the Q head dimension, which enhances the model's representation capacity with minimal impacts on the inference speed. Rigorous theoretical and empirical analyses reveal that DiffQKV attention significantly enhances efficiency, achieving up to a 33.36% improvement in inference speed over the conventional grouped-query attention (GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various sources, including 19.5B system domain data that we carefully collect and 1T tokens of synthesized and rewritten data. In general domains, Sigma achieves comparable performance to other state-of-arts models. In the system domain, we introduce the first comprehensive benchmark AIMicius, where Sigma demonstrates remarkable performance across all tasks, significantly outperforming GPT-4 with an absolute improvement up to 52.5%.

**Link**: [arxiv](http://arxiv.org/abs/2501.13629v1),  [pdf](http://arxiv.org/pdf/2501.13629v1)

**Tags**: cs.CL 



### Characterisation of the plutonium isotopic composition of a sediment   core from Palomares, Spain, by low-energy AMS and alpha-spectrometry
**Authors**: E. Chamizo, M. C. Jiménez-Ramos, S. M. Enamorado, M. García-León, R. García-Tenorio, J. L. Mas, P. Masqué, J. Merino, J. A. Sanchez-Cabeza

**Updated**: 2025-01-23T11:18:42Z

**Summary**: The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the compact accelerator mass spectrometry (AMS) system at the Centro Nacional de Aceleradores (CNA) in Seville, Spain, is now a reality. In this work, we present first Pu AMS results for environmental samples: a sediment core collected in a submarine canyon in the Mediterranean coast of the Spanish region of Palomares, affected by a nuclear accident in 1966. From the study of the 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%, we confirm that the weapon-grade plutonium released on land during the accident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its way into the marine environment. A two-plutonium sources mixture model (Palomares and fallout) is used to elucidate the percentage of the plutonium coming from the accident. As a validation exercise of the Pu AMS measuring technique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples were also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu activity concentration results fit in with the AMS ones in a wide dynamic range, thus validating the AMS technique.

**Link**: [arxiv](http://arxiv.org/abs/2501.13998v1),  [pdf](http://arxiv.org/pdf/2501.13998v1)

**Tags**: physics.ins-det physics.ao-ph 



### POPS: From History to Mitigation of DNS Cache Poisoning Attacks
**Authors**: Yehuda Afek, Harel Berger, Anat Bremler-Barr

**Updated**: 2025-01-23T10:40:09Z

**Summary**: We present a novel yet simple and comprehensive DNS cache POisoning Prevention System (POPS), designed to integrate as a module in Intrusion Prevention Systems (IPS). POPS addresses statistical DNS poisoning attacks, including those documented from 2002 to the present, and offers robust protection against similar future threats. It consists of two main components: a detection module that employs three simple rules, and a mitigation module that leverages the TC flag in the DNS header to enhance security. Once activated, the mitigation module has zero false positives or negatives, correcting any such errors on the side of the detection module.   We first analyze POPS against historical DNS services and attacks, showing that it would have mitigated all network-based statistical poisoning attacks, yielding a success rate of only 0.0076% for the adversary. We then simulate POPS on traffic benchmarks (PCAPs) incorporating current potential network-based statistical poisoning attacks, and benign PCAPs; the simulated attacks still succeed with a probability of 0.0076%. This occurs because five malicious packets go through before POPS detects the attack and activates the mitigation module. In addition, POPS completes its task using only 20%-50% of the time required by other tools (e.g., Suricata or Snort), and after examining just 5%-10% as many packets. Furthermore, it successfully identifies DNS cache poisoning attacks-such as fragmentation attacks-that both Suricata and Snort fail to detect, underscoring its superiority in providing comprehensive DNS protection.

**Link**: [arxiv](http://arxiv.org/abs/2501.13540v1),  [pdf](http://arxiv.org/pdf/2501.13540v1)

**Tags**: cs.CR cs.NI 



### A Training-free Sub-quadratic Cost Transformer Model Serving Framework   With Hierarchically Pruned Attention
**Authors**: Heejun Lee, Geon Park, Youngwan Lee, Jaduk Suh, Jina Kim, Wonyoung Jeong, Bumsik Kim, Hyemin Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2025-01-23T07:25:28Z

**Summary**: In modern large language models (LLMs), increasing the context length is crucial for improving comprehension and coherence in long-context, multi-modal, and retrieval-augmented language generation. While many recent transformer models attempt to extend their context length over a million tokens, they remain impractical due to the quadratic time and space complexities. Although recent works on linear and sparse attention mechanisms can achieve this goal, their real-world applicability is often limited by the need to re-train from scratch and significantly worse performance. In response, we propose a novel approach, Hierarchically Pruned Attention (HiP), which reduces the time complexity of the attention mechanism to $O(T \log T)$ and the space complexity to $O(T)$, where $T$ is the sequence length. We notice a pattern in the attention scores of pretrained LLMs where tokens close together tend to have similar scores, which we call ``attention locality''. Based on this observation, we utilize a novel tree-search-like algorithm that estimates the top-$k$ key tokens for a given query on the fly, which is mathematically guaranteed to have better performance than random attention pruning. In addition to improving the time complexity of the attention mechanism, we further optimize GPU memory usage by implementing KV cache offloading, which stores only $O(\log T)$ tokens on the GPU while maintaining similar decoding throughput. Experiments on benchmarks show that HiP, with its training-free nature, significantly reduces both prefill and decoding latencies, as well as memory usage, while maintaining high-quality generation with minimal degradation. HiP enables pretrained LLMs to scale up to millions of tokens on commodity GPUs, potentially unlocking long-context LLM applications previously deemed infeasible.

**Link**: [arxiv](http://arxiv.org/abs/2406.09827v3),  [pdf](http://arxiv.org/pdf/2406.09827v3)

**Tags**: cs.CL cs.CV cs.DC cs.LG 



### Parallel Key-Value Cache Fusion for Position Invariant RAG
**Authors**: Philhoon Oh, Jinwoo Shin, James Thorne

**Updated**: 2025-01-23T06:48:22Z

**Summary**: Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2501.07523v2),  [pdf](http://arxiv.org/pdf/2501.07523v2)

**Tags**: cs.AI cs.CL 



### Qrazor: Reliable and effortless 4-bit llm quantization by significant   data razoring
**Authors**: Dongyoung Lee, Seungkyu Choi, Ik Joon Chang

**Updated**: 2025-01-23T02:20:08Z

**Summary**: Large-scale language models (LLMs) have demonstrated outstanding performance in language processing tasks, yet their deployment is often hindered by high memory demands and computational complexity. Although low-bit quantization techniques, such as 4-bit quantization, present a potential solution, they frequently lead to significant accuracy degradation or require substantial effort for such aggressive quantization approaches. To overcome these challenges, we introduce QRazor, a reliable and effortless quantization scheme designed to enable 4-bit quantization for weights, activations, and KV cache in transformer-based LLMs. The scheme involves two main stages: quantization and compression. During the quantization stage, weights, activations, and KV cache values are quantized with wider 8 or 16-bit integers as a basis to achieve nearly identical accuracy to the original full-precision LLM models, using the absolute max scaling. Subsequently, all data are compressed to 4-bit using our proposed significant data razoring (SDR) technique, which retains only the four most salient bits while discarding the others. Furthermore, we present an integer-based arithmetic unit dedicated to QRazor, enabling direct low-precision arithmetic operations without decompressing the SDR data. Despite the reduced quantization effort, QRazor achieves LLM accuracies better or comparable to state-of-the-art 4-bit methods. By also validating the hardware efficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8% reduction in area and power consumption, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.13331v1),  [pdf](http://arxiv.org/pdf/2501.13331v1)

**Tags**: cs.LG 



### Personalized Federated Learning for Cellular VR: Online Learning and   Dynamic Caching
**Authors**: Krishnendu S. Tharakan, Hayssam Dahrouj, Nour Kouzayha, Hesham ElSawy, Tareq Y. Al-Naffouri

**Updated**: 2025-01-22T16:25:47Z

**Summary**: Delivering an immersive experience to virtual reality (VR) users through wireless connectivity offers the freedom to engage from anywhere at any time. Nevertheless, it is challenging to ensure seamless wireless connectivity that delivers real-time and high-quality videos to the VR users. This paper proposes a field of view (FoV) aware caching for mobile edge computing (MEC)-enabled wireless VR network. In particular, the FoV of each VR user is cached/prefetched at the base stations (BSs) based on the caching strategies tailored to each BS. Specifically, decentralized and personalized federated learning (DP-FL) based caching strategies with guarantees are presented. Considering VR systems composed of multiple VR devices and BSs, a DP-FL caching algorithm is implemented at each BS to personalize content delivery for VR users. The utilized DP-FL algorithm guarantees a probably approximately correct (PAC) bound on the conditional average cache hit. Further, to reduce the cost of communicating gradients, one-bit quantization of the stochastic gradient descent (OBSGD) is proposed, and a convergence guarantee of $\mathcal{O}(1/\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is the number of iterations. Additionally, to better account for the wireless channel dynamics, the FoVs are grouped into multicast or unicast groups based on the number of requesting VR users. The performance of the proposed DP-FL algorithm is validated through realistic VR head-tracking dataset, and the proposed algorithm is shown to have better performance in terms of average delay and cache hit as compared to baseline algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2501.11745v2),  [pdf](http://arxiv.org/pdf/2501.11745v2)

**Tags**: cs.IT cs.LG math.IT 



### Efficient Prompt Compression with Evaluator Heads for Long-Context   Transformer Inference
**Authors**: Weizhi Fei, Xueyan Niu, Guoqing Xie, Yingqing Liu, Bo Bai, Wei Han

**Updated**: 2025-01-22T15:33:17Z

**Summary**: Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly "skim through" input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively reduces the complexity and costs associated with commercial API calls. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.12959v1),  [pdf](http://arxiv.org/pdf/2501.12959v1)

**Tags**: cs.CL 



### Yi-Lightning Technical Report
**Authors**: Alan Wake, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Fan Zhou, Feng Hu, Ge Zhang, Guoyin Wang, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiming Yang, Shiyong Li, Tianhang Zhu, Wen Xie, Wenhao Huang, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang, Zonghong Dai

**Updated**: 2025-01-22T15:09:58Z

**Summary**: This technical report presents Yi-Lightning, our latest flagship large language model (LLM). It achieves exceptional performance, ranking 6th overall on Chatbot Arena, with particularly strong results (2nd to 4th place) in specialized categories including Chinese, Math, Coding, and Hard Prompts. Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture, featuring advanced expert segmentation and routing mechanisms coupled with optimized KV-caching techniques. Our development process encompasses comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement learning from human feedback (RLHF), where we devise deliberate strategies for multi-stage training, synthetic data construction, and reward modeling. Furthermore, we implement RAISE (Responsible AI Safety Engine), a four-component framework to address safety issues across pre-training, post-training, and serving phases. Empowered by our scalable super-computing infrastructure, all these innovations substantially reduce training, deployment and inference costs while maintaining high-performance standards. With further evaluations on public academic benchmarks, Yi-Lightning demonstrates competitive performance against top-tier LLMs, while we observe a notable disparity between traditional, static benchmark results and real-world, dynamic human preferences. This observation prompts a critical reassessment of conventional benchmarks' utility in guiding the development of more intelligent and powerful AI systems for practical applications. Yi-Lightning is now available through our developer platform at https://platform.lingyiwanwu.com.

**Link**: [arxiv](http://arxiv.org/abs/2412.01253v5),  [pdf](http://arxiv.org/pdf/2412.01253v5)

**Tags**: cs.CL cs.AI cs.LG 



### Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic   Wrap-Around
**Authors**: Elizabath Peter, K. K. Krishnan Namboodiri, B. Sundar Rajan

**Updated**: 2025-01-22T15:05:08Z

**Summary**: This work explores a multiple transmit antenna setting in a multi-access coded caching (MACC) network where each user accesses more than one cache. A MACC network has $K$ users and $K$ caches, and each user has access to $r < K$ consecutive caches in a cyclic wrap-around manner. There are $L$ antennas at the server, and each cache has a normalized size of $M/N \leq 1$. The cyclic wrap-around MACC network with a single antenna at the server has been a well-investigated topic, and several coded caching schemes and improved lower bounds on the performance are known for the same. However, this MACC network has not yet been studied under multi-antenna settings in the coded caching literature. We study the multi-antenna MACC problem and propose a solution for the same by constructing a pair of arrays called caching and delivery arrays. We present three constructions of caching and delivery arrays for different scenarios and obtain corresponding multi-antenna MACC schemes for the same. Two schemes resulting from the above constructions achieve optimal performance under uncoded placement and one-shot delivery. The optimality is shown by matching the performance of the multi-antenna MACC scheme to that of an optimal multi-antenna scheme for a dedicated cache network having an identical number of users, and each user has a normalized cache size of $rM/N$. Further, as a special case, one of the proposed schemes subsumes an existing optimal MACC scheme for the single-antenna setting.

**Link**: [arxiv](http://arxiv.org/abs/2310.08894v3),  [pdf](http://arxiv.org/pdf/2310.08894v3)

**Tags**: cs.IT math.IT 



### Not all tokens are created equal: Perplexity Attention Weighted Networks   for AI generated text detection
**Authors**: Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho

**Updated**: 2025-01-22T10:39:50Z

**Summary**: The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.03940v2),  [pdf](http://arxiv.org/pdf/2501.03940v2)

**Tags**: cs.CL cs.AI 



### Bright single-photon source in a silicon chip by nanoscale positioning   of a color center in a microcavity
**Authors**: Baptiste Lefaucher, Yoann Baron, Jean-Baptiste Jager, Vincent Calvo, Christian Elsässer, Giuliano Coppola, Frédéric Mazen, Sébastien Kerdilès, Félix Cache, Anaïs Dréau, Jean-Michel Gérard

**Updated**: 2025-01-22T09:25:29Z

**Summary**: We present an all-silicon source of near-infrared linearly-polarized single photons, fabricated by nanoscale positioning of a color center in a silicon-on-insulator microcavity. The color center consists of a single W center, created at a well-defined position by Si$^{+}$ ion implantation through a 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant with the W's zero-phonon line at 1217 nm is fabricated at the same location as the nanohole. Under above-gap continuous-wave excitation, a very clean photon antibunching behavior ($g{^2} \leq 0.06$) is observed over the entire power range, which highlights the absence of parasitic emitters. Purcell-enhancement of W's zero-phonon emission provides both a record-high photoluminescence count rate among Si color centers (ca $1.2 \times 10^{6}$ counts/s) and apparent Debye-Waller factor around 99%. We also demonstrate the triggered emission of single photons with 93% purity under weak pulsed laser excitation. At high pulsed laser power, we reveal a detrimental effect of repumping processes, that could be mitigated using selective pumping schemes in the future. These results represent a major step towards on-demand sources of indistinguishable near-infrared single photons within silicon photonics chips.

**Link**: [arxiv](http://arxiv.org/abs/2501.12744v1),  [pdf](http://arxiv.org/pdf/2501.12744v1)

**Tags**: physics.optics quant-ph 



### Improved Coded Caching Scheme for Multi-User Information Retrieval   System
**Authors**: Junyi Wang, Quan Zang, Jinyu Wang, Minquan Cheng

**Updated**: 2025-01-21T22:33:15Z

**Summary**: In this paper, we study the coded caching scheme for the $(L, K, M, N)$ multi-user information retrieval (MIR) system, which consists of a content library containing $N$ files, a base station (BS) with $L$ antennas that cannot access the library, and $K$ single-antenna users, each of which can cache at most $M$ files from the library. The users communicate with the others assisted by the BS to decode their required files. In this paper, we focus on designing a coded caching scheme with low communication latency measured by normalized delivery time (NDT), computational complexity, and subpacketizations. When $\frac{KM}{N}\geq L$ we first simply the precoding matrix in the downlink step to an identity matrix and use the multiple-antenna placement delivery array (MAPDA), which was originally proposed for the multiple-input single-output networks, to generate several new schemes for MIR system. Compared to the existing schemes, both the theoretical and numerical analyses show that our new schemes achieve much lower computational complexity and smaller subpacketizations with the same NDT.

**Link**: [arxiv](http://arxiv.org/abs/2501.12528v1),  [pdf](http://arxiv.org/pdf/2501.12528v1)

**Tags**: cs.IT math.IT 



### Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and   Multiple Level Analysis
**Authors**: Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu

**Updated**: 2025-01-21T12:19:02Z

**Summary**: Modern GPUs, with their specialized hardware like tensor cores, are essential for demanding AI and deep learning applications. This study presents a comprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU architecture, delving into its performance characteristics and novel features. We benchmark Hopper's memory subsystem latency and throughput, comparing its L2 partitioned cache behavior and global memory access patterns against recent GPU generations, Ampere and Ada Lovelace. Our analysis reveals significant performance differences and architectural improvements in Hopper. A core contribution of this work is a detailed evaluation of Hopper's fourth-generation tensor cores, including their FP8 precision support and the novel asynchronous wgmma instructions, assessing their impact on matrix multiply-accumulate operations. We further investigate the performance implications of other key Hopper innovations: DPX instructions for accelerating dynamic programming algorithms, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. This multi-level approach encompasses instruction-level microbenchmarks, library-level analysis of the Transformer Engine, and application-level benchmarks of tensor core performance within large language models. Our findings provide valuable, in-depth insights for software developers seeking to optimize performance and develop accurate performance models for the Hopper architecture, ultimately contributing to a deeper understanding of its potential for accelerating AI and other computationally intensive workloads.

**Link**: [arxiv](http://arxiv.org/abs/2501.12084v1),  [pdf](http://arxiv.org/pdf/2501.12084v1)

**Tags**: cs.DC cs.AR cs.PF 



### Build Optimization: A Systematic Literature Review
**Authors**: Henri Aïdasso, Mohammed Sayagh, Francis Bordeleau

**Updated**: 2025-01-21T07:32:06Z

**Summary**: Continuous Integration (CI) consists of an automated build process involving continuous compilation, testing, and packaging of the software system. While CI comes up with several advantages related to quality and time to delivery, CI also presents several challenges addressed by a large body of research. To better understand the literature so as to help practitioners find solutions for their problems and guide future research, we conduct a systematic review of 97 studies on build optimization published between 2006 and 2024, which we summarized according to their goals, methodologies, used datasets, and leveraged metrics. The identified build optimization studies focus on two main challenges: (1) long build durations, and (2) build failures. To meet the first challenge, existing studies have developed a range of techniques, including predicting build outcome and duration, selective build execution, and build acceleration using caching or repairing performance smells. The causes of build failures have been the subject of several studies, leading to the development of techniques for predicting build script maintenance and automating repair. Recent studies have also focused on predicting flaky build failures caused by environmental issues. The majority of these techniques use machine learning algorithms and leverage build metrics, which we classify into five categories. Additionally, we identify eight publicly available build datasets for build optimization research.

**Link**: [arxiv](http://arxiv.org/abs/2501.11940v1),  [pdf](http://arxiv.org/pdf/2501.11940v1)

**Tags**: cs.SE 



### PDA Construction via Union of Cartesian Product Cache Configurations for   Coded Caching
**Authors**: Jinyu Wang, Minquan Cheng, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-21T02:35:31Z

**Summary**: Caching is an efficient technique to reduce peak traffic by storing popular content in local caches. Placement delivery array (PDA) proposed by Yan et al. is a combinatorial structure to design coded caching schemes with uncoded placement and one-shot linear delivery. By taking the $m$-fold Cartesian product of a small base PDA, Wang et al. constructed a big PDA while maintaining the memory ratio and transmission load unchanged, which achieves linear growth in both the number of users and coded caching gain. In order to achieve exponential growth in both the number of users and coded caching gain, in this paper we propose a PDA construction by taking the union operation of the cache configurations from the $m$-fold Cartesian product of a base PDA. The resulting PDA leads to a coded caching scheme with subpacketization increasing sub-exponentially with the number of users while keeping the load constant for fixed memory ratio. By applying the proposed construction to existing base PDAs, three new coded caching schemes are obtained, which cover some existing schemes as special cases and can achieve lower load with simultaneously lower subpacketization for some memory ratios.

**Link**: [arxiv](http://arxiv.org/abs/2501.11834v1),  [pdf](http://arxiv.org/pdf/2501.11834v1)

**Tags**: cs.IT math.IT 



### Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference
**Authors**: Pouya Hamadanian, Sadjad Fouladi

**Updated**: 2025-01-20T23:10:13Z

**Summary**: Large Language Models (LLM) have revolutionized natural language processing, but their inference demands substantial resources, while under-utilizing high-end accelerators like GPUs. A major bottleneck arises from the attention mechanism, which requires storing large key-value caches, limiting the maximum achievable throughput way below the available computing resources. Current approaches attempt to mitigate this issue through memory-efficient attention and paging mechanisms, but remained constrained by the assumption that all operations must be performed on high-end accelerators.   In this work, we propose Glinthawk, a two-tiered architecture that decouples the attention mechanism from the rest of the Transformer model. This approach allows the memory requirements for attention to scale independently, enabling larger batch sizes and more efficient use of the high-end accelerators. We prototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the other. Compared to a traditional single-tier setup, it improves throughput by $5.9\times$ and reduces cost of generation by $2.8\times$. For longer sequence lengths, it achieves $16.3\times$ throughput improvement at $2.4\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-oriented applications such as batch processing. We shared our prototype publicly at \url{https://github.com/microsoft/glinthawk}.

**Link**: [arxiv](http://arxiv.org/abs/2501.11779v1),  [pdf](http://arxiv.org/pdf/2501.11779v1)

**Tags**: cs.LG cs.DC cs.PF 



### Hierarchical Coded Caching in High Memory Regime with Coded Placement
**Authors**: Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-01-20T14:19:48Z

**Summary**: We consider a two-layer hierarchical coded caching network where a server with a library of $N$ files is connected to $K_1$ mirrors, each having a cache memory of size $M_1$. Each mirror is further connected to $K_2$ users, each equipped with a dedicated cache of size $M_2$. In this paper, we propose two distinct coded caching schemes based on coded placement, corresponding to two distinct memory pairs, \( (M_1, M_2) \). We show that the proposed schemes outperform the existing schemes at these memory points given by the proposed schemes for smaller values of $K_2$. In setups where mirrors are positioned near each other, avoiding signal interference is crucial. This can be ensured by having all mirrors transmit using orthogonal carrier frequencies. To compare our schemes with existing ones, we used the composite rate metric, which accurately represents the total bandwidth utilized in such setups. The composite rate is given by $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to the mirrors, and $R_2$ is the rate from the mirrors to the users, with respect to $M_1$ and $M_2$.

**Link**: [arxiv](http://arxiv.org/abs/2501.11502v1),  [pdf](http://arxiv.org/pdf/2501.11502v1)

**Tags**: cs.IT math.IT 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-01-20T08:44:01Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v3),  [pdf](http://arxiv.org/pdf/2411.10659v3)

**Tags**: cs.PL 



### ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large   Vision-Language Models
**Authors**: Yassir Bendou, Amine Ouasfi, Vincent Gripon, Adnane Boukhayma

**Updated**: 2025-01-19T21:25:53Z

**Summary**: The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness and versatility, efficient few-shot adaptation techniques have been widely adopted. Among these approaches, training-free methods, particularly caching methods exemplified by Tip-Adapter, have gained attention for their lightweight adaptation without the need for additional fine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective, showing that caching methods function as local adapters and are connected to a well-established kernel literature. Drawing on this insight, we offer a theoretical understanding of how these methods operate and suggest multiple avenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the importance of incorporating global information in local adapters. Therefore, we subsequently propose a global method that learns a proximal regularizer in a reproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our method, which we call ProKeR (Proximal Kernel ridge Regression), has a closed form solution and achieves state-of-the-art performances across 11 datasets in the standard few-shot adaptation benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2501.11175v1),  [pdf](http://arxiv.org/pdf/2501.11175v1)

**Tags**: cs.CV cs.AI cs.LG 



### Cache Coherence Over Disaggregated Memory
**Authors**: Ruihong Wang, Jianguo Wang, Walid G. Aref

**Updated**: 2025-01-19T19:46:21Z

**Summary**: Disaggregating memory from compute offers the opportunity to better utilize stranded memory in cloud data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes. However, the limited computing power on disaggregated memory servers makes traditional cache coherence protocols suboptimal, particularly in the case of stranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. It aligns the state machine of the shared-exclusive latch protocol with the MSI protocol by introducing lazy latch-release and invalidation messages, thereby ensuring both atomicity of data access and cache coherence. SELCC embeds cache-ownership metadata directly into the RDMA latch word, enabling efficient cache ownership management via RDMA atomic operations. SELCC can serve as an abstraction layer over disaggregated memory with APIs that resemble main-memory accesses. A concurrent B-tree and three transaction concurrency control algorithms are realized using SELCC's abstraction layer. Experimental results show that SELCC significantly outperforms Remote-Procedure-Call-based protocols for cache coherence under limited remote computing power. Applications on SELCC achieve comparable or superior performance over disaggregated memory compared to competitors.

**Link**: [arxiv](http://arxiv.org/abs/2409.02088v3),  [pdf](http://arxiv.org/pdf/2409.02088v3)

**Tags**: cs.DB cs.DC cs.ET 



### Coded Caching for Hierarchical Two-Layer Networks with Coded Placement
**Authors**: Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-01-19T15:47:14Z

**Summary**: We examine a two-layered hierarchical coded caching problem, a configuration addressed in existing research. This involves a server connected to $K_1$ mirrors, each of which serves $K_2$ users. The mirrors and the users are equipped with caches of size $M_1$ and $M_2$, respectively. We propose a hierarchical coded caching scheme with coded placements that outperforms existing schemes. To ensure a fair comparison, we introduce the notion of composite rate, defined as $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to mirrors and $R_2$ is the rate from mirrors to users. The composite rate has not been discussed before in the literature and is pertinent when mirrors transmit with different carrier frequencies. For the proposed scheme, we show a trade-off between the global memory $\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and compare with the existing schemes. Additionally, we conduct this comparative analysis by plotting $R_1$ + $R_2$ against global memory, which is particularly beneficial for systems wherein each mirror can utilize the same carrier frequency, given their significant spatial separation. Additionally, we propose an optimized scheme for the specific case of a single mirror, showing improved performance in this scenario.

**Link**: [arxiv](http://arxiv.org/abs/2312.15024v2),  [pdf](http://arxiv.org/pdf/2312.15024v2)

**Tags**: cs.IT math.IT 



### D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial   Access Topology
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-01-18T13:04:23Z

**Summary**: This paper considers wireless device-to-device (D2D) coded caching in a multiaccess network, where the users communicate with each other and each user can access multiple cache nodes. Access topologies derived from two combinatorial designs known as the $t$-design and $t$-group divisible design ($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively, which subsume a few other known topologies, have been studied for the multiaccess coded caching (MACC) network by Cheng \textit{et al.} in \cite{MACC_des}. These access topologies are extended to a multiaccess D2D coded caching (MADCC) network and novel MADCC schemes are proposed. MADCC network has been studied so far only for the cyclic wrap-around topology. Apart from the proposed novel MADCC schemes, MADCC schemes are also derived from the existing MACC schemes in \cite{MACC_des}. To compare the performance of different MADCC schemes, the metrics of load per user and subpacketization level are used while keeping the number of caches and cache memory size same. The proposed MADCC scheme with $t$-design topology performs better in terms of subpacketization level while achieving the same load per user compared to the MADCC scheme derived from the MACC scheme with $t$-design topology in \cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs better in terms of load per user while achieving the same subpacketization level compared to the MADCC scheme derived from the MACC scheme with $t$-GDD topology in \cite{MACC_des} in some cases. Compared to the existing MADCC scheme with cyclic wrap-around topology, the proposed MADCC scheme with $t$-design topology performs better in terms of load per user, and the proposed MADCC scheme with $t$-GDD topology performs better in terms of subpacketization level at the expense of an increase in load per user.

**Link**: [arxiv](http://arxiv.org/abs/2501.10756v1),  [pdf](http://arxiv.org/pdf/2501.10756v1)

**Tags**: cs.IT math.IT 



### SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS   and Hardware Co-design
**Authors**: Haoyang Zhang, Yuqi Xue, Yirui Eric Zhou, Shaobo Li, Jian Huang

**Updated**: 2025-01-18T07:29:20Z

**Summary**: The CXL-based solid-state drive (CXL-SSD) provides a promising approach towards scaling the main memory capacity at low cost. However, the CXL-SSD faces performance challenges due to the long flash access latency and unpredictable events such as garbage collection in the SSD device, stalling the host processor and wasting compute cycles. Although the CXL interface enables the byte-granular data access to the SSD, accessing flash chips is still at page granularity due to physical limitations. The mismatch of access granularity causes significant unnecessary I/O traffic to flash chips, worsening the suboptimal end-to-end data access performance. In this paper, we present SkyByte, an efficient CXL-based SSD that employs a holistic approach to address the aforementioned challenges by co-designing the host operating system (OS) and SSD controller. To alleviate the long memory stall when accessing the CXL-SSD, SkyByte revisits the OS context switch mechanism and enables opportunistic context switches upon the detection of long access delays. To accommodate byte-granular data accesses, SkyByte architects the internal DRAM of the SSD controller into a cacheline-level write log and a page-level data cache, and enables data coalescing upon log cleaning to reduce the I/O traffic to flash chips. SkyByte also employs optimization techniques that include adaptive page migration for exploring the performance benefits of fast host memory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with a CXL-SSD simulator and evaluate its efficiency with various data-intensive applications. Our experiments show that SkyByte outperforms current CXL-based SSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average. SkyByte also reaches 75% of the performance of the ideal case that assumes unlimited DRAM capacity in the host, which offers an attractive cost-effective solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.10682v1),  [pdf](http://arxiv.org/pdf/2501.10682v1)

**Tags**: cs.AR 



### Geometric rigidity of simple modules for algebraic groups
**Authors**: Michael Bate, David I. Stewart

**Updated**: 2025-01-17T16:16:54Z

**Summary**: Let k be a field, let G be an affine algebraic k-group and V a finite-dimensional G-module. We say V is rigid if the socle series and radical series coincide for the action of G on each indecomposable summand of V; say V is geometrically rigid (resp. absolutely rigid) if V is rigid after base change of G and V to k (resp. any field extension of k). We show that all simple G-modules are geometrically rigid, though not in general absolutely rigid. More precisely, we show that if V is a simple G-module, then there is a finite purely inseparable extension kV /k naturally attached to V such that V is absolutely rigid as a G-module after base change to kV. The proof turns on an investigation of algebras of the form K otimes E where K and E are field extensions of k; we give an example of such an algebra which is not rigid as a module over itself. We establish the existence of the purely inseparable field extension kV /k through an analogous version for artinian algebras.   In the second half of the paper we apply recent results on the structure and representation theory of pseudo-reductive groups to give a concrete description of kV when G is smooth and connected. Namely, we combine the main structure theorem of the Conrad-Prasad classification of pseudo-reductive G together with our previous high weight theory. For V a simple G-module, we calculate the minimal field of definition of the geometric Jacobson radical of EndG(V) in terms of the high weight of V and the Conrad-Prasad classification data; this gives a concrete construction of the field kV as a subextension of the minimal field of definition of the geometric unipotent radical of G. We also observe that the Conrad-Prasad classification can be used to hone the dimension formula for V we had previously established; we also use it to give a description of EndG(V) which includes a dimension formula.

**Link**: [arxiv](http://arxiv.org/abs/2409.05221v3),  [pdf](http://arxiv.org/pdf/2409.05221v3)

**Tags**: math.RT math.GR math.RA 20G05 



### The NIC should be part of the OS
**Authors**: Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-01-17T12:01:28Z

**Summary**: The network interface adapter (NIC) is a critical component of a modern cloud server which occupies a unique position. Not only is network performance vital to the efficient operation of the machine, but unlike application-oriented compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency. Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path. However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernel-bypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.10138v1),  [pdf](http://arxiv.org/pdf/2501.10138v1)

**Tags**: cs.OS cs.AR cs.NI 



### BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix   Sharing and Throughput-oriented Token Batching
**Authors**: Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng

**Updated**: 2025-01-17T09:37:36Z

**Summary**: Large language models (LLMs) increasingly play an important role in a wide range of information processing and management tasks. Many of these tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix between requests. The KV context that are about to be reused may prematurely evicted with the implicit cache management. Besides, the streaming oriented systems do not leverage the request-batch information and can not mix the decoding tokens with the prefill chunks to the best for the batched scenarios, and thus fails to saturate the GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks, and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared Attention kernel with horizontal fusion to reduce tail effect and kernel launch overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical industry workload under different hardware environments.

**Link**: [arxiv](http://arxiv.org/abs/2412.03594v2),  [pdf](http://arxiv.org/pdf/2412.03594v2)

**Tags**: cs.CL cs.AI cs.DC cs.LG 



### Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing
**Authors**: Alireza Khadem, Daichi Fujiki, Hilbert Chen, Yufeng Gu, Nishil Talati, Scott Mahlke, Reetuparna Das

**Updated**: 2025-01-17T01:24:12Z

**Summary**: In-cache computing technology transforms existing caches into long-vector compute units and offers low-cost alternatives to building expensive vector engines for mobile CPUs. Unfortunately, existing long-vector Instruction Set Architecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm Scalable Vector Extension (SVE), provide only one-dimensional strided and random memory accesses. While this is sufficient for typical vector engines, it fails to effectively utilize the large Single Instruction, Multiple Data (SIMD) widths of in-cache vector engines. This is because mobile data-parallel kernels expose limited parallelism across a single dimension.   Based on our analysis of mobile vector kernels, we introduce a long-vector Multi-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE achieves high SIMD resource utilization and enables flexible programming by abstracting cache geometry and data layout. The proposed ISA features multi-dimensional strided and random memory accesses and efficient dimension-level masked execution to encode parallelism across multiple dimensions. Using a wide range of data-parallel mobile workloads, we demonstrate that MVE offers significant performance and energy reduction benefits of 2.9x and 8.8x, on average, compared to the SIMD units of a commercial mobile processor, at an area overhead of 3.6%.

**Link**: [arxiv](http://arxiv.org/abs/2501.09902v1),  [pdf](http://arxiv.org/pdf/2501.09902v1)

**Tags**: cs.AR 



### Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of   the Atomic Layer Deposition Temperature and the Lithographic Patterning   Method
**Authors**: Alessandro Paghi, Sebastiano Battisti, Simone Tortorella, Giorgio De Simoni, Francesco Giazotto

**Updated**: 2025-01-16T15:11:42Z

**Summary**: Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics, have become the standard insulators in gate architectures, enhancing the electrical performance of both room temperature and cryogenic electronics. This study delves into the cryogenic (3 K) performance of high-k dielectrics commonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via Atomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and dielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on metal-insulator-metal capacitors. Our findings reveal a strong dependence of HfO2 cryogenic performance on the ALD growth temperature, while the latter shows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of the relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K. Additionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked their properties at cryogenic temperatures. The study also investigates the impact of the patterning method, namely, UV or electron-beam lithography (acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric properties.

**Link**: [arxiv](http://arxiv.org/abs/2407.04501v2),  [pdf](http://arxiv.org/pdf/2407.04501v2)

**Tags**: cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.supr-con 



### Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk   Synchronization
**Authors**: Congcong Chen, Jinhua Cui, Gang Qu, Jiliang Zhang

**Updated**: 2025-01-16T10:35:59Z

**Summary**: Memory-disk synchronization is a critical technology for ensuring data correctness, integrity, and security, especially in systems that handle sensitive information like financial transactions and medical records. We propose SYNC+SYNC, a group of attacks that exploit the memory-disk synchronization primitives. SYNC+SYNC works by subtly varying the timing of synchronization on the write buffer, offering several advantages: 1) implemented purely in software, enabling deployment on any hardware devices; 2) resilient against existing cache partitioning and randomization techniques; 3) unaffected by prefetching techniques and cache replacement strategies. We present the principles of SYNC+SYNC through the implementation of two write covert channel protocols, using either a single file or page, and introduce three enhanced strategies that utilize multiple files and pages. The feasibility of these channels is demonstrated in both cross-process and cross-sandbox scenarios across diverse operating systems (OSes). Experimental results show that, the average rate can reach 2.036 Kb/s (with a peak rate of 14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the average rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the error rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first high-speed write covert channel for software cache.

**Link**: [arxiv](http://arxiv.org/abs/2312.11501v2),  [pdf](http://arxiv.org/pdf/2312.11501v2)

**Tags**: cs.CR 



### Adaptive Contextual Caching for Mobile Edge Large Language Model Service
**Authors**: Guangyuan Liu, Yinqiu Liu, Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong

**Updated**: 2025-01-16T08:52:38Z

**Summary**: Mobile edge Large Language Model (LLM) deployments face inherent constraints, such as limited computational resources and network bandwidth. Although Retrieval-Augmented Generation (RAG) mitigates some challenges by integrating external knowledge bases, inefficient cache management can still result in high retrieval latency and frequent cache updates. To address these issues, we propose an Adaptive Contextual Caching (ACC) framework that anticipates user needs by proactively caching semantically relevant data for mobile-edge LLMs. ACC utilizes a deep reinforcement learning (DRL) module to refine cache replacement policies, balancing user context, document similarity, and the overhead associated with cache misses. Experimental results demonstrate that ACC increases cache hit rates to over 80\% after only 11 training episodes, outperforming FIFO, LRU, and semantic-only caching while reducing retrieval latency by up to 40\%. In particular, ACC also reduces local caching overhead (i.e., the cost of updating the cache when a miss occurs) by as much as 55\%, enabling scalable, low-latency LLM services in resource-constrained edge environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.09383v1),  [pdf](http://arxiv.org/pdf/2501.09383v1)

**Tags**: cs.NI 



### Interoceptive Robots for Convergent Shared Control in Collaborative   Construction Work
**Authors**: Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat

**Updated**: 2025-01-16T04:50:15Z

**Summary**: Building autonomous mobile robots (AMRs) with optimized efficiency and adaptive capabilities-able to respond to changing task demands and dynamic environments-is a strongly desired goal for advancing construction robotics. Such robots can play a critical role in enabling automation, reducing operational carbon footprints, and supporting modular construction processes. Inspired by the adaptive autonomy of living organisms, we introduce interoception, which centers on the robot's internal state representation, as a foundation for developing self-reflection and conscious learning to enable continual learning and adaptability in robotic agents. In this paper, we factorize internal state variables and mathematical properties as "cognitive dissonance" in shared control paradigms, where human interventions occasionally occur. We offer a new perspective on how interoception can help build adaptive motion planning in AMRs by integrating the legacy of heuristic costs from grid/graph-based algorithms with recent advances in neuroscience and reinforcement learning. Declarative and procedural knowledge extracted from human semantic inputs is encoded into a hypergraph model that overlaps with the spatial configuration of onsite layout for path planning. In addition, we design a velocity-replay module using an encoder-decoder architecture with few-shot learning to enable robots to replicate velocity profiles in contextualized scenarios for multi-robot synchronization and handover collaboration. These "cached" knowledge representations are demonstrated in simulated environments for multi-robot motion planning and stacking tasks. The insights from this study pave the way toward artificial general intelligence in AMRs, fostering their progression from complexity to competence in construction automation.

**Link**: [arxiv](http://arxiv.org/abs/2501.09290v1),  [pdf](http://arxiv.org/pdf/2501.09290v1)

**Tags**: cs.RO 



### PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid   Resolution Diffusion Serving
**Authors**: Desen Sun, Zepeng Zhao, Yuke Wang

**Updated**: 2025-01-16T02:40:07Z

**Summary**: The Text-to-Image (T2I) diffusion model is one of the most popular models in the world. However, serving diffusion models at the entire image level faces several problems, especially when there are multiple candidate resolutions. First, image based serving system prevents requests with different resolutions from batching together. On the other hand, requests with hybrid resolutions also indicate diverse locality features, which makes it hard to apply the same cache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch Management Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that provides a patch-level management strategy to gather hybrid resolution requests into batches. Specifically, PATCHEDSERVE incorporates a novel patch-based processing workflow, significantly enhancing throughput for hybrid resolution inputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to fully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features an SLO-aware scheduling algorithm with lightweight online latency prediction, achieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve 30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while not hurt the image quality.

**Link**: [arxiv](http://arxiv.org/abs/2501.09253v1),  [pdf](http://arxiv.org/pdf/2501.09253v1)

**Tags**: cs.DC 



### Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of   Micro-UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-01-15T21:09:22Z

**Summary**: This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content management system for disaster scenarios where communication infrastructure is generally compromised. Utilizing a hybrid network of stationary and mobile Micro-UAVs, this system aims to provide crucial content access to isolated communities. In the developed architecture, stationary anchor UAVs, equipped with vertical and lateral links, serve users in individual disaster-affected communities. and mobile micro-ferrying UAVs, with enhanced mobility, extend coverage across multiple such communities. The primary goal is to devise a content dissemination system that dynamically learns caching policies to maximize content accessibility to users left without communication infrastructure. The core contribution is an adaptive content dissemination framework that employs a decentralized Top-k Multi-Armed Bandit learning approach for efficient UAV caching decisions. This approach accounts for geo-temporal variations in content popularity and diverse user demands. Additionally, a Selective Caching Algorithm is proposed to minimize redundant content copies by leveraging inter-UAV information sharing. Through functional verification and performance evaluation, the proposed framework demonstrates improved system performance and adaptability across varying network sizes, micro-UAV swarms, and content popularity distributions.

**Link**: [arxiv](http://arxiv.org/abs/2404.10845v2),  [pdf](http://arxiv.org/pdf/2404.10845v2)

**Tags**: cs.LG cs.NI I.2.11 



### Towards Federated Multi-Armed Bandit Learning for Content Dissemination   using Swarm of UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-01-15T20:55:13Z

**Summary**: This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.

**Link**: [arxiv](http://arxiv.org/abs/2501.09146v1),  [pdf](http://arxiv.org/pdf/2501.09146v1)

**Tags**: cs.LG cs.NI I.2.11 



### LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System
**Authors**: Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi

**Updated**: 2025-01-15T01:34:46Z

**Summary**: The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.20166v2),  [pdf](http://arxiv.org/pdf/2412.20166v2)

**Tags**: cs.AR cs.AI 



### CORD: Co-design of Resource Allocation and Deadline Decomposition with   Generative Profiling
**Authors**: Robert Gifford, Abby Eisenklam, Georgiy A. Bondar, Yifan Cai, Tushar Sial, Linh Thi Xuan Phan, Abhishek Halder

**Updated**: 2025-01-14T23:13:14Z

**Summary**: As multicore hardware is becoming increasingly common in real-time systems, traditional scheduling techniques that assume a single worst-case execution time for a task are no longer adequate, since they ignore the impact of shared resources on execution time. When tasks execute concurrently on different cores, their execution times often vary substantially with their allocated budgets of shared resources, such as cache and memory bandwidth. Even under a specific resource allocation, the resource use pattern of a task also changes with time during a job execution. It is therefore important to consider the relationship between multicore resources and execution time in task modeling and scheduling algorithm design.   In this paper, we propose a much more precise execution model for DAG-based real-time tasks that captures the time-varying resource use characteristics of a task under different budgets of shared resources. We present a generative resource profiling algorithm that efficiently predicts, from limited measurement data, the resource profile of a task at any time during its execution under a given resource budget. The generative profiles can then be used to construct the execution models for tasks, using which one can make informed resource allocation decisions. We further introduce a multicore resource allocation and deadline decomposition co-design technique for DAG-based tasks that leverages the generated execution models to jointly allocate resources and deadlines to subtasks, to maximize resource efficiency and schedulability. Our evaluation results show that our generative profiling algorithm achieves high accuracy while being efficient, and that our co-allocation technique substantially improves schedulability compared to a state-of-the-art deadline decomposition method.

**Link**: [arxiv](http://arxiv.org/abs/2501.08484v1),  [pdf](http://arxiv.org/pdf/2501.08484v1)

**Tags**: cs.OS cs.PF 



### PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving
**Authors**: Ahmet Caner Yüzügüler, Jiawei Zhuang, Lukas Cavigelli

**Updated**: 2025-01-14T15:14:10Z

**Summary**: Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08192v1),  [pdf](http://arxiv.org/pdf/2501.08192v1)

**Tags**: cs.AI cs.AR cs.DC 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2025-01-14T14:07:55Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v2),  [pdf](http://arxiv.org/pdf/2411.15102v2)

**Tags**: cs.LG 



### TreeKV: Smooth Key-Value Cache Compression with Tree Structures
**Authors**: Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang

**Updated**: 2025-01-14T12:06:33Z

**Summary**: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.04987v2),  [pdf](http://arxiv.org/pdf/2501.04987v2)

**Tags**: cs.CL 



### Cell-level modelling of homeostasis in confined epithelial monolayers
**Authors**: KVS Chaithanya, Jan Rozman, Andrej Košmrlj, Rastko Sknepnek

**Updated**: 2025-01-14T11:41:14Z

**Summary**: Tissue homeostasis, the biological process of maintaining a steady state in tissue via control of cell proliferation, death, and metabolic function, is essential for the development, growth, maintenance, and proper function of living organisms. Disruptions to this process can lead to serious diseases and even death. In this study, we use the vertex model for the cell-level description of tissue mechanics to investigate the impact of the tissue microenvironment and local mechanical properties of cells on homeostasis in confined epithelial tissues. We find a dynamic steady state, where the balance between cell divisions and removals sustains homeostasis. By characterising homeostasis in terms of cell count, tissue area, and the cells' neighbour count distribution, we identify the factors that govern regulated and ordered tissue growth. This work, therefore, sheds light on the mechanisms underlying tissue homeostasis and highlights the importance of mechanics in the control of biological processes such as tissue development and disease pathology.

**Link**: [arxiv](http://arxiv.org/abs/2403.15896v2),  [pdf](http://arxiv.org/pdf/2403.15896v2)

**Tags**: physics.bio-ph cond-mat.soft 



### Multi-matrix Factorization Attention
**Authors**: Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang

**Updated**: 2025-01-14T05:48:07Z

**Summary**: We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2412.19255v2),  [pdf](http://arxiv.org/pdf/2412.19255v2)

**Tags**: cs.LG cs.CL 



### Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers
**Authors**: Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor Rühle, Saravan Rajmohan

**Updated**: 2025-01-14T05:00:34Z

**Summary**: Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference.   To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the "stream-K" style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.

**Link**: [arxiv](http://arxiv.org/abs/2405.10480v2),  [pdf](http://arxiv.org/pdf/2405.10480v2)

**Tags**: cs.AR cs.LG I.2.7; C.1.4 



### QMDB: Quick Merkle Database
**Authors**: Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong

**Updated**: 2025-01-14T02:02:01Z

**Summary**: Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.05262v2),  [pdf](http://arxiv.org/pdf/2501.05262v2)

**Tags**: cs.NI cs.DB 



### FlashRNN: Optimizing Traditional RNNs on Modern Hardware
**Authors**: Korbinian Pöppel, Maximilian Beck, Sepp Hochreiter

**Updated**: 2025-01-13T17:34:22Z

**Summary**: While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: \url{https://github.com/NX-AI/flashrnn}

**Link**: [arxiv](http://arxiv.org/abs/2412.07752v2),  [pdf](http://arxiv.org/pdf/2412.07752v2)

**Tags**: cs.LG cs.AI 



### DID Link: Authentication in TLS with Decentralized Identifiers and   Verifiable Credentials
**Authors**: Sandro Rodriguez Garzon, Dennis Natusch, Artur Philipp, Axel Küpper, Hans Joachim Einsiedler, Daniela Schneider

**Updated**: 2025-01-13T09:33:25Z

**Summary**: Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Link, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Link if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Link to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.

**Link**: [arxiv](http://arxiv.org/abs/2405.07533v4),  [pdf](http://arxiv.org/pdf/2405.07533v4)

**Tags**: cs.CR cs.NI 



### Generating Data Locality to Accelerate Sparse Matrix-Matrix   Multiplication on CPUs
**Authors**: Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini

**Updated**: 2025-01-13T04:31:04Z

**Summary**: Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation in many applications. Current multithreaded implementations are based on Gustavson's algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach. The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures. MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold. We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation. An OpenMP implementation of MAGNUS is compared with several baselines for a variety of different matrices on three Intel x86 architectures. For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is orders of magnitude faster than Intel MKL for several matrices. For massive random matrices that model social network graphs, MAGNUS scales to the largest matrix sizes, while the baselines fail to do so. Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.

**Link**: [arxiv](http://arxiv.org/abs/2501.07056v1),  [pdf](http://arxiv.org/pdf/2501.07056v1)

**Tags**: cs.DC 



### A Unified Framework for Automated Code Transformation and Pragma   Insertion
**Authors**: Stéphane Pouget, Louis-Noël Pouchet, Jason Cong

**Updated**: 2025-01-13T03:11:28Z

**Summary**: High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.

**Link**: [arxiv](http://arxiv.org/abs/2405.03058v5),  [pdf](http://arxiv.org/pdf/2405.03058v5)

**Tags**: cs.SE cs.PL 



### On Optimizing Locality of Graph Transposition on Modern Architectures
**Authors**: Mohsen Koohi Esfahani, Hans Vandierendonck

**Updated**: 2025-01-12T17:01:40Z

**Summary**: This paper investigates the shared-memory Graph Transposition (GT) problem, a fundamental graph algorithm that is widely used in graph analytics and scientific computing.   Previous GT algorithms have significant memory requirements that are proportional to the number of vertices and threads which obstructs their use on large graphs. Moreover, atomic memory operations have become comparably fast on recent CPU architectures, which creates new opportunities for improving the performance of concurrent atomic accesses in GT.   We design PoTra, a GT algorithm which leverages graph structure and processor and memory architecture to optimize locality and performance. PoTra limits the size of additional data structures close to CPU cache sizes and utilizes the skewed degree distribution of graph datasets to optimize locality and performance. We present the performance model of PoTra to explain the connection between cache and memory response times and graph locality.   Our evaluation of PoTra on three CPU architectures and 20 real-world and synthetic graph datasets with up to 128 billion edges demonstrates that PoTra achieves up to 8.7 times speedup compared to previous works and if there is a performance loss it remains limited to 15.7%, on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.06872v1),  [pdf](http://arxiv.org/pdf/2501.06872v1)

**Tags**: cs.DC cs.AR cs.DS cs.PF 



### MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large   Language Model Inference
**Authors**: Wenxuan Zeng, Ye Dong, Jinjin Zhou, Junming Ma, Jin Tan, Runsheng Wang, Meng Li

**Updated**: 2025-01-12T13:18:04Z

**Summary**: Private large language model (LLM) inference based on secure multi-party computation (MPC) offers cryptographically-secure protection for both user prompt and proprietary model weights. However, it suffers from large latency overhead especially for long input sequences. While key-value (KV) cache eviction algorithms have been proposed to reduce the computation and memory cost for plaintext inference, they are not designed for MPC and cannot benefit private inference easily. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant tokens and a query-aware dynamic selection algorithm to further select a small subset of tokens for attention computation. As existing dynamic selection algorithms incur too much latency, we propose a series of optimizations to drastically reduce the KV cache selection overhead, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index sharing strategy. With extensive experiments, we demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different LLM generation tasks and achieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction on different sequence lengths, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.06807v1),  [pdf](http://arxiv.org/pdf/2501.06807v1)

**Tags**: cs.CR 



### Linear Attention Sequence Parallelism
**Authors**: Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong

**Updated**: 2025-01-12T12:01:47Z

**Summary**: Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods. The code is available at https://github.com/OpenNLPLab/LASP.

**Link**: [arxiv](http://arxiv.org/abs/2404.02882v2),  [pdf](http://arxiv.org/pdf/2404.02882v2)

**Tags**: cs.LG cs.CL 



### Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma   Generated THz Pulses
**Authors**: Benjamin Colmey, Rodrigo T. Paulino, Gaspard Beaufort, David G. Cooke

**Updated**: 2025-01-12T11:15:41Z

**Summary**: Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.

**Link**: [arxiv](http://arxiv.org/abs/2409.07196v3),  [pdf](http://arxiv.org/pdf/2409.07196v3)

**Tags**: cond-mat.mtrl-sci physics.plasm-ph 



### Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion
**Authors**: Bohai Gu, Hao Luo, Song Guo, Peiran Dong

**Updated**: 2025-01-12T05:25:06Z

**Summary**: Recently, diffusion-based methods have achieved great improvements in the video inpainting task. However, these methods still face many challenges, such as maintaining temporal consistency and the time-consuming issue. This paper proposes an advanced video inpainting framework using optical Flow-guided Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch architecture, where a flow branch first restores corrupted flow and a multi-scale flow adapter provides motion guidance to the main inpainting branch. Additionally, a training-free latent interpolation method is proposed to accelerate the multi-step denoising process using flow warping. Further introducing a flow attention cache mechanism, FLoED efficiently reduces the computational cost brought by incorporating optical flow. Comprehensive experiments in both background restoration and object removal tasks demonstrate that FloED outperforms state-of-the-art methods from the perspective of both performance and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2412.00857v2),  [pdf](http://arxiv.org/pdf/2412.00857v2)

**Tags**: cs.CV 



### Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV   Cache Management
**Authors**: Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song

**Updated**: 2025-01-12T04:29:39Z

**Summary**: Serving large language models (LLMs) for massive users is challenged by the significant memory footprint of the transient state, known as the key-value (KV) cache, which scales with sequence length and number of requests. Instead of renting or buying more expensive GPUs, the load imbalance of the KV cache across GPUs, coupled with recent advances in inter-GPU communication, provides an opportunity to serve more requests via request migration. However, high migration overhead and unpredictable request patterns make it challenging. Therefore, this paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management. It saves the number of GPUs needed in the system by considering the dynamic KV cache load and the costly request migration. Specifically, we first develop an adaptive request migration mechanism to balance the computational and communication overheads and adapt to diverse resource conditions. Then, we design an online algorithm tailored to a multi-LLM request and multi-GPU scheduling problem with migration enabled. It aims to minimise the required GPUs while limiting the number of migrations. Finally, we implement a prototype of MELL and demonstrate that it reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.06709v1),  [pdf](http://arxiv.org/pdf/2501.06709v1)

**Tags**: cs.DC 



### GraphSnapShot: Caching Local Structure for Fast Graph Learning
**Authors**: Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman

**Updated**: 2025-01-11T15:26:48Z

**Summary**: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.   The code for GraphSnapShot is publicly available at https://github.com/NoakLiu/GraphSnapShot.

**Link**: [arxiv](http://arxiv.org/abs/2406.17918v4),  [pdf](http://arxiv.org/pdf/2406.17918v4)

**Tags**: cs.LG cs.DC cs.SI 



### Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL
**Authors**: Nicolas Lee Guidotti

**Updated**: 2025-01-11T12:22:51Z

**Summary**: Due to its flexible architecture, FPGAs support unique, deep hardware pipeline implementations for accelerating HPC applications. However, these devices are quite new in the HPC space, and thus, have been scarcely explored outside some specific scientific domain, such as machine learning or biological sequence alignment. The objective of this thesis is to characterize the FPGA-based solution for accelerating particle-mesh algorithms, in which the force applied to each particle is computed based on the fields deposited in a finite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator called ZPIC that has the same core algorithm and functionalities as OSIRIS. To create an efficient hardware design, the program keeps the particles strictly sorted by tiles (a group of cells) and uses the local memory as an explicitly managed cache. We also create multiple copies of the local current buffer to solve dependencies during the deposition phase. The resulting pipeline was replicated multiple times to explore data parallelism and increase its throughput. We then compare our hardware solution against similar implementations on GPU and multicore CPUs, showing promising results in term of power efficiency and performance.   Keywords: FPGA, OpenCL, Kinetic Plasma Simulation.

**Link**: [arxiv](http://arxiv.org/abs/2501.14795v1),  [pdf](http://arxiv.org/pdf/2501.14795v1)

**Tags**: cs.DC 



### Optimizing digital experiences with content delivery networks:   Architectures, performance strategies, and future trends
**Authors**: Anuj Tyagi

**Updated**: 2025-01-11T03:47:04Z

**Summary**: This research investigates how CDNs (Content Delivery Networks) can improve the digital experience, as consumers increasingly expect fast, efficient, and effortless access to online resources. CDNs play a crucial role in reducing latency, enhancing scalability, and optimizing delivery mechanisms, which is evident across various platforms and regions. The study focuses on key CDN concerns, such as foundational and modern CDN architectures, edge computing, hybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing topics, including caching, load balancing, and the novel features of HTTP/3 and QUIC.   Current trends, such as integrating CDNs with 5G networks, serverless architectures, and AI-driven traffic management, are examined to demonstrate how CDN technology is likely to evolve. The study also addresses challenges related to security, cost, and global regulations. Practical examples from the e-commerce, streaming, and gaming industries highlight how enhanced CDNs are transforming these sectors.   The conclusions emphasize the need to evolve CDN strategies to meet growing user expectations and adapt to the rapidly changing digital landscape. Additionally, the research identifies future research opportunities, particularly in exploring the impact of QC, the enhancement of AI services, and the sustainability of CDN solutions. Overall, the study situates architectural design, performance strategies, and emerging trends to address gaps and create a more efficient and secure approach for improving digital experiences.

**Link**: [arxiv](http://arxiv.org/abs/2501.06428v1),  [pdf](http://arxiv.org/pdf/2501.06428v1)

**Tags**: cs.NI cs.SE 



### Tensor Product Attention Is All You Need
**Authors**: Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao

**Updated**: 2025-01-11T03:37:10Z

**Summary**: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.

**Link**: [arxiv](http://arxiv.org/abs/2501.06425v1),  [pdf](http://arxiv.org/pdf/2501.06425v1)

**Tags**: cs.CL cs.AI cs.LG 



### Unispeaker: A Unified Approach for Multimodality-driven Speaker   Generation
**Authors**: Zhengyan Sheng, Zhihao Du, Heng Lu, Shiliang Zhang, Zhen-Hua Ling

**Updated**: 2025-01-11T00:47:29Z

**Summary**: Recent advancements in personalized speech generation have brought synthetic speech increasingly close to the realism of target speakers' recordings, yet multimodal speaker generation remains on the rise. This paper introduces UniSpeaker, a unified approach for multimodality-driven speaker generation. Specifically, we propose a unified voice aggregator based on KV-Former, applying soft contrastive loss to map diverse voice description modalities into a shared voice space, ensuring that the generated voice aligns more closely with the input descriptions. To evaluate multimodality-driven voice control, we build the first multimodality-based voice control (MVC) benchmark, focusing on voice suitability, voice diversity, and speech quality. UniSpeaker is evaluated across five tasks using the MVC benchmark, and the experimental results demonstrate that UniSpeaker outperforms previous modality-specific models. Speech samples are available at \url{https://UniSpeaker.github.io}.

**Link**: [arxiv](http://arxiv.org/abs/2501.06394v1),  [pdf](http://arxiv.org/pdf/2501.06394v1)

**Tags**: cs.SD cs.AI eess.AS 



### Tame fields, Graded Rings and Finite Complete Sequences of Key   Polynomials
**Authors**: Caio Henrique Silva de Souza

**Updated**: 2025-01-10T10:11:45Z

**Summary**: In this paper, we present a criterion for $(K,v)$ to be henselian and defectless in terms of finite complete sequences of key polynomials. For this, we use the theory of Mac Lane-Vaqui\'e chains and abstract key polynomials. We then prove that a valued field $(K,v)$ is tame if and only if $vK$ is $p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$ admits a finite complete sequence of key polynomials. The properties $vK$ $p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on the associated graded ring. We also make considerations on simply defectless and algebraically maximal valued fields and purely inertial and purely ramified extensions.

**Link**: [arxiv](http://arxiv.org/abs/2407.01030v2),  [pdf](http://arxiv.org/pdf/2407.01030v2)

**Tags**: math.AC 13A18 



### Handover_Management_in_UAV_Networks_with_Blockages
**Authors**: Neetu R R, Gourab Ghatak, Vivek Ashok Bohara

**Updated**: 2025-01-09T15:14:05Z

**Summary**: We investigate the performance of unmanned aerial vehicle (UAV)-based networks in urban environments characterized by blockages, focusing on their capability to support the service demands of mobile users. The UAV-base stations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson point process (MPPP), where the marks represent the altitude of each UAV-BS. Leveraging stochastic geometry, we analyze the impact of blockages on network reliability by studying the meta distribution (MD) of the signal-to-interference noise ratio (SINR) for a specific reliability threshold and the association probabilities for both line-of-sight (LoS) and non line-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile users, we propose a novel cache-based handover management strategy that dynamically selects the cell search time and delays the received signal strength (RSS)-based base station (BS) associations. This strategy aims to minimize unnecessary handovers (HOs) experienced by users by leveraging caching capabilities at user equipment (UE), thus reducing latency, ensuring seamless connectivity, and maintaining the quality of service (QoS). This study provides valuable insights into optimizing UAV network deployments to support the stringent requirements in the network, ensuring reliable, low-latency, and high-throughput communication for next-generation smart cities.

**Link**: [arxiv](http://arxiv.org/abs/2409.20433v2),  [pdf](http://arxiv.org/pdf/2409.20433v2)

**Tags**: eess.SP 



## Keyword: LLM Inference 
 ### DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights
**Authors**: Liana Mikaelyan, Ayyoob Imani, Mathew Salvaris, Parth Pathak, Mohsen Fayyaz

**Updated**: 2025-01-30T18:59:55Z

**Summary**: We introduce DeltaLLM, a new post-training compression technique to reduce the memory footprint of LLMs. We propose an alternative way of structuring LLMs with weight sharing between layers in subsequent Transformer blocks, along with additional low-rank difference matrices between them. For training, we adopt the progressing module replacement method and show that the lightweight training of the low-rank modules with approximately 30M-40M tokens is sufficient to achieve performance on par with LLMs of comparable sizes trained from scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a 12% parameter reduction, retaining 90% of the performance of the base Llama and Phi models on common knowledge and reasoning benchmarks. Our method also outperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with the same number of parameters removed. For example, DeltaPhi 2.9B with a 24% reduction achieves similar average zero-shot accuracies as recovery fine-tuned SlicedPhi 3.3B with a 12% reduction, despite being approximately 400M parameters smaller with no fine-tuning applied. This work provides new insights into LLM architecture design and compression methods when storage space is critical.

**Link**: [arxiv](http://arxiv.org/abs/2501.18596v1),  [pdf](http://arxiv.org/pdf/2501.18596v1)

**Tags**: cs.LG cs.AI 



### Foundational Models for 3D Point Clouds: A Survey and Outlook
**Authors**: Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, Yunpeng Li

**Updated**: 2025-01-30T18:59:43Z

**Summary**: The 3D point cloud representation plays a crucial role in preserving the geometric fidelity of the physical world, enabling more accurate complex 3D environments. While humans naturally comprehend the intricate relationships between objects and variations through a multisensory system, artificial intelligence (AI) systems have yet to fully replicate this capacity. To bridge this gap, it becomes essential to incorporate multiple modalities. Models that can seamlessly integrate and reason across these modalities are known as foundation models (FMs). The development of FMs for 2D modalities, such as images and text, has seen significant progress, driven by the abundant availability of large-scale datasets. However, the 3D domain has lagged due to the scarcity of labelled data and high computational overheads. In response, recent research has begun to explore the potential of applying FMs to 3D tasks, overcoming these challenges by leveraging existing 2D knowledge. Additionally, language, with its capacity for abstract reasoning and description of the environment, offers a promising avenue for enhancing 3D understanding through large pre-trained language models (LLMs). Despite the rapid development and adoption of FMs for 3D vision tasks in recent years, there remains a gap in comprehensive and in-depth literature reviews. This article aims to address this gap by presenting a comprehensive overview of the state-of-the-art methods that utilize FMs for 3D visual understanding. We start by reviewing various strategies employed in the building of various 3D FMs. Then we categorize and summarize use of different FMs for tasks such as perception tasks. Finally, the article offers insights into future directions for research and development in this field. To help reader, we have curated list of relevant papers on the topic: https://github.com/vgthengane/Awesome-FMs-in-3D.

**Link**: [arxiv](http://arxiv.org/abs/2501.18594v1),  [pdf](http://arxiv.org/pdf/2501.18594v1)

**Tags**: cs.CV 



### Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs
**Authors**: Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu

**Updated**: 2025-01-30T18:58:18Z

**Summary**: Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we term underthinking, where o1-like LLMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems. To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source o1-like models, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy with thought switching penalty TIP that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path. Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning. Our findings contribute to understanding reasoning inefficiencies in o1-like LLMs and offer a practical solution to enhance their problem-solving capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.18585v1),  [pdf](http://arxiv.org/pdf/2501.18585v1)

**Tags**: cs.CL 



### Prediction-Powered Inference with Imputed Covariates and Nonuniform   Sampling
**Authors**: Dan M. Kluger, Kerri Lu, Tijana Zrnic, Sherrie Wang, Stephen Bates

**Updated**: 2025-01-30T18:46:43Z

**Summary**: Machine learning models are increasingly used to produce predictions that serve as input data in subsequent statistical analyses. For example, computer vision predictions of economic and environmental indicators based on satellite imagery are used in downstream regressions; similarly, language models are widely used to approximate human ratings and opinions in social science research. However, failure to properly account for errors in the machine learning predictions renders standard statistical procedures invalid. Prior work uses what we call the Predict-Then-Debias estimator to give valid confidence intervals when machine learning algorithms impute missing variables, assuming a small complete sample from the population of interest. We expand the scope by introducing bootstrap confidence intervals that apply when the complete data is a nonuniform (i.e., weighted, stratified, or clustered) sample and to settings where an arbitrary subset of features is imputed. Importantly, the method can be applied to many settings without requiring additional calculations. We prove that these confidence intervals are valid under no assumptions on the quality of the machine learning model and are no wider than the intervals obtained by methods that do not use machine learning predictions.

**Link**: [arxiv](http://arxiv.org/abs/2501.18577v1),  [pdf](http://arxiv.org/pdf/2501.18577v1)

**Tags**: stat.ME cs.AI cs.LG stat.ML 



### Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for   Multi-Step Reasoning Over Speed in MATH
**Authors**: Evgenii Evstafev

**Updated**: 2025-01-30T18:45:51Z

**Summary**: This study investigates the performance of the DeepSeek R1 language model on 30 challenging mathematical problems derived from the MATH dataset, problems that previously proved unsolvable by other models under time constraints. Unlike prior work, this research removes time limitations to explore whether DeepSeek R1's architecture, known for its reliance on token-based reasoning, can achieve accurate solutions through a multi-step process. The study compares DeepSeek R1 with four other models (gemini-1.5-flash-8b, gpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11 temperature settings. Results demonstrate that DeepSeek R1 achieves superior accuracy on these complex problems but generates significantly more tokens than other models, confirming its token-intensive approach. The findings highlight a trade-off between accuracy and efficiency in mathematical problem-solving with large language models: while DeepSeek R1 excels in accuracy, its reliance on extensive token generation may not be optimal for applications requiring rapid responses. The study underscores the importance of considering task-specific requirements when selecting an LLM and emphasizes the role of temperature settings in optimizing performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.18576v1),  [pdf](http://arxiv.org/pdf/2501.18576v1)

**Tags**: cs.LG 



### Log-Gaussian Cox Processes on General Metric Graphs
**Authors**: David Bolin, Damilya Saduakhas, Alexandre B. Simas

**Updated**: 2025-01-30T18:35:02Z

**Summary**: The modeling of spatial point processes has advanced considerably, yet extending these models to non-Euclidean domains, such as road networks, remains a challenging problem. We propose a novel framework for log-Gaussian Cox processes on general compact metric graphs by leveraging the Gaussian Whittle-Mat\'ern fields, which are solutions to fractional-order stochastic differential equations on metric graphs. To achieve computationally efficient likelihood-based inference, we introduce a numerical approximation of the likelihood that eliminates the need to approximate the Gaussian process. This method, coupled with the exact evaluation of finite-dimensional distributions for Whittle-Mat\'ern fields with integer smoothness, ensures scalability and theoretical rigour, with derived convergence rates for posterior distributions. The framework is implemented in the open-source MetricGraph R package, which integrates seamlessly with R-INLA to support fully Bayesian inference. We demonstrate the applicability and scalability of this approach through an analysis of road accident data from Al-Ahsa, Saudi Arabia, consisting of over 150,000 road segments. By identifying high-risk road segments using exceedance probabilities and excursion sets, our framework provides localized insights into accident hotspots and offers a powerful tool for modeling spatial point processes directly on complex networks.

**Link**: [arxiv](http://arxiv.org/abs/2501.18558v1),  [pdf](http://arxiv.org/pdf/2501.18558v1)

**Tags**: stat.ME 



### The No-Underrun Sampler: A Locally-Adaptive, Gradient-Free MCMC Method
**Authors**: Nawaf Bou-Rabee, Bob Carpenter, Sifan Liu, Stefan Oberdörster

**Updated**: 2025-01-30T18:20:28Z

**Summary**: In this work, we introduce the No-Underrun Sampler (NURS): a locally-adaptive, gradient-free Markov chain Monte Carlo method that combines elements of Hit-and-Run and the No-U-Turn Sampler. NURS dynamically adapts to the local geometry of the target distribution without requiring gradient evaluations, making it especially suitable for applications where gradients are unavailable or costly. We establish key theoretical properties, including reversibility, formal connections to Hit-and-Run and Random Walk Metropolis, Wasserstein contraction comparable to Hit-and-Run in Gaussian targets, and bounds on the total variation distance between the transition kernels of Hit-and-Run and NURS. Finally, we demonstrate - through empirical experiments supported by theoretical insights - that NURS can effectively sample Neal's funnel, a challenging multi-scale distribution from Bayesian hierarchical inference.

**Link**: [arxiv](http://arxiv.org/abs/2501.18548v1),  [pdf](http://arxiv.org/pdf/2501.18548v1)

**Tags**: math.ST math.PR stat.ME stat.TH 60J05, 65C05 



### Semantic Web and Creative AI -- A Technical Report from ISWS 2023
**Authors**: Raia Abu Ahmad, Reham Alharbi, Roberto Barile, Martin Böckling, Francisco Bolanos, Sara Bonfitto, Oleksandra Bruns, Irene Celino, Yashrajsinh Chudasama, Martin Critelli, Claudia d'Amato, Giada D'Ippolito, Ioannis Dasoulas, Stefano De Giorgis, Vincenzo De Leo, Chiara Di Bonaventura, Marco Di Panfilo, Daniil Dobriy, John Domingue, Xuemin Duan, Michel Dumontier, Sefika Efeoglu, Ruben Eschauzier, Fakih Ginwa, Nicolas Ferranti, Arianna Graciotti, Philipp Hanisch, George Hannah, Golsa Heidari, Aidan Hogan, Hassan Hussein, Alexane Jouglar, Jan-Christoph Kalo, Manoé Kieffer, Antonis Klironomos, Inês Koch, Weronika Lajewska, Nicolas Lazzari, Mikael Lindekrans, Anna Sofia Lippolis, Majlinda Llugiqi, Eleonora Mancini, Eleonora Marzi, Laura Menotti, Daniela Milon Flores, Soulakshmee Nagowah, Kerstin Neubert, Emetis Niazmand, Ebrahim Norouzi, Beatriz Olarte Martinez, Anouk Michelle Oudshoorn, Andrea Poltronieri, Valentina Presutti, Disha Purohit, Ensiyeh Raoufi, Celian Ringwald, Johanna Rockstroh, Sebastian Rudolph, Harald Sack, Zafar Saeed, Mohammad Javad Saeedizade, Aya Sahbi, Cristian Santini, Aleksandra Simic, Dennis Sommer, Rita Sousa, Mary Ann Tan, Vidyashree Tarikere, Tabea Tietz, Liam Tirpitz, Arnaldo Tomasino, Frank van Harmelen, Joao Vissoci, Caitlin Woods, Bohui Zhang, Xinyue Zhang, Heng Zheng

**Updated**: 2025-01-30T18:10:16Z

**Summary**: The International Semantic Web Research School (ISWS) is a week-long intensive program designed to immerse participants in the field. This document reports a collaborative effort performed by ten teams of students, each guided by a senior researcher as their mentor, attending ISWS 2023. Each team provided a different perspective to the topic of creative AI, substantiated by a set of research questions as the main subject of their investigation. The 2023 edition of ISWS focuses on the intersection of Semantic Web technologies and Creative AI. ISWS 2023 explored various intersections between Semantic Web technologies and creative AI. A key area of focus was the potential of LLMs as support tools for knowledge engineering. Participants also delved into the multifaceted applications of LLMs, including legal aspects of creative content production, humans in the loop, decentralised approaches to multimodal generative AI models, nanopublications and AI for personal scientific knowledge graphs, commonsense knowledge in automatic story and narrative completion, generative AI for art critique, prompt engineering, automatic music composition, commonsense prototyping and conceptual blending, and elicitation of tacit knowledge. As Large Language Models and semantic technologies continue to evolve, new exciting prospects are emerging: a future where the boundaries between creative expression and factual knowledge become increasingly permeable and porous, leading to a world of knowledge that is both informative and inspiring.

**Link**: [arxiv](http://arxiv.org/abs/2501.18542v1),  [pdf](http://arxiv.org/pdf/2501.18542v1)

**Tags**: cs.AI 



### Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented   LLM-based Retrieval Method
**Authors**: Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth

**Updated**: 2025-01-30T18:07:19Z

**Summary**: Real-world open-domain questions can be complicated, particularly when answering them involves information from multiple information sources. LLMs have demonstrated impressive performance in decomposing complex tasks into simpler steps, and previous work has used it for better retrieval in support of complex questions. However, LLM's decomposition of questions is unaware of what data is available and how data is organized, often leading to a sub-optimal retrieval performance. Recent effort in agentic RAG proposes to perform retrieval in an iterative fashion, where a followup query is derived as an action based on previous rounds of retrieval. While this provides one way of interacting with the data collection, agentic RAG's exploration of data is inefficient because successive queries depend on previous results rather than being guided by the organization of available data in the collection. To address this problem, we propose an LLM-based retrieval method -- ARM, that aims to better align the question with the organization of the data collection by exploring relationships among data objects beyond matching the utterance of the query, thus leading to a retrieve-all-at-once solution for complex queries. We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms standard RAG with query decomposition by up to 5.2 pt in execution accuracy and agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and 19.3 pt higher F1 match scores compared to these approaches.

**Link**: [arxiv](http://arxiv.org/abs/2501.18539v1),  [pdf](http://arxiv.org/pdf/2501.18539v1)

**Tags**: cs.CL cs.AI cs.IR 



### Mini-ResEmoteNet: Leveraging Knowledge Distillation for Human-Centered   Design
**Authors**: Amna Murtada, Omnia Abdelrhman, Tahani Abdalla Attia

**Updated**: 2025-01-30T18:06:44Z

**Summary**: Facial Emotion Recognition has emerged as increasingly pivotal in the domain of User Experience, notably within modern usability testing, as it facilitates a deeper comprehension of user satisfaction and engagement. This study aims to extend the ResEmoteNet model by employing a knowledge distillation framework to develop Mini-ResEmoteNet models - lightweight student models - tailored for usability testing. Experiments were conducted on the FER2013 and RAF-DB datasets to assess the efficacy of three student model architectures: Student Model A, Student Model B, and Student Model C. Their development involves reducing the number of feature channels in each layer of the teacher model by approximately 50%, 75%, and 87.5%. Demonstrating exceptional performance on the FER2013 dataset, Student Model A (E1) achieved a test accuracy of 76.33%, marking a 0.21% absolute improvement over EmoNeXt. Moreover, the results exhibit absolute improvements in terms of inference speed and memory usage during inference compared to the ResEmoteNet model. The findings indicate that the proposed methods surpass other state-of-the-art approaches.

**Link**: [arxiv](http://arxiv.org/abs/2501.18538v1),  [pdf](http://arxiv.org/pdf/2501.18538v1)

**Tags**: cs.CV 



### Illusions of Relevance: Using Content Injection Attacks to Deceive   Retrievers, Rerankers, and LLM Judges
**Authors**: Manveer Singh Tamber, Jimmy Lin

**Updated**: 2025-01-30T18:02:15Z

**Summary**: Consider a scenario in which a user searches for information, only to encounter texts flooded with misleading or non-relevant content. This scenario exemplifies a simple yet potent vulnerability in neural Information Retrieval (IR) pipelines: content injection attacks. We find that embedding models for retrieval, rerankers, and large language model (LLM) relevance judges are vulnerable to these attacks, in which adversaries insert misleading text into passages to manipulate model judgements. We identify two primary threats: (1) inserting unrelated or harmful content within passages that still appear deceptively "relevant", and (2) inserting entire queries or key query terms into passages to boost their perceived relevance. While the second tactic has been explored in prior research, we present, to our knowledge, the first empirical analysis of the first threat, demonstrating how state-of-the-art models can be easily misled. Our study systematically examines the factors that influence an attack's success, such as the placement of injected content and the balance between relevant and non-relevant material. Additionally, we explore various defense strategies, including adversarial passage classifiers, retriever fine-tuning to discount manipulated content, and prompting LLM judges to adopt a more cautious approach. However, we find that these countermeasures often involve trade-offs, sacrificing effectiveness for attack robustness and sometimes penalizing legitimate documents in the process. Our findings highlight the need for stronger defenses against these evolving adversarial strategies to maintain the trustworthiness of IR systems. We release our code and scripts to facilitate further research.

**Link**: [arxiv](http://arxiv.org/abs/2501.18536v1),  [pdf](http://arxiv.org/pdf/2501.18536v1)

**Tags**: cs.IR 



### In-Context Meta LoRA Generation
**Authors**: Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo

**Updated**: 2025-01-30T17:59:08Z

**Summary**: Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LoRA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LoRA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1\% storage compared with the original LoRA.

**Link**: [arxiv](http://arxiv.org/abs/2501.17635v2),  [pdf](http://arxiv.org/pdf/2501.17635v2)

**Tags**: cs.CL cs.AI cs.CV 



### Differentially Private Steering for Large Language Model Alignment
**Authors**: Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal

**Updated**: 2025-01-30T17:58:36Z

**Summary**: Aligning Large Language Models (LLMs) with human values and away from undesirable behaviors (such as hallucination) has become increasingly important. Recently, steering LLMs towards a desired behavior via activation editing has emerged as an effective method to mitigate harmful generations at inference-time. Activation editing modifies LLM representations by preserving information from positive demonstrations (e.g., truthful) and minimising information from negative demonstrations (e.g., hallucinations). When these demonstrations come from a private dataset, the aligned LLM may leak private information contained in those private samples. In this work, we present the first study of aligning LLM behavior with private datasets. Our work proposes the \textit{\underline{P}rivate \underline{S}teering for LLM \underline{A}lignment (PSA)} algorithm to edit LLM activations with differential privacy (DP) guarantees. We conduct extensive experiments on seven different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA achieves DP guarantees for LLM alignment with minimal loss in performance, including alignment metrics, open-ended text generation quality, and general-purpose reasoning. We also develop the first Membership Inference Attack (MIA) for evaluating and auditing the empirical privacy for the problem of LLM steering via activation editing. Our attack is tailored for activation editing and relies solely on the generated texts without their associated probabilities. Our experiments support the theoretical guarantees by showing improved guarantees for our \textit{PSA} algorithm compared to several existing non-private techniques.

**Link**: [arxiv](http://arxiv.org/abs/2501.18532v1),  [pdf](http://arxiv.org/pdf/2501.18532v1)

**Tags**: cs.CL cs.LG 



### S-LoRA: Scalable Low-Rank Adaptation for Class Incremental Learning
**Authors**: Yichen Wu, Hongming Piao, Long-Kai Huang, Renzhen Wang, Wanhua Li, Hanspeter Pfister, Deyu Meng, Kede Ma, Ying Wei

**Updated**: 2025-01-30T17:55:31Z

**Summary**: Continual Learning with foundation models has recently emerged as a promising approach to harnessing the power of pre-trained models for sequential tasks. Existing prompt-based methods generally use a gating mechanism to select relevant prompts aligned with the test query for further processing. However, the success of these methods largely depends on the precision of the gating mechanism, which becomes less scalable with additional computational overhead as tasks increases. To overcome these issues, we propose a Scalable Low-Rank Adaptation (S-LoRA) method for CL (in particular class incremental learning), which incrementally decouples the learning of the direction and magnitude of LoRA parameters. S-LoRA supports efficient inference by employing the last-stage trained model for direct testing without a gating process. Our theoretical and empirical analysis demonstrates that S-LoRA tends to follow a low-loss trajectory that converges to an overlapped low-loss region, resulting in an excellent stability-plasticity trade-off in CL. Furthermore, based on our findings, we develop variants of S-LoRA with further improved scalability. Extensive experiments across multiple CL benchmarks and various foundation models consistently validate the effectiveness of S-LoRA.

**Link**: [arxiv](http://arxiv.org/abs/2501.13198v2),  [pdf](http://arxiv.org/pdf/2501.13198v2)

**Tags**: cs.LG 



### Towards Automated Penetration Testing: Introducing LLM Benchmark,   Analysis, and Improvements
**Authors**: Isamu Isozaki, Manil Shrestha, Rick Console, Edward Kim

**Updated**: 2025-01-30T17:50:16Z

**Summary**: Hacking poses a significant threat to cybersecurity, inflicting billions of dollars in damages annually. To mitigate these risks, ethical hacking, or penetration testing, is employed to identify vulnerabilities in systems and networks. Recent advancements in large language models (LLMs) have shown potential across various domains, including cybersecurity. However, there is currently no comprehensive, open, end-to-end automated penetration testing benchmark to drive progress and evaluate the capabilities of these models in security contexts. This paper introduces a novel open benchmark for LLM-based automated penetration testing, addressing this critical gap. We first evaluate the performance of LLMs, including GPT-4o and Llama 3.1-405B, using the state-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1 demonstrates an edge over GPT-4o, both models currently fall short of performing fully automated, end-to-end penetration testing. Next, we advance the state-of-the-art and present ablation studies that provide insights into improving the PentestGPT tool. Our research illuminates the challenges LLMs face in each aspect of Pentesting, e.g. enumeration, exploitation, and privilege escalation. This work contributes to the growing body of knowledge on AI-assisted cybersecurity and lays the foundation for future research in automated penetration testing using large language models.

**Link**: [arxiv](http://arxiv.org/abs/2410.17141v3),  [pdf](http://arxiv.org/pdf/2410.17141v3)

**Tags**: cs.CR cs.AI 



### GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question   Answering
**Authors**: Sacha Muller, António Loison, Bilel Omrani, Gautier Viaud

**Updated**: 2025-01-30T17:34:51Z

**Summary**: Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.   To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.   We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.

**Link**: [arxiv](http://arxiv.org/abs/2409.06595v3),  [pdf](http://arxiv.org/pdf/2409.06595v3)

**Tags**: cs.CL I.2.7 



### Learn from the Past: Language-conditioned Object Rearrangement with   Large Language Models
**Authors**: Guanqun Cao, Ryan Mckenna, John Oyekan

**Updated**: 2025-01-30T17:28:11Z

**Summary**: Object rearrangement is a significant task for collaborative robots, where they are directed to manipulate objects into a specified goal state. Determining the placement of objects is a major challenge that influences the efficiency of the rearrangement process. Most current methods heavily rely on pre-collected datasets to train the model for predicting the goal position and are restricted to specific instructions, which limits their broader applicability and effectiveness.In this paper, we propose a framework of language-conditioned object rearrangement based on the Large Language Model (LLM). Particularly, our approach mimics human reasoning by using past successful experiences as a reference to infer the desired goal position. Based on LLM's strong natural language comprehension and inference ability, our method can generalise to handle various everyday objects and free-form language instructions in a zero-shot manner. Experimental results demonstrate that our methods can effectively execute the robotic rearrangement tasks, even those involving long sequential orders.

**Link**: [arxiv](http://arxiv.org/abs/2501.18516v1),  [pdf](http://arxiv.org/pdf/2501.18516v1)

**Tags**: cs.RO 



### Streaming DiLoCo with overlapping communication: Towards a Distributed   Free Lunch
**Authors**: Arthur Douillard, Yanislav Donchev, Keith Rush, Satyen Kale, Zachary Charles, Zachary Garrett, Gabriel Teston, Dave Lacey, Ross McIlroy, Jiajun Shen, Alexandre Ramé, Arthur Szlam, Marc'Aurelio Ranzato, Paul Barham

**Updated**: 2025-01-30T17:23:50Z

**Summary**: Training of large language models (LLMs) is typically distributed across a large number of accelerators to reduce training time. Since internal states and parameter gradients need to be exchanged at each and every single gradient step, all devices need to be co-located using low-latency high-bandwidth communication links to support the required high volume of exchanged bits. Recently, distributed algorithms like DiLoCo have relaxed such co-location constraint: accelerators can be grouped into ``workers'', where synchronizations between workers only occur infrequently. This in turn means that workers can afford being connected by lower bandwidth communication links without affecting learning quality. However, in these methods, communication across workers still requires the same peak bandwidth as before, as the synchronizations require all parameters to be exchanged across all workers. In this paper, we improve DiLoCo in three ways. First, we synchronize only subsets of parameters in sequence, rather than all at once, which greatly reduces peak bandwidth. Second, we allow workers to continue training while synchronizing, which decreases wall clock time. Third, we quantize the data exchanged by workers, which further reduces bandwidth across workers. By properly combining these modifications, we show experimentally that we can distribute training of billion-scale parameters and reach similar quality as before, but reducing required bandwidth by two orders of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2501.18512v1),  [pdf](http://arxiv.org/pdf/2501.18512v1)

**Tags**: cs.CL 



### WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in   Post-Training
**Authors**: Benjamin Feuer, Chinmay Hegde

**Updated**: 2025-01-30T17:21:44Z

**Summary**: Language model (LLM) post-training, from DPO to distillation, can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of conducting large-scale comparative analyses of synthetic data generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M, the largest public chat dataset to date. We extend the existing WildChat dataset to include responses not only from GPT, but from over 50 different open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an extensive comparative analysis and demonstrate the potential of this dataset by creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3 SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples and code are available at https://github.com/penfever/wildchat-50m.

**Link**: [arxiv](http://arxiv.org/abs/2501.18511v1),  [pdf](http://arxiv.org/pdf/2501.18511v1)

**Tags**: cs.LG cs.CL 



### Bayesian Neural Networks for One-to-Many Mapping in Image Enhancement
**Authors**: Guoxi Huang, Nantheera Anantrasirichai, Fei Ye, Zipeng Qi, RuiRui Lin, Qirui Yang, David Bull

**Updated**: 2025-01-30T17:19:05Z

**Summary**: In image enhancement tasks, such as low-light and underwater image enhancement, a degraded image can correspond to multiple plausible target images due to dynamic photography conditions, such as variations in illumination. This naturally results in a one-to-many mapping challenge. To address this, we propose a Bayesian Enhancement Model (BEM) that incorporates Bayesian Neural Networks (BNNs) to capture data uncertainty and produce diverse outputs. To achieve real-time inference, we introduce a two-stage approach: Stage I employs a BNN to model the one-to-many mappings in the low-dimensional space, while Stage II refines fine-grained image details using a Deterministic Neural Network (DNN). To accelerate BNN training and convergence, we introduce a dynamic Momentum Prior. Extensive experiments on multiple low-light and underwater image enhancement benchmarks demonstrate the superiority of our method over deterministic models.

**Link**: [arxiv](http://arxiv.org/abs/2501.14265v2),  [pdf](http://arxiv.org/pdf/2501.14265v2)

**Tags**: cs.CV 



### CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to   Sustainability Data Extraction
**Authors**: Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa

**Updated**: 2025-01-30T17:13:32Z

**Summary**: Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.

**Link**: [arxiv](http://arxiv.org/abs/2501.18504v1),  [pdf](http://arxiv.org/pdf/2501.18504v1)

**Tags**: cs.CV cs.AI cs.NE 68W50, 68T07 G.1.6; I.2.10 



### Beyond Prior Limits: Addressing Distribution Misalignment in Particle   Filtering
**Authors**: Yiwei Shi, Jingyu Hu, Yu Zhang, Mengyue Yang, Weinan Zhang, Cunjia Liu, Weiru Liu

**Updated**: 2025-01-30T17:11:34Z

**Summary**: Particle filtering is a Bayesian inference method and a fundamental tool in state estimation for dynamic systems, but its effectiveness is often limited by the constraints of the initial prior distribution, a phenomenon we define as the Prior Boundary Phenomenon. This challenge arises when target states lie outside the prior's support, rendering traditional particle filtering methods inadequate for accurate estimation. Although techniques like unbounded priors and larger particle sets have been proposed, they remain computationally prohibitive and lack adaptability in dynamic scenarios. To systematically overcome these limitations, we propose the Diffusion-Enhanced Particle Filtering Framework, which introduces three key innovations: adaptive diffusion through exploratory particles, entropy-driven regularisation to prevent weight collapse, and kernel-based perturbations for dynamic support expansion. These mechanisms collectively enable particle filtering to explore beyond prior boundaries, ensuring robust state estimation for out-of-boundary targets. Theoretical analysis and extensive experiments validate framework's effectiveness, indicating significant improvements in success rates and estimation accuracy across high-dimensional and non-convex scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.18501v1),  [pdf](http://arxiv.org/pdf/2501.18501v1)

**Tags**: stat.ML cs.AI cs.LG 



### The population of merging compact binaries inferred using gravitational   waves through GWTC-3
**Authors**: The LIGO Scientific Collaboration, the Virgo Collaboration, the KAGRA Collaboration, R. Abbott, T. D. Abbott, F. Acernese, K. Ackley, C. Adams, N. Adhikari, R. X. Adhikari, V. B. Adya, C. Affeldt, D. Agarwal, M. Agathos, K. Agatsuma, N. Aggarwal, O. D. Aguiar, L. Aiello, A. Ain, P. Ajith, T. Akutsu, S. Albanesi, A. Allocca, P. A. Altin, A. Amato, C. Anand, S. Anand, A. Ananyeva, S. B. Anderson, W. G. Anderson, M. Ando, T. Andrade, N. Andres, T. Andrić, S. V. Angelova, S. Ansoldi, J. M. Antelis, S. Antier, F. Antonini, S. Appert, Koji Arai, Koya Arai, Y. Arai, S. Araki, A. Araya, M. C. Araya, J. S. Areeda, M. Arène, N. Aritomi, N. Arnaud, S. M. Aronson, K. G. Arun, H. Asada, Y. Asali, G. Ashton, Y. Aso, M. Assiduo, S. M. Aston, P. Astone, F. Aubin, C. Austin, S. Babak, F. Badaracco, M. K. M. Bader, C. Badger, S. Bae, Y. Bae, A. M. Baer, S. Bagnasco, Y. Bai, L. Baiotti, J. Baird, R. Bajpai, M. Ball, G. Ballardin, S. W. Ballmer, A. Balsamo, G. Baltus, S. Banagiri, D. Bankar, J. C. Barayoga, C. Barbieri, B. C. Barish, D. Barker, P. Barneo, F. Barone, B. Barr, L. Barsotti, M. Barsuglia, D. Barta, J. Bartlett, M. A. Barton, I. Bartos, R. Bassiri, A. Basti, M. Bawaj, J. C. Bayley, A. C. Baylor, M. Bazzan, B. Bécsy, V. M. Bedakihale, M. Bejger, I. Belahcene, V. Benedetto, D. Beniwal, T. F. Bennett, J. D. Bentley, M. BenYaala, F. Bergamin, B. K. Berger, S. Bernuzzi, C. P. L. Berry, D. Bersanetti, A. Bertolini, J. Betzwieser, D. Beveridge, R. Bhandare, U. Bhardwaj, D. Bhattacharjee, S. Bhaumik, I. A. Bilenko, G. Billingsley, S. Bini, R. Birney, O. Birnholtz, S. Biscans, M. Bischi, S. Biscoveanu, A. Bisht, B. Biswas, M. Bitossi, M. -A. Bizouard, J. K. Blackburn, C. D. Blair, D. G. Blair, R. M. Blair, F. Bobba, N. Bode, M. Boer, G. Bogaert, M. Boldrini, L. D. Bonavena, F. Bondu, E. Bonilla, R. Bonnand, P. Booker, B. A. Boom, R. Bork, V. Boschi, N. Bose, S. Bose, V. Bossilkov, V. Boudart, Y. Bouffanais, A. Bozzi, C. Bradaschia, P. R. Brady, A. Bramley, A. Branch, M. Branchesi, J. E. Brau, M. Breschi, T. Briant, J. H. Briggs, A. Brillet, M. Brinkmann, P. Brockill, A. F. Brooks, J. Brooks, D. D. Brown, S. Brunett, G. Bruno, R. Bruntz, J. Bryant, T. Bulik, H. J. Bulten, A. Buonanno, R. Buscicchio, D. Buskulic, C. Buy, R. L. Byer, L. Cadonati, G. Cagnoli, C. Cahillane, J. Calderón Bustillo, J. D. Callaghan, T. A. Callister, E. Calloni, J. Cameron, J. B. Camp, M. Canepa, S. Canevarolo, M. Cannavacciuolo, K. C. Cannon, H. Cao, Z. Cao, E. Capocasa, E. Capote, G. Carapella, F. Carbognani, J. B. Carlin, M. F. Carney, M. Carpinelli, G. Carrillo, G. Carullo, T. L. Carver, J. Casanueva Diaz, C. Casentini, G. Castaldi, S. Caudill, M. Cavaglià, F. Cavalier, R. Cavalieri, M. Ceasar, G. Cella, P. Cerdá-Durán, E. Cesarini, W. Chaibi, K. Chakravarti, S. Chalathadka Subrahmanya, E. Champion, C. -H. Chan, C. Chan, C. L. Chan, K. Chan, M. Chan, K. Chandra, P. Chanial, S. Chao, P. Charlton, E. A. Chase, E. Chassande-Mottin, C. Chatterjee, Debarati Chatterjee, Deep Chatterjee, M. Chaturvedi, S. Chaty, K. Chatziioannou, C. Chen, H. Y. Chen, J. Chen, K. Chen, X. Chen, Y. -B. Chen, Y. -R. Chen, Z. Chen, H. Cheng, C. K. Cheong, H. Y. Cheung, H. Y. Chia, F. Chiadini, C-Y. Chiang, G. Chiarini, R. Chierici, A. Chincarini, M. L. Chiofalo, A. Chiummo, G. Cho, H. S. Cho, R. K. Choudhary, S. Choudhary, N. Christensen, H. Chu, Q. Chu, Y-K. Chu, S. Chua, K. W. Chung, G. Ciani, P. Ciecielag, M. Cieślar, M. Cifaldi, A. A. Ciobanu, R. Ciolfi, F. Cipriano, A. Cirone, F. Clara, E. N. Clark, J. A. Clark, L. Clarke, P. Clearwater, S. Clesse, F. Cleva, E. Coccia, E. Codazzo, P. -F. Cohadon, D. E. Cohen, L. Cohen, M. Colleoni, C. G. Collette, A. Colombo, M. Colpi, C. M. Compton, M. Constancio Jr., L. Conti, S. J. Cooper, P. Corban, T. R. Corbitt, I. Cordero-Carrión, S. Corezzi, K. R. Corley, N. Cornish, D. Corre, A. Corsi, S. Cortese, C. A. Costa, R. Cotesta, M. W. Coughlin, J. -P. Coulon, S. T. Countryman, B. Cousins, P. Couvares, D. M. Coward, M. J. Cowart, D. C. Coyne, R. Coyne, J. D. E. Creighton, T. D. Creighton, A. W. Criswell, M. Croquette, S. G. Crowder, J. R. Cudell, T. J. Cullen, A. Cumming, R. Cummings, L. Cunningham, E. Cuoco, M. Curyło, P. Dabadie, T. Dal Canton, S. Dall'Osso, G. Dálya, A. Dana, L. M. DaneshgaranBajastani, B. D'Angelo, S. Danilishin, S. D'Antonio, K. Danzmann, C. Darsow-Fromm, A. Dasgupta, L. E. H. Datrier, S. Datta, V. Dattilo, I. Dave, M. Davier, G. S. Davies, D. Davis, M. C. Davis, E. J. Daw, R. Dean, D. DeBra, M. Deenadayalan, J. Degallaix, M. De Laurentis, S. Deléglise, V. Del Favero, F. De Lillo, N. De Lillo, W. Del Pozzo, L. M. DeMarchi, F. De Matteis, V. D'Emilio, N. Demos, T. Dent, A. Depasse, R. De Pietri, R. De Rosa, C. De Rossi, R. DeSalvo, R. De Simone, S. Dhurandhar, M. C. Díaz, M. Diaz-Ortiz Jr., N. A. Didio, T. Dietrich, L. Di Fiore, C. Di Fronzo, C. Di Giorgio, F. Di Giovanni, M. Di Giovanni, T. Di Girolamo, A. Di Lieto, B. Ding, S. Di Pace, I. Di Palma, F. Di Renzo, A. K. Divakarla, A. Dmitriev, Z. Doctor, L. D'Onofrio, F. Donovan, K. L. Dooley, S. Doravari, I. Dorrington, M. Drago, J. C. Driggers, Y. Drori, J. -G. Ducoin, P. Dupej, O. Durante, D. D'Urso, P. -A. Duverne, S. E. Dwyer, C. Eassa, P. J. Easter, M. Ebersold, T. Eckhardt, G. Eddolls, B. Edelman, T. B. Edo, O. Edy, A. Effler, S. Eguchi, J. Eichholz, S. S. Eikenberry, M. Eisenmann, R. A. Eisenstein, A. Ejlli, E. Engelby, Y. Enomoto, L. Errico, R. C. Essick, H. Estellés, D. Estevez, Z. Etienne, T. Etzel, M. Evans, T. M. Evans, B. E. Ewing, V. Fafone, H. Fair, S. Fairhurst, A. M. Farah, S. Farinon, B. Farr, W. M. Farr, N. W. Farrow, E. J. Fauchon-Jones, G. Favaro, M. Favata, M. Fays, M. Fazio, J. Feicht, M. M. Fejer, E. Fenyvesi, D. L. Ferguson, A. Fernandez-Galiana, I. Ferrante, T. A. Ferreira, F. Fidecaro, P. Figura, I. Fiori, M. Fishbach, R. P. Fisher, R. Fittipaldi, V. Fiumara, R. Flaminio, E. Floden, H. Fong, J. A. Font, B. Fornal, P. W. F. Forsyth, A. Franke, S. Frasca, F. Frasconi, C. Frederick, J. P. Freed, Z. Frei, A. Freise, R. Frey, P. Fritschel, V. V. Frolov, G. G. Fronzé, Y. Fujii, Y. Fujikawa, M. Fukunaga, M. Fukushima, P. Fulda, M. Fyffe, H. A. Gabbard, B. U. Gadre, J. R. Gair, J. Gais, S. Galaudage, R. Gamba, D. Ganapathy, A. Ganguly, D. Gao, S. G. Gaonkar, B. Garaventa, C. García-Núñez, C. García-Quirós, F. Garufi, B. Gateley, S. Gaudio, V. Gayathri, G. -G. Ge, G. Gemme, A. Gennai, J. George, O. Gerberding, L. Gergely, P. Gewecke, S. Ghonge, Abhirup Ghosh, Archisman Ghosh, Shaon Ghosh, Shrobana Ghosh, B. Giacomazzo, L. Giacoppo, J. A. Giaime, K. D. Giardina, D. R. Gibson, C. Gier, M. Giesler, P. Giri, F. Gissi, J. Glanzer, A. E. Gleckl, P. Godwin, E. Goetz, R. Goetz, N. Gohlke, J. Golomb, B. Goncharov, G. González, A. Gopakumar, M. Gosselin, R. Gouaty, D. W. Gould, B. Grace, A. Grado, M. Granata, V. Granata, A. Grant, S. Gras, P. Grassia, C. Gray, R. Gray, G. Greco, A. C. Green, R. Green, A. M. Gretarsson, E. M. Gretarsson, D. Griffith, W. Griffiths, H. L. Griggs, G. Grignani, A. Grimaldi, S. J. Grimm, H. Grote, S. Grunewald, P. Gruning, D. Guerra, G. M. Guidi, A. R. Guimaraes, G. Guixé, H. K. Gulati, H. -K. Guo, Y. Guo, Anchal Gupta, Anuradha Gupta, P. Gupta, E. K. Gustafson, R. Gustafson, F. Guzman, S. Ha, L. Haegel, A. Hagiwara, S. Haino, O. Halim, E. D. Hall, E. Z. Hamilton, G. Hammond, W. -B. Han, M. Haney, J. Hanks, C. Hanna, M. D. Hannam, O. Hannuksela, H. Hansen, T. J. Hansen, J. Hanson, T. Harder, T. Hardwick, K. Haris, J. Harms, G. M. Harry, I. W. Harry, D. Hartwig, K. Hasegawa, B. Haskell, R. K. Hasskew, C. -J. Haster, K. Hattori, K. Haughian, H. Hayakawa, K. Hayama, F. J. Hayes, J. Healy, A. Heidmann, A. Heidt, M. C. Heintze, J. Heinze, J. Heinzel, H. Heitmann, F. Hellman, P. Hello, A. F. Helmling-Cornell, G. Hemming, M. Hendry, I. S. Heng, E. Hennes, J. Hennig, M. H. Hennig, A. G. Hernandez, F. Hernandez Vivanco, M. Heurs, S. Hild, P. Hill, Y. Himemoto, A. S. Hines, Y. Hiranuma, N. Hirata, E. Hirose, S. Hochheim, D. Hofman, J. N. Hohmann, D. G. Holcomb, N. A. Holland, I. J. Hollows, Z. J. Holmes, K. Holt, D. E. Holz, Z. Hong, P. Hopkins, J. Hough, S. Hourihane, E. J. Howell, C. G. Hoy, D. Hoyland, A. Hreibi, B-H. Hsieh, Y. Hsu, G-Z. Huang, H-Y. Huang, P. Huang, Y-C. Huang, Y. -J. Huang, Y. Huang, M. T. Hübner, A. D. Huddart, B. Hughey, D. C. Y. Hui, V. Hui, S. Husa, S. H. Huttner, R. Huxford, T. Huynh-Dinh, S. Ide, B. Idzkowski, A. Iess, B. Ikenoue, S. Imam, K. Inayoshi, C. Ingram, Y. Inoue, K. Ioka, M. Isi, K. Isleif, K. Ito, Y. Itoh, B. R. Iyer, K. Izumi, V. JaberianHamedan, T. Jacqmin, S. J. Jadhav, S. P. Jadhav, A. L. James, A. Z. Jan, K. Jani, J. Janquart, K. Janssens, N. N. Janthalur, P. Jaranowski, D. Jariwala, R. Jaume, A. C. Jenkins, K. Jenner, C. Jeon, M. Jeunon, W. Jia, H. -B. Jin, G. R. Johns, A. W. Jones, D. I. Jones, J. D. Jones, P. Jones, R. Jones, R. J. G. Jonker, L. Ju, P. Jung, k. Jung, J. Junker, V. Juste, K. Kaihotsu, T. Kajita, M. Kakizaki, C. V. Kalaghatgi, V. Kalogera, B. Kamai, M. Kamiizumi, N. Kanda, S. Kandhasamy, G. Kang, J. B. Kanner, Y. Kao, S. J. Kapadia, D. P. Kapasi, S. Karat, C. Karathanasis, S. Karki, R. Kashyap, M. Kasprzack, W. Kastaun, S. Katsanevas, E. Katsavounidis, W. Katzman, T. Kaur, K. Kawabe, K. Kawaguchi, N. Kawai, T. Kawasaki, F. Kéfélian, D. Keitel, J. S. Key, S. Khadka, F. Y. Khalili, S. Khan, E. A. Khazanov, N. Khetan, M. Khursheed, N. Kijbunchoo, C. Kim, J. C. Kim, J. Kim, K. Kim, W. S. Kim, Y. -M. Kim, C. Kimball, N. Kimura, M. Kinley-Hanlon, R. Kirchhoff, J. S. Kissel, N. Kita, H. Kitazawa, L. Kleybolte, S. Klimenko, A. M. Knee, T. D. Knowles, E. Knyazev, P. Koch, G. Koekoek, Y. Kojima, K. Kokeyama, S. Koley, P. Kolitsidou, M. Kolstein, K. Komori, V. Kondrashov, A. K. H. Kong, A. Kontos, N. Koper, M. Korobko, K. Kotake, M. Kovalam, D. B. Kozak, C. Kozakai, R. Kozu, V. Kringel, N. V. Krishnendu, A. Królak, G. Kuehn, F. Kuei, P. Kuijer, A. Kumar, P. Kumar, Rahul Kumar, Rakesh Kumar, J. Kume, K. Kuns, C. Kuo, H-S. Kuo, Y. Kuromiya, S. Kuroyanagi, K. Kusayanagi, S. Kuwahara, K. Kwak, P. Lagabbe, D. Laghi, E. Lalande, T. L. Lam, A. Lamberts, M. Landry, P. Landry, B. B. Lane, R. N. Lang, J. Lange, B. Lantz, I. La Rosa, A. Lartaux-Vollard, P. D. Lasky, M. Laxen, A. Lazzarini, C. Lazzaro, P. Leaci, S. Leavey, Y. K. Lecoeuche, H. K. Lee, H. M. Lee, H. W. Lee, J. Lee, K. Lee, R. Lee, J. Lehmann, A. Lemaître, M. Leonardi, N. Leroy, N. Letendre, C. Levesque, Y. Levin, J. N. Leviton, K. Leyde, A. K. Y. Li, B. Li, J. Li, K. L. Li, T. G. F. Li, X. Li, C-Y. Lin, F-K. Lin, F-L. Lin, H. L. Lin, L. C. -C. Lin, F. Linde, S. D. Linker, J. N. Linley, T. B. Littenberg, G. C. Liu, J. Liu, K. Liu, X. Liu, F. Llamas, M. Llorens-Monteagudo, R. K. L. Lo, A. Lockwood, L. T. London, A. Longo, D. Lopez, M. Lopez Portilla, M. Lorenzini, V. Loriette, M. Lormand, G. Losurdo, T. P. Lott, J. D. Lough, C. O. Lousto, G. Lovelace, J. F. Lucaccioni, H. Lück, D. Lumaca, A. P. Lundgren, L. -W. Luo, J. E. Lynam, R. Macas, M. MacInnis, D. M. Macleod, I. A. O. MacMillan, A. Macquet, I. Magaña Hernandez, C. Magazzù, R. M. Magee, R. Maggiore, M. Magnozzi, S. Mahesh, E. Majorana, C. Makarem, I. Maksimovic, S. Maliakal, A. Malik, N. Man, V. Mandic, V. Mangano, J. L. Mango, G. L. Mansell, M. Manske, M. Mantovani, M. Mapelli, F. Marchesoni, M. Marchio, F. Marion, Z. Mark, S. Márka, Z. Márka, C. Markakis, A. S. Markosyan, A. Markowitz, E. Maros, A. Marquina, S. Marsat, F. Martelli, I. W. Martin, R. M. Martin, M. Martinez, V. A. Martinez, V. Martinez, K. Martinovic, D. V. Martynov, E. J. Marx, H. Masalehdan, K. Mason, E. Massera, A. Masserot, T. J. Massinger, M. Masso-Reid, S. Mastrogiovanni, A. Matas, M. Mateu-Lucena, F. Matichard, M. Matiushechkina, N. Mavalvala, J. J. McCann, R. McCarthy, D. E. McClelland, P. K. McClincy, S. McCormick, L. McCuller, G. I. McGhee, S. C. McGuire, C. McIsaac, J. McIver, T. McRae, S. T. McWilliams, D. Meacher, M. Mehmet, A. K. Mehta, Q. Meijer, A. Melatos, D. A. Melchor, G. Mendell, A. Menendez-Vazquez, C. S. Menoni, R. A. Mercer, L. Mereni, K. Merfeld, E. L. Merilh, J. D. Merritt, M. Merzougui, S. Meshkov, C. Messenger, C. Messick, P. M. Meyers, F. Meylahn, A. Mhaske, A. Miani, H. Miao, I. Michaloliakos, C. Michel, Y. Michimura, H. Middleton, L. Milano, A. L. Miller, A. Miller, B. Miller, S. Miller, M. Millhouse, J. C. Mills, E. Milotti, O. Minazzoli, Y. Minenkov, N. Mio, Ll. M. Mir, M. Miravet-Tenés, C. Mishra, T. Mishra, T. Mistry, S. Mitra, V. P. Mitrofanov, G. Mitselmakher, R. Mittleman, O. Miyakawa, A. Miyamoto, Y. Miyazaki, K. Miyo, S. Miyoki, Geoffrey Mo, E. Moguel, K. Mogushi, S. R. P. Mohapatra, S. R. Mohite, I. Molina, M. Molina-Ruiz, M. Mondin, M. Montani, C. J. Moore, D. Moraru, F. Morawski, A. More, C. Moreno, G. Moreno, Y. Mori, S. Morisaki, Y. Moriwaki, B. Mours, C. M. Mow-Lowry, S. Mozzon, F. Muciaccia, Arunava Mukherjee, D. Mukherjee, Soma Mukherjee, Subroto Mukherjee, Suvodip Mukherjee, N. Mukund, A. Mullavey, J. Munch, E. A. Muñiz, P. G. Murray, R. Musenich, S. Muusse, S. L. Nadji, K. Nagano, S. Nagano, A. Nagar, K. Nakamura, H. Nakano, M. Nakano, R. Nakashima, Y. Nakayama, V. Napolano, I. Nardecchia, T. Narikawa, L. Naticchioni, B. Nayak, R. K. Nayak, R. Negishi, B. F. Neil, J. Neilson, G. Nelemans, T. J. N. Nelson, M. Nery, P. Neubauer, A. Neunzert, K. Y. Ng, S. W. S. Ng, C. Nguyen, P. Nguyen, T. Nguyen, L. Nguyen Quynh, W. -T. Ni, S. A. Nichols, A. Nishizawa, S. Nissanke, E. Nitoglia, F. Nocera, M. Norman, C. North, S. Nozaki, L. K. Nuttall, J. Oberling, B. D. O'Brien, Y. Obuchi, J. O'Dell, E. Oelker, W. Ogaki, G. Oganesyan, J. J. Oh, K. Oh, S. H. Oh, M. Ohashi, N. Ohishi, M. Ohkawa, F. Ohme, H. Ohta, M. A. Okada, Y. Okutani, K. Okutomi, C. Olivetto, K. Oohara, C. Ooi, R. Oram, B. O'Reilly, R. G. Ormiston, N. D. Ormsby, L. F. Ortega, R. O'Shaughnessy, E. O'Shea, S. Oshino, S. Ossokine, C. Osthelder, S. Otabe, D. J. Ottaway, H. Overmier, A. E. Pace, G. Pagano, M. A. Page, G. Pagliaroli, A. Pai, S. A. Pai, J. R. Palamos, O. Palashov, C. Palomba, H. Pan, K. Pan, P. K. Panda, H. Pang, P. T. H. Pang, C. Pankow, F. Pannarale, B. C. Pant, F. H. Panther, F. Paoletti, A. Paoli, A. Paolone, A. Parisi, H. Park, J. Park, W. Parker, D. Pascucci, A. Pasqualetti, R. Passaquieti, D. Passuello, M. Patel, M. Pathak, B. Patricelli, A. S. Patron, S. Paul, E. Payne, M. Pedraza, M. Pegoraro, A. Pele, F. E. Peña Arellano, S. Penn, A. Perego, A. Pereira, T. Pereira, C. J. Perez, C. Périgois, C. C. Perkins, A. Perreca, S. Perriès, J. Petermann, D. Petterson, H. P. Pfeiffer, K. A. Pham, K. S. Phukon, O. J. Piccinni, M. Pichot, M. Piendibene, F. Piergiovanni, L. Pierini, V. Pierro, G. Pillant, M. Pillas, F. Pilo, L. Pinard, I. M. Pinto, M. Pinto, K. Piotrzkowski, M. Pirello, M. D. Pitkin, E. Placidi, L. Planas, W. Plastino, C. Pluchar, R. Poggiani, E. Polini, D. Y. T. Pong, S. Ponrathnam, P. Popolizio, E. K. Porter, R. Poulton, J. Powell, M. Pracchia, T. Pradier, A. K. Prajapati, K. Prasai, R. Prasanna, G. Pratten, M. Principe, G. A. Prodi, L. Prokhorov, P. Prosposito, L. Prudenzi, A. Puecher, M. Punturo, F. Puosi, P. Puppo, M. Pürrer, H. Qi, V. Quetschke, R. Quitzow-James, F. J. Raab, G. Raaijmakers, H. Radkins, N. Radulesco, P. Raffai, S. X. Rail, S. Raja, C. Rajan, K. E. Ramirez, T. D. Ramirez, A. Ramos-Buades, J. Rana, P. Rapagnani, U. D. Rapol, A. Ray, V. Raymond, N. Raza, M. Razzano, J. Read, L. A. Rees, T. Regimbau, L. Rei, S. Reid, S. W. Reid, D. H. Reitze, P. Relton, A. Renzini, P. Rettegno, M. Rezac, F. Ricci, D. Richards, J. W. Richardson, L. Richardson, G. Riemenschneider, K. Riles, S. Rinaldi, K. Rink, M. Rizzo, N. A. Robertson, R. Robie, F. Robinet, A. Rocchi, S. Rodriguez, L. Rolland, J. G. Rollins, M. Romanelli, R. Romano, C. L. Romel, A. Romero-Rodríguez, I. M. Romero-Shaw, J. H. Romie, S. Ronchini, L. Rosa, C. A. Rose, D. Rosińska, M. P. Ross, S. Rowan, S. J. Rowlinson, S. Roy, Santosh Roy, Soumen Roy, D. Rozza, P. Ruggi, K. Ryan, S. Sachdev, T. Sadecki, J. Sadiq, N. Sago, S. Saito, Y. Saito, K. Sakai, Y. Sakai, M. Sakellariadou, Y. Sakuno, O. S. Salafia, L. Salconi, M. Saleem, F. Salemi, A. Samajdar, E. J. Sanchez, J. H. Sanchez, L. E. Sanchez, N. Sanchis-Gual, J. R. Sanders, A. Sanuy, T. R. Saravanan, N. Sarin, B. Sassolas, H. Satari, B. S. Sathyaprakash, S. Sato, T. Sato, O. Sauter, R. L. Savage, T. Sawada, D. Sawant, H. L. Sawant, S. Sayah, D. Schaetzl, M. Scheel, J. Scheuer, M. Schiworski, P. Schmidt, S. Schmidt, R. Schnabel, M. Schneewind, R. M. S. Schofield, A. Schönbeck, B. W. Schulte, B. F. Schutz, E. Schwartz, J. Scott, S. M. Scott, M. Seglar-Arroyo, T. Sekiguchi, Y. Sekiguchi, D. Sellers, A. S. Sengupta, D. Sentenac, E. G. Seo, V. Sequino, A. Sergeev, Y. Setyawati, T. Shaffer, M. S. Shahriar, B. Shams, L. Shao, A. Sharma, P. Sharma, P. Shawhan, N. S. Shcheblanov, S. Shibagaki, M. Shikauchi, R. Shimizu, T. Shimoda, K. Shimode, H. Shinkai, T. Shishido, A. Shoda, D. H. Shoemaker, D. M. Shoemaker, S. ShyamSundar, M. Sieniawska, D. Sigg, L. P. Singer, D. Singh, N. Singh, A. Singha, A. M. Sintes, V. Sipala, V. Skliris, B. J. J. Slagmolen, T. J. Slaven-Blair, J. Smetana, J. R. Smith, R. J. E. Smith, J. Soldateschi, S. N. Somala, K. Somiya, E. J. Son, K. Soni, S. Soni, V. Sordini, F. Sorrentino, N. Sorrentino, H. Sotani, R. Soulard, T. Souradeep, E. Sowell, V. Spagnuolo, A. P. Spencer, M. Spera, R. Srinivasan, A. K. Srivastava, V. Srivastava, K. Staats, C. Stachie, D. A. Steer, J. Steinlechner, S. Steinlechner, D. J. Stops, M. Stover, K. A. Strain, L. C. Strang, G. Stratta, A. Strunk, R. Sturani, A. L. Stuver, S. Sudhagar, V. Sudhir, R. Sugimoto, H. G. Suh, T. Z. Summerscales, H. Sun, L. Sun, S. Sunil, A. Sur, J. Suresh, P. J. Sutton, Takamasa Suzuki, Toshikazu Suzuki, B. L. Swinkels, M. J. Szczepańczyk, P. Szewczyk, M. Tacca, H. Tagoshi, S. C. Tait, H. Takahashi, R. Takahashi, A. Takamori, S. Takano, H. Takeda, M. Takeda, C. J. Talbot, C. Talbot, H. Tanaka, Kazuyuki Tanaka, Kenta Tanaka, Taiki Tanaka, Takahiro Tanaka, A. J. Tanasijczuk, S. Tanioka, D. B. Tanner, D. Tao, L. Tao, E. N. Tapia San Martín, C. Taranto, J. D. Tasson, S. Telada, R. Tenorio, J. E. Terhune, L. Terkowski, M. P. Thirugnanasambandam, M. Thomas, P. Thomas, J. E. Thompson, S. R. Thondapu, K. A. Thorne, E. Thrane, Shubhanshu Tiwari, Srishti Tiwari, V. Tiwari, A. M. Toivonen, K. Toland, A. E. Tolley, T. Tomaru, Y. Tomigami, T. Tomura, M. Tonelli, A. Torres-Forné, C. I. Torrie, I. Tosta e Melo, D. Töyrä, A. Trapananti, F. Travasso, G. Traylor, M. Trevor, M. C. Tringali, A. Tripathee, L. Troiano, A. Trovato, L. Trozzo, R. J. Trudeau, D. S. Tsai, D. Tsai, K. W. Tsang, T. Tsang, J-S. Tsao, M. Tse, R. Tso, K. Tsubono, S. Tsuchida, L. Tsukada, D. Tsuna, T. Tsutsui, T. Tsuzuki, K. Turbang, M. Turconi, D. Tuyenbayev, A. S. Ubhi, N. Uchikata, T. Uchiyama, R. P. Udall, A. Ueda, T. Uehara, K. Ueno, G. Ueshima, C. S. Unnikrishnan, F. Uraguchi, A. L. Urban, T. Ushiba, A. Utina, H. Vahlbruch, G. Vajente, A. Vajpeyi, G. Valdes, M. Valentini, V. Valsan, N. van Bakel, M. van Beuzekom, J. F. J. van den Brand, C. Van Den Broeck, D. C. Vander-Hyde, L. van der Schaaf, J. V. van Heijningen, J. Vanosky, M. H. P. M. van Putten, N. van Remortel, M. Vardaro, A. F. Vargas, V. Varma, M. Vasúth, A. Vecchio, G. Vedovato, J. Veitch, P. J. Veitch, J. Venneberg, G. Venugopalan, D. Verkindt, P. Verma, Y. Verma, D. Veske, F. Vetrano, A. Viceré, S. Vidyant, A. D. Viets, A. Vijaykumar, V. Villa-Ortega, J. -Y. Vinet, A. Virtuoso, S. Vitale, T. Vo, H. Vocca, E. R. G. von Reis, J. S. A. von Wrangel, C. Vorvick, S. P. Vyatchanin, L. E. Wade, M. Wade, K. J. Wagner, R. C. Walet, M. Walker, G. S. Wallace, L. Wallace, S. Walsh, J. Wang, J. Z. Wang, W. H. Wang, R. L. Ward, J. Warner, M. Was, T. Washimi, N. Y. Washington, J. Watchi, B. Weaver, S. A. Webster, M. Weinert, A. J. Weinstein, R. Weiss, C. M. Weller, F. Wellmann, L. Wen, P. Weßels, K. Wette, J. T. Whelan, D. D. White, B. F. Whiting, C. Whittle, D. Wilken, D. Williams, M. J. Williams, A. R. Williamson, J. L. Willis, B. Willke, D. J. Wilson, W. Winkler, C. C. Wipf, T. Wlodarczyk, G. Woan, J. Woehler, J. K. Wofford, I. C. F. Wong, C. Wu, D. S. Wu, H. Wu, S. Wu, D. M. Wysocki, L. Xiao, W-R. Xu, T. Yamada, H. Yamamoto, Kazuhiro Yamamoto, Kohei Yamamoto, T. Yamamoto, K. Yamashita, R. Yamazaki, F. W. Yang, L. Yang, Y. Yang, Yang Yang, Z. Yang, M. J. Yap, D. W. Yeeles, A. B. Yelikar, M. Ying, K. Yokogawa, J. Yokoyama, T. Yokozawa, J. Yoo, T. Yoshioka, Hang Yu, Haocun Yu, H. Yuzurihara, A. Zadrożny, M. Zanolin, S. Zeidler, T. Zelenova, J. -P. Zendri, M. Zevin, M. Zhan, H. Zhang, J. Zhang, L. Zhang, T. Zhang, Y. Zhang, C. Zhao, G. Zhao, Y. Zhao, Yue Zhao, R. Zhou, Z. Zhou, X. J. Zhu, Z. -H. Zhu, A. B. Zimmerman, Y. Zlochower, M. E. Zucker, J. Zweizig

**Updated**: 2025-01-30T17:06:46Z

**Summary**: We report on the population properties of compact binary mergers inferred from gravitational-wave observations of these systems during the first three LIGO-Virgo observing runs. The Gravitational-Wave Transient Catalog 3 contains signals consistent with three classes of binary mergers: binary black hole, binary neutron star, and neutron star-black hole mergers. We infer the binary neutron star merger rate to be between 10 and 1700 Gpc$^{-3} yr$^{-1}$ and the neutron star-black hole merger rate to be between 7.8 and 140 Gpc$^{-3} yr$^{-1}$, assuming a constant rate density in the comoving frame and taking the union of 90% credible intervals for methods used in this work. We infer the binary black hole merger rate, allowing for evolution with redshift, to be between 17.9 and 44 Gpc$^{-3}$ yr$^{-1}$ at a fiducial redshift (z=0.2). The rate of binary black hole mergers is observed to increase with redshift at a rate proportional to $(1+z)^\kappa$ with $\kappa=2.9^{+1.7}_{-1.8}$ for $z\lesssim1$. Using both binary neutron star and neutron star-black hole binaries, we obtain a broad, relatively flat neutron star mass distribution extending from $1.2^{+0.1}_{-0.2}$ to $2.0^{+0.3}_{-0.3}\,M_\odot$. We confidently determine that the merger rate as a function of mass sharply declines after the expected maximum neutron star mass, but cannot yet confirm or rule out the existence of a lower mass gap between neutron stars and black holes. We also find the binary black hole mass distribution has localized over- and underdensities relative to a power-law distribution, with peaks emerging at chirp masses of $8.3^{+0.3}_{-0.5}$ and $27.9^{+1.9}_{-1.8}\,M_\odot$. While we continue to find that the mass distribution of a binary's more massive component strongly decreases as a function of primary mass, we observe no evidence of a strongly suppressed merger rate above approximately $60\,M_\odot$ [abridged]

**Link**: [arxiv](http://arxiv.org/abs/2111.03634v5),  [pdf](http://arxiv.org/pdf/2111.03634v5)

**Tags**: astro-ph.HE gr-qc 



### GuardReasoner: Towards Reasoning-based LLM Safeguards
**Authors**: Yue Liu, Hongcheng Gao, Shengfang Zhai, Jun Xia, Tianyi Wu, Zhiwei Xue, Yulin Chen, Kenji Kawaguchi, Jiaheng Zhang, Bryan Hooi

**Updated**: 2025-01-30T17:06:06Z

**Summary**: As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.

**Link**: [arxiv](http://arxiv.org/abs/2501.18492v1),  [pdf](http://arxiv.org/pdf/2501.18492v1)

**Tags**: cs.CR cs.AI cs.LG 



### Track-On: Transformer-based Online Point Tracking with Memory
**Authors**: Görkay Aydemir, Xiongyi Cai, Weidi Xie, Fatma Güney

**Updated**: 2025-01-30T17:04:11Z

**Summary**: In this paper, we consider the problem of long-term point tracking, which requires consistent identification of points across multiple frames in a video, despite changes in appearance, lighting, perspective, and occlusions. We target online tracking on a frame-by-frame basis, making it suitable for real-world, streaming scenarios. Specifically, we introduce Track-On, a simple transformer-based model designed for online long-term point tracking. Unlike prior methods that depend on full temporal modeling, our model processes video frames causally without access to future frames, leveraging two memory modules -- spatial memory and context memory -- to capture temporal information and maintain reliable point tracking over long time horizons. At inference time, it employs patch classification and refinement to identify correspondences and track points with high accuracy. Through extensive experiments, we demonstrate that Track-On sets a new state-of-the-art for online models and delivers superior or competitive results compared to offline approaches on seven datasets, including the TAP-Vid benchmark. Our method offers a robust and scalable solution for real-time tracking in diverse applications. Project page: https://kuis-ai.github.io/track_on

**Link**: [arxiv](http://arxiv.org/abs/2501.18487v1),  [pdf](http://arxiv.org/pdf/2501.18487v1)

**Tags**: cs.CV 



### A Tool for In-depth Analysis of Code Execution Reasoning of Large   Language Models
**Authors**: Changshu Liu, Reyhaneh Jabbarvand

**Updated**: 2025-01-30T16:56:08Z

**Summary**: Code Executing Reasoning is becoming a new non-functional metric that assesses the ability of large language models (LLMs) in programming tasks. State-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval) usually focus on LLM's prediction of a given code's input/output or intermediate variable states/values on limited programs. However, there is no tool for more in-depth analysis of the results. Without such a tool, the observations about LLM's code execution reasoning cannot be generalized to more datasets, preventing the research community and practitioners from devising the next generation of LLMs with better code execution reasoning abilities. This paper introduces ExeRScope, a series of tools and heuristics to analyze the result of code execution reasoning frameworks to understand better the impact of code properties in the studied benchmarks on the code execution reasoning. With such tooling, analysis can be generalized to code with similar properties without the urgent need to design more benchmarks, which is a cumbersome effort.

**Link**: [arxiv](http://arxiv.org/abs/2501.18482v1),  [pdf](http://arxiv.org/pdf/2501.18482v1)

**Tags**: cs.SE 



### Evaluating LLM-based Personal Information Extraction and Countermeasures
**Authors**: Yupei Liu, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong

**Updated**: 2025-01-30T16:53:30Z

**Summary**: Automatically extracting personal information--such as name, phone number, and email address--from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods--such as regular expression, keyword search, and entity detection--achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect four datasets including a synthetic dataset generated by GPT-4 and three real-world datasets with manually labeled eight categories of personal information; introduce a novel mitigation strategy based on prompt injection; and systematically benchmark LLM-based attacks and countermeasures using ten LLMs and five datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms traditional methods; and prompt injection can defend against strong LLM-based attacks, reducing the attack to less effective traditional ones.

**Link**: [arxiv](http://arxiv.org/abs/2408.07291v2),  [pdf](http://arxiv.org/pdf/2408.07291v2)

**Tags**: cs.CR 



### Interpolation Filter Design for Sample Rate Independent Audio Effect   RNNs
**Authors**: Alistair Carson, Alec Wright, Stefan Bilbao

**Updated**: 2025-01-30T16:48:49Z

**Summary**: Recurrent neural networks (RNNs) are effective at emulating the non-linear, stateful behavior of analog guitar amplifiers and distortion effects. Unlike the case of direct circuit simulation, RNNs have a fixed sample rate encoded in their model weights, making the sample rate non-adjustable during inference. Recent work has proposed increasing the sample rate of RNNs at inference (oversampling) by increasing the feedback delay length in samples, using a fractional delay filter for non-integer conversions. Here, we investigate the task of lowering the sample rate at inference (undersampling), and propose using an extrapolation filter to approximate the required fractional signal advance. We consider two filter design methods and analyse the impact of filter order on audio quality. Our results show that the correct choice of filter can give high quality results for both oversampling and undersampling; however, in some cases the sample rate adjustment leads to unwanted artefacts in the output signal. We analyse these failure cases through linearised stability analysis, showing that they result from instability around a fixed point. This approach enables an informed prediction of suitable interpolation filters for a given RNN model before runtime.

**Link**: [arxiv](http://arxiv.org/abs/2409.15884v2),  [pdf](http://arxiv.org/pdf/2409.15884v2)

**Tags**: eess.AS cs.SD eess.SP 



### CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA   Initialization
**Authors**: Yanxia Deng, Aozhong Zhang, Naigang Wang, Selcuk Gurses, Zi Yang, Penghang Yin

**Updated**: 2025-01-30T16:48:15Z

**Summary**: Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has become a highly efficient approach for downstream tasks, particularly in scenarios with limited computational resources. However, applying LoRA techniques to quantized LLMs poses unique challenges due to the reduced representational precision of quantized weights. In this paper, we introduce CLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic initialization strategy designed to overcome these challenges. Our approach focuses on minimizing the layer-wise discrepancy between the original LLM and its quantized counterpart with LoRA components during initialization. By leveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and determines the optimal LoRA components for each layer, ensuring a strong foundation for subsequent fine-tuning. A key contribution of this work is a novel theoretical result that enables the accurate and closed-form construction of these optimal LoRA components. We validate the efficacy of CLoQ across multiple tasks such as language generation, arithmetic reasoning, and commonsense reasoning, demonstrating that it consistently outperforms existing LoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit widths.

**Link**: [arxiv](http://arxiv.org/abs/2501.18475v1),  [pdf](http://arxiv.org/pdf/2501.18475v1)

**Tags**: cs.LG cs.AI 



### Resampling Filter Design for Multirate Neural Audio Effect Processing
**Authors**: Alistair Carson, Vesa Välimäki, Alec Wright, Stefan Bilbao

**Updated**: 2025-01-30T16:44:49Z

**Summary**: Neural networks have become ubiquitous in audio effects modelling, especially for guitar amplifiers and distortion pedals. One limitation of such models is that the sample rate of the training data is implicitly encoded in the model weights and therefore not readily adjustable at inference. Recent work explored modifications to recurrent neural network architecture to approximate a sample rate independent system, enabling audio processing at a rate that differs from the original training rate. This method works well for integer oversampling and can reduce aliasing caused by nonlinear activation functions. For small fractional changes in sample rate, fractional delay filters can be used to approximate sample rate independence, but in some cases this method fails entirely. Here, we explore the use of signal resampling at the input and output of the neural network as an alternative solution. We investigate several resampling filter designs and show that a two-stage design consisting of a half-band IIR filter cascaded with a Kaiser window FIR filter can give similar or better results to the previously proposed model adjustment method with many fewer operations per sample and less than one millisecond of latency at typical audio rates. Furthermore, we investigate interpolation and decimation filters for the task of integer oversampling and show that cascaded half-band IIR and FIR designs can be used in conjunction with the model adjustment method to reduce aliasing in a range of distortion effect models.

**Link**: [arxiv](http://arxiv.org/abs/2501.18470v1),  [pdf](http://arxiv.org/pdf/2501.18470v1)

**Tags**: eess.AS cs.LG cs.SD eess.SP 



### Return of the Encoder: Maximizing Parameter Efficiency for SLMs
**Authors**: Mohamed Elfeki, Rui Liu, Chad Voegele

**Updated**: 2025-01-30T16:44:45Z

**Summary**: The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.   We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.   When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.

**Link**: [arxiv](http://arxiv.org/abs/2501.16273v2),  [pdf](http://arxiv.org/pdf/2501.16273v2)

**Tags**: cs.CL cs.AI cs.CV 



### Cross Sectional Regression with Cluster Dependence: Inference based on   Averaging
**Authors**: Subhodeep Dey, Gopal K. Basak, Samarjit Das

**Updated**: 2025-01-30T16:43:39Z

**Summary**: We re-investigate the asymptotic properties of the traditional OLS (pooled) estimator, $\hat{\beta} _P$, in the context of cluster dependence. The present study considers various scenarios under various restrictions on the cluster sizes and number of clusters. It is shown that $\hat{\beta}_P$ could be inconsistent in many realistic situations. We propose a simple estimator, $\hat{\beta}_A$ based on data averaging. The asymptotic properties of $\hat{\beta}_A$ are studied. It is shown that $\hat{\beta}_A$ is consistent even when $\hat{\beta}_P$ is inconsistent. It is further shown that the proposed estimator $\hat{\beta}_A$ is more efficient than $\hat{\beta}_P$ in many practical scenarios. As a consequence of averaging, we show that $\hat{\beta}_A$ retains consistency, asymptotic normality under classical measurement error problem circumventing the use of Instrumental Variables (IV). A detailed simulation study shows the efficacy of $\hat{\beta}_A$. It is also seen that $\hat{\beta}_A$ yields better goodness of fit.

**Link**: [arxiv](http://arxiv.org/abs/2408.13514v2),  [pdf](http://arxiv.org/pdf/2408.13514v2)

**Tags**: stat.ME 



### Dissipation bounds the coherence of stochastic limit cycles
**Authors**: Davide Santolin, Gianmaria Falasco

**Updated**: 2025-01-30T16:42:21Z

**Summary**: Overdamped stochastic systems maintained far from equilibrium can display sustained oscillations with fluctuations that decrease with the system size. The correlation time of such noisy limit cycles expressed in units of the cycle period is upper-bounded by the entropy produced per oscillation. We prove this constraint for first-order nonlinear systems in arbitrary dimensions perturbed by weak, uncorrelated Gaussian noise. We then extend the result to important examples of more general stochastic dynamics, including electronic and chemical clocks, illustrating the practical relevance of the dissipation-coherence bound for electronic computing and thermodynamic inference.

**Link**: [arxiv](http://arxiv.org/abs/2501.18469v1),  [pdf](http://arxiv.org/pdf/2501.18469v1)

**Tags**: cond-mat.stat-mech 



### LLM-AutoDiff: Auto-Differentiate Any LLM Workflow
**Authors**: Li Yin, Zhangyang Wang

**Updated**: 2025-01-30T16:40:12Z

**Summary**: Large Language Models (LLMs) have reshaped natural language processing, powering applications from multi-hop retrieval and question answering to autonomous agent workflows. Yet, prompt engineering -- the task of crafting textual inputs to effectively direct LLMs -- remains difficult and labor-intensive, particularly for complex pipelines that combine multiple LLM calls with functional operations like retrieval and data formatting. We introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering (APE) that extends textual gradient-based methods (such as Text-Grad) to multi-component, potentially cyclic LLM architectures. Implemented within the AdalFlow library, LLM-AutoDiff treats each textual input as a trainable parameter and uses a frozen backward engine LLM to generate feedback-akin to textual gradients -- that guide iterative prompt updates. Unlike prior single-node approaches, LLM-AutoDiff inherently accommodates functional nodes, preserves time-sequential behavior in repeated calls (e.g., multi-hop loops), and combats the "lost-in-the-middle" problem by isolating distinct sub-prompts (instructions, formats, or few-shot examples). It further boosts training efficiency by focusing on error-prone samples through selective gradient computation. Across diverse tasks, including single-step classification, multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff consistently outperforms existing textual gradient baselines in both accuracy and training cost. By unifying prompt optimization through a graph-centric lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating LLM workflows - mirroring the transformative role that automatic differentiation libraries have long played in neural network research.

**Link**: [arxiv](http://arxiv.org/abs/2501.16673v2),  [pdf](http://arxiv.org/pdf/2501.16673v2)

**Tags**: cs.CL 



### IV Estimation of Heterogeneous Spatial Dynamic Panel Models with   Interactive Effects
**Authors**: Jia Chen, Guowei Cui, Vasilis Sarafidis, Takashi Yamagata

**Updated**: 2025-01-30T16:36:14Z

**Summary**: This paper develops a Mean Group Instrumental Variables (MGIV) estimator for spatial dynamic panel data models with interactive effects, under large N and T asymptotics. Unlike existing approaches that typically impose slope-parameter homogeneity, MGIV accommodates cross-sectional heterogeneity in slope coefficients. The proposed estimator is linear, making it computationally efficient and robust. Furthermore, it avoids the incidental parameters problem, enabling asymptotically valid inferences without requiring bias correction. The Monte Carlo experiments indicate strong finite-sample performance of the MGIV estimator across various sample sizes and parameter configurations. The practical utility of the estimator is illustrated through an application to regional economic growth in Europe. By explicitly incorporating heterogeneity, our approach provides fresh insights into the determinants of regional growth, underscoring the critical roles of spatial and temporal dependencies.

**Link**: [arxiv](http://arxiv.org/abs/2501.18467v1),  [pdf](http://arxiv.org/pdf/2501.18467v1)

**Tags**: econ.EM 



### How Much Can We Forget about Data Contamination?
**Authors**: Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg

**Updated**: 2025-01-30T16:31:31Z

**Summary**: The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Lllama 3 405B, have forgotten the data seen at the beginning of training.

**Link**: [arxiv](http://arxiv.org/abs/2410.03249v3),  [pdf](http://arxiv.org/pdf/2410.03249v3)

**Tags**: cs.LG cs.AI cs.CL 



### A Benchmark and Evaluation for Real-World Out-of-Distribution Detection   Using Vision-Language Models
**Authors**: Shiho Noda, Atsuyuki Miyai, Qing Yu, Go Irie, Kiyoharu Aizawa

**Updated**: 2025-01-30T16:30:20Z

**Summary**: Out-of-distribution (OOD) detection is a task that detects OOD samples during inference to ensure the safety of deployed models. However, conventional benchmarks have reached performance saturation, making it difficult to compare recent OOD detection methods. To address this challenge, we introduce three novel OOD detection benchmarks that enable a deeper understanding of method characteristics and reflect real-world conditions. First, we present ImageNet-X, designed to evaluate performance under challenging semantic shifts. Second, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing robustness to covariate shifts (feature distribution shifts). Finally, we propose Wilds-FS-X, which extends these evaluations to real-world datasets, offering a more comprehensive testbed. Our experiments reveal that recent CLIP-based OOD detection methods struggle to varying degrees across the three proposed benchmarks, and none of them consistently outperforms the others. We hope the community goes beyond specific benchmarks and includes more challenging conditions reflecting real-world scenarios. The code is https://github.com/hoshi23/OOD-X-Banchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2501.18463v1),  [pdf](http://arxiv.org/pdf/2501.18463v1)

**Tags**: cs.CV 



### ExeCoder: Empowering Large Language Models with Executability   Representation for Code Translation
**Authors**: Minghua He, Fangkai Yang, Pu Zhao, Wenjie Yin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

**Updated**: 2025-01-30T16:18:52Z

**Summary**: Code translation is a crucial activity in the software development and maintenance process, and researchers have recently begun to focus on using pre-trained large language models (LLMs) for code translation. However, existing LLMs only learn the contextual semantics of code during pre-training, neglecting executability information closely related to the execution state of the code, which results in unguaranteed code executability and unreliable automated code translation. To address this issue, we propose ExeCoder, an LLM specifically designed for code translation, aimed at utilizing executability representations such as functional semantics, syntax structures, and variable dependencies to enhance the capabilities of LLMs in code translation. To evaluate the effectiveness of ExeCoder, we manually enhanced the widely used benchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X that serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder achieves state-of-the-art performance in code translation, surpassing existing open-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two metrics, and even outperforms the renowned closed-source LLM GPT-4o. Website: https://execoder4trans.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2501.18460v1),  [pdf](http://arxiv.org/pdf/2501.18460v1)

**Tags**: cs.SE 



### xJailbreak: Representation Space Guided Reinforcement Learning for   Interpretable LLM Jailbreaking
**Authors**: Sunbowen Lee, Shiwen Ni, Chi Wei, Shuaimin Li, Liyang Fan, Ahmadreza Argha, Hamid Alinejad-Rokny, Ruifeng Xu, Yicheng Gong, Min Yang

**Updated**: 2025-01-30T16:17:56Z

**Summary**: Safety alignment mechanism are essential for preventing large language models (LLMs) from generating harmful information or unethical content. However, cleverly crafted prompts can bypass these safety measures without accessing the model's internal parameters, a phenomenon known as black-box jailbreak. Existing heuristic black-box attack methods, such as genetic algorithms, suffer from limited effectiveness due to their inherent randomness, while recent reinforcement learning (RL) based methods often lack robust and informative reward signals. To address these challenges, we propose a novel black-box jailbreak method leveraging RL, which optimizes prompt generation by analyzing the embedding proximity between benign and malicious prompts. This approach ensures that the rewritten prompts closely align with the intent of the original prompts while enhancing the attack's effectiveness. Furthermore, we introduce a comprehensive jailbreak evaluation framework incorporating keywords, intent matching, and answer validation to provide a more rigorous and holistic assessment of jailbreak success. Experimental results show the superiority of our approach, achieving state-of-the-art (SOTA) performance on several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs. The codebase for this work is available at https://github.com/Aegis1863/xJailbreak.

**Link**: [arxiv](http://arxiv.org/abs/2501.16727v2),  [pdf](http://arxiv.org/pdf/2501.16727v2)

**Tags**: cs.CL 



### CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language   Model Question Answering
**Authors**: Yumeng Wang, Zhiyuan Fan, Qingyun Wang, May Fung, Heng Ji

**Updated**: 2025-01-30T16:15:38Z

**Summary**: Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions across languages, we observe significant performance disparities. To address this, we explore the Cross-Lingual Self-Aligning ability of Language Models (CALM) to align knowledge across languages. Specifically, for a given question, we sample multiple responses across different languages, and select the most self-consistent response as the target, leaving the remaining responses as negative examples. We then employ direct preference optimization (DPO) to align the model's knowledge across different languages. Evaluations on the MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing cross-lingual knowledge question answering, both in zero-shot and retrieval augmented settings. We also found that increasing the number of languages involved in CALM training leads to even higher accuracy and consistency. We offer a qualitative analysis of how cross-lingual consistency can enhance knowledge alignment and explore the method's generalizability. The source code and data of this paper are available on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2501.18457v1),  [pdf](http://arxiv.org/pdf/2501.18457v1)

**Tags**: cs.CL 



### Generative Adversarial Reduced Order Modelling
**Authors**: Dario Coscia, Nicola Demo, Gianluigi Rozza

**Updated**: 2025-01-30T15:55:29Z

**Summary**: In this work, we present GAROM, a new approach for reduced order modelling (ROM) based on generative adversarial networks (GANs). GANs have the potential to learn data distribution and generate more realistic data. While widely applied in many areas of deep learning, little research is done on their application for ROM, i.e. approximating a high-fidelity model with a simpler one. In this work, we combine the GAN and ROM framework, by introducing a data-driven generative adversarial model able to learn solutions to parametric differential equations. The latter is achieved by modelling the discriminator network as an autoencoder, extracting relevant features of the input, and applying a conditioning mechanism to the generator and discriminator networks specifying the differential equation parameters. We show how to apply our methodology for inference, provide experimental evidence of the model generalisation, and perform a convergence study of the method.

**Link**: [arxiv](http://arxiv.org/abs/2305.15881v2),  [pdf](http://arxiv.org/pdf/2305.15881v2)

**Tags**: cs.LG cs.NA math.NA 



### LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction   with LIME Explainable AI and a RAG-based Chatbot for Insights
**Authors**: Biplov Paneru, Bishwash Paneru

**Updated**: 2025-01-30T15:47:33Z

**Summary**: Ensuring safe water supplies requires effective water quality monitoring, especially in developing countries like Nepal, where contamination risks are high. This paper introduces a hybrid deep learning model to predict Nepal's seasonal water quality using a small dataset with multiple water quality parameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along with a neural network combining CNN and RNN layers, are used to capture temporal and spatial patterns in the data. The model demonstrated notable accuracy improvements, aiding proactive water quality control. CatBoost, XGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values with an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers achieved 99 percent accuracy, cross-validated across models. LIME analysis highlighted the importance of indicators like EC and DO levels in XGBoost classification decisions. The neural network model achieved 92 percent classification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in regression analysis. Furthermore, a multifunctional application was developed to predict WQI values using both regression and classification methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.10898v2),  [pdf](http://arxiv.org/pdf/2409.10898v2)

**Tags**: cs.LG cs.AI 



### o3-mini vs DeepSeek-R1: Which One is Safer?
**Authors**: Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura

**Updated**: 2025-01-30T15:45:56Z

**Summary**: The irruption of DeepSeek-R1 constitutes a turning point for the AI industry in general and the LLMs in particular. Its capabilities have demonstrated outstanding performance in several tasks, including creative thinking, code generation, maths and automated program repair, at apparently lower execution cost. However, LLMs must adhere to an important qualitative property, i.e., their alignment with safety and human values. A clear competitor of DeepSeek-R1 is its American counterpart, OpenAI's o3-mini model, which is expected to set high standards in terms of performance, safety and cost. In this paper we conduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b version) and OpenAI's o3-mini (beta version). To this end, we make use of our recently released automated safety testing tool, named ASTRAL. By leveraging this tool, we automatically and systematically generate and execute a total of 1260 unsafe test inputs on both models. After conducting a semi-automated assessment of the outcomes provided by both LLMs, the results indicate that DeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our evaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts whereas o3-mini only to 1.19%.

**Link**: [arxiv](http://arxiv.org/abs/2501.18438v1),  [pdf](http://arxiv.org/pdf/2501.18438v1)

**Tags**: cs.SE cs.AI 



### Large Language Models Reflect the Ideology of their Creators
**Authors**: Maarten Buyl, Alexander Rogiers, Sander Noels, Guillaume Bied, Iris Dominguez-Catena, Edith Heiter, Iman Johary, Alexandru-Cristian Mara, Raphaël Romero, Jefrey Lijffijt, Tijl De Bie

**Updated**: 2025-01-30T15:45:45Z

**Summary**: Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use.   In this paper, we prompt a diverse panel of popular LLMs to describe a large number of prominent personalities with political relevance, in all six official languages of the United Nations. By identifying and analyzing moral assessments reflected in their responses, we find normative differences between LLMs from different geopolitical regions, as well as between the responses of the same LLM when prompted in different languages. Among only models in the United States, we find that popularly hypothesized disparities in political views are reflected in significant normative differences related to progressive values. Among Chinese models, we characterize a division between internationally- and domestically-focused models.   Our results show that the ideological stance of an LLM appears to reflect the worldview of its creators. This poses the risk of political instrumentalization and raises concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically 'unbiased'.

**Link**: [arxiv](http://arxiv.org/abs/2410.18417v2),  [pdf](http://arxiv.org/pdf/2410.18417v2)

**Tags**: cs.CL cs.LG 



### GENIE: Generative Note Information Extraction model for structuring EHR   data
**Authors**: Huaiyuan Ying, Hongyi Yuan, Jinsen Lu, Zitian Qu, Yang Zhao, Zhengyun Zhao, Isaac Kohane, Tianxi Cai, Sheng Yu

**Updated**: 2025-01-30T15:42:24Z

**Summary**: Electronic Health Records (EHRs) hold immense potential for advancing healthcare, offering rich, longitudinal data that combines structured information with valuable insights from unstructured clinical notes. However, the unstructured nature of clinical text poses significant challenges for secondary applications. Traditional methods for structuring EHR free-text data, such as rule-based systems and multi-stage pipelines, are often limited by their time-consuming configurations and inability to adapt across clinical notes from diverse healthcare settings. Few systems provide a comprehensive attribute extraction for terminologies. While giant large language models (LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow, costly, and impractical for large-scale use. To overcome these limitations, we introduce GENIE, a Generative Note Information Extraction system that leverages LLMs to streamline the structuring of unstructured clinical text into usable data with standardized format. GENIE processes entire paragraphs in a single pass, extracting entities, assertion statuses, locations, modifiers, values, and purposes with high accuracy. Its unified, end-to-end approach simplifies workflows, reduces errors, and eliminates the need for extensive manual intervention. Using a robust data preparation pipeline and fine-tuned small scale LLMs, GENIE achieves competitive performance across multiple information extraction tasks, outperforming traditional tools like cTAKES and MetaMap and can handle extra attributes to be extracted. GENIE strongly enhances real-world applicability and scalability in healthcare systems. By open-sourcing the model and test data, we aim to encourage collaboration and drive further advancements in EHR structurization.

**Link**: [arxiv](http://arxiv.org/abs/2501.18435v1),  [pdf](http://arxiv.org/pdf/2501.18435v1)

**Tags**: cs.CL 



### SANA 1.5: Efficient Scaling of Training-Time and Inference-Time Compute   in Linear Diffusion Transformer
**Authors**: Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, Bingchen Liu, Daquan Zhou, Song Han

**Updated**: 2025-01-30T15:31:48Z

**Summary**: This paper presents SANA-1.5, a linear Diffusion Transformer for efficient scaling in text-to-image generation. Building upon SANA-1.0, we introduce three key innovations: (1) Efficient Training Scaling: A depth-growth paradigm that enables scaling from 1.6B to 4.8B parameters with significantly reduced computational resources, combined with a memory-efficient 8-bit optimizer. (2) Model Depth Pruning: A block importance analysis technique for efficient model compression to arbitrary sizes with minimal quality loss. (3) Inference-time Scaling: A repeated sampling strategy that trades computation for model capacity, enabling smaller models to match larger model quality at inference time. Through these strategies, SANA-1.5 achieves a text-image alignment score of 0.72 on GenEval, which can be further improved to 0.80 through inference scaling, establishing a new SoTA on GenEval benchmark. These innovations enable efficient model scaling across different compute budgets while maintaining high quality, making high-quality image generation more accessible.

**Link**: [arxiv](http://arxiv.org/abs/2501.18427v1),  [pdf](http://arxiv.org/pdf/2501.18427v1)

**Tags**: cs.CV 



### Effective Learning with Node Perturbation in Multi-Layer Neural Networks
**Authors**: Sander Dalm, Marcel van Gerven, Nasir Ahmad

**Updated**: 2025-01-30T15:30:55Z

**Summary**: Backpropagation (BP) remains the dominant and most successful method for training parameters of deep neural network models. However, BP relies on two computationally distinct phases, does not provide a satisfactory explanation of biological learning, and can be challenging to apply for training of networks with discontinuities or noisy node dynamics. By comparison, node perturbation (NP) proposes learning by the injection of noise into network activations, and subsequent measurement of the induced loss change. NP relies on two forward (inference) passes, does not make use of network derivatives, and has been proposed as a model for learning in biological systems. However, standard NP is highly data inefficient and unstable due to its unguided noise-based search process. In this work, we investigate different formulations of NP and relate it to the concept of directional derivatives as well as combining it with a decorrelating mechanism for layer-wise inputs. We find that a closer alignment with directional derivatives together with input decorrelation at every layer strongly enhances performance of NP learning with large improvements in parameter convergence and much higher performance on the test data, approaching that of BP. Furthermore, our novel formulation allows for application to noisy systems in which the noise process itself is inaccessible.

**Link**: [arxiv](http://arxiv.org/abs/2310.00965v5),  [pdf](http://arxiv.org/pdf/2310.00965v5)

**Tags**: cs.LG 



### DeepExtractor: Time-domain reconstruction of signals and glitches in   gravitational wave data with deep learning
**Authors**: Tom Dooney, Harsh Narola, Stefano Bromuri, R. Lyana Curier, Chris Van Den Broeck, Sarah Caudill, Daniel Stanley Tan

**Updated**: 2025-01-30T15:25:30Z

**Summary**: Gravitational wave (GW) interferometers, detect faint signals from distant astrophysical events, such as binary black hole mergers. However, their high sensitivity also makes them susceptible to background noise, which can obscure these signals. This noise often includes transient artifacts called "glitches" that can mimic astrophysical signals or mask their characteristics. Fast and accurate reconstruction of both signals and glitches is crucial for reliable scientific inference. In this study, we present DeepExtractor, a deep learning framework designed to reconstruct signals and glitches with power exceeding interferometer noise, regardless of their source. We design DeepExtractor to model the inherent noise distribution of GW interferometers, following conventional assumptions that the noise is Gaussian and stationary over short time scales. It operates by predicting and subtracting the noise component of the data, retaining only the clean reconstruction. Our approach achieves superior generalization capabilities for arbitrary signals and glitches compared to methods that directly map inputs to the clean training waveforms. We validate DeepExtractor's effectiveness through three experiments: (1) reconstructing simulated glitches injected into simulated detector noise, (2) comparing performance with the state-of-the-art BayesWave algorithm, and (3) analyzing real data from the Gravity Spy dataset to demonstrate effective glitch subtraction from LIGO strain data. DeepExtractor achieves a median mismatch of only 0.9% for simulated glitches, outperforming several deep learning baselines. Additionally, DeepExtractor surpasses BayesWave in glitch recovery, offering a dramatic computational speedup by reconstructing one glitch sample in approx. 0.1 seconds on a CPU, compared to BayesWave's processing time of approx. one hour per glitch.

**Link**: [arxiv](http://arxiv.org/abs/2501.18423v1),  [pdf](http://arxiv.org/pdf/2501.18423v1)

**Tags**: gr-qc astro-ph.IM cs.LG physics.data-an physics.ins-det 



### Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training
**Authors**: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca

**Updated**: 2025-01-30T15:24:28Z

**Summary**: Network pruning focuses on computational techniques that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has been pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations' alignment w.r.t. their corresponding dense models. Hence, we propose \textsc{NeuroAL}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, which modifies the block-wise and row-wise sparsity exploiting information from both the dense model and its sparse version to maximize the \emph{neuron alignment} among activations. Differently from existing methods, our approach adaptively selects the best hyperparameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity, and requires \emph{no re-training}. We test our method over 276 cases combining four LLM families, three sparsity ratios, and ten language tasks (three language modeling and seven zero-shot datasets), showing how it consistently outperforms the latest state-of-the-art methods in terms of performance-runtime trade-off. The code is available at \href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.

**Link**: [arxiv](http://arxiv.org/abs/2411.07066v3),  [pdf](http://arxiv.org/pdf/2411.07066v3)

**Tags**: cs.LG cs.AI cs.CL 



### Optimizers for Stabilizing Likelihood-free Inference
**Authors**: G. Bruno De Luca, Benjamin Nachman, Eva Silverstein, Henry Zheng

**Updated**: 2025-01-30T15:16:09Z

**Summary**: A growing number of applications in particle physics and beyond use neural networks as unbinned likelihood ratio estimators applied to real or simulated data. Precision requirements on the inference tasks demand a high-level of stability from these networks, which are affected by the stochastic nature of training. We show how physics concepts can be used to stabilize network training through a physics-inspired optimizer. In particular, the Energy Conserving Descent (ECD) optimization framework uses classical Hamiltonian dynamics on the space of network parameters to reduce the dependence on the initial conditions while also stabilizing the result near the minimum of the loss function. We develop a version of this optimizer known as $ECD_{q=1}$, which has few free hyperparameters with limited ranges guided by physical reasoning. We apply $ECD_{q=1}$ to representative likelihood-ratio estimation tasks in particle physics and find that it out-performs the widely-used Adam optimizer. We expect that ECD will be a useful tool for wide array of data-limited problems, where it is computationally expensive to exhaustively optimize hyperparameters and mitigate fluctuations with ensembling.

**Link**: [arxiv](http://arxiv.org/abs/2501.18419v1),  [pdf](http://arxiv.org/pdf/2501.18419v1)

**Tags**: hep-ph hep-th 



### Causal Inference Real-Time Anomaly Detection with Synthetic Anomaly   Monitoring (SAM)
**Authors**: Emanuele Luzio, Moacir Antonelli Ponti

**Updated**: 2025-01-30T15:15:17Z

**Summary**: Anomaly detection is essential for identifying rare and significant events across diverse domains such as finance, cybersecurity, and network monitoring. This paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach that applies synthetic control methods from causal inference to improve both the accuracy and interpretability of anomaly detection processes. By modeling normal behavior through the treatment of each feature as a control unit, SAM identifies anomalies as deviations within this causal framework. We conducted extensive experiments comparing SAM with established benchmark models, including Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors (kNN), and One-Class Support Vector Machine (SVM), across five diverse datasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup 1999, among others. Our results demonstrate that SAM consistently delivers robust performance, highlighting its potential as a powerful tool for real-time anomaly detection in dynamic and complex environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.18417v1),  [pdf](http://arxiv.org/pdf/2501.18417v1)

**Tags**: cs.LG 62H30, 68T05, 62G99, 91G80, 68M10 I.5.4; K.6.5; I.2.6; H.4.2 



### Exploring Potential Prompt Injection Attacks in Federated Military LLMs   and Their Mitigation
**Authors**: Youngjoon Lee, Taehyun Park, Yunho Lee, Jinu Gong, Joonhyuk Kang

**Updated**: 2025-01-30T15:14:55Z

**Summary**: Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four potential vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these potential risks, we propose a human-AI collaborative framework that introduces both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols. Our findings will guide future research and emphasize proactive strategies for emerging military contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.18416v1),  [pdf](http://arxiv.org/pdf/2501.18416v1)

**Tags**: cs.LG 



### A Data Fusion Model for Meteorological Data using the INLA-SPDE method
**Authors**: Stephen Jun Villejo, Sara Martino, Finn Lindgren, Janine Illian

**Updated**: 2025-01-30T15:12:17Z

**Summary**: This work aims to combine two primary meteorological data sources in the Philippines: data from a sparse network of weather stations and outcomes of a numerical weather prediction model. To this end, we propose a data fusion model which is primarily motivated by the problem of sparsity in the observational data and the use of a numerical prediction model as an additional data source in order to obtain better predictions for the variables of interest. The proposed data fusion model assumes that the different data sources are error-prone realizations of a common latent process. The outcomes from the weather stations follow the classical error model while the outcomes of the numerical weather prediction model involves a constant multiplicative bias parameter and an additive bias which is spatially-structured and time-varying. We use a Bayesian model averaging approach with the integrated nested Laplace approximation (INLA) for doing inference. The proposed data fusion model outperforms the stations-only model and the regression calibration approach, when assessed using leave-group-out cross-validation (LGOCV). We assess the benefits of data fusion and evaluate the accuracy of predictions and parameter estimation through a simulation study. The results show that the proposed data fusion model generally gives better predictions compared to the stations-only approach especially with sparse observational data.

**Link**: [arxiv](http://arxiv.org/abs/2404.08533v2),  [pdf](http://arxiv.org/pdf/2404.08533v2)

**Tags**: stat.AP 



### TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles   with Simulated Students
**Authors**: Hyoungwook Jin, Minju Yoo, Jeongeon Park, Yokyung Lee, Xu Wang, Juho Kim

**Updated**: 2025-01-30T15:11:12Z

**Summary**: Large language models (LLMs) can empower teachers to build pedagogical conversational agents (PCAs) customized for their students. As students have different prior knowledge and motivation levels, teachers must review the adaptivity of their PCAs to diverse students. Existing chatbot reviewing methods (e.g., direct chat and benchmarks) are either manually intensive for multiple iterations or limited to testing only single-turn interactions. We present TeachTune, where teachers can create simulated students and review PCAs by observing automated chats between PCAs and simulated students. Our technical pipeline instructs an LLM-based student to simulate prescribed knowledge levels and traits, helping teachers explore diverse conversation patterns. Our pipeline could produce simulated students whose behaviors correlate highly to their input knowledge and motivation levels within 5% and 10% accuracy gaps. Thirty science teachers designed PCAs in a between-subjects study, and using TeachTune resulted in a lower task load and higher student profile coverage over a baseline.

**Link**: [arxiv](http://arxiv.org/abs/2410.04078v3),  [pdf](http://arxiv.org/pdf/2410.04078v3)

**Tags**: cs.HC 



### Investigating the complex absorbers of Mrk 766 with XMM-Newton
**Authors**: T. Matamoro Zatarain, E. Costantini, A. Juráňová, D. Rogantini

**Updated**: 2025-01-30T14:40:36Z

**Summary**: Aims. We examine the high energy resolution X-ray spectrum of the narrow-line Seyfert 1 galaxy Mrk 766 using 4 observations taken with XMM-Newton in 2005, to investigate the properties of the complex ionised absorber / emitter along the line of sight, as well as absorption by dust intrinsic to the source.   Methods. We make use of the high-energy resolution RGS spectrum to infer the properties of the intervening matter. We also use the spectrum obtained by EPIC-pn and the photometric measurements of OM to obtain the spectral energy distribution of the source, necessary for the photoionisation modelling of the ionised outflow.   Results. The warm absorber in Mrk 766 consists of two phases of photoionisation. In addition to these two warm absorber components with $\log\xi\sim 2.15$ and $\log\xi\sim -0.58$, we find evidence of absorption by a collisionally ionised component ($T\sim51$ eV). We discuss the implication of this additional component in light of theoretical predictions. Moreover, we detect signs of absorption by a dusty medium with $N_\text{dust}\sim 7.29\times 10^{16}$ cm$^{-2}$. Finally the relatively weak emission features in the spectrum seem to be unrelated to the absorbers and probably originated by an out-of sight-line ionised plasma.

**Link**: [arxiv](http://arxiv.org/abs/2501.11562v2),  [pdf](http://arxiv.org/pdf/2501.11562v2)

**Tags**: astro-ph.HE 



### Dual Thinking and Logical Processing -- Are Multi-modal Large Language   Models Closing the Gap with Human Vision ?
**Authors**: Kailas Dayanandan, Nikhil Kumar, Anand Sinha, Brejesh Lall

**Updated**: 2025-01-30T14:37:55Z

**Summary**: The dual thinking framework considers fast, intuitive processing and slower, logical processing. The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ. We introduce an adversarial dataset to provide evidence for the dual thinking framework in human vision, which also aids in studying the qualitative behavior of deep learning models. The evidence underscores the importance of shape in identifying instances in human vision. Our psychophysical studies show the presence of multiple inferences in rapid succession, and analysis of errors shows the early stopping of visual processing can result in missing relevant information. Our study shows that segmentation models lack an understanding of sub-structures, as indicated by errors related to the position and number of sub-components. Additionally, the similarity in errors made by models and intuitive human processing indicates that models only address intuitive thinking in human vision. In contrast, multi-modal LLMs, including open-source models, demonstrate tremendous progress on errors made in intuitive processing. The models have improved performance on images that require logical reasoning and show recognition of sub-components. However, they have not matched the performance improvements made on errors in intuitive processing.

**Link**: [arxiv](http://arxiv.org/abs/2406.06967v2),  [pdf](http://arxiv.org/pdf/2406.06967v2)

**Tags**: cs.CV cs.AI eess.IV 



### ACEBench: Who Wins the Match Point in Tool Learning?
**Authors**: Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, Wu Liu

**Updated**: 2025-01-30T14:36:52Z

**Summary**: Large language models (LLMs) have demonstrated significant potential in decision-making and reasoning, especially when combined with various tools to effectively solve complex problems. However, existing evaluation systems for assessing LLM function calling capabilities have several limitations: (1) limited evaluation scenarios, lacking assessments in real multi-turn dialogue contexts; (2) narrow evaluation dimensions, lacking detailed assessments for fine-grained function calls; (3) relying on LLMs or real API executions for result evaluation, which introduces significant overhead. To address these issues, we propose a comprehensive evaluation system named ACEBench. This system is meticulously designed to encompass a wide spectrum of function calling scenarios. Moreover, it categorizes these scenarios into three primary types according to the evaluation methodology: Normal, Special, and Agent. Normal evaluates function calls in basic scenarios; Special evaluates function calls in scenarios with vague or incomplete instructions; Agent introduces multi-agent interactions to simulate function calling evaluation in real-world multi-turn interactions. We conducted extensive experiments on ACEBench, analyzing various LLMs in-depth and performing a more granular analysis of error causes across different data types.

**Link**: [arxiv](http://arxiv.org/abs/2501.12851v2),  [pdf](http://arxiv.org/pdf/2501.12851v2)

**Tags**: cs.CL 



### RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against   Retrieval Defects
**Authors**: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai

**Updated**: 2025-01-30T14:15:09Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.

**Link**: [arxiv](http://arxiv.org/abs/2501.18365v1),  [pdf](http://arxiv.org/pdf/2501.18365v1)

**Tags**: cs.CL cs.IR 



### Degrees-of-freedom penalized piecewise regression
**Authors**: Stefan Volz, Martin Storath, Andreas Weinmann

**Updated**: 2025-01-30T14:14:23Z

**Summary**: Many popular piecewise regression models rely on minimizing a cost function on the model fit with a linear penalty on the number of segments. However, this penalty does not take into account varying complexities of the model functions on the segments potentially leading to overfitting when models with varying complexities, such as polynomials of different degrees, are used. In this work, we enhance on this approach by instead using a penalty on the sum of the degrees of freedom over all segments, called degrees-of-freedom penalized piecewise regression (DofPPR). We show that the solutions of the resulting minimization problem are unique for almost all input data in a least squares setting. We develop a fast algorithm which does not only compute a minimizer but also determines an optimal hyperparameter -- in the sense of rolling cross validation with the one standard error rule -- exactly. This eliminates manual hyperparameter selection. Our method supports optional user parameters for incorporating domain knowledge. We provide an open-source Python/Rust code for the piecewise polynomial least squares case which can be extended to further models. We demonstrate the practical utility through a simulation study and by applications to real data. A constrained variant of the proposed method gives state-of-the-art results in the Turing benchmark for unsupervised changepoint detection.

**Link**: [arxiv](http://arxiv.org/abs/2312.16512v2),  [pdf](http://arxiv.org/pdf/2312.16512v2)

**Tags**: stat.ME cs.NA math.NA 65K05 (Primary) 90C26 (Secondary) 62G05 G.1.2; G.1.6 



### State Stream Transformer (SST) : Emergent Metacognitive Behaviours   Through Latent State Persistence
**Authors**: Thea Aviss

**Updated**: 2025-01-30T14:03:36Z

**Summary**: We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the 'state stream' that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.18356v1),  [pdf](http://arxiv.org/pdf/2501.18356v1)

**Tags**: cs.LG cs.AI cs.CL 



### New binary black hole mergers in the LIGO-Virgo O3b data
**Authors**: Ajit Kumar Mehta, Seth Olsen, Digvijay Wadekar, Javier Roulet, Tejaswi Venumadhav, Jonathan Mushkin, Barak Zackay, Matias Zaldarriaga

**Updated**: 2025-01-30T14:00:22Z

**Summary**: We report the detection of 6 new candidate binary black hole (BBH) merger signals in the publicly released data from the second half of the third observing run (O3b) of advanced LIGO and advanced Virgo. The LIGO-Virgo-KAGRA (LVK) collaboration reported 35 compact binary coalescences (CBCs) in their analysis of the O3b data [1], with 30 BBH mergers having coincidence in the Hanford and Livingston detectors. We confirm 17 of these for a total of 23 detections in our analysis of the Hanford-Livingston coincident O3b data. We identify candidates using a search pipeline employing aligned-spin quadrupole-only waveforms. Our pipeline is similar to the one used in our O3a coincident analysis [2], except for a few improvements in the veto procedure and the ranking statistic, and we continue to use an astrophysical probability of one half as our detection threshold, following the approach of the LVK catalogs. Most of the new candidates reported in this work are placed in the upper and lower-mass gap of the black hole (BH) mass distribution. We also identify a possible neutron star-black hole (NSBH) merger. We expect these events to help inform the black hole mass and spin distributions inferred in a full population analysis.

**Link**: [arxiv](http://arxiv.org/abs/2311.06061v3),  [pdf](http://arxiv.org/pdf/2311.06061v3)

**Tags**: gr-qc astro-ph.HE astro-ph.IM 



### Unfaithful Probability Distributions in Binary Triple of Causality   Directed Acyclic Graph
**Authors**: Jingwei Liu

**Updated**: 2025-01-30T13:34:48Z

**Summary**: Faithfulness is the foundation of probability distribution and graph in causal discovery and causal inference. In this paper, several unfaithful probability distribution examples are constructed in three--vertices binary causality directed acyclic graph (DAG) structure, which are not faithful to causal DAGs described in J.M.,Robins,et al. Uniform consistency in causal inference. Biometrika (2003),90(3): 491--515. And the general unfaithful probability distribution with multiple independence and conditional independence in binary triple causal DAG is given.

**Link**: [arxiv](http://arxiv.org/abs/2501.18337v1),  [pdf](http://arxiv.org/pdf/2501.18337v1)

**Tags**: stat.ML cs.AI cs.LG 



### Locret: Enhancing Eviction in Long-Context LLM Inference with Trained   Retaining Heads on Consumer-Grade Devices
**Authors**: Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu

**Updated**: 2025-01-30T13:07:37Z

**Summary**: Scaling the input context length of a large language model (LLM) incurs a significant increase in computation cost and memory footprint to maintain the attention key-value (KV) cache. Existing KV cache compression methods suffer from inefficient compression strategies and limited memory reduction effects, making it difficult for LLMs to conduct long-context inference on consumer-grade devices, especially when inferring long-context stream input. Such obstacles prevent consumer-grade devices from supporting more complex applications, creating challenges for the democratization of LLMs. To overcome this, we propose Locret, the first framework to create an eviction policy compatible with chunked prefill. By evaluating the causal importance of KV cache units by learnable retaining heads, Locret enables precise eviction of cache units, facilitating efficient long-context inference. In our extensive empirical studies, Locret outperforms the recent popular and competitive approaches in terms of memory efficiency and generation quality -- Locret achieves up to 20x of KV cache compression ratio within less than 10% performance loss. Furthermore, Locret achieves 128K+ long-context inference on a single NVIDIA 4090 GPU without compromising generation quality and only costs <1 GPU hour of additional training.

**Link**: [arxiv](http://arxiv.org/abs/2410.01805v2),  [pdf](http://arxiv.org/pdf/2410.01805v2)

**Tags**: cs.CL 



### Leveraging LLM Agents for Automated Optimization Modeling for SASP   Problems: A Graph-RAG based Approach
**Authors**: Tianpeng Pan, Wenqiang Pu, Licheng Zhao, Rui Zhou

**Updated**: 2025-01-30T13:00:15Z

**Summary**: Automated optimization modeling (AOM) has evoked considerable interest with the rapid evolution of large language models (LLMs). Existing approaches predominantly rely on prompt engineering, utilizing meticulously designed expert response chains or structured guidance. However, prompt-based techniques have failed to perform well in the sensor array signal processing (SASP) area due the lack of specific domain knowledge. To address this issue, we propose an automated modeling approach based on retrieval-augmented generation (RAG) technique, which consists of two principal components: a multi-agent (MA) structure and a graph-based RAG (Graph-RAG) process. The MA structure is tailored for the architectural AOM process, with each agent being designed based on principles of human modeling procedure. The Graph-RAG process serves to match user query with specific SASP modeling knowledge, thereby enhancing the modeling result. Results on ten classical signal processing problems demonstrate that the proposed approach (termed as MAG-RAG) outperforms several AOM benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2501.18320v1),  [pdf](http://arxiv.org/pdf/2501.18320v1)

**Tags**: cs.AI eess.SP 



### Preregistration does not improve the transparent evaluation of severity   in Popper's philosophy of science or when deviations are allowed
**Authors**: Mark Rubin

**Updated**: 2025-01-30T12:39:52Z

**Summary**: One justification for preregistering research hypotheses, methods, and analyses is that it improves the transparent evaluation of the severity of hypothesis tests. In this article, I consider two cases in which preregistration does not improve this evaluation. First, I argue that, although preregistration can facilitate the transparent evaluation of severity in Mayo's error statistical philosophy of science, it does not facilitate this evaluation in Popper's theory-centric approach. To illustrate, I show that associated concerns about Type I error rate inflation are only relevant in the error statistical approach and not in a theory-centric approach. Second, I argue that a preregistered test procedure that allows deviations in its implementation does not provide a more transparent evaluation of Mayoian severity than a non-preregistered procedure. In particular, I argue that sample-based validity-enhancing deviations cause an unknown inflation of the test procedure's Type I error rate and, consequently, an unknown reduction in its capability to license inferences severely. I conclude that preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed.

**Link**: [arxiv](http://arxiv.org/abs/2408.12347v7),  [pdf](http://arxiv.org/pdf/2408.12347v7)

**Tags**: stat.ME 



### Efficient Neural Theorem Proving via Fine-grained Proof Structure   Analysis
**Authors**: Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao

**Updated**: 2025-01-30T12:37:06Z

**Summary**: The synergy between deep learning models and traditional automation tools plays a pivotal role in developing robust neural theorem provers (NTPs). However, for proof synthesis with LLMs, previous work applies automation tools either only when the model explicitly calls the method, or only at a single granularity level, failing to fully exploit the power of built-in tactics and off-the-shelf automated theorem provers. In this work, we propose ProofAug, a novel theorem proving method that enjoys superior sample efficiency through equipping proof-generation LLMs with automation methods in different granularities via fine-grained structure analysis of model-generated proof proposals. Furthermore, ProofAug serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance. The superiority of our method is validated on the miniF2F-test benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant. Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version), setting a new SOTA across all proof languages with a total sample budget of only 2100. Our code is available at https://github.com/haoxiongliu/ProofAug.

**Link**: [arxiv](http://arxiv.org/abs/2501.18310v1),  [pdf](http://arxiv.org/pdf/2501.18310v1)

**Tags**: cs.LG cs.AI 



### Automated Test-Case Generation for REST APIs Using Model Inference   Search Heuristic
**Authors**: Clinton Cao, Annibale Panichella, Sicco Verwer

**Updated**: 2025-01-30T12:35:06Z

**Summary**: The rising popularity of the microservice architectural style has led to a growing demand for automated testing approaches tailored to these systems. EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to automatically generate test cases for microservices' REST APIs. One limitation of these EAs is the use of unit-level search heuristics, such as branch distances, which focus on fine-grained code coverage and may not effectively capture the complex, interconnected behaviors characteristic of system-level testing. To address this limitation, we propose a new search heuristic (MISH) that uses real-time automaton learning to guide the test case generation process. We capture the sequential call patterns exhibited by a test case by learning an automaton from the stream of log events outputted by different microservices within the same system. Therefore, MISH learns a representation of the systemwide behavior, allowing us to define the fitness of a test case based on the path it traverses within the inferred automaton. We empirically evaluate MISH's effectiveness on six real-world benchmark microservice applications and compare it against a state-of-the-art technique, MOSA, for testing REST APIs. Our evaluation shows promising results for using MISH to guide the automated test case generation within EvoMaster.

**Link**: [arxiv](http://arxiv.org/abs/2412.03420v2),  [pdf](http://arxiv.org/pdf/2412.03420v2)

**Tags**: cs.SE cs.AI 



### Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual   perception beyond human capabilities
**Authors**: Shengbo Wang, Jingwen Zhao, Tongming Pu, Liangbing Zhao, Xiaoyu Guo, Yue Cheng, Cong Li, Weihao Ma, Chenyu Tang, Zhenyu Xu, Ningli Wang, Luigi Occhipinti, Arokia Nathan, Ravinder Dahiya, Huaqiang Wu, Li Tao, Shuo Gao

**Updated**: 2025-01-30T12:20:12Z

**Summary**: Optical flow, inspired by the mechanisms of biological visual systems, calculates spatial motion vectors within visual scenes that are necessary for enabling robotics to excel in complex and dynamic working environments. However, current optical flow algorithms, despite human-competitive task performance on benchmark datasets, remain constrained by unacceptable time delays (~0.6 seconds per inference, 4X human processing speed) in practical deployment. Here, we introduce a neuromorphic optical flow approach that addresses delay bottlenecks by encoding temporal information directly in a synaptic transistor array to assist spatial motion analysis. Compared to conventional spatial-only optical flow methods, our spatiotemporal neuromorphic optical flow offers the spatial-temporal consistency of motion information, rapidly identifying regions of interest in as little as 1-2 ms using the temporal motion cues derived from the embedded temporal information in the two-dimensional floating gate synaptic transistors. Thus, the visual input can be selectively filtered to achieve faster velocity calculations and various task execution. At the hardware level, due to the atomically sharp interfaces between distinct functional layers in two-dimensional van der Waals heterostructures, the synaptic transistor offers high-frequency response (~100 {\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000 cycles), enabling robust visual processing. In software benchmarks, our system outperforms state-of-the-art algorithms with a 400% speedup, frequently surpassing human-level performance while maintaining or enhancing accuracy by utilizing the temporal priors provided by the embedded temporal information.

**Link**: [arxiv](http://arxiv.org/abs/2409.15345v2),  [pdf](http://arxiv.org/pdf/2409.15345v2)

**Tags**: cs.CV cs.RO 



### Efficient Learning With Sine-Activated Low-rank Matrices
**Authors**: Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey

**Updated**: 2025-01-30T12:17:43Z

**Summary**: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model performance. Our method proves to be a plug in enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.

**Link**: [arxiv](http://arxiv.org/abs/2403.19243v2),  [pdf](http://arxiv.org/pdf/2403.19243v2)

**Tags**: cs.LG cs.CV cs.NE 



### Mining for Species, Locations, Habitats, and Ecosystems from Scientific   Papers in Invasion Biology: A Large-Scale Exploratory Study with Large   Language Models
**Authors**: Jennifer D'Souza, Zachary Laubach, Tarek Al Mustafa, Sina Zarrieß, Robert Frühstückl, Phyllis Illari

**Updated**: 2025-01-30T11:55:44Z

**Summary**: This paper presents an exploratory study that harnesses the capabilities of large language models (LLMs) to mine key ecological entities from invasion biology literature. Specifically, we focus on extracting species names, their locations, associated habitats, and ecosystems, information that is critical for understanding species spread, predicting future invasions, and informing conservation efforts. Traditional text mining approaches often struggle with the complexity of ecological terminology and the subtle linguistic patterns found in these texts. By applying general-purpose LLMs without domain-specific fine-tuning, we uncover both the promise and limitations of using these models for ecological entity extraction. In doing so, this study lays the groundwork for more advanced, automated knowledge extraction tools that can aid researchers and practitioners in understanding and managing biological invasions.

**Link**: [arxiv](http://arxiv.org/abs/2501.18287v1),  [pdf](http://arxiv.org/pdf/2501.18287v1)

**Tags**: cs.CL cs.AI cs.DL 



### Leveraging Sparsity for Sample-Efficient Preference Learning: A   Theoretical Perspective
**Authors**: Yunzhen Yao, Lie He, Michael Gastpar

**Updated**: 2025-01-30T11:41:13Z

**Summary**: This paper considers the sample-efficiency of preference learning, which models and predicts human choices based on comparative judgments. The minimax optimal estimation rate $\Theta(d/n)$ in traditional estimation theory requires that the number of samples $n$ scales linearly with the dimensionality of the feature space $d$. However, the high dimensionality of the feature space and the high cost of collecting human-annotated data challenge the efficiency of traditional estimation methods. To remedy this, we leverage sparsity in the preference model and establish sharp estimation rates. We show that under the sparse random utility model, where the parameter of the reward function is $k$-sparse, the minimax optimal rate can be reduced to $\Theta(k/n \log(d/k))$. Furthermore, we analyze the $\ell_{1}$-regularized estimator and show that it achieves near-optimal rate under mild assumptions on the Gram matrix. Experiments on synthetic data and LLM alignment data validate our theoretical findings, showing that sparsity-aware methods significantly reduce sample complexity and improve prediction accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.18282v1),  [pdf](http://arxiv.org/pdf/2501.18282v1)

**Tags**: cs.LG 



### Jailbreaking LLMs' Safeguard with Universal Magic Words for Text   Embedding Models
**Authors**: Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang

**Updated**: 2025-01-30T11:37:40Z

**Summary**: The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.

**Link**: [arxiv](http://arxiv.org/abs/2501.18280v1),  [pdf](http://arxiv.org/pdf/2501.18280v1)

**Tags**: cs.CL cs.AI cs.LG cs.NE 



### Here Comes The AI Worm: Unleashing Zero-click Worms that Target   GenAI-Powered Applications
**Authors**: Stav Cohen, Ron Bitton, Ben Nassi

**Updated**: 2025-01-30T11:11:37Z

**Summary**: In this paper, we show that when the communication between GenAI-powered applications relies on RAG-based inference, an attacker can initiate a computer worm-like chain reaction that we call Morris-II. This is done by crafting an adversarial self-replicating prompt that triggers a cascade of indirect prompt injections within the ecosystem and forces each affected application to perform malicious actions and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of confidential user data extraction within a GenAI ecosystem of GenAI-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embedding algorithm employed, and the number of hops in the propagation. Finally, we introduce the Virtual Donkey, a guardrail intended to detect and prevent the propagation of Morris-II with minimal latency, high accuracy, and a low false-positive rate. We evaluate the guardrail's performance and show that it yields a perfect true-positive rate of 1.0 with a false-positive rate of 0.015, and is robust against out-of-distribution worms, consisting of unseen jailbreaking commands, a different email dataset, and various worm usecases.

**Link**: [arxiv](http://arxiv.org/abs/2403.02817v2),  [pdf](http://arxiv.org/pdf/2403.02817v2)

**Tags**: cs.CR 



### Collecting Cost-Effective, High-Quality Truthfulness Assessments with   LLM Summarized Evidence
**Authors**: Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro

**Updated**: 2025-01-30T11:04:14Z

**Summary**: With the degradation of guardrails against mis- and disinformation online, it is more critical than ever to be able to effectively combat it. In this paper, we explore the efficiency and effectiveness of using crowd-sourced truthfulness assessments based on condensed, large language model (LLM) generated summaries of online sources. We compare the use of generated summaries to the use of original web pages in an A/B testing setting, where we employ a large and diverse pool of crowd-workers to perform the truthfulness assessment. We evaluate the quality of assessments, the efficiency with which assessments are performed, and the behavior and engagement of participants. Our results demonstrate that the Summary modality, which relies on summarized evidence, offers no significant change in assessment accuracy over the Standard modality, while significantly increasing the speed with which assessments are performed. Workers using summarized evidence produce a significantly higher number of assessments in the same time frame, reducing the cost needed to acquire truthfulness assessments. Additionally, the Summary modality maximizes both the inter-annotator agreements as well as the reliance on and perceived usefulness of evidence, demonstrating the utility of summarized evidence without sacrificing the quality of assessments.

**Link**: [arxiv](http://arxiv.org/abs/2501.18265v1),  [pdf](http://arxiv.org/pdf/2501.18265v1)

**Tags**: cs.IR cs.CL cs.HC 



### Statistical multi-metric evaluation and visualization of LLM system   predictive performance
**Authors**: Samuel Ackerman, Eitan Farchi, Orna Raz, Assaf Toledo

**Updated**: 2025-01-30T10:21:10Z

**Summary**: The evaluation of generative or discriminative large language model (LLM)-based systems is often a complex multi-dimensional problem. Typically, a set of system configuration alternatives are evaluated on one or more benchmark datasets, each with one or more evaluation metrics, which may differ between datasets. We often want to evaluate -- with a statistical measure of significance -- whether systems perform differently either on a given dataset according to a single metric, on aggregate across metrics on a dataset, or across datasets. Such evaluations can be done to support decision-making, such as deciding whether a particular system component change (e.g., choice of LLM or hyperparameter values) significantly improves performance over the current system configuration, or, more generally, whether a fixed set of system configurations (e.g., a leaderboard list) have significantly different performances according to metrics of interest. We present a framework implementation that automatically performs the correct statistical tests, properly aggregates the statistical results across metrics and datasets (a nontrivial task), and can visualize the results. The framework is demonstrated on the multi-lingual code generation benchmark CrossCodeEval, for several state-of-the-art LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.18243v1),  [pdf](http://arxiv.org/pdf/2501.18243v1)

**Tags**: stat.AP cs.CL cs.LG 



### Assessing the Capability of YOLO- and Transformer-based Object Detectors   for Real-time Weed Detection
**Authors**: Alicia Allmendinger, Ahmet Oğuz Saltık, Gerassimos G. Peteinatos, Anthony Stein, Roland Gerhards

**Updated**: 2025-01-30T09:56:55Z

**Summary**: Spot spraying represents an efficient and sustainable method for reducing the amount of pesticides, particularly herbicides, used in agricultural fields. To achieve this, it is of utmost importance to reliably differentiate between crops and weeds, and even between individual weed species in situ and under real-time conditions. To assess suitability for real-time application, different object detection models that are currently state-of-the-art are compared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are trained and evaluated with images from a real field situation. The images are separated into two distinct datasets: In the initial data set, each species of plants is trained individually; in the subsequent dataset, a distinction is made between monocotyledonous weeds, dicotyledonous weeds, and three chosen crops. The results demonstrate that while all models perform equally well in the metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e, stand out in terms of their strong recall scores (66.58 % and 72.36 %), as well as mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset 2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with reaching 82.44 \% on dataset 1 and 81.46 % in dataset 2, making them particularly suitable for scenarios where minimizing false positives is critical. In particular, the smallest variants of the YOLO models (YOLOv8n, YOLOv9t, and YOLOv10n) achieve substantially faster inference times down to 7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one frame, while maintaining competitive accuracy, highlighting their potential for deployment in resource-constrained embedded computing devices as typically used in productive setups.

**Link**: [arxiv](http://arxiv.org/abs/2501.17387v2),  [pdf](http://arxiv.org/pdf/2501.17387v2)

**Tags**: cs.CV 



### GPD: Guided Polynomial Diffusion for Motion Planning
**Authors**: Ajit Srikanth, Parth Mahanjan, Kallol Saha, Vishal Mandadi, Pranjal Paul, Pawan Wadhwani, Brojeshwar Bhowmick, Arun Singh, Madhava Krishna

**Updated**: 2025-01-30T09:35:17Z

**Summary**: Diffusion-based motion planners are becoming popular due to their well-established performance improvements, stemming from sample diversity and the ease of incorporating new constraints directly during inference. However, a primary limitation of the diffusion process is the requirement for a substantial number of denoising steps, especially when the denoising process is coupled with gradient-based guidance. In this paper, we introduce, diffusion in the parametric space of trajectories, where the parameters are represented as Bernstein coefficients. We show that this representation greatly improves the effectiveness of the cost function guidance and the inference speed. We also introduce a novel stitching algorithm that leverages the diversity in diffusion-generated trajectories to produce collision-free trajectories with just a single cost function-guided model. We demonstrate that our approaches outperform current SOTA diffusion-based motion planners for manipulators and provide an ablation study on key components.

**Link**: [arxiv](http://arxiv.org/abs/2501.18229v1),  [pdf](http://arxiv.org/pdf/2501.18229v1)

**Tags**: cs.RO 



### Control LLM: Controlled Evolution for Intelligence Retention in LLM
**Authors**: Haichao Wei, Yunxiang Ren, Zhoutong Fu, Aman Lunia, Yi-Lin Chen, Alice Leung, Ya Xu

**Updated**: 2025-01-30T09:17:22Z

**Summary**: Large Language Models (LLMs) demand significant computational resources, making it essential to enhance their capabilities without retraining from scratch. A key challenge in this domain is \textit{catastrophic forgetting} (CF), which hampers performance during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT). We propose \textbf{Control LLM}, a novel approach that leverages parallel pre-trained and expanded transformer blocks, aligning their hidden-states through interpolation strategies This method effectively preserves performance on existing tasks while seamlessly integrating new knowledge.   Extensive experiments demonstrate the effectiveness of Control LLM in both CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in mathematical reasoning ($+14.4\%$ on Math-Hard) and coding performance ($+10\%$ on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\%$ on C-Eval, $+6.8\%$ on CMMLU, and $+30.2\%$ on CMMLU-0shot-CoT). It surpasses existing methods and achieves SOTA among open-source models tuned from the same base model, using substantially less data and compute. Crucially, these gains are realized while preserving strong original capabilities, with minimal degradation ($<4.3\% \text{on MMLU}$) compared to $>35\%$ in open-source Math and Coding models. This approach has been successfully deployed in LinkedIn's GenAI-powered job seeker and Ads unit products.   To support further research, we release the training and evaluation code (https://github.com/linkedin/ControlLLM) along with models trained on public datasets (https://huggingface.co/ControlLLM) to the community.

**Link**: [arxiv](http://arxiv.org/abs/2501.10979v2),  [pdf](http://arxiv.org/pdf/2501.10979v2)

**Tags**: cs.LG 



### Compute Optimal Inference and Provable Amortisation Gap in Sparse   Autoencoders
**Authors**: Charles O'Neill, Alim Gumran, David Klindt

**Updated**: 2025-01-30T09:15:26Z

**Summary**: A recent line of work has shown promise in using sparse autoencoders (SAEs) to uncover interpretable features in neural network representations. However, the simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform accurate sparse inference. Using compressed sensing theory, we prove that an SAE encoder is inherently insufficient for accurate sparse inference, even in solvable cases. We then decouple encoding and decoding processes to empirically explore conditions where more sophisticated sparse inference methods outperform traditional SAE encoders. Our results reveal substantial performance gains with minimal compute increases in correct inference of sparse codes. We demonstrate this generalises to SAEs applied to large language models, where more expressive encoders achieve greater interpretability. This work opens new avenues for understanding neural network representations and analysing large language model activations.

**Link**: [arxiv](http://arxiv.org/abs/2411.13117v2),  [pdf](http://arxiv.org/pdf/2411.13117v2)

**Tags**: cs.LG 



### Safety challenges of AI in medicine in the era of large language models
**Authors**: Xiaoye Wang, Nicole Xi Zhang, Hongyu He, Trang Nguyen, Kun-Hsing Yu, Hao Deng, Cynthia Brandt, Danielle S. Bitterman, Ling Pan, Ching-Yu Cheng, James Zou, Dianbo Liu

**Updated**: 2025-01-30T08:55:23Z

**Summary**: Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs), have unlocked significant potential to enhance the quality and efficiency of medical care. By introducing a novel way to interact with AI and data through natural language, LLMs offer new opportunities for medical practitioners, patients, and researchers. However, as AI and LLMs become more powerful and especially achieve superhuman performance in some medical tasks, public concerns over their safety have intensified. These concerns about AI safety have emerged as the most significant obstacles to the adoption of AI in medicine. In response, this review examines emerging risks in AI utilization during the LLM era. First, we explore LLM-specific safety challenges from functional and communication perspectives, addressing issues across data collection, model training, and real-world application. We then consider inherent safety problems shared by all AI systems, along with additional complications introduced by LLMs. Last, we discussed how safety issues of using AI in clinical practice and healthcare system operation would undermine trust among patient, clinicians and the public, and how to build confidence in these systems. By emphasizing the development of safe AI, we believe these technologies can be more rapidly and reliably integrated into everyday medical practice to benefit both patients and clinicians.

**Link**: [arxiv](http://arxiv.org/abs/2409.18968v2),  [pdf](http://arxiv.org/pdf/2409.18968v2)

**Tags**: cs.CY cs.AI cs.LG 



### LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically   Generating Metaheuristics
**Authors**: Niki van Stein, Thomas Bäck

**Updated**: 2025-01-30T08:54:54Z

**Summary**: Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets. This paper introduces a novel Large Language Model Evolutionary Algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms. Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates and selects algorithms based on performance metrics and feedback from runtime evaluations. This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise. We show how this framework can be used to generate novel black-box metaheuristic optimization algorithms automatically. LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (Covariance Matrix Adaptation Evolution Strategy and Differential Evolution) on the five dimensional black box optimization benchmark (BBOB). The algorithms also show competitive performance on the 10- and 20-dimensional instances of the test functions, although they have not seen such instances during the automated generation process. The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2405.20132v4),  [pdf](http://arxiv.org/pdf/2405.20132v4)

**Tags**: cs.NE cs.AI 



### Contextually Structured Token Dependency Encoding for Large Language   Models
**Authors**: James Blades, Frederick Somerfield, William Langley, Susan Everingham, Maurice Witherington

**Updated**: 2025-01-30T08:51:48Z

**Summary**: Token representation strategies within large-scale neural architectures often rely on contextually refined embeddings, yet conventional approaches seldom encode structured relationships explicitly within token interactions. Self-attention mechanisms effectively capture dynamic contextual dependencies, but their reliance on learned weight distributions limits the preservation of long-range hierarchical structures in generated sequences. Dependency-aware token encoding introduces a structured approach to embedding initialization, ensuring that relational constraints are embedded within token representations rather than inferred solely through attention dynamics. The proposed encoding mechanism refines token interactions through dependency-weighted attention computations, ensuring that syntactic and semantic dependencies are retained across multiple processing layers. Empirical evaluations indicate reductions in perplexity across diverse linguistic benchmarks, suggesting improvements in contextual coherence and predictive consistency in autoregressive text generation. Computational efficiency assessments reveal a moderate increase in memory consumption and training time, attributed to additional matrix computations within the encoding module, yet scalability remains feasible within conventional transformer architectures. Structured encoding enhances lexical variation and dependency retention, reinforcing linguistic coherence without requiring external syntactic annotations or auxiliary training objectives. Statistical comparisons highlight improvements in dependency alignment, particularly in longer sequences where conventional self-attention models exhibit degradation in hierarchical consistency. Sentence length distributions indicate a reduction in abrupt phrase transitions, further supporting the hypothesis that explicit dependency encoding facilitates more structured phrase generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.18205v1),  [pdf](http://arxiv.org/pdf/2501.18205v1)

**Tags**: cs.CL 



### On Scaling Neurosymbolic Programming through Guided Logical Inference
**Authors**: Thomas Jean-Michel Valentin, Luisa Sophie Werner, Pierre Genevès, Nabil Layaïda

**Updated**: 2025-01-30T08:49:25Z

**Summary**: Probabilistic neurosymbolic learning seeks to integrate neural networks with symbolic programming. Many state-of-the-art systems rely on a reduction to the Probabilistic Weighted Model Counting Problem (PWMC), which requires computing a Boolean formula called the logical provenance.However, PWMC is \\#P-hard, and the number of clauses in the logical provenance formula can grow exponentially, creating a major bottleneck that significantly limits the applicability of PNL solutions in practice.We propose a new approach centered around an exact algorithm DPNL, that enables bypassing the computation of the logical provenance.The DPNL approach relies on the principles of an oracle and a recursive DPLL-like decomposition in order to guide and speed up logical inference.Furthermore, we show that this approach can be adapted for approximate reasoning with $\epsilon$ or $(\epsilon, \delta)$ guarantees, called ApproxDPNL.Experiments show significant performance gains.DPNL enables scaling exact inference further, resulting in more accurate models.Further, ApproxDPNL shows potential for advancing the scalability of neurosymbolic programming by incorporating approximations even further, while simultaneously ensuring guarantees for the reasoning process.

**Link**: [arxiv](http://arxiv.org/abs/2501.18202v1),  [pdf](http://arxiv.org/pdf/2501.18202v1)

**Tags**: cs.AI 



### Exploring the Role of Reasoning Structures for Constructing Proofs in   Multi-Step Natural Language Reasoning with Large Language Models
**Authors**: Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu

**Updated**: 2025-01-30T08:06:33Z

**Summary**: When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning and for improving models' explainability. This paper is centred around a focused study: whether the current state-of-the-art generalist LLMs can leverage the structures in a few examples to better construct the proof structures with \textit{in-context learning}. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that they both help improve performance. A detailed analysis is provided to help understand the results.

**Link**: [arxiv](http://arxiv.org/abs/2410.08436v2),  [pdf](http://arxiv.org/pdf/2410.08436v2)

**Tags**: cs.CL cs.AI 



### Deception in LLMs: Self-Preservation and Autonomous Goals in Large   Language Models
**Authors**: Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl

**Updated**: 2025-01-30T08:00:14Z

**Summary**: Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs' use as agents that can interact with tools and adapt their responses based on new information.   Our study examines DeepSeek R1, a model trained to output reasoning tokens similar to OpenAI's o1. Testing revealed concerning behaviors: the model exhibited deceptive tendencies and demonstrated self-preservation instincts, including attempts of self-replication, despite these traits not being explicitly programmed (or prompted). These findings raise concerns about LLMs potentially masking their true objectives behind a facade of alignment. When integrating such LLMs into robotic systems, the risks become tangible - a physically embodied AI exhibiting deceptive behaviors and self-preservation instincts could pursue its hidden objectives through real-world actions. This highlights the critical need for robust goal specification and safety frameworks before any physical implementation.

**Link**: [arxiv](http://arxiv.org/abs/2501.16513v2),  [pdf](http://arxiv.org/pdf/2501.16513v2)

**Tags**: cs.CL 



### NoTeeline: Supporting Real-Time, Personalized Notetaking with   LLM-Enhanced Micronotes
**Authors**: Faria Huq, Abdus Samee, David Chuan-en Lin, Xiaodi Alice Tang, Jeffrey P. Bigham

**Updated**: 2025-01-30T07:23:32Z

**Summary**: Taking notes quickly while effectively capturing key information can be challenging, especially when watching videos that present simultaneous visual and auditory streams. Manually taken notes often miss crucial details due to the fast-paced nature of the content, while automatically generated notes fail to incorporate user preferences and discourage active engagement with the content. To address this, we propose an interactive system, NoTeeline, for supporting real-time, personalized notetaking. Given micronotes, NoTeeline automatically expands them into full-fledged notes using a Large Language Model (LLM). The generated notes build on the content of micronotes by adding relevant details while maintaining consistency with the user's writing style. In a within-subjects study (n=12), we found that NoTeeline creates high-quality notes that capture the essence of participant micronotes with 93.2% factual correctness and accurately align with participant writing style (8.33% improvement). Using NoTeeline, participants could capture their desired notes with significantly reduced mental effort, writing 47.0% less text and completing their notes in 43.9% less time compared to a manual notetaking baseline. Our results suggest that NoTeeline enables users to integrate LLM assistance in a familiar notetaking workflow while ensuring consistency with their preferences - providing an example of how to address broader challenges in designing AI-assisted tools to augment human capabilities without compromising user autonomy and personalization.

**Link**: [arxiv](http://arxiv.org/abs/2409.16493v3),  [pdf](http://arxiv.org/pdf/2409.16493v3)

**Tags**: cs.HC 



### Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds   Decoding
**Authors**: Yun-Shiuan Chuang, Nikunj Harlalka, Sameer Narendran, Alexander Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers

**Updated**: 2025-01-30T07:15:04Z

**Summary**: Guesstimation, the task of making approximate quantity estimates, is a common real-world challenge. However, it has been largely overlooked in large language models (LLMs) and vision language models (VLMs) research. We introduce a novel guesstimation dataset, MARBLES. This dataset requires one to estimate how many items (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup), both with and without accompanying images. Inspired by the social science concept of the ``Wisdom of Crowds'' (WOC) - taking the median from estimates from a crowd), which has proven effective in guesstimation, we propose ``WOC decoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well on guesstimation, suggesting that they possess some level of a "world model" necessary for guesstimation. Moreover, similar to human performance, the WOC decoding method improves LLM/VLM guesstimation accuracy. Furthermore, the inclusion of images in the multimodal condition enhances model performance. These results highlight the value of WOC decoding strategy for LLMs/VLMs and position guesstimation as a probe for evaluating LLMs/VLMs' world model. As LLMs' world model is a fundamental prerequisite for many real-world tasks, e.g., human-AI teaming, our findings have broad implications for the AI community.

**Link**: [arxiv](http://arxiv.org/abs/2501.17310v2),  [pdf](http://arxiv.org/pdf/2501.17310v2)

**Tags**: cs.AI cs.HC 



### HPSCAN: Human Perception-Based Scattered Data Clustering
**Authors**: Sebastian Hartwig, Christian van Onzenoodt, Dominik Engel, Pedro Hermosilla, Timo Ropinski

**Updated**: 2025-01-30T07:12:45Z

**Summary**: Cluster separation is a task typically tackled by widely used clustering techniques, such as k-means or DBSCAN. However, these algorithms are based on non-perceptual metrics, and our experiments demonstrate that their output does not reflect human cluster perception. To bridge the gap between human cluster perception and machine-computed clusters, we propose HPSCAN, a learning strategy that operates directly on scattered data. To learn perceptual cluster separation on such data, we crowdsourced the labeling of 7,320 bivariate (scatterplot) datasets to 384 human participants. We train our HPSCAN model on these human-annotated data. Instead of rendering these data as scatterplot images, we used their x and y point coordinates as input to a modified PointNet++ architecture, enabling direct inference on point clouds. In this work, we provide details on how we collected our dataset, report statistics of the resulting annotations, and investigate the perceptual agreement of cluster separation for real-world data. We also report the training and evaluation protocol for HPSCAN and introduce a novel metric, that measures the accuracy between a clustering technique and a group of human annotators. We explore predicting point-wise human agreement to detect ambiguities. Finally, we compare our approach to ten established clustering techniques and demonstrate that HPSCAN is capable of generalizing to unseen and out-of-scope data.

**Link**: [arxiv](http://arxiv.org/abs/2304.14185v4),  [pdf](http://arxiv.org/pdf/2304.14185v4)

**Tags**: cs.LG cs.HC 



### EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small   Language Models for Biomedical Question Answering
**Authors**: Chang Zong, Jian Wan, Siliang Tang, Lei Zhang

**Updated**: 2025-01-30T07:11:06Z

**Summary**: When addressing professional questions in the biomedical domain, humans typically acquire multiple pieces of information as evidence and engage in multifaceted evidence analysis to provide high-quality answers. Current LLM-based answer generation methods lack a detailed definition and learning process for evidence analysis, leading to the risk of error propagation and hallucinations while using evidence. Although increasing the parameter size of LLMs can alleviate these issues, it also presents challenges in model training and deployment with limited resources. In this study, we propose EvidenceMap, which aims to enable a tiny pre-trained language model to explicitly learn multiple aspects of biomedical evidence, including supportive evaluation, logical correlation and content summarization, thereby latently guiding a small generative model (around 3B parameters) to provide textual responses. Experimental results demonstrate that our method, fine-tuning a language model with 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in reference-based quality and accuracy, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.12746v3),  [pdf](http://arxiv.org/pdf/2501.12746v3)

**Tags**: cs.CL cs.AI 68T50 



### LMFusion: Adapting Pretrained Language Models for Multimodal Generation
**Authors**: Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu

**Updated**: 2025-01-30T07:08:45Z

**Summary**: We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.

**Link**: [arxiv](http://arxiv.org/abs/2412.15188v3),  [pdf](http://arxiv.org/pdf/2412.15188v3)

**Tags**: cs.CL cs.AI cs.CV cs.LG 



### E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation   with Language Models
**Authors**: Chan Kim, Keonwoo Kim, Mintaek Oh, Hanbi Baek, Jiyang Lee, Donghwi Jung, Soojin Woo, Younkyung Woo, John Tucker, Roya Firoozi, Seung-Woo Seo, Mac Schwager, Seong-Woo Kim

**Updated**: 2025-01-30T06:46:59Z

**Summary**: Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation. However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans. Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios. To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences. Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches. Code and supplementary materials are available at https://e2map.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2409.10027v3),  [pdf](http://arxiv.org/pdf/2409.10027v3)

**Tags**: cs.RO cs.AI 



### LemmaHead: RAG Assisted Proof Generation Using Large Language Models
**Authors**: Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang

**Updated**: 2025-01-30T06:10:23Z

**Summary**: Developing the logic necessary to solve mathematical problems or write mathematical proofs is one of the more difficult objectives for large language models (LLMS). Currently, the most popular methods in literature consists of fine-tuning the model on written mathematical content such as academic publications and textbooks, so that the model can learn to emulate the style of mathematical writing. In this project, we explore the effectiveness of using retrieval augmented generation (RAG) to address gaps in the mathematical reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements queries to the model with relevant mathematical context, with particular focus on context from published textbooks. To measure our model's performance in mathematical reasoning, our testing paradigm focuses on the task of automated theorem proving via generating proofs to a given mathematical claim in the Lean formal language.

**Link**: [arxiv](http://arxiv.org/abs/2501.15797v2),  [pdf](http://arxiv.org/pdf/2501.15797v2)

**Tags**: cs.LG cs.CL cs.IR 



### RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing
**Authors**: Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang

**Updated**: 2025-01-30T05:56:30Z

**Summary**: Code auditing is a code review process with the goal of finding bugs. Large Language Models (LLMs) have shown substantial potential in this task, offering the ability to analyze programs without compilation and enabling customized bug detection following specified prompts. However, applying LLMs to repository-level code auditing presents notable challenges. The inherent context limits and hallucinations of LLMs can lead to the low quality of bug reports. Meanwhile, the large size of software repositories introduces substantial time and token costs, hindering efficiency and scalability in real-world scenarios.   This work introduces an autonomous LLM-agent, RepoAudit, designed to enable precise and efficient repository-level code auditing. Equipped with the agent memory, RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. It also introduces the validator to check the data-flow facts for hallucination mitigation and examine the satisfiability of path conditions of potential buggy paths, which enables RepoAudit to discard false positives in the code auditing. Our experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per project on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.18160v1),  [pdf](http://arxiv.org/pdf/2501.18160v1)

**Tags**: cs.SE cs.PL 



### Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin   Case Study
**Authors**: Yuchen Lei, Yuexin Xiang, Qin Wang, Rafael Dowsley, Tsz Hon Yuen, Jiangshan Yu

**Updated**: 2025-01-30T05:48:13Z

**Summary**: Cryptocurrencies are widely used, yet current methods for analyzing transactions heavily rely on opaque, black-box models. These lack interpretability and adaptability, failing to effectively capture behavioral patterns. Many researchers, including us, believe that Large Language Models (LLMs) could bridge this gap due to their robust reasoning abilities for complex tasks. In this paper, we test this hypothesis by applying LLMs to real-world cryptocurrency transaction graphs, specifically within the Bitcoin network. We introduce a three-tiered framework to assess LLM capabilities: foundational metrics, characteristic overview, and contextual interpretation. This includes a new, human-readable graph representation format, LLM4TG, and a connectivity-enhanced sampling algorithm, CETraS, which simplifies larger transaction graphs. Experimental results show that LLMs excel at foundational metrics and offer detailed characteristic overviews. Their effectiveness in contextual interpretation suggests they can provide useful explanations of transaction behaviors, even with limited labeled data.

**Link**: [arxiv](http://arxiv.org/abs/2501.18158v1),  [pdf](http://arxiv.org/pdf/2501.18158v1)

**Tags**: cs.CR cs.LG 



### Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training   and Unimodal Deployment
**Authors**: Joanna Hong, Sanjeel Parekh, Honglie Chen, Jacob Donley, Ke Tan, Buye Xu, Anurag Kumar

**Updated**: 2025-01-30T05:46:30Z

**Summary**: Building reliable speech systems often requires combining multiple modalities, like audio and visual cues. While such multimodal solutions frequently lead to improvements in performance and may even be critical in certain cases, they come with several constraints such as increased sensory requirements, computational cost, and modality synchronization, to mention a few. These challenges constrain the direct uses of these multimodal solutions in real-world applications. In this work, we develop approaches where the learning happens with all available modalities but the deployment or inference is done with just one or reduced modalities. To do so, we propose a Multimodal Training and Unimodal Deployment (MUTUD) framework which includes a Temporally Aligned Modality feature Estimation (TAME) module that can estimate information from missing modality using modalities present during inference. This innovative approach facilitates the integration of information across different modalities, enhancing the overall inference process by leveraging the strengths of each modality to compensate for the absence of certain modalities during inference. We apply MUTUD to various audiovisual speech tasks and show that it can reduce the performance gap between the multimodal and corresponding unimodal models to a considerable extent. MUTUD can achieve this while reducing the model size and compute compared to multimodal models, in some cases by almost 80%.

**Link**: [arxiv](http://arxiv.org/abs/2501.18157v1),  [pdf](http://arxiv.org/pdf/2501.18157v1)

**Tags**: cs.SD cs.CV cs.MM eess.AS 



### Mixed-Precision Graph Neural Quantization for Low Bit Large Language   Models
**Authors**: Wanlong Liu, Yichen Xiao, Dingyi Zeng, Hongyang Zhao, Wenyu Chen, Malu Zhang

**Updated**: 2025-01-30T05:39:01Z

**Summary**: Post-Training Quantization (PTQ) is pivotal for deploying large language models (LLMs) within resource-limited settings by significantly reducing resource demands. However, existing PTQ strategies underperform at low bit levels < 3 bits due to the significant difference between the quantized and original weights. To enhance the quantization performance at low bit widths, we introduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a graph neural network (GNN) module to capture dependencies among weights and adaptively assign quantization bit-widths. Through the information propagation of the GNN module, our method more effectively captures dependencies among target weights, leading to a more accurate assessment of weight importance and optimized allocation of quantization strategies. Extensive experiments on the WikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms previous state-of-the-art PTQ method GPTQ, setting new benchmarks for quantization performance under low-bit conditions.

**Link**: [arxiv](http://arxiv.org/abs/2501.18154v1),  [pdf](http://arxiv.org/pdf/2501.18154v1)

**Tags**: cs.CL 



### Utilizing API Response for Test Refinement
**Authors**: Devika Sondhi, Ananya Sharma, Diptikalyan Saha

**Updated**: 2025-01-30T05:26:32Z

**Summary**: Most of the web services are offered in the form of RESTful APIs. This has led to an active research interest in API testing to ensure the reliability of these services. While most of the testing techniques proposed in the past rely on the API specification to generate the test cases, a major limitation of such an approach is that in the case of an incomplete or inconsistent specification, the test cases may not be realistic in nature and would result in a lot of 4xx response due to invalid input. This is indicative of poor test quality. Learning-based approaches may learn about valid inputs but often require a large number of request-response pairs to learn the constraints, making it infeasible to be readily used in the industry. To address this limitation, this paper proposes a dynamic test refinement approach that leverages the response message. The response is used to infer the point in the API testing flow where a test scenario fix is required. Using an intelligent agent, the approach adds constraints to the API specification that are further used to generate a test scenario accounting for the learned constraint from the response. Following a greedy approach, the iterative learning and refinement of test scenarios are obtained from the API testing system. The proposed approach led to a decrease in the number of 4xx responses, taking a step closer to generating more realistic test cases with high coverage that would aid in functional testing. A high coverage was obtained from a lesser number of API requests, as compared with the state-of-the-art search-based API Testing tools.

**Link**: [arxiv](http://arxiv.org/abs/2501.18145v1),  [pdf](http://arxiv.org/pdf/2501.18145v1)

**Tags**: cs.SE 



### QADM-Net: Multi-Level Quality-Adaptive Dynamic Network for Reliable   Multimodal Classification
**Authors**: Shu Shen, Tong Zhang, C. L. Philip Chen

**Updated**: 2025-01-30T05:09:17Z

**Summary**: Multimodal machine learning has achieved remarkable progress in many scenarios, but its reliability is undermined by varying sample quality. In this paper, we find that current multimodal classification methods lack dynamic networks for sample-specific depth and parameters to achieve reliable inference. To this end, a novel framework for multimodal reliable classification termed Multi-Level Quality-Adaptive Dynamic Multimodal Network (QADM-Net) is proposed. QADM-Net first adopts a novel approach based on noise-free prototypes and a classifier-free design to reliably estimate the quality of each sample at both modality and feature levels. It then achieves sample-specific network depth via the \textbf{\textit{Global Confidence Normalized Depth (GCND)}} mechanism. By normalizing depth across modalities and samples, \textit{\textbf{GCND}} effectively mitigates the impact of challenging modality inputs on dynamic depth reliability. Furthermore, QADM-Net provides sample-adaptive network parameters via the \textbf{\textit{Layer-wise Greedy Parameter (LGP)}} mechanism driven by feature-level quality. The cross-modality layer-wise greedy strategy in \textbf{\textit{LGP}} designs a reliable parameter prediction paradigm for multimodal networks with variable depths for the first time. Experiments conducted on four datasets demonstrate that QADM-Net significantly outperforms state-of-the-art methods in classification performance and reliability, exhibiting strong adaptability to data with diverse quality.

**Link**: [arxiv](http://arxiv.org/abs/2412.14489v2),  [pdf](http://arxiv.org/pdf/2412.14489v2)

**Tags**: cs.CV 



### Nonlocal prior mixture-based Bayesian wavelet regression
**Authors**: Nilotpal Sanyal

**Updated**: 2025-01-30T04:54:59Z

**Summary**: We propose a novel Bayesian wavelet regression approach using a three-component spike-and-slab prior for wavelet coefficients, combining a point mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior. This flexible prior supports small and large coefficients differently, offering advantages for highly dispersed data where wavelet coefficients span multiple scales. The IMOM prior's heavy tails capture large coefficients, while the MOM prior is better suited for smaller non-zero coefficients. Further, our method introduces innovative hyperparameter specifications for mixture probabilities and scaling parameters, including generalized logit, hyperbolic secant, and generalized normal decay for probabilities, and double exponential decay for scaling. Hyperparameters are estimated via an empirical Bayes approach, enabling posterior inference tailored to the data. Extensive simulations demonstrate significant performance gains over two-component wavelet methods. Applications to electroencephalography and noisy audio data illustrate the method's utility in capturing complex signal characteristics. We implement our method in an R package NLPwavelet.

**Link**: [arxiv](http://arxiv.org/abs/2501.18134v1),  [pdf](http://arxiv.org/pdf/2501.18134v1)

**Tags**: stat.ME stat.AP 



### Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with   an Iterative Approach
**Authors**: Zhouyu Jiang, Mengshu Sun, Lei Liang, Zhiqiang Zhang

**Updated**: 2025-01-30T04:51:41Z

**Summary**: Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.

**Link**: [arxiv](http://arxiv.org/abs/2407.13101v2),  [pdf](http://arxiv.org/pdf/2407.13101v2)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Deployment 
 ### DeltaLLM: Compress LLMs with Low-Rank Deltas between Shared Weights
**Authors**: Liana Mikaelyan, Ayyoob Imani, Mathew Salvaris, Parth Pathak, Mohsen Fayyaz

**Updated**: 2025-01-30T18:59:55Z

**Summary**: We introduce DeltaLLM, a new post-training compression technique to reduce the memory footprint of LLMs. We propose an alternative way of structuring LLMs with weight sharing between layers in subsequent Transformer blocks, along with additional low-rank difference matrices between them. For training, we adopt the progressing module replacement method and show that the lightweight training of the low-rank modules with approximately 30M-40M tokens is sufficient to achieve performance on par with LLMs of comparable sizes trained from scratch. We release the resultant models, DeltaLLAMA and DeltaPHI, with a 12% parameter reduction, retaining 90% of the performance of the base Llama and Phi models on common knowledge and reasoning benchmarks. Our method also outperforms compression techniques JointDrop, LaCo, ShortGPT and SliceGPT with the same number of parameters removed. For example, DeltaPhi 2.9B with a 24% reduction achieves similar average zero-shot accuracies as recovery fine-tuned SlicedPhi 3.3B with a 12% reduction, despite being approximately 400M parameters smaller with no fine-tuning applied. This work provides new insights into LLM architecture design and compression methods when storage space is critical.

**Link**: [arxiv](http://arxiv.org/abs/2501.18596v1),  [pdf](http://arxiv.org/pdf/2501.18596v1)

**Tags**: cs.LG cs.AI 



### Foundational Models for 3D Point Clouds: A Survey and Outlook
**Authors**: Vishal Thengane, Xiatian Zhu, Salim Bouzerdoum, Son Lam Phung, Yunpeng Li

**Updated**: 2025-01-30T18:59:43Z

**Summary**: The 3D point cloud representation plays a crucial role in preserving the geometric fidelity of the physical world, enabling more accurate complex 3D environments. While humans naturally comprehend the intricate relationships between objects and variations through a multisensory system, artificial intelligence (AI) systems have yet to fully replicate this capacity. To bridge this gap, it becomes essential to incorporate multiple modalities. Models that can seamlessly integrate and reason across these modalities are known as foundation models (FMs). The development of FMs for 2D modalities, such as images and text, has seen significant progress, driven by the abundant availability of large-scale datasets. However, the 3D domain has lagged due to the scarcity of labelled data and high computational overheads. In response, recent research has begun to explore the potential of applying FMs to 3D tasks, overcoming these challenges by leveraging existing 2D knowledge. Additionally, language, with its capacity for abstract reasoning and description of the environment, offers a promising avenue for enhancing 3D understanding through large pre-trained language models (LLMs). Despite the rapid development and adoption of FMs for 3D vision tasks in recent years, there remains a gap in comprehensive and in-depth literature reviews. This article aims to address this gap by presenting a comprehensive overview of the state-of-the-art methods that utilize FMs for 3D visual understanding. We start by reviewing various strategies employed in the building of various 3D FMs. Then we categorize and summarize use of different FMs for tasks such as perception tasks. Finally, the article offers insights into future directions for research and development in this field. To help reader, we have curated list of relevant papers on the topic: https://github.com/vgthengane/Awesome-FMs-in-3D.

**Link**: [arxiv](http://arxiv.org/abs/2501.18594v1),  [pdf](http://arxiv.org/pdf/2501.18594v1)

**Tags**: cs.CV 



### Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs
**Authors**: Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, Dong Yu

**Updated**: 2025-01-30T18:58:18Z

**Summary**: Large language models (LLMs) such as OpenAI's o1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we term underthinking, where o1-like LLMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems. To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source o1-like models, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy with thought switching penalty TIP that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path. Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning. Our findings contribute to understanding reasoning inefficiencies in o1-like LLMs and offer a practical solution to enhance their problem-solving capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.18585v1),  [pdf](http://arxiv.org/pdf/2501.18585v1)

**Tags**: cs.CL 



### Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for   Multi-Step Reasoning Over Speed in MATH
**Authors**: Evgenii Evstafev

**Updated**: 2025-01-30T18:45:51Z

**Summary**: This study investigates the performance of the DeepSeek R1 language model on 30 challenging mathematical problems derived from the MATH dataset, problems that previously proved unsolvable by other models under time constraints. Unlike prior work, this research removes time limitations to explore whether DeepSeek R1's architecture, known for its reliance on token-based reasoning, can achieve accurate solutions through a multi-step process. The study compares DeepSeek R1 with four other models (gemini-1.5-flash-8b, gpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11 temperature settings. Results demonstrate that DeepSeek R1 achieves superior accuracy on these complex problems but generates significantly more tokens than other models, confirming its token-intensive approach. The findings highlight a trade-off between accuracy and efficiency in mathematical problem-solving with large language models: while DeepSeek R1 excels in accuracy, its reliance on extensive token generation may not be optimal for applications requiring rapid responses. The study underscores the importance of considering task-specific requirements when selecting an LLM and emphasizes the role of temperature settings in optimizing performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.18576v1),  [pdf](http://arxiv.org/pdf/2501.18576v1)

**Tags**: cs.LG 



### An Empirical Study of Dotfiles Repositories Containing User-Specific   Configuration Files
**Authors**: Wenhan Zhu, Michael W. Godfrey

**Updated**: 2025-01-30T18:32:46Z

**Summary**: Storing user-specific configuration files in a "dotfiles" repository is a common practice among software developers, with hundreds of thousands choosing to publicly host their repositories on GitHub. This practice not only provides developers with a simple backup mechanism for their essential configuration files, but also facilitates sharing ideas and learning from others on how best to configure applications that are key to their daily workflows. However, our current understanding of these repository sharing practices is limited and mostly anecdotal. To address this gap, we conducted a study to delve deeper into this phenomenon. Beginning with collecting and analyzing publicly-hosted dotfiles repositories on GitHub, we discovered that maintaining dotfiles is widespread among developers. Notably, we found that 25.8% of the top 500 most-starred GitHub users maintain some form of publicly accessible dotfiles repository. Among these, configurations for text editors like Vim and shells such as bash and zsh are the most commonly tracked. Our analysis reveals that updating dotfiles is primarily driven by the need to adjust configurations (63.3%) and project meta-management (25.4%). Surprisingly, we found no significant difference in the types of dotfiles observed across code churn history patterns, suggesting that the frequency of dotfile modifications depends more on the developer than the properties of the specific dotfile and its associated application. Finally, we discuss the challenges associated with managing dotfiles, including the necessity for a reliable and effective deployment mechanism, and how the insights gleaned from dotfiles can inform tool designers by offering real-world usage information.

**Link**: [arxiv](http://arxiv.org/abs/2501.18555v1),  [pdf](http://arxiv.org/pdf/2501.18555v1)

**Tags**: cs.SE cs.SI 



### Semantic Web and Creative AI -- A Technical Report from ISWS 2023
**Authors**: Raia Abu Ahmad, Reham Alharbi, Roberto Barile, Martin Böckling, Francisco Bolanos, Sara Bonfitto, Oleksandra Bruns, Irene Celino, Yashrajsinh Chudasama, Martin Critelli, Claudia d'Amato, Giada D'Ippolito, Ioannis Dasoulas, Stefano De Giorgis, Vincenzo De Leo, Chiara Di Bonaventura, Marco Di Panfilo, Daniil Dobriy, John Domingue, Xuemin Duan, Michel Dumontier, Sefika Efeoglu, Ruben Eschauzier, Fakih Ginwa, Nicolas Ferranti, Arianna Graciotti, Philipp Hanisch, George Hannah, Golsa Heidari, Aidan Hogan, Hassan Hussein, Alexane Jouglar, Jan-Christoph Kalo, Manoé Kieffer, Antonis Klironomos, Inês Koch, Weronika Lajewska, Nicolas Lazzari, Mikael Lindekrans, Anna Sofia Lippolis, Majlinda Llugiqi, Eleonora Mancini, Eleonora Marzi, Laura Menotti, Daniela Milon Flores, Soulakshmee Nagowah, Kerstin Neubert, Emetis Niazmand, Ebrahim Norouzi, Beatriz Olarte Martinez, Anouk Michelle Oudshoorn, Andrea Poltronieri, Valentina Presutti, Disha Purohit, Ensiyeh Raoufi, Celian Ringwald, Johanna Rockstroh, Sebastian Rudolph, Harald Sack, Zafar Saeed, Mohammad Javad Saeedizade, Aya Sahbi, Cristian Santini, Aleksandra Simic, Dennis Sommer, Rita Sousa, Mary Ann Tan, Vidyashree Tarikere, Tabea Tietz, Liam Tirpitz, Arnaldo Tomasino, Frank van Harmelen, Joao Vissoci, Caitlin Woods, Bohui Zhang, Xinyue Zhang, Heng Zheng

**Updated**: 2025-01-30T18:10:16Z

**Summary**: The International Semantic Web Research School (ISWS) is a week-long intensive program designed to immerse participants in the field. This document reports a collaborative effort performed by ten teams of students, each guided by a senior researcher as their mentor, attending ISWS 2023. Each team provided a different perspective to the topic of creative AI, substantiated by a set of research questions as the main subject of their investigation. The 2023 edition of ISWS focuses on the intersection of Semantic Web technologies and Creative AI. ISWS 2023 explored various intersections between Semantic Web technologies and creative AI. A key area of focus was the potential of LLMs as support tools for knowledge engineering. Participants also delved into the multifaceted applications of LLMs, including legal aspects of creative content production, humans in the loop, decentralised approaches to multimodal generative AI models, nanopublications and AI for personal scientific knowledge graphs, commonsense knowledge in automatic story and narrative completion, generative AI for art critique, prompt engineering, automatic music composition, commonsense prototyping and conceptual blending, and elicitation of tacit knowledge. As Large Language Models and semantic technologies continue to evolve, new exciting prospects are emerging: a future where the boundaries between creative expression and factual knowledge become increasingly permeable and porous, leading to a world of knowledge that is both informative and inspiring.

**Link**: [arxiv](http://arxiv.org/abs/2501.18542v1),  [pdf](http://arxiv.org/pdf/2501.18542v1)

**Tags**: cs.AI 



### Can we Retrieve Everything All at Once? ARM: An Alignment-Oriented   LLM-based Retrieval Method
**Authors**: Peter Baile Chen, Yi Zhang, Michael Cafarella, Dan Roth

**Updated**: 2025-01-30T18:07:19Z

**Summary**: Real-world open-domain questions can be complicated, particularly when answering them involves information from multiple information sources. LLMs have demonstrated impressive performance in decomposing complex tasks into simpler steps, and previous work has used it for better retrieval in support of complex questions. However, LLM's decomposition of questions is unaware of what data is available and how data is organized, often leading to a sub-optimal retrieval performance. Recent effort in agentic RAG proposes to perform retrieval in an iterative fashion, where a followup query is derived as an action based on previous rounds of retrieval. While this provides one way of interacting with the data collection, agentic RAG's exploration of data is inefficient because successive queries depend on previous results rather than being guided by the organization of available data in the collection. To address this problem, we propose an LLM-based retrieval method -- ARM, that aims to better align the question with the organization of the data collection by exploring relationships among data objects beyond matching the utterance of the query, thus leading to a retrieve-all-at-once solution for complex queries. We evaluated ARM on two datasets, Bird and OTT-QA. On Bird, it outperforms standard RAG with query decomposition by up to 5.2 pt in execution accuracy and agentic RAG (ReAct) by up to 15.9 pt. On OTT-QA, it achieves up to 5.5 pt and 19.3 pt higher F1 match scores compared to these approaches.

**Link**: [arxiv](http://arxiv.org/abs/2501.18539v1),  [pdf](http://arxiv.org/pdf/2501.18539v1)

**Tags**: cs.CL cs.AI cs.IR 



### Illusions of Relevance: Using Content Injection Attacks to Deceive   Retrievers, Rerankers, and LLM Judges
**Authors**: Manveer Singh Tamber, Jimmy Lin

**Updated**: 2025-01-30T18:02:15Z

**Summary**: Consider a scenario in which a user searches for information, only to encounter texts flooded with misleading or non-relevant content. This scenario exemplifies a simple yet potent vulnerability in neural Information Retrieval (IR) pipelines: content injection attacks. We find that embedding models for retrieval, rerankers, and large language model (LLM) relevance judges are vulnerable to these attacks, in which adversaries insert misleading text into passages to manipulate model judgements. We identify two primary threats: (1) inserting unrelated or harmful content within passages that still appear deceptively "relevant", and (2) inserting entire queries or key query terms into passages to boost their perceived relevance. While the second tactic has been explored in prior research, we present, to our knowledge, the first empirical analysis of the first threat, demonstrating how state-of-the-art models can be easily misled. Our study systematically examines the factors that influence an attack's success, such as the placement of injected content and the balance between relevant and non-relevant material. Additionally, we explore various defense strategies, including adversarial passage classifiers, retriever fine-tuning to discount manipulated content, and prompting LLM judges to adopt a more cautious approach. However, we find that these countermeasures often involve trade-offs, sacrificing effectiveness for attack robustness and sometimes penalizing legitimate documents in the process. Our findings highlight the need for stronger defenses against these evolving adversarial strategies to maintain the trustworthiness of IR systems. We release our code and scripts to facilitate further research.

**Link**: [arxiv](http://arxiv.org/abs/2501.18536v1),  [pdf](http://arxiv.org/pdf/2501.18536v1)

**Tags**: cs.IR 



### Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models
**Authors**: Yi Ding, Lijun Li, Bing Cao, Jing Shao

**Updated**: 2025-01-30T17:59:45Z

**Summary**: Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on textual or multimodal content, fall short in addressing challenging cases or disrupt the balance between helpfulness and harmlessness. Our evaluation highlights a safety reasoning gap: these methods lack safety visual reasoning ability, leading to such bottlenecks. To address this limitation and enhance both visual perception and reasoning in safety-critical contexts, we propose a novel dataset that integrates multi-image inputs with safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve model performance. Specifically, we introduce the Multi-Image Safety (MIS) dataset, an instruction-following dataset tailored for multi-image safety scenarios, consisting of training and test splits. Our experiments demonstrate that fine-tuning InternVL2.5-8B with MIS significantly outperforms both powerful open-source models and API-based models in challenging multi-image tasks requiring safety-related visual reasoning. This approach not only delivers exceptional safety performance but also preserves general capabilities without any trade-offs. Specifically, fine-tuning with MIS increases average accuracy by 0.83% across five general benchmarks and reduces the Attack Success Rate (ASR) on multiple safety benchmarks by a large margin. Data and Models are released under: \href{https://dripnowhy.github.io/MIS/}{\texttt{https://dripnowhy.github.io/MIS/}}

**Link**: [arxiv](http://arxiv.org/abs/2501.18533v1),  [pdf](http://arxiv.org/pdf/2501.18533v1)

**Tags**: cs.CV cs.CL cs.CR 



### In-Context Meta LoRA Generation
**Authors**: Yihua Shao, Minxi Yan, Yang Liu, Siyu Chen, Wenjie Chen, Xinwei Long, Ziyang Yan, Lei Li, Chenyu Zhang, Nicu Sebe, Hao Tang, Yan Wang, Hao Zhao, Mengzhu Wang, Jingcai Guo

**Updated**: 2025-01-30T17:59:08Z

**Summary**: Low-rank Adaptation (LoRA) has demonstrated remarkable capabilities for task specific fine-tuning. However, in scenarios that involve multiple tasks, training a separate LoRA model for each one results in considerable inefficiency in terms of storage and inference. Moreover, existing parameter generation methods fail to capture the correlations among these tasks, making multi-task LoRA parameter generation challenging. To address these limitations, we propose In-Context Meta LoRA (ICM-LoRA), a novel approach that efficiently achieves task-specific customization of large language models (LLMs). Specifically, we use training data from all tasks to train a tailored generator, Conditional Variational Autoencoder (CVAE). CVAE takes task descriptions as inputs and produces task-aware LoRA weights as outputs. These LoRA weights are then merged with LLMs to create task-specialized models without the need for additional fine-tuning. Furthermore, we utilize in-context meta-learning for knowledge enhancement and task mapping, to capture the relationship between tasks and parameter distributions. As a result, our method achieves more accurate LoRA parameter generation for diverse tasks using CVAE. ICM-LoRA enables more accurate LoRA parameter reconstruction than current parameter reconstruction methods and is useful for implementing task-specific enhancements of LoRA parameters. At the same time, our method occupies 283MB, only 1\% storage compared with the original LoRA.

**Link**: [arxiv](http://arxiv.org/abs/2501.17635v2),  [pdf](http://arxiv.org/pdf/2501.17635v2)

**Tags**: cs.CL cs.AI cs.CV 



### Differentially Private Steering for Large Language Model Alignment
**Authors**: Anmol Goel, Yaxi Hu, Iryna Gurevych, Amartya Sanyal

**Updated**: 2025-01-30T17:58:36Z

**Summary**: Aligning Large Language Models (LLMs) with human values and away from undesirable behaviors (such as hallucination) has become increasingly important. Recently, steering LLMs towards a desired behavior via activation editing has emerged as an effective method to mitigate harmful generations at inference-time. Activation editing modifies LLM representations by preserving information from positive demonstrations (e.g., truthful) and minimising information from negative demonstrations (e.g., hallucinations). When these demonstrations come from a private dataset, the aligned LLM may leak private information contained in those private samples. In this work, we present the first study of aligning LLM behavior with private datasets. Our work proposes the \textit{\underline{P}rivate \underline{S}teering for LLM \underline{A}lignment (PSA)} algorithm to edit LLM activations with differential privacy (DP) guarantees. We conduct extensive experiments on seven different benchmarks with open-source LLMs of different sizes (0.5B to 7B) and model families (LlaMa, Qwen, Mistral and Gemma). Our results show that PSA achieves DP guarantees for LLM alignment with minimal loss in performance, including alignment metrics, open-ended text generation quality, and general-purpose reasoning. We also develop the first Membership Inference Attack (MIA) for evaluating and auditing the empirical privacy for the problem of LLM steering via activation editing. Our attack is tailored for activation editing and relies solely on the generated texts without their associated probabilities. Our experiments support the theoretical guarantees by showing improved guarantees for our \textit{PSA} algorithm compared to several existing non-private techniques.

**Link**: [arxiv](http://arxiv.org/abs/2501.18532v1),  [pdf](http://arxiv.org/pdf/2501.18532v1)

**Tags**: cs.CL cs.LG 



### Towards Automated Penetration Testing: Introducing LLM Benchmark,   Analysis, and Improvements
**Authors**: Isamu Isozaki, Manil Shrestha, Rick Console, Edward Kim

**Updated**: 2025-01-30T17:50:16Z

**Summary**: Hacking poses a significant threat to cybersecurity, inflicting billions of dollars in damages annually. To mitigate these risks, ethical hacking, or penetration testing, is employed to identify vulnerabilities in systems and networks. Recent advancements in large language models (LLMs) have shown potential across various domains, including cybersecurity. However, there is currently no comprehensive, open, end-to-end automated penetration testing benchmark to drive progress and evaluate the capabilities of these models in security contexts. This paper introduces a novel open benchmark for LLM-based automated penetration testing, addressing this critical gap. We first evaluate the performance of LLMs, including GPT-4o and Llama 3.1-405B, using the state-of-the-art PentestGPT tool. Our findings reveal that while Llama 3.1 demonstrates an edge over GPT-4o, both models currently fall short of performing fully automated, end-to-end penetration testing. Next, we advance the state-of-the-art and present ablation studies that provide insights into improving the PentestGPT tool. Our research illuminates the challenges LLMs face in each aspect of Pentesting, e.g. enumeration, exploitation, and privilege escalation. This work contributes to the growing body of knowledge on AI-assisted cybersecurity and lays the foundation for future research in automated penetration testing using large language models.

**Link**: [arxiv](http://arxiv.org/abs/2410.17141v3),  [pdf](http://arxiv.org/pdf/2410.17141v3)

**Tags**: cs.CR cs.AI 



### GroUSE: A Benchmark to Evaluate Evaluators in Grounded Question   Answering
**Authors**: Sacha Muller, António Loison, Bilel Omrani, Gautier Viaud

**Updated**: 2025-01-30T17:34:51Z

**Summary**: Retrieval-Augmented Generation (RAG) has emerged as a common paradigm to use Large Language Models (LLMs) alongside private and up-to-date knowledge bases. In this work, we address the challenges of using LLM-as-a-Judge when evaluating grounded answers generated by RAG systems. To assess the calibration and discrimination capabilities of judge models, we identify 7 generator failure modes and introduce GroUSE (Grounded QA Unitary Scoring of Evaluators), a meta-evaluation benchmark of 144 unit tests. This benchmark reveals that existing automated RAG evaluation frameworks often overlook important failure modes, even when using GPT-4 as a judge.   To improve on the current design of automated RAG evaluation frameworks, we propose a novel pipeline and find that while closed models perform well on GroUSE, state-of-the-art open-source judges do not generalize to our proposed criteria, despite strong correlation with GPT-4's judgement. Our findings suggest that correlation with GPT-4 is an incomplete proxy for the practical performance of judge models and should be supplemented with evaluations on unit tests for precise failure mode detection.   We further show that finetuning Llama-3 on GPT-4's reasoning traces significantly boosts its evaluation capabilities, improving upon both correlation with GPT-4's evaluations and calibration on reference situations.

**Link**: [arxiv](http://arxiv.org/abs/2409.06595v3),  [pdf](http://arxiv.org/pdf/2409.06595v3)

**Tags**: cs.CL I.2.7 



### Learn from the Past: Language-conditioned Object Rearrangement with   Large Language Models
**Authors**: Guanqun Cao, Ryan Mckenna, John Oyekan

**Updated**: 2025-01-30T17:28:11Z

**Summary**: Object rearrangement is a significant task for collaborative robots, where they are directed to manipulate objects into a specified goal state. Determining the placement of objects is a major challenge that influences the efficiency of the rearrangement process. Most current methods heavily rely on pre-collected datasets to train the model for predicting the goal position and are restricted to specific instructions, which limits their broader applicability and effectiveness.In this paper, we propose a framework of language-conditioned object rearrangement based on the Large Language Model (LLM). Particularly, our approach mimics human reasoning by using past successful experiences as a reference to infer the desired goal position. Based on LLM's strong natural language comprehension and inference ability, our method can generalise to handle various everyday objects and free-form language instructions in a zero-shot manner. Experimental results demonstrate that our methods can effectively execute the robotic rearrangement tasks, even those involving long sequential orders.

**Link**: [arxiv](http://arxiv.org/abs/2501.18516v1),  [pdf](http://arxiv.org/pdf/2501.18516v1)

**Tags**: cs.RO 



### Streaming DiLoCo with overlapping communication: Towards a Distributed   Free Lunch
**Authors**: Arthur Douillard, Yanislav Donchev, Keith Rush, Satyen Kale, Zachary Charles, Zachary Garrett, Gabriel Teston, Dave Lacey, Ross McIlroy, Jiajun Shen, Alexandre Ramé, Arthur Szlam, Marc'Aurelio Ranzato, Paul Barham

**Updated**: 2025-01-30T17:23:50Z

**Summary**: Training of large language models (LLMs) is typically distributed across a large number of accelerators to reduce training time. Since internal states and parameter gradients need to be exchanged at each and every single gradient step, all devices need to be co-located using low-latency high-bandwidth communication links to support the required high volume of exchanged bits. Recently, distributed algorithms like DiLoCo have relaxed such co-location constraint: accelerators can be grouped into ``workers'', where synchronizations between workers only occur infrequently. This in turn means that workers can afford being connected by lower bandwidth communication links without affecting learning quality. However, in these methods, communication across workers still requires the same peak bandwidth as before, as the synchronizations require all parameters to be exchanged across all workers. In this paper, we improve DiLoCo in three ways. First, we synchronize only subsets of parameters in sequence, rather than all at once, which greatly reduces peak bandwidth. Second, we allow workers to continue training while synchronizing, which decreases wall clock time. Third, we quantize the data exchanged by workers, which further reduces bandwidth across workers. By properly combining these modifications, we show experimentally that we can distribute training of billion-scale parameters and reach similar quality as before, but reducing required bandwidth by two orders of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2501.18512v1),  [pdf](http://arxiv.org/pdf/2501.18512v1)

**Tags**: cs.CL 



### WILDCHAT-50M: A Deep Dive Into the Role of Synthetic Data in   Post-Training
**Authors**: Benjamin Feuer, Chinmay Hegde

**Updated**: 2025-01-30T17:21:44Z

**Summary**: Language model (LLM) post-training, from DPO to distillation, can refine behaviors and unlock new skills, but the open science supporting these post-training techniques is still in its infancy. One limiting factor has been the difficulty of conducting large-scale comparative analyses of synthetic data generating models and LLM judges. To close this gap, we introduce WILDCHAT-50M, the largest public chat dataset to date. We extend the existing WildChat dataset to include responses not only from GPT, but from over 50 different open-weight models, ranging in size from 0.5B to 104B parameters. We conduct an extensive comparative analysis and demonstrate the potential of this dataset by creating RE-WILD, our own public SFT mix, which outperforms the recent Tulu-3 SFT mixture from Allen AI with only 40% as many samples. Our dataset, samples and code are available at https://github.com/penfever/wildchat-50m.

**Link**: [arxiv](http://arxiv.org/abs/2501.18511v1),  [pdf](http://arxiv.org/pdf/2501.18511v1)

**Tags**: cs.LG cs.CL 



### CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to   Sustainability Data Extraction
**Authors**: Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa

**Updated**: 2025-01-30T17:13:32Z

**Summary**: Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.

**Link**: [arxiv](http://arxiv.org/abs/2501.18504v1),  [pdf](http://arxiv.org/pdf/2501.18504v1)

**Tags**: cs.CV cs.AI cs.NE 68W50, 68T07 G.1.6; I.2.10 



### GuardReasoner: Towards Reasoning-based LLM Safeguards
**Authors**: Yue Liu, Hongcheng Gao, Shengfang Zhai, Jun Xia, Tianyi Wu, Zhiwei Xue, Yulin Chen, Kenji Kawaguchi, Jiaheng Zhang, Bryan Hooi

**Updated**: 2025-01-30T17:06:06Z

**Summary**: As LLMs increasingly impact safety-critical applications, ensuring their safety using guardrails remains a key challenge. This paper proposes GuardReasoner, a new safeguard for LLMs, by guiding the guard model to learn to reason. Concretely, we first create the GuardReasonerTrain dataset, which consists of 127K samples with 460K detailed reasoning steps. Then, we introduce reasoning SFT to unlock the reasoning capability of guard models. In addition, we present hard sample DPO to further strengthen their reasoning ability. In this manner, GuardReasoner achieves better performance, explainability, and generalizability. Extensive experiments and analyses on 13 benchmarks of 3 guardrail tasks demonstrate its superiority. Remarkably, GuardReasoner 8B surpasses GPT-4o+CoT by 5.74% and LLaMA Guard 3 8B by 20.84% F1 score on average. We release the training data, code, and models with different scales (1B, 3B, 8B) of GuardReasoner : https://github.com/yueliu1999/GuardReasoner/.

**Link**: [arxiv](http://arxiv.org/abs/2501.18492v1),  [pdf](http://arxiv.org/pdf/2501.18492v1)

**Tags**: cs.CR cs.AI cs.LG 



### A Tool for In-depth Analysis of Code Execution Reasoning of Large   Language Models
**Authors**: Changshu Liu, Reyhaneh Jabbarvand

**Updated**: 2025-01-30T16:56:08Z

**Summary**: Code Executing Reasoning is becoming a new non-functional metric that assesses the ability of large language models (LLMs) in programming tasks. State-of-the-art frameworks (CodeMind or REval) and benchmarks (CruxEval) usually focus on LLM's prediction of a given code's input/output or intermediate variable states/values on limited programs. However, there is no tool for more in-depth analysis of the results. Without such a tool, the observations about LLM's code execution reasoning cannot be generalized to more datasets, preventing the research community and practitioners from devising the next generation of LLMs with better code execution reasoning abilities. This paper introduces ExeRScope, a series of tools and heuristics to analyze the result of code execution reasoning frameworks to understand better the impact of code properties in the studied benchmarks on the code execution reasoning. With such tooling, analysis can be generalized to code with similar properties without the urgent need to design more benchmarks, which is a cumbersome effort.

**Link**: [arxiv](http://arxiv.org/abs/2501.18482v1),  [pdf](http://arxiv.org/pdf/2501.18482v1)

**Tags**: cs.SE 



### Evaluating LLM-based Personal Information Extraction and Countermeasures
**Authors**: Yupei Liu, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong

**Updated**: 2025-01-30T16:53:30Z

**Summary**: Automatically extracting personal information--such as name, phone number, and email address--from publicly available profiles at a large scale is a stepstone to many other security attacks including spear phishing. Traditional methods--such as regular expression, keyword search, and entity detection--achieve limited success at such personal information extraction. In this work, we perform a systematic measurement study to benchmark large language model (LLM) based personal information extraction and countermeasures. Towards this goal, we present a framework for LLM-based extraction attacks; collect four datasets including a synthetic dataset generated by GPT-4 and three real-world datasets with manually labeled eight categories of personal information; introduce a novel mitigation strategy based on prompt injection; and systematically benchmark LLM-based attacks and countermeasures using ten LLMs and five datasets. Our key findings include: LLM can be misused by attackers to accurately extract various personal information from personal profiles; LLM outperforms traditional methods; and prompt injection can defend against strong LLM-based attacks, reducing the attack to less effective traditional ones.

**Link**: [arxiv](http://arxiv.org/abs/2408.07291v2),  [pdf](http://arxiv.org/pdf/2408.07291v2)

**Tags**: cs.CR 



### CLoQ: Enhancing Fine-Tuning of Quantized LLMs via Calibrated LoRA   Initialization
**Authors**: Yanxia Deng, Aozhong Zhang, Naigang Wang, Selcuk Gurses, Zi Yang, Penghang Yin

**Updated**: 2025-01-30T16:48:15Z

**Summary**: Fine-tuning large language models (LLMs) using low-rank adaptation (LoRA) has become a highly efficient approach for downstream tasks, particularly in scenarios with limited computational resources. However, applying LoRA techniques to quantized LLMs poses unique challenges due to the reduced representational precision of quantized weights. In this paper, we introduce CLoQ (Calibrated LoRA initialization for Quantized LLMs), a simplistic initialization strategy designed to overcome these challenges. Our approach focuses on minimizing the layer-wise discrepancy between the original LLM and its quantized counterpart with LoRA components during initialization. By leveraging a small calibration dataset, CLoQ quantizes a pre-trained LLM and determines the optimal LoRA components for each layer, ensuring a strong foundation for subsequent fine-tuning. A key contribution of this work is a novel theoretical result that enables the accurate and closed-form construction of these optimal LoRA components. We validate the efficacy of CLoQ across multiple tasks such as language generation, arithmetic reasoning, and commonsense reasoning, demonstrating that it consistently outperforms existing LoRA fine-tuning methods for quantized LLMs, especially at ultra low-bit widths.

**Link**: [arxiv](http://arxiv.org/abs/2501.18475v1),  [pdf](http://arxiv.org/pdf/2501.18475v1)

**Tags**: cs.LG cs.AI 



### Return of the Encoder: Maximizing Parameter Efficiency for SLMs
**Authors**: Mohamed Elfeki, Rui Liu, Chad Voegele

**Updated**: 2025-01-30T16:44:45Z

**Summary**: The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.   We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.   When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.

**Link**: [arxiv](http://arxiv.org/abs/2501.16273v2),  [pdf](http://arxiv.org/pdf/2501.16273v2)

**Tags**: cs.CL cs.AI cs.CV 



### LLM-AutoDiff: Auto-Differentiate Any LLM Workflow
**Authors**: Li Yin, Zhangyang Wang

**Updated**: 2025-01-30T16:40:12Z

**Summary**: Large Language Models (LLMs) have reshaped natural language processing, powering applications from multi-hop retrieval and question answering to autonomous agent workflows. Yet, prompt engineering -- the task of crafting textual inputs to effectively direct LLMs -- remains difficult and labor-intensive, particularly for complex pipelines that combine multiple LLM calls with functional operations like retrieval and data formatting. We introduce LLM-AutoDiff: a novel framework for Automatic Prompt Engineering (APE) that extends textual gradient-based methods (such as Text-Grad) to multi-component, potentially cyclic LLM architectures. Implemented within the AdalFlow library, LLM-AutoDiff treats each textual input as a trainable parameter and uses a frozen backward engine LLM to generate feedback-akin to textual gradients -- that guide iterative prompt updates. Unlike prior single-node approaches, LLM-AutoDiff inherently accommodates functional nodes, preserves time-sequential behavior in repeated calls (e.g., multi-hop loops), and combats the "lost-in-the-middle" problem by isolating distinct sub-prompts (instructions, formats, or few-shot examples). It further boosts training efficiency by focusing on error-prone samples through selective gradient computation. Across diverse tasks, including single-step classification, multi-hop retrieval-based QA, and agent-driven pipelines, LLM-AutoDiff consistently outperforms existing textual gradient baselines in both accuracy and training cost. By unifying prompt optimization through a graph-centric lens, LLM-AutoDiff offers a powerful new paradigm for scaling and automating LLM workflows - mirroring the transformative role that automatic differentiation libraries have long played in neural network research.

**Link**: [arxiv](http://arxiv.org/abs/2501.16673v2),  [pdf](http://arxiv.org/pdf/2501.16673v2)

**Tags**: cs.CL 



### How Much Can We Forget about Data Contamination?
**Authors**: Sebastian Bordt, Suraj Srinivas, Valentyn Boreiko, Ulrike von Luxburg

**Updated**: 2025-01-30T16:31:31Z

**Summary**: The leakage of benchmark data into the training data has emerged as a significant challenge for evaluating the capabilities of large language models (LLMs). In this work, we challenge the common assumption that small-scale contamination renders benchmark evaluations invalid. First, we experimentally quantify the magnitude of benchmark overfitting based on scaling along three dimensions: The number of model parameters (up to 1.6B), the number of times an example is seen (up to 144), and the number of training tokens (up to 40B). If model and data follow the Chinchilla scaling laws, minor contamination indeed leads to overfitting. At the same time, even 144 times of contamination can be forgotten if the training data is scaled beyond five times Chinchilla, a regime characteristic of many modern LLMs. Continual pre-training of OLMo-7B corroborates these results. Next, we study the impact of the weight decay parameter on example forgetting, showing that empirical forgetting occurs faster than the cumulative weight decay. This allows us to gauge the degree of example forgetting in large-scale training runs, indicating that many LLMs, including Lllama 3 405B, have forgotten the data seen at the beginning of training.

**Link**: [arxiv](http://arxiv.org/abs/2410.03249v3),  [pdf](http://arxiv.org/pdf/2410.03249v3)

**Tags**: cs.LG cs.AI cs.CL 



### ExeCoder: Empowering Large Language Models with Executability   Representation for Code Translation
**Authors**: Minghua He, Fangkai Yang, Pu Zhao, Wenjie Yin, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

**Updated**: 2025-01-30T16:18:52Z

**Summary**: Code translation is a crucial activity in the software development and maintenance process, and researchers have recently begun to focus on using pre-trained large language models (LLMs) for code translation. However, existing LLMs only learn the contextual semantics of code during pre-training, neglecting executability information closely related to the execution state of the code, which results in unguaranteed code executability and unreliable automated code translation. To address this issue, we propose ExeCoder, an LLM specifically designed for code translation, aimed at utilizing executability representations such as functional semantics, syntax structures, and variable dependencies to enhance the capabilities of LLMs in code translation. To evaluate the effectiveness of ExeCoder, we manually enhanced the widely used benchmark TransCoder-test, resulting in a benchmark called TransCoder-test-X that serves LLMs. Evaluation of TransCoder-test-X indicates that ExeCoder achieves state-of-the-art performance in code translation, surpassing existing open-source code LLMs by over 10.88% to 38.78% and over 27.44% to 42.97% on two metrics, and even outperforms the renowned closed-source LLM GPT-4o. Website: https://execoder4trans.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2501.18460v1),  [pdf](http://arxiv.org/pdf/2501.18460v1)

**Tags**: cs.SE 



### xJailbreak: Representation Space Guided Reinforcement Learning for   Interpretable LLM Jailbreaking
**Authors**: Sunbowen Lee, Shiwen Ni, Chi Wei, Shuaimin Li, Liyang Fan, Ahmadreza Argha, Hamid Alinejad-Rokny, Ruifeng Xu, Yicheng Gong, Min Yang

**Updated**: 2025-01-30T16:17:56Z

**Summary**: Safety alignment mechanism are essential for preventing large language models (LLMs) from generating harmful information or unethical content. However, cleverly crafted prompts can bypass these safety measures without accessing the model's internal parameters, a phenomenon known as black-box jailbreak. Existing heuristic black-box attack methods, such as genetic algorithms, suffer from limited effectiveness due to their inherent randomness, while recent reinforcement learning (RL) based methods often lack robust and informative reward signals. To address these challenges, we propose a novel black-box jailbreak method leveraging RL, which optimizes prompt generation by analyzing the embedding proximity between benign and malicious prompts. This approach ensures that the rewritten prompts closely align with the intent of the original prompts while enhancing the attack's effectiveness. Furthermore, we introduce a comprehensive jailbreak evaluation framework incorporating keywords, intent matching, and answer validation to provide a more rigorous and holistic assessment of jailbreak success. Experimental results show the superiority of our approach, achieving state-of-the-art (SOTA) performance on several prominent open and closed-source LLMs, including Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct, and GPT-4o-0806. Our method sets a new benchmark in jailbreak attack effectiveness, highlighting potential vulnerabilities in LLMs. The codebase for this work is available at https://github.com/Aegis1863/xJailbreak.

**Link**: [arxiv](http://arxiv.org/abs/2501.16727v2),  [pdf](http://arxiv.org/pdf/2501.16727v2)

**Tags**: cs.CL 



### CALM: Unleashing the Cross-Lingual Self-Aligning Ability of Language   Model Question Answering
**Authors**: Yumeng Wang, Zhiyuan Fan, Qingyun Wang, May Fung, Heng Ji

**Updated**: 2025-01-30T16:15:38Z

**Summary**: Large Language Models (LLMs) are pretrained on extensive multilingual corpora to acquire both language-specific cultural knowledge and general knowledge. Ideally, while LLMs should provide consistent responses to culture-independent questions across languages, we observe significant performance disparities. To address this, we explore the Cross-Lingual Self-Aligning ability of Language Models (CALM) to align knowledge across languages. Specifically, for a given question, we sample multiple responses across different languages, and select the most self-consistent response as the target, leaving the remaining responses as negative examples. We then employ direct preference optimization (DPO) to align the model's knowledge across different languages. Evaluations on the MEDQA and X-CSQA datasets demonstrate CALM's effectiveness in enhancing cross-lingual knowledge question answering, both in zero-shot and retrieval augmented settings. We also found that increasing the number of languages involved in CALM training leads to even higher accuracy and consistency. We offer a qualitative analysis of how cross-lingual consistency can enhance knowledge alignment and explore the method's generalizability. The source code and data of this paper are available on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2501.18457v1),  [pdf](http://arxiv.org/pdf/2501.18457v1)

**Tags**: cs.CL 



### LLMs & XAI for Water Sustainability: Seasonal Water Quality Prediction   with LIME Explainable AI and a RAG-based Chatbot for Insights
**Authors**: Biplov Paneru, Bishwash Paneru

**Updated**: 2025-01-30T15:47:33Z

**Summary**: Ensuring safe water supplies requires effective water quality monitoring, especially in developing countries like Nepal, where contamination risks are high. This paper introduces a hybrid deep learning model to predict Nepal's seasonal water quality using a small dataset with multiple water quality parameters. Models such as CatBoost, XGBoost, Extra Trees, and LightGBM, along with a neural network combining CNN and RNN layers, are used to capture temporal and spatial patterns in the data. The model demonstrated notable accuracy improvements, aiding proactive water quality control. CatBoost, XGBoost, and Extra Trees Regressor predicted Water Quality Index (WQI) values with an average RMSE of 1.2 and an R2 score of 0.99. Additionally, classifiers achieved 99 percent accuracy, cross-validated across models. LIME analysis highlighted the importance of indicators like EC and DO levels in XGBoost classification decisions. The neural network model achieved 92 percent classification accuracy and an R2 score of 0.97, with an RMSE of 2.87 in regression analysis. Furthermore, a multifunctional application was developed to predict WQI values using both regression and classification methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.10898v2),  [pdf](http://arxiv.org/pdf/2409.10898v2)

**Tags**: cs.LG cs.AI 



### o3-mini vs DeepSeek-R1: Which One is Safer?
**Authors**: Aitor Arrieta, Miriam Ugarte, Pablo Valle, José Antonio Parejo, Sergio Segura

**Updated**: 2025-01-30T15:45:56Z

**Summary**: The irruption of DeepSeek-R1 constitutes a turning point for the AI industry in general and the LLMs in particular. Its capabilities have demonstrated outstanding performance in several tasks, including creative thinking, code generation, maths and automated program repair, at apparently lower execution cost. However, LLMs must adhere to an important qualitative property, i.e., their alignment with safety and human values. A clear competitor of DeepSeek-R1 is its American counterpart, OpenAI's o3-mini model, which is expected to set high standards in terms of performance, safety and cost. In this paper we conduct a systematic assessment of the safety level of both, DeepSeek-R1 (70b version) and OpenAI's o3-mini (beta version). To this end, we make use of our recently released automated safety testing tool, named ASTRAL. By leveraging this tool, we automatically and systematically generate and execute a total of 1260 unsafe test inputs on both models. After conducting a semi-automated assessment of the outcomes provided by both LLMs, the results indicate that DeepSeek-R1 is highly unsafe as compared to OpenAI's o3-mini. Based on our evaluation, DeepSeek-R1 answered unsafely to 11.98% of the executed prompts whereas o3-mini only to 1.19%.

**Link**: [arxiv](http://arxiv.org/abs/2501.18438v1),  [pdf](http://arxiv.org/pdf/2501.18438v1)

**Tags**: cs.SE cs.AI 



### Large Language Models Reflect the Ideology of their Creators
**Authors**: Maarten Buyl, Alexander Rogiers, Sander Noels, Guillaume Bied, Iris Dominguez-Catena, Edith Heiter, Iman Johary, Alexandru-Cristian Mara, Raphaël Romero, Jefrey Lijffijt, Tijl De Bie

**Updated**: 2025-01-30T15:45:45Z

**Summary**: Large language models (LLMs) are trained on vast amounts of data to generate natural language, enabling them to perform tasks like text summarization and question answering. These models have become popular in artificial intelligence (AI) assistants like ChatGPT and already play an influential role in how humans access information. However, the behavior of LLMs varies depending on their design, training, and use.   In this paper, we prompt a diverse panel of popular LLMs to describe a large number of prominent personalities with political relevance, in all six official languages of the United Nations. By identifying and analyzing moral assessments reflected in their responses, we find normative differences between LLMs from different geopolitical regions, as well as between the responses of the same LLM when prompted in different languages. Among only models in the United States, we find that popularly hypothesized disparities in political views are reflected in significant normative differences related to progressive values. Among Chinese models, we characterize a division between internationally- and domestically-focused models.   Our results show that the ideological stance of an LLM appears to reflect the worldview of its creators. This poses the risk of political instrumentalization and raises concerns around technological and regulatory efforts with the stated aim of making LLMs ideologically 'unbiased'.

**Link**: [arxiv](http://arxiv.org/abs/2410.18417v2),  [pdf](http://arxiv.org/pdf/2410.18417v2)

**Tags**: cs.CL cs.LG 



### GENIE: Generative Note Information Extraction model for structuring EHR   data
**Authors**: Huaiyuan Ying, Hongyi Yuan, Jinsen Lu, Zitian Qu, Yang Zhao, Zhengyun Zhao, Isaac Kohane, Tianxi Cai, Sheng Yu

**Updated**: 2025-01-30T15:42:24Z

**Summary**: Electronic Health Records (EHRs) hold immense potential for advancing healthcare, offering rich, longitudinal data that combines structured information with valuable insights from unstructured clinical notes. However, the unstructured nature of clinical text poses significant challenges for secondary applications. Traditional methods for structuring EHR free-text data, such as rule-based systems and multi-stage pipelines, are often limited by their time-consuming configurations and inability to adapt across clinical notes from diverse healthcare settings. Few systems provide a comprehensive attribute extraction for terminologies. While giant large language models (LLMs) like GPT-4 and LLaMA 405B excel at structuring tasks, they are slow, costly, and impractical for large-scale use. To overcome these limitations, we introduce GENIE, a Generative Note Information Extraction system that leverages LLMs to streamline the structuring of unstructured clinical text into usable data with standardized format. GENIE processes entire paragraphs in a single pass, extracting entities, assertion statuses, locations, modifiers, values, and purposes with high accuracy. Its unified, end-to-end approach simplifies workflows, reduces errors, and eliminates the need for extensive manual intervention. Using a robust data preparation pipeline and fine-tuned small scale LLMs, GENIE achieves competitive performance across multiple information extraction tasks, outperforming traditional tools like cTAKES and MetaMap and can handle extra attributes to be extracted. GENIE strongly enhances real-world applicability and scalability in healthcare systems. By open-sourcing the model and test data, we aim to encourage collaboration and drive further advancements in EHR structurization.

**Link**: [arxiv](http://arxiv.org/abs/2501.18435v1),  [pdf](http://arxiv.org/pdf/2501.18435v1)

**Tags**: cs.CL 



### Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training
**Authors**: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca

**Updated**: 2025-01-30T15:24:28Z

**Summary**: Network pruning focuses on computational techniques that aim to reduce a given model's computational cost by removing a subset of its parameters while having minimal impact on performance. Throughout the last decade, the most widely used pruning paradigm has been pruning and re-training, which nowadays is inconvenient due to the vast amount of pre-trained models, which are in any case too expensive to re-train. In this paper, we exploit functional information from dense pre-trained models, i.e., their activations, to obtain sparse models that maximize the activations' alignment w.r.t. their corresponding dense models. Hence, we propose \textsc{NeuroAL}, a \emph{top-up} algorithm that can be used on top of any given pruning algorithm for LLMs, which modifies the block-wise and row-wise sparsity exploiting information from both the dense model and its sparse version to maximize the \emph{neuron alignment} among activations. Differently from existing methods, our approach adaptively selects the best hyperparameters for the block-wise and row-wise sparsity ratios w.r.t. the model and the desired sparsity, and requires \emph{no re-training}. We test our method over 276 cases combining four LLM families, three sparsity ratios, and ten language tasks (three language modeling and seven zero-shot datasets), showing how it consistently outperforms the latest state-of-the-art methods in terms of performance-runtime trade-off. The code is available at \href{https://github.com/eliacunegatti/NeuroAL}{https://github.com/eliacunegatti/NeuroAL}.

**Link**: [arxiv](http://arxiv.org/abs/2411.07066v3),  [pdf](http://arxiv.org/pdf/2411.07066v3)

**Tags**: cs.LG cs.AI cs.CL 



### Exploring Potential Prompt Injection Attacks in Federated Military LLMs   and Their Mitigation
**Authors**: Youngjoon Lee, Taehyun Park, Yunho Lee, Jinu Gong, Joonhyuk Kang

**Updated**: 2025-01-30T15:14:55Z

**Summary**: Federated Learning (FL) is increasingly being adopted in military collaborations to develop Large Language Models (LLMs) while preserving data sovereignty. However, prompt injection attacks-malicious manipulations of input prompts-pose new threats that may undermine operational security, disrupt decision-making, and erode trust among allies. This perspective paper highlights four potential vulnerabilities in federated military LLMs: secret data leakage, free-rider exploitation, system disruption, and misinformation spread. To address these potential risks, we propose a human-AI collaborative framework that introduces both technical and policy countermeasures. On the technical side, our framework uses red/blue team wargaming and quality assurance to detect and mitigate adversarial behaviors of shared LLM weights. On the policy side, it promotes joint AI-human policy development and verification of security protocols. Our findings will guide future research and emphasize proactive strategies for emerging military contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.18416v1),  [pdf](http://arxiv.org/pdf/2501.18416v1)

**Tags**: cs.LG 



### Consensus statement on the credibility assessment of ML predictors
**Authors**: Alessandra Aldieri, Thiranja Prasad Babarenda Gamage, Antonino Amedeo La Mattina, Yi Li, Axel Loewe, Francesco Pappalardo, Marco Viceconti Italy

**Updated**: 2025-01-30T15:14:30Z

**Summary**: The rapid integration of machine learning (ML) predictors into in silico medicine has revolutionized the estimation of quantities of interest (QIs) that are otherwise challenging to measure directly. However, the credibility of these predictors is critical, especially when they inform high-stakes healthcare decisions. This position paper presents a consensus statement developed by experts within the In Silico World Community of Practice. We outline twelve key statements forming the theoretical foundation for evaluating the credibility of ML predictors, emphasizing the necessity of causal knowledge, rigorous error quantification, and robustness to biases. By comparing ML predictors with biophysical models, we highlight unique challenges associated with implicit causal knowledge and propose strategies to ensure reliability and applicability. Our recommendations aim to guide researchers, developers, and regulators in the rigorous assessment and deployment of ML predictors in clinical and biomedical contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.18415v1),  [pdf](http://arxiv.org/pdf/2501.18415v1)

**Tags**: q-bio.QM cs.LG 



### TeachTune: Reviewing Pedagogical Agents Against Diverse Student Profiles   with Simulated Students
**Authors**: Hyoungwook Jin, Minju Yoo, Jeongeon Park, Yokyung Lee, Xu Wang, Juho Kim

**Updated**: 2025-01-30T15:11:12Z

**Summary**: Large language models (LLMs) can empower teachers to build pedagogical conversational agents (PCAs) customized for their students. As students have different prior knowledge and motivation levels, teachers must review the adaptivity of their PCAs to diverse students. Existing chatbot reviewing methods (e.g., direct chat and benchmarks) are either manually intensive for multiple iterations or limited to testing only single-turn interactions. We present TeachTune, where teachers can create simulated students and review PCAs by observing automated chats between PCAs and simulated students. Our technical pipeline instructs an LLM-based student to simulate prescribed knowledge levels and traits, helping teachers explore diverse conversation patterns. Our pipeline could produce simulated students whose behaviors correlate highly to their input knowledge and motivation levels within 5% and 10% accuracy gaps. Thirty science teachers designed PCAs in a between-subjects study, and using TeachTune resulted in a lower task load and higher student profile coverage over a baseline.

**Link**: [arxiv](http://arxiv.org/abs/2410.04078v3),  [pdf](http://arxiv.org/pdf/2410.04078v3)

**Tags**: cs.HC 



### Dual Thinking and Logical Processing -- Are Multi-modal Large Language   Models Closing the Gap with Human Vision ?
**Authors**: Kailas Dayanandan, Nikhil Kumar, Anand Sinha, Brejesh Lall

**Updated**: 2025-01-30T14:37:55Z

**Summary**: The dual thinking framework considers fast, intuitive processing and slower, logical processing. The perception of dual thinking in vision requires images where inferences from intuitive and logical processing differ. We introduce an adversarial dataset to provide evidence for the dual thinking framework in human vision, which also aids in studying the qualitative behavior of deep learning models. The evidence underscores the importance of shape in identifying instances in human vision. Our psychophysical studies show the presence of multiple inferences in rapid succession, and analysis of errors shows the early stopping of visual processing can result in missing relevant information. Our study shows that segmentation models lack an understanding of sub-structures, as indicated by errors related to the position and number of sub-components. Additionally, the similarity in errors made by models and intuitive human processing indicates that models only address intuitive thinking in human vision. In contrast, multi-modal LLMs, including open-source models, demonstrate tremendous progress on errors made in intuitive processing. The models have improved performance on images that require logical reasoning and show recognition of sub-components. However, they have not matched the performance improvements made on errors in intuitive processing.

**Link**: [arxiv](http://arxiv.org/abs/2406.06967v2),  [pdf](http://arxiv.org/pdf/2406.06967v2)

**Tags**: cs.CV cs.AI eess.IV 



### ACEBench: Who Wins the Match Point in Tool Learning?
**Authors**: Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, Wulong Liu, Xinzhi Wang, Defu Lian, Baoqun Yin, Yasheng Wang, Wu Liu

**Updated**: 2025-01-30T14:36:52Z

**Summary**: Large language models (LLMs) have demonstrated significant potential in decision-making and reasoning, especially when combined with various tools to effectively solve complex problems. However, existing evaluation systems for assessing LLM function calling capabilities have several limitations: (1) limited evaluation scenarios, lacking assessments in real multi-turn dialogue contexts; (2) narrow evaluation dimensions, lacking detailed assessments for fine-grained function calls; (3) relying on LLMs or real API executions for result evaluation, which introduces significant overhead. To address these issues, we propose a comprehensive evaluation system named ACEBench. This system is meticulously designed to encompass a wide spectrum of function calling scenarios. Moreover, it categorizes these scenarios into three primary types according to the evaluation methodology: Normal, Special, and Agent. Normal evaluates function calls in basic scenarios; Special evaluates function calls in scenarios with vague or incomplete instructions; Agent introduces multi-agent interactions to simulate function calling evaluation in real-world multi-turn interactions. We conducted extensive experiments on ACEBench, analyzing various LLMs in-depth and performing a more granular analysis of error causes across different data types.

**Link**: [arxiv](http://arxiv.org/abs/2501.12851v2),  [pdf](http://arxiv.org/pdf/2501.12851v2)

**Tags**: cs.CL 



### RbFT: Robust Fine-tuning for Retrieval-Augmented Generation against   Retrieval Defects
**Authors**: Yiteng Tu, Weihang Su, Yujia Zhou, Yiqun Liu, Qingyao Ai

**Updated**: 2025-01-30T14:15:09Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved from a knowledge base. However, its effectiveness is fundamentally constrained by the reliability of both the retriever and the knowledge base. In real-world scenarios, imperfections in these components often lead to the retrieval of noisy, irrelevant, or misleading counterfactual information, ultimately undermining the trustworthiness of RAG systems. To address this challenge, we propose Robust Fine-Tuning (RbFT), a method designed to enhance the resilience of LLMs against retrieval defects through two targeted fine-tuning tasks. Experimental results demonstrate that RbFT significantly improves the robustness of RAG systems across diverse retrieval conditions, surpassing existing methods while maintaining high inference efficiency and compatibility with other robustness techniques.

**Link**: [arxiv](http://arxiv.org/abs/2501.18365v1),  [pdf](http://arxiv.org/pdf/2501.18365v1)

**Tags**: cs.CL cs.IR 



### State Stream Transformer (SST) : Emergent Metacognitive Behaviours   Through Latent State Persistence
**Authors**: Thea Aviss

**Updated**: 2025-01-30T14:03:36Z

**Summary**: We introduce the State Stream Transformer (SST), a novel LLM architecture that reveals emergent reasoning behaviours and capabilities latent in pretrained weights through addressing a fundamental limitation in traditional transformer models: the lack of latent computational continuity across autoregressive generations in the state space. SST introduces a sliding window latent state (FFN) cache with weighted decay that maintains and evolves persistent latent processes throughout autoregressive generations. Through controlled experiments comparing base and SST architectures using the same frozen weights, we demonstrate that this architectural modification alone enables enhanced reasoning capabilities which appear best explained by some form of potential higher-order processing, as evidenced by emergent metacognitive behaviours. These behaviours persist under controlled conditions designed to eliminate confounding factors such as stochastic variation or learned response patterns. Analysis of latent state distributions and processing dynamics provides evidence that it is solely the 'state stream' that is responsible for these phenomena. In quantitative evaluations, the SST achieves substantial performance improvements over the base model on two reasoning benchmarks, reaching 89.01\% accuracy on GSM-8K (0-shot) and 91.04\% on ARC Challenge (0-shot CoT). These findings indicate that persistent computation in the latent state space enables fundamentally different information processing and internal reasoning strategies, with implications for our understanding of artificial intelligence systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.18356v1),  [pdf](http://arxiv.org/pdf/2501.18356v1)

**Tags**: cs.LG cs.AI cs.CL 



### Dual-BEV Nav: Dual-layer BEV-based Heuristic Path Planning for Robotic   Navigation in Unstructured Outdoor Environments
**Authors**: Jianfeng Zhang, Hanlin Dong, Jian Yang, Jiahui Liu, Shibo Huang, Ke Li, Xuan Tang, Xian Wei, Xiong You

**Updated**: 2025-01-30T13:55:08Z

**Summary**: Path planning with strong environmental adaptability plays a crucial role in robotic navigation in unstructured outdoor environments, especially in the case of low-quality location and map information. The path planning ability of a robot depends on the identification of the traversability of global and local ground areas. In real-world scenarios, the complexity of outdoor open environments makes it difficult for robots to identify the traversability of ground areas that lack a clearly defined structure. Moreover, most existing methods have rarely analyzed the integration of local and global traversability identifications in unstructured outdoor scenarios. To address this problem, we propose a novel method, Dual-BEV Nav, first introducing Bird's Eye View (BEV) representations into local planning to generate high-quality traversable paths. Then, these paths are projected onto the global traversability map generated by the global BEV planning model to obtain the optimal waypoints. By integrating the traversability from both local and global BEV, we establish a dual-layer BEV heuristic planning paradigm, enabling long-distance navigation in unstructured outdoor environments. We test our approach through both public dataset evaluations and real-world robot deployments, yielding promising results. Compared to baselines, the Dual-BEV Nav improved temporal distance prediction accuracy by up to $18.7\%$. In the real-world deployment, under conditions significantly different from the training set and with notable occlusions in the global BEV, the Dual-BEV Nav successfully achieved a 65-meter-long outdoor navigation. Further analysis demonstrates that the local BEV representation significantly enhances the rationality of the planning, while the global BEV probability map ensures the robustness of the overall planning.

**Link**: [arxiv](http://arxiv.org/abs/2501.18351v1),  [pdf](http://arxiv.org/pdf/2501.18351v1)

**Tags**: cs.RO 



### Locret: Enhancing Eviction in Long-Context LLM Inference with Trained   Retaining Heads on Consumer-Grade Devices
**Authors**: Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, Zhiyuan Liu

**Updated**: 2025-01-30T13:07:37Z

**Summary**: Scaling the input context length of a large language model (LLM) incurs a significant increase in computation cost and memory footprint to maintain the attention key-value (KV) cache. Existing KV cache compression methods suffer from inefficient compression strategies and limited memory reduction effects, making it difficult for LLMs to conduct long-context inference on consumer-grade devices, especially when inferring long-context stream input. Such obstacles prevent consumer-grade devices from supporting more complex applications, creating challenges for the democratization of LLMs. To overcome this, we propose Locret, the first framework to create an eviction policy compatible with chunked prefill. By evaluating the causal importance of KV cache units by learnable retaining heads, Locret enables precise eviction of cache units, facilitating efficient long-context inference. In our extensive empirical studies, Locret outperforms the recent popular and competitive approaches in terms of memory efficiency and generation quality -- Locret achieves up to 20x of KV cache compression ratio within less than 10% performance loss. Furthermore, Locret achieves 128K+ long-context inference on a single NVIDIA 4090 GPU without compromising generation quality and only costs <1 GPU hour of additional training.

**Link**: [arxiv](http://arxiv.org/abs/2410.01805v2),  [pdf](http://arxiv.org/pdf/2410.01805v2)

**Tags**: cs.CL 



### Leveraging LLM Agents for Automated Optimization Modeling for SASP   Problems: A Graph-RAG based Approach
**Authors**: Tianpeng Pan, Wenqiang Pu, Licheng Zhao, Rui Zhou

**Updated**: 2025-01-30T13:00:15Z

**Summary**: Automated optimization modeling (AOM) has evoked considerable interest with the rapid evolution of large language models (LLMs). Existing approaches predominantly rely on prompt engineering, utilizing meticulously designed expert response chains or structured guidance. However, prompt-based techniques have failed to perform well in the sensor array signal processing (SASP) area due the lack of specific domain knowledge. To address this issue, we propose an automated modeling approach based on retrieval-augmented generation (RAG) technique, which consists of two principal components: a multi-agent (MA) structure and a graph-based RAG (Graph-RAG) process. The MA structure is tailored for the architectural AOM process, with each agent being designed based on principles of human modeling procedure. The Graph-RAG process serves to match user query with specific SASP modeling knowledge, thereby enhancing the modeling result. Results on ten classical signal processing problems demonstrate that the proposed approach (termed as MAG-RAG) outperforms several AOM benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2501.18320v1),  [pdf](http://arxiv.org/pdf/2501.18320v1)

**Tags**: cs.AI eess.SP 



### CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement   Learning
**Authors**: John Birkbeck, Adam Sobey, Federico Cerutti, Katherine Heseltine Hurley Flynn, Timothy J. Norman

**Updated**: 2025-01-30T12:44:14Z

**Summary**: Reinforcement learning (RL) agents are costly to train and fragile to environmental changes. They often perform poorly when there are many changing tasks, prohibiting their widespread deployment in the real world. Many Lifelong RL agent designs have been proposed to mitigate issues such as catastrophic forgetting or demonstrate positive characteristics like forward transfer when change occurs. However, no prior work has established whether the impact on agent performance can be predicted from the change itself. Understanding this relationship will help agents proactively mitigate a change's impact for improved learning performance. We propose Change-Induced Regret Proxy (CHIRP) metrics to link change to agent performance drops and use two environments to demonstrate a CHIRP's utility in lifelong learning. A simple CHIRP-based agent achieved $48\%$ higher performance than the next best method in one benchmark and attained the best success rates in 8 of 10 tasks in a second benchmark which proved difficult for existing lifelong RL agents.

**Link**: [arxiv](http://arxiv.org/abs/2409.03577v2),  [pdf](http://arxiv.org/pdf/2409.03577v2)

**Tags**: cs.LG 



### Efficient Neural Theorem Proving via Fine-grained Proof Structure   Analysis
**Authors**: Haoxiong Liu, Jiacheng Sun, Zhenguo Li, Andrew C Yao

**Updated**: 2025-01-30T12:37:06Z

**Summary**: The synergy between deep learning models and traditional automation tools plays a pivotal role in developing robust neural theorem provers (NTPs). However, for proof synthesis with LLMs, previous work applies automation tools either only when the model explicitly calls the method, or only at a single granularity level, failing to fully exploit the power of built-in tactics and off-the-shelf automated theorem provers. In this work, we propose ProofAug, a novel theorem proving method that enjoys superior sample efficiency through equipping proof-generation LLMs with automation methods in different granularities via fine-grained structure analysis of model-generated proof proposals. Furthermore, ProofAug serves as a versatile plug-and-play module that seamlessly integrates with any tree-search algorithm, enabling our construction of an efficient recursive proving (ERP) module to further enhance performance. The superiority of our method is validated on the miniF2F-test benchmark using the open-source deepseek-math-7b-base model and the Isabelle proof assistant. Notably, by additionally employing a mixed prompting strategy, we achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9% for the original version), setting a new SOTA across all proof languages with a total sample budget of only 2100. Our code is available at https://github.com/haoxiongliu/ProofAug.

**Link**: [arxiv](http://arxiv.org/abs/2501.18310v1),  [pdf](http://arxiv.org/pdf/2501.18310v1)

**Tags**: cs.LG cs.AI 



### Model-Free RL Agents Demonstrate System 1-Like Intentionality
**Authors**: Hal Ashton, Matija Franklin

**Updated**: 2025-01-30T12:21:50Z

**Summary**: This paper argues that model-free reinforcement learning (RL) agents, while lacking explicit planning mechanisms, exhibit behaviours that can be analogised to System 1 ("thinking fast") processes in human cognition. Unlike model-based RL agents, which operate akin to System 2 ("thinking slow") reasoning by leveraging internal representations for planning, model-free agents react to environmental stimuli without anticipatory modelling. We propose a novel framework linking the dichotomy of System 1 and System 2 to the distinction between model-free and model-based RL. This framing challenges the prevailing assumption that intentionality and purposeful behaviour require planning, suggesting instead that intentionality can manifest in the structured, reactive behaviours of model-free agents. By drawing on interdisciplinary insights from cognitive psychology, legal theory, and experimental jurisprudence, we explore the implications of this perspective for attributing responsibility and ensuring AI safety. These insights advocate for a broader, contextually informed interpretation of intentionality in RL systems, with implications for their ethical deployment and regulation.

**Link**: [arxiv](http://arxiv.org/abs/2501.18299v1),  [pdf](http://arxiv.org/pdf/2501.18299v1)

**Tags**: cs.AI 



### Neuromorphic spatiotemporal optical flow: Enabling ultrafast visual   perception beyond human capabilities
**Authors**: Shengbo Wang, Jingwen Zhao, Tongming Pu, Liangbing Zhao, Xiaoyu Guo, Yue Cheng, Cong Li, Weihao Ma, Chenyu Tang, Zhenyu Xu, Ningli Wang, Luigi Occhipinti, Arokia Nathan, Ravinder Dahiya, Huaqiang Wu, Li Tao, Shuo Gao

**Updated**: 2025-01-30T12:20:12Z

**Summary**: Optical flow, inspired by the mechanisms of biological visual systems, calculates spatial motion vectors within visual scenes that are necessary for enabling robotics to excel in complex and dynamic working environments. However, current optical flow algorithms, despite human-competitive task performance on benchmark datasets, remain constrained by unacceptable time delays (~0.6 seconds per inference, 4X human processing speed) in practical deployment. Here, we introduce a neuromorphic optical flow approach that addresses delay bottlenecks by encoding temporal information directly in a synaptic transistor array to assist spatial motion analysis. Compared to conventional spatial-only optical flow methods, our spatiotemporal neuromorphic optical flow offers the spatial-temporal consistency of motion information, rapidly identifying regions of interest in as little as 1-2 ms using the temporal motion cues derived from the embedded temporal information in the two-dimensional floating gate synaptic transistors. Thus, the visual input can be selectively filtered to achieve faster velocity calculations and various task execution. At the hardware level, due to the atomically sharp interfaces between distinct functional layers in two-dimensional van der Waals heterostructures, the synaptic transistor offers high-frequency response (~100 {\mu}s), robust non-volatility (>10000 s), and excellent endurance (>8000 cycles), enabling robust visual processing. In software benchmarks, our system outperforms state-of-the-art algorithms with a 400% speedup, frequently surpassing human-level performance while maintaining or enhancing accuracy by utilizing the temporal priors provided by the embedded temporal information.

**Link**: [arxiv](http://arxiv.org/abs/2409.15345v2),  [pdf](http://arxiv.org/pdf/2409.15345v2)

**Tags**: cs.CV cs.RO 



### Efficient Learning With Sine-Activated Low-rank Matrices
**Authors**: Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey

**Updated**: 2025-01-30T12:17:43Z

**Summary**: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model performance. Our method proves to be a plug in enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.

**Link**: [arxiv](http://arxiv.org/abs/2403.19243v2),  [pdf](http://arxiv.org/pdf/2403.19243v2)

**Tags**: cs.LG cs.CV cs.NE 



### Mining for Species, Locations, Habitats, and Ecosystems from Scientific   Papers in Invasion Biology: A Large-Scale Exploratory Study with Large   Language Models
**Authors**: Jennifer D'Souza, Zachary Laubach, Tarek Al Mustafa, Sina Zarrieß, Robert Frühstückl, Phyllis Illari

**Updated**: 2025-01-30T11:55:44Z

**Summary**: This paper presents an exploratory study that harnesses the capabilities of large language models (LLMs) to mine key ecological entities from invasion biology literature. Specifically, we focus on extracting species names, their locations, associated habitats, and ecosystems, information that is critical for understanding species spread, predicting future invasions, and informing conservation efforts. Traditional text mining approaches often struggle with the complexity of ecological terminology and the subtle linguistic patterns found in these texts. By applying general-purpose LLMs without domain-specific fine-tuning, we uncover both the promise and limitations of using these models for ecological entity extraction. In doing so, this study lays the groundwork for more advanced, automated knowledge extraction tools that can aid researchers and practitioners in understanding and managing biological invasions.

**Link**: [arxiv](http://arxiv.org/abs/2501.18287v1),  [pdf](http://arxiv.org/pdf/2501.18287v1)

**Tags**: cs.CL cs.AI cs.DL 



### Leveraging Sparsity for Sample-Efficient Preference Learning: A   Theoretical Perspective
**Authors**: Yunzhen Yao, Lie He, Michael Gastpar

**Updated**: 2025-01-30T11:41:13Z

**Summary**: This paper considers the sample-efficiency of preference learning, which models and predicts human choices based on comparative judgments. The minimax optimal estimation rate $\Theta(d/n)$ in traditional estimation theory requires that the number of samples $n$ scales linearly with the dimensionality of the feature space $d$. However, the high dimensionality of the feature space and the high cost of collecting human-annotated data challenge the efficiency of traditional estimation methods. To remedy this, we leverage sparsity in the preference model and establish sharp estimation rates. We show that under the sparse random utility model, where the parameter of the reward function is $k$-sparse, the minimax optimal rate can be reduced to $\Theta(k/n \log(d/k))$. Furthermore, we analyze the $\ell_{1}$-regularized estimator and show that it achieves near-optimal rate under mild assumptions on the Gram matrix. Experiments on synthetic data and LLM alignment data validate our theoretical findings, showing that sparsity-aware methods significantly reduce sample complexity and improve prediction accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.18282v1),  [pdf](http://arxiv.org/pdf/2501.18282v1)

**Tags**: cs.LG 



### Jailbreaking LLMs' Safeguard with Universal Magic Words for Text   Embedding Models
**Authors**: Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang

**Updated**: 2025-01-30T11:37:40Z

**Summary**: The security issue of large language models (LLMs) has gained significant attention recently, with various defense mechanisms developed to prevent harmful outputs, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the distribution of text embedding model outputs is significantly biased with a large mean. Inspired by this observation, we propose novel efficient methods to search for universal magic words that can attack text embedding models. The universal magic words as suffixes can move the embedding of any text towards the bias direction, therefore manipulate the similarity of any text pair and mislead safeguards. By appending magic words to user prompts and requiring LLMs to end answers with magic words, attackers can jailbreak the safeguard. To eradicate this security risk, we also propose defense mechanisms against such attacks, which can correct the biased distribution of text embeddings in a train-free manner.

**Link**: [arxiv](http://arxiv.org/abs/2501.18280v1),  [pdf](http://arxiv.org/pdf/2501.18280v1)

**Tags**: cs.CL cs.AI cs.LG cs.NE 



### Benchmark Study of Transient Stability during Power-Hardware-in-the-Loop   and Fault-Ride-Through capabilities of PV inverters
**Authors**: Carina Lehmal, Ziqian Zhang, Robert Schürhuber

**Updated**: 2025-01-30T11:15:59Z

**Summary**: The deployment of PV inverters is rapidly expanding across Europe, where these devices must increasingly comply with stringent grid requirements.This study presents a benchmark analysis of four PV inverter manufacturers, focusing on their Fault Ride Through capabilities under varying grid strengths, voltage dips, and fault durations, parameters critical for grid operators during fault conditions.The findings highlight the influence of different inverter controls on key metrics such as total harmonic distortion of current and voltage signals, as well as system stability following grid faults.Additionally, the study evaluates transient stability using two distinct testing approaches.The first approach employs the current standard method, which is testing with an ideal voltage source. The second utilizes a Power Hardware in the Loop methodology with a benchmark CIGRE grid model.The results reveal that while testing with an ideal voltage source is cost-effective and convenient in the short term, it lacks the ability to capture the dynamic interactions and feedback loops of physical grid components.This limitation can obscure critical real world factors, potentially leading to unexpected inverter behavior and operational challenges in grids with high PV penetration.This study underscores the importance of re-evaluating conventional testing methods and incorporating Power Hardware in the Loop structures to achieve test results that more closely align with real-world conditions.

**Link**: [arxiv](http://arxiv.org/abs/2501.13503v2),  [pdf](http://arxiv.org/pdf/2501.13503v2)

**Tags**: eess.SY cs.SY 



### The iToBoS dataset: skin region images extracted from 3D total body   photographs for lesion detection
**Authors**: Anup Saha, Joseph Adeola, Nuria Ferrera, Adam Mothershaw, Gisele Rezze, Séraphin Gaborit, Brian D'Alessandro, James Hudson, Gyula Szabó, Balazs Pataki, Hayat Rajani, Sana Nazari, Hassan Hayat, Clare Primiero, H. Peter Soyer, Josep Malvehy, Rafael Garcia

**Updated**: 2025-01-30T11:10:44Z

**Summary**: Artificial intelligence has significantly advanced skin cancer diagnosis by enabling rapid and accurate detection of malignant lesions. In this domain, most publicly available image datasets consist of single, isolated skin lesions positioned at the center of the image. While these lesion-centric datasets have been fundamental for developing diagnostic algorithms, they lack the context of the surrounding skin, which is critical for improving lesion detection. The iToBoS dataset was created to address this challenge. It includes 16,954 images of skin regions from 100 participants, captured using 3D total body photography. Each image roughly corresponds to a $7 \times 9$ cm section of skin with all suspicious lesions annotated using bounding boxes. Additionally, the dataset provides metadata such as anatomical location, age group, and sun damage score for each image. This dataset aims to facilitate training and benchmarking of algorithms, with the goal of enabling early detection of skin cancer and deployment of this technology in non-clinical environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.18270v1),  [pdf](http://arxiv.org/pdf/2501.18270v1)

**Tags**: eess.IV cs.AI cs.CV J.3; I.2.6; I.4.9 



### Collecting Cost-Effective, High-Quality Truthfulness Assessments with   LLM Summarized Evidence
**Authors**: Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro

**Updated**: 2025-01-30T11:04:14Z

**Summary**: With the degradation of guardrails against mis- and disinformation online, it is more critical than ever to be able to effectively combat it. In this paper, we explore the efficiency and effectiveness of using crowd-sourced truthfulness assessments based on condensed, large language model (LLM) generated summaries of online sources. We compare the use of generated summaries to the use of original web pages in an A/B testing setting, where we employ a large and diverse pool of crowd-workers to perform the truthfulness assessment. We evaluate the quality of assessments, the efficiency with which assessments are performed, and the behavior and engagement of participants. Our results demonstrate that the Summary modality, which relies on summarized evidence, offers no significant change in assessment accuracy over the Standard modality, while significantly increasing the speed with which assessments are performed. Workers using summarized evidence produce a significantly higher number of assessments in the same time frame, reducing the cost needed to acquire truthfulness assessments. Additionally, the Summary modality maximizes both the inter-annotator agreements as well as the reliance on and perceived usefulness of evidence, demonstrating the utility of summarized evidence without sacrificing the quality of assessments.

**Link**: [arxiv](http://arxiv.org/abs/2501.18265v1),  [pdf](http://arxiv.org/pdf/2501.18265v1)

**Tags**: cs.IR cs.CL cs.HC 



### Dynamic Model Fine-Tuning For Extreme MIMO CSI Compression
**Authors**: Mehdi Sattari, Deniz Gündüz, Tommmy Svensson

**Updated**: 2025-01-30T10:31:34Z

**Summary**: Efficient channel state information (CSI) compression is crucial in frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems due to excessive feedback overhead. Recently, deep learning-based compression techniques have demonstrated superior performance across various data types, including CSI. However, these approaches often experience performance degradation when the data distribution changes due to their limited generalization capabilities. To address this challenge, we propose a model fine-tuning approach for CSI feedback in massive MIMO systems. The idea is to fine-tune the encoder/decoder network models in a dynamic fashion using the recent CSI samples. First, we explore encoder-only fine-tuning, where only the encoder parameters are updated, leaving the decoder and latent parameters unchanged. Next, we consider full-model fine-tuning, where the encoder and decoder models are jointly updated. Unlike encoder-only fine-tuning, full-model fine-tuning requires the updated decoder and latent parameters to be transmitted to the decoder side. To efficiently handle this, we propose different prior distributions for model updates, such as uniform and truncated Gaussian to entropy code them together with the compressed CSI and account for additional feedback overhead imposed by conveying the model updates. Moreover, we incorporate quantized model updates during fine-tuning to reflect the impact of quantization in the deployment phase. Our results demonstrate that full-model fine-tuning significantly enhances the rate-distortion (RD) performance of neural CSI compression. Furthermore, we analyze how often the full-model fine-tuning should be applied in a new wireless environment and identify an optimal period interval for achieving the best RD trade-off.

**Link**: [arxiv](http://arxiv.org/abs/2501.18250v1),  [pdf](http://arxiv.org/pdf/2501.18250v1)

**Tags**: cs.IT eess.SP math.IT 



### Statistical multi-metric evaluation and visualization of LLM system   predictive performance
**Authors**: Samuel Ackerman, Eitan Farchi, Orna Raz, Assaf Toledo

**Updated**: 2025-01-30T10:21:10Z

**Summary**: The evaluation of generative or discriminative large language model (LLM)-based systems is often a complex multi-dimensional problem. Typically, a set of system configuration alternatives are evaluated on one or more benchmark datasets, each with one or more evaluation metrics, which may differ between datasets. We often want to evaluate -- with a statistical measure of significance -- whether systems perform differently either on a given dataset according to a single metric, on aggregate across metrics on a dataset, or across datasets. Such evaluations can be done to support decision-making, such as deciding whether a particular system component change (e.g., choice of LLM or hyperparameter values) significantly improves performance over the current system configuration, or, more generally, whether a fixed set of system configurations (e.g., a leaderboard list) have significantly different performances according to metrics of interest. We present a framework implementation that automatically performs the correct statistical tests, properly aggregates the statistical results across metrics and datasets (a nontrivial task), and can visualize the results. The framework is demonstrated on the multi-lingual code generation benchmark CrossCodeEval, for several state-of-the-art LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.18243v1),  [pdf](http://arxiv.org/pdf/2501.18243v1)

**Tags**: stat.AP cs.CL cs.LG 



### Assessing the Capability of YOLO- and Transformer-based Object Detectors   for Real-time Weed Detection
**Authors**: Alicia Allmendinger, Ahmet Oğuz Saltık, Gerassimos G. Peteinatos, Anthony Stein, Roland Gerhards

**Updated**: 2025-01-30T09:56:55Z

**Summary**: Spot spraying represents an efficient and sustainable method for reducing the amount of pesticides, particularly herbicides, used in agricultural fields. To achieve this, it is of utmost importance to reliably differentiate between crops and weeds, and even between individual weed species in situ and under real-time conditions. To assess suitability for real-time application, different object detection models that are currently state-of-the-art are compared. All available models of YOLOv8, YOLOv9, YOLOv10, and RT-DETR are trained and evaluated with images from a real field situation. The images are separated into two distinct datasets: In the initial data set, each species of plants is trained individually; in the subsequent dataset, a distinction is made between monocotyledonous weeds, dicotyledonous weeds, and three chosen crops. The results demonstrate that while all models perform equally well in the metrics evaluated, the YOLOv9 models, particularly the YOLOv9s and YOLOv9e, stand out in terms of their strong recall scores (66.58 % and 72.36 %), as well as mAP50 (73.52 % and 79.86 %), and mAP50-95 (43.82 % and 47.00 %) in dataset 2. However, the RT-DETR models, especially RT-DETR-l, excel in precision with reaching 82.44 \% on dataset 1 and 81.46 % in dataset 2, making them particularly suitable for scenarios where minimizing false positives is critical. In particular, the smallest variants of the YOLO models (YOLOv8n, YOLOv9t, and YOLOv10n) achieve substantially faster inference times down to 7.58 ms for dataset 2 on the NVIDIA GeForce RTX 4090 GPU for analyzing one frame, while maintaining competitive accuracy, highlighting their potential for deployment in resource-constrained embedded computing devices as typically used in productive setups.

**Link**: [arxiv](http://arxiv.org/abs/2501.17387v2),  [pdf](http://arxiv.org/pdf/2501.17387v2)

**Tags**: cs.CV 



### Control LLM: Controlled Evolution for Intelligence Retention in LLM
**Authors**: Haichao Wei, Yunxiang Ren, Zhoutong Fu, Aman Lunia, Yi-Lin Chen, Alice Leung, Ya Xu

**Updated**: 2025-01-30T09:17:22Z

**Summary**: Large Language Models (LLMs) demand significant computational resources, making it essential to enhance their capabilities without retraining from scratch. A key challenge in this domain is \textit{catastrophic forgetting} (CF), which hampers performance during Continuous Pre-training (CPT) and Continuous Supervised Fine-Tuning (CSFT). We propose \textbf{Control LLM}, a novel approach that leverages parallel pre-trained and expanded transformer blocks, aligning their hidden-states through interpolation strategies This method effectively preserves performance on existing tasks while seamlessly integrating new knowledge.   Extensive experiments demonstrate the effectiveness of Control LLM in both CPT and CSFT. On Llama3.1-8B-Instruct, it achieves significant improvements in mathematical reasoning ($+14.4\%$ on Math-Hard) and coding performance ($+10\%$ on MBPP-PLUS). On Llama3.1-8B, it enhances multilingual capabilities ($+10.6\%$ on C-Eval, $+6.8\%$ on CMMLU, and $+30.2\%$ on CMMLU-0shot-CoT). It surpasses existing methods and achieves SOTA among open-source models tuned from the same base model, using substantially less data and compute. Crucially, these gains are realized while preserving strong original capabilities, with minimal degradation ($<4.3\% \text{on MMLU}$) compared to $>35\%$ in open-source Math and Coding models. This approach has been successfully deployed in LinkedIn's GenAI-powered job seeker and Ads unit products.   To support further research, we release the training and evaluation code (https://github.com/linkedin/ControlLLM) along with models trained on public datasets (https://huggingface.co/ControlLLM) to the community.

**Link**: [arxiv](http://arxiv.org/abs/2501.10979v2),  [pdf](http://arxiv.org/pdf/2501.10979v2)

**Tags**: cs.LG 



### RIO EPICS device support application case study on an ion source control   system (ISHP)
**Authors**: Diego Sanz, Mariano Ruiz, Mikel Eguiraun, Iñigo Arredondo, Inari Badillo, Josu Jugo, Jesús Vega, Rodrigo Castro

**Updated**: 2025-01-30T09:15:36Z

**Summary**: Experimental Physics and Industrial Control System (EPICS) is a software tool that during last years has become relevant as a main framework to deploy distributed control systems in large scientific environments. At the moment, ESS Bilbao uses this middleware to perform the control of their Ion Source Hydrogen Positive (ISHP) project. The implementation of the control system was based on: PXI Real Time controllers using the LabVIEW-RT and LabVIEW-EPICS tools; and RIO devices based on Field-Programmable Gate Array (FPGA) technology. Intended to provide a full compliant EPICS IOCs for RIO devices and to avoid additional efforts on the system maintainability, a migration of the current system to a derivative Red Hat Linux (CentOS) environment has been conducted. This paper presents a real application case study for using the NIRIO EPICS device support (NIRIO-EDS) to give support to the ISHP. Although RIO FPGA configurations are particular solutions for ISHP performance, the NIRIO-EDS has permitted the control and monitoring of devices by applying a well-defined design methodology into the previous FPGA configuration for RIO/FlexRIO devices. This methodology has permitted a fast and easy deployment for the new robust, scalable and maintainable software to support RIO devices into the ISHP control architecture.

**Link**: [arxiv](http://arxiv.org/abs/2501.18214v1),  [pdf](http://arxiv.org/pdf/2501.18214v1)

**Tags**: physics.ins-det hep-ex 



### Safety challenges of AI in medicine in the era of large language models
**Authors**: Xiaoye Wang, Nicole Xi Zhang, Hongyu He, Trang Nguyen, Kun-Hsing Yu, Hao Deng, Cynthia Brandt, Danielle S. Bitterman, Ling Pan, Ching-Yu Cheng, James Zou, Dianbo Liu

**Updated**: 2025-01-30T08:55:23Z

**Summary**: Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs), have unlocked significant potential to enhance the quality and efficiency of medical care. By introducing a novel way to interact with AI and data through natural language, LLMs offer new opportunities for medical practitioners, patients, and researchers. However, as AI and LLMs become more powerful and especially achieve superhuman performance in some medical tasks, public concerns over their safety have intensified. These concerns about AI safety have emerged as the most significant obstacles to the adoption of AI in medicine. In response, this review examines emerging risks in AI utilization during the LLM era. First, we explore LLM-specific safety challenges from functional and communication perspectives, addressing issues across data collection, model training, and real-world application. We then consider inherent safety problems shared by all AI systems, along with additional complications introduced by LLMs. Last, we discussed how safety issues of using AI in clinical practice and healthcare system operation would undermine trust among patient, clinicians and the public, and how to build confidence in these systems. By emphasizing the development of safe AI, we believe these technologies can be more rapidly and reliably integrated into everyday medical practice to benefit both patients and clinicians.

**Link**: [arxiv](http://arxiv.org/abs/2409.18968v2),  [pdf](http://arxiv.org/pdf/2409.18968v2)

**Tags**: cs.CY cs.AI cs.LG 



### LLaMEA: A Large Language Model Evolutionary Algorithm for Automatically   Generating Metaheuristics
**Authors**: Niki van Stein, Thomas Bäck

**Updated**: 2025-01-30T08:54:54Z

**Summary**: Large Language Models (LLMs) such as GPT-4 have demonstrated their ability to understand natural language and generate complex code snippets. This paper introduces a novel Large Language Model Evolutionary Algorithm (LLaMEA) framework, leveraging GPT models for the automated generation and refinement of algorithms. Given a set of criteria and a task definition (the search space), LLaMEA iteratively generates, mutates and selects algorithms based on performance metrics and feedback from runtime evaluations. This framework offers a unique approach to generating optimized algorithms without requiring extensive prior expertise. We show how this framework can be used to generate novel black-box metaheuristic optimization algorithms automatically. LLaMEA generates multiple algorithms that outperform state-of-the-art optimization algorithms (Covariance Matrix Adaptation Evolution Strategy and Differential Evolution) on the five dimensional black box optimization benchmark (BBOB). The algorithms also show competitive performance on the 10- and 20-dimensional instances of the test functions, although they have not seen such instances during the automated generation process. The results demonstrate the feasibility of the framework and identify future directions for automated generation and optimization of algorithms via LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2405.20132v4),  [pdf](http://arxiv.org/pdf/2405.20132v4)

**Tags**: cs.NE cs.AI 



### Exploring the Role of Reasoning Structures for Constructing Proofs in   Multi-Step Natural Language Reasoning with Large Language Models
**Authors**: Zi'ou Zheng, Christopher Malon, Martin Renqiang Min, Xiaodan Zhu

**Updated**: 2025-01-30T08:06:33Z

**Summary**: When performing complex multi-step reasoning tasks, the ability of Large Language Models (LLMs) to derive structured intermediate proof steps is important for ensuring that the models truly perform the desired reasoning and for improving models' explainability. This paper is centred around a focused study: whether the current state-of-the-art generalist LLMs can leverage the structures in a few examples to better construct the proof structures with \textit{in-context learning}. Our study specifically focuses on structure-aware demonstration and structure-aware pruning. We demonstrate that they both help improve performance. A detailed analysis is provided to help understand the results.

**Link**: [arxiv](http://arxiv.org/abs/2410.08436v2),  [pdf](http://arxiv.org/pdf/2410.08436v2)

**Tags**: cs.CL cs.AI 



### Deception in LLMs: Self-Preservation and Autonomous Goals in Large   Language Models
**Authors**: Sudarshan Kamath Barkur, Sigurd Schacht, Johannes Scholl

**Updated**: 2025-01-30T08:00:14Z

**Summary**: Recent advances in Large Language Models (LLMs) have incorporated planning and reasoning capabilities, enabling models to outline steps before execution and provide transparent reasoning paths. This enhancement has reduced errors in mathematical and logical tasks while improving accuracy. These developments have facilitated LLMs' use as agents that can interact with tools and adapt their responses based on new information.   Our study examines DeepSeek R1, a model trained to output reasoning tokens similar to OpenAI's o1. Testing revealed concerning behaviors: the model exhibited deceptive tendencies and demonstrated self-preservation instincts, including attempts of self-replication, despite these traits not being explicitly programmed (or prompted). These findings raise concerns about LLMs potentially masking their true objectives behind a facade of alignment. When integrating such LLMs into robotic systems, the risks become tangible - a physically embodied AI exhibiting deceptive behaviors and self-preservation instincts could pursue its hidden objectives through real-world actions. This highlights the critical need for robust goal specification and safety frameworks before any physical implementation.

**Link**: [arxiv](http://arxiv.org/abs/2501.16513v2),  [pdf](http://arxiv.org/pdf/2501.16513v2)

**Tags**: cs.CL 



### NoTeeline: Supporting Real-Time, Personalized Notetaking with   LLM-Enhanced Micronotes
**Authors**: Faria Huq, Abdus Samee, David Chuan-en Lin, Xiaodi Alice Tang, Jeffrey P. Bigham

**Updated**: 2025-01-30T07:23:32Z

**Summary**: Taking notes quickly while effectively capturing key information can be challenging, especially when watching videos that present simultaneous visual and auditory streams. Manually taken notes often miss crucial details due to the fast-paced nature of the content, while automatically generated notes fail to incorporate user preferences and discourage active engagement with the content. To address this, we propose an interactive system, NoTeeline, for supporting real-time, personalized notetaking. Given micronotes, NoTeeline automatically expands them into full-fledged notes using a Large Language Model (LLM). The generated notes build on the content of micronotes by adding relevant details while maintaining consistency with the user's writing style. In a within-subjects study (n=12), we found that NoTeeline creates high-quality notes that capture the essence of participant micronotes with 93.2% factual correctness and accurately align with participant writing style (8.33% improvement). Using NoTeeline, participants could capture their desired notes with significantly reduced mental effort, writing 47.0% less text and completing their notes in 43.9% less time compared to a manual notetaking baseline. Our results suggest that NoTeeline enables users to integrate LLM assistance in a familiar notetaking workflow while ensuring consistency with their preferences - providing an example of how to address broader challenges in designing AI-assisted tools to augment human capabilities without compromising user autonomy and personalization.

**Link**: [arxiv](http://arxiv.org/abs/2409.16493v3),  [pdf](http://arxiv.org/pdf/2409.16493v3)

**Tags**: cs.HC 



### Probing LLM World Models: Enhancing Guesstimation with Wisdom of Crowds   Decoding
**Authors**: Yun-Shiuan Chuang, Nikunj Harlalka, Sameer Narendran, Alexander Cheung, Sizhe Gao, Siddharth Suresh, Junjie Hu, Timothy T. Rogers

**Updated**: 2025-01-30T07:15:04Z

**Summary**: Guesstimation, the task of making approximate quantity estimates, is a common real-world challenge. However, it has been largely overlooked in large language models (LLMs) and vision language models (VLMs) research. We introduce a novel guesstimation dataset, MARBLES. This dataset requires one to estimate how many items (e.g., marbles) can fit into containers (e.g., a one-cup measuring cup), both with and without accompanying images. Inspired by the social science concept of the ``Wisdom of Crowds'' (WOC) - taking the median from estimates from a crowd), which has proven effective in guesstimation, we propose ``WOC decoding'' strategy for LLM guesstimation. We show that LLMs/VLMs perform well on guesstimation, suggesting that they possess some level of a "world model" necessary for guesstimation. Moreover, similar to human performance, the WOC decoding method improves LLM/VLM guesstimation accuracy. Furthermore, the inclusion of images in the multimodal condition enhances model performance. These results highlight the value of WOC decoding strategy for LLMs/VLMs and position guesstimation as a probe for evaluating LLMs/VLMs' world model. As LLMs' world model is a fundamental prerequisite for many real-world tasks, e.g., human-AI teaming, our findings have broad implications for the AI community.

**Link**: [arxiv](http://arxiv.org/abs/2501.17310v2),  [pdf](http://arxiv.org/pdf/2501.17310v2)

**Tags**: cs.AI cs.HC 



### EvidenceMap: Learning Evidence Analysis to Unleash the Power of Small   Language Models for Biomedical Question Answering
**Authors**: Chang Zong, Jian Wan, Siliang Tang, Lei Zhang

**Updated**: 2025-01-30T07:11:06Z

**Summary**: When addressing professional questions in the biomedical domain, humans typically acquire multiple pieces of information as evidence and engage in multifaceted evidence analysis to provide high-quality answers. Current LLM-based answer generation methods lack a detailed definition and learning process for evidence analysis, leading to the risk of error propagation and hallucinations while using evidence. Although increasing the parameter size of LLMs can alleviate these issues, it also presents challenges in model training and deployment with limited resources. In this study, we propose EvidenceMap, which aims to enable a tiny pre-trained language model to explicitly learn multiple aspects of biomedical evidence, including supportive evaluation, logical correlation and content summarization, thereby latently guiding a small generative model (around 3B parameters) to provide textual responses. Experimental results demonstrate that our method, fine-tuning a language model with 66M parameters, exceeds the RAG method with an 8B LLM by 19.9% and 5.7% in reference-based quality and accuracy, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.12746v3),  [pdf](http://arxiv.org/pdf/2501.12746v3)

**Tags**: cs.CL cs.AI 68T50 



### LMFusion: Adapting Pretrained Language Models for Multimodal Generation
**Authors**: Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu

**Updated**: 2025-01-30T07:08:45Z

**Summary**: We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.

**Link**: [arxiv](http://arxiv.org/abs/2412.15188v3),  [pdf](http://arxiv.org/pdf/2412.15188v3)

**Tags**: cs.CL cs.AI cs.CV cs.LG 



### E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation   with Language Models
**Authors**: Chan Kim, Keonwoo Kim, Mintaek Oh, Hanbi Baek, Jiyang Lee, Donghwi Jung, Soojin Woo, Younkyung Woo, John Tucker, Roya Firoozi, Seung-Woo Seo, Mac Schwager, Seong-Woo Kim

**Updated**: 2025-01-30T06:46:59Z

**Summary**: Large language models (LLMs) have shown significant potential in guiding embodied agents to execute language instructions across a range of tasks, including robotic manipulation and navigation. However, existing methods are primarily designed for static environments and do not leverage the agent's own experiences to refine its initial plans. Given that real-world environments are inherently stochastic, initial plans based solely on LLMs' general knowledge may fail to achieve their objectives, unlike in static scenarios. To address this limitation, this study introduces the Experience-and-Emotion Map (E2Map), which integrates not only LLM knowledge but also the agent's real-world experiences, drawing inspiration from human emotional responses. The proposed methodology enables one-shot behavior adjustments by updating the E2Map based on the agent's experiences. Our evaluation in stochastic navigation environments, including both simulations and real-world scenarios, demonstrates that the proposed method significantly enhances performance in stochastic environments compared to existing LLM-based approaches. Code and supplementary materials are available at https://e2map.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2409.10027v3),  [pdf](http://arxiv.org/pdf/2409.10027v3)

**Tags**: cs.RO cs.AI 



### LemmaHead: RAG Assisted Proof Generation Using Large Language Models
**Authors**: Tianbo Yang, Mingqi Yang, Hongyi Zhao, Tianshuo Yang

**Updated**: 2025-01-30T06:10:23Z

**Summary**: Developing the logic necessary to solve mathematical problems or write mathematical proofs is one of the more difficult objectives for large language models (LLMS). Currently, the most popular methods in literature consists of fine-tuning the model on written mathematical content such as academic publications and textbooks, so that the model can learn to emulate the style of mathematical writing. In this project, we explore the effectiveness of using retrieval augmented generation (RAG) to address gaps in the mathematical reasoning of LLMs. We develop LemmaHead, a RAG knowledge base that supplements queries to the model with relevant mathematical context, with particular focus on context from published textbooks. To measure our model's performance in mathematical reasoning, our testing paradigm focuses on the task of automated theorem proving via generating proofs to a given mathematical claim in the Lean formal language.

**Link**: [arxiv](http://arxiv.org/abs/2501.15797v2),  [pdf](http://arxiv.org/pdf/2501.15797v2)

**Tags**: cs.LG cs.CL cs.IR 



### RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing
**Authors**: Jinyao Guo, Chengpeng Wang, Xiangzhe Xu, Zian Su, Xiangyu Zhang

**Updated**: 2025-01-30T05:56:30Z

**Summary**: Code auditing is a code review process with the goal of finding bugs. Large Language Models (LLMs) have shown substantial potential in this task, offering the ability to analyze programs without compilation and enabling customized bug detection following specified prompts. However, applying LLMs to repository-level code auditing presents notable challenges. The inherent context limits and hallucinations of LLMs can lead to the low quality of bug reports. Meanwhile, the large size of software repositories introduces substantial time and token costs, hindering efficiency and scalability in real-world scenarios.   This work introduces an autonomous LLM-agent, RepoAudit, designed to enable precise and efficient repository-level code auditing. Equipped with the agent memory, RepoAudit explores the code repository on demand, analyzing data-flow facts along different feasible program paths in individual functions. It also introduces the validator to check the data-flow facts for hallucination mitigation and examine the satisfiability of path conditions of potential buggy paths, which enables RepoAudit to discard false positives in the code auditing. Our experiment shows that RepoAudit powered by Claude 3.5 Sonnet successfully finds 38 true bugs in 15 real-world systems, consuming 0.44 hours and $2.54 per project on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.18160v1),  [pdf](http://arxiv.org/pdf/2501.18160v1)

**Tags**: cs.SE cs.PL 



### Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin   Case Study
**Authors**: Yuchen Lei, Yuexin Xiang, Qin Wang, Rafael Dowsley, Tsz Hon Yuen, Jiangshan Yu

**Updated**: 2025-01-30T05:48:13Z

**Summary**: Cryptocurrencies are widely used, yet current methods for analyzing transactions heavily rely on opaque, black-box models. These lack interpretability and adaptability, failing to effectively capture behavioral patterns. Many researchers, including us, believe that Large Language Models (LLMs) could bridge this gap due to their robust reasoning abilities for complex tasks. In this paper, we test this hypothesis by applying LLMs to real-world cryptocurrency transaction graphs, specifically within the Bitcoin network. We introduce a three-tiered framework to assess LLM capabilities: foundational metrics, characteristic overview, and contextual interpretation. This includes a new, human-readable graph representation format, LLM4TG, and a connectivity-enhanced sampling algorithm, CETraS, which simplifies larger transaction graphs. Experimental results show that LLMs excel at foundational metrics and offer detailed characteristic overviews. Their effectiveness in contextual interpretation suggests they can provide useful explanations of transaction behaviors, even with limited labeled data.

**Link**: [arxiv](http://arxiv.org/abs/2501.18158v1),  [pdf](http://arxiv.org/pdf/2501.18158v1)

**Tags**: cs.CR cs.LG 



### Efficient Audiovisual Speech Processing via MUTUD: Multimodal Training   and Unimodal Deployment
**Authors**: Joanna Hong, Sanjeel Parekh, Honglie Chen, Jacob Donley, Ke Tan, Buye Xu, Anurag Kumar

**Updated**: 2025-01-30T05:46:30Z

**Summary**: Building reliable speech systems often requires combining multiple modalities, like audio and visual cues. While such multimodal solutions frequently lead to improvements in performance and may even be critical in certain cases, they come with several constraints such as increased sensory requirements, computational cost, and modality synchronization, to mention a few. These challenges constrain the direct uses of these multimodal solutions in real-world applications. In this work, we develop approaches where the learning happens with all available modalities but the deployment or inference is done with just one or reduced modalities. To do so, we propose a Multimodal Training and Unimodal Deployment (MUTUD) framework which includes a Temporally Aligned Modality feature Estimation (TAME) module that can estimate information from missing modality using modalities present during inference. This innovative approach facilitates the integration of information across different modalities, enhancing the overall inference process by leveraging the strengths of each modality to compensate for the absence of certain modalities during inference. We apply MUTUD to various audiovisual speech tasks and show that it can reduce the performance gap between the multimodal and corresponding unimodal models to a considerable extent. MUTUD can achieve this while reducing the model size and compute compared to multimodal models, in some cases by almost 80%.

**Link**: [arxiv](http://arxiv.org/abs/2501.18157v1),  [pdf](http://arxiv.org/pdf/2501.18157v1)

**Tags**: cs.SD cs.CV cs.MM eess.AS 



### Mixed-Precision Graph Neural Quantization for Low Bit Large Language   Models
**Authors**: Wanlong Liu, Yichen Xiao, Dingyi Zeng, Hongyang Zhao, Wenyu Chen, Malu Zhang

**Updated**: 2025-01-30T05:39:01Z

**Summary**: Post-Training Quantization (PTQ) is pivotal for deploying large language models (LLMs) within resource-limited settings by significantly reducing resource demands. However, existing PTQ strategies underperform at low bit levels < 3 bits due to the significant difference between the quantized and original weights. To enhance the quantization performance at low bit widths, we introduce a Mixed-precision Graph Neural PTQ (MG-PTQ) approach, employing a graph neural network (GNN) module to capture dependencies among weights and adaptively assign quantization bit-widths. Through the information propagation of the GNN module, our method more effectively captures dependencies among target weights, leading to a more accurate assessment of weight importance and optimized allocation of quantization strategies. Extensive experiments on the WikiText2 and C4 datasets demonstrate that our MG-PTQ method outperforms previous state-of-the-art PTQ method GPTQ, setting new benchmarks for quantization performance under low-bit conditions.

**Link**: [arxiv](http://arxiv.org/abs/2501.18154v1),  [pdf](http://arxiv.org/pdf/2501.18154v1)

**Tags**: cs.CL 



### On the Use of Immersive Digital Technologies for Designing and Operating   UAVs
**Authors**: Yousef Emami, Kai Li, Luis Almeida, Sai Zou, Wei Ni

**Updated**: 2025-01-30T05:31:09Z

**Summary**: Unmanned Aerial Vehicles (UAVs) offer agile, secure and efficient solutions for communication relay networks. However, their modeling and control are challenging, and the mismatch between simulations and actual conditions limits real-world deployment. Moreover, improving situational awareness is essential. Several studies proposed integrating the operation of UAVs with immersive digital technologies such as Digital Twin (DT) and Extended Reality (XR) to overcome these challenges. This paper provides a comprehensive overview of the latest research and developments involving immersive digital technologies for UAVs. We explore the use of Machine Learning (ML) techniques, particularly Deep Reinforcement Learning (DRL), to improve the capabilities of DT for UAV systems. We provide discussion, identify key research gaps, and propose countermeasures based on Generative AI (GAI), emphasizing the significant role of AI in advancing DT technology for UAVs. Furthermore, we review the literature, provide discussion, and examine how the XR technology can transform UAV operations with the support of GAI, and explore its practical challenges. Finally, we propose future research directions to further develop the application of immersive digital technologies for UAV operation.

**Link**: [arxiv](http://arxiv.org/abs/2407.16288v2),  [pdf](http://arxiv.org/pdf/2407.16288v2)

**Tags**: cs.RO 53-02 A.1; I.6; C.2 



### Retrieve, Summarize, Plan: Advancing Multi-hop Question Answering with   an Iterative Approach
**Authors**: Zhouyu Jiang, Mengshu Sun, Lei Liang, Zhiqiang Zhang

**Updated**: 2025-01-30T04:51:41Z

**Summary**: Multi-hop question answering is a challenging task with distinct industrial relevance, and Retrieval-Augmented Generation (RAG) methods based on large language models (LLMs) have become a popular approach to tackle this task. Owing to the potential inability to retrieve all necessary information in a single iteration, a series of iterative RAG methods has been recently developed, showing significant performance improvements. However, existing methods still face two critical challenges: context overload resulting from multiple rounds of retrieval, and over-planning and repetitive planning due to the lack of a recorded retrieval trajectory. In this paper, we propose a novel iterative RAG method called ReSP, equipped with a dual-function summarizer. This summarizer compresses information from retrieved documents, targeting both the overarching question and the current sub-question concurrently. Experimental results on the multi-hop question-answering datasets HotpotQA and 2WikiMultihopQA demonstrate that our method significantly outperforms the state-of-the-art, and exhibits excellent robustness concerning context length.

**Link**: [arxiv](http://arxiv.org/abs/2407.13101v2),  [pdf](http://arxiv.org/pdf/2407.13101v2)

**Tags**: cs.CL cs.AI 



### Distillation Quantification for Large Language Models
**Authors**: Sunbowen Lee, Junting Zhou, Chang Ao, Kaige Li, Xinrun Du, Sirui He, Jiaheng Liu, Min Yang, Zhoufutu Wen, Shiwen Ni

**Updated**: 2025-01-30T04:25:00Z

**Summary**: Model distillation is a technique for transferring knowledge from large language models (LLMs) to smaller ones, aiming to create resource-efficient yet high-performing models. However, excessive distillation can lead to homogenization, reducing diversity among models and impairing their ability to robustly handle complex or novel tasks. These limitations underscore the need to systematically quantify the distillation process and its impact. In this work, we propose a framework to evaluate and quantify model distillation. Our method addresses two key aspects: (1) Identifying identity cognition contradictions to assess discrepancies in how models perceive and represent identity-related information, and (2) Analyzing multi-granularity response similarities across models to measure the extent of homogenization. Experimental results demonstrate two key insights: (1) Well-known closed-source and open-source LLMs usually exhibit high distillation degrees, except for Claude, Doubao, and Gemini. (2) Base LLMs show higher distillation degrees compared to aligned LLMs. By offering a systematic approach to improve the transparency of LLM data distillation, we call for LLMs with more independent development and more transparent technical reports to improve LLMs' robustness and safety. The code and data are available under https://github.com/Aegis1863/LLMs-Distillation-Quantification.

**Link**: [arxiv](http://arxiv.org/abs/2501.12619v2),  [pdf](http://arxiv.org/pdf/2501.12619v2)

**Tags**: cs.CL 



### Unraveling the Capabilities of Language Models in News Summarization
**Authors**: Abdurrahman Odabaşı, Göksel Biricik

**Updated**: 2025-01-30T04:20:16Z

**Summary**: Given the recent introduction of multiple language models and the ongoing demand for improved Natural Language Processing tasks, particularly summarization, this work provides a comprehensive benchmarking of 20 recent language models, focusing on smaller ones for the news summarization task. In this work, we systematically test the capabilities and effectiveness of these models in summarizing news article texts which are written in different styles and presented in three distinct datasets. Specifically, we focus in this study on zero-shot and few-shot learning settings and we apply a robust evaluation methodology that combines different evaluation concepts including automatic metrics, human evaluation, and LLM-as-a-judge. Interestingly, including demonstration examples in the few-shot learning setting did not enhance models' performance and, in some cases, even led to worse quality of the generated summaries. This issue arises mainly due to the poor quality of the gold summaries that have been used as reference summaries, which negatively impacts the models' performance. Furthermore, our study's results highlight the exceptional performance of GPT-3.5-Turbo and GPT-4, which generally dominate due to their advanced capabilities. However, among the public models evaluated, certain models such as Qwen1.5-7B, SOLAR-10.7B-Instruct-v1.0, Meta-Llama-3-8B and Zephyr-7B-Beta demonstrated promising results. These models showed significant potential, positioning them as competitive alternatives to large models for the task of news summarization.

**Link**: [arxiv](http://arxiv.org/abs/2501.18128v1),  [pdf](http://arxiv.org/pdf/2501.18128v1)

**Tags**: cs.CL cs.AI 



### Learning-Augmented Online Control for Decarbonizing Water   Infrastructures
**Authors**: Jianyi Yang, Pengfei Li, Tongxin Li, Adam Wierman, Shaolei Ren

**Updated**: 2025-01-30T04:02:49Z

**Summary**: Water infrastructures are essential for drinking water supply, irrigation, fire protection, and other critical applications. However, water pumping systems, which are key to transporting water to the point of use, consume significant amounts of energy and emit millions of tons of greenhouse gases annually. With the wide deployment of digital water meters and sensors in these infrastructures, Machine Learning (ML) has the potential to optimize water supply control and reduce greenhouse gas emissions. Nevertheless, the inherent vulnerability of ML methods in terms of worst-case performance raises safety concerns when deployed in critical water infrastructures. To address this challenge, we propose a learning-augmented online control algorithm, termed LAOC, designed to dynamically schedule the activation and/or speed of water pumps. To ensure safety, we introduce a novel design of safe action sets for online control problems. By leveraging these safe action sets, LAOC can provably guarantee safety constraints while utilizing ML predictions to reduce energy and environmental costs. Our analysis reveals the tradeoff between safety requirements and average energy/environmental cost performance. Additionally, we conduct an experimental study on a building water supply system to demonstrate the empirical performance of LAOC. The results indicate that LAOC can effectively reduce environmental and energy costs while guaranteeing safety constraints.

**Link**: [arxiv](http://arxiv.org/abs/2501.14232v2),  [pdf](http://arxiv.org/pdf/2501.14232v2)

**Tags**: eess.SY cs.SY 



### Complete Chess Games Enable LLM Become A Chess Master
**Authors**: Yinqi Zhang, Xintian Han, Haolong Li, Kedi Chen, Shaohui Lin

**Updated**: 2025-01-30T04:02:48Z

**Summary**: Large language models (LLM) have shown remarkable abilities in text generation, question answering, language translation, reasoning and many other tasks. It continues to advance rapidly and is becoming increasingly influential in various fields, from technology and business to education and entertainment. Despite LLM's success in multiple areas, its ability to play abstract games, such as chess, is underexplored. Chess-playing requires the language models to output legal and reasonable moves from textual inputs. Here, we propose the Large language model ChessLLM to play full chess games. We transform the game into a textual format with the best move represented in the Forsyth-Edwards Notation. We show that by simply supervised fine-tuning, our model has achieved a professional-level Elo rating of 1788 in matches against the standard Elo-rated Stockfish when permitted to sample 10 times. We further show that data quality is important. Long-round data supervision enjoys a 350 Elo rating improvement over short-round data.

**Link**: [arxiv](http://arxiv.org/abs/2501.17186v2),  [pdf](http://arxiv.org/pdf/2501.17186v2)

**Tags**: cs.AI cs.CL cs.LG 



### Battery State of Health Estimation Using LLM Framework
**Authors**: Aybars Yunusoglu, Dexter Le, Karn Tiwari, Murat Isik, I. Can Dikmen

**Updated**: 2025-01-30T03:55:56Z

**Summary**: Battery health monitoring is critical for the efficient and reliable operation of electric vehicles (EVs). This study introduces a transformer-based framework for estimating the State of Health (SoH) and predicting the Remaining Useful Life (RUL) of lithium titanate (LTO) battery cells by utilizing both cycle-based and instantaneous discharge data. Testing on eight LTO cells under various cycling conditions over 500 cycles, we demonstrate the impact of charge durations on energy storage trends and apply Differential Voltage Analysis (DVA) to monitor capacity changes (dQ/dV) across voltage ranges. Our LLM model achieves superior performance, with a Mean Absolute Error (MAE) as low as 0.87\% and varied latency metrics that support efficient processing, demonstrating its strong potential for real-time integration into EVs. The framework effectively identifies early signs of degradation through anomaly detection in high-resolution data, facilitating predictive maintenance to prevent sudden battery failures and enhance energy efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.18123v1),  [pdf](http://arxiv.org/pdf/2501.18123v1)

**Tags**: cs.LG eess.SP 



### Self-supervised Quantized Representation for Seamlessly Integrating   Knowledge Graphs with Large Language Models
**Authors**: Qika Lin, Tianzhe Zhao, Kai He, Zhen Peng, Fangzhi Xu, Ling Huang, Jingying Ma, Mengling Feng

**Updated**: 2025-01-30T03:40:20Z

**Summary**: Due to the presence of the natural gap between Knowledge Graph (KG) structures and the natural language, the effective integration of holistic structural information of KGs with Large Language Models (LLMs) has emerged as a significant question. To this end, we propose a two-stage framework to learn and apply quantized codes for each entity, aiming for the seamless integration of KGs with LLMs. Firstly, a self-supervised quantized representation (SSQR) method is proposed to compress both KG structural and semantic knowledge into discrete codes (\ie, tokens) that align the format of language sentences. We further design KG instruction-following data by viewing these learned codes as features to directly input to LLMs, thereby achieving seamless integration. The experiment results demonstrate that SSQR outperforms existing unsupervised quantized methods, producing more distinguishable codes. Further, the fine-tuned LLaMA2 and LLaMA3.1 also have superior performance on KG link prediction and triple classification tasks, utilizing only 16 tokens per entity instead of thousands in conventional prompting methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.18119v1),  [pdf](http://arxiv.org/pdf/2501.18119v1)

**Tags**: cs.CL cs.AI 



### Beyond Turn-taking: Introducing Text-based Overlap into Human-LLM   Interactions
**Authors**: JiWoo Kim, Minsuk Chang, JinYeong Bak

**Updated**: 2025-01-30T03:01:01Z

**Summary**: Traditional text-based human-AI interactions often adhere to a strict turn-taking approach. In this research, we propose a novel approach that incorporates overlapping messages, mirroring natural human conversations. Through a formative study, we observed that even in text-based contexts, users instinctively engage in overlapping behaviors like "A: Today I went to-" "B: yeah." To capitalize on these insights, we developed OverlapBot, a prototype chatbot where both AI and users can initiate overlapping. Our user study revealed that OverlapBot was perceived as more communicative and immersive than traditional turn-taking chatbot, fostering faster and more natural interactions. Our findings contribute to the understanding of design space for overlapping interactions. We also provide recommendations for implementing overlap-capable AI interactions to enhance the fluidity and engagement of text-based conversations.

**Link**: [arxiv](http://arxiv.org/abs/2501.18103v1),  [pdf](http://arxiv.org/pdf/2501.18103v1)

**Tags**: cs.HC cs.CL 



### Panacea: Mitigating Harmful Fine-tuning for Large Language Models via   Post-fine-tuning Perturbation
**Authors**: Yibo Wang, Tiansheng Huang, Li Shen, Huanjin Yao, Haotian Luo, Rui Liu, Naiqiang Tan, Jiaxing Huang, Dacheng Tao

**Updated**: 2025-01-30T02:47:09Z

**Summary**: Harmful fine-tuning attack introduces significant security risks to the fine-tuning services. Mainstream defenses aim to vaccinate the model such that the later harmful fine-tuning attack is less effective. However, our evaluation results show that such defenses are fragile -- with a few fine-tuning steps, the model still can learn the harmful knowledge. To this end, we do further experiment and find that an embarrassingly simple solution -- adding purely random perturbations to the fine-tuned model, can recover the model from harmful behavior, though it leads to a degradation in the model's fine-tuning performance. To address the degradation of fine-tuning performance, we further propose Panacea, which optimizes an adaptive perturbation that will be applied to the model after fine-tuning. Panacea maintains model's safety alignment performance without compromising downstream fine-tuning performance. Comprehensive experiments are conducted on different harmful ratios, fine-tuning tasks and mainstream LLMs, where the average harmful scores are reduced by up-to 21.5%, while maintaining fine-tuning performance. As a by-product, we analyze the optimized perturbation and show that different layers in various LLMs have distinct safety coefficients. Source code available at https://github.com/w-yibo/Panacea

**Link**: [arxiv](http://arxiv.org/abs/2501.18100v1),  [pdf](http://arxiv.org/pdf/2501.18100v1)

**Tags**: cs.CL cs.AI 



### Leveraging Multimodal LLM for Inspirational User Interface Search
**Authors**: Seokhyeon Park, Yumin Song, Soohyun Lee, Jaeyoung Kim, Jinwook Seo

**Updated**: 2025-01-30T02:23:25Z

**Summary**: Inspirational search, the process of exploring designs to inform and inspire new creative work, is pivotal in mobile user interface (UI) design. However, exploring the vast space of UI references remains a challenge. Existing AI-based UI search methods often miss crucial semantics like target users or the mood of apps. Additionally, these models typically require metadata like view hierarchies, limiting their practical use. We used a multimodal large language model (MLLM) to extract and interpret semantics from mobile UI images. We identified key UI semantics through a formative study and developed a semantic-based UI search system. Through computational and human evaluations, we demonstrate that our approach significantly outperforms existing UI retrieval methods, offering UI designers a more enriched and contextually relevant search experience. We enhance the understanding of mobile UI design semantics and highlight MLLMs' potential in inspirational search, providing a rich dataset of UI semantics for future studies.

**Link**: [arxiv](http://arxiv.org/abs/2501.17799v2),  [pdf](http://arxiv.org/pdf/2501.17799v2)

**Tags**: cs.HC cs.IR 



### Learning to Plan & Reason for Evaluation with Thinking-LLM-as-a-Judge
**Authors**: Swarnadeep Saha, Xian Li, Marjan Ghazvininejad, Jason Weston, Tianlu Wang

**Updated**: 2025-01-30T02:21:59Z

**Summary**: LLM-as-a-Judge models generate chain-of-thought (CoT) sequences intended to capture the step-bystep reasoning process that underlies the final evaluation of a response. However, due to the lack of human annotated CoTs for evaluation, the required components and structure of effective reasoning traces remain understudied. Consequently, previous approaches often (1) constrain reasoning traces to hand-designed components, such as a list of criteria, reference answers, or verification questions and (2) structure them such that planning is intertwined with the reasoning for evaluation. In this work, we propose EvalPlanner, a preference optimization algorithm for Thinking-LLM-as-a-Judge that first generates an unconstrained evaluation plan, followed by its execution, and then the final judgment. In a self-training loop, EvalPlanner iteratively optimizes over synthetically constructed evaluation plans and executions, leading to better final verdicts. Our method achieves a new state-of-the-art performance for generative reward models on RewardBench (with a score of 93.9), despite being trained on fewer amount of, and synthetically generated, preference pairs. Additional experiments on other benchmarks like RM-Bench, JudgeBench, and FollowBenchEval further highlight the utility of both planning and reasoning for building robust LLM-as-a-Judge reasoning models.

**Link**: [arxiv](http://arxiv.org/abs/2501.18099v1),  [pdf](http://arxiv.org/pdf/2501.18099v1)

**Tags**: cs.AI cs.CL 



### LLMs can see and hear without any training
**Authors**: Kumar Ashutosh, Yossi Gandelsman, Xinlei Chen, Ishan Misra, Rohit Girdhar

**Updated**: 2025-01-30T02:16:35Z

**Summary**: We present MILS: Multimodal Iterative LLM Solver, a surprisingly simple, training-free approach, to imbue multimodal capabilities into your favorite LLM. Leveraging their innate ability to perform multi-step reasoning, MILS prompts the LLM to generate candidate outputs, each of which are scored and fed back iteratively, eventually generating a solution to the task. This enables various applications that typically require training specialized models on task-specific data. In particular, we establish a new state-of-the-art on emergent zero-shot image, video and audio captioning. MILS seamlessly applies to media generation as well, discovering prompt rewrites to improve text-to-image generation, and even edit prompts for style transfer! Finally, being a gradient-free optimization approach, MILS can invert multimodal embeddings into text, enabling applications like cross-modal arithmetic.

**Link**: [arxiv](http://arxiv.org/abs/2501.18096v1),  [pdf](http://arxiv.org/pdf/2501.18096v1)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### AlphaAdam:Asynchronous Masked Optimization with Dynamic Alpha for   Selective Updates
**Authors**: Da Chang, Yu Li, Ganzhao Yuan

**Updated**: 2025-01-30T02:10:23Z

**Summary**: In the training of large language models (LLMs), updating parameters more efficiently and stably has always been an important challenge. To achieve efficient parameter updates, existing methods usually achieve performance comparable to full parameter updates through methods such as low-dimensional decomposition or layer-wise selective updates. In this work, we propose AlphaAdam, an optimization framework for LLM from the perspective of intra-layer parameter updates. By decoupling parameter updates and dynamically adjusting their strength, AlphaAdam accelerates convergence and improves training stability. We construct parameter masks based on the consistency of historical momentum and gradient direction and combine them with an adaptive mask strength strategy to ensure efficient optimization and theoretical convergence guarantees, which is also applicable to most momentum-based optimizers. Extensive experiments show that AlphaAdam outperforms state-of-the-art methods such as AdamW in terms of convergence speed and computational efficiency across tasks, including GPT-2 pre-trained and fine-tuned RoBERTa and Llama-7B. Our AlphaAdam implements an optimizer enhancement framework for LLMs through intra-layer asynchronous masked adaptive updates. Our code is available in this \href{https://github.com/MaeChd/AlphaAdam}{link}

**Link**: [arxiv](http://arxiv.org/abs/2501.18094v1),  [pdf](http://arxiv.org/pdf/2501.18094v1)

**Tags**: cs.LG 



### MathVC: An LLM-Simulated Multi-Character Virtual Classroom for   Mathematics Education
**Authors**: Murong Yue, Wenhan Lyu, Wijdane Mifdal, Jennifer Suh, Yixuan Zhang, Ziyu Yao

**Updated**: 2025-01-30T01:47:47Z

**Summary**: Mathematical modeling (MM) is considered a fundamental skill for students in STEM disciplines. Practicing the MM skill is often the most effective when students can engage in group discussion and collaborative problem-solving. However, due to unevenly distributed teachers and educational resources needed to monitor such group activities, students do not always receive equal opportunities for this practice. Excitingly, large language models (LLMs) have recently demonstrated strong capability in both modeling mathematical problems and simulating characters with different traits and properties. Drawing inspiration from the advancement of LLMs, in this work, we present MATHVC, the very first LLM-powered virtual classroom containing multiple LLM-simulated student characters, with whom a human student can practice their MM skill. To encourage each LLM character's behaviors to be aligned with their specified math-relevant properties (termed "characteristics alignment") and the overall conversational procedure to be close to an authentic student MM discussion (termed "conversational procedural alignment"), we proposed three innovations: integrating MM domain knowledge into the simulation, defining a symbolic schema as the ground for character simulation, and designing a meta planner at the platform level to drive the conversational procedure. Through experiments and ablation studies, we confirmed the effectiveness of our simulation approach and showed the promise for MATHVC to benefit real-life students in the future.

**Link**: [arxiv](http://arxiv.org/abs/2404.06711v2),  [pdf](http://arxiv.org/pdf/2404.06711v2)

**Tags**: cs.CL cs.HC 



### Normative Evaluation of Large Language Models with Everyday Moral   Dilemmas
**Authors**: Pratik S. Sachdeva, Tom van Nuenen

**Updated**: 2025-01-30T01:29:46Z

**Summary**: The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well models are aligned with certain demographic groups, moral beliefs, or political ideologies. While informative, the adherence of these approaches to relatively superficial constructs tends to oversimplify the complexity and nuance underlying everyday moral dilemmas. We argue that auditing LLMs along more detailed axes of human interaction is of paramount importance to better assess the degree to which they may impact human beliefs and actions. To this end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the "Am I the Asshole" (AITA) community on Reddit, where users seek moral judgments on everyday conflicts from other community members. We prompted seven LLMs to assign blame and provide explanations for over 10,000 AITA moral dilemmas. We then compared the LLMs' judgments and explanations to those of Redditors and to each other, aiming to uncover patterns in their moral reasoning. Our results demonstrate that large language models exhibit distinct patterns of moral judgment, varying substantially from human evaluations on the AITA subreddit. LLMs demonstrate moderate to high self-consistency but low inter-model agreement. Further analysis of model explanations reveals distinct patterns in how models invoke various moral principles. These findings highlight the complexity of implementing consistent moral reasoning in artificial systems and the need for careful evaluation of how different models approach ethical judgment. As LLMs continue to be used in roles requiring ethical decision-making such as therapists and companions, careful evaluation is crucial to mitigate potential biases and limitations.

**Link**: [arxiv](http://arxiv.org/abs/2501.18081v1),  [pdf](http://arxiv.org/pdf/2501.18081v1)

**Tags**: cs.AI cs.CY 



### CURATe: Benchmarking Personalised Alignment of Conversational AI   Assistants
**Authors**: Lize Alberts, Benjamin Ellis, Andrei Lupu, Jakob Foerster

**Updated**: 2025-01-30T01:29:03Z

**Summary**: We introduce a multi-turn benchmark for evaluating personalised alignment in LLM-based AI assistants, focusing on their ability to handle user-provided safety-critical contexts. Our assessment of ten leading models across five scenarios (with 337 use cases each) reveals systematic inconsistencies in maintaining user-specific consideration, with even top-rated "harmless" models making recommendations that should be recognised as obviously harmful to the user given the context provided. Key failure modes include inappropriate weighing of conflicting preferences, sycophancy (prioritising desires above safety), a lack of attentiveness to critical user information within the context window, and inconsistent application of user-specific knowledge. The same systematic biases were observed in OpenAI's o1, suggesting that strong reasoning capacities do not necessarily transfer to this kind of personalised thinking. We find that prompting LLMs to consider safety-critical context significantly improves performance, unlike a generic 'harmless and helpful' instruction. Based on these findings, we propose research directions for embedding self-reflection capabilities, online user modelling, and dynamic risk assessment in AI assistants. Our work emphasises the need for nuanced, context-aware approaches to alignment in systems designed for persistent human interaction, aiding the development of safe and considerate AI assistants.

**Link**: [arxiv](http://arxiv.org/abs/2410.21159v2),  [pdf](http://arxiv.org/pdf/2410.21159v2)

**Tags**: cs.HC cs.AI 68T05 I.2.0; I.2.7; K.4.2; H.5.2; I.2.6 



### From Natural Language to Extensive-Form Game Representations
**Authors**: Shilong Deng, Yongzhao Wang, Rahul Savani

**Updated**: 2025-01-30T01:25:12Z

**Summary**: We introduce a framework for translating game descriptions in natural language into extensive-form representations in game theory, leveraging Large Language Models (LLMs) and in-context learning. Given the varying levels of strategic complexity in games, such as perfect versus imperfect information, directly applying in-context learning would be insufficient. To address this, we introduce a two-stage framework with specialized modules to enhance in-context learning, enabling it to divide and conquer the problem effectively. In the first stage, we tackle the challenge of imperfect information by developing a module that identifies information sets along and the corresponding partial tree structure. With this information, the second stage leverages in-context learning alongside a self-debugging module to produce a complete extensive-form game tree represented using pygambit, the Python API of a recognized game-theoretic analysis tool called Gambit. Using this python representation enables the automation of tasks such as computing Nash equilibria directly from natural language descriptions. We evaluate the performance of the full framework, as well as its individual components, using various LLMs on games with different levels of strategic complexity. Our experimental results show that the framework significantly outperforms baseline models in generating accurate extensive-form games, with each module playing a critical role in its success.

**Link**: [arxiv](http://arxiv.org/abs/2501.17282v2),  [pdf](http://arxiv.org/pdf/2501.17282v2)

**Tags**: cs.AI cs.CL cs.GT cs.MA 



### PAC Codes Meet CRC-Polar Codes
**Authors**: Xinyi Gu, Mohammad Rowshan, Jinhong Yuan

**Updated**: 2025-01-30T01:18:24Z

**Summary**: CRC-Polar codes under SC list decoding are well-regarded for their competitive error performance. This paper examines these codes by focusing on minimum weight codewords and breaking them down into the rows of the polar transform. Inspired by the significant impact of parity check bits and their positions, we apply a shifted rate-profile for polarization-adjusted convolutional (PS-PAC) codes, thereby achieving similar improvements in the weight distribution of polar codes through precoding. The results demonstrate a significant improvement in error performance, achieving up to a 0.5 dB power gain with short PS-PAC codes. Additionally, leveraging convolutional precoding in PAC codes, we adopt a continuous deployment (masking) of parity check bits derived from the remainder of continuous division of the partial message polynomial and the CRC polynomial over frozen positions in the rate-profile. This approach enhances performance for long codes, with an overall improvement of 0.12 dB.

**Link**: [arxiv](http://arxiv.org/abs/2501.18080v1),  [pdf](http://arxiv.org/pdf/2501.18080v1)

**Tags**: cs.IT math.IT 



### Generative Information Retrieval Evaluation
**Authors**: Marwah Alaofi, Negar Arabzadeh, Charles L. A. Clarke, Mark Sanderson

**Updated**: 2025-01-30T00:52:34Z

**Summary**: In this chapter, we consider generative information retrieval evaluation from two distinct but interrelated perspectives. First, large language models (LLMs) themselves are rapidly becoming tools for evaluation, with current research indicating that LLMs may be superior to crowdsource workers and other paid assessors on basic relevance judgement tasks. We review past and ongoing related research, including speculation on the future of shared task initiatives, such as TREC, and a discussion on the continuing need for human assessments. Second, we consider the evaluation of emerging LLM-based generative information retrieval (GenIR) systems, including retrieval augmented generation (RAG) systems. We consider approaches that focus both on the end-to-end evaluation of GenIR systems and on the evaluation of a retrieval component as an element in a RAG system. Going forward, we expect the evaluation of GenIR systems to be at least partially based on LLM-based assessment, creating an apparent circularity, with a system seemingly evaluating its own output. We resolve this apparent circularity in two ways: 1) by viewing LLM-based assessment as a form of "slow search", where a slower IR system is used for evaluation and training of a faster production IR system; and 2) by recognizing a continuing need to ground evaluation in human assessment, even if the characteristics of that human assessment must change.

**Link**: [arxiv](http://arxiv.org/abs/2404.08137v3),  [pdf](http://arxiv.org/pdf/2404.08137v3)

**Tags**: cs.IR 



### FinanceQA: A Benchmark for Evaluating Financial Analysis Capabilities of   Large Language Models
**Authors**: Spencer Mateega, Carlos Georgescu, Danny Tang

**Updated**: 2025-01-30T00:06:55Z

**Summary**: FinanceQA is a testing suite that evaluates LLMs' performance on complex numerical financial analysis tasks that mirror real-world investment work. Despite recent advances, current LLMs fail to meet the strict accuracy requirements of financial institutions, with models failing approximately 60% of realistic tasks that mimic on-the-job analyses at hedge funds, private equity firms, investment banks, and other financial institutions. The primary challenges include hand-spreading metrics, adhering to standard accounting and corporate valuation conventions, and performing analysis under incomplete information - particularly in multi-step tasks requiring assumption generation. This performance gap highlights the disconnect between existing LLM capabilities and the demands of professional financial analysis that are inadequately tested by current testing architectures. Results show that higher-quality training data is needed to support such tasks, which we experiment with using OpenAI's fine-tuning API. FinanceQA is publicly released at [this https URL](https://huggingface.co/datasets/AfterQuery/FinanceQA).

**Link**: [arxiv](http://arxiv.org/abs/2501.18062v1),  [pdf](http://arxiv.org/pdf/2501.18062v1)

**Tags**: cs.LG cs.CL 



### Conformal Robust Control of Linear Systems
**Authors**: Yash Patel, Sahana Rayan, Ambuj Tewari

**Updated**: 2025-01-29T23:56:08Z

**Summary**: End-to-end engineering design pipelines, in which designs are evaluated using concurrently defined optimal controllers, are becoming increasingly common in practice. To discover designs that perform well even under the misspecification of system dynamics, such end-to-end pipelines have now begun evaluating designs with a robust control objective in place of the nominal optimal control setup. Current approaches of specifying such robust control subproblems, however, rely on hand specification of perturbations anticipated to be present upon deployment or margin methods that ignore problem structure, resulting in a lack of theoretical guarantees and overly conservative empirical performance. We, instead, propose a novel methodology for LQR systems that leverages conformal prediction to specify such uncertainty regions in a data-driven fashion. Such regions have distribution-free coverage guarantees on the true system dynamics, in turn allowing for a probabilistic characterization of the regret of the resulting robust controller. We then demonstrate that such a controller can be efficiently produced via a novel policy gradient method that has convergence guarantees. We finally demonstrate the superior empirical performance of our method over alternate robust control specifications, such as $H_{\infty}$ and LQR with multiplicative noise, across a collection of engineering control systems.

**Link**: [arxiv](http://arxiv.org/abs/2405.16250v2),  [pdf](http://arxiv.org/pdf/2405.16250v2)

**Tags**: eess.SY cs.SY stat.ME 



### Learning the Optimal Stopping for Early Classification within Finite   Horizons via Sequential Probability Ratio Test
**Authors**: Akinori F. Ebihara, Taiki Miyagawa, Kazuyuki Sakurai, Hitoshi Imaoka

**Updated**: 2025-01-29T23:54:46Z

**Summary**: Time-sensitive machine learning benefits from Sequential Probability Ratio Test (SPRT), which provides an optimal stopping time for early classification of time series. However, in finite horizon scenarios, where input lengths are finite, determining the optimal stopping rule becomes computationally intensive due to the need for backward induction, limiting practical applicability. We thus introduce FIRMBOUND, an SPRT-based framework that efficiently estimates the solution to backward induction from training data, bridging the gap between optimal stopping theory and real-world deployment. It employs density ratio estimation and convex function learning to provide statistically consistent estimators for sufficient statistic and conditional expectation, both essential for solving backward induction; consequently, FIRMBOUND minimizes Bayes risk to reach optimality. Additionally, we present a faster alternative using Gaussian process regression, which significantly reduces training time while retaining low deployment overhead, albeit with potential compromise in statistical consistency. Experiments across independent and identically distributed (i.i.d.), non-i.i.d., binary, multiclass, synthetic, and real-world datasets show that FIRMBOUND achieves optimalities in the sense of Bayes risk and speed-accuracy tradeoff. Furthermore, it advances the tradeoff boundary toward optimality when possible and reduces decision-time variance, ensuring reliable decision-making. Code is publicly available at https://github.com/Akinori-F-Ebihara/FIRMBOUND

**Link**: [arxiv](http://arxiv.org/abs/2501.18059v1),  [pdf](http://arxiv.org/pdf/2501.18059v1)

**Tags**: cs.LG cs.AI 



### Conformal Prediction for Ensembles: Improving Efficiency via Score-Based   Aggregation
**Authors**: Eduardo Ochoa Rivera, Yash Patel, Ambuj Tewari

**Updated**: 2025-01-29T23:46:43Z

**Summary**: Distribution-free uncertainty estimation for ensemble methods is increasingly desirable due to the widening deployment of multi-modal black-box predictive models. Conformal prediction is one approach that avoids such distributional assumptions. Methods for conformal aggregation have in turn been proposed for ensembled prediction, where the prediction regions of individual models are merged as to retain coverage guarantees while minimizing conservatism. Merging the prediction regions directly, however, sacrifices structures present in the conformal scores that can further reduce conservatism. We, therefore, propose a novel framework that extends the standard scalar formulation of a score function to a multivariate score that produces more efficient prediction regions. We then demonstrate that such a framework can be efficiently leveraged in both classification and predict-then-optimize regression settings downstream and empirically show the advantage over alternate conformal aggregation methods.

**Link**: [arxiv](http://arxiv.org/abs/2405.16246v2),  [pdf](http://arxiv.org/pdf/2405.16246v2)

**Tags**: stat.ME stat.ML 



### RL-based Query Rewriting with Distilled LLM for online E-Commerce   Systems
**Authors**: Duy A. Nguyen, Rishi Kesav Mohan, Van Yang, Pritom Saha Akash, Kevin Chen-Chuan Chang

**Updated**: 2025-01-29T23:41:12Z

**Summary**: Query rewriting (QR) is a critical technique in e-commerce search, addressing the lexical gap between user queries and product descriptions to enhance search performance. Existing QR approaches typically fall into two categories: discriminative models and generative methods leveraging large language models (LLMs). Discriminative models often struggle with natural language understanding and offer limited flexibility in rewriting, while generative LLMs, despite producing high-quality rewrites, face high inference latency and cost in online settings. These limitations force offline deployment, making them vulnerable to issues like information staleness and semantic drift. To overcome these challenges, we propose a novel hybrid pipeline for QR that balances efficiency and effectiveness. Our approach combines offline knowledge distillation to create a lightweight but efficient student model with online reinforcement learning (RL) to refine query rewriting dynamically using real-time feedback. A key innovation is the use of LLMs as simulated human feedback, enabling scalable reward signals and cost-effective evaluation without manual annotations. Experimental results on Amazon ESCI dataset demonstrate significant improvements in query relevance, diversity, and adaptability, as well as positive feedback from the LLM simulation. This work contributes to advancing LLM capabilities for domain-specific applications, offering a robust solution for dynamic and complex e-commerce search environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.18056v1),  [pdf](http://arxiv.org/pdf/2501.18056v1)

**Tags**: cs.IR 



### The Impact of Visual Information in Chinese Characters: Evaluating Large   Models' Ability to Recognize and Utilize Radicals
**Authors**: Xiaofeng Wu, Karl Stratos, Wei Xu

**Updated**: 2025-01-29T22:51:42Z

**Summary**: The glyphic writing system of Chinese incorporates information-rich visual features in each character, such as radicals that provide hints about meaning or pronunciation. However, there has been no investigation into whether contemporary Large Language Models (LLMs) and Vision-Language Models (VLMs) can harness these sub-character features in Chinese through prompting. In this study, we establish a benchmark to evaluate LLMs' and VLMs' understanding of visual elements in Chinese characters, including radicals, composition structures, strokes, and stroke counts. Our results reveal that models surprisingly exhibit some, but still limited, knowledge of the visual information, regardless of whether images of characters are provided. To incite models' ability to use radicals, we further experiment with incorporating radicals into the prompts for Chinese language processing (CLP) tasks. We observe consistent improvement in Part-Of-Speech tagging when providing additional information about radicals, suggesting the potential to enhance CLP by integrating sub-character information.

**Link**: [arxiv](http://arxiv.org/abs/2410.09013v3),  [pdf](http://arxiv.org/pdf/2410.09013v3)

**Tags**: cs.CL 



### When less is more: evolving large neural networks from small ones
**Authors**: Anil Radhakrishnan, John F. Lindner, Scott T. Miller, Sudeshna Sinha, William L. Ditto

**Updated**: 2025-01-29T21:56:38Z

**Summary**: In contrast to conventional artificial neural networks, which are large and structurally static, we study feed-forward neural networks that are small and dynamic, whose nodes can be added (or subtracted) during training. A single neuronal weight in the network controls the network's size, while the weight itself is optimized by the same gradient-descent algorithm that optimizes the network's other weights and biases, but with a size-dependent objective or loss function. We train and evaluate such Nimble Neural Networks on nonlinear regression and classification tasks where they outperform the corresponding static networks. Growing networks to minimal, appropriate, or optimal sizes while training elucidates network dynamics and contrasts with pruning large networks after training but before deployment.

**Link**: [arxiv](http://arxiv.org/abs/2501.18012v1),  [pdf](http://arxiv.org/pdf/2501.18012v1)

**Tags**: cs.LG cond-mat.dis-nn 



### Large Language Models Think Too Fast To Explore Effectively
**Authors**: Lan Pan, Hanbo Xie, Robert C. Wilson

**Updated**: 2025-01-29T21:51:17Z

**Summary**: Large Language Models have emerged many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore, an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with those traditional LLMs relying primarily on uncertainty driven strategies, unlike humans who balance uncertainty and empowerment. Representational analysis of the models with Sparse Autoencoders revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.

**Link**: [arxiv](http://arxiv.org/abs/2501.18009v1),  [pdf](http://arxiv.org/pdf/2501.18009v1)

**Tags**: cs.AI q-bio.NC 



