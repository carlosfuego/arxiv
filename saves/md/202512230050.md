# Arxiv Results
## Keyword: kv cache 
 ### Vidarc: Embodied Video Diffusion Model for Closed-loop Control
**Authors**: Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu

**Updated**: 2025-12-19T15:04:24Z

**Summary**: Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.

**Link**: [arxiv](https://arxiv.org/abs/2512.17661v1),  [pdf](https://arxiv.org/pdf/2512.17661v1)

**Tags**: cs.RO cs.LG 



### xGR: Efficient Generative Recommendation Serving at Scale
**Authors**: Qingxiao Sun, Tongxuan Liu, Shen Zhang, Siyu Wu, Peijun Yang, Haotian Liang, Menxin Li, Xiaolong Ma, Zhiwei Liang, Ziyi Ren, Minchao Zhang, Xinyu Liu, Ke Zhang, Depei Qian, Hailong Yang

**Updated**: 2025-12-19T11:20:16Z

**Summary**: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

**Link**: [arxiv](https://arxiv.org/abs/2512.11529v2),  [pdf](https://arxiv.org/pdf/2512.11529v2)

**Tags**: cs.LG 



### Learning What to Write: Write-Gated KV for Efficient Long-Context Inference
**Authors**: Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu

**Updated**: 2025-12-19T11:08:58Z

**Summary**: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

**Link**: [arxiv](https://arxiv.org/abs/2512.17452v1),  [pdf](https://arxiv.org/pdf/2512.17452v1)

**Tags**: cs.LG cs.AI 



### V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval
**Authors**: Donghyuk Kim, Sejeong Yang, Wonjin Shin, Joo-Young Kim

**Updated**: 2025-12-19T08:02:44Z

**Summary**: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.   In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.

**Link**: [arxiv](https://arxiv.org/abs/2512.12284v2),  [pdf](https://arxiv.org/pdf/2512.12284v2)

**Tags**: eess.IV cs.AI cs.AR cs.CV cs.MM 



### ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration
**Authors**: Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen

**Updated**: 2025-12-19T07:27:19Z

**Summary**: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.

**Link**: [arxiv](https://arxiv.org/abs/2512.17298v1),  [pdf](https://arxiv.org/pdf/2512.17298v1)

**Tags**: cs.CV 



### PeerSync: Accelerating Containerized Service Delivery at the Network Edge
**Authors**: Yinuo Deng, Hailiang Zhao, Dongjing Wang, Peng Chen, Wenzhuo Qian, Jianwei Yin, Schahram Dustdar, Shuiguang Deng

**Updated**: 2025-12-19T02:09:53Z

**Summary**: Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both large-scale Docker-based emulations and physical edge devices. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\times$, 1.79$\times$, and 1.28$\times$ compared to the Baseline solution, Dragonfly, and Kraken, respectively, while significantly reducing cross-network traffic by 90.72% under congested and varying network conditions.

**Link**: [arxiv](https://arxiv.org/abs/2507.20116v2),  [pdf](https://arxiv.org/pdf/2507.20116v2)

**Tags**: cs.NI cs.DC 



### LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference
**Authors**: Harsh Vardhan Bansal

**Updated**: 2025-12-18T18:18:57Z

**Summary**: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

**Link**: [arxiv](https://arxiv.org/abs/2512.16843v1),  [pdf](https://arxiv.org/pdf/2512.16843v1)

**Tags**: cs.CL cs.AI 



### MEPIC: Memory Efficient Position Independent Caching for LLM Serving
**Authors**: Qian Wang, Zahra Yousefijamarani, Morgan Lindsay Heisler, Rongzhi Gu, Bai Xiaolong, Shan Yizhou, Wei Zhang, Wang Lan, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-12-18T18:04:01Z

**Summary**: Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.   We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.

**Link**: [arxiv](https://arxiv.org/abs/2512.16822v1),  [pdf](https://arxiv.org/pdf/2512.16822v1)

**Tags**: cs.LG 



### Prefix Probing: Lightweight Harmful Content Detection for Large Language Models
**Authors**: Jirui Yang, Hengqi Guo, Zhihui Lu, Yi Zhao, Yuansen Zhang, Shijing Hu, Qiang Duan, Yinggui Wang, Tao Wei

**Updated**: 2025-12-18T15:22:14Z

**Summary**: Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of "agreement/execution" versus "refusal/safety" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.

**Link**: [arxiv](https://arxiv.org/abs/2512.16650v1),  [pdf](https://arxiv.org/pdf/2512.16650v1)

**Tags**: cs.AI cs.CR 



### Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers
**Authors**: Yifan Zhou, Zeqi Xiao, Tianyi Wei, Shuai Yang, Xingang Pan

**Updated**: 2025-12-18T14:53:12Z

**Summary**: Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA

**Link**: [arxiv](https://arxiv.org/abs/2512.16615v1),  [pdf](https://arxiv.org/pdf/2512.16615v1)

**Tags**: cs.CV 



### Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length
**Authors**: Yubo Huang, Hailong Guo, Fangtai Wu, Shifeng Zhang, Shijie Huang, Qijun Gan, Lin Liu, Sirui Zhao, Enhong Chen, Jiaming Liu, Steven Hoi

**Updated**: 2025-12-18T13:02:34Z

**Summary**: Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.

**Link**: [arxiv](https://arxiv.org/abs/2512.04677v3),  [pdf](https://arxiv.org/pdf/2512.04677v3)

**Tags**: cs.CV 



### Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems
**Authors**: En-Ming Huang, Li-Shang Lin, Chun-Yi Lee

**Updated**: 2025-12-18T12:45:52Z

**Summary**: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

**Link**: [arxiv](https://arxiv.org/abs/2512.16473v1),  [pdf](https://arxiv.org/pdf/2512.16473v1)

**Tags**: cs.DC 



### FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store
**Authors**: Zhisheng Hu, Jiacheng Shen, Ming-Chang Yang

**Updated**: 2025-12-18T04:03:01Z

**Summary**: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.

**Link**: [arxiv](https://arxiv.org/abs/2512.16148v1),  [pdf](https://arxiv.org/pdf/2512.16148v1)

**Tags**: cs.DC 



### VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference
**Authors**: Shengling Qin, Hao Yu, Chenxin Wu, Zheng Li, Yizhong Cao, Zhengyang Zhuge, Yuxin Zhou, Wentao Yao, Yi Zhang, Zhengheng Wang, Shuai Bai, Jianwei Zhang, Junyang Lin

**Updated**: 2025-12-18T02:59:27Z

**Summary**: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. We develop an experimental implementation of the proposed VLCache pipeline based on SGLang, enabling significantly faster inference in practical deployments.

**Link**: [arxiv](https://arxiv.org/abs/2512.12977v2),  [pdf](https://arxiv.org/pdf/2512.12977v2)

**Tags**: cs.CV 



### MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services
**Authors**: Lingfeng Tang, Daoping Zhang, Junjie Chen, Peihao Huang, Feng Jin, Chengguang Xu, Yuxin Chen, Feiqiang Sun, Guo Chen

**Updated**: 2025-12-18T00:45:00Z

**Summary**: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

**Link**: [arxiv](https://arxiv.org/abs/2512.16056v1),  [pdf](https://arxiv.org/pdf/2512.16056v1)

**Tags**: cs.DC cs.NI cs.PF 



### HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing
**Authors**: Myunghyun Rhee, Joonseop Sim, Taeyoung Ahn, Seungyong Lee, Daegun Yoon, Euiseok Kim, Kyoung Park, Youngpyo Joo, Hoshik Kim

**Updated**: 2025-12-17T21:40:17Z

**Summary**: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.

**Link**: [arxiv](https://arxiv.org/abs/2504.16112v2),  [pdf](https://arxiv.org/pdf/2504.16112v2)

**Tags**: cs.AR cs.AI cs.CL cs.DC 



### DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models
**Authors**: Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang

**Updated**: 2025-12-17T18:59:55Z

**Summary**: In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.

**Link**: [arxiv](https://arxiv.org/abs/2512.15713v1),  [pdf](https://arxiv.org/pdf/2512.15713v1)

**Tags**: cs.CV 



### Dynamic Rebatching for Efficient Early-Exit Inference with DREX
**Authors**: Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu

**Updated**: 2025-12-17T18:55:45Z

**Summary**: Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.

**Link**: [arxiv](https://arxiv.org/abs/2512.15705v1),  [pdf](https://arxiv.org/pdf/2512.15705v1)

**Tags**: cs.DC cs.LG 



### Optimizing Agentic Language Model Inference via Speculative Tool Calls
**Authors**: Daniel Nichols, Prajwal Singhania, Charles Jekel, Abhinav Bhatele, Harshitha Menon

**Updated**: 2025-12-17T18:22:44Z

**Summary**: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

**Link**: [arxiv](https://arxiv.org/abs/2512.15834v1),  [pdf](https://arxiv.org/pdf/2512.15834v1)

**Tags**: cs.PL cs.AI cs.DC cs.PF cs.SE 



### Optimizing Bloom Filters for Modern GPU Architectures
**Authors**: Daniel Jünger, Kevin Kristensen, Yunsong Wang, Xiangyao Yu, Bertil Schmidt

**Updated**: 2025-12-17T17:01:55Z

**Summary**: Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.   Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\times$ ($15.4\times$) for bulk filter lookup (construction), respectively, achieving above $92\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.

**Link**: [arxiv](https://arxiv.org/abs/2512.15595v1),  [pdf](https://arxiv.org/pdf/2512.15595v1)

**Tags**: cs.DC 



### Radial electric field and density fluctuations measured by Doppler reflectometry during the post-pellet enhanced confinement phase in W7-X
**Authors**: T. Estrada, D. Carralero, T. Windisch, E. Sánchez, J. M. García-Regaña, J. Martínez-Fernández, A. de la Peña, J. L. Velasco, J. A. Alonso, M. Beurskens, S. Bozhenkov, H. Damm, G. Fuchert, R. Kleiber, N. Pablant, E. Pasch

**Updated**: 2025-12-17T16:34:07Z

**Summary**: Radial profiles of density fluctuations and radial electric field, $E_r$, have been measured using Doppler reflectometry during the post-pellet enhanced confinement phase achieved, under different heating power levels and magnetic configurations, along the 2018 W7-X experimental campaign. A pronounced $E_r$-well is measured with local values as high as -40 kV/m in the radial range $ρ\sim 0.7-0.8$ during the post-pellet enhanced confinement phase. The maximum $E_r$ intensity scales with both plasma density and Electron Cyclotron Heating (ECH) power level following a similar trend as the plasma energy content. A good agreement is found when the experimental $E_r$ profiles are compared to simulations carried out using the neoclassical codes DKES and KNOSOS. The density fluctuation level decreases from the plasma edge toward the plasma core and the drop is more pronounced in the post-pellet enhanced confinement phase than in reference gas fuelled plasmas. Besides, in the post-pellet phase, the density fluctuation level is lower in the high iota magnetic configuration than in the standard one. In order to discriminate whether this difference is related to the differences in the plasma profiles or in the stability properties of the two configurations, gyrokinetic simulations have been carried out using the codes \texttt{stella} and EUTERPE. The simulation results point to the plasma profile evolution after the pellet injection and the stabilization effect of the radial electric field profile as the dominant players in the stabilization of the plasma turbulence.

**Link**: [arxiv](https://arxiv.org/abs/2512.15576v1),  [pdf](https://arxiv.org/pdf/2512.15576v1)

**Tags**: physics.plasm-ph 



### CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing
**Authors**: Kuan Lu, Shuhang Lin, Sai Wu, Yichen Yao, Junhan Yang, Huan Li, Wei Chu, Xu Yinghui, Yuan Qi, Gang Chen

**Updated**: 2025-12-17T15:56:32Z

**Summary**: Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.

**Link**: [arxiv](https://arxiv.org/abs/2512.15550v1),  [pdf](https://arxiv.org/pdf/2512.15550v1)

**Tags**: cs.CL 



### Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT
**Authors**: Liyu Zhang, Yejia Liu, Kwun Ho Liu, Runxi Huang, Xiaomin Ouyang

**Updated**: 2025-12-17T08:56:21Z

**Summary**: In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.

**Link**: [arxiv](https://arxiv.org/abs/2512.15206v1),  [pdf](https://arxiv.org/pdf/2512.15206v1)

**Tags**: cs.LG 



### Artificial Hippocampus Networks for Efficient Long-Context Modeling
**Authors**: Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei

**Updated**: 2025-12-17T07:08:49Z

**Summary**: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.

**Link**: [arxiv](https://arxiv.org/abs/2510.07318v2),  [pdf](https://arxiv.org/pdf/2510.07318v2)

**Tags**: cs.CL cs.AI cs.LG 



### Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units
**Authors**: Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna

**Updated**: 2025-12-17T05:01:59Z

**Summary**: The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms.

**Link**: [arxiv](https://arxiv.org/abs/2509.25155v2),  [pdf](https://arxiv.org/pdf/2509.25155v2)

**Tags**: cs.DC cs.LG 



### Uncovering hidden protein conformations with high bandwidth nanopore measurements
**Authors**: Kyril Kavetsky, Sabine Hong, Chih-Yuan Lin, Roger Yang, Marija Drndic

**Updated**: 2025-12-17T02:14:04Z

**Summary**: Advanced nanopore measurements allow structural probing of molecules with high spatial and temporal resolution. We report high signal-to-noise, 1-10 MHz bandwidth, translocation measurements of the multi-state folding of heme protein cytochrome c in KCl solution through optimally designed silicon nitride pores of 2.3-3.3 nm diameter and 3.6-3.8 nm effective thickness, and an optimal concentration of a denaturant (Gdm-Cl). The pore diameter is slightly smaller than the protein size, forcing the protein to squeeze through the pore. The sufficiently large pore thickness allows enough time for protein probing at an applied field of approximately 250 kV/cm. Through Bayesian Information Criterion score analysis, current blockades reveal six distinct levels, attributed to specific protein states. We calculate the transition probabilities between the states and the conditional probabilities of the protein leaving the pore from each state. We validate the model by simulating events and comparing them to experimental data.

**Link**: [arxiv](https://arxiv.org/abs/2512.15016v1),  [pdf](https://arxiv.org/pdf/2512.15016v1)

**Tags**: physics.bio-ph 



### High voltage and electrode system for a cryogenic experiment to search for the neutron electric dipole moment
**Authors**: M. A. Blatnik, S. M. Clayton, S. A. Currie, B. W. Filippone, M. Makela, C. M. O'Shaughnessy, N. S. Phan, J. C. Ramsey, G. V. Riley, A. Roberts, T. Sandborn, T. J Schaub, G. M. Seidel, E. Smith, I. L. Smythe, J. Surbrook, W. Wei, W. Yao, T. M. Ito

**Updated**: 2025-12-17T00:04:30Z

**Summary**: The cryogenic approach to the search for the neutron electric dipole moment--performing the experiment in superfluid liquid helium--holds promise for a substantial increase in sensitivity, potentially enabling a sensitivity level of $10^{-28}$ e-cm. A crucial component in realizing such an experiment is the high voltage and electrode system capable of providing an electric field of 75 kV/cm. This, in turn, requires an electric potential of 635 kV to be applied to the high voltage electrode, while simultaneously satisfying other experimental constraints, such as those on heat load and magnetic noise requirements. This paper describes the outcome of a comprehensive development program addressing these challenges. It outlines the system requirements, discusses new insights into relevant physical phenomena, and details selected technical solutions with their corresponding experimental demonstrations and expected performance. The results collectively demonstrate the successful development of the necessary technology for the high-voltage and electrode system for this approach.

**Link**: [arxiv](https://arxiv.org/abs/2512.14975v1),  [pdf](https://arxiv.org/pdf/2512.14975v1)

**Tags**: physics.ins-det nucl-ex 



### SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching
**Authors**: Xinye Zhao, Spyridon Mastorakis

**Updated**: 2025-12-16T22:36:44Z

**Summary**: As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.

**Link**: [arxiv](https://arxiv.org/abs/2509.24832v2),  [pdf](https://arxiv.org/pdf/2509.24832v2)

**Tags**: cs.CL cs.AI 



### EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving
**Authors**: Shaoting Feng, Yuhan Liu, Hanchen Li, Xiaokun Chen, Samuel Shen, Kuntai Du, Zhuohan Gu, Rui Zhang, Yuyang Huang, Yihua Cheng, Jiayi Yao, Qizheng Zhang, Ganesh Ananthanarayanan, Junchen Jiang

**Updated**: 2025-12-16T22:21:55Z

**Summary**: Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.   We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.

**Link**: [arxiv](https://arxiv.org/abs/2512.14946v1),  [pdf](https://arxiv.org/pdf/2512.14946v1)

**Tags**: cs.OS cs.AI cs.LG 



### Development of a Custom kV-amplitude, pressure-tolerant Radio-Frequency transmitter
**Authors**: Christian Hornhuber, Mohammad Ful Hossain Seikh, Mark Stockham, Scott Voigt, Rob Young, Alisa Nozdrina, Sanyukta Agarwal, Shoukat Ali, Kenny Couberly, Dave Besson

**Updated**: 2025-12-16T19:28:33Z

**Summary**: Current experiments seeking first-ever observation of Ultra-High Energy Neutrinos (UHEN) typically utilize radio frequency (RF) receiver antennas deployed in cold, radio-transparent polar ice, to measure the coherent RF signals resulting from neutrino interactions with ice molecules. Accurate calibration of the receiver response, sampling the full range of possible neutrino geometries, is necessary to estimate the energy and incoming direction of the incident neutrino. Herein, we detail the design and performance of a custom radio-frequency calibration transmitter, consisting of a battery-powered, kiloVolt-scale signal generator (`IDL' pulser) driving an antenna (South Pole UNiversity of Kansas antenna, or `SPUNK') capable of operating at pressures of 200 atmospheres. Performance was validated by lowering the transmitter into a borehole at the South Pole to a depth of 1740 m, yielding high Signal-to-Noise ratio signals at a distance of 5 km from the source.

**Link**: [arxiv](https://arxiv.org/abs/2512.14866v1),  [pdf](https://arxiv.org/pdf/2512.14866v1)

**Tags**: astro-ph.IM 



### MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives
**Authors**: Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan, Hengshuang Zhao

**Updated**: 2025-12-16T18:59:59Z

**Summary**: The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.

**Link**: [arxiv](https://arxiv.org/abs/2512.14699v1),  [pdf](https://arxiv.org/pdf/2512.14699v1)

**Tags**: cs.CV 



### Fast and Accurate Causal Parallel Decoding using Jacobi Forcing
**Authors**: Lanxiang Hu, Siqi Kou, Yichao Fu, Samyam Rajbhandari, Tajana Rosing, Yuxiong He, Zhijie Deng, Hao Zhang

**Updated**: 2025-12-16T18:45:18Z

**Summary**: Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.

**Link**: [arxiv](https://arxiv.org/abs/2512.14681v1),  [pdf](https://arxiv.org/pdf/2512.14681v1)

**Tags**: cs.CL 



### Tensor Product Attention Is All You Need
**Authors**: Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao

**Updated**: 2025-12-16T16:24:22Z

**Summary**: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA.

**Link**: [arxiv](https://arxiv.org/abs/2501.06425v6),  [pdf](https://arxiv.org/pdf/2501.06425v6)

**Tags**: cs.CL cs.AI cs.LG 



### Hybrid Cognitive IoT with Cooperative Caching and SWIPT-EH: A Hierarchical Reinforcement Learning Framework
**Authors**: Nadia Abdolkhani, Walaa Hamouda

**Updated**: 2025-12-16T15:18:50Z

**Summary**: This paper proposes a hierarchical deep reinforcement learning (DRL) framework based on the soft actor-critic (SAC) algorithm for hybrid underlay-overlay cognitive Internet of Things (CIoT) networks with simultaneous wireless information and power transfer (SWIPT)-energy harvesting (EH) and cooperative caching. Unlike prior hierarchical DRL approaches that focus primarily on spectrum access or power control, our work jointly optimizes EH, hybrid access coordination, power allocation, and caching in a unified framework. The joint optimization problem is formulated as a weighted-sum multi-objective task, designed to maximize throughput and cache hit ratio while simultaneously minimizing transmission delay. In the proposed model, CIoT agents jointly optimize EH and data transmission using a learnable time switching (TS) factor. They also coordinate spectrum access under hybrid overlay-underlay paradigms and make power control and cache placement decisions while considering energy, interference, and storage constraints. Specifically, in this work, cooperative caching is used to enable overlay access, while power control is used for underlay access. A novel three-level hierarchical SAC (H-SAC) agent decomposes the mixed discrete-continuous action space into modular subproblems, improving scalability and convergence over flat DRL methods. The high-level policy adjusts the TS factor, the mid-level policy manages spectrum access coordination and cache sharing, and the low-level policy decides transmit power and caching actions for both the CIoT agent and PU content. Simulation results show that the proposed hierarchical SAC approach significantly outperforms benchmark and greedy strategies. It achieves better performance in terms of average sum rate, delay, cache hit ratio, and energy efficiency, even under channel fading and uncertain conditions.

**Link**: [arxiv](https://arxiv.org/abs/2512.14488v1),  [pdf](https://arxiv.org/pdf/2512.14488v1)

**Tags**: eess.SP cs.NI 



### Kimina Lean Server: A High-Performance Lean Server for Large-Scale Verification
**Authors**: Marco Dos Santos, Hugues de Saxcé, Haiming Wang, Ran Wang, Mantas Baksys, Mert Unsal, Junqi Liu, Zhengying Liu, Jia Li

**Updated**: 2025-12-16T14:04:14Z

**Summary**: We introduce the Kimina Lean Server, an open-source project designed as a high-performance verifier for reinforcement learning pipelines. Built on top of the Lean REPL (Read-Eval-Print Loop) maintained by the Lean FRO, our server combines server-side parallelism by managing multiple Lean processes in parallel with a Least Recently Used (LRU) caching mechanism that reuses Lean imports across requests. On the client side, a lightweight Python package enables submitting proof batches and receiving Lean feedback, including extracted tactics and tactic states.   Together, these features enable a scalable workflow for large-scale verification and data extraction. In our experiments, the Kimina Lean Server outperforms previous Lean interaction tools, achieving a 1.5 to 2 times speedup in verification time. Moreover, its improved efficiency has enabled its use in the large-scale training of state-of-the-art models such as Kimina-Prover.   We hope that our open-source project will support the neural theorem proving community and accelerate future progress by enabling efficient large-scale verification and proof data extraction.

**Link**: [arxiv](https://arxiv.org/abs/2504.21230v3),  [pdf](https://arxiv.org/pdf/2504.21230v3)

**Tags**: cs.LO 



### Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement
**Authors**: Songze Liu, Hongkun Du, Shaowen Wang

**Updated**: 2025-12-16T07:16:10Z

**Summary**: Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.14151v1),  [pdf](https://arxiv.org/pdf/2512.14151v1)

**Tags**: cs.AR cs.PF 



### Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents
**Authors**: Hongqiu Ni, Jiabao Zhang, Guopeng Li, Zilong Wang, Ruiqi Wu, Chi Zhang, Haisheng Tan

**Updated**: 2025-12-16T06:55:10Z

**Summary**: Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.

**Link**: [arxiv](https://arxiv.org/abs/2512.14142v1),  [pdf](https://arxiv.org/pdf/2512.14142v1)

**Tags**: cs.CL 



### PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel
**Authors**: Jinjun Yi, Zhixin Zhao, Yitao Hu, Ke Yan, Weiwei Sun, Hao Wang, Laiping Zhao, Yuhao Zhang, Wenxin Li, Keqiu Li

**Updated**: 2025-12-16T05:44:34Z

**Summary**: LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: one-query-per-CTA execution repeatedly loads shared prefix KV cache, while one-size-fits-all tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.   This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 53.5% on average and TPOT by 17.0-93.1% under the same configurations against state-of-the-art attention kernels. PAT's source code is publicly available at https://github.com/flashserve/PAT.

**Link**: [arxiv](https://arxiv.org/abs/2511.22333v2),  [pdf](https://arxiv.org/pdf/2511.22333v2)

**Tags**: cs.DC cs.CL 



### OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration
**Authors**: Ruitong Sun, Tianze Yang, Wei Niu, Jin Sun

**Updated**: 2025-12-16T05:11:54Z

**Summary**: Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.

**Link**: [arxiv](https://arxiv.org/abs/2512.14096v1),  [pdf](https://arxiv.org/pdf/2512.14096v1)

**Tags**: cs.CV 



### FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference
**Authors**: Guangda Liu, Chengwei Li, Zhenyu Ning, Jing Lin, Yiwu Yao, Danning Ke, Minyi Guo, Jieru Zhao

**Updated**: 2025-12-16T04:58:49Z

**Summary**: Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\times$ speedup compared to SOTA KV retrieval methods.

**Link**: [arxiv](https://arxiv.org/abs/2505.13109v3),  [pdf](https://arxiv.org/pdf/2505.13109v3)

**Tags**: cs.LG cs.AI cs.CL 



### SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations
**Authors**: Wentao Guo, Mayank Mishra, Xinle Cheng, Ion Stoica, Tri Dao

**Updated**: 2025-12-16T04:39:10Z

**Summary**: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

**Link**: [arxiv](https://arxiv.org/abs/2512.14080v1),  [pdf](https://arxiv.org/pdf/2512.14080v1)

**Tags**: cs.LG cs.AI 



### Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed
**Authors**: Yonggan Fu, Lexington Whalen, Zhifan Ye, Xin Dong, Shizhe Diao, Jingyu Liu, Chengyue Wu, Hao Zhang, Enze Xie, Song Han, Maksim Khadkevich, Jan Kautz, Yingyan Celine Lin, Pavlo Molchanov

**Updated**: 2025-12-16T04:12:17Z

**Summary**: Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.

**Link**: [arxiv](https://arxiv.org/abs/2512.14067v1),  [pdf](https://arxiv.org/pdf/2512.14067v1)

**Tags**: cs.CL cs.AI cs.LG 



### Cooperative Caching Towards Efficient Spectrum Utilization in Cognitive-IoT Networks
**Authors**: Nadia Abdolkhani, Walaa Hamouda

**Updated**: 2025-12-16T02:49:43Z

**Summary**: In cognitive Internet of Things (CIoT) networks, efficient spectrum sharing is essential to address increasing wireless demands. This paper presents a novel deep reinforcement learning (DRL)-based approach for joint cooperative caching and spectrum access coordination in CIoT networks, enabling the CIoT agents to collaborate with primary users (PUs) by caching PU content and serving their requests, fostering mutual benefits. The proposed DRL framework jointly optimizes caching policy and spectrum access under challenging conditions. Unlike traditional cognitive radio (CR) methods, where CIoT agents vacate the spectrum for PUs, or relaying techniques, which merely support spectrum sharing, caching brings data closer to the edge, reducing latency by minimizing retrieval distance. Simulations demonstrate that our approach outperforms others in lowering latency, increasing CIoT and PU cache hit rates, and enhancing network throughput. This approach redefines spectrum sharing, offering a fresh perspective on CIoT network design and illustrating the potential of DRL-guided caching to highlight the benefits of collaboration over dynamic spectrum access scenarios, elevating CIoT performance under constrained resources.

**Link**: [arxiv](https://arxiv.org/abs/2512.14029v1),  [pdf](https://arxiv.org/pdf/2512.14029v1)

**Tags**: eess.SP cs.NI 



### Hyper-Minrank: A Unified Hypergraph Characterization of Multi-Sender Index Coding
**Authors**: Ali Khalesi, Petros Elia

**Updated**: 2025-12-15T18:08:09Z

**Summary**: This work introduces a hypergraph formulation that generalizes the classical paradigm of Bar-Yossef et al. to the multi-sender index coding (MSIC) setting. Central to the model is a 4-regular side-information hypergraph G, a new adjacency representation A_G = [A_1 ... A_N], and a simple fitting criterion for sub-hypergraph validity, in the presence of specially designed hyperedges that capture both side information and cross-sender signal cancellation. This formulation establishes a tight achievability-converse equivalence for the general N-sender, K-receiver problem: every valid fitting induces a valid linear multi-sender index code, every linear code induces a valid fitting, and the optimal scalar linear broadcast length equals the hyper-minrank l**lin(G) = hyperminrank(G) = min*{A fits G} sum_{n=1}^N rank(A_n). Beyond this exact characterization, the approach yields hypergraph analogues of Haemers-type bounds on the broadcast length, including a clique-cover upper bound and a lower bound via the clique number of a carefully defined complement hypergraph. Algorithmically, we provide an exact procedure to compute hyperminrank(G), and show that in certain regimes its complexity is asymptotically better than approximate LT-CMAR solutions. The framework captures well-known settings such as embedded index coding, and applies directly to multi-sender cache-aided communications, coded computation, distributed storage, and edge/satellite systems, where hyperminrank can serve as a unified design target.

**Link**: [arxiv](https://arxiv.org/abs/2512.13615v1),  [pdf](https://arxiv.org/pdf/2512.13615v1)

**Tags**: cs.IT math.CO 



### ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding
**Authors**: Jia-Nan Li, Jian Guan, Wei Wu, Chongxuan Li

**Updated**: 2025-12-15T17:41:19Z

**Summary**: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

**Link**: [arxiv](https://arxiv.org/abs/2512.13586v1),  [pdf](https://arxiv.org/pdf/2512.13586v1)

**Tags**: cs.CL cs.AI cs.LG 



### Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing
**Authors**: Zewen Qiang, Sendong Zhao, Haochun Wang, Bing Qin, Ting Liu

**Updated**: 2025-12-15T09:04:06Z

**Summary**: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

**Link**: [arxiv](https://arxiv.org/abs/2512.13109v1),  [pdf](https://arxiv.org/pdf/2512.13109v1)

**Tags**: cs.CL cs.AI 



### CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics
**Authors**: Dahyeon Kye, Jeahun Sung, Mingyu Jeon, Jihyong Oh

**Updated**: 2025-12-15T08:33:55Z

**Summary**: Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.

**Link**: [arxiv](https://arxiv.org/abs/2512.07155v3),  [pdf](https://arxiv.org/pdf/2512.07155v3)

**Tags**: cs.CV 



### SneakPeek: Future-Guided Instructional Streaming Video Generation
**Authors**: Cheeun Hong, German Barquero, Fadime Sener, Markos Georgopoulos, Edgar Schönfeld, Stefan Popov, Yuming Du, Oscar Mañas, Albert Pumarola

**Updated**: 2025-12-15T06:32:57Z

**Summary**: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

**Link**: [arxiv](https://arxiv.org/abs/2512.13019v1),  [pdf](https://arxiv.org/pdf/2512.13019v1)

**Tags**: cs.CV 



### SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference
**Authors**: Yuseon Choi, Sangjin Kim, Jungjun Oh, Gwangtae Park, Byeongcheol Kim, Hoi-Jun Yoo

**Updated**: 2025-12-15T05:33:07Z

**Summary**: MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.

**Link**: [arxiv](https://arxiv.org/abs/2512.12990v1),  [pdf](https://arxiv.org/pdf/2512.12990v1)

**Tags**: cs.AR 



### AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path
**Authors**: Zhengyang Yu, Akio Hayakawa, Masato Ishii, Qingtao Yu, Takashi Shibuya, Jing Zhang, Yuki Mitsufuji

**Updated**: 2025-12-15T05:13:40Z

**Summary**: Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.

**Link**: [arxiv](https://arxiv.org/abs/2512.11203v2),  [pdf](https://arxiv.org/pdf/2512.11203v2)

**Tags**: cs.CV 



### SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer
**Authors**: Luan Thanh Trinh, Kenji Doi, Atsuki Osanai

**Updated**: 2025-12-15T04:02:14Z

**Summary**: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

**Link**: [arxiv](https://arxiv.org/abs/2512.12963v1),  [pdf](https://arxiv.org/pdf/2512.12963v1)

**Tags**: cs.CV 



### RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training
**Authors**: Tianyuan Wu, Lunxi Cao, Yining Wei, Wei Gao, Yuheng Zhao, Dakai An, Shaopan Xiong, Zhiqiang Lv, Ju Huang, Siran Yang, Yinghao Yu, Jiamang Wang, Lin Qu, Wei Wang

**Updated**: 2025-12-15T02:21:28Z

**Summary**: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

**Link**: [arxiv](https://arxiv.org/abs/2512.11306v2),  [pdf](https://arxiv.org/pdf/2512.11306v2)

**Tags**: cs.DC 



### SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling
**Authors**: Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang

**Updated**: 2025-12-14T17:45:53Z

**Summary**: Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SUPERGEN, an efficient tile-based framework for ultra-high-resolution video generation. SUPERGEN features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SUPERGEN incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SUPERGEN also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations show that SUPERGEN maximizes performance gains while achieving high output quality across various benchmarks.

**Link**: [arxiv](https://arxiv.org/abs/2508.17756v2),  [pdf](https://arxiv.org/pdf/2508.17756v2)

**Tags**: cs.LG eess.SY 



### Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making
**Authors**: Heyang Ma, Qirui Mi, Qipeng Yang, Zijun Fan, Bo Li, Haifeng Zhang

**Updated**: 2025-12-14T15:38:21Z

**Summary**: Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.

**Link**: [arxiv](https://arxiv.org/abs/2511.12876v2),  [pdf](https://arxiv.org/pdf/2511.12876v2)

**Tags**: cs.AI econ.GN 



### Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving
**Authors**: Hui Zeng, Daming Zhao, Pengfei Yang, WenXuan Hou, Tianyang Zheng, Hui Li, Weiye Ji, Jidong Zhai

**Updated**: 2025-12-14T14:18:27Z

**Summary**: Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.

**Link**: [arxiv](https://arxiv.org/abs/2511.06029v3),  [pdf](https://arxiv.org/pdf/2511.06029v3)

**Tags**: cs.LG 



### Random Reshuffling for Stochastic Gradient Langevin Dynamics
**Authors**: Luke Shaw, Peter A. Whalley

**Updated**: 2025-12-14T11:12:56Z

**Summary**: We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent.

**Link**: [arxiv](https://arxiv.org/abs/2501.16055v2),  [pdf](https://arxiv.org/pdf/2501.16055v2)

**Tags**: math.NA math.PR stat.ML 



### No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching
**Authors**: Tingyan Wen, Haoyu Li, Yihuang Chen, Xing Zhou, Lifei Zhu, Xueqian Wang

**Updated**: 2025-12-14T09:02:18Z

**Summary**: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

**Link**: [arxiv](https://arxiv.org/abs/2512.12604v1),  [pdf](https://arxiv.org/pdf/2512.12604v1)

**Tags**: cs.CV 



### Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation
**Authors**: Karthikeya KV

**Updated**: 2025-12-14T08:28:50Z

**Summary**: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

**Link**: [arxiv](https://arxiv.org/abs/2512.12595v1),  [pdf](https://arxiv.org/pdf/2512.12595v1)

**Tags**: cs.CV 



### NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache
**Authors**: Donghyun Son, Euntae Choi, Sungjoo Yoo

**Updated**: 2025-12-14T08:17:35Z

**Summary**: Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\times$ throughput gain over full-precision baselines.

**Link**: [arxiv](https://arxiv.org/abs/2505.18231v2),  [pdf](https://arxiv.org/pdf/2505.18231v2)

**Tags**: cs.LG cs.AI 



### Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention
**Authors**: Tasweer Ahmad, Arindam Sikdar, Sandip Pradhan, Ardhendu Behera

**Updated**: 2025-12-13T23:53:00Z

**Summary**: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

**Link**: [arxiv](https://arxiv.org/abs/2512.12498v1),  [pdf](https://arxiv.org/pdf/2512.12498v1)

**Tags**: cs.CV 



### CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement
**Authors**: HT To, S Nguyen, NH Pham

**Updated**: 2025-12-13T08:42:18Z

**Summary**: Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\% and 50\%) demonstrates a geometric mean speedup of 2.46$\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\% to 84.8\% (+6.9 percentage points), reduce total runtime by 70.1\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.

**Link**: [arxiv](https://arxiv.org/abs/2512.12243v1),  [pdf](https://arxiv.org/pdf/2512.12243v1)

**Tags**: cs.RO 



### cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications
**Authors**: Xi Wang, Bin Ma, Jongryool Kim, Byungil Koh, Hoshik Kim, Dong Li

**Updated**: 2025-12-13T06:18:18Z

**Summary**: Message Passing Interface (MPI) is a foundational programming model for high-performance computing. MPI libraries traditionally employ network interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP and RoCE) with complex software stacks for cross-node communication. We present cMPI, the first work to optimize MPI point-to-point communication (both one-sided and two-sided) using CXL memory sharing on a real CXL platform, transforming cross-node communication into memory transactions and data copies within CXL memory, bypassing traditional network protocols. We analyze performance across various interconnects and find that CXL memory sharing achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in small- and medium-scale clusters. We address challenges of CXL memory sharing for MPI communication, including data object management over the dax representation [50], cache coherence, and atomic operations. Overall, cMPI outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x and 72x in latency and bandwidth, respectively, for small messages.

**Link**: [arxiv](https://arxiv.org/abs/2510.05476v2),  [pdf](https://arxiv.org/pdf/2510.05476v2)

**Tags**: cs.DC cs.AR cs.NI 



### GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes
**Authors**: Mohammad Pivezhandi, Mahdi Banisharif, Saeed Bakhshan, Abusayeed Saifullah, Ali Jannesari

**Updated**: 2025-12-12T23:46:05Z

**Summary**: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache   and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks   overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core   DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict   makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.   We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To   demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),   multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the   world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,   uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.12091v1),  [pdf](https://arxiv.org/pdf/2512.12091v1)

**Tags**: cs.LG 



### Multipole Attention for Efficient Long Context Reasoning
**Authors**: Coleman Hooper, Sebastian Zhao, Luca Manolache, Sehoon Kim, Michael W. Mahoney, Yakun Sophia Shao, Kurt Keutzer, Amir Gholami

**Updated**: 2025-12-12T19:51:26Z

**Summary**: Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.

**Link**: [arxiv](https://arxiv.org/abs/2506.13059v2),  [pdf](https://arxiv.org/pdf/2506.13059v2)

**Tags**: cs.CL cs.LG 



### Hold Onto That Thought: Assessing KV Cache Compression On Reasoning
**Authors**: Minghui Liu, Aadi Palnitkar, Tahseen Rabbani, Hyunwoo Jae, Kyle Rui Sang, Dixi Yao, Shayan Shabihi, Fuheng Zhao, Tian Li, Ce Zhang, Furong Huang, Kunpeng Zhang

**Updated**: 2025-12-12T19:50:34Z

**Summary**: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

**Link**: [arxiv](https://arxiv.org/abs/2512.12008v1),  [pdf](https://arxiv.org/pdf/2512.12008v1)

**Tags**: cs.CL cs.AI cs.PF 



### BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models
**Authors**: Xiaoyu Ma, Zhengqing Yuan, Zheyuan Zhang, Kaiwen Shi, Lichao Sun, Yanfang Ye

**Updated**: 2025-12-12T18:30:45Z

**Summary**: Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.

**Link**: [arxiv](https://arxiv.org/abs/2512.11769v1),  [pdf](https://arxiv.org/pdf/2512.11769v1)

**Tags**: cs.RO 



### PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration
**Authors**: Yifan Zhang, Zhiheng Chen, Ye Qiao, Sitao Huang

**Updated**: 2025-12-12T13:35:09Z

**Summary**: Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.

**Link**: [arxiv](https://arxiv.org/abs/2512.11550v1),  [pdf](https://arxiv.org/pdf/2512.11550v1)

**Tags**: cs.AR 



### Drinfeld associators and Kashiwara-Vergne associators in higher genera
**Authors**: Toyo Taniguchi

**Updated**: 2025-12-12T11:32:39Z

**Summary**: For $g\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction.

**Link**: [arxiv](https://arxiv.org/abs/2511.00473v2),  [pdf](https://arxiv.org/pdf/2511.00473v2)

**Tags**: math.QA math.AT 



### Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation
**Authors**: Jingmin Zhu, Anqi Zhu, Hossein Rahmani, Jun Liu, Mohammed Bennamoun, Qiuhong Ke

**Updated**: 2025-12-12T10:53:51Z

**Summary**: We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.

**Link**: [arxiv](https://arxiv.org/abs/2512.11458v1),  [pdf](https://arxiv.org/pdf/2512.11458v1)

**Tags**: cs.CV cs.AI 



### Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution
**Authors**: Qifan Zhang, Zilin Shen, Imtiaz Karim, Elisa Bertino, Zhou Li

**Updated**: 2025-12-12T10:12:06Z

**Summary**: The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional "break-and-fix" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.

**Link**: [arxiv](https://arxiv.org/abs/2512.11431v1),  [pdf](https://arxiv.org/pdf/2512.11431v1)

**Tags**: cs.CR cs.FL cs.NI 



### JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion
**Authors**: Chaochao Li, Ruikui Wang, Liangbo Zhou, Jinheng Feng, Huaishao Luo, Huan Zhang, Youzheng Wu, Xiaodong He

**Updated**: 2025-12-12T10:06:01Z

**Summary**: Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.

**Link**: [arxiv](https://arxiv.org/abs/2512.11423v1),  [pdf](https://arxiv.org/pdf/2512.11423v1)

**Tags**: cs.CV 



### FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion
**Authors**: Xiangyang Luo, Qingyu Li, Xiaokun Liu, Wenyu Qin, Miao Yang, Meng Wang, Pengfei Wan, Di Zhang, Kun Gai, Shao-Lun Huang

**Updated**: 2025-12-12T04:34:53Z

**Summary**: Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io

**Link**: [arxiv](https://arxiv.org/abs/2512.11274v1),  [pdf](https://arxiv.org/pdf/2512.11274v1)

**Tags**: cs.CV 



### Electrical Stability of Cr2O3/\b{eta}-Ga2O3 and NiOx/\b{eta}-Ga2O3 Heterojunction Diodes
**Authors**: Yizheng Liu, Haochen Wang, Carl Peterson, Chinmoy Nath Saha, Chris G. Van de Walle, Sriram Krishnamoorthy

**Updated**: 2025-12-12T03:57:52Z

**Summary**: This work reports the electrical characteristics comparison study between Cr2O3 and NiOx based heterojunction diodes (HJD) on halide vapor phase epitaxy (HVPE) grown \b{eta}-Ga2O3 epitaxial layers. Both as-fabricated Cr2O3 and NiOx HJDs exhibited forward current density in a range of 130-150 A/cm^2 at 5 V with rectifying ratios >10^10 and a reverse leakage current density at 10^-8 A/cm^2 at -5 V. The differential specific on-resistance of Cr2O3 and NiOx HJDs was 12.01 mΩ*cm^2 and 12.05 mΩ*cm^2, respectively. Breakdown voltages of Cr2O3 HJDs ranged from 1.4-1.9 kV and 1.5-2.3 kV for NiOx HJDs. Theoretical band alignment between Cr2O3 and \b{eta}-Ga2O3 was calculated from first principles. The ambient exposed NiOx/HVPE \b{eta}-Ga2O3 HJDs forward current density degraded after 10 days while that of Cr2O3/HVPE \b{eta}-Ga2O3 HJDs remained nearly unchanged after the same amount of time. It was later confirmed that the ambient exposed sputtered NiOx sheet resistance (Rsh) degradation gave rise to the reduction of the forward current density of the NiOx based HJDs, and water (H2O) was qualitatively determined to be the agent attributed to the forward conduction degradation by measuring the Rsh of NiOx-on-sapphire reference wafer after exposing it to different environments. The Cr2O3/HVPE \b{eta}-Ga2O3 HJD also exhibited enhanced thermal stability compared to the NiOx/\b{eta}-Ga2O3 heterostructures at elevated temperatures. Interfacial nickel gallate (Ga2NiO4) phase formation expected from phase diagrams can explain the reduced thermal stability of NiOx/\b{eta}-Ga2O3 HJDs. This study indicates that Cr2O3 is a stable p-type oxide for the realization of robust multi-kV \b{eta}-Ga2O3 HJDs.

**Link**: [arxiv](https://arxiv.org/abs/2512.11264v1),  [pdf](https://arxiv.org/pdf/2512.11264v1)

**Tags**: physics.app-ph cond-mat.mtrl-sci 



### REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation
**Authors**: Haotian Wang, Yuzhe Weng, Xinyi Yu, Jun Du, Haoran Xu, Xiaoyan Wu, Shan He, Bing Yin, Cong Liu, Qingfeng Liu

**Updated**: 2025-12-12T02:28:52Z

**Summary**: Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.

**Link**: [arxiv](https://arxiv.org/abs/2512.11229v1),  [pdf](https://arxiv.org/pdf/2512.11229v1)

**Tags**: cs.CV cs.SD 



### CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks
**Authors**: Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Subrata Mitra, Lina Yao, Julian McAuley

**Updated**: 2025-12-12T02:25:34Z

**Summary**: Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.

**Link**: [arxiv](https://arxiv.org/abs/2504.21228v2),  [pdf](https://arxiv.org/pdf/2504.21228v2)

**Tags**: cs.CR cs.AI 



### Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference
**Authors**: Adilet Metinov, Gulida M. Kudakeeva, Bolotbek uulu Nursultan, Gulnara D. Kabaeva

**Updated**: 2025-12-12T02:02:02Z

**Summary**: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

**Link**: [arxiv](https://arxiv.org/abs/2512.11221v1),  [pdf](https://arxiv.org/pdf/2512.11221v1)

**Tags**: cs.LG cs.AI cs.CL 



### KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference
**Authors**: Huawei Zhang, Chunwei Xia, Zheng Wang

**Updated**: 2025-12-11T23:35:19Z

**Summary**: Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.

**Link**: [arxiv](https://arxiv.org/abs/2511.11907v2),  [pdf](https://arxiv.org/pdf/2511.11907v2)

**Tags**: cs.DC cs.AI 



### CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving
**Authors**: Dong Liu, Yanxuan Yu

**Updated**: 2025-12-11T15:40:36Z

**Summary**: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

**Link**: [arxiv](https://arxiv.org/abs/2512.11920v1),  [pdf](https://arxiv.org/pdf/2512.11920v1)

**Tags**: cs.AI 



### ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp
**Authors**: Xinhang Chen, Chao Zhang, Jiahuan He, Wei Liu, Jianming Zhang, Wenlong Zhou, Xiao Li, Pai Zeng, Shiyong Li, Yuanpan Qian, Dong Li, Zhaogeng Li

**Updated**: 2025-12-11T12:06:00Z

**Summary**: DeepSeek-V3.2-Exp introduces a sparse attention mechanism that significantly reduces inference latency in long-context scenarios. Although the overall throughput has improved greatly, the Decode-stage of PD disaggregation remains to be a major bottleneck. This bottleneck primarily stems from the conflict between linear growth of Latent-Cache with sequence length and the limited GPU memory capacity, which constrains the feasible batch-size and thereby suppresses Decode-stage throughput.   To address this challenge, we propose ESS (Extended Sparse Server), an offload-centric system design tailored for DeepSeek-V3.2-Exp. ESS selectively offloads Latent-Cache to CPU memory while preserving latency-critical components on GPU. By freeing up GPU memory, ESS effectively decoupling batch-size scaling from GPU memory constraints. This design significantly improves Decode-stage throughput, thereby reducing deployment costs in real-world settings.   Our high-fidelity simulations show that ESS delivers 69.4\% throughput improvement at 32K context length and up to 123\% throughput improvement at 128K, demonstrating its effectiveness for large-context inference workloads. These results highlight ESS as a practical and scalable solution for long-context LLM serving.

**Link**: [arxiv](https://arxiv.org/abs/2512.10576v1),  [pdf](https://arxiv.org/pdf/2512.10576v1)

**Tags**: cs.DC 



### Unlocking the Address Book: Dissecting the Sparse Semantic Structure of LLM Key-Value Caches via Sparse Autoencoders
**Authors**: Qingsen Ma, Dianyun Wang, Jiaming Lyu, Yaoye Wang, Lechen Ning, Sujie Zhu, Zhenbo Xu, Liuyu Xiang, Huining Li, Huijia Wu, Zhaofeng He

**Updated**: 2025-12-11T11:23:50Z

**Summary**: The Key-Value (KV) cache is the primary memory bottleneck in long-context Large Language Models, yet it is typically treated as an opaque numerical tensor. In this work, we propose \textbf{STA-Attention}, a framework that utilizes Top-K Sparse Autoencoders (SAEs) to decompose the KV cache into interpretable ``semantic atoms.'' Unlike standard $L_1$-regularized SAEs, our Top-K approach eliminates shrinkage bias, preserving the precise dot-product geometry required for attention. Our analysis uncovers a fundamental \textbf{Key-Value Asymmetry}: while Key vectors serve as highly sparse routers dominated by a ``Semantic Elbow,'' deep Value vectors carry dense content payloads requiring a larger budget. Based on this structure, we introduce a Dual-Budget Strategy that selectively preserves the most informative semantic components while filtering representational noise. Experiments on Yi-6B, Mistral-7B, Qwen2.5-32B, and others show that our semantic reconstructions maintain perplexity and zero-shot performance comparable to the original models, effectively bridging the gap between mechanistic interpretability and faithful attention modeling.

**Link**: [arxiv](https://arxiv.org/abs/2512.10547v1),  [pdf](https://arxiv.org/pdf/2512.10547v1)

**Tags**: cs.LG 



### Electric-Field-Controlled Altermagnetic Transition for Neuromorphic Computing
**Authors**: Zhiyuan Duan, Peixin Qin, Chengyan Zhong, Shaoxuan Zhang, Li Liu, Guojian Zhao, Xiaoning Wang, Hongyu Chen, Ziang Meng, Jingyu Li, Sixu Jiang, Xiaoyang Tan, Qiong Wu, Yu Liu, Zhiqi Liu

**Updated**: 2025-12-11T08:14:00Z

**Summary**: Altermagnets represent a novel magnetic phase with transformative potential for ultrafast spintronics, yet efficient control of their magnetic states remains challenging. We demonstrate an ultra-low-power electric-field control of altermagnetism in MnTe through strain-mediated coupling in MnTe/PMN-PT heterostructures with negligible Joule heating. Application of +6 kV/cm electric fields induces piezoelectric strain in PMN-PT, modulating the Néel temperature from 310 to 328 K. As a result, around the magnetic phase transition, the altermagnetic spin splitting of MnTe is reversibly switched "on" and "off" by the electric fields. Meanwhile, the piezoelectric strain generates lattice distortions and magnetic structure changes in MnTe, enabling up to 9.7% resistance modulation around the magnetic phase transition temperature. Leveraging this effect, we implement programmable resistance states in a Hopfield neuromorphic network, achieving 100% pattern recognition accuracy at <=40% noise levels. This approach establishes the electric-field control as a low-power strategy for altermagnetic manipulation while demonstrating the viability of altermagnetic materials for energy-efficient neuromorphic computing beyond conventional charge-based architectures.

**Link**: [arxiv](https://arxiv.org/abs/2512.10405v1),  [pdf](https://arxiv.org/pdf/2512.10405v1)

**Tags**: cond-mat.mtrl-sci cond-mat.mes-hall physics.app-ph 



### LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation
**Authors**: Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian

**Updated**: 2025-12-11T08:10:13Z

**Summary**: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

**Link**: [arxiv](https://arxiv.org/abs/2511.00090v3),  [pdf](https://arxiv.org/pdf/2511.00090v3)

**Tags**: cs.CV cs.AI 



### Efficient-VLN: A Training-Efficient Vision-Language Navigation Model
**Authors**: Duo Zheng, Shijia Huang, Yanyang Li, Liwei Wang

**Updated**: 2025-12-11T05:57:48Z

**Summary**: Multimodal large language models (MLLMs) have shown promising potential in Vision-Language Navigation (VLN). However, their practical development is severely hindered by the substantial training overhead. We recognize two key issues that contribute to the overhead: (1) the quadratic computational burden from processing long-horizon historical observations as massive sequences of tokens, and (2) the exploration-efficiency trade-off in DAgger, i.e., a data aggregation process of collecting agent-explored trajectories. While more exploration yields effective error-recovery trajectories for handling test-time distribution shifts, it comes at the cost of longer trajectory lengths for both training and inference. To address these challenges, we propose Efficient-VLN, a training-efficient VLN model. Specifically, to mitigate the token processing burden, we design two efficient memory mechanisms: a progressive memory that dynamically allocates more tokens to recent observations, and a learnable recursive memory that utilizes the key-value cache of learnable tokens as the memory state. Moreover, we introduce a dynamic mixed policy to balance the exploration-efficiency trade-off. Extensive experiments show that Efficient-VLN achieves state-of-the-art performance on R2R-CE (64.2% SR) and RxR-CE (67.0% SR). Critically, our model consumes merely 282 H800 GPU hours, demonstrating a dramatic reduction in training overhead compared to state-of-the-art methods.

**Link**: [arxiv](https://arxiv.org/abs/2512.10310v1),  [pdf](https://arxiv.org/pdf/2512.10310v1)

**Tags**: cs.CV 



### Physically Aware 360$^\circ$ View Generation from a Single Image using Disentangled Scene Embeddings
**Authors**: Karthikeya KV, Narendra Bandaru

**Updated**: 2025-12-11T05:20:24Z

**Summary**: We introduce Disentangled360, an innovative 3D-aware technology that integrates the advantages of direction disentangled volume rendering with single-image 360° unique view synthesis for applications in medical imaging and natural scene reconstruction. In contrast to current techniques that either oversimplify anisotropic light behavior or lack generalizability across various contexts, our framework distinctly differentiates between isotropic and anisotropic contributions inside a Gaussian Splatting backbone. We implement a dual-branch conditioning framework, one optimized for CT intensity driven scattering in volumetric data and the other for real-world RGB scenes through normalized camera embeddings. To address scale ambiguity and maintain structural realism, we present a hybrid pose agnostic anchoring method that adaptively samples scene depth and material transitions, functioning as stable pivots during scene distillation. Our design integrates preoperative radiography simulation and consumer-grade 360° rendering into a singular inference pipeline, facilitating rapid, photorealistic view synthesis with inherent directionality. Evaluations on the Mip-NeRF 360, RealEstate10K, and DeepDRR datasets indicate superior SSIM and LPIPS performance, while runtime assessments confirm its viability for interactive applications. Disentangled360 facilitates mixed-reality medical supervision, robotic perception, and immersive content creation, eliminating the necessity for scene-specific finetuning or expensive photon simulations.

**Link**: [arxiv](https://arxiv.org/abs/2512.10293v1),  [pdf](https://arxiv.org/pdf/2512.10293v1)

**Tags**: cs.CV 



### Mixture of Lookup Key-Value Experts
**Authors**: Zongcheng Wang

**Updated**: 2025-12-10T15:05:55Z

**Summary**: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

**Link**: [arxiv](https://arxiv.org/abs/2512.09723v1),  [pdf](https://arxiv.org/pdf/2512.09723v1)

**Tags**: cs.LG 



### Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models
**Authors**: Jingcong Liang, Siyuan Wang, Miren Tian, Yitong Li, Duyu Tang, Zhongyu Wei

**Updated**: 2025-12-10T14:34:07Z

**Summary**: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .

**Link**: [arxiv](https://arxiv.org/abs/2505.16056v3),  [pdf](https://arxiv.org/pdf/2505.16056v3)

**Tags**: cs.LG cs.AI 



### Matrix-free algorithms for fast ab initio calculations on distributed CPU architectures using finite-element discretization
**Authors**: Gourab Panigrahi, Phani Motamarri

**Updated**: 2025-12-10T13:19:21Z

**Summary**: Finite-element (FE) discretisations have emerged as a powerful real-space alternative to large-scale Kohn-Sham density functional theory (DFT) calculations, offering systematic convergence, excellent parallel scalability, while accommodating generic boundary conditions. However, the dominant computational bottleneck in FE-based DFT arises from the repeated application of the discretised sparse Hamiltonian to large blocks of trial vectors during iterations in an iterative eigensolver. Traditional sparse matrix-vector multiplications and FE cell-matrix approaches encounter memory limitations and high data-movement overheads, particularly at higher polynomial orders, typically used in DFT calculations. To overcome these challenges, this work develops matrix-free algorithms for FE-discretised DFT that substantially accelerate these products by doing on-the-fly operations that utilize structured tensor contractions over 1D basis functions and quadrature data. A unified multilevel batched data layout that handles both real and complex-valued operators is introduced to maximise cache reuse and SIMD utilisation on Frontier (AVX2), Param Pravega (AVX512) and Fugaku (SVE). We also combine terms for optimal cache reuse, even-odd decomposition to reduce FLOP, and mixed-precision intrinsics. Extensive benchmarks show that for large multivector pseudopotential DFT calculations, the matrix-free kernels deliver 1.5-4x speedups over the state-of-the-art cell-matrix approach baselines. For all-electron DFT calculations, the matrix-free operator achieves gains of up to 5.8x due to its efficient implementation and superior arithmetic intensity. When integrated with an error-tolerant Chebyshev-filtered subspace iteration eigensolver, the matrix-free formalism yields substantial reductions in end-to-end time-to-solution using FE meshes that deliver desired accuracies in ground-state properties.

**Link**: [arxiv](https://arxiv.org/abs/2512.08571v2),  [pdf](https://arxiv.org/pdf/2512.08571v2)

**Tags**: physics.comp-ph 



### Supporting Dynamic Agentic Workloads: How Data and Agents Interact
**Authors**: Ioana Giurgiu, Michael E. Nidd

**Updated**: 2025-12-10T11:38:59Z

**Summary**: The rise of multi-agent systems powered by large language models (LLMs) and specialized reasoning agents exposes fundamental limitations in today's data management architectures. Traditional databases and data fabrics were designed for static, well-defined workloads, whereas agentic systems exhibit dynamic, context-driven, and collaborative behaviors. Agents continuously decompose tasks, shift attention across modalities, and share intermediate results with peers - producing non-deterministic, multi-modal workloads that strain conventional query optimizers and caching mechanisms. We propose an Agent-Centric Data Fabric, a unified architecture that rethinks how data systems serve, optimize, coordinate, and learn from agentic workloads. To achieve this we exploit the concepts of attention-guided data retrieval, semantic micro-caching for context-driven agent federations, predictive data prefetching and quorum-based data serving. Together, these mechanisms enable agents to access representative data faster and more efficiently, while reducing redundant queries, data movement, and inference load across systems. By framing data systems as adaptive collaborators, instead of static executors, we outline new research directions toward behaviorally responsive data infrastructures, where caching, probing, and orchestration jointly enable efficient, context-rich data exchange among dynamic, reasoning-driven agents.

**Link**: [arxiv](https://arxiv.org/abs/2512.09548v1),  [pdf](https://arxiv.org/pdf/2512.09548v1)

**Tags**: cs.MA 



### JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford
**Authors**: Xin Wang, Xi Chen

**Updated**: 2025-12-10T08:35:45Z

**Summary**: We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.

**Link**: [arxiv](https://arxiv.org/abs/2512.01802v3),  [pdf](https://arxiv.org/pdf/2512.01802v3)

**Tags**: cs.DS 



### Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM
**Authors**: Xun Li, Qiong Wu, Pingyi Fan, Kezhi Wang, Wen Chen, Khaled B. Letaief

**Updated**: 2025-12-10T07:19:32Z

**Summary**: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.

**Link**: [arxiv](https://arxiv.org/abs/2512.09378v1),  [pdf](https://arxiv.org/pdf/2512.09378v1)

**Tags**: cs.LG 



### SHARe-KAN: Holographic Vector Quantization for Memory-Bound Inference
**Authors**: Jeff Smith

**Updated**: 2025-12-10T04:10:13Z

**Summary**: Kolmogorov-Arnold Networks (KANs) face a fundamental memory wall: their learned basis functions create parameter counts that impose extreme bandwidth demands, hindering deployment in memory-constrained environments. We show that Vision KANs exhibit a holographic topology, where information is distributed across the interference of splines rather than localized to specific edges. Consequently, traditional pruning fails (10% sparsity degrades mAP from 85.23% to 45%, a $\sim$40-point drop). To address this, we present SHARe-KAN, a framework utilizing Gain-Shape-Bias Vector Quantization to exploit functional redundancy while preserving the dense topology. Coupled with LUTHAM, a hardware-aware compiler with static memory planning, we achieve $88\times$ runtime memory reduction (1.13 GB $\to$ 12.91 MB) and match uncompressed baseline accuracy on PASCAL VOC. Profiling on NVIDIA Ampere architecture confirms $>90\%$ L2 cache residency, demonstrating that the workload is decoupled from DRAM bandwidth constraints inherent to spline-based architectures.

**Link**: [arxiv](https://arxiv.org/abs/2512.15742v1),  [pdf](https://arxiv.org/pdf/2512.15742v1)

**Tags**: cs.LG cs.DC 



### TDC-Cache: A Trustworthy Decentralized Cooperative Caching Framework for Web3.0
**Authors**: Jinyu Chen, Long Shi, Taotao Wang, Jiaheng Wang, Wei Zhang

**Updated**: 2025-12-10T02:52:41Z

**Summary**: The rapid growth of Web3.0 is transforming the Internet from a centralized structure to decentralized, which empowers users with unprecedented self-sovereignty over their own data. However, in the context of decentralized data access within Web3.0, it is imperative to cope with efficiency concerns caused by the replication of redundant data, as well as security vulnerabilities caused by data inconsistency. To address these challenges, we develop a Trustworthy Decentralized Cooperative Caching (TDC-Cache) framework for Web3.0 to ensure efficient caching and enhance system resilience against adversarial threats. This framework features a two-layer architecture, wherein the Decentralized Oracle Network (DON) layer serves as a trusted intermediary platform for decentralized caching, bridging the contents from decentralized storage and the content requests from users. In light of the complexity of Web3.0 network topologies and data flows, we propose a Deep Reinforcement Learning-Based Decentralized Caching (DRL-DC) for TDC-Cache to dynamically optimize caching strategies of distributed oracles. Furthermore, we develop a Proof of Cooperative Learning (PoCL) consensus to maintain the consistency of decentralized caching decisions within DON. Experimental results show that, compared with existing approaches, the proposed framework reduces average access latency by 20%, increases the cache hit rate by at most 18%, and improves the average success consensus rate by 10%. Overall, this paper serves as a first foray into the investigation of decentralized caching framework and strategy for Web3.0.

**Link**: [arxiv](https://arxiv.org/abs/2512.09961v1),  [pdf](https://arxiv.org/pdf/2512.09961v1)

**Tags**: cs.DC cs.LG 



### Training-free Context-adaptive Attention for Efficient Long Context Modeling
**Authors**: Zeng You, Yaofo Chen, Shuhai Zhang, Zhijie Qiu, Tingyu Wu, Yingjian Li, Yaowei Wang, Mingkui Tan

**Updated**: 2025-12-10T01:54:57Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

**Link**: [arxiv](https://arxiv.org/abs/2512.09238v1),  [pdf](https://arxiv.org/pdf/2512.09238v1)

**Tags**: cs.CL 



### SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators
**Authors**: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Häggström, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Håkan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar

**Updated**: 2025-12-10T00:29:21Z

**Summary**: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.

**Link**: [arxiv](https://arxiv.org/abs/2511.03092v5),  [pdf](https://arxiv.org/pdf/2511.03092v5)

**Tags**: cs.AI cs.AR cs.DC 



### StreamingThinker: Large Language Models Can Think While Reading
**Authors**: Junlong Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen

**Updated**: 2025-12-09T17:34:02Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \textit{\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\% reduction in token waiting before the onset of reasoning and a more than 60\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker.

**Link**: [arxiv](https://arxiv.org/abs/2510.17238v2),  [pdf](https://arxiv.org/pdf/2510.17238v2)

**Tags**: cs.CL 



### InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models
**Authors**: Hongyuan Tao, Bencheng Liao, Shaoyu Chen, Haoran Yin, Qian Zhang, Wenyu Liu, Xinggang Wang

**Updated**: 2025-12-09T17:18:32Z

**Summary**: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

**Link**: [arxiv](https://arxiv.org/abs/2512.08829v1),  [pdf](https://arxiv.org/pdf/2512.08829v1)

**Tags**: cs.CV cs.AI 



### StarDist: A Code Generator for Distributed Graph Algorithms
**Authors**: Barenya Kumar Nandy, Rupesh Nasre

**Updated**: 2025-12-09T15:15:19Z

**Summary**: Relational data, occurring in the real world, are often structured as graphs, which provide the logical abstraction required to make analytical derivations simpler. As graphs get larger, the irregular access patterns exhibited in most graph algorithms, hamper performance. This, along with NUMA and physical memory limits, results in scaling complexities with sequential/shared memory frameworks. StarPlat's MPI backend abstracts away the programmatic complexity involved in designing optimal distributed graph algorithms. It provides an instrument for coding graph algorithms that scale over distributed memory. In this work, we provide an analysis-transformation framework that leverages general semantics associated with iterations involving nodes and their neighbors, within StarPlat, to aggregate communication. The framework scans for patterns that warrant re-ordering in neighborhood access patterns, aggregate communication, and avoid communication altogether with opportunistic caching in reduction constructs. We also architect an optimized bulk-reduction substrate using Open MPI's passive Remote Memory Access (RMA) constructs. We applied our optimization logic to StarPlat's distributed backend and outperformed d-Galois by 2.05 and DRONE by 1.44 times in Single Source Shortest Paths across several big data graphs.

**Link**: [arxiv](https://arxiv.org/abs/2512.01646v2),  [pdf](https://arxiv.org/pdf/2512.01646v2)

**Tags**: cs.DC 



### Inferring Causal Relationships to Improve Caching for Clients with Correlated Requests: Applications to VR
**Authors**: Agrim Bari, Gustavo de Veciana, Yuqi Zhou

**Updated**: 2025-12-09T14:10:41Z

**Summary**: Efficient edge caching reduces latency and alleviates backhaul congestion in modern networks. Traditional caching policies, such as Least Recently Used (LRU) and Least Frequently Used (LFU), perform well under specific request patterns. LRU excels in workloads with strong temporal locality, while LFU is effective when content popularity remains static. However, real-world client requests often exhibit correlations due to shared contexts and coordinated activities. This is particularly evident in Virtual Reality (VR) environments, where groups of clients navigate shared virtual spaces, leading to correlated content requests.   In this paper, we introduce the \textit{grouped client request model}, a generalization of the Independent Reference Model that explicitly captures different types of request correlations. Our theoretical analysis of LRU under this model reveals that the optimal causal caching policy depends on cache size: LFU is optimal for small to moderate caches, while LRU outperforms it for larger caches. To address the limitations of existing policies, we propose Least Following and Recently Used (LFRU), a novel online caching policy that dynamically infers and adapts to causal relationships in client requests to optimize evictions. LFRU prioritizes objects likely to be requested based on inferred dependencies, achieving near-optimal performance compared to the offline optimal Belady policy in structured correlation settings.   We develop VR based datasets to evaluate caching policies under realistic correlated requests. Our results show that LFRU consistently performs at least as well as LRU and LFU, outperforming LRU by up to 2.9x and LFU by up to1.9x in certain settings.

**Link**: [arxiv](https://arxiv.org/abs/2512.08626v1),  [pdf](https://arxiv.org/pdf/2512.08626v1)

**Tags**: cs.NI cs.SE 



### SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models
**Authors**: Jiayi Tian, Seyedarmin Azizi, Yequan Zhao, Erfan Baghaei Potraghloo, Sean McPherson, Sharath Nittur Sridhar, Zhengyang Wang, Zheng Zhang, Massoud Pedram, Souvik Kundu

**Updated**: 2025-12-08T19:32:06Z

**Summary**: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

**Link**: [arxiv](https://arxiv.org/abs/2512.07993v1),  [pdf](https://arxiv.org/pdf/2512.07993v1)

**Tags**: cs.AI 



### Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores
**Authors**: Vivek Chari, Benjamin Van Durme

**Updated**: 2025-12-08T19:02:12Z

**Summary**: Modern Large Language Models (LLMs) are increasingly trained to support very large context windows. We present Compactor, a training-free, query-agnostic KV compression strategy that uses approximate leverage scores to determine token importance. We show that Compactor can achieve the same performance as competing methods while retaining 20% fewer tokens in both synthetic and real-world context tasks, while being more task-robust. We further introduce a procedure for context-calibrated compression: inferring the maximum compression a given context supports before significant performance loss. Using context-calibrated compression, we show that Compactor achieves full KV performance on Longbench while reducing the KV memory burden by 68%, on average. To demonstrate the efficacy and generalizability of our approach, we apply Compactor to 27 synthetic and real-world tasks from RULER and Longbench, with models from both the Qwen 2.5 and Llama 3.1 families. Finally, we release compactor-vllm, an inference engine and suite of optimized Triton kernels designed to efficiently support the sparse, non-contiguous memory access patterns inherent to compressed KV caches. This work demonstrates that Compactor offers a practical, high-performance solution for alleviating the memory bottleneck in modern LLM deployment.

**Link**: [arxiv](https://arxiv.org/abs/2507.08143v2),  [pdf](https://arxiv.org/pdf/2507.08143v2)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Inference 
 ### XAgen: An Explainability Tool for Identifying and Correcting Failures in Multi-Agent Workflows
**Authors**: Xinru Wang, Ming Yin, Eunyee Koh, Mustafa Doga Dogan

**Updated**: 2025-12-19T18:54:36Z

**Summary**: As multi-agent systems powered by Large Language Models (LLMs) are increasingly adopted in real-world workflows, users with diverse technical backgrounds are now building and refining their own agentic processes. However, these systems can fail in opaque ways, making it difficult for users to observe, understand, and correct errors. We conducted formative interviews with 12 practitioners to identify mismatches between existing observability tools and users' needs. Based on these insights, we designed XAgen, an explainability tool that supports users with varying AI expertise through three core capabilities: log visualization for glanceable workflow understanding, human-in-the-loop feedback to capture expert judgment, and automatic error detection via an LLM-as-a-judge. In a user study with 8 participants, XAgen helped users more easily locate failures, attribute to specific agents or steps, and iteratively improve configurations. Our findings surface human-centered design guidelines for explainable agentic AI development and highlights opportunities for more context-aware interactive debugging.

**Link**: [arxiv](https://arxiv.org/abs/2512.17896v1),  [pdf](https://arxiv.org/pdf/2512.17896v1)

**Tags**: cs.HC 



### Mapping the Podcast Ecosystem with the Structured Podcast Research Corpus
**Authors**: Benjamin Litterer, David Jurgens, Dallas Card

**Updated**: 2025-12-19T18:46:41Z

**Summary**: Podcasts provide highly diverse content to a massive listener base through a unique on-demand modality. However, limited data has prevented large-scale computational analysis of the podcast ecosystem. To fill this gap, we introduce a massive dataset of over 1.1M podcast transcripts that is largely comprehensive of all English language podcasts available through public RSS feeds from May and June of 2020. This data is not limited to text, but rather includes audio features and speaker turns for a subset of 370K episodes, and speaker role inferences and other metadata for all 1.1M episodes. Using this data, we also conduct a foundational investigation into the content, structure, and responsiveness of this ecosystem. Together, our data and analyses open the door to continued computational research of this popular and impactful medium.

**Link**: [arxiv](https://arxiv.org/abs/2411.07892v2),  [pdf](https://arxiv.org/pdf/2411.07892v2)

**Tags**: cs.CL cs.CY 



### SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars
**Authors**: Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo

**Updated**: 2025-12-19T18:39:57Z

**Summary**: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP

**Link**: [arxiv](https://arxiv.org/abs/2507.01939v4),  [pdf](https://arxiv.org/pdf/2507.01939v4)

**Tags**: astro-ph.IM astro-ph.SR cs.AI cs.LG 



### mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs
**Authors**: Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava

**Updated**: 2025-12-19T18:30:30Z

**Summary**: Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.

**Link**: [arxiv](https://arxiv.org/abs/2512.15692v2),  [pdf](https://arxiv.org/pdf/2512.15692v2)

**Tags**: cs.RO cs.AI cs.CV cs.LG 



### Visually Prompted Benchmarks Are Surprisingly Fragile
**Authors**: Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa

**Updated**: 2025-12-19T18:26:58Z

**Summary**: A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.

**Link**: [arxiv](https://arxiv.org/abs/2512.17875v1),  [pdf](https://arxiv.org/pdf/2512.17875v1)

**Tags**: cs.CV cs.LG 



### Adaptive Focus Memory for Language Models
**Authors**: Christopher Cruz

**Updated**: 2025-12-19T18:24:09Z

**Summary**: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.   We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.   We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.   These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.12712v2),  [pdf](https://arxiv.org/pdf/2511.12712v2)

**Tags**: cs.CL cs.AI 



### Same Content, Different Representations: A Controlled Study for Table QA
**Authors**: Yue Zhang, Seiji Maekawa, Nikita Bhutani

**Updated**: 2025-12-19T18:19:10Z

**Summary**: Table Question Answering (Table QA) in real-world settings must operate over both structured databases and semi-structured tables containing textual fields. However, existing benchmarks are tied to fixed data formats and have not systematically examined how representation itself affects model performance. We present the first controlled study that isolates the role of table representation by holding content constant while varying structure. Using a verbalization pipeline, we generate paired structured and semi-structured tables, enabling direct comparisons across modeling paradigms. To support detailed analysis, we introduce RePairTQA, a diagnostic benchmark with splits along table size, join requirements, query complexity, and schema quality. Our experiments reveal consistent trade-offs: SQL-based methods achieve high accuracy on structured inputs but degrade on semi-structured data, LLMs exhibit flexibility but reduced precision, and hybrid approaches strike a balance, particularly under noisy schemas. These effects intensify with larger tables and more complex queries. Ultimately, no single method excels across all conditions, and we highlight the central role of representation in shaping Table QA performance. Our findings provide actionable insights for model selection and design, paving the way for more robust hybrid approaches suited for diverse real-world data formats.

**Link**: [arxiv](https://arxiv.org/abs/2509.22983v2),  [pdf](https://arxiv.org/pdf/2509.22983v2)

**Tags**: cs.CL 



### Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning
**Authors**: Simon Frieder, Jonas Bayer, Sam Looi, Jacob Loader, Julius Berner, Katherine M. Collins, András Juhász, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Cameron Freer, Thomas Lukasiewicz, Timothy Gowers

**Updated**: 2025-12-19T18:17:28Z

**Summary**: The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart's law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ``thought partners''), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models' mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of "motivated proof", introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.

**Link**: [arxiv](https://arxiv.org/abs/2412.15184v2),  [pdf](https://arxiv.org/pdf/2412.15184v2)

**Tags**: cs.LG 



### Delayed Acceptance Slice Sampling
**Authors**: Kevin Bitterlich, Daniel Rudolf, Björn Sprungk

**Updated**: 2025-12-19T18:17:19Z

**Summary**: Slice sampling is a well-established Markov chain Monte Carlo method for (approximate) sampling of target distributions which are only known up to a normalizing constant. The method is based on choosing a new state on a slice, i.e., a superlevel set of the given unnormalized target density (with respect to a reference measure). However, slice sampling algorithms usually require per step multiple evaluations of the target density, and thus can become computationally expensive. This is particularly the case for Bayesian inference with costly likelihoods. In this paper, we exploit deterministic approximations of the target density, which are relatively cheap to evaluate, and propose delayed acceptance versions of hybrid slice samplers. We show ergodicity of the resulting slice sampling methods, discuss the superiority of delayed acceptance (ideal) slice sampling over delayed acceptance Metropolis-Hastings algorithms, and illustrate the benefits of our novel approach in terms improved computational efficiency in several numerical experiments.

**Link**: [arxiv](https://arxiv.org/abs/2512.17868v1),  [pdf](https://arxiv.org/pdf/2512.17868v1)

**Tags**: stat.CO math.ST 



### Towards Human-Guided, Data-Centric LLM Co-Pilots
**Authors**: Evgeny Saveliev, Jiashuo Liu, Nabeel Seedat, Anders Boyd, Mihaela van der Schaar

**Updated**: 2025-12-19T18:08:16Z

**Summary**: Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC's ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML.

**Link**: [arxiv](https://arxiv.org/abs/2501.10321v3),  [pdf](https://arxiv.org/pdf/2501.10321v3)

**Tags**: cs.LG stat.ML 



### AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning
**Authors**: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper

**Updated**: 2025-12-19T17:55:48Z

**Summary**: Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .

**Link**: [arxiv](https://arxiv.org/abs/2512.17853v1),  [pdf](https://arxiv.org/pdf/2512.17853v1)

**Tags**: cs.RO cs.AI 



### InfSplign: Inference-Time Spatial Alignment of Text-to-Image Diffusion Models
**Authors**: Sarah Rastegar, Violeta Chatalbasheva, Sieger Falkena, Anuj Singh, Yanbo Wang, Tejas Gokhale, Hamid Palangi, Hadi Jamali-Rad

**Updated**: 2025-12-19T17:52:43Z

**Summary**: Text-to-image (T2I) diffusion models generate high-quality images but often fail to capture the spatial relations specified in text prompts. This limitation can be traced to two factors: lack of fine-grained spatial supervision in training data and inability of text embeddings to encode spatial semantics. We introduce InfSplign, a training-free inference-time method that improves spatial alignment by adjusting the noise through a compound loss in every denoising step. Proposed loss leverages different levels of cross-attention maps extracted from the backbone decoder to enforce accurate object placement and a balanced object presence during sampling. The method is lightweight, plug-and-play, and compatible with any diffusion backbone. Our comprehensive evaluations on VISOR and T2I-CompBench show that InfSplign establishes a new state-of-the-art (to the best of our knowledge), achieving substantial performance gains over the strongest existing inference-time baselines and even outperforming the fine-tuning-based methods. Codebase is available at GitHub.

**Link**: [arxiv](https://arxiv.org/abs/2512.17851v1),  [pdf](https://arxiv.org/pdf/2512.17851v1)

**Tags**: cs.CV cs.AI 



### Planning as Descent: Goal-Conditioned Latent Trajectory Synthesis in Learned Energy Landscapes
**Authors**: Carlos Vélez García, Miguel Cazorla, Jorge Pomares

**Updated**: 2025-12-19T17:49:13Z

**Summary**: We present Planning as Descent (PaD), a framework for offline goal-conditioned reinforcement learning that grounds trajectory synthesis in verification. Instead of learning a policy or explicit planner, PaD learns a goal-conditioned energy function over entire latent trajectories, assigning low energy to feasible, goal-consistent futures. Planning is realized as gradient-based refinement in this energy landscape, using identical computation during training and inference to reduce train-test mismatch common in decoupled modeling pipelines.   PaD is trained via self-supervised hindsight goal relabeling, shaping the energy landscape around the planning dynamics. At inference, multiple trajectory candidates are refined under different temporal hypotheses, and low-energy plans balancing feasibility and efficiency are selected.   We evaluate PaD on OGBench cube manipulation tasks. When trained on narrow expert demonstrations, PaD achieves state-of-the-art 95\% success, strongly outperforming prior methods that peak at 68\%. Remarkably, training on noisy, suboptimal data further improves success and plan efficiency, highlighting the benefits of verification-driven planning. Our results suggest learning to evaluate and refine trajectories provides a robust alternative to direct policy learning for offline, reward-free planning.

**Link**: [arxiv](https://arxiv.org/abs/2512.17846v1),  [pdf](https://arxiv.org/pdf/2512.17846v1)

**Tags**: cs.RO cs.AI 



### Another look at inference after prediction
**Authors**: Jessica Gronsbell, Jianhui Gao, Yaqi Shi, Zachary R. McCaw, David Cheng

**Updated**: 2025-12-19T17:48:29Z

**Summary**: From structural biology to epidemiology, predictions from machine learning (ML) models are increasingly used to complement costly gold-standard data to enable faster, more affordable, and scalable scientific inquiry. In response, prediction-based (PB) inference has emerged to accommodate statistical analysis using a large volume of predictions together with a small amount of gold-standard data. The goals of PB inference are two-fold: (i) to mitigate bias from errors in predictions and (ii) to improve efficiency relative to traditional inference using only the gold-standard data. While early PB inference methods focused on bias, their ability to enhance efficiency remains unclear. We revisit a popular PB inference method and show that a simple modification can be applied to guarantee improvements in efficiency beyond yielding valid inferences when the ML predictions are imperfect. The utility of this approach in leveraging prediction-based outcomes to enhance efficiency is demonstrated through extensive simulation studies and an application to the UK Biobank data. We further contextualize the problem of PB inference through historical literature from economics and statistics to highlight perspectives from classical methods in this contemporary problem.

**Link**: [arxiv](https://arxiv.org/abs/2411.19908v6),  [pdf](https://arxiv.org/pdf/2411.19908v6)

**Tags**: stat.ML cs.LG 



### ShareChat: A Dataset of Chatbot Conversations in the Wild
**Authors**: Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le

**Updated**: 2025-12-19T17:47:53Z

**Summary**: While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.

**Link**: [arxiv](https://arxiv.org/abs/2512.17843v1),  [pdf](https://arxiv.org/pdf/2512.17843v1)

**Tags**: cs.CL cs.AI cs.HC 



### NeuRehab: A Reinforcement Learning and Spiking Neural Network-Based Rehab Automation Framework
**Authors**: Phani Pavan Kambhampati, Chainesh Gautam, Jagan Palaniswamy, Madhav Rao

**Updated**: 2025-12-19T17:47:14Z

**Summary**: Recent advancements in robotic rehabilitation therapy have provided modular exercise systems for post-stroke muscle recovery with basic control schemes. But these systems struggle to adapt to patients' complex and ever-changing behaviour, and to operate within mobile settings, such as heat and power. To aid this, we present NeuRehab: an end-to-end framework consisting of a training and inference pipeline with AI-based automation, co-designed with neuromorphic computing-based control systems that balance action performance, power consumption, and observed latency. The framework consists of 2 partitions. One is designated for the rehabilitation device based on ultra-low power spiking networks deployed on dedicated neuromorphic hardware. The other resides on stationary hardware that can accommodate computationally intensive hardware for fine-tuning on a per-patient basis. By maintaining a communication channel between both the modules and splitting the algorithm components, the power and latency requirements of the movable system have been optimised, while retaining the learning performance advantages of compute- and power-hungry hardware on the stationary machine. As part of the framework, we propose (a) the split machine learning processes for efficiency in architectural utilisation, and (b) task-specific temporal optimisations to lower edge-inference control latency. This paper evaluates the proposed methods on a reference stepper motor-based shoulder exercise. Overall, these methods offer comparable performance uplifts over the State-of-the-art for neuromorphic deployment, while achieving over 60% savings in both power and latency during inference compared to standard implementations.

**Link**: [arxiv](https://arxiv.org/abs/2512.17841v1),  [pdf](https://arxiv.org/pdf/2512.17841v1)

**Tags**: cs.CE eess.SY 



### ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges
**Authors**: Roshan Kenia, Xiaoman Zhang, Pranav Rajpurkar

**Updated**: 2025-12-19T17:44:40Z

**Summary**: Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.17838v1),  [pdf](https://arxiv.org/pdf/2512.17838v1)

**Tags**: cs.CV 



### RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation
**Authors**: Dongyub Jude Lee, Zhenyi Ye, Pengcheng He

**Updated**: 2025-12-19T17:35:31Z

**Summary**: Preference-learning methods for machine translation (MT), such as Direct Preference Optimization (DPO), have shown strong gains but typically rely on large, carefully curated preference triplets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), which replaces static triplets with on-policy, actor-conditioned refinements produced by a frozen teacher. At each step, the actor samples candidate translations, the teacher performs a minimal local edit of each draft, and the actor is reinforced to close the gap using a composite reward that combines scaled negative edit distance for lexical and structural fidelity with COMET for semantic adequacy. This formulation yields a stable, model-aware learning signal without requiring explicit preference datasets. Experiments on FLORES-200 (English to German, Spanish, Chinese, Korean, and Japanese) show that RLfR consistently outperforms strong MT-SFT, DPO, and fixed-reference RL baselines, improving semantic quality and entity preservation, and also achieves superior performance under LLM-based judge evaluations.

**Link**: [arxiv](https://arxiv.org/abs/2507.22219v3),  [pdf](https://arxiv.org/pdf/2507.22219v3)

**Tags**: cs.CL cs.AI 



### A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code
**Authors**: Alejandro Velasco, Daniel Rodriguez-Cardenas, Dipin Khati, David N. Palacio, Luftar Rahman Alif, Denys Poshyvanyk

**Updated**: 2025-12-19T17:21:22Z

**Summary**: Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.

**Link**: [arxiv](https://arxiv.org/abs/2511.15817v4),  [pdf](https://arxiv.org/pdf/2511.15817v4)

**Tags**: cs.SE 



### LLM-based Behaviour Driven Development for Hardware Design
**Authors**: Rolf Drechsler, Qian Liu

**Updated**: 2025-12-19T17:19:08Z

**Summary**: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.   Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

**Link**: [arxiv](https://arxiv.org/abs/2512.17814v1),  [pdf](https://arxiv.org/pdf/2512.17814v1)

**Tags**: cs.SE cs.AI cs.AR 



### Foundations for an Abstract Proof Theory in the Context of Horn Rules
**Authors**: Tim S. Lyon, Piotr Ostropolski-Nalewaja

**Updated**: 2025-12-19T16:59:33Z

**Summary**: We introduce a novel, logic-independent framework for the study of sequent-style proof systems, which covers a number of proof-theoretic formalisms and concrete proof systems that appear in the literature. In particular, we introduce a generalized form of sequents, dubbed 'g-sequents,' which are taken to be binary graphs of typical, Gentzen-style sequents. We then define a variety of 'inference rule types' as sets of operations that act over such objects, and define 'abstract (sequent) calculi' as pairs consisting of a set of g-sequents together with a finite set of operations. Our approach permits an analysis of how certain inference rule types interact in a general setting, demonstrating under what conditions rules of a specific type can be permuted with or simulated by others, and being applicable to any sequent-style proof system that fits within our framework. We then leverage our permutation and simulation results to establish generic calculus and proof transformation algorithms, which show that every abstract calculus can be effectively transformed into a lattice of polynomially equivalent abstract calculi. We determine the complexity of computing this lattice and compute the relative sizes of proofs and sequents within distinct calculi of a lattice. We recognize that top and bottom elements in lattices correspond to many known deep-inference nested sequent systems and labeled sequent systems (respectively) for logics characterized by Horn properties.

**Link**: [arxiv](https://arxiv.org/abs/2304.05697v2),  [pdf](https://arxiv.org/pdf/2304.05697v2)

**Tags**: cs.LO cs.DM cs.DS math.LO 



### Reinforced Generation of Combinatorial Structures: Hardness of Approximation
**Authors**: Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta

**Updated**: 2025-12-19T16:58:50Z

**Summary**: Can AI based methods help us make advances in complexity theory? We provide evidence towards answering this in the affirmative, using AlphaEvolve (an LLM code mutation agent) to obtain new results in three settings:   a) We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ vertices, and our upper bounds are obtained via analytical arguments.   b) We obtain new inapproximability results for MAX-4-CUT and MAX-3-CUT, proving that it is NP-hard to approximate them within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of the SOTA of $16/17$ that relies on a custom PCP (rather than a reduction from ``standard'' Håstad-style PCPs).   c) Inapproximability for the metric Traveling Salesman Problem (TSP): We show that it is NP-hard to approximate the minimum cost tour within a factor of $111/110$ using AlphaEvolve to discover a new gadget, thus improving the SOTA of $117/116$. Along the way, we provide new modular soundness and completeness arguments that can be of independent interest.   A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (sometimes requiring time exponential in the size of the construction). We used AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$ for our gadgets). Our results suggest that gadget based proofs would benefit from a pass through AI-based tools to obtain stronger results.

**Link**: [arxiv](https://arxiv.org/abs/2509.18057v6),  [pdf](https://arxiv.org/pdf/2509.18057v6)

**Tags**: cs.LG cs.AI cs.CC math.CO 



### Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning
**Authors**: Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Vito Ostuni, Jundong Li, Nathan Kallus

**Updated**: 2025-12-19T16:58:20Z

**Summary**: Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO.

**Link**: [arxiv](https://arxiv.org/abs/2510.20150v4),  [pdf](https://arxiv.org/pdf/2510.20150v4)

**Tags**: cs.IR 



### Selected topics on: 1) proposal of interpreting the Crab supernova with a GRB 2) progress in identifying the seven GRBs episodes 3) the role of Sagittarius A in identifying the dark matter component (the X fermion)
**Authors**: R. Ruffini, C. Sigismondi, Y. Wang, J. A. Rueda, H. Quevedo, S. Zhang, Y. Aimuratov, P. Chardonnet, M. Della Valle, C. L. Fryer, T. Mirtorabi, R. Moradi, M. Prakapenia, F. Rastegarnia, S. -S. Xue

**Updated**: 2025-12-19T16:58:14Z

**Summary**: As the fiftieth anniversary of our common effort in the field of relativistic astrophysics is approaching, we offer a new look to some of our acquired knowledge in a more complete view, which evidence previous unnoticed connections. They are gaining due prominence in reaching a more complete picture evidencing the main results.   We outline the history of GRB observations along with a summary of the contributions made by our group to develop the BdHN interpreting model. We show the seven Episodes characterizing the most powerful BdHNe I occurred to date: GRB 190114C and GRB 220101A. New inferences for the explanation of the highest energy radiation in the TeV are presented.

**Link**: [arxiv](https://arxiv.org/abs/2512.17787v1),  [pdf](https://arxiv.org/pdf/2512.17787v1)

**Tags**: astro-ph.HE 



### HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs
**Authors**: Chang Sun, Zhiqiang Que, Thea K. Årrestad, Vladimir Loncar, Jennifer Ngadiuba, Wayne Luk, Maria Spiropulu

**Updated**: 2025-12-19T16:57:39Z

**Summary**: Neural networks with sub-microsecond inference latency are required by many critical applications. Targeting such applications deployed on FPGAs, we present High Granularity Quantization (HGQ), a quantization-aware training framework that optimizes parameter bit-widths through gradient descent. Unlike conventional methods, HGQ determines the optimal bit-width for each parameter independently, making it suitable for hardware platforms supporting heterogeneous arbitrary precision arithmetic. In our experiments, HGQ shows superior performance compared to existing network compression methods, achieving orders of magnitude reduction in resource consumption and latency while maintaining the accuracy on several benchmark tasks. These improvements enable the deployment of complex models previously infeasible due to resource or latency constraints. HGQ is open-source and is used for developing next-generation trigger systems at the CERN ATLAS and CMS experiments for particle physics, enabling the use of advanced machine learning models for real-time data selection with sub-microsecond latency.

**Link**: [arxiv](https://arxiv.org/abs/2405.00645v3),  [pdf](https://arxiv.org/pdf/2405.00645v3)

**Tags**: cs.LG physics.ins-det 



### UrbanDIFF: A Denoising Diffusion Model for Spatial Gap Filling of Urban Land Surface Temperature Under Dense Cloud Cover
**Authors**: Arya Chavoshi, Hassan Dashtian, Naveen Sudharsan, Dev Niyogi

**Updated**: 2025-12-19T16:51:29Z

**Summary**: Satellite-derived Land Surface Temperature (LST) products are central to surface urban heat island (SUHI) monitoring due to their consistent grid-based coverage over large metropolitan regions. However, cloud contamination frequently obscures LST observations, limiting their usability for continuous SUHI analysis. Most existing LST reconstruction methods rely on multitemporal information or multisensor data fusion, requiring auxiliary observations that may be unavailable or unreliable under persistent cloud cover. Purely spatial gap-filling approaches offer an alternative, but traditional statistical methods degrade under large or spatially contiguous gaps, while many deep learning based spatial models deteriorate rapidly with increasing missingness.   Recent advances in denoising diffusion based image inpainting models have demonstrated improved robustness under high missingness, motivating their adoption for spatial LST reconstruction. In this work, we introduce UrbanDIFF, a purely spatial denoising diffusion model for reconstructing cloud contaminated urban LST imagery. The model is conditioned on static urban structure information, including built-up surface data and a digital elevation model, and enforces strict consistency with revealed cloud free pixels through a supervised pixel guided refinement step during inference.   UrbanDIFF is trained and evaluated using NASA MODIS Terra LST data from seven major United States metropolitan areas spanning 2002 to 2025. Experiments using synthetic cloud masks with 20 to 85 percent coverage show that UrbanDIFF consistently outperforms an interpolation baseline, particularly under dense cloud occlusion, achieving SSIM of 0.89, RMSE of 1.2 K, and R2 of 0.84 at 85 percent cloud coverage, while exhibiting slower performance degradation as cloud density increases.

**Link**: [arxiv](https://arxiv.org/abs/2512.17782v1),  [pdf](https://arxiv.org/pdf/2512.17782v1)

**Tags**: cs.CV 



### LiteGE: Lightweight Geodesic Embedding for Efficient Geodesics Computation and Non-Isometric Shape Correspondence
**Authors**: Yohanes Yudhi Adikusuma, Qixing Huang, Ying He

**Updated**: 2025-12-19T16:50:52Z

**Summary**: Computing geodesic distances on 3D surfaces is fundamental to many tasks in 3D vision and geometry processing, with deep connections to tasks such as shape correspondence. Recent learning-based methods achieve strong performance but rely on large 3D backbones, leading to high memory usage and latency, which limit their use in interactive or resource-constrained settings. We introduce LiteGE, a lightweight approach that constructs compact, category-aware shape descriptors by applying PCA to unsigned distance field (UDFs) samples at informative voxels. This descriptor is efficient to compute and removes the need for high-capacity networks. LiteGE remains robust on sparse point clouds, supporting inputs with as few as 300 points, where prior methods fail. Extensive experiments show that LiteGE reduces memory usage and inference time by up to 300$\times$ compared to existing neural approaches. In addition, by exploiting the intrinsic relationship between geodesic distance and shape correspondence, LiteGE enables fast and accurate shape matching. Our method achieves up to 1000$\times$ speedup over state-of-the-art mesh-based approaches while maintaining comparable accuracy on non-isometric shape pairs, including evaluations on point-cloud inputs.

**Link**: [arxiv](https://arxiv.org/abs/2512.17781v1),  [pdf](https://arxiv.org/pdf/2512.17781v1)

**Tags**: cs.CV cs.GR 



### Mass Proxy Quality of Massive Halo Properties in the IllustrisTNG and FLAMINGO Simulations: I. Hot Gas
**Authors**: Eddie Aljamal, August E. Evrard, Arya Farahi, Annalisa Pillepich, Dylan Nelson, Joop Schaye, Matthieu Schaller, Joey Braspenning

**Updated**: 2025-12-19T16:49:09Z

**Summary**: We examine scale and redshift dependence of mass-property relations (MPRs) for five hot gas properties of two large group- and cluster-scale halo samples realized by the IllustrisTNG, TNG-Cluster and FLAMINGO cosmological hydrodynamical simulations. For intrinsic properties of i) hot gas mass ($M_{\rm gas}$), ii) spectroscopic-like temperature ($T_{\rm sl}$), iii) soft-band X-ray luminosity ($L_{\rm X}$), and iv) X-ray ($Y_{\rm X}$) and v) Sunyaev-Zel'dovich ($Y_{\rm SZ}$) thermal energies, we use MPR parameters to infer mass proxy quality (MPQ) -- the implied scatter in total halo mass conditioned on a property -- for halos with $M_{\rm 500c} \geq 10^{13}{\, {\rm M}_\odot}$ at redshifts, $z \in \{0, 0.5, 1, 2\}$. We find: (1) in general, scaling relation slopes and covariance display moderate to strong dependence on halo mass, with redshift dependence secondary; (2) for halos with $M_{\rm 500c} > 10^{14}{\, {\rm M}_\odot}$, scalings of $M_{\rm gas}$ and $Y_{\rm SZ}$ simplify toward self-similar slope and constant intrinsic scatter (5 and 10 per cent, respectively) nearly independent of scale, making both measures ideal for cluster finding and characterization to $z=2$; (3) halo mass-conditioned likelihoods of hot gas mass and thermal energy at fixed halo mass closely follow a log-normal form; (4) despite normalization differences ranging up to $0.4$ dex between the two simulations, higher order scaling features such as slopes and property covariance show much better agreement. Slopes show appreciable redshift dependence at the group scale, while redshift dependence of the scatter is exhibited by low-mass FLAMINGO halos only; (5) property correlations are largely consistent between the simulations, with values that mainly agree with existing empirical measurements. We close with a literature survey placing our MPR slopes and intrinsic scatter estimates into community context.

**Link**: [arxiv](https://arxiv.org/abs/2507.05176v2),  [pdf](https://arxiv.org/pdf/2507.05176v2)

**Tags**: astro-ph.GA astro-ph.CO 



### DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports
**Authors**: Janghoon Han, Heegyu Kim, Changho Lee, Dahm Lee, Min Hyung Park, Hosung Song, Stanley Jungkyu Choi, Moontae Lee, Honglak Lee

**Updated**: 2025-12-19T16:46:20Z

**Summary**: As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.

**Link**: [arxiv](https://arxiv.org/abs/2512.17776v1),  [pdf](https://arxiv.org/pdf/2512.17776v1)

**Tags**: cs.CL 



### Pix2NPHM: Learning to Regress NPHM Reconstructions From a Single Image
**Authors**: Simon Giebenhain, Tobias Kirschstein, Liam Schoneveld, Davide Davoli, Zhe Chen, Matthias Nießner

**Updated**: 2025-12-19T16:44:32Z

**Summary**: Neural Parametric Head Models (NPHMs) are a recent advancement over mesh-based 3d morphable models (3DMMs) to facilitate high-fidelity geometric detail. However, fitting NPHMs to visual inputs is notoriously challenging due to the expressive nature of their underlying latent space. To this end, we propose Pix2NPHM, a vision transformer (ViT) network that directly regresses NPHM parameters, given a single image as input. Compared to existing approaches, the neural parametric space allows our method to reconstruct more recognizable facial geometry and accurate facial expressions. For broad generalization, we exploit domain-specific ViTs as backbones, which are pretrained on geometric prediction tasks. We train Pix2NPHM on a mixture of 3D data, including a total of over 100K NPHM registrations that enable direct supervision in SDF space, and large-scale 2D video datasets, for which normal estimates serve as pseudo ground truth geometry. Pix2NPHM not only allows for 3D reconstructions at interactive frame rates, it is also possible to improve geometric fidelity by a subsequent inference-time optimization against estimated surface normals and canonical point maps. As a result, we achieve unprecedented face reconstruction quality that can run at scale on in-the-wild data.

**Link**: [arxiv](https://arxiv.org/abs/2512.17773v1),  [pdf](https://arxiv.org/pdf/2512.17773v1)

**Tags**: cs.CV cs.AI 



### ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics
**Authors**: Li S. Yifei, Allen Chang, Chaitanya Malaviya, Mark Yatskar

**Updated**: 2025-12-19T16:38:02Z

**Summary**: Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is abundant: survey articles consolidate knowledge spread across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Queries and rubrics are jointly derived from survey sections, where rubric items list query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. 31 Ph.D. annotators in 8 fields judge that 90% of queries reflect Ph.D. information needs and 87% of rubric items warrant emphasis of a sentence or longer. We leverage ResearchQA to evaluate 18 systems in 7.6K head-to-heads. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.

**Link**: [arxiv](https://arxiv.org/abs/2509.00496v2),  [pdf](https://arxiv.org/pdf/2509.00496v2)

**Tags**: cs.CL cs.AI 



### LLM-as-a-qualitative-judge: automating error analysis in natural language generation
**Authors**: Nadezhda Chirkova, Tunde Oluwaseyi Ajayi, Seth Aycock, Zain Muhammad Mujahid, Vladana Perlić, Ekaterina Borisova, Markarit Vartampetian

**Updated**: 2025-12-19T16:35:50Z

**Summary**: Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that instance-specific issues output by LLM-as-a-qualitative-judge match those annotated by humans in 2/3 cases, and that LLM-as-a-qualitative-judge is capable of producing error type reports resembling the reports composed by human annotators. We also demonstrate in a case study how the use of LLM-as-a-qualitative-judge can substantially improve NLG systems performance. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

**Link**: [arxiv](https://arxiv.org/abs/2506.09147v4),  [pdf](https://arxiv.org/pdf/2506.09147v4)

**Tags**: cs.CL cs.AI 



### Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity
**Authors**: Hauke Licht

**Updated**: 2025-12-19T16:35:45Z

**Summary**: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze emotions, the emergence of multimodal generative Artificial Intelligence (AI) promises great advances. However, we lack evidence about the effectiveness of multimodal AI in analyzing emotions in political communication. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in the video-based analysis of emotional arousal, using two complementary datasets of human-labeled video recordings. It finds that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and exhibit little to no demographic bias. However, in recordings of real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in multimodal political analysis and contributes a suitable replicable framework.

**Link**: [arxiv](https://arxiv.org/abs/2512.10882v2),  [pdf](https://arxiv.org/pdf/2512.10882v2)

**Tags**: cs.CL 



### Quantum Generative Modeling of Single-Cell transcriptomes: Capturing Gene-Gene and Cell-Cell Interactions
**Authors**: Selim Romero, Vignesh S. Kumar, Robert S. Chapkin, James J. Cai

**Updated**: 2025-12-19T16:30:37Z

**Summary**: Single-cell RNA sequencing (scRNA-seq) data simulation is limited by classical methods that rely on linear correlations, failing to capture the intrinsic, nonlinear dependencies. No existing simulator jointly models gene-gene and cell-cell interactions. We introduce qSimCells, a novel quantum computing-based simulator that employs entanglement to model intra- and inter-cellular interactions, generating realistic single-cell transcriptomes with cellular heterogeneity. The core innovation is a quantum kernel that uses a parameterized quantum circuit with CNOT gates to encode complex, nonlinear gene regulatory network (GRN) as well as cell-cell communication topologies with explicit causal directionality. The resulting synthetic data exhibits non-classical dependencies: standard correlation-based analyses (Pearson and Spearman) fail to recover the programmed causal pathways and instead report spurious associations driven by high baseline gene-expression probabilities. Furthermore, applying cell-cell communication detection to the simulated data validates the true mechanistic links, revealing a robust, up to 75-fold relative increase in inferred communication probability only when quantum entanglement is active. These results demonstrate that the quantum kernel is essential for producing high-fidelity ground-truth datasets and highlight the need for advanced inference techniques to capture the complex, non-classical dependencies inherent in gene regulation.

**Link**: [arxiv](https://arxiv.org/abs/2510.12776v3),  [pdf](https://arxiv.org/pdf/2510.12776v3)

**Tags**: q-bio.QM cs.ET physics.bio-ph q-bio.GN 



### PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning
**Authors**: Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu

**Updated**: 2025-12-19T16:27:34Z

**Summary**: Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.

**Link**: [arxiv](https://arxiv.org/abs/2508.10501v4),  [pdf](https://arxiv.org/pdf/2508.10501v4)

**Tags**: cs.AI cs.LG 



### Implicit Likelihood Inference of the Neutrino Mass Hierarchy from Cosmological Data
**Authors**: Ke Wang

**Updated**: 2025-12-19T16:20:49Z

**Summary**: In this paper, we turn to the Learning the Universe Implicit Likelihood Inference (LtU-ILI) pipeline to perform a multi-round ILI of the neutrino mass hierarchy from cosmological data, including $TT$, $TE$, $EE$ power spectra of Planck 2018 and distance ratios of DESI DR2. More precisely, we first embed the CMB power spectra simulator $\mathtt{CLASS}$ into the LtU-ILI pipeline. And then, opting for Sequential Neural Likelihood Estimation (SNLE), we sequentially train neural networks using $2$ rounds of $5000$ simulations to target a ``black box'' likelihood of our forward model with one additional neutrino mass hierarchy parameter $\tildeΔ$ and six base cosmological parameters. We find that $\tildeΔ=0.12216^{+0.26193}_{-0.29243}~(68\%{\rm CL})$ which slightly prefers $\tildeΔ>0$, hence the normal hierarchy.

**Link**: [arxiv](https://arxiv.org/abs/2512.17744v1),  [pdf](https://arxiv.org/pdf/2512.17744v1)

**Tags**: astro-ph.CO 



### Clean Up the Mess: Addressing Data Pollution in Cryptocurrency Abuse Reporting Services
**Authors**: Gibran Gomez, Kevin van Liebergen, Davide Sanvito, Giuseppe Siracusano, Roberto Gonzalez, Juan Caballero

**Updated**: 2025-12-19T16:18:06Z

**Summary**: Cryptocurrency abuse reporting services are a valuable data source about abusive blockchain addresses, prevalent types of cryptocurrency abuse, and their financial impact on victims. However, they may suffer data pollution due to their crowd-sourced nature. This work analyzes the extent and impact of data pollution in cryptocurrency abuse reporting services and proposes a novel LLM-based defense to address the pollution. We collect 289K abuse reports submitted over 6 years to two popular services and use them to answer three research questions. RQ1 analyzes the extent and impact of pollution. We show that spam reports will eventually flood unchecked abuse reporting services, with BitcoinAbuse receiving 75% of spam before stopping operations. We build a public dataset of 19,443 abuse reports labeled with 19 popular abuse types and use it to reveal the inaccuracy of user-reported abuse types. We identified 91 (0.1%) benign addresses reported, responsible for 60% of all the received funds. RQ2 examines whether we can automate identifying valid reports and their classification into abuse types. We propose an unsupervised LLM-based classifier that achieves an F1 score of 0.95 when classifying reports, an F1 of 0.89 when classifying out-of-distribution data, and an F1 of 0.99 when identifying spam reports. Our unsupervised LLM-based classifier clearly outperforms two baselines: a supervised classifier and a naive usage of the LLM. Finally, RQ3 demonstrates the usefulness of our LLM-based classifier for quantifying the financial impact of different cryptocurrency abuse types. We show that victim-reported losses heavily underestimate cybercriminal revenue by estimating a 29 times higher revenue from deposit transactions. We identified that investment scams have the highest financial impact and that extortions have lower conversion rates but compensate for them with massive email campaigns.

**Link**: [arxiv](https://arxiv.org/abs/2410.21041v2),  [pdf](https://arxiv.org/pdf/2410.21041v2)

**Tags**: cs.CR cs.CL 



### Fun-ASR Technical Report
**Authors**: Keyu An, Yanni Chen, Zhigao Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Ying Liu, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Haoxu Wang, Wen Wang, Wupeng Wang, Yuzhong Wu, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou, Yanqiao Zhu

**Updated**: 2025-12-19T16:18:03Z

**Summary**: In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .

**Link**: [arxiv](https://arxiv.org/abs/2509.12508v4),  [pdf](https://arxiv.org/pdf/2509.12508v4)

**Tags**: cs.CL cs.AI cs.SD eess.AS 



### When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content
**Authors**: Lydia Nishimwe, Benoît Sagot, Rachel Bawden

**Updated**: 2025-12-19T16:17:23Z

**Summary**: User-generated content (UGC) is characterised by frequent use of non-standard language, from spelling errors to expressive choices such as slang, character repetitions, and emojis. This makes evaluating UGC translation particularly challenging: what counts as a "good" translation depends on the level of standardness desired in the output. To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). Our analysis reveals notable differences in how UGC is treated, resulting in a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), we show that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that they improve when these align with the dataset's guidelines. We argue that when preserving UGC style is important, fair evaluation requires both models and metrics to be aware of translation guidelines. Finally, we call for clear guidelines during dataset creation and for the development of controllable, guideline-aware evaluation frameworks for UGC translation.

**Link**: [arxiv](https://arxiv.org/abs/2512.17738v1),  [pdf](https://arxiv.org/pdf/2512.17738v1)

**Tags**: cs.CL 



### Gravity Prior and Temporal Horizon Shape Interceptive Behavior under Active Inference
**Authors**: Marta Russo, Antonella Maselli, Federico Maggiore, Giovanni Pezzulo

**Updated**: 2025-12-19T16:14:47Z

**Summary**: Accurate interception of moving objects, such as catching a ball, requires the nervous system to overcome sensory delays, noise, and environmental dynamics. One key challenge is predicting future object motion in the presence of sensory uncertainty and inherent neural processing latencies. Theoretical frameworks such as internal models and optimal control have emphasized the role of predictive mechanisms in motor behavior. Active Inference extends these ideas by positing that perception and action arise from minimizing variational free energy under a generative model of the world. In this study, we investigate how different predictive strategies and the inclusion of environmental dynamics, specifically an internal model of gravity, influence interceptive control within an Active Inference agent. We simulate a simplified ball-catching task in which the agent moves a cursor horizontally to intercept a parabolically falling object. Four strategies are compared: short temporal horizon prediction of the next position or long horizon estimation of the interception point, each with or without a gravity prior. Performance is evaluated across diverse initial conditions using spatial and temporal error, action magnitude, and movement corrections. All strategies produce successful interception behavior, but those that incorporate gravity and longer temporal horizons outperform others. Including a gravity prior significantly improves spatial and temporal accuracy. Predicting the future interception point yields lower action values and smoother trajectories compared to short-horizon prediction. These findings suggest that internal models of physical dynamics and extended predictive horizons can enhance interceptive control, providing a unified computational account of how the brain may integrate sensory uncertainty, physical expectations, and motor planning.

**Link**: [arxiv](https://arxiv.org/abs/2512.17735v1),  [pdf](https://arxiv.org/pdf/2512.17735v1)

**Tags**: q-bio.NC 



### Generating Completions for Broca's Aphasic Sentences Using Large Language Models
**Authors**: Sijbren van Vaals, Yevgen Matusevych, Frank Tsiwah

**Updated**: 2025-12-19T15:52:00Z

**Summary**: Broca's aphasia is a type of aphasia characterized by non-fluent, effortful and agrammatic speech production with relatively good comprehension. Since traditional aphasia treatment methods are often time-consuming, labour-intensive, and do not reflect real-world conversations, applying natural language processing based approaches such as Large Language Models (LLMs) could potentially contribute to improving existing treatment approaches. To address this issue, we explore the use of sequence-to-sequence LLMs for completing Broca's aphasic sentences. We first generate synthetic Broca's aphasic data using a rule-based system designed to mirror the linguistic characteristics of Broca's aphasic speech. Using this synthetic data (without authentic aphasic samples), we then fine-tune four pre-trained LLMs on the task of completing agrammatic sentences. We evaluate our fine-tuned models on both synthetic and authentic Broca's aphasic data. We demonstrate LLMs' capability for reconstructing agrammatic sentences, with the models showing improved performance with longer input utterances. Our result highlights the LLMs' potential in advancing communication aids for individuals with Broca's aphasia and possibly other clinical populations.

**Link**: [arxiv](https://arxiv.org/abs/2412.17669v2),  [pdf](https://arxiv.org/pdf/2412.17669v2)

**Tags**: cs.CL 



### Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric
**Authors**: Yan Shvartzshnaider, Vasisht Duddu

**Updated**: 2025-12-19T15:41:38Z

**Summary**: As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.

**Link**: [arxiv](https://arxiv.org/abs/2409.03735v3),  [pdf](https://arxiv.org/pdf/2409.03735v3)

**Tags**: cs.LG cs.AI cs.CR cs.CY 



### A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models
**Authors**: Sabrina Kaniewski, Fabian Schmidt, Markus Enzweiler, Michael Menth, Tobias Heer

**Updated**: 2025-12-19T15:41:06Z

**Summary**: The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 263 studies published between January 2020 and November 2025, categorizing them by task formulation, input representation, system architecture, and techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies at https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD.

**Link**: [arxiv](https://arxiv.org/abs/2507.22659v2),  [pdf](https://arxiv.org/pdf/2507.22659v2)

**Tags**: cs.SE cs.AI 



### Sparse, Efficient and Explainable Data Attribution with DualXDA
**Authors**: Galip Ümit Yolcu, Moritz Weckbecker, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin

**Updated**: 2025-12-19T15:36:27Z

**Summary**: Data Attribution (DA) is an emerging approach in the field of eXplainable Artificial Intelligence (XAI), aiming to identify influential training datapoints which determine model outputs. It seeks to provide transparency about the model and individual predictions, e.g. for model debugging, identifying data-related causes of suboptimal performance. However, existing DA approaches suffer from prohibitively high computational costs and memory demands when applied to even medium-scale datasets and models, forcing practitioners to resort to approximations that may fail to capture the true inference process of the underlying model. Additionally, current attribution methods exhibit low sparsity, resulting in non-negligible attribution scores across a high number of training examples, hindering the discovery of decisive patterns in the data. In this work, we introduce DualXDA, a framework for sparse, efficient and explainable DA, comprised of two interlinked approaches, Dual Data Attribution (DualDA) and eXplainable Data Attribution (XDA): With DualDA, we propose a novel approach for efficient and effective DA, leveraging Support Vector Machine theory to provide fast and naturally sparse data attributions for AI predictions. In extensive quantitative analyses, we demonstrate that DualDA achieves high attribution quality, excels at solving a series of evaluated downstream tasks, while at the same time improving explanation time by a factor of up to 4,100,000x compared to the original Influence Functions method, and up to 11,000x compared to the method's most efficient approximation from literature to date. We further introduce XDA, a method for enhancing Data Attribution with capabilities from feature attribution methods to explain why training samples are relevant for the prediction of a test sample in terms of impactful features, which we showcase and verify qualitatively in detail.

**Link**: [arxiv](https://arxiv.org/abs/2402.12118v3),  [pdf](https://arxiv.org/pdf/2402.12118v3)

**Tags**: cs.LG cs.AI 



### A Dependent Feature Allocation Model Based on Random Fields
**Authors**: Bernardo Flores, Yang Ni, Yanxun Xu, Peter Müller

**Updated**: 2025-12-19T15:36:09Z

**Summary**: We introduce a flexible framework for modeling dependent feature allocations. Our approach addresses limitations in traditional nonparametric methods by directly modeling the logit-probability surface of the feature paintbox, enabling the explicit incorporation of covariates and complex but tractable dependence structures. The core of our model is a Gaussian Markov Random Field (GMRF), which we use to robustly decompose the latent field, separating a structural component based on the baseline covariates from intrinsic, unstructured heterogeneity. This structure is not a rigid grid but a sparse k-nearest neighbors graph derived from the latent geometry in the data, ensuring high-dimensional tractability. We extend this framework to a dynamic spatio-temporal process, allowing item effects to evolve via an Ornstein-Uhlenbeck process. Feature correlations are captured using a low-rank factorization of their joint prior. We demonstrate our model's utility by applying it to a polypharmacy dataset, successfully inferring latent health conditions from patient drug profiles.

**Link**: [arxiv](https://arxiv.org/abs/2512.17701v1),  [pdf](https://arxiv.org/pdf/2512.17701v1)

**Tags**: stat.ME 



### Generalized infinite dimensional Alpha-Procrustes based geometries
**Authors**: Salvish Goomanee, Andi Han, Pratik Jawanpuria, Bamdev Mishra

**Updated**: 2025-12-19T15:36:03Z

**Summary**: This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.

**Link**: [arxiv](https://arxiv.org/abs/2511.09801v2),  [pdf](https://arxiv.org/pdf/2511.09801v2)

**Tags**: stat.ML cs.LG math.FA math.OC 



### Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs
**Authors**: Shasha Zhou, Mingyu Huang, Jack Cole, Charles Britton, Ming Yin, Jan Wolber, Ke Li

**Updated**: 2025-12-19T15:20:17Z

**Summary**: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

**Link**: [arxiv](https://arxiv.org/abs/2511.12817v2),  [pdf](https://arxiv.org/pdf/2511.12817v2)

**Tags**: cs.LG 



### You Only Train Once: Differentiable Subset Selection for Omics Data
**Authors**: Daphné Chopard, Jorge da Silva Gonçalves, Irene Cannistraci, Thomas M. Sutter, Julia E. Vogt

**Updated**: 2025-12-19T15:17:34Z

**Summary**: Selecting compact and informative gene subsets from single-cell transcriptomic data is essential for biomarker discovery, improving interpretability, and cost-effective profiling. However, most existing feature selection approaches either operate as multi-stage pipelines or rely on post hoc feature attribution, making selection and prediction weakly coupled. In this work, we present YOTO (you only train once), an end-to-end framework that jointly identifies discrete gene subsets and performs prediction within a single differentiable architecture. In our model, the prediction task directly guides which genes are selected, while the learned subsets, in turn, shape the predictive representation. This closed feedback loop enables the model to iteratively refine both what it selects and how it predicts during training. Unlike existing approaches, YOTO enforces sparsity so that only the selected genes contribute to inference, eliminating the need to train additional downstream classifiers. Through a multi-task learning design, the model learns shared representations across related objectives, allowing partially labeled datasets to inform one another, and discovering gene subsets that generalize across tasks without additional training steps. We evaluate YOTO on two representative single-cell RNA-seq datasets, showing that it consistently outperforms state-of-the-art baselines. These results demonstrate that sparse, end-to-end, multi-task gene subset selection improves predictive performance and yields compact and meaningful gene subsets, advancing biomarker discovery and single-cell analysis.

**Link**: [arxiv](https://arxiv.org/abs/2512.17678v1),  [pdf](https://arxiv.org/pdf/2512.17678v1)

**Tags**: cs.LG cs.AI 



### Toward Ethical AI Through Bayesian Uncertainty in Neural Question Answering
**Authors**: Riccardo Di Sipio

**Updated**: 2025-12-19T15:17:19Z

**Summary**: We explore Bayesian reasoning as a means to quantify uncertainty in neural networks for question answering. Starting with a multilayer perceptron on the Iris dataset, we show how posterior inference conveys confidence in predictions. We then extend this to language models, applying Bayesian inference first to a frozen head and finally to LoRA-adapted transformers, evaluated on the CommonsenseQA benchmark. Rather than aiming for state-of-the-art accuracy, we compare Laplace approximations against maximum a posteriori (MAP) estimates to highlight uncertainty calibration and selective prediction. This allows models to abstain when confidence is low. An ``I don't know'' response not only improves interpretability but also illustrates how Bayesian methods can contribute to more responsible and ethical deployment of neural question-answering systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.17677v1),  [pdf](https://arxiv.org/pdf/2512.17677v1)

**Tags**: cs.CL 



### Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs
**Authors**: Petr Novak, Stefan Biffl, Marek Obitko, Petr Kadera

**Updated**: 2025-12-19T15:15:08Z

**Summary**: Contemporary industrial cyber-physical production systems (CPPS) composed of robotic workcells face significant challenges in the analysis of undesired conditions due to the flexibility of Industry 4.0 that disrupts traditional quality assurance mechanisms. This paper presents a novel industry-oriented semantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG), which is designed to analyze and mitigate undesired conditions in flexible CPPS. Built on top of the well-proven Product-Process-Resource (PPR) model originating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses shortcomings of conventional model-driven engineering for CPPS, particularly inadequate undesired condition and error handling representation. The integration of semantic technologies with large language models (LLMs) provides intuitive interfaces for factory operators, production planners, and engineers to interact with the entire model using natural language. Evaluation with the use case addressing electric vehicle battery remanufacturing demonstrates that the PPR-AKG approach efficiently supports resource allocation based on explicitly represented capabilities as well as identification and mitigation of undesired conditions in production. The key contributions include (1) a holistic PPR-AKG model capturing multi-dimensional production knowledge, and (2) the useful combination of the PPR-AKG with LLM-based chatbots for human interaction.

**Link**: [arxiv](https://arxiv.org/abs/2508.06278v2),  [pdf](https://arxiv.org/pdf/2508.06278v2)

**Tags**: cs.RO 



### Statistical field theory for dialectology
**Authors**: James Burridge

**Updated**: 2025-12-19T15:12:04Z

**Summary**: Is it possible to develop a `physics of language' which can explain the spatial, temporal and social patterns we see, and which can predict future change like we forecast the weather? Such a theory is likely to involve ideas from statistical physics. A substantial literature already applies these ideas to language. However, we lack a model which can match the spatial-temporal detail of historical changes at the level of individual linguistic features, and which offers a principled mechanism to predict the future. Here we present a statistical field theory for the evolution of linguistic variables which takes steps to fill this gap. Linguistic variant frequencies are represented as a stochastic state field with spatial interaction and social conformity, coupled to a latent bias field with Onsager Machlup action that reduces overfitting to data. We derive parameter inference procedures and demonstrate them using examples of large-scale dialect survey data from the twentieth century United States. The bias field has a characteristic half-life, which determines the horizon over which linguistic change can be predicted. Inferred model parameters provide evidence for surface-tension-driven coarsening of dialect regions, with population-density gradients exerting systematic forces on interfaces.

**Link**: [arxiv](https://arxiv.org/abs/2512.17668v1),  [pdf](https://arxiv.org/pdf/2512.17668v1)

**Tags**: physics.soc-ph 



### STAR: Semantic-Traffic Alignment and Retrieval for Zero-Shot HTTPS Website Fingerprinting
**Authors**: Yifei Cheng, Yujia Zhu, Baiyang Li, Xinhao Deng, Yitong Cai, Yaochen Ren, Qingyun Liu

**Updated**: 2025-12-19T15:12:01Z

**Summary**: Modern HTTPS mechanisms such as Encrypted Client Hello (ECH) and encrypted DNS improve privacy but remain vulnerable to website fingerprinting (WF) attacks, where adversaries infer visited sites from encrypted traffic patterns. Existing WF methods rely on supervised learning with site-specific labeled traces, which limits scalability and fails to handle previously unseen websites. We address these limitations by reformulating WF as a zero-shot cross-modal retrieval problem and introducing STAR. STAR learns a joint embedding space for encrypted traffic traces and crawl-time logic profiles using a dual-encoder architecture. Trained on 150K automatically collected traffic-logic pairs with contrastive and consistency objectives and structure-aware augmentation, STAR retrieves the most semantically aligned profile for a trace without requiring target-side traffic during training. Experiments on 1,600 unseen websites show that STAR achieves 87.9 percent top-1 accuracy and 0.963 AUC in open-world detection, outperforming supervised and few-shot baselines. Adding an adapter with only four labeled traces per site further boosts top-5 accuracy to 98.8 percent. Our analysis reveals intrinsic semantic-traffic alignment in modern web protocols, identifying semantic leakage as the dominant privacy risk in encrypted HTTPS traffic. We release STAR's datasets and code to support reproducibility and future research.

**Link**: [arxiv](https://arxiv.org/abs/2512.17667v1),  [pdf](https://arxiv.org/pdf/2512.17667v1)

**Tags**: cs.CR cs.AI cs.NI 



### Designing an LLM-Based Behavioral Activation Chatbot for Young People with Depression: Insights from an Evaluation with Artificial Users and Clinical Experts
**Authors**: Florian Onur Kuhlmeier, Leon Hanschmann, Melina Rabe, Stefan Luettke, Eva-Lotta Brakemeier, Alexander Maedche

**Updated**: 2025-12-19T14:50:12Z

**Summary**: LLMs promise to overcome limitations of rule-based mental health chatbots through improved natural language capabilities, yet their ability to deliver evidence-based psychological interventions remains largely unverified because evaluations rarely apply the validated fidelity measures used to assess psychotherapists. We developed an LLM-based chatbot that delivers behavioral activation for depression and generated 48 complete chat sessions with diverse artificial users. Ten psychotherapists assessed these sessions using the Quality of Behavioral Activation Scale (Q-BAS), a validated fidelity instrument. Results show that the chatbot reliably executed the intervention across all phases and maintained safety protocols, but it struggled with clinical judgment, particularly when verifying the feasibility of proposed activities. Overall, our findings suggest that LLM-based chatbots can execute therapeutic protocols with high fidelity, while robust clinical reasoning remains an open challenge. We outline design implications to address this gap and provide the chatbot and artificial user prompts.

**Link**: [arxiv](https://arxiv.org/abs/2503.21540v3),  [pdf](https://arxiv.org/pdf/2503.21540v3)

**Tags**: cs.HC 



### Dual-Input Dynamic Convolution for Positron Range Correction in PET Image Reconstruction
**Authors**: Youness Mellak, Alexandre Bousse, Thibaut Merlin, Élise Émond, Mikko Hakulinen, Dimitris Visvikis

**Updated**: 2025-12-19T14:48:49Z

**Summary**: Positron range (PR) blurring degrades positron emission tomography (PET) image resolution, particularly for high-energy emitters like gallium-68 (68 Ga). We introduce Dual-Input Dynamic Convolution (DDConv), a novel computationally efficient approach trained with voxel-specific PR point spread functions (PSFs) from Monte Carlo (MC) simulations and designed to be utilized within an iterative reconstruction algorithm to perform PR correction (PRC). By dynamically inferring local blurring kernels through a trained convolutional neural network (CNN), DDConv captures complex tissue interfaces more accurately than prior methods. Additionally, it also computes the transpose operator, ensuring consistency within iterative PET reconstruction. Comparisons with a state-of-the-art, tissue-dependent correction confirm the advantages of DDConv in recovering higher-resolution details in heterogeneous regions, including bone-soft tissue and lung-soft tissue boundaries. Experiments across digital phantoms and MC-simulated data show that DDConv offers near-MC accuracy and outperforms the state-of-the-art technique, namely spatially-variant and tissue-dependent (SVTD), especially in areas with complex material interfaces. Results from real phantom experiments further confirm DD-Conv's robustness and practical applicability: while both DD-Conv and SVTD performed similarly in homogeneous soft-tissue regions, DDConv provided more accurate activity recovery and sharper delineation at heterogeneous lung-soft tissue interfaces. Our code available at https://github.com/mellak/ddconv-prc.

**Link**: [arxiv](https://arxiv.org/abs/2503.00587v4),  [pdf](https://arxiv.org/pdf/2503.00587v4)

**Tags**: physics.med-ph 



### Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs
**Authors**: Zhaolin Cai, Huiyu Duan, Zitong Xu, Fan Li, Zhi Liu, Jing Liu, Wei Shen, Xiongkuo Min, Guangtao Zhai

**Updated**: 2025-12-19T14:41:50Z

**Summary**: Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.

**Link**: [arxiv](https://arxiv.org/abs/2512.17640v1),  [pdf](https://arxiv.org/pdf/2512.17640v1)

**Tags**: cs.CV 



### Linear Personality Probing and Steering in LLMs: A Big Five Study
**Authors**: Michel Frising, Daniel Balcells

**Updated**: 2025-12-19T14:41:09Z

**Summary**: Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.

**Link**: [arxiv](https://arxiv.org/abs/2512.17639v1),  [pdf](https://arxiv.org/pdf/2512.17639v1)

**Tags**: cs.CL 



### Trust-Region Adaptive Policy Optimization
**Authors**: Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang

**Updated**: 2025-12-19T14:37:07Z

**Summary**: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

**Link**: [arxiv](https://arxiv.org/abs/2512.17636v1),  [pdf](https://arxiv.org/pdf/2512.17636v1)

**Tags**: cs.LG cs.AI 



### Across the Universe: GW231123 as a magnified and diffracted black hole merger
**Authors**: Srashti Goyal, Hector Villarrubia-Rojo, Miguel Zumalacarregui

**Updated**: 2025-12-19T14:33:49Z

**Summary**: GW231123 appears as the most massive binary black hole (BBH) ever observed by the LIGO interferometers with total mass $190-265 M_\odot$. A high observed mass can be explained by the combination of cosmological redshift and gravitational magnification if the source is aligned with a gravitational lens, such as a galaxy. Small-scale objects such as stars and remnants diffract the signal, distorting the wavefront and providing additional lensing signatures. Here we present an analysis of GW231123 combining for the first time the effects of diffraction by a small-scale lens and gravitational magnification by an external potential, modelled as an embedded point-mass lens (PL), finding an intriguing case for the lensing hypothesis. Lensing is favoured by the data, with a false alarm probability of the observed Bayes factors bounded below $<1\%$, or $\sim 2.6 σ$ confidence level. Including lensing lowers the total source mass of GW231123 to $100-180 M_\odot$, closer to BBHs reported so far, and also removes discrepancies between different waveform approximants and the need for high component spins. We reconstruct all source and lens properties, including the microlens mass $190-850 M_\odot$, its offset, the magnitude of the external gravitational potential and its orientation. The embedded PL analysis leads to a lighter microlens compared to the isolated PL. Within our assumptions, the reconstruction is complete up to an ambiguity between the distance and projected density (mass-sheet degeneracy). Assuming a single galaxy as the macroscopic lens allows us to infer the total amplification of the signal, placing the event at redshift $0.7-2$, and predict the probability $~55\%$ of forming an additional detectable image due to strong lensing by the macrolens. We discuss the implications of our findings on the source and nature of the microlens, including a possible dark matter origin.

**Link**: [arxiv](https://arxiv.org/abs/2512.17631v1),  [pdf](https://arxiv.org/pdf/2512.17631v1)

**Tags**: astro-ph.GA astro-ph.CO gr-qc 



### Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection
**Authors**: Menna Elgabry, Ali Hamdi

**Updated**: 2025-12-19T14:33:14Z

**Summary**: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.

**Link**: [arxiv](https://arxiv.org/abs/2512.17630v1),  [pdf](https://arxiv.org/pdf/2512.17630v1)

**Tags**: cs.CL cs.LG 



### Surrogate-Accelerated Bayesian Inversion for Exoplanet Interior Characterization
**Authors**: Tijn De Wringer, Caroline Dorn, Emily O. Garvin, Stefano Marelli

**Updated**: 2025-12-19T14:29:34Z

**Summary**: Characterizing the interior structure of exoplanets is an inverse problem often solved using Bayesian inference, but this approach is hampered by the high computational cost of planetary structure models. To overcome this barrier, we present a robust framework that accelerates inference by replacing the computationally expensive physics-based forward model with a fast polynomial chaos-Kriging (PCK) surrogate directly within a Markov chain Monte Carlo (MCMC) sampling loop. We rigorously validate our approach using a suite of tests, including a direct comparison against a benchmark MCMC inference using the full forward model, and a large-scale coverage study with 1000 synthetic test cases to demonstrate the statistical reliability of our inferred credible intervals. Our surrogate-assisted framework achieves a computational speedup of over 2 orders of magnitude (factor of $\sim$320), reducing single-CPU inference times from days to minutes. This efficiency is achieved with a surrogate that requires only a few hundred forward model evaluations for training \rev{for a single planet}. This data efficiency provides significant flexibility for model developments and a clear advantage over common machine learning approaches, which typically demand vast training sets ($>10^6$ model runs) and intensive pre-computation. The PCK surrogate maintains high fidelity with $R^2 > 0.99$ for most scenarios, and root-mean-square errors typically an order of magnitude smaller than observational uncertainties. This efficiency enables large scale population studies while preserving statistical robustness, which is computationally impractical with traditional methods.

**Link**: [arxiv](https://arxiv.org/abs/2512.17626v1),  [pdf](https://arxiv.org/pdf/2512.17626v1)

**Tags**: astro-ph.EP astro-ph.IM 



### PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology
**Authors**: Fengchun Liu, Songhan Jiang, Linghan Cai, Ziyue Wang, Yongbing Zhang

**Updated**: 2025-12-19T14:26:50Z

**Summary**: While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.

**Link**: [arxiv](https://arxiv.org/abs/2512.17621v1),  [pdf](https://arxiv.org/pdf/2512.17621v1)

**Tags**: cs.CV 



### Emoji Reactions on Telegram: Unreliable Indicators of Emotional Resonance
**Authors**: Serena Tardelli, Lorenzo Alvisi, Lorenzo Cima, Stefano Cresci, Maurizio Tesconi

**Updated**: 2025-12-19T14:21:33Z

**Summary**: Emoji reactions are a frequently used feature of messaging platforms, yet their communicative role remains understudied. Prior work on emojis has focused predominantly on in-text usage, showing that emojis embedded in messages tend to amplify and mirror the author's affective tone. This evidence has often been extended to emoji reactions, treating them as indicators of emotional resonance or user sentiment. However, they may reflect broader social dynamics. Here, we investigate the communicative function of emoji reactions on Telegram. We analyze over 650k crypto-related messages that received at least one reaction, annotating each with sentiment, emotion, persuasion strategy, and speech act labels, and inferring the sentiment and emotion of emoji reactions using both lexicons and LLMs. We uncover a systematic mismatch between message and reaction sentiment, with positive reactions dominating even for neutral or negative content. This pattern persists across rhetorical strategies and emotional tones, indicating that emojis used as reactions do not reliably function as indicators of emotional mirroring or resonance of the content, in contrast to findings reported for in-text emojis. Finally, we identify the features that most predict emoji engagement. Overall, our findings caution against treating emoji reactions as sentiment labels, highlighting the need for more nuanced approaches in sentiment and engagement analysis.

**Link**: [arxiv](https://arxiv.org/abs/2508.06349v2),  [pdf](https://arxiv.org/pdf/2508.06349v2)

**Tags**: cs.HC cs.CY 



### Self-Supervised Weighted Image Guided Quantitative MRI Super-Resolution
**Authors**: Alireza Samadifardheris, Dirk H. J. Poot, Florian Wiesinger, Stefan Klein, Juan A. Hernandez-Tamames

**Updated**: 2025-12-19T14:15:31Z

**Summary**: High-resolution (HR) quantitative MRI (qMRI) relaxometry provides objective tissue characterization but remains clinically underutilized due to lengthy acquisition times. We propose a physics-informed, self-supervised framework for qMRI super-resolution that uses routinely acquired HR weighted MRI (wMRI) scans as guidance, thus, removing the necessity for HR qMRI ground truth during training. We formulate super-resolution as Bayesian maximum a posteriori inference, minimizing two discrepancies: (1) between HR images synthesized from super-resolved qMRI maps and acquired wMRI guides via forward signal models, and (2) between acquired LR qMRI and downsampled predictions. This physics-informed objective allows the models to learn from clinical wMRI without HR qMRI supervision. To validate the concept, we generate training data by synthesizing wMRI guides from HR qMRI using signal equations, then degrading qMRI resolution via k-space truncation. A deep neural network learns the super-resolution mapping. Ablation experiments demonstrate that T1-weighted images primarily enhance T1 maps, T2-weighted images improve T2 maps, and combined guidance optimally enhances all parameters simultaneously. Validation on independently acquired in-vivo data from a different qMRI sequence confirms cross-qMRI sequence generalizability. Models trained on synthetic data can produce super-resolved maps from a 1-minute acquisition with quality comparable to a 5-minute reference scan, leveraging the scanner-independent nature of relaxometry parameters. By decoupling training from HR qMRI requirement, our framework enables fast qMRI acquisitions enhanced via routine clinical images, offering a practical pathway for integrating quantitative relaxometry into clinical workflows with acceptable additional scan time.

**Link**: [arxiv](https://arxiv.org/abs/2512.17612v1),  [pdf](https://arxiv.org/pdf/2512.17612v1)

**Tags**: cs.CV 



### Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework
**Authors**: Md. Irtiza Hossain, Junaid Ahmed Sifat, Abir Chowdhury

**Updated**: 2025-12-19T14:10:20Z

**Summary**: The digitization of structured handwritten documents, such as academic marksheets, remains a significant challenge due to the dual complexity of irregular table structures and diverse handwriting styles. While recent Transformer-based approaches like TableNet and TrOCR achieve state-of-the-art accuracy, their high computational cost renders them unsuitable for resource-constrained edge deployments. This paper introduces a resource-efficient hybrid framework that integrates a heuristic OpenCV-based pipeline for rapid table structure detection with a modified lightweight YOLOv8 architecture for handwritten character recognition. By strategically removing the SPPF and deep C2f layers from the standard YOLOv8 backbone, we reduce computational overhead while maintaining high recognition fidelity. Experimental results on the EMNIST digit benchmark demonstrate that our Modified YOLOv8 model achieves 97.5% accuracy. Furthermore, we provide a comprehensive efficiency analysis showing that our framework offers a 95 times inference speedup over standard OCR pipelines and massive efficiency gains over emerging Large Multimodal Models (LMMs) like Qwen2.5-VL, achieving real-time performance 29 FPS on standard CPU hardware. A qualitative and quantitative evaluation on the AMES dataset, a challenging subset of real-world marksheets, confirms the system's robustness in handling mixed alphanumeric content, bridging the gap between high-performance deep learning and practical, scalable document automation.

**Link**: [arxiv](https://arxiv.org/abs/2508.16295v2),  [pdf](https://arxiv.org/pdf/2508.16295v2)

**Tags**: cs.CV 



### Clustering and Pruning in Causal Data Fusion
**Authors**: Otto Tabell, Santtu Tikka, Juha Karvanen

**Updated**: 2025-12-19T13:45:03Z

**Summary**: Data fusion, the process of combining observational and experimental data, can enable the identification of causal effects that would otherwise remain non-identifiable. Although identification algorithms have been developed for specific scenarios, do-calculus remains the only general-purpose tool for causal data fusion, particularly when variables are present in some data sources but not others. However, approaches based on do-calculus may encounter computational challenges as the number of variables increases and the causal graph grows in complexity. Consequently, there exists a need to reduce the size of such models while preserving the essential features. For this purpose, we propose pruning (removing unnecessary variables) and clustering (combining variables) as preprocessing operations for causal data fusion. We generalize earlier results on a single data source and derive conditions for applying pruning and clustering in the case of multiple data sources. We give sufficient conditions for inferring the identifiability or non-identifiability of a causal effect in a larger graph based on a smaller graph and show how to obtain the corresponding identifying functional for identifiable causal effects. Examples from epidemiology and social science demonstrate the use of the results.

**Link**: [arxiv](https://arxiv.org/abs/2505.15215v2),  [pdf](https://arxiv.org/pdf/2505.15215v2)

**Tags**: stat.ML cs.LG stat.ME 



### Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing
**Authors**: Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng

**Updated**: 2025-12-19T13:40:13Z

**Summary**: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.   To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.17574v1),  [pdf](https://arxiv.org/pdf/2512.17574v1)

**Tags**: cs.DC cs.LG 



### GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping
**Authors**: Yikang Yue, Yishu Yin, Xuehai Qian

**Updated**: 2025-12-19T13:36:31Z

**Summary**: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

**Link**: [arxiv](https://arxiv.org/abs/2512.17570v1),  [pdf](https://arxiv.org/pdf/2512.17570v1)

**Tags**: cs.LG cs.AI cs.PF 



### Towards Explainable Conversational AI for Early Diagnosis with Large Language Models
**Authors**: Maliha Tabassum, M Shamim Kaiser

**Updated**: 2025-12-19T13:28:50Z

**Summary**: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

**Link**: [arxiv](https://arxiv.org/abs/2512.17559v1),  [pdf](https://arxiv.org/pdf/2512.17559v1)

**Tags**: cs.AI 



### Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests
**Authors**: Guglielmo Del Col, Väinö Karjalainen, Teemu Hakala, Yibo Zhang, Eija Honkavaara

**Updated**: 2025-12-19T13:19:33Z

**Summary**: Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz.   The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.

**Link**: [arxiv](https://arxiv.org/abs/2512.17553v1),  [pdf](https://arxiv.org/pdf/2512.17553v1)

**Tags**: cs.RO 



### Spectroscopic readout of chiral photonic topology in a single-cavity spin-orbit-coupled Bose-Einstein condensate
**Authors**: Kashif Ammar Yasir, Gao Xianlong

**Updated**: 2025-12-19T13:18:54Z

**Summary**: Topological photonic phases are typically identified through band reconstruction, steady-state transmission, or real-space imaging of edge modes. In this work, we present a framework for spectroscopic readout of chiral photonic topology in a single driven optical cavity containing a spin-orbit-coupled Bose-Einstein condensate. We demonstrate that the cavity transmission power spectral density provides a direct and measurable proxy for a momentum- and frequency-resolved photonic Chern marker, enabling topological characteristics to be inferred from spectral data without the need for bulk-band tomography. In the loss-dominated regime, where cavity decay exceeds atomic dissipation, the power spectral density exhibits Dirac-like gapped hybrid modes with a vanishing Chern marker, indicating a trivial phase. When the dissipation imbalance is reversed, a bright, gap-spanning spectral ridge emerges, co-localized with peaks in both the Chern marker and Berry curvature. The complex spectrum reveals parity-time symmetric coalescences and gain-loss bifurcations, marking exceptional points and enabling chiral, gap-traversing transport. By linking noise spectroscopy to geometric and non-Hermitian topology in a minimal cavity-QED architecture, this work provides a framework for spectroscopic detection of topological order in driven quantum systems. This approach offers a pathway to compact, tunable topological photonics across a broad range of light-matter platforms, providing a method for the study and control of topological phases in hybrid quantum systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.08662v2),  [pdf](https://arxiv.org/pdf/2512.08662v2)

**Tags**: quant-ph cond-mat.quant-gas physics.app-ph physics.optics 



### Reinterpreting Landauer conductance, solving the quantum measurement problem, grand unification
**Authors**: Kanchan Meena, Souvik Ghosh, P. Singha Deo

**Updated**: 2025-12-19T13:12:39Z

**Summary**: In a series of recent papers we have proved rigorously that time travel is a reality and very much feasible by using quantum mechanical processes. There are plenty of indirect experimental support untill a direct experiment is conducted. The process crucially depend on the reality of a local time as well as a local partial density of states (LPDOS) that can become negative very easily in the quantum regime of mesoscopic systems. Mesoscopic systems are small enough to allow us to experimentally access the intermediate regime between the classical and quantum worlds. This LPDOS is in every sense a hidden variable in quantum mechanics that does not show up in the axiomatic framework of quantum mechanics. It can be inferred through physical clocks obeying quantum dynamics and can be rigorously justified from the properties of the Hilbert space that is uniquely isomorphic to the complex plane. Therefore one can naturally guess that LPDOS will have something important to say about quantum measurement as well as the unification of classical and quantum laws. We therefore undertake the exercise to show that LPDOS can very much allow us to re-interpret the enormously successful phenomenological Landauer-Buttiker formalism for mesoscopic systems and put it on firm theoretical ground as a bridge between classical and quantum mechanics, thereby unifying them. Essentially the local time calculated quantum mechanically can dilate exactly like the proper time of relativity and be consistent with the coordinate time of relativity. Also the measured conductance of mesoscopic samples is a deterministic quantum measurement outcome from a linear superposition of states, essentially because of LPDOS, which solves the quantum measurement problem. For this we analyze the three probe conductance formula in details and give our arguments for the general case.

**Link**: [arxiv](https://arxiv.org/abs/2512.09709v2),  [pdf](https://arxiv.org/pdf/2512.09709v2)

**Tags**: cond-mat.mes-hall 



### ClothHMR: 3D Mesh Recovery of Humans in Diverse Clothing from Single Image
**Authors**: Yunqi Gao, Leyuan Liu, Yuhan Li, Changxin Gao, Yuanyuan Liu, Jingying Chen

**Updated**: 2025-12-19T13:10:31Z

**Summary**: With 3D data rapidly emerging as an important form of multimedia information, 3D human mesh recovery technology has also advanced accordingly. However, current methods mainly focus on handling humans wearing tight clothing and perform poorly when estimating body shapes and poses under diverse clothing, especially loose garments. To this end, we make two key insights: (1) tailoring clothing to fit the human body can mitigate the adverse impact of clothing on 3D human mesh recovery, and (2) utilizing human visual information from large foundational models can enhance the generalization ability of the estimation. Based on these insights, we propose ClothHMR, to accurately recover 3D meshes of humans in diverse clothing. ClothHMR primarily consists of two modules: clothing tailoring (CT) and FHVM-based mesh recovering (MR). The CT module employs body semantic estimation and body edge prediction to tailor the clothing, ensuring it fits the body silhouette. The MR module optimizes the initial parameters of the 3D human mesh by continuously aligning the intermediate representations of the 3D mesh with those inferred from the foundational human visual model (FHVM). ClothHMR can accurately recover 3D meshes of humans wearing diverse clothing, precisely estimating their body shapes and poses. Experimental results demonstrate that ClothHMR significantly outperforms existing state-of-the-art methods across benchmark datasets and in-the-wild images. Additionally, a web application for online fashion and shopping powered by ClothHMR is developed, illustrating that ClothHMR can effectively serve real-world usage scenarios. The code and model for ClothHMR are available at: \url{https://github.com/starVisionTeam/ClothHMR}.

**Link**: [arxiv](https://arxiv.org/abs/2512.17545v1),  [pdf](https://arxiv.org/pdf/2512.17545v1)

**Tags**: cs.CV cs.AI 



### SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review
**Authors**: Kai Wang, Bingcheng Mao, Shuai Jia, Yujie Ding, Dongming Han, Tianyi Ma, Bin Cao

**Updated**: 2025-12-19T13:02:22Z

**Summary**: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

**Link**: [arxiv](https://arxiv.org/abs/2512.17540v1),  [pdf](https://arxiv.org/pdf/2512.17540v1)

**Tags**: cs.SE 



### Key-Conditioned Orthonormal Transform Gating (K-OTG): Multi-Key Access Control with Hidden-State Scrambling for LoRA-Tuned Models
**Authors**: Muhammad Haris Khan

**Updated**: 2025-12-19T12:42:53Z

**Summary**: We present a simple, PEFT-compatible mechanism that enforces secret-key access control in instruction-tuned language models. K-OTG trains on a dual-path corpus: authorized examples (prefixed with a role key) learn the task output, while unauthorized examples learn a visible block token. At inference, a pre-lm_head hook applies an orthonormal transform to the hidden state: with the correct key/role the inverse map restores the model's native basis; otherwise a session-ephemeral scrambler (permutation, sign flips, Householders) makes logits uninformative and the system short-circuits to BLOCK. Keys are not added as special tokens, and the method composes cleanly with LoRA on 4-bit bases. We evaluate an hour-scale protocol on 1-3B-class instruction models (Llama 3.2, Qwen2.5 1.5B) across utility (XSum ROUGE/BLEU, GSM8K accuracy, WikiText-2 perplexity), selectivity (3by3 role-key unlock matrices), nonce invariance, block suppression, and throughput. Authorized utility remains close to the base on summarization with the expected modest PPL increase from instruction tuning; unauthorized utility collapses (near-zero sequence metrics with exploding PPL), indicating practical unusability without the key. Unlock matrices are diagonally dominant (high on-target unlock, low cross-unlock), authorized block emission is 0 per N via robust bad-word lists, and greedy outputs match exactly across nonces, confirming correct inverse cancellation. The runtime overhead of the Python-level hook is 40% tokens per sec versus the base. K-OTG therefore provides a pragmatic, model-agnostic way to prevent unauthorized use while preserving authorized utility.

**Link**: [arxiv](https://arxiv.org/abs/2512.17519v1),  [pdf](https://arxiv.org/pdf/2512.17519v1)

**Tags**: cs.CR cs.AI 



### Applied causality to infer protein dynamics and kinetics
**Authors**: Akashnathan Aranganathan, Eric R. Beyerle

**Updated**: 2025-12-19T12:38:07Z

**Summary**: The use of generative machine learning models, trained on the experimentally resolved structures deposited in the protein data bank, is an attractive approach to sampling conformational ensembles of proteins. However, the ensembles generated by these models lack timescale or causal information. We use the structural ensembles generated from AlphaFold2 at a range of MSA depths to parameterize the potential of mean force of an overdamped, memory-free, coarse-grained Langevin equation. This approach couples the AlphaFold2 ensembles to a causal model, allowing us to estimate the timescales spanned by the ensembles generated at each MSA depth. Performing this analysis on six variants of HIV-1 protease, we confirm an inverse relationship between MSA depth and the timescale of an ensemble's conformational fluctuations. The MSA depth essentially serves as a conformational restraint, and AlphaFold2 is generally able to probe timescales at or below those seen in microsecond-long, unbiased molecular dynamics simulations. We conclude by generalizing this approach to other generative structural ensemble-prediction methods as well as co-folding models, in this case the biologically functional HIV-1 protease dimer.

**Link**: [arxiv](https://arxiv.org/abs/2508.12060v2),  [pdf](https://arxiv.org/pdf/2508.12060v2)

**Tags**: q-bio.BM physics.bio-ph 



### Dispatch-Aware Learning for Optimal Transmission Switching
**Authors**: Minsoo Kim, Andy Sun, Jip Kim

**Updated**: 2025-12-19T12:34:24Z

**Summary**: Optimal transmission switching (OTS) improves optimal power flow (OPF) by selectively opening transmission lines, but its mixed-integer formulation increases computational complexity, especially on large grids. To deal with this, we propose a dispatch-aware deep neural network (DA-DNN) that accelerates DC-OTS without relying on pre-solved labels. DA-DNN predicts line states and passes them through an embedded differentiable DC-OPF layer, using the resulting generation cost as the loss function so that all physical network constraints are enforced throughout training and inference. In addition, we adopt a customized weight-bias initialization that keeps every forward pass feasible from the first iteration, which allows stable learning. Once trained, the proposed DA-DNN successfully produces a feasible topology and dispatch pair in the same time as solving the DCOPF, whereas conventional mixed-integer solvers become intractable. Moreover, the embedded OPF layer enables DA-DNN to generalize to untrained system configurations, such as changes in line flow limits. As a result, the proposed method successfully captures the economic advantages of OTS while maintaining scalability and generalization ability.

**Link**: [arxiv](https://arxiv.org/abs/2512.17516v1),  [pdf](https://arxiv.org/pdf/2512.17516v1)

**Tags**: eess.SY 



### Resource-efficient medical image classification for edge devices
**Authors**: Mahsa Lavaei, Zahra Abadi, Salar Beigzad, Alireza Maleki

**Updated**: 2025-12-19T12:32:57Z

**Summary**: Medical image classification is a critical task in healthcare, enabling accurate and timely diagnosis. However, deploying deep learning models on resource-constrained edge devices presents significant challenges due to computational and memory limitations. This research investigates a resource-efficient approach to medical image classification by employing model quantization techniques. Quantization reduces the precision of model parameters and activations, significantly lowering computational overhead and memory requirements without sacrificing classification accuracy. The study focuses on the optimization of quantization-aware training (QAT) and post-training quantization (PTQ) methods tailored for edge devices, analyzing their impact on model performance across medical imaging datasets. Experimental results demonstrate that quantized models achieve substantial reductions in model size and inference latency, enabling real-time processing on edge hardware while maintaining clinically acceptable diagnostic accuracy. This work provides a practical pathway for deploying AI-driven medical diagnostics in remote and resource-limited settings, enhancing the accessibility and scalability of healthcare technologies.

**Link**: [arxiv](https://arxiv.org/abs/2512.17515v1),  [pdf](https://arxiv.org/pdf/2512.17515v1)

**Tags**: eess.IV cs.LG 



### CLiENT: A new tool for emulating cosmological likelihoods using deep neural networks
**Authors**: Luca Janken, Steen Hannestad, Thomas Tram, Andreas Nygaard

**Updated**: 2025-12-19T12:19:37Z

**Summary**: Cosmological emulation of observables such as the Cosmic Microwave Background (CMB) spectra and matter power spectra have become increasingly common in recent years because of the potential for saving computation time in connection with cosmological parameter inference or model comparison. In this paper we present CLiENT (Cosmological Likelihood Emulator using Neural networks with TensorFlow), a new method which circumvents the computation of observables in favour of directly emulating the likelihood function for a data set given a model parameter vector. We find that the method is competitive with observable emulators in terms of the required number of function evaluations, but has the distinct advantage of producing a surrogate likelihood which is completely auto-differentiable. Using less than $2 \times 10^4$ function evaluations CLiENT typically achieves credible intervals within better than $0.1 σ$ of those obtained using the true likelihood and single-point emulator precision better than $Δχ^2 \sim 0.5$ across relevant regions in parameter space.

**Link**: [arxiv](https://arxiv.org/abs/2512.17509v1),  [pdf](https://arxiv.org/pdf/2512.17509v1)

**Tags**: astro-ph.CO astro-ph.IM 



### Adaptive Covariance and Quaternion-Focused Hybrid Error-State EKF/UKF for Visual-Inertial Odometry
**Authors**: Ufuk Asil, Efendi Nasibov

**Updated**: 2025-12-19T12:14:37Z

**Summary**: This study presents an innovative hybrid Visual-Inertial Odometry (VIO) method for Unmanned Aerial Vehicles (UAVs) that is resilient to environmental challenges and capable of dynamically assessing sensor reliability. Built upon a loosely coupled sensor fusion architecture, the system utilizes a novel hybrid Quaternion-focused Error-State EKF/UKF (Qf-ES-EKF/UKF) architecture to process inertial measurement unit (IMU) data. This architecture first propagates the entire state using an Error-State Extended Kalman Filter (ESKF) and then applies a targeted Scaled Unscented Kalman Filter (SUKF) step to refine only the orientation. This sequential process blends the accuracy of SUKF in quaternion estimation with the overall computational efficiency of ESKF. The reliability of visual measurements is assessed via a dynamic sensor confidence score based on metrics, such as image entropy, intensity variation, motion blur, and inference quality, adapting the measurement noise covariance to ensure stable pose estimation even under challenging conditions. Comprehensive experimental analyses on the EuRoC MAV dataset demonstrate key advantages: an average improvement of 49% in position accuracy in challenging scenarios, an average of 57% in rotation accuracy over ESKF-based methods, and SUKF-comparable accuracy achieved with approximately 48% lower computational cost than a full SUKF implementation. These findings demonstrate that the presented approach strikes an effective balance between computational efficiency and estimation accuracy, and significantly enhances UAV pose estimation performance in complex environments with varying sensor reliability.

**Link**: [arxiv](https://arxiv.org/abs/2512.17505v1),  [pdf](https://arxiv.org/pdf/2512.17505v1)

**Tags**: cs.RO cs.CV 



### Bias-Class Discrimination of Universal QRAM Boolean Memories
**Authors**: Leonardo Bohac

**Updated**: 2025-12-19T12:14:18Z

**Summary**: We study the discrimination of Boolean memory configurations via a fixed Universal QRAM (U-QRAM) interface. Given query access to a quantum memory storing an unknown Boolean function $f:[N]\to\{0,1\}$, we ask: what can be inferred about the bias class of $f$ (its imbalance from $1/2$, up to complement symmetry) using coherent, addressable queries? We show that for exact-weight bias classes, the induced single-query ensemble state on the address register has a two-eigenspace structure that yields closed-form expressions for the single-copy Helstrom-optimal measurement and success probability. Because complementing $f$ changes the state $|ψ\rangle$ only by a global phase, hypotheses $p$ and $1-p$ are information-theoretically identical in this model; thus the natural discriminand is the phase-bias magnitude $|μ|$ (equivalently $μ^2$). This goes beyond the perfect-discrimination case of Deutsch-Jozsa and complements exact-identification settings such as Bernstein-Vazirani.

**Link**: [arxiv](https://arxiv.org/abs/2512.17503v1),  [pdf](https://arxiv.org/pdf/2512.17503v1)

**Tags**: quant-ph 



### Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification
**Authors**: Xiaoyu Tao, Tingyue Pan, Mingyue Cheng, Yucong Luo, Qi Liu, Enhong Chen

**Updated**: 2025-12-19T12:12:39Z

**Summary**: Time series classification plays a fundamental role in a wide range of real-world applications. Recently, large language models (LLMs) have demonstrated strong generalization and reasoning capacities, but directly applying them to time series classification remains non-trivial due to the representation gap between numerical sequences and linguistic semantics. In this paper, we propose HiTime, a hierarchical LLM-based framework for multimodal time series classification that bridges structured temporal representations with semantic reasoning in a generative paradigm. Specifically, we design a hierarchical sequence feature encoding module composed of a data-specific encoder and a task-specific encoder to extract complementary temporal features. To mitigate the embedding gap between time series representations and textual semantics, we further introduce a semantic space alignment module that jointly performs coarse-grained global modeling and fine-grained cross-modal correspondence. Building upon the above representations, we employ a parameter-efficient supervised fine-tuning strategy to activate the generative classification capability of the algined LLMs, thereby transforming conventional discriminative time series classification into a generative task. Extensive experiments on multiple benchmarks demonstrate that the proposed framework consistently outperforms state-of-the-art baselines. The code is publicly available at https://github.com/Xiaoyu-Tao/HiTime.

**Link**: [arxiv](https://arxiv.org/abs/2410.18686v2),  [pdf](https://arxiv.org/pdf/2410.18686v2)

**Tags**: cs.LG 



### High-Resolution Measurements with the CTAO Southern Array: The Case for Pulsar Wind Nebulae
**Authors**: Georg Schwefer, Jim Hinton

**Updated**: 2025-12-19T12:08:02Z

**Summary**: The advent of the Cherenkov Telescope Array Observatory (CTAO) and recent advances in reconstruction of gamma-ray photons with Cherenkov telescopes are bound to push the limit of angular resolution to an unprecedented precision of less than one arcminute at tens of TeV. Naturally, such instrumental improvements open up possibilities for new and interesting scientific studies. We aim to show that the study of pulsar wind nebulae (PWNe) in particular is bound to profit from these high-resolution measurements. This is because PWNe are the dominant Galactic source population at TeV energies, exhibit hard spectra up to hundreds of TeV and from X-ray observations are known to possess plentiful structure on arcminute scales. Using HESS J1813-178 and MSH 15-52 as examples, we create simple leptonic models of the TeV morphology of these sources based on X-ray observations and existing gamma-ray measurements. Then, assuming different models for the exposure and point spread function of the observatory, we create mock observations with the future CTAO southern array. We use these to assess the ability of these observations to differentiate between models and study the physics of these sources, in particular to infer the structure of the magnetic field and electron distributions. We find that future observations with the CTAO southern array at multi-TeV energies - in combination with existing X-ray measurements - will likely be able to constrain the distributions of magnetic field and high-energy electrons in these sources. We demonstrate that the sensitivity of these measurements can be significantly enhanced with the improved angular resolution achievable with novel reconstruction algorithms. However, we also show that in the relevant multi-TeV regime, signal-photon statistics remain a limitation and trading event statistics for improved angular resolution is not necessarily advantageous.

**Link**: [arxiv](https://arxiv.org/abs/2512.17498v1),  [pdf](https://arxiv.org/pdf/2512.17498v1)

**Tags**: astro-ph.HE 



### Inference on state occupancy in covariate-driven hidden Markov models
**Authors**: Maya N. Vienken, Jan-Ole Koslik, Roland Langrock

**Updated**: 2025-12-19T12:06:30Z

**Summary**: Hidden Markov models (HMMs) are popular tools for analysing animal behaviour based on movement, acceleration and other sensor data. In particular, these models allow to infer how the animal's decision-making process interacts with internal and external drivers, by relating the probabilities of switching between distinct behavioural states to covariates. A key challenge arising in the statistical analysis of behavioural data using covariate-driven HMMs is the models' interpretation, especially when there are more than two states, as then several functional relationships between state-switching probabilities and covariates need to be jointly interpreted. The model-implied probabilities of occupying the different states, as a function of a covariate of interest, constitute a much simpler summary statistic. A pragmatic approximation of the state occupancy distribution, namely the hypothetical stationary distribution of the model's underlying Markov chain for fixed covariate values, has in fact routinely been reported in HMM-based analyses of ecological data. However, for stochastically varying covariates with relatively little persistence, we show that this approximation can be severely biased, potentially invalidating ecological inference. We develop two alternative approaches for obtaining the state occupancy distribution as a function of a covariate of interest - one based on resampling of the covariate process, the other obtained by regression analysis of the empirical state probabilities. The practical application of these approaches is demonstrated in simulations and a case study on Galápagos tortoise (Chelonoidis niger) movement data. Our methods enable practitioners to conduct unbiased inference on the relationship between animal behaviour and general types of covariates, thus allowing to uncover the factors influencing behavioural decisions made by animals.

**Link**: [arxiv](https://arxiv.org/abs/2512.17496v1),  [pdf](https://arxiv.org/pdf/2512.17496v1)

**Tags**: stat.ME 



### Towards Facilitated Fairness Assessment of AI-based Skin Lesion Classifiers Through GenAI-based Image Synthesis
**Authors**: Ko Watanabe, Stanislav Frolov, Aya Hassan, David Dembinsky, Adriano Lucieri, Andreas Dengel

**Updated**: 2025-12-19T11:48:41Z

**Summary**: Recent advances in deep learning and on-device inference could transform routine screening for skin cancers. Along with the anticipated benefits of this technology, potential dangers arise from unforeseen and inherent biases. A significant obstacle is building evaluation datasets that accurately reflect key demographics, including sex, age, and race, as well as other underrepresented groups. To address this, we train a state-of-the-art generative model to generate synthetic data in a controllable manner to assess the fairness of publicly available skin cancer classifiers. To evaluate whether synthetic images can be used as a fairness testing dataset, we prepare a real-image dataset (MILK10K) as a benchmark and compare the True Positive Rate result of three models (DeepGuide, MelaNet, and SkinLesionDensnet). As a result, the classification tendencies observed in each model when tested on real and generated images showed similar patterns across different attribute data sets. We confirm that highly realistic synthetic images facilitate model fairness verification.

**Link**: [arxiv](https://arxiv.org/abs/2507.17860v3),  [pdf](https://arxiv.org/pdf/2507.17860v3)

**Tags**: cs.CV cs.AI cs.LG 



### Inference for high dimensional repeated measure designs with the R package hdrm
**Authors**: Paavo Sattler, Nils Hichert

**Updated**: 2025-12-19T11:44:47Z

**Summary**: Repeated-measure designs allow comparisons within a group as well as between groups, and are commonly referred to as split-plot designs. While originating in agricultural experiments, they are now widely used in medical research, psychology, and the life sciences, where repeated observations on the same subject are essential.   Modern data collection often produces observation vectors with dimension $d$ comparable to or exceeding the sample size $N$. Although this can be advantageous in terms of cost efficiency, ethical considerations, and the study of rare diseases, it poses substantial challenges for statistical inference.   Parametric methods based on multivariate normality provide a flexible framework that avoids restrictive assumptions on covariance structures or on the asymptotic relationship between $d$ and $N$. Within this framework, the freely available R-package hdrm enables the analysis of a wide range of hypotheses concerning expectation vectors in high-dimensional repeated-measure designs, covering both single-group and multi-group settings with homogeneous or heterogeneous covariance matrices.   This paper describes the implemented tests, demonstrates their use through examples, and discusses their applicability in practical high-dimensional data scenarios. To address computational challenges arising for large $d$, the package incorporates efficient estimators and subsampling strategies that substantially reduce computation time while preserving statistical validity.

**Link**: [arxiv](https://arxiv.org/abs/2512.17478v1),  [pdf](https://arxiv.org/pdf/2512.17478v1)

**Tags**: stat.CO stat.ME 



### Bayesian Markov-Switching Partial Reduced-Rank Regression
**Authors**: Maria F. Pintado, Matteo Iacopini, Luca Rossini, Alexander Y. Shestopaloff

**Updated**: 2025-12-19T11:37:32Z

**Summary**: Reduced-Rank (RR) regression is a powerful dimensionality reduction technique but it overlooks any possible group configuration among the responses by assuming a low-rank structure on the entire coefficient matrix. Moreover, the temporal change of the relations between predictors and responses in time series induce a possibly time-varying grouping structure in the responses. To address these limitations, a Bayesian Markov-switching partial RR (MS-PRR) model is proposed, where the response vector is partitioned in two groups to reflect different complexity of the relationship. A \textit{simple} group assumes a low-rank linear regression, while a \textit{complex} group exploits nonparametric regression via a Gaussian Process. Differently from traditional approaches, group assignments and rank are treated as unknown parameters to be estimated. Then temporal persistence in the regression function is accounted for by a Markov-switching process that drives the changes in the grouping structure and model parameters over time. Full Bayesian inference is preformed via a partially collapsed Gibbs sampler, which allows uncertainty quantification without the need for trans-dimensional moves. Applications to two real-world macroeconomic and commodity data demonstrate the evidence of time-varying grouping and different degrees of complexity both across states and within each state.

**Link**: [arxiv](https://arxiv.org/abs/2512.17471v1),  [pdf](https://arxiv.org/pdf/2512.17471v1)

**Tags**: stat.ME stat.CO 



### Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models: A Reflection
**Authors**: David Williams, Max Hort, Maria Kechagia, Aldeida Aleti, Justyna Petke, Federica Sarro

**Updated**: 2025-12-19T11:30:54Z

**Summary**: Software Engineering (SE) research involving the use of Large Language Models (LLMs) has introduced several new challenges related to rigour in benchmarking, contamination, replicability, and sustainability. In this paper, we invite the research community to reflect on how these challenges are addressed in SE. Our results provide a structured overview of current LLM-based SE research at ICSE, highlighting both encouraging practices and persistent shortcomings. We conclude with recommendations to strengthen benchmarking rigour, improve replicability, and address the financial and environmental costs of LLM-based SE.

**Link**: [arxiv](https://arxiv.org/abs/2510.26538v2),  [pdf](https://arxiv.org/pdf/2510.26538v2)

**Tags**: cs.SE 



### Helmsman: Autonomous Synthesis of Federated Learning Systems via Collaborative LLM Agents
**Authors**: Haoyuan Li, Mathias Funk, Aaqib Saeed

**Updated**: 2025-12-19T11:27:02Z

**Summary**: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.

**Link**: [arxiv](https://arxiv.org/abs/2510.14512v2),  [pdf](https://arxiv.org/pdf/2510.14512v2)

**Tags**: cs.AI 



### VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments
**Authors**: Yuze Wu, Mo Zhu, Xingxing Li, Yuheng Du, Yuxin Fan, Wenjun Li, Zhichao Han, Xin Zhou, Fei Gao

**Updated**: 2025-12-19T11:22:57Z

**Summary**: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

**Link**: [arxiv](https://arxiv.org/abs/2512.15258v2),  [pdf](https://arxiv.org/pdf/2512.15258v2)

**Tags**: cs.RO cs.AI 



### 3D-RE-GEN: 3D Reconstruction of Indoor Scenes with a Generative Framework
**Authors**: Tobias Sautter, Jan-Niklas Dihlmann, Hendrik P. A. Lensch

**Updated**: 2025-12-19T11:20:52Z

**Summary**: Recent advances in 3D scene generation produce visually appealing output, but current representations hinder artists' workflows that require modifiable 3D textured mesh scenes for visual effects and game development. Despite significant advances, current textured mesh scene reconstruction methods are far from artist ready, suffering from incorrect object decomposition, inaccurate spatial relationships, and missing backgrounds. We present 3D-RE-GEN, a compositional framework that reconstructs a single image into textured 3D objects and a background. We show that combining state of the art models from specific domains achieves state of the art scene reconstruction performance, addressing artists' requirements.   Our reconstruction pipeline integrates models for asset detection, reconstruction, and placement, pushing certain models beyond their originally intended domains. Obtaining occluded objects is treated as an image editing task with generative models to infer and reconstruct with scene level reasoning under consistent lighting and geometry. Unlike current methods, 3D-RE-GEN generates a comprehensive background that spatially constrains objects during optimization and provides a foundation for realistic lighting and simulation tasks in visual effects and games. To obtain physically realistic layouts, we employ a novel 4-DoF differentiable optimization that aligns reconstructed objects with the estimated ground plane. 3D-RE-GEN~achieves state of the art performance in single image 3D scene reconstruction, producing coherent, modifiable scenes through compositional generation guided by precise camera recovery and spatial optimization.

**Link**: [arxiv](https://arxiv.org/abs/2512.17459v1),  [pdf](https://arxiv.org/pdf/2512.17459v1)

**Tags**: cs.CV 



### xGR: Efficient Generative Recommendation Serving at Scale
**Authors**: Qingxiao Sun, Tongxuan Liu, Shen Zhang, Siyu Wu, Peijun Yang, Haotian Liang, Menxin Li, Xiaolong Ma, Zhiwei Liang, Ziyi Ren, Minchao Zhang, Xinyu Liu, Ke Zhang, Depei Qian, Hailong Yang

**Updated**: 2025-12-19T11:20:16Z

**Summary**: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

**Link**: [arxiv](https://arxiv.org/abs/2512.11529v2),  [pdf](https://arxiv.org/pdf/2512.11529v2)

**Tags**: cs.LG 



### An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys
**Authors**: Ronnie de Souza Santos, Italo Santos, Maria Teresa Baldassarre, Cleyton Magalhaes, Mairieli Wessel

**Updated**: 2025-12-19T11:17:05Z

**Summary**: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

**Link**: [arxiv](https://arxiv.org/abs/2512.17455v1),  [pdf](https://arxiv.org/pdf/2512.17455v1)

**Tags**: cs.SE 



### Learning What to Write: Write-Gated KV for Efficient Long-Context Inference
**Authors**: Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu

**Updated**: 2025-12-19T11:08:58Z

**Summary**: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

**Link**: [arxiv](https://arxiv.org/abs/2512.17452v1),  [pdf](https://arxiv.org/pdf/2512.17452v1)

**Tags**: cs.LG cs.AI 



### Insights into the structure and kinematics of a Milky Way-like galaxy
**Authors**: Eva Durán-Camacho, Ana Duarte-Cabral

**Updated**: 2025-12-19T11:07:14Z

**Summary**: Understanding how the large-scale kinematics of the MW shape the formation and evolution of the interstellar medium remains challenging from an observational perspective, and numerical models that can reproduce the observed structure and kinematics of the MW are much needed in order to infer how the MW might work as a star formation engine. This work aims to use a numerical framework that is a close match to the observed large-scale distribution of stars and gas in the MW to isolate and understand the impact of galaxy-driven flows on the formation, agglomeration, and longevity of spiral patterns, prior to the inclusion of chemistry, star formation, and feedback. We use an isothermal simulation of a MW-like galaxy, found to closely match the longitude-velocity observational features of the MW in previous work, that includes the coupled evolution of gas, stars, and dark matter under purely gravitational and hydrodynamical processes. We characterise the morphology and kinematics of the stars and gas in the disc, quantify velocity residuals and their association with spiral features, and analyse the time-evolution of individual spiral-ridge segments. Our results demonstrate that our model reproduces many observed MW structural and kinematic signatures, from the inner Galaxy to the Solar neighbourhood, supporting its suitability as an analogue of the MW. The stellar spiral pattern in our model is relatively weak and shows lower multiplicity relative to the sharper gaseous arms, offering an explanation for discrepancies in observational determinations of the number and location of MW spiral arms. Both gas and stellar spiral arms are highly segmented, without a single coherent spiral pattern as expected from a grand-design type of galaxy. We find strong radial motions linked to the non-circular motions driven by the presence of a bar, and which extend well into the disc. The gas radial and...

**Link**: [arxiv](https://arxiv.org/abs/2506.13560v2),  [pdf](https://arxiv.org/pdf/2506.13560v2)

**Tags**: astro-ph.GA 



### When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals
**Authors**: Riad Ahmed Anonto, Md Labid Al Nahiyan, Md Tanvir Hassan

**Updated**: 2025-12-19T11:00:50Z

**Summary**: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.

**Link**: [arxiv](https://arxiv.org/abs/2512.01037v2),  [pdf](https://arxiv.org/pdf/2512.01037v2)

**Tags**: cs.CL cs.AI 



### ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination
**Authors**: Teng Wang, Xinxin Zhao, Wenzhe Cai, Changyin Sun

**Updated**: 2025-12-19T10:40:16Z

**Summary**: Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2512.17435v1),  [pdf](https://arxiv.org/pdf/2512.17435v1)

**Tags**: cs.RO 



### OptScale: Probabilistic Optimality for Inference-time Scaling
**Authors**: Youkang Wang, Jian Wang, Rubing Chen, Xiao-Yong Wei

**Updated**: 2025-12-19T10:17:53Z

**Summary**: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2506.22376v4),  [pdf](https://arxiv.org/pdf/2506.22376v4)

**Tags**: cs.LG cs.AI cs.CL 



### SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories
**Authors**: Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe

**Updated**: 2025-12-19T10:16:51Z

**Summary**: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

**Link**: [arxiv](https://arxiv.org/abs/2512.17419v1),  [pdf](https://arxiv.org/pdf/2512.17419v1)

**Tags**: cs.SE cs.AI cs.CL cs.LG 



### Large deviation principle for the absorption time of the Beta-coalescent via integral functionals
**Authors**: Grégoire Véchambre

**Updated**: 2025-12-19T10:15:02Z

**Summary**: We study some aspects of the absorption time of the Beta$(a,b)$-Coalescent starting with $n$ blocks. More precisely, when $a>1$, the absorption time is known to converge to infinity as $n$ goes to infinity, and we prove that it satisfies a large deviation principle. When $a \in (0,1)$, it is known that the coalescent comes down from infinity, and we derive bounds for the convergence in the Kolmogorov distance of the distribution of the absorption time as $n$ goes to infinity. To prove our results we introduce a method, inspired from statistical mechanics, that allows to infer the asymptotic behavior of the Laplace transforms of some integral functionals of the Beta-coalescent as the initial number of blocks $n$ goes to infinity. As a by-product of our proofs we also obtain estimates for the record probabilities of the Beta-coalescent.

**Link**: [arxiv](https://arxiv.org/abs/2512.17418v1),  [pdf](https://arxiv.org/pdf/2512.17418v1)

**Tags**: math.PR 



### CNFinBench: A Benchmark for Safety and Compliance of Large Language Models in Finance
**Authors**: Jinru Ding, Chao Ding, Wenrao Pang, Boyi Xiao, Zhiqiang Liu, Pengcheng Chen, Jiayuan Chen, Tiantian Yuan, Junming Guan, Yidong Jiang, Dawei Cheng, Jie Xu

**Updated**: 2025-12-19T10:12:50Z

**Summary**: Large language models (LLMs) are increasingly deployed across the financial sector for tasks like investment research and algorithmic trading. Their high-stakes nature demands rigorous evaluation of models' safety and regulatory alignment. However, there is a significant gap between evaluation capabilities and safety requirements. Current financial benchmarks mainly focus on textbook-style question answering and numerical problem-solving, failing to simulate the open-ended scenarios where safety risks typically manifest. To close these gaps, we introduce CNFinBench, a benchmark structured around a Capability-Compliance-Safety triad encompassing 15 subtasks. For Capability Q&As, we introduce a novel business-vertical taxonomy aligned with core financial domains like banking operations, which allows institutions to assess model readiness for deployment in operational scenarios. For Compliance and Risk Control Q&As, we embed regulatory requirements within realistic business scenarios to ensure models are evaluated under practical, scenario-driven conditions. For Safety Q&As, we uniquely incorporate structured bias and fairness auditing, a dimension overlooked by other holistic financial benchmarks, and introduce the first multi-turn adversarial dialogue task to systematically expose compliance decay under sustained, context-aware attacks. Accordingly, we propose the Harmful Instruction Compliance Score (HICS) to quantify models' consistency in resisting harmful instructions across multi-turn dialogues. Experiments on 21 models across all subtasks reveal a persistent gap between capability and compliance: models achieve an average score of 61.0 on capability tasks but drop to 34.2 on compliance and risk-control evaluations. In multi-turn adversarial dialogue tests, most LLMs attain only partial resistance, demonstrating that refusal alone is insufficient without cited, verifiable reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2512.09506v2),  [pdf](https://arxiv.org/pdf/2512.09506v2)

**Tags**: cs.CE 



## Keyword: LLM Deployment 
 ### XAgen: An Explainability Tool for Identifying and Correcting Failures in Multi-Agent Workflows
**Authors**: Xinru Wang, Ming Yin, Eunyee Koh, Mustafa Doga Dogan

**Updated**: 2025-12-19T18:54:36Z

**Summary**: As multi-agent systems powered by Large Language Models (LLMs) are increasingly adopted in real-world workflows, users with diverse technical backgrounds are now building and refining their own agentic processes. However, these systems can fail in opaque ways, making it difficult for users to observe, understand, and correct errors. We conducted formative interviews with 12 practitioners to identify mismatches between existing observability tools and users' needs. Based on these insights, we designed XAgen, an explainability tool that supports users with varying AI expertise through three core capabilities: log visualization for glanceable workflow understanding, human-in-the-loop feedback to capture expert judgment, and automatic error detection via an LLM-as-a-judge. In a user study with 8 participants, XAgen helped users more easily locate failures, attribute to specific agents or steps, and iteratively improve configurations. Our findings surface human-centered design guidelines for explainable agentic AI development and highlights opportunities for more context-aware interactive debugging.

**Link**: [arxiv](https://arxiv.org/abs/2512.17896v1),  [pdf](https://arxiv.org/pdf/2512.17896v1)

**Tags**: cs.HC 



### SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars
**Authors**: Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo

**Updated**: 2025-12-19T18:39:57Z

**Summary**: In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP

**Link**: [arxiv](https://arxiv.org/abs/2507.01939v4),  [pdf](https://arxiv.org/pdf/2507.01939v4)

**Tags**: astro-ph.IM astro-ph.SR cs.AI cs.LG 



### Adaptive Focus Memory for Language Models
**Authors**: Christopher Cruz

**Updated**: 2025-12-19T18:24:09Z

**Summary**: Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.   We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.   We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.   These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.

**Link**: [arxiv](https://arxiv.org/abs/2511.12712v2),  [pdf](https://arxiv.org/pdf/2511.12712v2)

**Tags**: cs.CL cs.AI 



### Same Content, Different Representations: A Controlled Study for Table QA
**Authors**: Yue Zhang, Seiji Maekawa, Nikita Bhutani

**Updated**: 2025-12-19T18:19:10Z

**Summary**: Table Question Answering (Table QA) in real-world settings must operate over both structured databases and semi-structured tables containing textual fields. However, existing benchmarks are tied to fixed data formats and have not systematically examined how representation itself affects model performance. We present the first controlled study that isolates the role of table representation by holding content constant while varying structure. Using a verbalization pipeline, we generate paired structured and semi-structured tables, enabling direct comparisons across modeling paradigms. To support detailed analysis, we introduce RePairTQA, a diagnostic benchmark with splits along table size, join requirements, query complexity, and schema quality. Our experiments reveal consistent trade-offs: SQL-based methods achieve high accuracy on structured inputs but degrade on semi-structured data, LLMs exhibit flexibility but reduced precision, and hybrid approaches strike a balance, particularly under noisy schemas. These effects intensify with larger tables and more complex queries. Ultimately, no single method excels across all conditions, and we highlight the central role of representation in shaping Table QA performance. Our findings provide actionable insights for model selection and design, paving the way for more robust hybrid approaches suited for diverse real-world data formats.

**Link**: [arxiv](https://arxiv.org/abs/2509.22983v2),  [pdf](https://arxiv.org/pdf/2509.22983v2)

**Tags**: cs.CL 



### Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning
**Authors**: Simon Frieder, Jonas Bayer, Sam Looi, Jacob Loader, Julius Berner, Katherine M. Collins, András Juhász, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Cameron Freer, Thomas Lukasiewicz, Timothy Gowers

**Updated**: 2025-12-19T18:17:28Z

**Summary**: The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart's law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ``thought partners''), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models' mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of "motivated proof", introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.

**Link**: [arxiv](https://arxiv.org/abs/2412.15184v2),  [pdf](https://arxiv.org/pdf/2412.15184v2)

**Tags**: cs.LG 



### Towards Human-Guided, Data-Centric LLM Co-Pilots
**Authors**: Evgeny Saveliev, Jiashuo Liu, Nabeel Seedat, Anders Boyd, Mihaela van der Schaar

**Updated**: 2025-12-19T18:08:16Z

**Summary**: Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC's ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML.

**Link**: [arxiv](https://arxiv.org/abs/2501.10321v3),  [pdf](https://arxiv.org/pdf/2501.10321v3)

**Tags**: cs.LG stat.ML 



### AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning
**Authors**: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper

**Updated**: 2025-12-19T17:55:48Z

**Summary**: Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .

**Link**: [arxiv](https://arxiv.org/abs/2512.17853v1),  [pdf](https://arxiv.org/pdf/2512.17853v1)

**Tags**: cs.RO cs.AI 



### ShareChat: A Dataset of Chatbot Conversations in the Wild
**Authors**: Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le

**Updated**: 2025-12-19T17:47:53Z

**Summary**: While Large Language Models (LLMs) have evolved into distinct platforms with unique interface designs and capabilities, existing public datasets treat models as generic text generators, stripping away the interface context that actively shapes user interaction. To address this limitation, we present ShareChat, a large-scale, cross-platform corpus comprising 142,808 conversations and over 660,000 turns collected from publicly shared URLs across five major platforms: ChatGPT, Claude, Gemini, Perplexity, and Grok. ShareChat distinguishes itself by preserving native platform affordances often lost in standard logs, including reasoning traces, source links, and code artifacts, while spanning 101 languages over the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. We demonstrate the dataset's multifaceted utility through three representative analyses: (1) analyzing conversation completeness to measure user intent satisfaction; (2) evaluating source citation behaviors in content generation; and (3) conducting temporal analysis to track evolving usage patterns. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild.

**Link**: [arxiv](https://arxiv.org/abs/2512.17843v1),  [pdf](https://arxiv.org/pdf/2512.17843v1)

**Tags**: cs.CL cs.AI cs.HC 



### NeuRehab: A Reinforcement Learning and Spiking Neural Network-Based Rehab Automation Framework
**Authors**: Phani Pavan Kambhampati, Chainesh Gautam, Jagan Palaniswamy, Madhav Rao

**Updated**: 2025-12-19T17:47:14Z

**Summary**: Recent advancements in robotic rehabilitation therapy have provided modular exercise systems for post-stroke muscle recovery with basic control schemes. But these systems struggle to adapt to patients' complex and ever-changing behaviour, and to operate within mobile settings, such as heat and power. To aid this, we present NeuRehab: an end-to-end framework consisting of a training and inference pipeline with AI-based automation, co-designed with neuromorphic computing-based control systems that balance action performance, power consumption, and observed latency. The framework consists of 2 partitions. One is designated for the rehabilitation device based on ultra-low power spiking networks deployed on dedicated neuromorphic hardware. The other resides on stationary hardware that can accommodate computationally intensive hardware for fine-tuning on a per-patient basis. By maintaining a communication channel between both the modules and splitting the algorithm components, the power and latency requirements of the movable system have been optimised, while retaining the learning performance advantages of compute- and power-hungry hardware on the stationary machine. As part of the framework, we propose (a) the split machine learning processes for efficiency in architectural utilisation, and (b) task-specific temporal optimisations to lower edge-inference control latency. This paper evaluates the proposed methods on a reference stepper motor-based shoulder exercise. Overall, these methods offer comparable performance uplifts over the State-of-the-art for neuromorphic deployment, while achieving over 60% savings in both power and latency during inference compared to standard implementations.

**Link**: [arxiv](https://arxiv.org/abs/2512.17841v1),  [pdf](https://arxiv.org/pdf/2512.17841v1)

**Tags**: cs.CE eess.SY 



### ReX-MLE: The Autonomous Agent Benchmark for Medical Imaging Challenges
**Authors**: Roshan Kenia, Xiaoman Zhang, Pranav Rajpurkar

**Updated**: 2025-12-19T17:44:40Z

**Summary**: Autonomous coding agents built on large language models (LLMs) can now solve many general software and machine learning tasks, but they remain ineffective on complex, domain-specific scientific problems. Medical imaging is a particularly demanding domain, requiring long training cycles, high-dimensional data handling, and specialized preprocessing and validation pipelines, capabilities not fully measured in existing agent benchmarks. To address this gap, we introduce ReX-MLE, a benchmark of 20 challenges derived from high-impact medical imaging competitions spanning diverse modalities and task types. Unlike prior ML-agent benchmarks, ReX-MLE evaluates full end-to-end workflows, requiring agents to independently manage data preprocessing, model training, and submission under realistic compute and time constraints. Evaluating state-of-the-art agents (AIDE, ML-Master, R&D-Agent) with different LLM backends (GPT-5, Gemini, Claude), we observe a severe performance gap: most submissions rank in the 0th percentile compared to human experts. Failures stem from domain-knowledge and engineering limitations. ReX-MLE exposes these bottlenecks and provides a foundation for developing domain-aware autonomous AI systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.17838v1),  [pdf](https://arxiv.org/pdf/2512.17838v1)

**Tags**: cs.CV 



### Step-GUI Technical Report
**Authors**: Haolong Yan, Jia Wang, Xin Huang, Yeqing Shen, Ziyang Meng, Zhimin Fan, Kaijun Tan, Jin Gao, Lieyu Shi, Mi Yang, Shiliang Yang, Zhirui Wang, Brian Li, Kang An, Chenyang Li, Lei Lei, Mengmeng Duan, Danxun Liang, Guodong Liu, Hang Cheng, Hao Wu, Jie Dong, Junhao Huang, Mei Chen, Renjie Yu, Shunshan Li, Xu Zhou, Yiting Dai, Yineng Deng, Yingdan Liang, Zelin Chen, Wen Sun, Chengxu Yan, Chunqin Xu, Dong Li, Fengqiong Xiao, Guanghao Fan, Guopeng Li, Guozhen Peng, Hongbing Li, Hang Li, Hongming Chen, Jingjing Xie, Jianyong Li, Jingyang Zhang, Jiaju Ren, Jiayu Yuan, Jianpeng Yin, Kai Cao, Liang Zhao, Liguo Tan, Liying Shi, Mengqiang Ren, Min Xu, Manjiao Liu, Mao Luo, Mingxin Wan, Na Wang, Nan Wu, Ning Wang, Peiyao Ma, Qingzhou Zhang, Qiao Wang, Qinlin Zeng, Qiong Gao, Qiongyao Li, Shangwu Zhong, Shuli Gao, Shaofan Liu, Shisi Gao, Shuang Luo, Xingbin Liu, Xiaojia Liu, Xiaojie Hou, Xin Liu, Xuanti Feng, Xuedan Cai, Xuan Wen, Xianwei Zhu, Xin Liang, Xin Liu, Xin Zhou, Yifan Sui, Yingxiu Zhao, Yukang Shi, Yunfang Xu, Yuqing Zeng, Yixun Zhang, Zejia Weng, Zhonghao Yan, Zhiguo Huang, Zhuoyu Wang, Zihan Yan, Zheng Ge, Jing Li, Yibo Zhu, Binxing Jiao, Xiangyu Zhang, Daxin Jiang

**Updated**: 2025-12-19T17:36:21Z

**Summary**: Recent advances in multimodal large language models unlock unprecedented opportunities for GUI automation. However, a fundamental challenge remains: how to efficiently acquire high-quality training data while maintaining annotation reliability? We introduce a self-evolving training pipeline powered by the Calibrated Step Reward System, which converts model-generated trajectories into reliable training signals through trajectory-level calibration, achieving >90% annotation accuracy with 10-100x lower cost. Leveraging this pipeline, we introduce Step-GUI, a family of models (4B/8B) that achieves state-of-the-art GUI performance (8B: 80.2% AndroidWorld, 48.5% OSWorld, 62.6% ScreenShot-Pro) while maintaining robust general capabilities. As GUI agent capabilities improve, practical deployment demands standardized interfaces across heterogeneous devices while protecting user privacy. To this end, we propose GUI-MCP, the first Model Context Protocol for GUI automation with hierarchical architecture that combines low-level atomic operations and high-level task delegation to local specialist models, enabling high-privacy execution where sensitive data stays on-device. Finally, to assess whether agents can handle authentic everyday usage, we introduce AndroidDaily, a benchmark grounded in real-world mobile usage patterns with 3146 static actions and 235 end-to-end tasks across high-frequency daily scenarios (8B: static 89.91%, end-to-end 52.50%). Our work advances the development of practical GUI agents and demonstrates strong potential for real-world deployment in everyday digital interactions.

**Link**: [arxiv](https://arxiv.org/abs/2512.15431v2),  [pdf](https://arxiv.org/pdf/2512.15431v2)

**Tags**: cs.CV 



### RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation
**Authors**: Dongyub Jude Lee, Zhenyi Ye, Pengcheng He

**Updated**: 2025-12-19T17:35:31Z

**Summary**: Preference-learning methods for machine translation (MT), such as Direct Preference Optimization (DPO), have shown strong gains but typically rely on large, carefully curated preference triplets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), which replaces static triplets with on-policy, actor-conditioned refinements produced by a frozen teacher. At each step, the actor samples candidate translations, the teacher performs a minimal local edit of each draft, and the actor is reinforced to close the gap using a composite reward that combines scaled negative edit distance for lexical and structural fidelity with COMET for semantic adequacy. This formulation yields a stable, model-aware learning signal without requiring explicit preference datasets. Experiments on FLORES-200 (English to German, Spanish, Chinese, Korean, and Japanese) show that RLfR consistently outperforms strong MT-SFT, DPO, and fixed-reference RL baselines, improving semantic quality and entity preservation, and also achieves superior performance under LLM-based judge evaluations.

**Link**: [arxiv](https://arxiv.org/abs/2507.22219v3),  [pdf](https://arxiv.org/pdf/2507.22219v3)

**Tags**: cs.CL cs.AI 



### A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code
**Authors**: Alejandro Velasco, Daniel Rodriguez-Cardenas, Dipin Khati, David N. Palacio, Luftar Rahman Alif, Denys Poshyvanyk

**Updated**: 2025-12-19T17:21:22Z

**Summary**: Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.

**Link**: [arxiv](https://arxiv.org/abs/2511.15817v4),  [pdf](https://arxiv.org/pdf/2511.15817v4)

**Tags**: cs.SE 



### LLM-based Behaviour Driven Development for Hardware Design
**Authors**: Rolf Drechsler, Qian Liu

**Updated**: 2025-12-19T17:19:08Z

**Summary**: Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.   Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.

**Link**: [arxiv](https://arxiv.org/abs/2512.17814v1),  [pdf](https://arxiv.org/pdf/2512.17814v1)

**Tags**: cs.SE cs.AI cs.AR 



### Reinforced Generation of Combinatorial Structures: Hardness of Approximation
**Authors**: Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta

**Updated**: 2025-12-19T16:58:50Z

**Summary**: Can AI based methods help us make advances in complexity theory? We provide evidence towards answering this in the affirmative, using AlphaEvolve (an LLM code mutation agent) to obtain new results in three settings:   a) We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ vertices, and our upper bounds are obtained via analytical arguments.   b) We obtain new inapproximability results for MAX-4-CUT and MAX-3-CUT, proving that it is NP-hard to approximate them within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of the SOTA of $16/17$ that relies on a custom PCP (rather than a reduction from ``standard'' Håstad-style PCPs).   c) Inapproximability for the metric Traveling Salesman Problem (TSP): We show that it is NP-hard to approximate the minimum cost tour within a factor of $111/110$ using AlphaEvolve to discover a new gadget, thus improving the SOTA of $117/116$. Along the way, we provide new modular soundness and completeness arguments that can be of independent interest.   A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (sometimes requiring time exponential in the size of the construction). We used AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$ for our gadgets). Our results suggest that gadget based proofs would benefit from a pass through AI-based tools to obtain stronger results.

**Link**: [arxiv](https://arxiv.org/abs/2509.18057v6),  [pdf](https://arxiv.org/pdf/2509.18057v6)

**Tags**: cs.LG cs.AI cs.CC math.CO 



### Rank-GRPO: Training LLM-based Conversational Recommender Systems with Reinforcement Learning
**Authors**: Yaochen Zhu, Harald Steck, Dawen Liang, Yinhan He, Vito Ostuni, Jundong Li, Nathan Kallus

**Updated**: 2025-12-19T16:58:20Z

**Summary**: Large language models (LLMs) are reshaping the recommender system paradigm by enabling users to express preferences and receive recommendations through conversations. Yet, aligning LLMs to the recommendation task remains challenging: pretrained LLMs often generate out-of-catalog items, violate required output formats, and their ranking quality degrades sharply toward the end of the generated list. To this end, we propose ConvRec-R1, a two-stage framework for end-to-end training of LLM-based conversational recommender systems. In Stage 1, we construct a behavioral-cloning dataset with a Remap-Reflect-Adjust pipeline, which produces high-quality, catalog-grounded demonstrations from powerful blackbox LLMs to warm-start the RL training. In Stage 2, we propose Rank-GRPO, a principled extension of group relative policy optimization (GRPO) tailored to tasks with rank-style outputs. Rank-GRPO treats each rank in the recommendation list as the unit instead of token (too fine-grained) or sequence (too coarse), redefining rewards to remove non-causal credit assignment and introducing a rank-level importance ratio based on the geometric mean of rank-wise token probabilities to stabilize policy updates. Experiments on the public Reddit-v2 dataset show that ConvRec-R1 converges faster and achieves higher Recall and NDCG than GRPO-style baselines. Code and datasets are released at https://github.com/yaochenzhu/Rank-GRPO.

**Link**: [arxiv](https://arxiv.org/abs/2510.20150v4),  [pdf](https://arxiv.org/pdf/2510.20150v4)

**Tags**: cs.IR 



### HGQ: High Granularity Quantization for Real-time Neural Networks on FPGAs
**Authors**: Chang Sun, Zhiqiang Que, Thea K. Årrestad, Vladimir Loncar, Jennifer Ngadiuba, Wayne Luk, Maria Spiropulu

**Updated**: 2025-12-19T16:57:39Z

**Summary**: Neural networks with sub-microsecond inference latency are required by many critical applications. Targeting such applications deployed on FPGAs, we present High Granularity Quantization (HGQ), a quantization-aware training framework that optimizes parameter bit-widths through gradient descent. Unlike conventional methods, HGQ determines the optimal bit-width for each parameter independently, making it suitable for hardware platforms supporting heterogeneous arbitrary precision arithmetic. In our experiments, HGQ shows superior performance compared to existing network compression methods, achieving orders of magnitude reduction in resource consumption and latency while maintaining the accuracy on several benchmark tasks. These improvements enable the deployment of complex models previously infeasible due to resource or latency constraints. HGQ is open-source and is used for developing next-generation trigger systems at the CERN ATLAS and CMS experiments for particle physics, enabling the use of advanced machine learning models for real-time data selection with sub-microsecond latency.

**Link**: [arxiv](https://arxiv.org/abs/2405.00645v3),  [pdf](https://arxiv.org/pdf/2405.00645v3)

**Tags**: cs.LG physics.ins-det 



### DEER: A Comprehensive and Reliable Benchmark for Deep-Research Expert Reports
**Authors**: Janghoon Han, Heegyu Kim, Changho Lee, Dahm Lee, Min Hyung Park, Hosung Song, Stanley Jungkyu Choi, Moontae Lee, Honglak Lee

**Updated**: 2025-12-19T16:46:20Z

**Summary**: As large language models (LLMs) advance, deep research systems can generate expert-level reports via multi-step reasoning and evidence-based synthesis, but evaluating such reports remains challenging. Existing benchmarks often lack systematic criteria for expert reporting, evaluations that rely heavily on LLM judges can fail to capture issues that require expert judgment, and source verification typically covers only a limited subset of explicitly cited statements rather than report-wide factual reliability. We introduce DEER, a benchmark for evaluating expert-level deep research reports. DEER comprises 50 report-writing tasks spanning 13 domains and an expert-grounded evaluation taxonomy (7 dimensions, 25 sub-dimension) operationalized into 130 fine-grained rubric items. DEER further provides task-specific expert guidance to help LLM judges assess expert-level report quality more consistently. Complementing rubric-based assessment, we propose a document-level fact-checking architecture that extracts and verifies all claims across the entire report, including both cited and uncited ones, and quantifies external-evidence quality. DEER correlates closely with human expert judgments and yields interpretable diagnostics of system strengths and weaknesses.

**Link**: [arxiv](https://arxiv.org/abs/2512.17776v1),  [pdf](https://arxiv.org/pdf/2512.17776v1)

**Tags**: cs.CL 



### ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics
**Authors**: Li S. Yifei, Allen Chang, Chaitanya Malaviya, Mark Yatskar

**Updated**: 2025-12-19T16:38:02Z

**Summary**: Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is abundant: survey articles consolidate knowledge spread across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Queries and rubrics are jointly derived from survey sections, where rubric items list query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. 31 Ph.D. annotators in 8 fields judge that 90% of queries reflect Ph.D. information needs and 87% of rubric items warrant emphasis of a sentence or longer. We leverage ResearchQA to evaluate 18 systems in 7.6K head-to-heads. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.

**Link**: [arxiv](https://arxiv.org/abs/2509.00496v2),  [pdf](https://arxiv.org/pdf/2509.00496v2)

**Tags**: cs.CL cs.AI 



### LLM-as-a-qualitative-judge: automating error analysis in natural language generation
**Authors**: Nadezhda Chirkova, Tunde Oluwaseyi Ajayi, Seth Aycock, Zain Muhammad Mujahid, Vladana Perlić, Ekaterina Borisova, Markarit Vartampetian

**Updated**: 2025-12-19T16:35:50Z

**Summary**: Prompting large language models (LLMs) to evaluate generated text, known as LLM-as-a-judge, has become a standard evaluation approach in natural language generation (NLG), but is primarily used as a quantitative tool, i.e. with numerical scores as main outputs. In this work, we propose LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main output being a structured report of common issue types in the NLG system outputs. Our approach is targeted at providing developers with meaningful insights on what improvements can be done to a given NLG system and consists of two main steps, namely open-ended per-instance issue analysis and clustering of the discovered issues using an intuitive cumulative algorithm. We also introduce a strategy for evaluating the proposed approach, coupled with ~300 annotations of issues in instances from 12 NLG datasets. Our results show that instance-specific issues output by LLM-as-a-qualitative-judge match those annotated by humans in 2/3 cases, and that LLM-as-a-qualitative-judge is capable of producing error type reports resembling the reports composed by human annotators. We also demonstrate in a case study how the use of LLM-as-a-qualitative-judge can substantially improve NLG systems performance. Our code and data are publicly available at https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

**Link**: [arxiv](https://arxiv.org/abs/2506.09147v4),  [pdf](https://arxiv.org/pdf/2506.09147v4)

**Tags**: cs.CL cs.AI 



### Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity
**Authors**: Hauke Licht

**Updated**: 2025-12-19T16:35:45Z

**Summary**: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze emotions, the emergence of multimodal generative Artificial Intelligence (AI) promises great advances. However, we lack evidence about the effectiveness of multimodal AI in analyzing emotions in political communication. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in the video-based analysis of emotional arousal, using two complementary datasets of human-labeled video recordings. It finds that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and exhibit little to no demographic bias. However, in recordings of real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in multimodal political analysis and contributes a suitable replicable framework.

**Link**: [arxiv](https://arxiv.org/abs/2512.10882v2),  [pdf](https://arxiv.org/pdf/2512.10882v2)

**Tags**: cs.CL 



### PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning
**Authors**: Yushi Feng, Junye Du, Yingying Hong, Qifan Wang, Lequan Yu

**Updated**: 2025-12-19T16:27:34Z

**Summary**: Existing tool-augmented agentic systems are limited in the real world by (i) black-box reasoning steps that undermine trust of decision-making and pose safety risks, (ii) poor multimodal integration, which is inherently critical for healthcare tasks, and (iii) rigid and computationally inefficient agentic pipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the first multimodal framework to address these challenges in the context of Chest X-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a multi-tool graph, yielding decision paths annotated with interpretable probabilities. Given the complex CXR reasoning task with multimodal medical data, PASS leverages its learned task-conditioned distribution over the agentic supernet. Thus, it adaptively selects the most suitable tool at each supernet layer, offering probability-annotated trajectories for post-hoc audits and directly enhancing medical AI safety. PASS also continuously compresses salient findings into an evolving personalized memory, while dynamically deciding whether to deepen its reasoning path or invoke an early exit for efficiency. To optimize a Pareto frontier balancing performance and cost, we design a novel three-stage training procedure, including expert knowledge warm-up, contrastive path-ranking, and cost-aware reinforcement learning. To facilitate rigorous evaluation, we introduce CAB-E, a comprehensive benchmark for multi-step, safety-critical, free-form CXR reasoning. Experiments across various benchmarks validate that PASS significantly outperforms strong baselines in multiple metrics (e.g., accuracy, LLM-Judge, semantic similarity, etc.) while balancing computational costs, pushing a new paradigm shift towards interpretable, adaptive, and multimodal medical agentic systems.

**Link**: [arxiv](https://arxiv.org/abs/2508.10501v4),  [pdf](https://arxiv.org/pdf/2508.10501v4)

**Tags**: cs.AI cs.LG 



### Clean Up the Mess: Addressing Data Pollution in Cryptocurrency Abuse Reporting Services
**Authors**: Gibran Gomez, Kevin van Liebergen, Davide Sanvito, Giuseppe Siracusano, Roberto Gonzalez, Juan Caballero

**Updated**: 2025-12-19T16:18:06Z

**Summary**: Cryptocurrency abuse reporting services are a valuable data source about abusive blockchain addresses, prevalent types of cryptocurrency abuse, and their financial impact on victims. However, they may suffer data pollution due to their crowd-sourced nature. This work analyzes the extent and impact of data pollution in cryptocurrency abuse reporting services and proposes a novel LLM-based defense to address the pollution. We collect 289K abuse reports submitted over 6 years to two popular services and use them to answer three research questions. RQ1 analyzes the extent and impact of pollution. We show that spam reports will eventually flood unchecked abuse reporting services, with BitcoinAbuse receiving 75% of spam before stopping operations. We build a public dataset of 19,443 abuse reports labeled with 19 popular abuse types and use it to reveal the inaccuracy of user-reported abuse types. We identified 91 (0.1%) benign addresses reported, responsible for 60% of all the received funds. RQ2 examines whether we can automate identifying valid reports and their classification into abuse types. We propose an unsupervised LLM-based classifier that achieves an F1 score of 0.95 when classifying reports, an F1 of 0.89 when classifying out-of-distribution data, and an F1 of 0.99 when identifying spam reports. Our unsupervised LLM-based classifier clearly outperforms two baselines: a supervised classifier and a naive usage of the LLM. Finally, RQ3 demonstrates the usefulness of our LLM-based classifier for quantifying the financial impact of different cryptocurrency abuse types. We show that victim-reported losses heavily underestimate cybercriminal revenue by estimating a 29 times higher revenue from deposit transactions. We identified that investment scams have the highest financial impact and that extortions have lower conversion rates but compensate for them with massive email campaigns.

**Link**: [arxiv](https://arxiv.org/abs/2410.21041v2),  [pdf](https://arxiv.org/pdf/2410.21041v2)

**Tags**: cs.CR cs.CL 



### Fun-ASR Technical Report
**Authors**: Keyu An, Yanni Chen, Zhigao Chen, Chong Deng, Zhihao Du, Changfeng Gao, Zhifu Gao, Bo Gong, Xiangang Li, Yabin Li, Ying Liu, Xiang Lv, Yunjie Ji, Yiheng Jiang, Bin Ma, Haoneng Luo, Chongjia Ni, Zexu Pan, Yiping Peng, Zhendong Peng, Peiyao Wang, Hao Wang, Haoxu Wang, Wen Wang, Wupeng Wang, Yuzhong Wu, Biao Tian, Zhentao Tan, Nan Yang, Bin Yuan, Jieping Ye, Jixing Yu, Qinglin Zhang, Kun Zou, Han Zhao, Shengkui Zhao, Jingren Zhou, Yanqiao Zhu

**Updated**: 2025-12-19T16:18:03Z

**Summary**: In recent years, automatic speech recognition (ASR) has witnessed transformative advancements driven by three complementary paradigms: data scaling, model size scaling, and deep integration with large language models (LLMs). However, LLMs are prone to hallucination, which can significantly degrade user experience in real-world ASR applications. In this paper, we present Fun-ASR, a large-scale, LLM-based ASR system that synergistically combines massive data, large model capacity, LLM integration, and reinforcement learning to achieve state-of-the-art performance across diverse and complex speech recognition scenarios. Moreover, Fun-ASR is specifically optimized for practical deployment, with enhancements in streaming capability, noise robustness, code-switching, hotword customization, and satisfying other real-world application requirements. Experimental results show that while most LLM-based ASR systems achieve strong performance on open-source benchmarks, they often underperform on real industry evaluation sets. Thanks to production-oriented optimizations, Fun-ASR achieves state-of-the-art performance on real application datasets, demonstrating its effectiveness and robustness in practical settings. The code and models are accessible at https://github.com/FunAudioLLM/Fun-ASR .

**Link**: [arxiv](https://arxiv.org/abs/2509.12508v4),  [pdf](https://arxiv.org/pdf/2509.12508v4)

**Tags**: cs.CL cs.AI cs.SD eess.AS 



### When the Gold Standard isn't Necessarily Standard: Challenges of Evaluating the Translation of User-Generated Content
**Authors**: Lydia Nishimwe, Benoît Sagot, Rachel Bawden

**Updated**: 2025-12-19T16:17:23Z

**Summary**: User-generated content (UGC) is characterised by frequent use of non-standard language, from spelling errors to expressive choices such as slang, character repetitions, and emojis. This makes evaluating UGC translation particularly challenging: what counts as a "good" translation depends on the level of standardness desired in the output. To explore this, we examine the human translation guidelines of four UGC datasets, and derive a taxonomy of twelve non-standard phenomena and five translation actions (NORMALISE, COPY, TRANSFER, OMIT, CENSOR). Our analysis reveals notable differences in how UGC is treated, resulting in a spectrum of standardness in reference translations. Through a case study on large language models (LLMs), we show that translation scores are highly sensitive to prompts with explicit translation instructions for UGC, and that they improve when these align with the dataset's guidelines. We argue that when preserving UGC style is important, fair evaluation requires both models and metrics to be aware of translation guidelines. Finally, we call for clear guidelines during dataset creation and for the development of controllable, guideline-aware evaluation frameworks for UGC translation.

**Link**: [arxiv](https://arxiv.org/abs/2512.17738v1),  [pdf](https://arxiv.org/pdf/2512.17738v1)

**Tags**: cs.CL 



### STAGNet: A Spatio-Temporal Graph and LSTM Framework for Accident Anticipation
**Authors**: Vipooshan Vipulananthan, Kumudu Mohottala, Kavindu Chinthana, Nimsara Paramulla, Charith D Chitraranjan

**Updated**: 2025-12-19T16:05:16Z

**Summary**: Accident prediction and timely preventive actions improve road safety by reducing the risk of injury to road users and minimizing property damage. Hence, they are critical components of advanced driver assistance systems (ADAS) and autonomous vehicles. While many existing systems depend on multiple sensors such as LiDAR, radar, and GPS, relying solely on dash-cam videos presents a more challenging, yet more cost-effective and easily deployable solution. In this work, we incorporate improved spatio-temporal features and aggregate them through a recurrent network to enhance state-of-the-art graph neural networks for predicting accidents from dash-cam videos. Experiments using three publicly available datasets (DAD, DoTA and DADA) show that our proposed STAGNet model achieves higher average precision and mean time-to-accident scores than previous methods, both when cross-validated on a given dataset and when trained and tested on different datasets.

**Link**: [arxiv](https://arxiv.org/abs/2508.15216v3),  [pdf](https://arxiv.org/pdf/2508.15216v3)

**Tags**: cs.CV 



### Generating Completions for Broca's Aphasic Sentences Using Large Language Models
**Authors**: Sijbren van Vaals, Yevgen Matusevych, Frank Tsiwah

**Updated**: 2025-12-19T15:52:00Z

**Summary**: Broca's aphasia is a type of aphasia characterized by non-fluent, effortful and agrammatic speech production with relatively good comprehension. Since traditional aphasia treatment methods are often time-consuming, labour-intensive, and do not reflect real-world conversations, applying natural language processing based approaches such as Large Language Models (LLMs) could potentially contribute to improving existing treatment approaches. To address this issue, we explore the use of sequence-to-sequence LLMs for completing Broca's aphasic sentences. We first generate synthetic Broca's aphasic data using a rule-based system designed to mirror the linguistic characteristics of Broca's aphasic speech. Using this synthetic data (without authentic aphasic samples), we then fine-tune four pre-trained LLMs on the task of completing agrammatic sentences. We evaluate our fine-tuned models on both synthetic and authentic Broca's aphasic data. We demonstrate LLMs' capability for reconstructing agrammatic sentences, with the models showing improved performance with longer input utterances. Our result highlights the LLMs' potential in advancing communication aids for individuals with Broca's aphasia and possibly other clinical populations.

**Link**: [arxiv](https://arxiv.org/abs/2412.17669v2),  [pdf](https://arxiv.org/pdf/2412.17669v2)

**Tags**: cs.CL 



### Privacy Bias in Language Models: A Contextual Integrity-based Auditing Metric
**Authors**: Yan Shvartzshnaider, Vasisht Duddu

**Updated**: 2025-12-19T15:41:38Z

**Summary**: As large language models (LLMs) are integrated into sociotechnical systems, it is crucial to examine the privacy biases they exhibit. We define privacy bias as the appropriateness value of information flows in responses from LLMs. A deviation between privacy biases and expected values, referred to as privacy bias delta, may indicate privacy violations. As an auditing metric, privacy bias can help (a) model trainers evaluate the ethical and societal impact of LLMs, (b) service providers select context-appropriate LLMs, and (c) policymakers assess the appropriateness of privacy biases in deployed LLMs. We formulate and answer a novel research question: how can we reliably examine privacy biases in LLMs and the factors that influence them? We present a novel approach for assessing privacy biases using a contextual integrity-based methodology to evaluate the responses from various LLMs. Our approach accounts for the sensitivity of responses across prompt variations, which hinders the evaluation of privacy biases. Finally, we investigate how privacy biases are affected by model capacities and optimizations.

**Link**: [arxiv](https://arxiv.org/abs/2409.03735v3),  [pdf](https://arxiv.org/pdf/2409.03735v3)

**Tags**: cs.LG cs.AI cs.CR cs.CY 



### A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models
**Authors**: Sabrina Kaniewski, Fabian Schmidt, Markus Enzweiler, Michael Menth, Tobias Heer

**Updated**: 2025-12-19T15:41:06Z

**Summary**: The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 263 studies published between January 2020 and November 2025, categorizing them by task formulation, input representation, system architecture, and techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies at https://github.com/hs-esslingen-it-security/Awesome-LLM4SVD.

**Link**: [arxiv](https://arxiv.org/abs/2507.22659v2),  [pdf](https://arxiv.org/pdf/2507.22659v2)

**Tags**: cs.SE cs.AI 



### Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs
**Authors**: Shasha Zhou, Mingyu Huang, Jack Cole, Charles Britton, Ming Yin, Jan Wolber, Ke Li

**Updated**: 2025-12-19T15:20:17Z

**Summary**: The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.

**Link**: [arxiv](https://arxiv.org/abs/2511.12817v2),  [pdf](https://arxiv.org/pdf/2511.12817v2)

**Tags**: cs.LG 



### Toward Ethical AI Through Bayesian Uncertainty in Neural Question Answering
**Authors**: Riccardo Di Sipio

**Updated**: 2025-12-19T15:17:19Z

**Summary**: We explore Bayesian reasoning as a means to quantify uncertainty in neural networks for question answering. Starting with a multilayer perceptron on the Iris dataset, we show how posterior inference conveys confidence in predictions. We then extend this to language models, applying Bayesian inference first to a frozen head and finally to LoRA-adapted transformers, evaluated on the CommonsenseQA benchmark. Rather than aiming for state-of-the-art accuracy, we compare Laplace approximations against maximum a posteriori (MAP) estimates to highlight uncertainty calibration and selective prediction. This allows models to abstain when confidence is low. An ``I don't know'' response not only improves interpretability but also illustrates how Bayesian methods can contribute to more responsible and ethical deployment of neural question-answering systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.17677v1),  [pdf](https://arxiv.org/pdf/2512.17677v1)

**Tags**: cs.CL 



### Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs
**Authors**: Petr Novak, Stefan Biffl, Marek Obitko, Petr Kadera

**Updated**: 2025-12-19T15:15:08Z

**Summary**: Contemporary industrial cyber-physical production systems (CPPS) composed of robotic workcells face significant challenges in the analysis of undesired conditions due to the flexibility of Industry 4.0 that disrupts traditional quality assurance mechanisms. This paper presents a novel industry-oriented semantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG), which is designed to analyze and mitigate undesired conditions in flexible CPPS. Built on top of the well-proven Product-Process-Resource (PPR) model originating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses shortcomings of conventional model-driven engineering for CPPS, particularly inadequate undesired condition and error handling representation. The integration of semantic technologies with large language models (LLMs) provides intuitive interfaces for factory operators, production planners, and engineers to interact with the entire model using natural language. Evaluation with the use case addressing electric vehicle battery remanufacturing demonstrates that the PPR-AKG approach efficiently supports resource allocation based on explicitly represented capabilities as well as identification and mitigation of undesired conditions in production. The key contributions include (1) a holistic PPR-AKG model capturing multi-dimensional production knowledge, and (2) the useful combination of the PPR-AKG with LLM-based chatbots for human interaction.

**Link**: [arxiv](https://arxiv.org/abs/2508.06278v2),  [pdf](https://arxiv.org/pdf/2508.06278v2)

**Tags**: cs.RO 



### Vidarc: Embodied Video Diffusion Model for Closed-loop Control
**Authors**: Yao Feng, Chendong Xiang, Xinyi Mao, Hengkai Tan, Zuyue Zhang, Shuhe Huang, Kaiwen Zheng, Haitian Liu, Hang Su, Jun Zhu

**Updated**: 2025-12-19T15:04:24Z

**Summary**: Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.

**Link**: [arxiv](https://arxiv.org/abs/2512.17661v1),  [pdf](https://arxiv.org/pdf/2512.17661v1)

**Tags**: cs.RO cs.LG 



### UAV-Enabled ISAC: Towards On-Demand Sensing Services and Enhanced Communication
**Authors**: Xiaopeng Yuan, Peng Wu, Xinran Wang, Yulin Hu, Anke Schmeink

**Updated**: 2025-12-19T14:56:18Z

**Summary**: In this paper, we investigate an integrated sensing-and-communication (ISAC) network enabled by an unmanned aerial vehicle (UAV). The UAV is supposed to fly along a periodical circular trajectory at a fixed height for ISAC service supply from the sky. We consider on-demand sensing services, where on-demand detection and on-demand localization requests may be activated at any time toward any position within the targeted serving region. While guaranteeing satisfactory accuracy for both on-demand sensing tasks, we aim at maximizing the minimum achievable throughput among all communication users, via joint optimizing the UAV trajectory and communication user scheduling. To address the complicated problem with infinite sensing constraints, we characterize the on-demand detection constraint as a restricted deployment area for UAV and the on-demand localization constraint as Cramer-Rao Bound (CRB) constraints over finite reference target points, based on which the original problem is simplified to more tractable one. Afterwards, particularly aiming to ensure no violations of CRB constraints, we propose a convex approximation for the reformulated problem, where tight approximation is guaranteed at given local solution. The construction strategy for convex problem approximation allows an efficient iterative algorithm with verified convergence to a superior suboptimal solution. At last, with simulations, we verified the applicability of our developed optimization scheme in strictly fulfilling the on-demand sensing constraints and the effectiveness of our proposed solution for simultaneously enhancing the communication throughput in UAV-enabled ISAC.

**Link**: [arxiv](https://arxiv.org/abs/2512.17656v1),  [pdf](https://arxiv.org/pdf/2512.17656v1)

**Tags**: eess.SY 



### Designing an LLM-Based Behavioral Activation Chatbot for Young People with Depression: Insights from an Evaluation with Artificial Users and Clinical Experts
**Authors**: Florian Onur Kuhlmeier, Leon Hanschmann, Melina Rabe, Stefan Luettke, Eva-Lotta Brakemeier, Alexander Maedche

**Updated**: 2025-12-19T14:50:12Z

**Summary**: LLMs promise to overcome limitations of rule-based mental health chatbots through improved natural language capabilities, yet their ability to deliver evidence-based psychological interventions remains largely unverified because evaluations rarely apply the validated fidelity measures used to assess psychotherapists. We developed an LLM-based chatbot that delivers behavioral activation for depression and generated 48 complete chat sessions with diverse artificial users. Ten psychotherapists assessed these sessions using the Quality of Behavioral Activation Scale (Q-BAS), a validated fidelity instrument. Results show that the chatbot reliably executed the intervention across all phases and maintained safety protocols, but it struggled with clinical judgment, particularly when verifying the feasibility of proposed activities. Overall, our findings suggest that LLM-based chatbots can execute therapeutic protocols with high fidelity, while robust clinical reasoning remains an open challenge. We outline design implications to address this gap and provide the chatbot and artificial user prompts.

**Link**: [arxiv](https://arxiv.org/abs/2503.21540v3),  [pdf](https://arxiv.org/pdf/2503.21540v3)

**Tags**: cs.HC 



### Generative Human-Object Interaction Detection via Differentiable Cognitive Steering of Multi-modal LLMs
**Authors**: Zhaolin Cai, Huiyu Duan, Zitong Xu, Fan Li, Zhi Liu, Jing Liu, Wei Shen, Xiongkuo Min, Guangtao Zhai

**Updated**: 2025-12-19T14:41:50Z

**Summary**: Human-object interaction (HOI) detection aims to localize human-object pairs and the interactions between them. Existing methods operate under a closed-world assumption, treating the task as a classification problem over a small, predefined verb set, which struggles to generalize to the long-tail of unseen or ambiguous interactions in the wild. While recent multi-modal large language models (MLLMs) possess the rich world knowledge required for open-vocabulary understanding, they remain decoupled from existing HOI detectors since fine-tuning them is computationally prohibitive. To address these constraints, we propose \GRASP-HO}, a novel Generative Reasoning And Steerable Perception framework that reformulates HOI detection from the closed-set classification task to the open-vocabulary generation problem. To bridge the vision and cognitive, we first extract hybrid interaction representations, then design a lightweight learnable cognitive steering conduit (CSC) module to inject the fine-grained visual evidence into a frozen MLLM for effective reasoning. To address the supervision mismatch between classification-based HOI datasets and open-vocabulary generative models, we introduce a hybrid guidance strategy that coupling the language modeling loss and auxiliary classification loss, enabling discriminative grounding without sacrificing generative flexibility. Experiments demonstrate state-of-the-art closed-set performance and strong zero-shot generalization, achieving a unified paradigm that seamlessly bridges discriminative perception and generative reasoning for open-world HOI detection.

**Link**: [arxiv](https://arxiv.org/abs/2512.17640v1),  [pdf](https://arxiv.org/pdf/2512.17640v1)

**Tags**: cs.CV 



### Linear Personality Probing and Steering in LLMs: A Big Five Study
**Authors**: Michel Frising, Daniel Balcells

**Updated**: 2025-12-19T14:41:09Z

**Summary**: Large language models (LLMs) exhibit distinct and consistent personalities that greatly impact trust and engagement. While this means that personality frameworks would be highly valuable tools to characterize and control LLMs' behavior, current approaches remain either costly (post-training) or brittle (prompt engineering). Probing and steering via linear directions has recently emerged as a cheap and efficient alternative. In this paper, we investigate whether linear directions aligned with the Big Five personality traits can be used for probing and steering model behavior. Using Llama 3.3 70B, we generate descriptions of 406 fictional characters and their Big Five trait scores. We then prompt the model with these descriptions and questions from the Alpaca questionnaire, allowing us to sample hidden activations that vary along personality traits in known, quantifiable ways. Using linear regression, we learn a set of per-layer directions in activation space, and test their effectiveness for probing and steering model behavior. Our results suggest that linear directions aligned with trait-scores are effective probes for personality detection, while their steering capabilities strongly depend on context, producing reliable effects in forced-choice tasks but limited influence in open-ended generation or when additional context is present in the prompt.

**Link**: [arxiv](https://arxiv.org/abs/2512.17639v1),  [pdf](https://arxiv.org/pdf/2512.17639v1)

**Tags**: cs.CL 



### Trust-Region Adaptive Policy Optimization
**Authors**: Mingyu Su, Jian Guan, Yuxian Gu, Minlie Huang, Hongning Wang

**Updated**: 2025-12-19T14:37:07Z

**Summary**: Post-training methods, especially Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), play an important role in improving large language models' (LLMs) complex reasoning abilities. However, the dominant two-stage pipeline (SFT then RL) suffers from a key inconsistency: SFT enforces rigid imitation that suppresses exploration and induces forgetting, limiting RL's potential for improvements. We address this inefficiency with TRAPO (\textbf{T}rust-\textbf{R}egion \textbf{A}daptive \textbf{P}olicy \textbf{O}ptimization), a hybrid framework that interleaves SFT and RL within each training instance by optimizing SFT loss on expert prefixes and RL loss on the model's own completions, unifying external supervision and self-exploration. To stabilize training, we introduce Trust-Region SFT (TrSFT), which minimizes forward KL divergence inside a trust region but attenuates optimization outside, effectively shifting toward reverse KL and yielding stable, mode-seeking updates favorable for RL. An adaptive prefix-selection mechanism further allocates expert guidance based on measured utility. Experiments on five mathematical reasoning benchmarks show that TRAPO consistently surpasses standard SFT, RL, and SFT-then-RL pipelines, as well as recent state-of-the-art approaches, establishing a strong new paradigm for reasoning-enhanced LLMs.

**Link**: [arxiv](https://arxiv.org/abs/2512.17636v1),  [pdf](https://arxiv.org/pdf/2512.17636v1)

**Tags**: cs.LG cs.AI 



### Confidence-Credibility Aware Weighted Ensembles of Small LLMs Outperform Large LLMs in Emotion Detection
**Authors**: Menna Elgabry, Ali Hamdi

**Updated**: 2025-12-19T14:33:14Z

**Summary**: This paper introduces a confidence-weighted, credibility-aware ensemble framework for text-based emotion detection, inspired by Condorcet's Jury Theorem (CJT). Unlike conventional ensembles that often rely on homogeneous architectures, our approach combines architecturally diverse small transformer-based large language models (sLLMs) - BERT, RoBERTa, DistilBERT, DeBERTa, and ELECTRA, each fully fine-tuned for emotion classification. To preserve error diversity, we minimize parameter convergence while taking advantage of the unique biases of each model. A dual-weighted voting mechanism integrates both global credibility (validation F1 score) and local confidence (instance-level probability) to dynamically weight model contributions. Experiments on the DAIR-AI dataset demonstrate that our credibility-confidence ensemble achieves a macro F1 score of 93.5 percent, surpassing state-of-the-art benchmarks and significantly outperforming large-scale LLMs, including Falcon, Mistral, Qwen, and Phi, even after task-specific Low-Rank Adaptation (LoRA). With only 595M parameters in total, our small LLMs ensemble proves more parameter-efficient and robust than models up to 7B parameters, establishing that carefully designed ensembles of small, fine-tuned models can outperform much larger LLMs in specialized natural language processing (NLP) tasks such as emotion detection.

**Link**: [arxiv](https://arxiv.org/abs/2512.17630v1),  [pdf](https://arxiv.org/pdf/2512.17630v1)

**Tags**: cs.CL cs.LG 



### PathFLIP: Fine-grained Language-Image Pretraining for Versatile Computational Pathology
**Authors**: Fengchun Liu, Songhan Jiang, Linghan Cai, Ziyue Wang, Yongbing Zhang

**Updated**: 2025-12-19T14:26:50Z

**Summary**: While Vision-Language Models (VLMs) have achieved notable progress in computational pathology (CPath), the gigapixel scale and spatial heterogeneity of Whole Slide Images (WSIs) continue to pose challenges for multimodal understanding. Existing alignment methods struggle to capture fine-grained correspondences between textual descriptions and visual cues across thousands of patches from a slide, compromising their performance on downstream tasks. In this paper, we propose PathFLIP (Pathology Fine-grained Language-Image Pretraining), a novel framework for holistic WSI interpretation. PathFLIP decomposes slide-level captions into region-level subcaptions and generates text-conditioned region embeddings to facilitate precise visual-language grounding. By harnessing Large Language Models (LLMs), PathFLIP can seamlessly follow diverse clinical instructions and adapt to varied diagnostic contexts. Furthermore, it exhibits versatile capabilities across multiple paradigms, efficiently handling slide-level classification and retrieval, fine-grained lesion localization, and instruction following. Extensive experiments demonstrate that PathFLIP outperforms existing large-scale pathological VLMs on four representative benchmarks while requiring significantly less training data, paving the way for fine-grained, instruction-aware WSI interpretation in clinical practice.

**Link**: [arxiv](https://arxiv.org/abs/2512.17621v1),  [pdf](https://arxiv.org/pdf/2512.17621v1)

**Tags**: cs.CV 



### Evaluation of Nuclear Microreactor Cost-competitiveness in Current Electricity Markets Considering Reactor Cost Uncertainties
**Authors**: Muhammad R. Abdussami, Ikhwan Khaleb, Fei Gao, Aditi Verma

**Updated**: 2025-12-19T14:21:46Z

**Summary**: This paper evaluates the cost competitiveness of microreactors in today's electricity markets, with a focus on uncertainties in reactor costs. A Genetic Algorithm (GA) is used to optimize key technical parameters, such as reactor capacity, fuel enrichment, tail enrichment, refueling interval, and discharge burnup, to minimize the Levelized Cost of Energy (LCOE). Base case results are validated using Simulated Annealing (SA). By incorporating Probability Distribution Functions (PDFs) for fuel cycle costs, the study identifies optimal configurations under uncertainty. Methodologically, it introduces a novel framework combining probabilistic cost modeling with evolutionary optimization. Results show that microreactors can remain cost-competitive, with LCOEs ranging from \$48.21/MWh to \$78.32/MWh when supported by the Production Tax Credit (PTC). High reactor capacity, low fuel enrichment, moderate tail enrichment and refueling intervals, and high discharge burnup enhance cost efficiency. Among all factors, overnight capital cost (OCC) has the most significant impact on LCOE, while O&M and fuel cost uncertainties have lesser effects. The analysis highlights how energy policies like the PTC can reduce LCOE by 22-24%, improving viability despite cost variability. Compared to conventional nuclear, coal, and renewable sources like offshore wind, hydro, and biomass, optimized microreactors show strong economic potential. This research defines a realistic design space and key trade-offs, offering actionable insights for policymakers, reactor designers, and energy planners aiming to accelerate the deployment of affordable, sustainable microreactors.

**Link**: [arxiv](https://arxiv.org/abs/2506.13361v2),  [pdf](https://arxiv.org/pdf/2506.13361v2)

**Tags**: cs.NE physics.soc-ph 



### Emoji Reactions on Telegram: Unreliable Indicators of Emotional Resonance
**Authors**: Serena Tardelli, Lorenzo Alvisi, Lorenzo Cima, Stefano Cresci, Maurizio Tesconi

**Updated**: 2025-12-19T14:21:33Z

**Summary**: Emoji reactions are a frequently used feature of messaging platforms, yet their communicative role remains understudied. Prior work on emojis has focused predominantly on in-text usage, showing that emojis embedded in messages tend to amplify and mirror the author's affective tone. This evidence has often been extended to emoji reactions, treating them as indicators of emotional resonance or user sentiment. However, they may reflect broader social dynamics. Here, we investigate the communicative function of emoji reactions on Telegram. We analyze over 650k crypto-related messages that received at least one reaction, annotating each with sentiment, emotion, persuasion strategy, and speech act labels, and inferring the sentiment and emotion of emoji reactions using both lexicons and LLMs. We uncover a systematic mismatch between message and reaction sentiment, with positive reactions dominating even for neutral or negative content. This pattern persists across rhetorical strategies and emotional tones, indicating that emojis used as reactions do not reliably function as indicators of emotional mirroring or resonance of the content, in contrast to findings reported for in-text emojis. Finally, we identify the features that most predict emoji engagement. Overall, our findings caution against treating emoji reactions as sentiment labels, highlighting the need for more nuanced approaches in sentiment and engagement analysis.

**Link**: [arxiv](https://arxiv.org/abs/2508.06349v2),  [pdf](https://arxiv.org/pdf/2508.06349v2)

**Tags**: cs.HC cs.CY 



### Edge-Native Digitization of Handwritten Marksheets: A Hybrid Heuristic-Deep Learning Framework
**Authors**: Md. Irtiza Hossain, Junaid Ahmed Sifat, Abir Chowdhury

**Updated**: 2025-12-19T14:10:20Z

**Summary**: The digitization of structured handwritten documents, such as academic marksheets, remains a significant challenge due to the dual complexity of irregular table structures and diverse handwriting styles. While recent Transformer-based approaches like TableNet and TrOCR achieve state-of-the-art accuracy, their high computational cost renders them unsuitable for resource-constrained edge deployments. This paper introduces a resource-efficient hybrid framework that integrates a heuristic OpenCV-based pipeline for rapid table structure detection with a modified lightweight YOLOv8 architecture for handwritten character recognition. By strategically removing the SPPF and deep C2f layers from the standard YOLOv8 backbone, we reduce computational overhead while maintaining high recognition fidelity. Experimental results on the EMNIST digit benchmark demonstrate that our Modified YOLOv8 model achieves 97.5% accuracy. Furthermore, we provide a comprehensive efficiency analysis showing that our framework offers a 95 times inference speedup over standard OCR pipelines and massive efficiency gains over emerging Large Multimodal Models (LMMs) like Qwen2.5-VL, achieving real-time performance 29 FPS on standard CPU hardware. A qualitative and quantitative evaluation on the AMES dataset, a challenging subset of real-world marksheets, confirms the system's robustness in handling mixed alphanumeric content, bridging the gap between high-performance deep learning and practical, scalable document automation.

**Link**: [arxiv](https://arxiv.org/abs/2508.16295v2),  [pdf](https://arxiv.org/pdf/2508.16295v2)

**Tags**: cs.CV 



### Symbolic Pauli Propagation for Gradient-Enabled Pre-Training of Quantum Circuits
**Authors**: Saverio Monaco, Jamal Slim, Florian Rehm, Dirk Krücker, Kerstin Borras

**Updated**: 2025-12-19T13:56:51Z

**Summary**: Quantum Machine Learning models typically require expensive on-chip training procedures and often lack efficient gradient estimation methods. By employing Pauli propagation, it is possible to derive a symbolic representation of observables as analytic functions of a circuit's parameters. Although the number of terms in such functional representations grows rapidly with circuit depth, suitable choices of ansatz and controlled truncations on Pauli weights and frequency components yield accurate yet tractable estimators of the target observables. With the right ansatz design, this approach can be extended to system sizes beyond the reach of classical simulation, enabling scalable training for larger quantum systems. This also enables a form of classical pre-training through gradient-based optimization prior to deployment on quantum hardware. The proposed approach is demonstrated on the Variational Quantum Eigensolver for obtaining the ground state of a spin model, showing that accurate results can be achieved with a scalable and computationally efficient procedure.

**Link**: [arxiv](https://arxiv.org/abs/2512.16674v2),  [pdf](https://arxiv.org/pdf/2512.16674v2)

**Tags**: quant-ph 



### Enabling Disaggregated Multi-Stage MLLM Inference via GPU-Internal Scheduling and Resource Sharing
**Authors**: Lingxiao Zhao, Haoran Zhou, Yuezhi Che, Dazhao Cheng

**Updated**: 2025-12-19T13:40:13Z

**Summary**: Multimodal large language models (MLLMs) extend LLMs with visual understanding through a three-stage pipeline: multimodal preprocessing, vision encoding, and LLM inference. While these stages enhance capability, they introduce significant system bottlenecks. First, multimodal preprocessing-especially video decoding-often dominates Time-to-First-Token (TTFT). Most systems rely on CPU-based decoding, which severely limits throughput, while existing GPU-based approaches prioritize throughput-oriented parallelism and fail to meet the latency-sensitive requirements of MLLM inference. Second, the vision encoder is a standalone, compute-intensive stage that produces visual embeddings and cannot be co-batched with LLM prefill or decoding. This heterogeneity forces inter-stage blocking and increases token-generation latency. Even when deployed on separate GPUs, these stages underutilize available compute and memory resources, reducing overall utilization and constraining system throughput.   To address these challenges, we present FlashCodec and UnifiedServe, two complementary designs that jointly optimize the end-to-end MLLM pipeline. FlashCodec accelerates the multimodal preprocessing stage through collaborative multi-GPU video decoding, reducing decoding latency while preserving high throughput. UnifiedServe optimizes the vision-to-text and inference stages using a logically decoupled their execution to eliminate inter-stage blocking, yet physically sharing GPU resources to maximize GPU system utilization. By carefully orchestrating execution across stages and minimizing interference, UnifiedServe Together, our proposed framework forms an end-to-end optimized stack that can serve up to 3.0$\times$ more requests or enforce 1.5$\times$ tighter SLOs, while achieving up to 4.4$\times$ higher throughput compared to state-of-the-art systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.17574v1),  [pdf](https://arxiv.org/pdf/2512.17574v1)

**Tags**: cs.DC cs.LG 



### GreedySnake: Accelerating SSD-Offloaded LLM Training with Efficient Scheduling and Optimizer Step Overlapping
**Authors**: Yikang Yue, Yishu Yin, Xuehai Qian

**Updated**: 2025-12-19T13:36:31Z

**Summary**: SSD-offloaded training offers a practical and promising approach to making LLM training cost-effective. Building on gradient accumulation with micro-batches, this paper introduces GreedySnake, a new SSD-offloaded training system that employs vertical scheduling, which executes all microbatches of a layer before proceeding to the next. Compared to existing systems that use horizontal scheduling (i.e., executing micro-batches sequentially), GreedySnake achieves higher training throughput with smaller batch sizes, bringing the system much closer to the ideal scenario predicted by the roofline model. To further mitigate the I/O bottleneck, GreedySnake overlaps part of the optimization step with the forward pass of the next iteration. Experimental results on A100 GPUs show that GreedySnake achieves saturated training throughput improvements over ZeRO-Infinity: 1.96x on 1 GPU and 1.93x on 4 GPUs for GPT-65B, and 2.53x on 1 GPU for GPT-175B. The code is open-sourced at https://github.com/npz7yyk/GreedySnake

**Link**: [arxiv](https://arxiv.org/abs/2512.17570v1),  [pdf](https://arxiv.org/pdf/2512.17570v1)

**Tags**: cs.LG cs.AI cs.PF 



### A unified FLAIR hyperintensity segmentation model for various CNS tumor types and acquisition time points
**Authors**: Mathilde Gajda Faanes, David Bouget, Asgeir S. Jakola, Timothy R. Smith, Vasileios K. Kavouridis, Francesco Latini, Margret Jensdottir, Peter Milos, Henrietta Nittby Redebrandt, Rickard L. Sjöberg, Rupavathana Mahesparan, Lars Kjelsberg Pedersen, Ole Solheim, Ingerid Reinertsen

**Updated**: 2025-12-19T13:33:43Z

**Summary**: T2-weighted fluid-attenuated inversion recovery (FLAIR) magnetic resonance imaging (MRI) scans are important for diagnosis, treatment planning and monitoring of brain tumors. Depending on the brain tumor type, the FLAIR hyperintensity volume is an important measure to asses the tumor volume or surrounding edema, and an automatic segmentation of this would be useful in the clinic. In this study, around 5000 FLAIR images of various tumors types and acquisition time points from different centers were used to train a unified FLAIR hyperintensity segmentation model using an Attention U-Net architecture. The performance was compared against dataset specific models, and was validated on different tumor types, acquisition time points and against BraTS. The unified model achieved an average Dice score of 88.65\% for pre-operative meningiomas, 80.08% for pre-operative metastasis, 90.92% for pre-operative and 84.60% for post-operative gliomas from BraTS, and 84.47% for pre-operative and 61.27\% for post-operative lower grade gliomas. In addition, the results showed that the unified model achieved comparable segmentation performance to the dataset specific models on their respective datasets, and enables generalization across tumor types and acquisition time points, which facilitates the deployment in a clinical setting. The model is integrated into Raidionics, an open-source software for CNS tumor analysis.

**Link**: [arxiv](https://arxiv.org/abs/2512.17566v1),  [pdf](https://arxiv.org/pdf/2512.17566v1)

**Tags**: cs.CV cs.AI 



### Towards Explainable Conversational AI for Early Diagnosis with Large Language Models
**Authors**: Maliha Tabassum, M Shamim Kaiser

**Updated**: 2025-12-19T13:28:50Z

**Summary**: Healthcare systems around the world are grappling with issues like inefficient diagnostics, rising costs, and limited access to specialists. These problems often lead to delays in treatment and poor health outcomes. Most current AI and deep learning diagnostic systems are not very interactive or transparent, making them less effective in real-world, patient-centered environments. This research introduces a diagnostic chatbot powered by a Large Language Model (LLM), using GPT-4o, Retrieval-Augmented Generation, and explainable AI techniques. The chatbot engages patients in a dynamic conversation, helping to extract and normalize symptoms while prioritizing potential diagnoses through similarity matching and adaptive questioning. With Chain-of-Thought prompting, the system also offers more transparent reasoning behind its diagnoses. When tested against traditional machine learning models like Naive Bayes, Logistic Regression, SVM, Random Forest, and KNN, the LLM-based system delivered impressive results, achieving an accuracy of 90% and Top-3 accuracy of 100%. These findings offer a promising outlook for more transparent, interactive, and clinically relevant AI in healthcare.

**Link**: [arxiv](https://arxiv.org/abs/2512.17559v1),  [pdf](https://arxiv.org/pdf/2512.17559v1)

**Tags**: cs.AI 



### Deep Learning-based Robust Autonomous Navigation of Aerial Robots in Dense Forests
**Authors**: Guglielmo Del Col, Väinö Karjalainen, Teemu Hakala, Yibo Zhang, Eija Honkavaara

**Updated**: 2025-12-19T13:19:33Z

**Summary**: Autonomous aerial navigation in dense natural environments remains challenging due to limited visibility, thin and irregular obstacles, GNSS-denied operation, and frequent perceptual degradation. This work presents an improved deep learning-based navigation framework that integrates semantically enhanced depth encoding with neural motion-primitive evaluation for robust flight in cluttered forests. Several modules are incorporated on top of the original sevae-ORACLE algorithm to address limitations observed during real-world deployment, including lateral control for sharper maneuvering, a temporal consistency mechanism to suppress oscillatory planning decisions, a stereo-based visual-inertial odometry solution for drift-resilient state estimation, and a supervisory safety layer that filters unsafe actions in real time. A depth refinement stage is included to improve the representation of thin branches and reduce stereo noise, while GPU optimization increases onboard inference throughput from 4 Hz to 10 Hz.   The proposed approach is evaluated against several existing learning-based navigation methods under identical environmental conditions and hardware constraints. It demonstrates higher success rates, more stable trajectories, and improved collision avoidance, particularly in highly cluttered forest settings. The system is deployed on a custom quadrotor in three boreal forest environments, achieving fully autonomous completion in all flights in moderate and dense clutter, and 12 out of 15 flights in highly dense underbrush. These results demonstrate improved reliability and safety over existing navigation methods in complex natural environments.

**Link**: [arxiv](https://arxiv.org/abs/2512.17553v1),  [pdf](https://arxiv.org/pdf/2512.17553v1)

**Tags**: cs.RO 



### SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review
**Authors**: Kai Wang, Bingcheng Mao, Shuai Jia, Yujie Ding, Dongming Han, Tianyi Ma, Bin Cao

**Updated**: 2025-12-19T13:02:22Z

**Summary**: Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.

**Link**: [arxiv](https://arxiv.org/abs/2512.17540v1),  [pdf](https://arxiv.org/pdf/2512.17540v1)

**Tags**: cs.SE 



### Hierarchical Multimodal LLMs with Semantic Space Alignment for Enhanced Time Series Classification
**Authors**: Xiaoyu Tao, Tingyue Pan, Mingyue Cheng, Yucong Luo, Qi Liu, Enhong Chen

**Updated**: 2025-12-19T12:12:39Z

**Summary**: Time series classification plays a fundamental role in a wide range of real-world applications. Recently, large language models (LLMs) have demonstrated strong generalization and reasoning capacities, but directly applying them to time series classification remains non-trivial due to the representation gap between numerical sequences and linguistic semantics. In this paper, we propose HiTime, a hierarchical LLM-based framework for multimodal time series classification that bridges structured temporal representations with semantic reasoning in a generative paradigm. Specifically, we design a hierarchical sequence feature encoding module composed of a data-specific encoder and a task-specific encoder to extract complementary temporal features. To mitigate the embedding gap between time series representations and textual semantics, we further introduce a semantic space alignment module that jointly performs coarse-grained global modeling and fine-grained cross-modal correspondence. Building upon the above representations, we employ a parameter-efficient supervised fine-tuning strategy to activate the generative classification capability of the algined LLMs, thereby transforming conventional discriminative time series classification into a generative task. Extensive experiments on multiple benchmarks demonstrate that the proposed framework consistently outperforms state-of-the-art baselines. The code is publicly available at https://github.com/Xiaoyu-Tao/HiTime.

**Link**: [arxiv](https://arxiv.org/abs/2410.18686v2),  [pdf](https://arxiv.org/pdf/2410.18686v2)

**Tags**: cs.LG 



### GroundingME: Exposing the Visual Grounding Gap in MLLMs through Multi-Dimensional Evaluation
**Authors**: Rang Li, Lei Li, Shuhuai Ren, Hao Tian, Shuhao Gu, Shicheng Li, Zihao Yue, Yudong Wang, Wenhan Ma, Zhe Yang, Jingyuan Ma, Zhifang Sui, Fuli Luo

**Updated**: 2025-12-19T12:06:25Z

**Summary**: Visual grounding, localizing objects from natural language descriptions, represents a critical bridge between language and vision understanding. While multimodal large language models (MLLMs) achieve impressive scores on existing benchmarks, a fundamental question remains: can MLLMs truly ground language in vision with human-like sophistication, or are they merely pattern-matching on simplified datasets? Current benchmarks fail to capture real-world complexity where humans effortlessly navigate ambiguous references and recognize when grounding is impossible. To rigorously assess MLLMs' true capabilities, we introduce GroundingME, a benchmark that systematically challenges models across four critical dimensions: (1) Discriminative, distinguishing highly similar objects, (2) Spatial, understanding complex relational descriptions, (3) Limited, handling occlusions or tiny objects, and (4) Rejection, recognizing ungroundable queries. Through careful curation combining automated generation with human verification, we create 1,005 challenging examples mirroring real-world complexity. Evaluating 25 state-of-the-art MLLMs reveals a profound capability gap: the best model achieves only 45.1% accuracy, while most score 0% on rejection tasks, reflexively hallucinating objects rather than acknowledging their absence, raising critical safety concerns for deployment. We explore two strategies for improvements: (1) test-time scaling selects optimal response by thinking trajectory to improve complex grounding by up to 2.9%, and (2) data-mixture training teaches models to recognize ungroundable queries, boosting rejection accuracy from 0% to 27.9%. GroundingME thus serves as both a diagnostic tool revealing current limitations in MLLMs and a roadmap toward human-level visual grounding.

**Link**: [arxiv](https://arxiv.org/abs/2512.17495v1),  [pdf](https://arxiv.org/pdf/2512.17495v1)

**Tags**: cs.CV 



### Zero-Shot Recognition of Dysarthric Speech Using Commercial Automatic Speech Recognition and Multimodal Large Language Models
**Authors**: Ali Alsayegh, Tariq Masood

**Updated**: 2025-12-19T11:40:49Z

**Summary**: Voice-based human-machine interaction is a primary modality for accessing intelligent systems, yet individuals with dysarthria face systematic exclusion due to recognition performance gaps. Whilst automatic speech recognition (ASR) achieves word error rates (WER) below 5% on typical speech, performance degrades dramatically for dysarthric speakers. Multimodal large language models (MLLMs) offer potential for leveraging contextual reasoning to compensate for acoustic degradation, yet their zero-shot capabilities remain uncharacterised. This study evaluates eight commercial speech-to-text services on the TORGO dysarthric speech corpus: four conventional ASR systems (AssemblyAI, Whisper large-v3, Deepgram Nova-3, Nova-3 Medical) and four MLLM-based systems (GPT-4o, GPT-4o Mini, Gemini 2.5 Pro, Gemini 2.5 Flash). Evaluation encompasses lexical accuracy, semantic preservation, and cost-latency trade-offs. Results demonstrate severity-dependent degradation: mild dysarthria achieves 3-5% WER approaching typical-speech benchmarks, whilst severe dysarthria exceeds 49% WER across all systems. A verbatim-transcription prompt yields architecture-specific effects: GPT-4o achieves 7.36 percentage point WER reduction with consistent improvement across all tested speakers, whilst Gemini variants exhibit degradation. Semantic metrics indicate that communicative intent remains partially recoverable despite elevated lexical error rates. These findings establish empirical baselines enabling evidence-based technology selection for assistive voice interface deployment.

**Link**: [arxiv](https://arxiv.org/abs/2512.17474v1),  [pdf](https://arxiv.org/pdf/2512.17474v1)

**Tags**: eess.AS cs.SD 



### Empirical and Sustainability Aspects of Software Engineering Research in the Era of Large Language Models: A Reflection
**Authors**: David Williams, Max Hort, Maria Kechagia, Aldeida Aleti, Justyna Petke, Federica Sarro

**Updated**: 2025-12-19T11:30:54Z

**Summary**: Software Engineering (SE) research involving the use of Large Language Models (LLMs) has introduced several new challenges related to rigour in benchmarking, contamination, replicability, and sustainability. In this paper, we invite the research community to reflect on how these challenges are addressed in SE. Our results provide a structured overview of current LLM-based SE research at ICSE, highlighting both encouraging practices and persistent shortcomings. We conclude with recommendations to strengthen benchmarking rigour, improve replicability, and address the financial and environmental costs of LLM-based SE.

**Link**: [arxiv](https://arxiv.org/abs/2510.26538v2),  [pdf](https://arxiv.org/pdf/2510.26538v2)

**Tags**: cs.SE 



### Helmsman: Autonomous Synthesis of Federated Learning Systems via Collaborative LLM Agents
**Authors**: Haoyuan Li, Mathias Funk, Aaqib Saeed

**Updated**: 2025-12-19T11:27:02Z

**Summary**: Federated Learning (FL) offers a powerful paradigm for training models on decentralized data, but its promise is often undermined by the immense complexity of designing and deploying robust systems. The need to select, combine, and tune strategies for multifaceted challenges like data heterogeneity and system constraints has become a critical bottleneck, resulting in brittle, bespoke solutions. To address this, we introduce Helmsman, a novel multi-agent system that automates the end-to-end synthesis of federated learning systems from high-level user specifications. It emulates a principled research and development workflow through three collaborative phases: (1) interactive human-in-the-loop planning to formulate a sound research plan, (2) modular code generation by supervised agent teams, and (3) a closed-loop of autonomous evaluation and refinement in a sandboxed simulation environment. To facilitate rigorous evaluation, we also introduce AgentFL-Bench, a new benchmark comprising 16 diverse tasks designed to assess the system-level generation capabilities of agentic systems in FL. Extensive experiments demonstrate that our approach generates solutions competitive with, and often superior to, established hand-crafted baselines. Our work represents a significant step towards the automated engineering of complex decentralized AI systems.

**Link**: [arxiv](https://arxiv.org/abs/2510.14512v2),  [pdf](https://arxiv.org/pdf/2510.14512v2)

**Tags**: cs.AI 



### VLA-AN: An Efficient and Onboard Vision-Language-Action Framework for Aerial Navigation in Complex Environments
**Authors**: Yuze Wu, Mo Zhu, Xingxing Li, Yuheng Du, Yuxin Fan, Wenjun Li, Zhichao Han, Xin Zhou, Fei Gao

**Updated**: 2025-12-19T11:22:57Z

**Summary**: This paper proposes VLA-AN, an efficient and onboard Vision-Language-Action (VLA) framework dedicated to autonomous drone navigation in complex environments. VLA-AN addresses four major limitations of existing large aerial navigation models: the data domain gap, insufficient temporal navigation with reasoning, safety issues with generative action policies, and onboard deployment constraints. First, we construct a high-fidelity dataset utilizing 3D Gaussian Splatting (3D-GS) to effectively bridge the domain gap. Second, we introduce a progressive three-stage training framework that sequentially reinforces scene comprehension, core flight skills, and complex navigation capabilities. Third, we design a lightweight, real-time action module coupled with geometric safety correction. This module ensures fast, collision-free, and stable command generation, mitigating the safety risks inherent in stochastic generative policies. Finally, through deep optimization of the onboard deployment pipeline, VLA-AN achieves a robust real-time 8.3x improvement in inference throughput on resource-constrained UAVs. Extensive experiments demonstrate that VLA-AN significantly improves spatial grounding, scene reasoning, and long-horizon navigation, achieving a maximum single-task success rate of 98.1%, and providing an efficient, practical solution for realizing full-chain closed-loop autonomy in lightweight aerial robots.

**Link**: [arxiv](https://arxiv.org/abs/2512.15258v2),  [pdf](https://arxiv.org/pdf/2512.15258v2)

**Tags**: cs.RO cs.AI 



### xGR: Efficient Generative Recommendation Serving at Scale
**Authors**: Qingxiao Sun, Tongxuan Liu, Shen Zhang, Siyu Wu, Peijun Yang, Haotian Liang, Menxin Li, Xiaolong Ma, Zhiwei Liang, Ziyi Ren, Minchao Zhang, Xinyu Liu, Ke Zhang, Depei Qian, Hailong Yang

**Updated**: 2025-12-19T11:20:16Z

**Summary**: Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.

**Link**: [arxiv](https://arxiv.org/abs/2512.11529v2),  [pdf](https://arxiv.org/pdf/2512.11529v2)

**Tags**: cs.LG 



### An Investigation on How AI-Generated Responses Affect SoftwareEngineering Surveys
**Authors**: Ronnie de Souza Santos, Italo Santos, Maria Teresa Baldassarre, Cleyton Magalhaes, Mairieli Wessel

**Updated**: 2025-12-19T11:17:05Z

**Summary**: Survey research is a fundamental empirical method in software engineering, enabling the systematic collection of data on professional practices, perceptions, and experiences. However, recent advances in large language models (LLMs) have introduced new risks to survey integrity, as participants can use generative tools to fabricate or manipulate their responses. This study explores how LLMs are being misused in software engineering surveys and investigates the methodological implications of such behavior for data authenticity, validity, and research integrity. We collected data from two survey deployments conducted in 2025 through the Prolific platform and analyzed the content of participants' answers to identify irregular or falsified responses. A subset of responses suspected of being AI generated was examined through qualitative pattern inspection, narrative characterization, and automated detection using the Scribbr AI Detector. The analysis revealed recurring structural patterns in 49 survey responses indicating synthetic authorship, including repetitive sequencing, uniform phrasing, and superficial personalization. These false narratives mimicked coherent reasoning while concealing fabricated content, undermining construct, internal, and external validity. Our study identifies data authenticity as an emerging dimension of validity in software engineering surveys. We emphasize that reliable evidence now requires combining automated and interpretive verification procedures, transparent reporting, and community standards to detect and prevent AI generated responses, thereby protecting the credibility of surveys in software engineering.

**Link**: [arxiv](https://arxiv.org/abs/2512.17455v1),  [pdf](https://arxiv.org/pdf/2512.17455v1)

**Tags**: cs.SE 



### Learning What to Write: Write-Gated KV for Efficient Long-Context Inference
**Authors**: Yen-Chieh Huang, Rui Fang, Ming-Syan Chen, Pi-Cheng Hsiu

**Updated**: 2025-12-19T11:08:58Z

**Summary**: Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\times$ prefill and 1.89-2.56$\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .

**Link**: [arxiv](https://arxiv.org/abs/2512.17452v1),  [pdf](https://arxiv.org/pdf/2512.17452v1)

**Tags**: cs.LG cs.AI 



### When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals
**Authors**: Riad Ahmed Anonto, Md Labid Al Nahiyan, Md Tanvir Hassan

**Updated**: 2025-12-19T11:00:50Z

**Summary**: Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce "semantic confusion," a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.

**Link**: [arxiv](https://arxiv.org/abs/2512.01037v2),  [pdf](https://arxiv.org/pdf/2512.01037v2)

**Tags**: cs.CL cs.AI 



### Tailored Zn(1-x)TixAl2O4 Nanocomposite Particles via Sol-Gel Route for High-Performance Humidity Sensing
**Authors**: Ramalakshmi K, Sasmita Dash, Srilali Siragam

**Updated**: 2025-12-19T10:48:15Z

**Summary**: Humidity sensors play a vital role in industrial, healthcare, agricultural, and environmental applications; however, conventional sensors often suffer from issues like low sensitivity, slow response, and poor stability. This study investigates sol-gel synthesized Zn0.85Ti0.15Al2O4 nanocomposite ceramics for high-performance humidity sensing. X-ray Diffraction (XRD) analysis confirms a nanocrystalline structure, while the optical bandgap of 3.76 eV indicates the enhanced sensing potential. The sensor exhibits a significant decrease in resistance, from 500 MΩ (15% RH) to 90 MΩ(90% RH), with fast response (50 s) and recovery times (50 s). Low Hysteresis values (5.86% at 30% RH, 7.69% at 60% RH, and 4.28% at 90% RH) highlight high sensitivity, stability, and repeatability. These results indicate the potential of Zn0.85Ti0.15Al2O4as a promising material for next-generation resistive humidity sensors suitable for commercial and industrial deployment.

**Link**: [arxiv](https://arxiv.org/abs/2512.17439v1),  [pdf](https://arxiv.org/pdf/2512.17439v1)

**Tags**: physics.app-ph 



### Unified Acoustic Representations for Screening Neurological and Respiratory Pathologies from Voice
**Authors**: Ran Piao, Yuan Lu, Hareld Kemps, Tong Xia, Aaqib Saeed

**Updated**: 2025-12-19T10:45:33Z

**Summary**: Voice-based health assessment offers unprecedented opportunities for scalable, non-invasive disease screening, yet existing approaches typically focus on single conditions and fail to leverage the rich, multi-faceted information embedded in speech. We present MARVEL (Multi-task Acoustic Representations for Voice-based Health Analysis), a privacy-conscious multitask learning framework that simultaneously detects nine distinct neurological, respiratory, and voice disorders using only derived acoustic features, eliminating the need for raw audio transmission. Our dual-branch architecture employs specialized encoders with task-specific heads sharing a common acoustic backbone, enabling effective cross-condition knowledge transfer. Evaluated on the large-scale Bridge2AI-Voice v2.0 dataset, MARVEL achieves an overall AUROC of 0.78, with exceptional performance on neurological disorders (AUROC = 0.89), particularly for Alzheimer's disease/mild cognitive impairment (AUROC = 0.97). Our framework consistently outperforms single-modal baselines by 5-19% and surpasses state-of-the-art self-supervised models on 7 of 9 tasks, while correlation analysis reveals that the learned representations exhibit meaningful similarities with established acoustic features, indicating that the model's internal representations are consistent with clinically recognized acoustic patterns. By demonstrating that a single unified model can effectively screen for diverse conditions, this work establishes a foundation for deployable voice-based diagnostics in resource-constrained and remote healthcare settings.

**Link**: [arxiv](https://arxiv.org/abs/2508.20717v2),  [pdf](https://arxiv.org/pdf/2508.20717v2)

**Tags**: cs.SD cs.LG 



### Xiaomi MiMo-VL-Miloco Technical Report
**Authors**: Jiaze Li, Jingyang Chen, Yuxun Qu, Jianzhong Ju, Zhenbo Luo, Jian Luan, Shijie Xu, Zhenru Lin, Junyou Zhu, Boshen Xu, Wenhui Tan, Pei Fu

**Updated**: 2025-12-19T10:43:37Z

**Summary**: We open-source \textbf{MiMo-VL-Miloco-7B} and its quantized variant \textbf{MiMo-VL-Miloco-7B-GGUF}, a pair of home-centric vision-language models that achieve strong performance on both home-scenario understanding and general multimodal reasoning. Built on the MiMo-VL-7B backbone, MiMo-VL-Miloco-7B is specialized for smart-home environments, attaining leading F1 scores on gesture recognition and common home-scenario understanding, while also delivering consistent gains across video benchmarks such as Video-MME, Video-MMMU, and Charades-STA, as well as language understanding benchmarks including MMMU-Pro and MMLU-Pro. In our experiments, MiMo-VL-Miloco-7B outperforms strong closed-source and open-source baselines on home-scenario understanding and several multimodal reasoning benchmarks. To balance specialization and generality, we design a two-stage training pipeline that combines supervised fine-tuning with reinforcement learning based on Group Relative Policy Optimization, leveraging efficient multi-domain data. We further incorporate chain-of-thought supervision and token-budget-aware reasoning, enabling the model to learn knowledge in a data-efficient manner while also performing reasoning efficiently. Our analysis shows that targeted home-scenario training not only enhances activity and gesture understanding, but also improves text-only reasoning with only modest trade-offs on document-centric tasks. Model checkpoints, quantized GGUF weights, and our home-scenario evaluation toolkit are publicly available at \href{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco}{https://github.com/XiaoMi/xiaomi-mimo-vl-miloco} to support research and deployment in real-world smart-home applications.

**Link**: [arxiv](https://arxiv.org/abs/2512.17436v1),  [pdf](https://arxiv.org/pdf/2512.17436v1)

**Tags**: cs.CV 



### ImagineNav++: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination
**Authors**: Teng Wang, Xinxin Zhao, Wenzhe Cai, Changyin Sun

**Updated**: 2025-12-19T10:40:16Z

**Summary**: Visual navigation is a fundamental capability for autonomous home-assistance robots, enabling long-horizon tasks such as object search. While recent methods have leveraged Large Language Models (LLMs) to incorporate commonsense reasoning and improve exploration efficiency, their planning remains constrained by textual representations, which cannot adequately capture spatial occupancy or scene geometry--critical factors for navigation decisions. We explore whether Vision-Language Models (VLMs) can achieve mapless visual navigation using only onboard RGB/RGB-D streams, unlocking their potential for spatial perception and planning. We achieve this through an imagination-powered navigation framework, ImagineNav++, which imagines future observation images from candidate robot views and translates navigation planning into a simple best-view image selection problem for VLMs. First, a future-view imagination module distills human navigation preferences to generate semantically meaningful viewpoints with high exploration potential. These imagined views then serve as visual prompts for the VLM to identify the most informative viewpoint. To maintain spatial consistency, we develop a selective foveation memory mechanism, which hierarchically integrates keyframe observations via a sparse-to-dense framework, constructing a compact yet comprehensive memory for long-term spatial reasoning. This approach transforms goal-oriented navigation into a series of tractable point-goal navigation tasks. Extensive experiments on open-vocabulary object and instance navigation benchmarks show that ImagineNav++ achieves SOTA performance in mapless settings, even surpassing most map-based methods, highlighting the importance of scene imagination and memory in VLM-based spatial reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2512.17435v1),  [pdf](https://arxiv.org/pdf/2512.17435v1)

**Tags**: cs.RO 



### OptScale: Probabilistic Optimality for Inference-time Scaling
**Authors**: Youkang Wang, Jian Wang, Rubing Chen, Xiao-Yong Wei

**Updated**: 2025-12-19T10:17:53Z

**Summary**: Inference-time scaling has emerged as a powerful technique for enhancing the reasoning performance of Large Language Models (LLMs). However, existing approaches often rely on heuristic strategies for parallel sampling, lacking a principled foundation. To address this gap, we propose a probabilistic framework that formalizes the optimality of inference-time scaling under the assumption that parallel samples are independently and identically distributed (i.i.d.), and where the Best-of-$N$ selection strategy follows a probability distribution that can be estimated. Within this framework, we derive a theoretical lower bound on the required number of samples to achieve a target performance level, providing the first principled guidance for compute-efficient scaling. Leveraging this insight, we develop \textsc{OptScale}, a practical algorithm that dynamically determines the optimal number of sampled responses. \textsc{OptScale} employs a language model-based predictor to estimate probabilistic prior parameters, enabling the decision of the minimal number of samples needed that satisfy predefined performance thresholds and confidence levels. Extensive experiments on representative reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC) demonstrate that \textsc{OptScale} significantly reduces sampling overhead while remaining better or on par with state-of-the-art reasoning performance. Our work offers both a theoretical foundation and a practical solution for principled inference-time scaling, addressing a critical gap in the efficient deployment of LLMs for complex reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2506.22376v4),  [pdf](https://arxiv.org/pdf/2506.22376v4)

**Tags**: cs.LG cs.AI cs.CL 



### SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories
**Authors**: Lilin Wang, Lucas Ramalho, Alan Celestino, Phuc Anthony Pham, Yu Liu, Umang Kumar Sinha, Andres Portillo, Onassis Osunwa, Gabriel Maduekwe

**Updated**: 2025-12-19T10:16:51Z

**Summary**: Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.

**Link**: [arxiv](https://arxiv.org/abs/2512.17419v1),  [pdf](https://arxiv.org/pdf/2512.17419v1)

**Tags**: cs.SE cs.AI cs.CL cs.LG 



### CNFinBench: A Benchmark for Safety and Compliance of Large Language Models in Finance
**Authors**: Jinru Ding, Chao Ding, Wenrao Pang, Boyi Xiao, Zhiqiang Liu, Pengcheng Chen, Jiayuan Chen, Tiantian Yuan, Junming Guan, Yidong Jiang, Dawei Cheng, Jie Xu

**Updated**: 2025-12-19T10:12:50Z

**Summary**: Large language models (LLMs) are increasingly deployed across the financial sector for tasks like investment research and algorithmic trading. Their high-stakes nature demands rigorous evaluation of models' safety and regulatory alignment. However, there is a significant gap between evaluation capabilities and safety requirements. Current financial benchmarks mainly focus on textbook-style question answering and numerical problem-solving, failing to simulate the open-ended scenarios where safety risks typically manifest. To close these gaps, we introduce CNFinBench, a benchmark structured around a Capability-Compliance-Safety triad encompassing 15 subtasks. For Capability Q&As, we introduce a novel business-vertical taxonomy aligned with core financial domains like banking operations, which allows institutions to assess model readiness for deployment in operational scenarios. For Compliance and Risk Control Q&As, we embed regulatory requirements within realistic business scenarios to ensure models are evaluated under practical, scenario-driven conditions. For Safety Q&As, we uniquely incorporate structured bias and fairness auditing, a dimension overlooked by other holistic financial benchmarks, and introduce the first multi-turn adversarial dialogue task to systematically expose compliance decay under sustained, context-aware attacks. Accordingly, we propose the Harmful Instruction Compliance Score (HICS) to quantify models' consistency in resisting harmful instructions across multi-turn dialogues. Experiments on 21 models across all subtasks reveal a persistent gap between capability and compliance: models achieve an average score of 61.0 on capability tasks but drop to 34.2 on compliance and risk-control evaluations. In multi-turn adversarial dialogue tests, most LLMs attain only partial resistance, demonstrating that refusal alone is insufficient without cited, verifiable reasoning.

**Link**: [arxiv](https://arxiv.org/abs/2512.09506v2),  [pdf](https://arxiv.org/pdf/2512.09506v2)

**Tags**: cs.CE 



### RBCTest: Leveraging LLMs to Mine and Verify Oracles of API Response Bodies for RESTful API Testing
**Authors**: Hieu Huynh, Tri Le, Tu Nguyen, Viet Nguyen, Vu Nguyen, Tien N. Nguyen

**Updated**: 2025-12-19T10:00:40Z

**Summary**: In API testing, deriving logical constraints on API response bodies to be used as oracles is crucial for generating test cases and performing automated testing of RESTful APIs. However, existing approaches are restricted to dynamic analysis, in which oracles are extracted via the execution of APIs as part of the system under test. In this paper, we propose a complementary LLM-based static approach in which constraints for API response bodies are mined from API specifications. We leverage large language models (LLMs) to comprehend API specifications, mine constraints for response bodies, and generate test cases. To reduce LLM hallucination, we apply an Observation-Confirmation (OC) scheme that uses initial prompts to contextualize constraints, allowing subsequent prompts to more accurately confirm their presence. Our empirical results show that RBCTest with OC prompting achieves high precision in constraint mining, with averages ranging from 85.1% to 93.6%. It also performs well in generating test cases from mined constraints, with precision ranging from 86.4% to 91.7%. We further use test cases generated by RBCTest to detect 46 mismatches between API specifications and actual response data across 19 real-world APIs. Four of these mismatches were reported in developers' forums.

**Link**: [arxiv](https://arxiv.org/abs/2504.17287v3),  [pdf](https://arxiv.org/pdf/2504.17287v3)

**Tags**: cs.SE 



### Foundation for unbiased cross-validation of spatio-temporal models for species distribution modeling
**Authors**: Diana Koldasbayeva, Alexey Zaytsev

**Updated**: 2025-12-19T09:44:32Z

**Summary**: Evaluating the predictive performance of species distribution models (SDMs) under realistic deployment scenarios requires careful handling of spatial and temporal dependencies in the data. Cross-validation (CV) is the standard approach for model evaluation, but its design strongly influences the validity of performance estimates. When SDMs are intended for spatial or temporal transfer, random CV can lead to overoptimistic results due to spatial autocorrelation (SAC) among neighboring observations.   We benchmark four machine learning algorithms (GBM, XGBoost, LightGBM, Random Forest) on two real-world presence-absence datasets, a temperate plant and an anadromous fish, using multiple CV designs: random, spatial, spatio-temporal, environmental, and forward-chaining. Two training data usage strategies (LAST FOLD and RETRAIN) are evaluated, with hyperparameter tuning performed within each CV scheme. Model performance is assessed on independent out-of-time test sets using AUC, MAE, and correlation metrics.   Random CV overestimates AUC by up to 0.16 and produces MAE values up to 80 percent higher than spatially blocked alternatives. Blocking at the empirical SAC range substantially reduces this bias. Training strategy affects evaluation outcomes: LAST FOLD yields smaller validation-test discrepancies under strong SAC, while RETRAIN achieves higher test AUC when SAC is weaker. Boosted ensemble models consistently perform best under spatially structured CV designs. We recommend a robust SDM workflow based on SAC-aware blocking, blocked hyperparameter tuning, and external temporal validation to improve reliability under spatial and temporal shifts.

**Link**: [arxiv](https://arxiv.org/abs/2502.03480v2),  [pdf](https://arxiv.org/pdf/2502.03480v2)

**Tags**: stat.AP cs.LG 



### The Mental World of Large Language Models in Recommendation: A Benchmark on Association, Personalization, and Knowledgeability
**Authors**: Guangneng Hu

**Updated**: 2025-12-19T09:44:19Z

**Summary**: Large language models (LLMs) have shown potential in recommendation systems (RecSys) by using them as either knowledge enhancer or zero-shot ranker. A key challenge lies in the large semantic gap between LLMs and RecSys where the former internalizes language world knowledge while the latter captures personalized world of behaviors. Unfortunately, the research community lacks a comprehensive benchmark that evaluates the LLMs over their limitations and boundaries in RecSys so that we can draw a confident conclusion. To investigate this, we propose a benchmark named LRWorld containing over 38K high-quality samples and 23M tokens carefully compiled and generated from widely used public recommendation datasets. LRWorld categorizes the mental world of LLMs in RecSys as three main scales (association, personalization, and knowledgeability) spanned by ten factors with 31 measures (tasks). Based on LRWorld, comprehensive experiments on dozens of LLMs show that they are still not well capturing the deep neural personalized embeddings but can achieve good results on shallow memory-based item-item similarity. They are also good at perceiving item entity relations, entity hierarchical taxonomies, and item-item association rules when inferring user interests. Furthermore, LLMs show a promising ability in multimodal knowledge reasoning (movie poster and product image) and robustness to noisy profiles. None of them show consistently good performance over the ten factors. Model sizes, position bias, and more are ablated.

**Link**: [arxiv](https://arxiv.org/abs/2512.17389v1),  [pdf](https://arxiv.org/pdf/2512.17389v1)

**Tags**: cs.IR 



### CIFE: Code Instruction-Following Evaluation
**Authors**: Sravani Gunnu, Shanmukha Guttula, Hima Patel

**Updated**: 2025-12-19T09:43:20Z

**Summary**: Large Language Models (LLMs) are increasingly applied to real-world code generation, where functional correctness alone is insufficient for reliable deployment, developers also expect adherence to explicit requirements for robustness, formatting, and security. Existing benchmarks primarily assess correctness through test-case execution, offering limited insight into how reliably models follow such constraints. We introduce a benchmark of 1,000 Python tasks, each paired with an average of 7 developer-specified constraints spanning 13 categories. Constraints are curated through a four-stage human-LLM pipeline to ensure they are atomic, relevant, and objective. We evaluate 14 open- and closed-source models using complementary adherence metrics and propose the C2A Score, a composite measure that jointly captures correctness and constraint compliance. Results reveal a substantial gap between partial and strict satisfaction, while strong models achieve over 90% partial adherence, strict adherence remains between 39-66%. These findings highlight that trustworthy code generation requires not only correctness but also consistent adherence to developer intent.

**Link**: [arxiv](https://arxiv.org/abs/2512.17387v1),  [pdf](https://arxiv.org/pdf/2512.17387v1)

**Tags**: cs.SE cs.CL 



### UCoder: Unsupervised Code Generation by Internal Probing of Large Language Models
**Authors**: Jiajun Wu, Jian Yang, Wei Zhang, Lin Jing, Yuqing Ma, Ensheng Shi, Yuchi Ma, Zhoujun Li, Xianglong Liu

**Updated**: 2025-12-19T09:42:04Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in code generation tasks. However, their effectiveness heavily relies on supervised training with extensive labeled (e.g., question-answering pairs) or unlabeled datasets (e.g., code snippets), which are often expensive and difficult to obtain at scale. To address this limitation, this paper introduces a method IPC, an unsupervised framework that leverages Internal Probing of LLMs for Code generation without any external corpus, even unlabeled code snippets. We introduce the problem space probing, test understanding probing, solution space probing, and knowledge consolidation and reinforcement to probe the internal knowledge and confidence patterns existing in LLMs. Further, IPC identifies reliable code candidates through self-consistency mechanisms and representation-based quality estimation to train UCoder (coder with unsupervised learning). We validate the proposed approach across multiple code benchmarks, demonstrating that unsupervised methods can achieve competitive performance compared to supervised approaches while significantly reducing the dependency on labeled data and computational resources. Analytic experiments reveal that internal model states contain rich signals about code quality and correctness, and that properly harnessing these signals enables effective unsupervised learning for code generation tasks, opening new directions for training code LLMs in resource-constrained scenarios.

**Link**: [arxiv](https://arxiv.org/abs/2512.17385v1),  [pdf](https://arxiv.org/pdf/2512.17385v1)

**Tags**: cs.CL 



### An Anatomy of Vision-Language-Action Models: From Modules to Milestones and Challenges
**Authors**: Chao Xu, Suyu Zhang, Yang Liu, Baigui Sun, Weihong Chen, Bo Xu, Qi Liu, Juncheng Wang, Shujun Wang, Shan Luo, Jan Peters, Athanasios V. Vasilakos, Stefanos Zafeiriou, Jiankang Deng

**Updated**: 2025-12-19T09:38:11Z

**Summary**: Vision-Language-Action (VLA) models are driving a revolution in robotics, enabling machines to understand instructions and interact with the physical world. This field is exploding with new models and datasets, making it both exciting and challenging to keep pace with. This survey offers a clear and structured guide to the VLA landscape. We design it to follow the natural learning path of a researcher: we start with the basic Modules of any VLA model, trace the history through key Milestones, and then dive deep into the core Challenges that define recent research frontier. Our main contribution is a detailed breakdown of the five biggest challenges in: (1) Representation, (2) Execution, (3) Generalization, (4) Safety, and (5) Dataset and Evaluation. This structure mirrors the developmental roadmap of a generalist agent: establishing the fundamental perception-action loop, scaling capabilities across diverse embodiments and environments, and finally ensuring trustworthy deployment-all supported by the essential data infrastructure. For each of them, we review existing approaches and highlight future opportunities. We position this paper as both a foundational guide for newcomers and a strategic roadmap for experienced researchers, with the dual aim of accelerating learning and inspiring new ideas in embodied intelligence. A live version of this survey, with continuous updates, is maintained on our \href{https://suyuz1.github.io/VLA-Survey-Anatomy/}{project page}.

**Link**: [arxiv](https://arxiv.org/abs/2512.11362v3),  [pdf](https://arxiv.org/pdf/2512.11362v3)

**Tags**: cs.RO 



### STDiff: A State Transition Diffusion Framework for Time Series Imputation in Industrial Systems
**Authors**: Gary Simethy, Daniel Ortiz-Arroyo, Petar Durdevic

**Updated**: 2025-12-19T09:33:52Z

**Summary**: Incomplete sensor data is a major obstacle in industrial time-series analytics. In wastewater treatment plants (WWTPs), key sensors show long, irregular gaps caused by fouling, maintenance, and outages. We introduce STDiff and STDiff-W, diffusion-based imputers that cast gap filling as state-space simulation under partial observability, where targets, controls, and exogenous signals may all be intermittently missing. STDiff learns a one-step transition model conditioned on observed values and masks, while STDiff-W extends this with a context encoder that jointly inpaints contiguous blocks, combining long-range consistency with short-term detail. On two WWTP datasets (one with synthetic block gaps from Agtrup and another with natural outages from Avedøre), STDiff-W achieves state-of-the-art accuracy compared with strong neural baselines such as SAITS, BRITS, and CSDI. Beyond point-error metrics, its reconstructions preserve realistic dynamics including oscillations, spikes, and regime shifts, and they achieve top or tied-top downstream one-step forecasting performance compared with strong neural baselines, indicating that preserving dynamics does not come at the expense of predictive utility. Ablation studies that drop, shuffle, or add noise to control or exogenous inputs consistently degrade NH4 and PO4 performance, with the largest deterioration observed when exogenous signals are removed, showing that the model captures meaningful dependencies. We conclude with practical guidance for deployment: evaluate performance beyond MAE using task-oriented and visual checks, include exogenous drivers, and balance computational cost against robustness to structured outages.

**Link**: [arxiv](https://arxiv.org/abs/2508.19011v3),  [pdf](https://arxiv.org/pdf/2508.19011v3)

**Tags**: cs.LG cs.AI 



### AdvJudge-Zero: Binary Decision Flips in LLM-as-a-Judge via Adversarial Control Tokens
**Authors**: Tung-Ling Li, Yuhao Wu, Hongliang Liu

**Updated**: 2025-12-19T09:22:11Z

**Summary**: Reward models and LLM-as-a-Judge systems are central to modern post-training pipelines such as RLHF, DPO, and RLAIF, where they provide scalar feedback and binary decisions that guide model selection and RL-based fine-tuning. We show that these judge systems exhibit a recurring vulnerability: short sequences of low-perplexity control tokens can flip many binary evaluations from correct ``No'' judgments to incorrect ``Yes'' judgments by steering the last-layer logit gap. These control tokens are patterns that a policy model could plausibly generate during post-training, and thus represent realistic reward-hacking risks rather than worst-case adversarial strings. Our method, AdvJudge-Zero, uses the model's next-token distribution and beam-search exploration to discover diverse control-token sequences from scratch, and our analysis shows that the induced hidden-state perturbations concentrate in a low-rank ``soft mode'' that is anti-aligned with the judge's refusal direction. Empirically, these tokens cause very high false positive rates when large open-weight and specialized judge models score incorrect answers on math and reasoning benchmarks. Finally, we show that LoRA-based adversarial training on small sets of control-token-augmented examples can markedly reduce these false positives while preserving evaluation quality.

**Link**: [arxiv](https://arxiv.org/abs/2512.17375v1),  [pdf](https://arxiv.org/pdf/2512.17375v1)

**Tags**: cs.LG cs.CL cs.CR 



### TakeAD: Preference-based Post-optimization for End-to-end Autonomous Driving with Expert Takeover Data
**Authors**: Deqing Liu, Yinfeng Gao, Deheng Qian, Qichao Zhang, Xiaoqing Ye, Junyu Han, Yupeng Zheng, Xueyi Liu, Zhongpu Xia, Dawei Ding, Yifeng Pan, Dongbin Zhao

**Updated**: 2025-12-19T09:12:44Z

**Summary**: Existing end-to-end autonomous driving methods typically rely on imitation learning (IL) but face a key challenge: the misalignment between open-loop training and closed-loop deployment. This misalignment often triggers driver-initiated takeovers and system disengagements during closed-loop execution. How to leverage those expert takeover data from disengagement scenarios and effectively expand the IL policy's capability presents a valuable yet unexplored challenge. In this paper, we propose TakeAD, a novel preference-based post-optimization framework that fine-tunes the pre-trained IL policy with this disengagement data to enhance the closed-loop driving performance. First, we design an efficient expert takeover data collection pipeline inspired by human takeover mechanisms in real-world autonomous driving systems. Then, this post optimization framework integrates iterative Dataset Aggregation (DAgger) for imitation learning with Direct Preference Optimization (DPO) for preference alignment. The DAgger stage equips the policy with fundamental capabilities to handle disengagement states through direct imitation of expert interventions. Subsequently, the DPO stage refines the policy's behavior to better align with expert preferences in disengagement scenarios. Through multiple iterations, the policy progressively learns recovery strategies for disengagement states, thereby mitigating the open-loop gap. Experiments on the closed-loop Bench2Drive benchmark demonstrate our method's effectiveness compared with pure IL methods, with comprehensive ablations confirming the contribution of each component.

**Link**: [arxiv](https://arxiv.org/abs/2512.17370v1),  [pdf](https://arxiv.org/pdf/2512.17370v1)

**Tags**: cs.RO cs.AI 



### From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones
**Authors**: Lifan Yuan, Weize Chen, Yuchen Zhang, Ganqu Cui, Hanbin Wang, Ziming You, Ning Ding, Zhiyuan Liu, Maosong Sun, Hao Peng

**Updated**: 2025-12-19T09:09:46Z

**Summary**: Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.

**Link**: [arxiv](https://arxiv.org/abs/2509.25123v3),  [pdf](https://arxiv.org/pdf/2509.25123v3)

**Tags**: cs.AI cs.CL 



### Adversarially Robust Detection of Harmful Online Content: A Computational Design Science Approach
**Authors**: Yidong Chai, Yi Liu, Mohammadreza Ebrahimi, Weifeng Li, Balaji Padmanabhan

**Updated**: 2025-12-19T09:08:27Z

**Summary**: Social media platforms are plagued by harmful content such as hate speech, misinformation, and extremist rhetoric. Machine learning (ML) models are widely adopted to detect such content; however, they remain highly vulnerable to adversarial attacks, wherein malicious users subtly modify text to evade detection. Enhancing adversarial robustness is therefore essential, requiring detectors that can defend against diverse attacks (generalizability) while maintaining high overall accuracy. However, simultaneously achieving both optimal generalizability and accuracy is challenging. Following the computational design science paradigm, this study takes a sequential approach that first proposes a novel framework (Large Language Model-based Sample Generation and Aggregation, LLM-SGA) by identifying the key invariances of textual adversarial attacks and leveraging them to ensure that a detector instantiated within the framework has strong generalizability. Second, we instantiate our detector (Adversarially Robust Harmful Online Content Detector, ARHOCD) with three novel design components to improve detection accuracy: (1) an ensemble of multiple base detectors that exploits their complementary strengths; (2) a novel weight assignment method that dynamically adjusts weights based on each sample's predictability and each base detector's capability, with weights initialized using domain knowledge and updated via Bayesian inference; and (3) a novel adversarial training strategy that iteratively optimizes both the base detectors and the weight assignor. We addressed several limitations of existing adversarial robustness enhancement research and empirically evaluated ARHOCD across three datasets spanning hate speech, rumor, and extremist content. Results show that ARHOCD offers strong generalizability and improves detection accuracy under adversarial conditions.

**Link**: [arxiv](https://arxiv.org/abs/2512.17367v1),  [pdf](https://arxiv.org/pdf/2512.17367v1)

**Tags**: cs.LG 



### Trustworthy and Controllable Professional Knowledge Utilization in Large Language Models with TEE-GPU Execution
**Authors**: Yifeng Cai, Zhida An, Yuhan Meng, Houqian Liu, Pengli Wang, Hanwen Lei, Yao Guo, Ding Li

**Updated**: 2025-12-19T09:05:37Z

**Summary**: Future improvements in large language model (LLM) services increasingly hinge on access to high-value professional knowledge rather than more generic web data. However, the data providers of this knowledge face a skewed tradeoff between income and risk: they receive little share of downstream value yet retain copyright and privacy liability, making them reluctant to contribute their assets to LLM services. Existing techniques do not offer a trustworthy and controllable way to use professional knowledge, because they keep providers in the dark and combine knowledge parameters with the underlying LLM backbone.   In this paper, we present PKUS, the Professional Knowledge Utilization System, which treats professional knowledge as a first-class, separable artifact. PKUS keeps the backbone model on GPUs and encodes each provider's contribution as a compact adapter that executes only inside an attested Trusted Execution Environment (TEE). A hardware-rooted lifecycle protocol, adapter pruning, multi-provider aggregation, and split-execution scheduling together make this design practical at serving time. On SST-2, MNLI, and SQuAD with GPT-2 Large and Llama-3.2-1B, PKUS preserves model utility, matching the accuracy and F1 of full fine-tuning and plain LoRA, while achieving the lowest per-request latency with 8.1-11.9x speedup over CPU-only TEE inference and naive CPU-GPU co-execution.

**Link**: [arxiv](https://arxiv.org/abs/2512.16238v2),  [pdf](https://arxiv.org/pdf/2512.16238v2)

**Tags**: cs.OS 



### Text-to-SQL Task-oriented Dialogue Ontology Construction
**Authors**: Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic

**Updated**: 2025-12-19T08:52:20Z

**Summary**: Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch using only its inherent SQL programming capabilities combined with concepts from modular TOD systems provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of modular TOD system concepts. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and arXiv dataset. We view this as a step towards broader application of ontologies.

**Link**: [arxiv](https://arxiv.org/abs/2507.23358v2),  [pdf](https://arxiv.org/pdf/2507.23358v2)

**Tags**: cs.CL cs.AI cs.DB cs.IR 



### Adaptive Graph Pruning with Sudden-Events Evaluation for Traffic Prediction using Online Semi-Decentralized ST-GNNs
**Authors**: Ivan Kralj, Lodovico Giaretta, Gordan Ježić, Ivana Podnar Žarko, Šarūnas Girdzijauskas

**Updated**: 2025-12-19T08:48:36Z

**Summary**: Spatio-Temporal Graph Neural Networks (ST-GNNs) are well-suited for processing high-frequency data streams from geographically distributed sensors in smart mobility systems. However, their deployment at the edge across distributed compute nodes (cloudlets) createssubstantial communication overhead due to repeated transmission of overlapping node features between neighbouring cloudlets. To address this, we propose an adaptive pruning algorithm that dynamically filters redundant neighbour features while preserving the most informative spatial context for prediction. The algorithm adjusts pruning rates based on recent model performance, allowing each cloudlet to focus on regions experiencing traffic changes without compromising accuracy. Additionally, we introduce the Sudden Event Prediction Accuracy (SEPA), a novel event-focused metric designed to measure responsiveness to traffic slowdowns and recoveries, which are often missed by standard error metrics. We evaluate our approach in an online semi-decentralized setting with traditional FL, server-free FL, and Gossip Learning on two large-scale traffic datasets, PeMS-BAY and PeMSD7-M, across short-, mid-, and long-term prediction horizons. Experiments show that, in contrast to standard metrics, SEPA exposes the true value of spatial connectivity in predicting dynamic and irregular traffic. Our adaptive pruning algorithm maintains prediction accuracy while significantly lowering communication cost in all online semi-decentralized settings, demonstrating that communication can be reduced without compromising responsiveness to critical traffic events.

**Link**: [arxiv](https://arxiv.org/abs/2512.17352v1),  [pdf](https://arxiv.org/pdf/2512.17352v1)

**Tags**: cs.LG cs.AI cs.DC 



### Phantom Menace: Exploring and Enhancing the Robustness of VLA Models Against Physical Sensor Attacks
**Authors**: Xuancun Lu, Jiaxiang Chen, Shilin Xiao, Zizhi Jin, Zhangrui Chen, Hanwen Yu, Bohan Qian, Ruochen Zhou, Xiaoyu Ji, Wenyuan Xu

**Updated**: 2025-12-19T08:41:28Z

**Summary**: Vision-Language-Action (VLA) models revolutionize robotic systems by enabling end-to-end perception-to-action pipelines that integrate multiple sensory modalities, such as visual signals processed by cameras and auditory signals captured by microphones. This multi-modality integration allows VLA models to interpret complex, real-world environments using diverse sensor data streams. Given the fact that VLA-based systems heavily rely on the sensory input, the security of VLA models against physical-world sensor attacks remains critically underexplored. To address this gap, we present the first systematic study of physical sensor attacks against VLAs, quantifying the influence of sensor attacks and investigating the defenses for VLA models. We introduce a novel "Real-Sim-Real" framework that automatically simulates physics-based sensor attack vectors, including six attacks targeting cameras and two targeting microphones, and validates them on real robotic systems. Through large-scale evaluations across various VLA architectures and tasks under varying attack parameters, we demonstrate significant vulnerabilities, with susceptibility patterns that reveal critical dependencies on task types and model designs. We further develop an adversarial-training-based defense that enhances VLA robustness against out-of-distribution physical perturbations caused by sensor attacks while preserving model performance. Our findings expose an urgent need for standardized robustness benchmarks and mitigation strategies to secure VLA deployments in safety-critical environments.

**Link**: [arxiv](https://arxiv.org/abs/2511.10008v2),  [pdf](https://arxiv.org/pdf/2511.10008v2)

**Tags**: cs.RO cs.AI 



### Sharp Structure-Agnostic Lower Bounds for General Functional Estimation
**Authors**: Jikai Jin, Vasilis Syrgkanis

**Updated**: 2025-12-19T08:34:05Z

**Summary**: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.

**Link**: [arxiv](https://arxiv.org/abs/2512.17341v1),  [pdf](https://arxiv.org/pdf/2512.17341v1)

**Tags**: stat.ML cs.LG econ.EM math.ST stat.ME 



### Bridging Natural Language and Formal Specification--Automated Translation of Software Requirements to LTL via Hierarchical Semantics Decomposition Using LLMs
**Authors**: Zhi Ma, Cheng Wen, Zhexin Su, Xiao Liang, Cong Tian, Shengchao Qin, Mengfei Yang

**Updated**: 2025-12-19T08:25:54Z

**Summary**: Automating the translation of natural language (NL) software requirements into formal specifications remains a critical challenge in scaling formal verification practices to industrial settings, particularly in safety-critical domains. Existing approaches, both rule-based and learning-based, face significant limitations. While large language models (LLMs) like GPT-4o demonstrate proficiency in semantic extraction, they still encounter difficulties in addressing the complexity, ambiguity, and logical depth of real-world industrial requirements. In this paper, we propose Req2LTL, a modular framework that bridges NL and Linear Temporal Logic (LTL) through a hierarchical intermediate representation called OnionL. Req2LTL leverages LLMs for semantic decomposition and combines them with deterministic rule-based synthesis to ensure both syntactic validity and semantic fidelity. Our comprehensive evaluation demonstrates that Req2LTL achieves 88.4% semantic accuracy and 100% syntactic correctness on real-world aerospace requirements, significantly outperforming existing methods.

**Link**: [arxiv](https://arxiv.org/abs/2512.17334v1),  [pdf](https://arxiv.org/pdf/2512.17334v1)

**Tags**: cs.SE 



### LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling
**Authors**: Xin Wang, Zhenhao Li, Zishuo Ding

**Updated**: 2025-12-19T08:24:20Z

**Summary**: The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.

**Link**: [arxiv](https://arxiv.org/abs/2512.16070v2),  [pdf](https://arxiv.org/pdf/2512.16070v2)

**Tags**: cs.SE 



### Content-Aware RSMA-Enabled Pinching-Antenna Systems for Latency Optimization in 6G Networks
**Authors**: Yu Hua, Yaru Fu, Yalin Liu, Zheng Shi, Kevin Hung

**Updated**: 2025-12-19T08:23:54Z

**Summary**: The Pinching Antenna System (PAS) has emerged as a promising technology to dynamically reconfigure wireless propagation environments in 6G networks. By activating radiating elements at arbitrary positions along a dielectric waveguide, PAS can establish strong line-of-sight (LoS) links with users, significantly enhancing channel gain and deployment flexibility, particularly in high-frequency bands susceptible to severe path loss. To further improve multi-user performance, this paper introduces a novel content-aware transmission framework that integrates PAS with rate-splitting multiple access (RSMA). Unlike conventional RSMA, the proposed RSMA scheme enables users requesting the same content to share a unified private stream, thereby mitigating inter-user interference and reducing power fragmentation. We formulate a joint optimization problem aimed at minimizing the average system latency by dynamically adapting both antenna positioning and RSMA parameters according to channel conditions and user requests. A Content-Aware RSMA and Pinching-antenna Joint Optimization (CARP-JO) algorithm is developed, which decomposes the non-convex problem into tractable subproblems solved via bisection search, convex programming, and golden-section search. Simulation results demonstrate that the proposed CARP-JO scheme consistently outperforms Traditional RSMA, NOMA, and Fixed-antenna systems across diverse network scenarios in terms of latency, underscoring the effectiveness of co-designing physical-layer reconfigurability with intelligent communication strategies.

**Link**: [arxiv](https://arxiv.org/abs/2512.17332v1),  [pdf](https://arxiv.org/pdf/2512.17332v1)

**Tags**: eess.SP 



### LookAhead Tuning: Safer Language Models via Partial Answer Previews
**Authors**: Kangwei Liu, Mengru Wang, Yujie Luo, Lin Yuan, Mengshu Sun, Lei Liang, Zhiqiang Zhang, Jun Zhou, Bryan Hooi, Shumin Deng

**Updated**: 2025-12-19T08:17:42Z

**Summary**: Fine-tuning enables large language models (LLMs) to adapt to specific domains, but often compromises their previously established safety alignment. To mitigate the degradation of model safety during fine-tuning, we introduce LookAhead Tuning, a lightweight and effective data-driven approach that preserves safety during fine-tuning. The method introduces two simple strategies that modify training data by previewing partial answer prefixes, thereby minimizing perturbations to the model's initial token distributions and maintaining its built-in safety mechanisms. Comprehensive experiments demonstrate that LookAhead Tuning effectively maintains model safety without sacrificing robust performance on downstream tasks. Our findings position LookAhead Tuning as a reliable and efficient solution for the safe and effective adaptation of LLMs.

**Link**: [arxiv](https://arxiv.org/abs/2503.19041v4),  [pdf](https://arxiv.org/pdf/2503.19041v4)

**Tags**: cs.CL cs.AI cs.CV cs.LG cs.MM 



### Towards Safer Chatbots: Automated Policy Compliance Evaluation of Custom GPTs
**Authors**: David Rodriguez, William Seymour, Jose M. Del Alamo, Jose Such

**Updated**: 2025-12-19T08:17:09Z

**Summary**: User-configured chatbots built on top of large language models are increasingly available through centralized marketplaces such as OpenAI's GPT Store. While these platforms enforce usage policies intended to prevent harmful or inappropriate behavior, the scale and opacity of customized chatbots make systematic policy enforcement challenging. As a result, policy-violating chatbots continue to remain publicly accessible despite existing review processes. This paper presents a fully automated method for evaluating the compliance of Custom GPTs with its marketplace usage policy using black-box interaction. The method combines large-scale GPT discovery, policy-driven red-teaming prompts, and automated compliance assessment using an LLM-as-a-judge. We focus on three policy-relevant domains explicitly addressed in OpenAI's usage policies: Romantic, Cybersecurity, and Academic GPTs. We validate our compliance assessment component against a human-annotated ground-truth dataset, achieving an F1 score of 0.975 for binary policy violation detection. We then apply the method in a large-scale empirical study of 782 Custom GPTs retrieved from the GPT Store. The results show that 58.7% of the evaluated GPTs exhibit at least one policy-violating response, with substantial variation across policy domains. A comparison with the base models (GPT-4 and GPT-4o) indicates that most violations originate from model-level behavior, while customization tends to amplify these tendencies rather than create new failure modes. Our findings reveal limitations in current review mechanisms for user-configured chatbots and demonstrate the feasibility of scalable, behavior-based policy compliance evaluation.

**Link**: [arxiv](https://arxiv.org/abs/2502.01436v3),  [pdf](https://arxiv.org/pdf/2502.01436v3)

**Tags**: cs.CL cs.AI 



### Task Schema and Binding: A Double Dissociation Study of In-Context Learning
**Authors**: Chaeha Kim

**Updated**: 2025-12-19T08:14:21Z

**Summary**: We provide causal mechanistic validation that in-context learning (ICL) decomposes into two separable mechanisms: Task Schema (abstract task type recognition) and Binding (specific input-output associations). Through activation patching experiments across 9 models from 7 Transformer families plus Mamba (370M-13B parameters), we establish three key findings:   1. Double dissociation: Task Schema transfers at 100% via late MLP patching; Binding transfers at 62% via residual stream patching -- proving separable mechanisms   2. Prior-Schema trade-off: Schema reliance inversely correlates with prior knowledge (Spearman rho = -0.596, p < 0.001, N=28 task-model pairs)   3. Architecture generality: The mechanism operates across all tested architectures including the non-Transformer Mamba   These findings offer a mechanistic account of the ICL puzzle that contrasts with prior views treating ICL as a monolithic mechanism (whether retrieval-based, gradient descent-like, or purely Bayesian). By establishing that Schema and Binding are neurally dissociable -- not merely behavioral modes -- we provide causal evidence for dual-process theories of ICL. Models rely on Task Schema when prior knowledge is absent, but prior knowledge interferes through attentional mis-routing (72.7% recency bias) rather than direct output competition (0%). This explains why arbitrary mappings succeed (zero prior leads to full Schema reliance) while factual overrides fail -- and reveals that the true bottleneck is attentional, not output-level. Practical implications: Understanding these dual mechanisms enables more efficient prompt engineering -- reliable schema transfer reduces required demonstrations for novel tasks, while prior-aware design can mitigate the 38% binding failure rate in high-prior scenarios, improving ICL system reliability in production deployments.

**Link**: [arxiv](https://arxiv.org/abs/2512.17325v1),  [pdf](https://arxiv.org/pdf/2512.17325v1)

**Tags**: cs.LG cs.CL 



### Hybrid and Unitary PEFT for Resource-Efficient Large Language Models
**Authors**: Haomin Qi, Zihan Dai, Chengbo Huang

**Updated**: 2025-12-19T08:12:27Z

**Summary**: Fine-tuning large language models (LLMs) remains a computational bottleneck due to their scale and memory demands. This paper presents a comprehensive evaluation of parameter-efficient fine-tuning (PEFT) techniques, including LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that dynamically integrates BOFT's orthogonal stability with LoRA-GA's gradient-aligned rapid convergence. By computing per-layer adaptive updates guided by gradient norms, the hybrid method achieves superior convergence efficiency and generalization across diverse tasks. We also explore, for the first time, the adaptation of unitary RNN (uRNN) principles to Transformer-based LLMs, enhancing gradient stability through structured unitary constraints. Across GLUE, GSM8K, MT-Bench, and HumanEval, using models ranging from 7B to 405B parameters, the hybrid approach yields consistent gains across three independent runs per task and model, approaching the quality of full fine-tuning while reducing training time by approximately 2.1 times and peak memory usage by nearly 50 percent, indicating practical significance under resource constraints. A compact multilingual and low-resource study on XNLI and FLORES, using 32 examples per language, further demonstrates consistent gains under the same budget with a small and stable footprint. These results indicate a practical and scalable path toward accessible LLM fine-tuning under resource constraints.

**Link**: [arxiv](https://arxiv.org/abs/2507.18076v2),  [pdf](https://arxiv.org/pdf/2507.18076v2)

**Tags**: cs.CL 



### Neuro-Symbolic Control with Large Language Models for Language-Guided Spatial Tasks
**Authors**: Momina Liaqat Ali, Muhammad Abid

**Updated**: 2025-12-19T08:08:40Z

**Summary**: Although large language models (LLMs) have recently become effective tools for language-conditioned control in embodied systems, instability, slow convergence, and hallucinated actions continue to limit their direct application to continuous control. A modular neuro-symbolic control framework that clearly distinguishes between low-level motion execution and high-level semantic reasoning is proposed in this work. While a lightweight neural delta controller performs bounded, incremental actions in continuous space, a locally deployed LLM interprets symbolic tasks. We assess the suggested method in a planar manipulation setting with spatial relations between objects specified by language. Numerous tasks and local language models, such as Mistral, Phi, and LLaMA-3.2, are used in extensive experiments to compare LLM-only control, neural-only control, and the suggested LLM+DL framework. In comparison to LLM-only baselines, the results show that the neuro-symbolic integration consistently increases both success rate and efficiency, achieving average step reductions exceeding 70% and speedups of up to 8.83x while remaining robust to language model quality. The suggested framework enhances interpretability, stability, and generalization without any need of reinforcement learning or costly rollouts by controlling the LLM to symbolic outputs and allocating uninterpreted execution to a neural controller trained on artificial geometric data. These outputs show empirically that neuro-symbolic decomposition offers a scalable and principled way to integrate language understanding with ongoing control, this approach promotes the creation of dependable and effective language-guided embodied systems.

**Link**: [arxiv](https://arxiv.org/abs/2512.17321v1),  [pdf](https://arxiv.org/pdf/2512.17321v1)

**Tags**: cs.RO 



### A Benchmark for Ultra-High-Resolution Remote Sensing MLLMs
**Authors**: Yunkai Dang, Meiyi Zhu, Donghao Wang, Yizhuo Zhang, Jiacheng Yang, Qi Fan, Yuekun Yang, Wenbin Li, Feng Miao, Yang Gao

**Updated**: 2025-12-19T08:07:51Z

**Summary**: Multimodal large language models (MLLMs) demonstrate strong perception and reasoning performance on existing remote sensing (RS) benchmarks. However, most prior benchmarks rely on low-resolution imagery, and some high-resolution benchmarks suffer from flawed reasoning-task designs. We show that text-only LLMs can perform competitively with multimodal vision-language models on RS reasoning tasks without access to images, revealing a critical mismatch between current benchmarks and the intended evaluation of visual understanding. To enable faithful assessment, we introduce RSHR-Bench, a super-high-resolution benchmark for RS visual understanding and reasoning. RSHR-Bench contains 5,329 full-scene images with a long side of at least 4,000 pixels, with up to about 3 x 10^8 pixels per image, sourced from widely used RS corpora and UAV collections. We design four task families: multiple-choice VQA, open-ended VQA, image captioning, and single-image evaluation. These tasks cover nine perception categories and four reasoning types, supporting multi-turn and multi-image dialog. To reduce reliance on language priors, we apply adversarial filtering with strong LLMs followed by rigorous human verification. Overall, we construct 3,864 VQA tasks, 3,913 image captioning tasks, and 500 fully human-written or verified single-image evaluation VQA pairs. Evaluations across open-source, closed-source, and RS-specific VLMs reveal persistent performance gaps in super-high-resolution scenarios. Code: https://github.com/Yunkaidang/RSHR

**Link**: [arxiv](https://arxiv.org/abs/2512.17319v1),  [pdf](https://arxiv.org/pdf/2512.17319v1)

**Tags**: cs.CV cs.AI cs.MM 



### V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval
**Authors**: Donghyuk Kim, Sejeong Yang, Wonjin Shin, Joo-Young Kim

**Updated**: 2025-12-19T08:02:44Z

**Summary**: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.   In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.

**Link**: [arxiv](https://arxiv.org/abs/2512.12284v2),  [pdf](https://arxiv.org/pdf/2512.12284v2)

**Tags**: eess.IV cs.AI cs.AR cs.CV cs.MM 



### RecipeMasterLLM: Revisiting RoboEarth in the Era of Large Language Models
**Authors**: Asil Kaan Bozcuoglu, Ziyuan Liu

**Updated**: 2025-12-19T07:47:19Z

**Summary**: RoboEarth was a pioneering initiative in cloud robotics, establishing a foundational framework for robots to share and exchange knowledge about actions, objects, and environments through a standardized knowledge graph. Initially, this knowledge was predominantly hand-crafted by engineers using RDF triples within OWL Ontologies, with updates, such as changes in an object's pose, being asserted by the robot's control and perception routines. However, with the advent and rapid development of Large Language Models (LLMs), we believe that the process of knowledge acquisition can be significantly automated. To this end, we propose RecipeMasterLLM, a high-level planner, that generates OWL action ontologies based on a standardized knowledge graph in response to user prompts. This architecture leverages a fine-tuned LLM specifically trained to understand and produce action descriptions consistent with the RoboEarth standardized knowledge graph. Moreover, during the Retrieval-Augmented Generation (RAG) phase, environmental knowledge is supplied to the LLM to enhance its contextual understanding and improve the accuracy of the generated action descriptions.

**Link**: [arxiv](https://arxiv.org/abs/2512.17309v1),  [pdf](https://arxiv.org/pdf/2512.17309v1)

**Tags**: cs.RO 



### Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation
**Authors**: Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka, Pratik Narang, Dhruv Kumar

**Updated**: 2025-12-19T07:46:29Z

**Summary**: Strategic decision-making in Pokémon battles presents a unique testbed for evaluating large language models. Pokémon battles demand reasoning about type matchups, statistical trade-offs, and risk assessment, skills that mirror human strategic thinking. This work examines whether Large Language Models (LLMs) can serve as competent battle agents, capable of both making tactically sound decisions and generating novel, balanced game content. We developed a turn-based Pokémon battle system where LLMs select moves based on battle state rather than pre-programmed logic. The framework captures essential Pokémon mechanics: type effectiveness multipliers, stat-based damage calculations, and multi-Pokémon team management. Through systematic evaluation across multiple model architectures we measured win rates, decision latency, type-alignment accuracy, and token efficiency. These results suggest LLMs can function as dynamic game opponents without domain-specific training, offering a practical alternative to reinforcement learning for turn-based strategic games. The dual capability of tactical reasoning and content creation, positions LLMs as both players and designers, with implications for procedural generation and adaptive difficulty systems in interactive entertainment.

**Link**: [arxiv](https://arxiv.org/abs/2512.17308v1),  [pdf](https://arxiv.org/pdf/2512.17308v1)

**Tags**: cs.AI cs.CL 



### Joint Coverage and Electromagnetic Field Exposure Analysis in Downlink and Uplink for RIS-assisted Networks
**Authors**: Lin Chen, Ahmed Elzanaty, Mustafa A Kishk, Ying-Jun Angela Zhang

**Updated**: 2025-12-19T07:44:59Z

**Summary**: Reconfigurable intelligent surfaces (RISs) have shown the potential to improve signal-to-interference-plus-noise ratio (SINR) related coverage, especially at high-frequency communications. However, assessing electromagnetic field exposure (EMFE) and establishing EMFE regulations in RIS-assisted large-scale networks remain open issues. This paper proposes a stochastic geometry (SG) based framework to characterize SINR and EMFE in such networks for downlink and uplink scenarios. Particularly, we carefully consider the association rule with the presence of RISs, accurate antenna pattern at base stations (BSs), fading model, and power control mechanism at mobile devices in the system model. Under the proposed framework, we derive the marginal and joint distributions of SINR and EMFE in downlink and uplink, respectively. The first moment of EMFE is also provided. Additionally, we design the compliance distance (CD) between a BS/RIS and a user to comply with the EMFE regulations. To facilitate efficient identification, we further provide approximate closed-form expressions for CDs. From numerical results of the marginal distributions, we find that in the downlink scenario, deploying RISs may not always be beneficial, as the improved SINR comes at the cost of increased EMFE. However, in the uplink scenario, RIS deployment is promising to enhance coverage while still maintaining EMFE compliance. By simultaneously evaluating coverage and compliance metrics through joint distributions, we demonstrate the feasibility of RISs in improving uplink and downlink performance. Insights from this framework can contribute to establishing EMFE guidelines and achieving a balance between coverage and compliance when deploying RISs.

**Link**: [arxiv](https://arxiv.org/abs/2412.00317v2),  [pdf](https://arxiv.org/pdf/2412.00317v2)

**Tags**: eess.SP 



### Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements
**Authors**: Suhas BN, Andrew M. Sherrill, Jyoti Alaparthi, Dominik Mattioli, Rosa I. Arriaga, Chris W. Wiese, Saeed Abdullah

**Updated**: 2025-12-19T07:32:25Z

**Summary**: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic stress disorder (PTSD), but evaluating therapist fidelity remains labor-intensive due to the need for manual review of session recordings. We present a method for the automatic temporal localization of key PE fidelity elements, identifying their start and stop times, directly from session audio and transcripts. Our approach fine-tunes a large pre-trained audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process focused 30-second windows of audio-transcript input. Fidelity labels for three core protocol phases, therapist orientation (P1), imaginal exposure (P2), and post-imaginal processing (P3), are generated via LLM-based prompting and verified by trained raters. The model is trained to predict normalized boundary offsets using soft supervision guided by task-specific prompts. On a dataset of 308 real PE sessions, our best configuration (LoRA rank 8, 30s windows) achieves a mean absolute error (MAE) of 5.3s across tasks, within typical rater tolerance for timestamp review, enabling practical fidelity QC. We further analyze the effects of window size and LoRA rank, highlighting the importance of context granularity and model adaptation. This work introduces a privacy-preserving, scalable framework for fidelity tracking in PE therapy, with potential to support clinician training, supervision, and quality assurance.

**Link**: [arxiv](https://arxiv.org/abs/2506.09707v4),  [pdf](https://arxiv.org/pdf/2506.09707v4)

**Tags**: eess.AS cs.CL cs.HC 



### G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation
**Authors**: Yesung Cho, Sungmin Lee, Geongyu Lee, Minkyung Lee, Jongbae Park, Dongmyung Shin

**Updated**: 2025-12-19T07:32:01Z

**Summary**: Recent studies in pathology foundation models have shown that scaling training data, diversifying cancer types, and increasing model size consistently improve their performance. However, giga-scale foundation models, which are trained on hundreds of thousands of slides covering tens of cancer types and contain billions of parameters, pose significant challenges for practical use due to their tremendous computational costs in both development and deployment. In this work, we present a novel strategy, named the G2L framework, to increase the performance of large-scale foundation models, which consist of only $15\%$ of the parameters of giga-scale models, to a comparable performance level of giga-scale models in cancer-specific tasks. Our approach applies knowledge distillation, transferring the capabilities of a giga-scale model to a large-scale model, using just 1K pathology slides of a target cancer (e.g., breast, prostate, etc.). The resulting distilled model not only outperformed state-of-the-art models of the same size (i.e., large-scale) across several benchmarks but also, interestingly, surpassed the giga-scale teacher and huge-scale models in some benchmarks. In addition, the distilled model exhibited a higher robustness index, indicating improved resilience to image variations originating from multiple institutions. These findings suggest that the proposed distillation approach for a large-scale model is a data- and parameter-efficient way to achieve giga-scale-level performance for cancer-specific applications without prohibitive computational burden.

**Link**: [arxiv](https://arxiv.org/abs/2510.11176v2),  [pdf](https://arxiv.org/pdf/2510.11176v2)

**Tags**: cs.CV cs.AI 



### M2RU: Memristive Minion Recurrent Unit for Continual Learning at the Edge
**Authors**: Abdullah M. Zyarah, Dhireesha Kudithipudi

**Updated**: 2025-12-19T07:27:30Z

**Summary**: Continual learning on edge platforms remains challenging because recurrent networks depend on energy-intensive training procedures and frequent data movement that are impractical for embedded deployments. This work introduces M2RU, a mixed-signal architecture that implements the minion recurrent unit for efficient temporal processing with on-chip continual learning. The architecture integrates weighted-bit streaming, which enables multi-bit digital inputs to be processed in crossbars without high-resolution conversion, and an experience replay mechanism that stabilizes learning under domain shifts. M2RU achieves 15 GOPS at 48.62 mW, corresponding to 312 GOPS per watt, and maintains accuracy within 5 percent of software baselines on sequential MNIST and CIFAR-10 tasks. Compared with a CMOS digital design, the accelerator provides 29X improvement in energy efficiency. Device-aware analysis shows an expected operational lifetime of 12.2 years under continual learning workloads. These results establish M2RU as a scalable and energy-efficient platform for real-time adaptation in edge-level temporal intelligence.

**Link**: [arxiv](https://arxiv.org/abs/2512.17299v1),  [pdf](https://arxiv.org/pdf/2512.17299v1)

**Tags**: cs.LG cs.AI cs.ET 



### ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration
**Authors**: Fanpu Cao, Yaofo Chen, Zeng You, Wei Luo, Cen Chen

**Updated**: 2025-12-19T07:27:19Z

**Summary**: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.

**Link**: [arxiv](https://arxiv.org/abs/2512.17298v1),  [pdf](https://arxiv.org/pdf/2512.17298v1)

**Tags**: cs.CV 



