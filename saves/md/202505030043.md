# Arxiv Results
## Keyword: kv cache 
 ### FreqKV: Frequency Domain Key-Value Compression for Efficient Context   Window Extension
**Authors**: Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin

**Updated**: 2025-05-01T14:53:12Z

**Summary**: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2505.00570v1),  [pdf](http://arxiv.org/pdf/2505.00570v1)

**Tags**: cs.CL cs.AI 



### Mixture of Sparse Attention: Content-Based Learnable Sparse Attention   via Expert-Choice Routing
**Authors**: Piotr Piękos, Róbert Csordás, Jürgen Schmidhuber

**Updated**: 2025-05-01T05:22:11Z

**Summary**: Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.00315v1),  [pdf](http://arxiv.org/pdf/2505.00315v1)

**Tags**: cs.LG cs.CL 



### QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM   Serving
**Authors**: Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han

**Updated**: 2025-05-01T02:14:05Z

**Summary**: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.

**Link**: [arxiv](http://arxiv.org/abs/2405.04532v3),  [pdf](http://arxiv.org/pdf/2405.04532v3)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### Soft-Label Caching and Sharpening for Communication-Efficient Federated   Distillation
**Authors**: Kitsuya Azuma, Takayuki Nishio, Yuichi Kitagawa, Wakako Nakano, Takahito Tanimura

**Updated**: 2025-05-01T00:13:06Z

**Summary**: Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining accuracy. Enhanced ERA can be tuned to adapt to non-IID data variations, ensuring robust aggregation and performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.

**Link**: [arxiv](http://arxiv.org/abs/2504.19602v2),  [pdf](http://arxiv.org/pdf/2504.19602v2)

**Tags**: cs.LG 



### Accelerating Diffusion Transformer via Error-Optimized Cache
**Authors**: Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, Xingyu Zhu, Yanbin Hao

**Updated**: 2025-04-30T19:48:41Z

**Summary**: Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the Error-Optimized Cache (EOC). This method introduces three key improvements: (1) Prior knowledge extraction: Extract and process the caching differences; (2) A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; (3) Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching, especially excessive caching. On the ImageNet dataset, without substantially increasing the computational load, this method improves the FID of the generated images when the rule-based model FORA has a caching level of 75%, 50%, and 25%, and the training-based model Learning-to-cache has a caching level of 22%. Specifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857 to 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%) respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19243v2),  [pdf](http://arxiv.org/pdf/2501.19243v2)

**Tags**: cs.CV 



### SDW driven "magnetic breakdown" in a d-wave altermagnet KV$_2$Se$_2$O
**Authors**: Xu Yan, Ziyin Song, Juntao Song, Zhong Fang, Hongming Weng, Quansheng Wu

**Updated**: 2025-04-30T18:00:02Z

**Summary**: Altermagnets, combining zero net magnetization with intrinsic spin splitting, demonstrate unique quantum phenomena crucial for spintronic applications. KV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a checkerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave (SDW) state as the temperature decreases. After phase transition, the apparent paradox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals negligible Fermi surface modifications, while physical property measurement system (PPMS) measurements uncover substantial changes in transport properties. Our study explores the microscopic mechanisms governing phase-dependent transport properties of KV$_2$Se$_2$O base on first-principles calculations. The spin canting driven by periodic spin modulation in the SDW phase reduces the magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting and Fermi surface reconstruction induce the ``magnetic breakdown" phenomenon, which alters carrier trajectories, modifies carrier concentration, strengthens electron-hole compensation, and ultimately accounts for the contrasting magnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our work proposes an innovative method for identifying the electronic structure evolution across phase transitions from transport signatures, providing a novel paradigm for altermagnets research.

**Link**: [arxiv](http://arxiv.org/abs/2505.00074v1),  [pdf](http://arxiv.org/pdf/2505.00074v1)

**Tags**: cond-mat.mtrl-sci 



### Switching Transients in Constrained Transformer-Line/Cable   Configurations
**Authors**: Y. Xiang, L. Wu, K. Velitsikakis, A. L. J. Janssen

**Updated**: 2025-04-30T12:51:59Z

**Summary**: This paper investigates the transient phenomena that occur in two special cases in the Netherlands: (A) during the energization of a power transformer via a cable feeder and (B) the energization of a power transformer together with an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV transformer are connected and energized at the same time. In Case B a 150/50 kV transformer and a short 50 kV OHL are connected and energized simultaneously. The reason behind this kind of situations is related to space restrictions and cost efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2504.21594v1),  [pdf](http://arxiv.org/pdf/2504.21594v1)

**Tags**: eess.SY cs.SY 



### Kimina Lean Server: Technical Report
**Authors**: Marco Dos Santos, Haiming Wang, Hugues de Saxcé, Ran Wang, Mantas Baksys, Mert Unsal, Junqi Liu, Zhengying Liu, Jia Li

**Updated**: 2025-04-29T23:43:59Z

**Summary**: We introduce the Kimina Lean Server, an open-source project that enables fast and scalable interaction with Lean 4 via a unified REST API, designed as a simple verifier for reinforcement learning pipelines. Built on top of the Lean FRO's LeanREPL, it combines server-side parallelization by managing multiple Lean REPL processes in parallel, with an LRU caching strategy that reuses Lean imports across multiple requests. These features help reduce initialization overhead and allow large-scale batch processing of Lean code. The client-side interface allows users to submit batches of proofs and receive Lean feedback, including extracted tactics and tactic states via infotree processing. These features enable a high-performance, scalable workflow for both interaction and extraction of proofs, tactics, and tactic states. We open source our implementation on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2504.21230v1),  [pdf](http://arxiv.org/pdf/2504.21230v1)

**Tags**: cs.LO 



### CachePrune: Neural-Based Attribution Defense Against Indirect Prompt   Injection Attacks
**Authors**: Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Lina Yao, Julian McAuley

**Updated**: 2025-04-29T23:42:21Z

**Summary**: Large Language Models (LLMs) are identified as being susceptible to indirect prompt injection attack, where the model undesirably deviates from user-provided instructions by executing tasks injected in the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune that defends against this attack by identifying and pruning task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to treat the text spans of input prompt context as only pure data, instead of any indicator of instruction following. These neurons are identified via feature attribution with a loss function induced from an upperbound of the Direct Preference Optimization (DPO) objective. We show that such a loss function enables effective feature attribution with only a few samples. We further improve on the quality of feature attribution, by exploiting an observed triggering effect in instruction following. Our approach does not impose any formatting on the original prompt or introduce extra test-time LLM calls. Experiments show that CachePrune significantly reduces attack success rates without compromising the response quality. Note: This paper aims to defend against indirect prompt injection attacks, with the goal of developing more secure and robust AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.21228v1),  [pdf](http://arxiv.org/pdf/2504.21228v1)

**Tags**: cs.CR cs.AI 



### An Achievable Scheme for the K-user Linear Computation Broadcast Channel
**Authors**: Yinbin Ma, Daniela Tuninetti

**Updated**: 2025-04-29T17:54:42Z

**Summary**: This paper presents a new achievable scheme for the K-user Linear Computation Broadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K users, each aiming to retrieve a desired linear function of the data by leveraging their prior locally available side information in the form of another linear function of the data. The proposed scheme is based on a subspace decomposition derived from representable polymatroid spaces. This decomposition enables the server to effectively design multicast messages that simultaneously benefit multiple users and allow users to eliminate interference using their available side information. This work extends existing results for the 3-LCBC by introducing a linear programming framework to optimize multicast opportunities across an arbitrary number of users. The proposed approach can be used to derive achievable scheme for the K-user coded caching problem with linear coded placement and scalar linear function retrieval, which was our original motivation to investigate the K-LCBC.

**Link**: [arxiv](http://arxiv.org/abs/2501.12322v2),  [pdf](http://arxiv.org/pdf/2501.12322v2)

**Tags**: cs.IT math.IT 



### Activated LoRA: Fine-tuned LLMs for Intrinsics
**Authors**: Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox

**Updated**: 2025-04-29T14:25:08Z

**Summary**: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is highly inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. highly specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We use aLoRA to train a set of intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits.

**Link**: [arxiv](http://arxiv.org/abs/2504.12397v2),  [pdf](http://arxiv.org/pdf/2504.12397v2)

**Tags**: cs.LG cs.AI 



### VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with   Delayed Hits
**Authors**: Bowen Jiang, Chaofan Ma, Duo Wang

**Updated**: 2025-04-29T00:58:59Z

**Summary**: Caches are fundamental to latency-sensitive systems like Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit phenomenon where multiple requests for an object occur during its fetch from the remote server after a miss significantly inflates user-perceived latency. While recent algorithms acknowledge delayed hits by estimating the resulting aggregate delay, they predominantly focus on its mean value. We identify and demonstrate that such approaches are insufficient, as the real aggregate delay frequently exhibits substantial variance in the true production system, leading to suboptimal latency performance when ignored. Thus, we propose VA-CDH, a variance-aware method to optimize latency for caching with delayed hits. It employs a novel ranking function that explicitly incorporates both the empirically estimated mean and standard deviation of aggregate delay, allowing caching decisions to account for its variation. We derive the analytical distribution of aggregate delay under Poisson arrivals as a theoretical contribution, offering more statistical insight beyond the mean value. Through the simulations conducted on synthetic and real-world datasets, we show that VA-CDH reduces the total latency by 1%-6% approximately compared to state-of-the-art algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2504.20335v1),  [pdf](http://arxiv.org/pdf/2504.20335v1)

**Tags**: cs.NI 



### Tree embedding based mapping system for low-latency mobile applications   in multi-access networks
**Authors**: Yu Mi, Randeep Bhatia, Fang Hao, An Wang, Steve Benno, Tv Lakshman

**Updated**: 2025-04-28T20:30:59Z

**Summary**: Low-latency applications like AR/VR and online gaming need fast, stable connections. New technologies such as V2X, LEO satellites, and 6G bring unique challenges in mobility management. Traditional solutions based on centralized or distributed anchors often fall short in supporting rapid mobility due to inefficient routing, low versatility, and insufficient multi-access support. In this paper, we design a new end-to-end system for tracking multi-connected mobile devices at scale and optimizing performance for latency-sensitive, highly dynamic applications. Our system, based on the locator/ID separation principle, extends to multi-access networks without requiring specialized routers or caching. Using a novel tree embedding-based overlay, we enable fast session setup while allowing endpoints to directly handle mobility between them. Evaluation with real network data shows our solution cuts connection latency to 7.42% inflation over the shortest path, compared to LISP's 359\% due to cache misses. It also significantly reduces location update overhead and disruption time during mobility.

**Link**: [arxiv](http://arxiv.org/abs/2504.20246v1),  [pdf](http://arxiv.org/pdf/2504.20246v1)

**Tags**: cs.NI 



### Hierarchical Coded Caching with Low Subpacketization and Coding Delay   using Combinatorial t-Designs
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-04-28T17:17:53Z

**Summary**: Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.12747v3),  [pdf](http://arxiv.org/pdf/2405.12747v3)

**Tags**: cs.IT math.IT 



### 3D MPSoC with On-Chip Cache Support -- Design and Exploitation
**Authors**: Rodrigo Cataldo, Cesar Marcon, Debora Matos

**Updated**: 2025-04-28T16:59:13Z

**Summary**: The increasing density of transistors in Integrated Circuits (ICs) has enabled the development of highly integrated Systems-on-Chip (SoCs) and, more recently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability challenges in communication and memory performance, three-dimensional (3D) Network-on-Chip (NoC) architectures have emerged, offering improvements in communication latency and throughput. However, memory system efficiency remains a critical bottleneck in NoC-based designs. This work proposes the design and experimental exploration of 3D MPSoCs with on-chip cache support by employing distinct communication infrastructures for inter-processor and memory interactions. Specifically, packet-based NoCs are adopted for inter-processor communication, while a crossbar-based infrastructure supports a cache coherence hierarchy for memory access. A two-layer system architecture is introduced, combining a Uniform Memory Access (UMA) model within clusters and a No Remote Memory Access (NORMA) model between clusters, aiming to balance scalability and coherence requirements. Emerging memory technologies such as PCRAM and MRAM are explored to optimize performance, energy consumption, and area usage. Experimental evaluations are conducted using the Gem5 simulator, targeting a model based on the ARM Versatile Express platform. The outcomes of this study aim to enhance MPSoC scalability while meeting the stringent demands of memory-centric applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.19984v1),  [pdf](http://arxiv.org/pdf/2504.19984v1)

**Tags**: cs.AR 



### TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate
**Authors**: Amir Zandieh, Majid Daliri, Majid Hadian, Vahab Mirrokni

**Updated**: 2025-04-28T15:05:35Z

**Summary**: Vector quantization, a problem rooted in Shannon's source coding theory, aims to quantize high-dimensional Euclidean vectors while minimizing distortion in their geometric structure. We propose TurboQuant to address both mean-squared error (MSE) and inner product distortion, overcoming limitations of existing methods that fail to achieve optimal distortion rates. Our data-oblivious algorithms, suitable for online applications, achieve near-optimal distortion rates (within a small constant factor) across all bit-widths and dimensions. TurboQuant achieves this by randomly rotating input vectors, inducing a concentrated Beta distribution on coordinates, and leveraging the near-independence property of distinct coordinates in high dimensions to simply apply optimal scalar quantizers per each coordinate. Recognizing that MSE-optimal quantizers introduce bias in inner product estimation, we propose a two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL (QJL) transform on the residual, resulting in an unbiased inner product quantizer. We also provide a formal proof of the information-theoretic lower bounds on best achievable distortion rate by any vector quantizer, demonstrating that TurboQuant closely matches these bounds, differing only by a small constant ($\approx 2.7$) factor. Experimental results validate our theoretical findings, showing that for KV cache quantization, we achieve absolute quality neutrality with 3.5 bits per channel and marginal quality degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search tasks, our method outperforms existing product quantization techniques in recall while reducing indexing time to virtually zero.

**Link**: [arxiv](http://arxiv.org/abs/2504.19874v1),  [pdf](http://arxiv.org/pdf/2504.19874v1)

**Tags**: cs.LG cs.AI cs.DB cs.DS 



### semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated   Computation and Unified Storage
**Authors**: Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, Yun Liang, Yu Wang

**Updated**: 2025-04-28T15:00:03Z

**Summary**: Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.   In this paper, we identify that the advantage of the disaggregated system lies in the disaggregated computation, i.e., partitioning the computational resource to enable the asynchronous computation of two phases. Thus, we propose a novel LLM serving system, semi-PD, characterized by disaggregated computation and unified storage. In semi-PD, we introduce a computation resource controller to achieve disaggregated computation at the streaming multi-processor (SM) level, and a unified memory manager to manage the asynchronous memory access from both phases. semi-PD has a low-overhead resource adjustment mechanism between the two phases, and a service-level objective (SLO) aware dynamic partitioning algorithm to optimize the SLO attainment. Compared to state-of-the-art systems, semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models.

**Link**: [arxiv](http://arxiv.org/abs/2504.19867v1),  [pdf](http://arxiv.org/pdf/2504.19867v1)

**Tags**: cs.CL cs.DC cs.LG 



### Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching   for Small Buffer or Small Rate
**Authors**: Han Fang, Nan Liu, Wei Kang

**Updated**: 2025-04-28T09:03:45Z

**Summary**: We consider the secure coded caching problem proposed by Ravindrakumar et. al where no user can obtain information about files other than the one requested. We first propose three new schemes for the three cases of cache size $M=1$, $N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files and $K$ users, and the general case for arbitrary $N$ files and $K$ users, respectively. Then we derive converse results by characterizing new properties of secure coded caching schemes. As a result, we characterize the two end-points of the optimal memory-rate tradeoff curve for arbitrary number of users and files. Furthermore, for the case of $N=2$ files and arbitrary number of users, we also characterize a segment of the optimal memory-rate tradeoff curve, where the cache size is relatively small.

**Link**: [arxiv](http://arxiv.org/abs/2504.19601v1),  [pdf](http://arxiv.org/pdf/2504.19601v1)

**Tags**: cs.IT math.IT 



### Quantifying Memory Utilization with Effective State-Size
**Authors**: Rom N. Parnichkun, Neehal Tumma, Armin W. Thomas, Alessandro Moro, Qi An, Taiji Suzuki, Atsushi Yamashita, Michael Poli, Stefano Massaroli

**Updated**: 2025-04-28T08:12:30Z

**Summary**: The need to develop a general framework for architecture analysis is becoming increasingly important, given the expanding design space of sequence models. To this end, we draw insights from classical signal processing and control theory, to develop a quantitative measure of \textit{memory utilization}: the internal mechanisms through which a model stores past information to produce future outputs. This metric, which we call \textbf{\textit{effective state-size}} (ESS), is tailored to the fundamental class of systems with \textit{input-invariant} and \textit{input-varying linear operators}, encompassing a variety of computational units such as variants of attention, convolutions, and recurrences. Unlike prior work on memory utilization, which either relies on raw operator visualizations (e.g. attention maps), or simply the total \textit{memory capacity} (i.e. cache size) of a model, our metrics provide highly interpretable and actionable measurements. In particular, we show how ESS can be leveraged to improve initialization strategies, inform novel regularizers and advance the performance-efficiency frontier through model distillation. Furthermore, we demonstrate that the effect of context delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural differences in how large language models utilize their available memory to recall information. Overall, we find that ESS provides valuable insights into the dynamics that dictate memory utilization, enabling the design of more efficient and effective sequence models.

**Link**: [arxiv](http://arxiv.org/abs/2504.19561v1),  [pdf](http://arxiv.org/pdf/2504.19561v1)

**Tags**: cs.LG 



### Prisma: An Open Source Toolkit for Mechanistic Interpretability in   Vision and Video
**Authors**: Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian Lapuschkin, Lee Sharkey, Blake Aaron Richards

**Updated**: 2025-04-28T04:31:24Z

**Summary**: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.

**Link**: [arxiv](http://arxiv.org/abs/2504.19475v1),  [pdf](http://arxiv.org/pdf/2504.19475v1)

**Tags**: cs.CV cs.AI cs.LG 



### From Cluster to Desktop: A Cache-Accelerated INR framework for   Interactive Visualization of Tera-Scale Data
**Authors**: Daniel Zavorotny, Qi Wu, David Bauer, Kwan-Liu Ma

**Updated**: 2025-04-28T04:02:30Z

**Summary**: Machine learning has enabled the use of implicit neural representations (INRs) to efficiently compress and reconstruct massive scientific datasets. However, despite advances in fast INR rendering algorithms, INR-based rendering remains computationally expensive, as computing data values from an INR is significantly slower than reading them from GPU memory. This bottleneck currently restricts interactive INR visualization to professional workstations. To address this challenge, we introduce an INR rendering framework accelerated by a scalable, multi-resolution GPU cache capable of efficiently representing tera-scale datasets. By minimizing redundant data queries and prioritizing novel volume regions, our method reduces the number of INR computations per frame, achieving an average 5x speedup over the state-of-the-art INR rendering method while still maintaining high visualization quality. Coupled with existing hardware-accelerated INR compressors, our framework enables scientists to generate and compress massive datasets in situ on high-performance computing platforms and then interactively explore them on consumer-grade hardware post hoc.

**Link**: [arxiv](http://arxiv.org/abs/2504.18001v2),  [pdf](http://arxiv.org/pdf/2504.18001v2)

**Tags**: cs.GR 



### Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and   Generalizable Point Cloud Analysis
**Authors**: Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai

**Updated**: 2025-04-28T02:58:27Z

**Summary**: This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data (often inaccessible during online inference) and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop \textbf{Point-Cache}, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2503.12150v3),  [pdf](http://arxiv.org/pdf/2503.12150v3)

**Tags**: cs.CV 



### AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration
**Authors**: Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou

**Updated**: 2025-04-27T22:05:14Z

**Summary**: Graphics Processing Units (GPUs) have become essential for computationally intensive applications. However, emerging workloads such as recommender systems, graph analytics, and data analytics often involve processing data exceeding GPU on-chip memory capacity. To mitigate this issue, existing solutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them, the GPU-centric approach lets GPU threads directly initiate NVMe requests, eliminating CPU intervention overhead over traditional methods. However, the SOTA GPU-centric approach adopts a synchronous IO model, and threads must tolerate the long latency in communication before starting any tasks.   In this work, we propose AGILE, a lightweight and efficient asynchronous library allowing GPU threads to access SSDs asynchronously while eliminating deadlock risks. AGILE also integrates a flexible software cache using GPU High-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric IO achieves up to 1.88$\times$ improvement in workloads with different computation-to-communication (CTC) ratios. We also compare AGILE with the SOTA work BaM on Deep Learning Recommendation Models (DLRM) with various settings, and the results show that AGILE achieves 1.75$\times$ performance improvement due to its efficient design and the overlapping strategy enabled by an asynchronous IO model. We further evaluate AGILE's API overhead on graph applications, and the results demonstrate AGILE reduces software cache overhead by up to 3.12$\times$ and overhead in NVMe IO requests by up to 2.85$\times$. Compared with BaM, AGILE consumes fewer registers and exhibits up to 1.32$\times$ reduction in the usage of registers.

**Link**: [arxiv](http://arxiv.org/abs/2504.19365v1),  [pdf](http://arxiv.org/pdf/2504.19365v1)

**Tags**: cs.DC 



### OpenFusion++: An Open-vocabulary Real-time Scene Understanding System
**Authors**: Xiaofeng Jin, Matteo Frosi, Matteo Matteucci

**Updated**: 2025-04-27T14:46:43Z

**Summary**: Real-time open-vocabulary scene understanding is essential for efficient 3D perception in applications such as vision-language navigation, embodied intelligence, and augmented reality. However, existing methods suffer from imprecise instance segmentation, static semantic updates, and limited handling of complex queries. To address these issues, we present OpenFusion++, a TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach refines 3D point clouds by fusing confidence maps from foundational models, dynamically updates global semantic labels via an adaptive cache based on instance area, and employs a dual-path encoding framework that integrates object attributes with environmental context for precise query responses. Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate that OpenFusion++ significantly outperforms the baseline in both semantic accuracy and query responsiveness.

**Link**: [arxiv](http://arxiv.org/abs/2504.19266v1),  [pdf](http://arxiv.org/pdf/2504.19266v1)

**Tags**: cs.CV 68T45, 68U05 I.2.10; I.4.8 



### WuNeng: Hybrid State with Attention
**Authors**: Liu Xiao, Li Zhiyuan, Lin Yueyu

**Updated**: 2025-04-27T10:48:56Z

**Summary**: The WuNeng architecture introduces a novel approach to enhancing the expressivity and power of large language models by integrating recurrent neural network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing heightened contextual coherence over reducing KV cache size. Building upon the hybrid-head concept from Hymba, WuNeng augments standard multi-head attention with additional RWKV-7 state-driven heads, rather than replacing existing heads, to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among standard, state-driven, and newly introduced middle heads, leveraging concatenation, additive modulation, and gated fusion for robust information integration. Furthermore, a multi-token state processing mechanism harnesses the continuous RWKV-7 state to capture intricate, sequence-wide dependencies, significantly boosting expressivity. Remarkably, these enhancements are achieved with minimal additional parameters, ensuring efficiency while empowering the model to excel in complex reasoning and sequence generation tasks. WuNeng sets a new standard for balancing expressivity and computational efficiency in modern neural architectures.

**Link**: [arxiv](http://arxiv.org/abs/2504.19191v1),  [pdf](http://arxiv.org/pdf/2504.19191v1)

**Tags**: cs.CL 



### I Know What You Sync: Covert and Side Channel Attacks on File Systems   via syncfs
**Authors**: Cheng Gu, Yicheng Zhang, Nael Abu-Ghazaleh

**Updated**: 2025-04-26T12:07:35Z

**Summary**: Operating Systems enforce logical isolation using abstractions such as processes, containers, and isolation technologies to protect a system from malicious or buggy code. In this paper, we show new types of side channels through the file system that break this logical isolation. The file system plays a critical role in the operating system, managing all I/O activities between the application layer and the physical storage device. We observe that the file system implementation is shared, leading to timing leakage when using common I/O system calls. Specifically, we found that modern operating systems take advantage of any flush operation (which saves cached blocks in memory to the SSD or disk) to flush all of the I/O buffers, even those used by other isolation domains. Thus, by measuring the delay of syncfs, the attacker can infer the I/O behavior of victim programs. We then demonstrate a syncfs covert channel attack on multiple file systems, including both Linux native file systems and the Windows file system, achieving a maximum bandwidth of 5 Kbps with an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on Windows. In addition, we construct three side-channel attacks targeting both Linux and Android devices. On Linux devices, we implement a website fingerprinting attack and a video fingerprinting attack by tracking the write patterns of temporary buffering files. On Android devices, we design an application fingerprinting attack that leaks application write patterns during boot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally, we demonstrate that these attacks can be exploited across containers implementing a container detection technique and a cross-container covert channel attack.

**Link**: [arxiv](http://arxiv.org/abs/2411.10883v2),  [pdf](http://arxiv.org/pdf/2411.10883v2)

**Tags**: cs.CR 



### ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM   Inference
**Authors**: Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen

**Updated**: 2025-04-25T19:40:54Z

**Summary**: With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.

**Link**: [arxiv](http://arxiv.org/abs/2410.21465v3),  [pdf](http://arxiv.org/pdf/2410.21465v3)

**Tags**: cs.LG cs.CL 



### Constructing Hamiltonian Decompositions of Complete $k$-Uniform   Hypergraphs
**Authors**: Javad Maheri, Petros Elia

**Updated**: 2025-04-25T15:45:36Z

**Summary**: Motivated by the wide-ranging applications of Hamiltonian decompositions in distributed computing, coded caching, routing, resource allocation, load balancing, and fault tolerance, our work presents a comprehensive design for Hamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$. Building upon the resolution of the long-standing conjecture of the existence of Hamiltonian decompositions of complete hypergraphs, a problem that was resolved using existence-based methods, our contribution goes beyond the previous explicit designs, which were confined to the specific cases of $k=2$ and $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing for a broad applicability of Hamiltonian decompositions in various settings.

**Link**: [arxiv](http://arxiv.org/abs/2504.18434v1),  [pdf](http://arxiv.org/pdf/2504.18434v1)

**Tags**: cs.IT math.IT 



### FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack
**Authors**: Xuzheng Chen, Jie Zhang, Baolin Zhu, Xueying Zhu, Zhongqing Chen, Shu Ma, Lingjun Zhu, Chao Shi, Yin Zhang, Zeke Wang

**Updated**: 2025-04-25T15:44:38Z

**Summary**: As the gap between network and CPU speeds rapidly increases, the CPU-centric network stack proves inadequate due to excessive CPU and memory overhead. While hardware-offloaded network stacks alleviate these issues, they suffer from limited flexibility in both control and data planes. Offloading network stack to off-path SmartNIC seems promising to provide high flexibility; however, throughput remains constrained by inherent SmartNIC architectural limitations.   To this end, we design FlexiNS, a SmartNIC-centric network stack with software transport programmability and line-rate packet processing capabilities. To grapple with the limitation of SmartNIC-induced challenges, FlexiNS introduces: (a) a header-only offloading TX path; (b) an unlimited-working-set in-cache processing RX path; (c) a high-performance DMA-only notification pipe; and (d) a programmable offloading engine. We prototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box RDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\times$ higher throughput than the microkernel-based baseline in block storage disaggregation and 1.3$\times$ higher throughput than the hardware-offloaded baseline in KVCache transfer.

**Link**: [arxiv](http://arxiv.org/abs/2504.18432v1),  [pdf](http://arxiv.org/pdf/2504.18432v1)

**Tags**: cs.NI 



### Demand Private Coded Caching: Small Cache Size
**Authors**: Qinyi Lu, Nan Liu, Wei Kang, Chunguo Li

**Updated**: 2025-04-25T10:43:23Z

**Summary**: We investigate the demand private coded caching problem, which is an $(N,K)$ coded caching problem with $N$ files, $K$ users, each equipped with a cache of size $M$, and an additional privacy constraint on user demands, i.e., each user can not gain any information about the demands of other users. We focus on scenarios where the size of users' caches is small, aiming to further characterize the fundamental limits of this problem. We first present a new virtual-user-based achievable scheme for arbitrary number of users and files, and two MDS-code-based achievable schemes for the case $N \le K$. With a newly derived converse bound for the case $N \le K$, these proposed schemes lead to the optimal memory-rate tradeoff of the demand private coded caching problem for $M \in \big[0, \frac{N}{(K+1)(N-1)} \big] $ where $N \le K \le 2N-2$, and the optimal memory-rate tradeoff for $M \in \big[0, \frac{1}{K+1} \big] $ where $ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users, by deriving another new converse bound, the optimal memory-rate tradeoff is characterized for $M\in \big[0,\frac{2}{K}\big] \cup \big[\frac{2(K-1)}{K+1},2\big]$. Finally, we provide the optimal memory-rate tradeoff of the demand private coded caching problem for 2 files and 3 users.

**Link**: [arxiv](http://arxiv.org/abs/2504.18242v1),  [pdf](http://arxiv.org/pdf/2504.18242v1)

**Tags**: cs.IT math.IT 



### Efficient GNN Training Through Structure-Aware Randomized Mini-Batching
**Authors**: Vignesh Balaji, Christos Kozyrakis, Gal Chechik, Haggai Maron

**Updated**: 2025-04-25T05:16:53Z

**Summary**: Graph Neural Networks (GNNs) enable learning on realworld graphs and mini-batch training has emerged as the de facto standard for training GNNs because it can scale to very large graphs and improve convergence. Current mini-batch construction policies largely ignore efficiency considerations of GNN training. Specifically, existing mini-batching techniques employ randomization schemes to improve accuracy and convergence. However, these randomization schemes are often agnostic to the structural properties of the graph (for eg. community structure), resulting in highly irregular memory access patterns during GNN training that make suboptimal use of on-chip GPU caches. On the other hand, while deterministic mini-batching based solely on graph structure delivers fast runtime performance, the lack of randomness compromises both the final model accuracy and training convergence speed. In this paper, we present Community-structure-aware Randomized Mini-batching (COMM-RAND), a novel methodology that bridges the gap between the above extremes. COMM-RAND allows practitioners to explore the space between pure randomness and pure graph structural awareness during mini-batch construction, leading to significantly more efficient GNN training with similar accuracy. We evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an accuracy that is within 1.79% points (0.42% on average) compared to popular random mini-batching approaches.

**Link**: [arxiv](http://arxiv.org/abs/2504.18082v1),  [pdf](http://arxiv.org/pdf/2504.18082v1)

**Tags**: cs.LG cs.AI 



### Optimizing ML Concurrent Computation and Communication with GPU DMA   Engines
**Authors**: Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam

**Updated**: 2025-04-25T05:08:45Z

**Summary**: Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). That is, C3 on average achieves only 21% of ideal speedup. This is so, due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).   To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build concurrent communication collectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2412.14335v2),  [pdf](http://arxiv.org/pdf/2412.14335v2)

**Tags**: cs.AR cs.DC 



### Fluctuated lattice-driven charge density wave far above the condensation   temperature in kagome superconductor KV$_3$Sb$_5$
**Authors**: Haoran Liu, Shaofeng Duan, Xiangqi Liu, Zhihua Liu, Shichong Wang, Lingxiao Gu, Jiongyu Huang, Wenxuan Yang, Jianzhe Liu, Dong Qian, Yanfeng Guo, Wentao Zhang

**Updated**: 2025-04-25T05:05:49Z

**Summary**: The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including an unconventional charge density wave (CDW). Elucidating the underlying mechanism behind the CDW transition is crucial for unraveling the complex interactions among these phases. However, the driving force of the CDW remains a topic of debate due to the intertwined interactions among the system's various excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by isolating the ultrafast electronic phase transition using time- and angleresolved photoemission spectroscopy. An ultrafast electronic phase transition was observed at a critical photoexcitation fluence, F$_c$, without reduction in CDW lattice-distortion-induced band folding. This folded band persisted up to 150 K under equilibrium heating, well above the CDW condensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts at F$_c$ were comparable to those caused by thermal effects at T$_c$. These findings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges above 150 K, with out-of-plane electronic correlations leading to the $2\times2 \times 2$ CDW near T$_c$, offering key insights into the interplay between the electronic and structural dynamics in AV$_3$Sb$_5$.

**Link**: [arxiv](http://arxiv.org/abs/2504.16620v2),  [pdf](http://arxiv.org/pdf/2504.16620v2)

**Tags**: cond-mat.str-el cond-mat.mtrl-sci cond-mat.supr-con 



### Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A   first-principles DFT+$U$+$V$ study
**Authors**: Indukuru Ramesh Reddy, Sayandeep Ghosh, Bongjae Kim, Chang-Jong Kang

**Updated**: 2025-04-25T00:41:43Z

**Summary**: Nonlocal Coulomb interactions play a crucial role in stabilizing distinct electronic phases in kagome materials. In this work, we systematically investigate the effects of on-site ($U$) and inter-site ($V$) Coulomb interactions on the electronic structure and stability of charge-density-wave (CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory (DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and stability of CDW phases, whereas $U$ suppresses these phases, highlighting a fundamental competition between local and nonlocal electronic correlations. By directly comparing our theoretical results with angle-resolved photoemission spectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that accurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings establish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable insights into the correlated electronic states in kagome metals and serving as a foundation for future explorations of correlation-driven phenomena in related materials.

**Link**: [arxiv](http://arxiv.org/abs/2504.17995v1),  [pdf](http://arxiv.org/pdf/2504.17995v1)

**Tags**: cond-mat.str-el 



### Updated parameters of the LArQL model
**Authors**: L. Paulucci, F. Cavanna, V. Vale, F. Marinho

**Updated**: 2025-04-24T18:09:25Z

**Summary**: The need for a microscopic description of scintillation light generation in liquid argon becomes increasingly desirable with the upcoming operation of large scale LArTPCs in the next decade. While a detailed mathematical account of the process is still to be achieved, a phenomenological model for simultaneously treating ionization and scintillation, LArQL, has been successfully employed to describe the range of electric fields from 0 to 0.75 kV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the free ionization charge and scintillation light. A reanalysis of the original model parameter values has been performed within a global fit procedure and is presented.

**Link**: [arxiv](http://arxiv.org/abs/2504.17866v1),  [pdf](http://arxiv.org/pdf/2504.17866v1)

**Tags**: hep-ex 



### L3: DIMM-PIM Integrated Architecture and Coordination for Scalable   Long-Context LLM Inference
**Authors**: Qingyuan Liu, Liyan Chen, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen

**Updated**: 2025-04-24T14:14:07Z

**Summary**: Large Language Models (LLMs) increasingly require processing long text sequences, but GPU memory limitations force difficult trade-offs between memory capacity and bandwidth. While HBM-based acceleration offers high bandwidth, its capacity remains constrained. Offloading data to host-side DIMMs improves capacity but introduces costly data swapping overhead. We identify that the critical memory bottleneck lies in the decoding phase of multi-head attention (MHA) exclusively, which demands substantial capacity for storing KV caches and high bandwidth for attention computation. Our key insight reveals this operation uniquely aligns with modern DIMM-based processing-in-memory (PIM) architectures, which offers scalability of both capacity and bandwidth.   Based on this observation and insight, we propose L3, a hardware-software co-designed system integrating DIMM-PIM and GPU devices. L3 introduces three innovations: First, hardware redesigns resolve data layout mismatches and computational element mismatches in DIMM-PIM, enhancing LLM inference utilization. Second, communication optimization enables hiding the data transfer overhead with the computation. Third, an adaptive scheduler coordinates GPU-DIMM-PIM operations to maximize parallelism between devices. Evaluations using real-world traces show L3 achieves up to 6.1$\times$ speedup over state-of-the-art HBM-PIM solutions while significantly improving batch sizes.

**Link**: [arxiv](http://arxiv.org/abs/2504.17584v1),  [pdf](http://arxiv.org/pdf/2504.17584v1)

**Tags**: cs.AR cs.LG 



### Rethinking PM Crash Consistency in the CXL Era
**Authors**: João Oliveira, João Gonçalves, Miguel Matos

**Updated**: 2025-04-24T13:47:35Z

**Summary**: Persistent Memory (PM) introduces new opportunities for designing crash-consistent applications without the traditional storage overheads. However, ensuring crash consistency in PM demands intricate knowledge of CPU, cache, and memory interactions. Hardware and software mechanisms have been proposed to ease this burden, but neither proved sufficient, prompting a variety of bug detection tools.   With the sunset of Intel Optane comes the rise of Compute Express Link (CXL) for PM. In this position paper, we discuss the impact of CXL's disaggregated and heterogeneous nature in the development of crash-consistent PM applications, and outline three research directions: hardware primitives, persistency frameworks, and bug detection tools.

**Link**: [arxiv](http://arxiv.org/abs/2504.17554v1),  [pdf](http://arxiv.org/pdf/2504.17554v1)

**Tags**: cs.ET 



### Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent   Interconnects
**Authors**: Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe

**Updated**: 2025-04-24T08:39:13Z

**Summary**: Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA) to offload data transfer, descriptor rings for buffering and queuing, and interrupts for asynchrony between cores and device.   In this paper we question this wisdom in the light of two trends: modern and emerging cache-coherent interconnects like CXL3.0, and workloads, particularly microservices and serverless computing. Like some others before us, we argue that the assumptions of the DMA-based model are obsolete, and in many use-cases programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, delivers a more efficient system.   However, we push this idea much further. We show, in a real hardware implementation, the gains in latency for fine-grained communication achievable using an open cache-coherence protocol which exposes cache transitions to a smart device, and that throughput is competitive with DMA over modern interconnects. We also demonstrate three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting serverless functions, comparing our use of coherence with both traditional DMA-style interaction and a highly-optimized implementation using memory-mapped programmed I/O over PCIe.

**Link**: [arxiv](http://arxiv.org/abs/2409.08141v3),  [pdf](http://arxiv.org/pdf/2409.08141v3)

**Tags**: cs.AR cs.OS 



### SPAARC: Spatial Proximity and Association based prefetching for   Augmented Reality in edge Cache
**Authors**: Nikhil Sreekumar, Abhishek Chandra, Jon Weissman

**Updated**: 2025-04-24T04:36:20Z

**Summary**: Mobile Augmented Reality (MAR) applications face performance challenges due to their high computational demands and need for low-latency responses. Traditional approaches like on-device storage or reactive data fetching from the cloud often result in limited AR experiences or unacceptable lag. Edge caching, which caches AR objects closer to the user, provides a promising solution. However, existing edge caching approaches do not consider AR-specific features such as AR object sizes, user interactions, and physical location. This paper investigates how to further optimize edge caching by employing AR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and Association-based Prefetching policy specifically designed for MAR Caches. SPAARC intelligently prioritizes the caching of virtual objects based on their association with other similar objects and the user's proximity to them. It also considers the recency of associations and uses a lazy fetching strategy to efficiently manage edge resources and maximize Quality of Experience (QoE).   Through extensive evaluation using both synthetic and real-world workloads, we demonstrate that SPAARC significantly improves cache hit rates compared to standard caching algorithms, achieving gains ranging from 3% to 40% while reducing the need for on-demand data retrieval from the cloud. Further, we present an adaptive tuning algorithm that automatically tunes SPAARC parameters to achieve optimal performance. Our findings demonstrate the potential of SPAARC to substantially enhance the user experience in MAR applications by ensuring the timely availability of virtual objects.

**Link**: [arxiv](http://arxiv.org/abs/2502.15192v2),  [pdf](http://arxiv.org/pdf/2502.15192v2)

**Tags**: cs.ET cs.DC 



### Efficient Pretraining Length Scaling
**Authors**: Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou

**Updated**: 2025-04-24T04:13:49Z

**Summary**: Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.14992v2),  [pdf](http://arxiv.org/pdf/2504.14992v2)

**Tags**: cs.CL 



### Cognitive Memory in Large Language Models
**Authors**: Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu

**Updated**: 2025-04-24T01:47:25Z

**Summary**: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.

**Link**: [arxiv](http://arxiv.org/abs/2504.02441v2),  [pdf](http://arxiv.org/pdf/2504.02441v2)

**Tags**: cs.CL cs.AI 



### KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM   Inference in Resource-Constrained Environments
**Authors**: Junyoung Park, Dalton Jones, Matt J Morse, Raghavv Goel, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T18:02:55Z

**Summary**: In this work, we demonstrate that distinctive keys during LLM inference tend to have high attention scores. We explore this phenomenon and propose KeyDiff, a training-free KV cache eviction method based on key similarity. This method facilitates the deployment of LLM-based application requiring long input prompts in resource-constrained environments with limited memory and compute budgets. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses. We demonstrate that KeyDiff computes the optimal solution to a KV cache selection problem that maximizes key diversity, providing a theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse tasks and models, illustrating a performance gap of less than 0.04\% with 8K cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

**Link**: [arxiv](http://arxiv.org/abs/2504.15364v2),  [pdf](http://arxiv.org/pdf/2504.15364v2)

**Tags**: cs.AI 



### Iris: A Next Generation Digital Pathology Rendering Engine
**Authors**: Ryan Erik Landvater, Ulysses Balis

**Updated**: 2025-04-23T15:02:16Z

**Summary**: Digital pathology is a tool of rapidly evolving importance within the discipline of pathology. Whole slide imaging promises numerous advantages; however, adoption is limited by challenges in ease of use and speed of high-quality image rendering relative to the simplicity and visual quality of glass slides. We introduce Iris, a new high-performance digital pathology rendering system. Specifically, we outline and detail the performance metrics of Iris Core, the core rendering engine technology. Iris Core comprises machine code modules written from the ground up in C++ and using Vulkan, a low-level and low-overhead cross-platform graphical processing unit application program interface, and our novel rapid tile buffering algorithms. We provide a detailed explanation of Iris Core's system architecture, including the stateless isolation of core processes, interprocess communication paradigms, and explicit synchronization paradigms that provide powerful control over the graphical processing unit. Iris Core achieves slide rendering at the sustained maximum frame rate on all tested platforms and buffers an entire new slide field of, view without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is able to buffer and compute high-fidelity reduction-enhancements for viewing low-power cytology with increased visual quality at a rate of 100-160 us per slide tile, and with a cumulative median buffering rate of 1.36 GB of decompressed image data per second. This buffering rate allows for an entirely new field of view to be fully buffered and rendered in less than a single monitor refresh on a standard display, and high detail features within 2-3 monitor refresh frames. These metrics far exceed previously published specifications, beyond an order of magnitude in some contexts. The system shows no slowing with high use loads, but rather increases performance due to cache mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2504.15437v2),  [pdf](http://arxiv.org/pdf/2504.15437v2)

**Tags**: cs.GR 



### The NIC should be part of the OS
**Authors**: Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-04-23T10:48:52Z

**Summary**: The network interface adapter (NIC) is a critical component of a cloud server occupying a unique position. Not only is network performance vital to efficient operation of the machine, but unlike compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency.   Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path.   However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernelbypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.10138v2),  [pdf](http://arxiv.org/pdf/2501.10138v2)

**Tags**: cs.OS cs.AR cs.NI 



### CAOTE: KV Caching through Attention Output Error based Token Eviction
**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T05:04:58Z

**Summary**: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.

**Link**: [arxiv](http://arxiv.org/abs/2504.14051v2),  [pdf](http://arxiv.org/pdf/2504.14051v2)

**Tags**: cs.LG 



### ML-based Adaptive Prefetching and Data Placement for US HEP Systems
**Authors**: Venkat Sai Suman Lamba Karanam, Sarat Sasank Barla, Byrav Ramamurthy, Derek Weitzel

**Updated**: 2025-04-23T04:21:49Z

**Summary**: Although benefits from caching in US HEP are well-known, current caching strategies are not adaptive i.e they do not adapt to changing cache access patterns. Newer developments such as the High-Luminosity - Large Hadron Collider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward streaming readout based Data Acquisition systems (DAQs) will increase the data production exponentially and hence burden the storage, compute & network infrastructures. Moreover, existing caching frameworks are optimized to reduce latency, but not optimized for storage. This, in combination with limited cache capacities relative to total data, makes it difficult to achieve data locality.   In this work, we present Machine Learning-aided (ML) caching strategies. Specifically, we first present a Long Short-Term Memory-based (LSTM) hourly and multi-step cache usage prediction. Second, we present an hourly file-level access prediction model based on CatboostRegressor. To date, most ML-based cache prediction strategies in HEP have focused on daily cache usage and limited works tackled hourly cache usage and even fewer strategies addressed hourly file-level access prediction. File-level access prediction allows for the design of intelligent prefetching and data placement strategies with fine-grained control. We validated our cache prediction strategies using data collected from SoCal MINI caches in August 2024. We are currently extending the WRENCH simulator to reflect the US HEP ecosystem at the storage, network and compute levels. We plan to deploy our cache prediction strategies into WRENCH and later perform extensive analysis with complex data access patterns and candidate infrastructure configurations.

**Link**: [arxiv](http://arxiv.org/abs/2503.06015v2),  [pdf](http://arxiv.org/pdf/2503.06015v2)

**Tags**: cs.DC 



### The Dawn of Disaggregation and the Coherence Conundrum: A Call for   Federated Coherence
**Authors**: Jaewan Hong, Marcos K. Aguilera, Emmanuel Amaro, Vincent Liu, Aurojit Panda, Ion Stoica

**Updated**: 2025-04-22T23:52:13Z

**Summary**: Disaggregated memory is an upcoming data center technology that will allow nodes (servers) to share data efficiently. Sharing data creates a debate on the level of cache coherence the system should provide. While current proposals aim to provide coherence for all or parts of the disaggregated memory, we argue that this approach is problematic, because of scalability limitations and hardware complexity. Instead, we propose and formally define federated coherence, a model that provides coherence only within nodes, not across nodes. Federated coherence can use current intra-node coherence provided by processors without requiring expensive mechanisms for inter-node coherence. Developers can use federated coherence with a few simple programming paradigms and a synchronization library. We sketch some potential applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16324v1),  [pdf](http://arxiv.org/pdf/2504.16324v1)

**Tags**: cs.DC cs.AR 



### Key, Value, Compress: A Systematic Exploration of KV Cache Compression   Techniques
**Authors**: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

**Updated**: 2025-04-22T17:34:34Z

**Summary**: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.

**Link**: [arxiv](http://arxiv.org/abs/2503.11816v3),  [pdf](http://arxiv.org/pdf/2503.11816v3)

**Tags**: cs.CL 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hoßfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-04-22T17:23:28Z

**Summary**: As AI workloads drive soaring memory requirements, there is a need for higher-density on-chip memory for domain-specific accelerators that goes beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, there has been little work in factoring dynamic application profiles into such design decisions. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and computes data lifetimes in domain-specific accelerators. By combining instrumentation and simulation across retargetable hardware backends, GainSight aligns heterogeneous memory designs with workload-specific traffic and lifetime metrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA H100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3) Up to 90% of GPU cache fetches are never reused, highlighting inefficiencies in terms of cache pollution. These insights that GainSight provides can be used to better understand the design spaces of both emerging on-chip memories and software algorithmic optimizations for the next generation of AI accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v2),  [pdf](http://arxiv.org/pdf/2504.14866v2)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### Optimizing SLO-oriented LLM Serving with PD-Multiplexing
**Authors**: Weihao Cui, Yukang Chen, Han Zhao, Ziyi Xu, Quan Chen, Xusheng Chen, Yangjie Zhou, Shixuan Sun, Minyi Guo

**Updated**: 2025-04-22T15:19:48Z

**Summary**: Modern LLM services demand high throughput and stringent SLO guarantees across two distinct inference phases-prefill and decode-and complex multi-turn workflows. However, current systems face a fundamental tradeoff: out-of-place compute partition enables per-phase SLO attainment, while in-place memory sharing maximizes throughput via KV cache reuse. Moreover, existing in-place compute partition also encounters low utilization and high overhead due to phase-coupling design. We present Drift, a new LLM serving framework that resolves this tension via PD multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs, while preserving in-place memory sharing. To fully leverage the multiplexing capability, Drift introduces an adaptive gang scheduling mechanism, a contention-free modeling method, and a SLO-aware dispatching policy. Evaluation shows that Drift achieves an average $5.1\times$ throughput improvement (up to $17.5\times$) over state-of-the-art baselines, while consistently meeting SLO targets under complex LLM workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.14489v2),  [pdf](http://arxiv.org/pdf/2504.14489v2)

**Tags**: cs.OS 



### SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large   Language Model Inference
**Authors**: Yihao Zhao, Jiadun Chen, Peng Sun, Lei Li, Xuanzhe Liu, Xin Jin

**Updated**: 2025-04-22T09:08:46Z

**Summary**: Large language models (LLMs) with different architectures and sizes have been developed. Serving each LLM with dedicated GPUs leads to resource waste and service inefficiency due to the varying demand of LLM requests. A common practice is to share multiple LLMs. However, existing sharing systems either do not consider the autoregressive pattern of LLM services, or only focus on improving the throughput, which impairs the sharing performance, especially the serving latency. We present SeaLLM, which enables service-aware and latency-optimized LLM sharing. SeaLLM improves the overall sharing performance by (1) a latency-optimized scheduling algorithm utilizing the characteristics of LLM services, (2) a placement algorithm to determine the placement plan and an adaptive replacement algorithm to decide the replacement interval, and (3) a unified key-value cache to share GPU memory among LLM services efficiently. Our evaluation under real-world traces and LLM services demonstrates that SeaLLM improves the normalized latency by up to $13.60\times$, the tail latency by up to $18.69\times$, and the SLO attainment by up to $3.64\times$ compared to existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2504.15720v1),  [pdf](http://arxiv.org/pdf/2504.15720v1)

**Tags**: cs.DC 



### Reimagining Memory Access for LLM Inference: Compression-Aware Memory   Controller Design
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-04-21T22:13:07Z

**Summary**: The efficiency of Large Language Model~(LLM) inference is often constrained by substantial memory bandwidth and capacity demands. Existing techniques, such as pruning, quantization, and mixture of experts/depth, reduce memory capacity and/or bandwidth consumption at the cost of slight degradation in inference quality. This paper introduces a design solution that further alleviates memory bottlenecks by enhancing the on-chip memory controller in AI accelerators to achieve two main objectives: (1) significantly reducing memory capacity and bandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of model weights and key-value (KV) cache without compromising inference quality, and (2) enabling memory bandwidth and energy consumption to scale proportionally with context-dependent dynamic quantization. These goals are accomplished by equipping the on-chip memory controller with mechanisms to improve fine-grained bit-level accessibility and compressibility of weights and KV cache through LLM-aware configuration of in-memory placement and representation. Experimental results on publicly available LLMs demonstrate the effectiveness of this approach, showing memory footprint reductions of 25.2\% for model weights and 46.9\% for KV cache. In addition, our hardware prototype at 4\,GHz and 32 lanes (7\,nm) achieves 8\,TB/s throughput with a modest area overhead (under 3.8\,mm\(^2\)), which underscores the viability of LLM-aware memory control as a key to efficient large-scale inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.18869v3),  [pdf](http://arxiv.org/pdf/2503.18869v3)

**Tags**: cs.AR 



### FlashInfer: Efficient and Customizable Attention Engine for LLM   Inference Serving
**Authors**: Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze

**Updated**: 2025-04-21T20:10:11Z

**Summary**: Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01005v2),  [pdf](http://arxiv.org/pdf/2501.01005v2)

**Tags**: cs.DC cs.AI cs.LG 



### Joint Knowledge and Power Management for Secure Semantic Communication   Networks
**Authors**: Xuesong Liu, Yansong Liu, Haoyu Tang, Fangzhou Zhao, Le Xia, Yao Sun

**Updated**: 2025-04-21T17:39:59Z

**Summary**: Recently, semantic communication (SemCom) has shown its great superiorities in resource savings and information exchanges. However, while its unique background knowledge guarantees accurate semantic reasoning and recovery, semantic information security-related concerns are introduced at the same time. Since the potential eavesdroppers may have the same background knowledge to accurately decrypt the private semantic information transmitted between legal SemCom users, this makes the knowledge management in SemCom networks rather challenging in joint consideration with the power control. To this end, this paper focuses on jointly addressing three core issues of power allocation, knowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in secure SemCom networks. We first develop a novel performance metric, namely semantic secrecy throughput (SST), to quantify the information security level that can be achieved at each pair of D2D SemCom users. Next, an SST maximization problem is formulated subject to secure SemCom-related delay and reliability constraints. Afterward, we propose a security-aware resource management solution using the Lagrange primal-dual method and a two-stage method. Simulation results demonstrate our proposed solution nearly doubles the SST performance and realizes less than half of the queuing delay performance compared to different benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.15260v1),  [pdf](http://arxiv.org/pdf/2504.15260v1)

**Tags**: eess.SP 



### Lance: Efficient Random Access in Columnar Storage through Adaptive   Structural Encodings
**Authors**: Weston Pace, Chang She, Lei Xu, Will Jones, Albert Lockett, Jun Wang, Raunak Shah

**Updated**: 2025-04-21T17:22:18Z

**Summary**: The growing interest in artificial intelligence has created workloads that require both sequential and random access. At the same time, NVMe-backed storage solutions have emerged, providing caching capability for large columnar datasets in cloud storage. Current columnar storage libraries fall short of effectively utilizing an NVMe device's capabilities, especially when it comes to random access. Historically, this has been assumed an implicit weakness in columnar storage formats, but this has not been sufficiently explored. In this paper, we examine the effectiveness of popular columnar formats such as Apache Arrow, Apache Parquet, and Lance in both random access and full scan tasks against NVMe storage.   We argue that effective encoding of a column's structure, such as the repetition and validity information, is the key to unlocking the disk's performance. We show that Parquet, when configured correctly, can achieve over 60x better random access performance than default settings. We also show that this high random access performance requires making minor trade-offs in scan performance and RAM utilization. We then describe the Lance structural encoding scheme, which alternates between two different structural encodings based on data width, and achieves better random access performance without making trade-offs in scan performance or RAM utilization.

**Link**: [arxiv](http://arxiv.org/abs/2504.15247v1),  [pdf](http://arxiv.org/pdf/2504.15247v1)

**Tags**: cs.DB H.3.2 



### A Unified Framework for Quantitative Cache Analysis
**Authors**: Sophie Kahlen, Jan Reineke

**Updated**: 2025-04-21T15:36:53Z

**Summary**: In this work we unify two existing lines of work towards cache analysis for non-LRU policies. To this end, we extend the notion of competitiveness to block competitiveness and systematically analyze the competitiveness and block competitiveness of FIFO and MRU relative to LRU for arbitrary associativities. We show how competitiveness and block competitiveness can be exploited in state-of-the-art WCET analysis based on the results of existing persistence analyses for LRU. Unlike prior work, our approach is applicable to microarchitectures that exhibit timing anomalies. We experimentally evaluate the precision and cost of our approach on benchmarks from TACLeBench. The experiments demonstrate that quantitative cache analysis for FIFO and MRU comes close to the precision of LRU.

**Link**: [arxiv](http://arxiv.org/abs/2503.16588v3),  [pdf](http://arxiv.org/pdf/2503.16588v3)

**Tags**: cs.PL 68 D.3.4 



### LServe: Efficient Long-sequence LLM Serving with Unified Sparse   Attention
**Authors**: Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han

**Updated**: 2025-04-21T15:13:44Z

**Summary**: Large language models (LLMs) have shown remarkable potential in processing long sequences and complex reasoning tasks, yet efficiently serving these models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context and reasoning capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.

**Link**: [arxiv](http://arxiv.org/abs/2502.14866v2),  [pdf](http://arxiv.org/pdf/2502.14866v2)

**Tags**: cs.CL cs.AI cs.DC cs.LG cs.PF 



### Is Intelligence the Right Direction in New OS Scheduling for Multiple   Resources in Cloud Environments?
**Authors**: Xinglei Dou, Lei Liu, Limin Xiao

**Updated**: 2025-04-21T11:09:43Z

**Summary**: Making it intelligent is a promising way in System/OS design. This paper proposes OSML+, a new ML-based resource scheduling mechanism for co-located cloud services. OSML+ intelligently schedules the cache and main memory bandwidth resources at the memory hierarchy and the computing core resources simultaneously. OSML+ uses a multi-model collaborative learning approach during its scheduling and thus can handle complicated cases, e.g., avoiding resource cliffs, sharing resources among applications, enabling different scheduling policies for applications with different priorities, etc. OSML+ can converge faster using ML models than previous studies. Moreover, OSML+ can automatically learn on the fly and handle dynamically changing workloads accordingly. Using transfer learning technologies, we show our design can work well across various cloud servers, including the latest off-the-shelf large-scale servers. Our experimental results show that OSML+ supports higher loads and meets QoS targets with lower overheads than previous studies.

**Link**: [arxiv](http://arxiv.org/abs/2504.15021v1),  [pdf](http://arxiv.org/pdf/2504.15021v1)

**Tags**: cs.DC cs.LG cs.PF 



### Context Parallelism for Scalable Million-Token Inference
**Authors**: Amy Yang, Jingyi Yang, Aya Ibrahim, Xinfeng Xie, Bangsheng Tang, Grigory Sizov, Jeremy Reizenstein, Jongsoo Park, Jianyu Huang

**Updated**: 2025-04-21T03:40:10Z

**Summary**: We present context parallelism for long-context large language model inference, which achieves near-linear scaling for long-context prefill latency with up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M context prefill with Llama3 405B model in 77s (93% parallelization efficiency, 63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two lossless exact ring attention variants: pass-KV and pass-Q to cover a wide range of use cases with the state-of-the-art performance: full prefill, persistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected with RDMA and TCP both show similar scalability for long-context prefill, demonstrating that our method scales well using common commercial data center with medium-to-low inter-host bandwidth.

**Link**: [arxiv](http://arxiv.org/abs/2411.01783v3),  [pdf](http://arxiv.org/pdf/2411.01783v3)

**Tags**: cs.DC cs.AI cs.LG 



### gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM   Serving with Token Throttling
**Authors**: Tianyu Guo, Xianwei Zhang, Jiangsu Du, Zhiguang Chen, Nong Xiao, Yutong Lu

**Updated**: 2025-04-21T00:07:49Z

**Summary**: Pipeline parallelism has emerged as a predominant approach for deploying large language models (LLMs) across distributed nodes, owing to its lower communication overhead compared to tensor parallelism. While demonstrating high throughput in request serving, pipeline parallelism often suffers from performance limitations caused by pipeline bubbles, which are primarily resulted from imbalanced computation delays across batches. Existing methods like Sarathi-Serve attempt to address this through hybrid scheduling of chunked prefill and decode tokens using a fixed token budget. However, such methods may experience significant fluctuations due to either insufficient prefill tokens or uneven distribution of decode tokens, ultimately leading to computational imbalance. To overcome these inefficiencies, we present gLLM, a globally balanced pipeline parallelism system incorporating Token Throttling to effectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a fine-grained scheduling policy that independently regulates the quantities of prefill and decode tokens, thus enabling balanced computation by leveraging global information from the inference system. Specifically, for decode tokens, gLLM maintains near-consistent token count across processing batches. For prefill tokens, it dynamically adjusts batch sizes based on both total pending tokens and the memory utilization rates of key-value cache (KV cache). Furthermore, gLLM runtime adopts an asynchronous execution and message passing architecture specifically optimized for pipeline parallelism characteristics. Experimental evaluations with representative LLMs show that gLLM achieves significant performance improvements, delivering 11% to 398% higher maximum throughput compared to state-of-the-art pipeline or tensor parallelism systems, while simultaneously maintaining lower latency.

**Link**: [arxiv](http://arxiv.org/abs/2504.14775v1),  [pdf](http://arxiv.org/pdf/2504.14775v1)

**Tags**: cs.DC 



### Star Attention: Efficient LLM Inference over Long Sequences
**Authors**: Shantanu Acharya, Fei Jia, Boris Ginsburg

**Updated**: 2025-04-20T21:50:03Z

**Summary**: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2411.17116v2),  [pdf](http://arxiv.org/pdf/2411.17116v2)

**Tags**: cs.CL cs.AI cs.LG 



### Understanding and Optimizing Multi-Stage AI Inference Pipelines
**Authors**: Abhimanyu Rajeshkumar Bambhaniya, Hanjiang Wu, Suvinay Subramanian, Sudarshan Srinivasan, Souvik Kundu, Amir Yazdanbakhsh, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna

**Updated**: 2025-04-20T19:57:16Z

**Summary**: The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.   To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.09775v3),  [pdf](http://arxiv.org/pdf/2504.09775v3)

**Tags**: cs.AR cs.AI cs.DC cs.LG 



### Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink   of an Eye
**Authors**: Bradley Morgan, Gal Horowitz, Sioli O'Connell, Stephan van Schaik, Chitchanok Chuengsatiansup, Daniel Genkin, Olaf Maennel, Paul Montague, Eyal Ronen, Yuval Yarom

**Updated**: 2025-04-20T07:53:09Z

**Summary**: An essential step for mounting cache attacks is finding eviction sets, collections of memory locations that contend on cache space. On Intel processors, one of the main challenges for identifying contending addresses is the sliced cache design, where the processor hashes the physical address to determine where in the cache a memory location is stored. While past works have demonstrated that the hash function can be reversed, they also showed that it depends on physical address bits that the adversary does not know.   In this work, we make three main contributions to the art of finding eviction sets. We first exploit microarchitectural races to compare memory access times and identify the cache slice to which an address maps. We then use the known hash function to both reduce the error rate in our slice identification method and to reduce the work by extrapolating slice mappings to untested memory addresses. Finally, we show how to propagate information on eviction sets across different page offsets for the hitherto unexplored case of non-linear hash functions.   Our contributions allow for entire LLC eviction set generation in 0.7 seconds on the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear functions. This represents a significant improvement compared to state-of-the-art techniques taking 9x and 10x longer, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2504.11208v2),  [pdf](http://arxiv.org/pdf/2504.11208v2)

**Tags**: cs.CR 



### Deuteronomy 2.0: Record Caching and Latch Freedom
**Authors**: David Lomet

**Updated**: 2025-04-20T00:49:27Z

**Summary**: The Deuteronomy transactional key-value store is unique architecturally in providing separation between transaction functionality -- its Transactional Component (TC) and data management -- its Data Component (DC). It is unique in technology by (1) supporting record caching, a smaller unit than the traditional page; and (2) protecting resources during concurrent execution using a latch-free approach. Both technologies are enabled by delta updating. This paper explains how record caching improves cache cost/performance. It also shows how a new latch-free approach makes implementation easier and improves performance.

**Link**: [arxiv](http://arxiv.org/abs/2504.14435v1),  [pdf](http://arxiv.org/pdf/2504.14435v1)

**Tags**: cs.DB 



### A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated   in a coupled reactive transport HPC simulation
**Authors**: Max Lübke, Marco De Lucia, Stefan Petri, Bettina Schnor

**Updated**: 2025-04-19T18:25:20Z

**Summary**: Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.

**Link**: [arxiv](http://arxiv.org/abs/2504.14374v1),  [pdf](http://arxiv.org/pdf/2504.14374v1)

**Tags**: cs.DC 



### Room-temperature high-average-power strong-field terahertz source based   on industrial high-repetition-rate femtosecond laser
**Authors**: Deyin Kong, Yichen Su, Cheng Song, Xiaojun Wu

**Updated**: 2025-04-19T06:18:56Z

**Summary**: Free-space strong-field terahertz (THz) pulses, generated via optical rectification of femtosecond lasers in nonlinear crystals, are pivotal in various applications. However, conventional Ti:sapphire lasers struggle to produce high-average-power THz due to their limited output power. While kilowatt ytterbium lasers are increasingly adopted, their application in THz generation faces challenges: low optical-to-THz conversion efficiency (attributed to long pulse durations and low energy) and crystal damage under high pumping power. Here, we report a high-average-power strong-field THz source using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ, 50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By systematically optimizing TPFP implementations and comparing grating- and echelon-type configurations, we achieve a THz source with 64.5 mW average power at 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at 0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in cobalt-iron ferromagnetic nanofilms. This high-repetition-rate, high-average-power THz system, combined with its potential capabilities in high signal-to-noise spectroscopy and imaging, promises transformative impacts in quantum matter manipulation, non-destructive testing, and biomedicine.

**Link**: [arxiv](http://arxiv.org/abs/2504.14196v1),  [pdf](http://arxiv.org/pdf/2504.14196v1)

**Tags**: physics.optics 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-04-19T05:57:44Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v5),  [pdf](http://arxiv.org/pdf/2411.10659v5)

**Tags**: cs.PL 



### LogicTree: Structured Proof Exploration for Coherent and Rigorous   Logical Reasoning with Large Language Models
**Authors**: Kang He, Kaushik Roy

**Updated**: 2025-04-18T22:10:02Z

**Summary**: Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average.

**Link**: [arxiv](http://arxiv.org/abs/2504.14089v1),  [pdf](http://arxiv.org/pdf/2504.14089v1)

**Tags**: cs.CL cs.AI cs.LG 



### Gradual Binary Search and Dimension Expansion : A general method for   activation quantization in LLMs
**Authors**: Lucas Maisonnave, Cyril Moineau, Olivier Bichler, Fabrice Rastello

**Updated**: 2025-04-18T13:46:58Z

**Summary**: Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40\% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley algorithm. We theoretically demonstrates the superiority of Hadamard matrices in reducing outliers.We achieved 3-bit quantization for weights, activations, and KV cache, significantly enhancing model performance. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.

**Link**: [arxiv](http://arxiv.org/abs/2504.13989v1),  [pdf](http://arxiv.org/pdf/2504.13989v1)

**Tags**: cs.LG cs.AI cs.CL 



### CacheFormer: High Attention-Based Segment Caching
**Authors**: Sushant Singh, Ausif Mahmood

**Updated**: 2025-04-18T06:34:57Z

**Summary**: Efficiently handling long contexts in transformer-based language models with low perplexity is an active area of research. Numerous recent approaches like Linformer, Longformer, Performer, and Structured state space models (SSMs)., have not fully resolved this problem. All these models strive to reduce the quadratic time complexity of the attention mechanism while minimizing the loss in quality due to the effective compression of the long context. Inspired by the cache and virtual memory principle in computers, where in case of a cache miss, not only the needed data is retrieved from the memory, but the adjacent data is also obtained, we apply this concept to handling long contexts by dividing it into small segments. In our design, we retrieve the nearby segments in an uncompressed form when high segment-level attention occurs at the compressed level. Our en-hancements for handling long context include aggregating four attention mechanisms consisting of short sliding window attention, long compressed segmented attention, dynamically retrieving top k high attention uncompressed segments, and overlapping segments in long segment attention to avoid segment fragmentation. These enhancements result in an architecture that outperforms ex-isting SOTA architectures with an average perplexity improvement of 8.5% over similar model sizes.

**Link**: [arxiv](http://arxiv.org/abs/2504.13981v1),  [pdf](http://arxiv.org/pdf/2504.13981v1)

**Tags**: cs.LG cs.AI 



### Towards Federated Multi-Armed Bandit Learning for Content Dissemination   using Swarm of UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-04-18T05:13:52Z

**Summary**: This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.

**Link**: [arxiv](http://arxiv.org/abs/2501.09146v2),  [pdf](http://arxiv.org/pdf/2501.09146v2)

**Tags**: cs.LG cs.NI I.2.11 



### HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM   Inference via GPU Co-processing
**Authors**: Myunghyun Rhee, Joonseop Sim, Taeyoung Ahn, Seungyong Lee, Daegun Yoon, Euiseok Kim, Kyoung Park, Youngpyo Joo, Hosik Kim

**Updated**: 2025-04-18T03:31:08Z

**Summary**: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2504.16112v1),  [pdf](http://arxiv.org/pdf/2504.16112v1)

**Tags**: cs.AR cs.AI cs.CL cs.DC 



### EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for   Enhanced Cache Occupancy Attacks
**Authors**: Tianhong Xu, Aidong Adam Ding, Yunsi Fei

**Updated**: 2025-04-18T00:21:00Z

**Summary**: Cache occupancy attacks exploit the shared nature of cache hierarchies to infer a victim's activities by monitoring overall cache usage, unlike access-driven cache attacks that focus on specific cache lines or sets. There exists some prior work that target the last-level cache (LLC) of Intel processors, which is inclusive of higher-level caches, and L2 caches of ARM systems. In this paper, we target the System-Level Cache (SLC) of Apple M-series SoCs, which is exclusive to higher-level CPU caches. We address the challenges of the exclusiveness and propose a suite of SLC-cache occupancy attacks, the first of its kind, where an adversary can monitor GPU and other CPU cluster activities from their own CPU cluster. We first discover the structure of SLC in Apple M1 SOC and various policies pertaining to access and sharing through reverse engineering. We propose two attacks against websites. One is a coarse-grained fingerprinting attack, recognizing which website is accessed based on their different GPU memory access patterns monitored through the SLC occupancy channel. The other attack is a fine-grained pixel stealing attack, which precisely monitors the GPU memory usage for rendering different pixels, through the SLC occupancy channel. Third, we introduce a novel screen capturing attack which works beyond webpages, with the monitoring granularity of 57 rows of pixels (there are 1600 rows for the screen). This significantly expands the attack surface, allowing the adversary to retrieve any screen display, posing a substantial new threat to system security. Our findings reveal critical vulnerabilities in Apple's M-series SoCs and emphasize the urgent need for effective countermeasures against cache occupancy attacks in heterogeneous computing environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.13385v1),  [pdf](http://arxiv.org/pdf/2504.13385v1)

**Tags**: cs.CR cs.AR 



### Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN   Heterostructures
**Authors**: Seungheon Shin, Kyle Liddy, Yinxuan Zhu, Chandan Joishi, Brianna A. Klein, Andrew Armstrong, Andrew A. Allerman, Siddharth Rajan

**Updated**: 2025-04-17T23:45:51Z

**Summary**: We report on energy bands and breakdown characteristics of Al2O3 dielectrics on ultra-wide bandgap (UWBG) AlGaN heterostructures. Metal-dielectric-semiconductor structures are important to sustain high fields needed for future high-performance UWBG transistors. Using systematic experiments, we determined the fixed charge density (> 1013 cm-2), the dielectric/interface, and electric fields in the oxide of under flat-band conditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x 10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In lateral metal-semiconductor-insulator test structures, breakdown voltage exceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013 cm-2. The effective peak electric field and average breakdown field were estimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings demonstrate the potential of Al2O2 integration for enhancing the breakdown performance of UWBG AlGaN HEMTs.

**Link**: [arxiv](http://arxiv.org/abs/2504.01291v2),  [pdf](http://arxiv.org/pdf/2504.01291v2)

**Tags**: cond-mat.mtrl-sci physics.app-ph 



### Long-Context Autoregressive Video Modeling with Next-Frame Prediction
**Authors**: Yuchao Gu, Weijia Mao, Mike Zheng Shou

**Updated**: 2025-04-17T15:26:04Z

**Summary**: Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context video modeling faces challenges due to visual redundancy. Training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle this issue, we propose balancing locality and long-range dependency through long short-term context modeling. A high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length, thereby significantly reducing training time and memory usage. Furthermore, we propose a multi-level KV cache designed to support the long short-term context modeling, which accelerating inference on long video sequences. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling. The code is released at https://github.com/showlab/FAR.

**Link**: [arxiv](http://arxiv.org/abs/2503.19325v2),  [pdf](http://arxiv.org/pdf/2503.19325v2)

**Tags**: cs.CV 



### In-context KV-Cache Eviction for LLMs via Attention-Gate
**Authors**: Zihao Zeng, Bokai Lin, Tianqi Hou, Hao Zhang, Zhijie Deng

**Updated**: 2025-04-17T03:51:06Z

**Summary**: The KV-Cache technique has become the standard for the inference of large language models (LLMs). Yet, it is widely criticized that KV-Cache can become a bottleneck of the LLM inference system. This paper enables a novel dynamic KV-Cache eviction policy by injecting a lightweight module called Attention-Gate to the model. It accepts the global context as input and yields eviction flags for each token. The self-attention modules in the model proceed according to the flags and cache only a subset of the KV states for next token prediction. The Attention-Gates can yield various flags for different heads and layers and be easily tuned on top of a pre-trained LLM via continual pre-training or supervised fine-tuning. The computational and memory overhead introduced by Attention-Gates can be minimal. We empirically evaluate the proposed approach across multiple scenarios, showing that effective eviction of redundant tokens can not only improve efficiency but also enhance performance.

**Link**: [arxiv](http://arxiv.org/abs/2410.12876v3),  [pdf](http://arxiv.org/pdf/2410.12876v3)

**Tags**: cs.CL cs.LG 



### Demoting Security via Exploitation of Cache Demote Operation in Intel's   Latest ISA Extension
**Authors**: Taehun Kim, Hyerean Jang, Youngjoo Shin

**Updated**: 2025-04-17T00:38:24Z

**Summary**: ISA extensions are increasingly adopted to boost the performance of specialized workloads without requiring an entire architectural redesign. However, these enhancements can inadvertently expose new attack surfaces in the microarchitecture. In this paper, we investigate Intel's recently introduced cldemote extension, which promotes efficient data sharing by transferring cache lines from upper-level caches to the Last Level Cache (LLC). Despite its performance benefits, we uncover critical properties-unprivileged access, inter-cache state transition, and fault suppression-that render cldemote exploitable for microarchitectural attacks. We propose two new attack primitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote constructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate of 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on Linux. Furthermore, we show that leveraging cldemote accelerates eviction set construction in non-inclusive LLC designs by obviating the need for helper threads or extensive cache conflicts, thereby reducing construction time by 36% yet retaining comparable success rates. Finally, we examine how ISA extensions contribute to broader microarchitectural attacks, identifying five key exploitable characteristics and categorizing four distinct attack types. We also discuss potential countermeasures, highlighting the far-reaching security implications of emerging ISA extensions.

**Link**: [arxiv](http://arxiv.org/abs/2503.10074v2),  [pdf](http://arxiv.org/pdf/2503.10074v2)

**Tags**: cs.CR 



### MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context   Language Models
**Authors**: Junyang Zhang, Tianyi Zhu, Cheng Luo, Anima Anandkumar

**Updated**: 2025-04-16T23:15:09Z

**Summary**: Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that partitions critical layers into smaller "mini-sequences" and integrates seamlessly with KV cache offloading. Experiments on various Llama, Qwen, and Mistral models demonstrate that MOM reduces peak memory usage by over 50\% on average. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k to 455k tokens on a single A100 80GB GPU, while keeping outputs identical and not compromising accuracy. MOM also maintains highly competitive throughput due to minimal computational overhead and efficient last-layer processing. Compared to traditional chunked prefill methods, MOM achieves a 35\% greater context length extension. More importantly, our method drastically reduces prefill memory consumption, eliminating it as the longstanding dominant memory bottleneck during inference. This breakthrough fundamentally changes research priorities, redirecting future efforts from prefill-stage optimizations to improving decode-stage residual KV cache efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2504.12526v1),  [pdf](http://arxiv.org/pdf/2504.12526v1)

**Tags**: cs.LG cs.AI cs.CL 



### Cobra: Efficient Line Art COlorization with BRoAder References
**Authors**: Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan

**Updated**: 2025-04-16T16:45:19Z

**Summary**: The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.

**Link**: [arxiv](http://arxiv.org/abs/2504.12240v1),  [pdf](http://arxiv.org/pdf/2504.12240v1)

**Tags**: cs.CV 



### Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache   Offloading
**Authors**: Kihyun Kim, Jinwoo Kim, Hyunsun Chung, Myung-Hoon Cha, Hong-Yeon Kim, Youngjae Kim

**Updated**: 2025-04-16T07:02:38Z

**Summary**: LLM inference is essential for applications like text summarization, translation, and data analysis, but the high cost of GPU instances from Cloud Service Providers (CSPs) like AWS is a major burden. This paper proposes InferSave, a cost-efficient VM selection framework for cloud based LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload charac teristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. Experiments on AWS GPU instances show that selecting lower-cost instances without KV cache offloading improves cost efficiency by up to 73.7% for online workloads, while KV cache offloading saves up to 20.19% for offline workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.11816v1),  [pdf](http://arxiv.org/pdf/2504.11816v1)

**Tags**: cs.LG cs.DC 



### Efficient Architecture for RISC-V Vector Memory Access
**Authors**: Hongyi Guan, Yichuan Gao, Chenlu Miao, Haoyang Wu, Hang Zhu, Mingfeng Lin, Huayue Liang

**Updated**: 2025-04-16T05:57:08Z

**Summary**: Vector processors frequently suffer from inefficient memory accesses, particularly for strided and segment patterns. While coalescing strided accesses is a natural solution, effectively gathering or scattering elements at fixed strides remains challenging. Naive approaches rely on high-overhead crossbars that remap any byte between memory and registers, leading to physical design issues. Segment operations require row-column transpositions, typically handled using either element-level in-place transposition (degrading performance) or large buffer-based bulk transposition (incurring high area overhead). In this paper, we present EARTH, a novel vector memory access architecture designed to overcome these challenges through shifting-based optimizations. For strided accesses, EARTH integrates specialized shift networks for gathering and scattering elements. After coalescing multiple accesses within the same cache line, data is routed between memory and registers through the shifting network with minimal overhead. For segment operations, EARTH employs a shifted register bank enabling direct column-wise access, eliminating dedicated segment buffers while providing high-performance, in-place bulk transposition. Implemented on FPGA with Chisel HDL based on an open-source RISC-V vector unit, EARTH enhances performance for strided memory accesses, achieving 4x-8x speedups in benchmarks dominated by strided operations. Compared to conventional designs, EARTH reduces hardware area by 9% and power consumption by 41%, significantly advancing both performance and efficiency of vector processors.

**Link**: [arxiv](http://arxiv.org/abs/2504.08334v3),  [pdf](http://arxiv.org/pdf/2504.08334v3)

**Tags**: cs.AR cs.DC 



### Shared Disk KV Cache Management for Efficient Multi-Instance Inference   in RAG-Powered LLMs
**Authors**: Hyungwoo Lee, Kihyun Kim, Jinwoo Kim, Jungmin So, Myung-Hoon Cha, Hong-Yeon Kim, James J. Kim, Youngjae Kim

**Updated**: 2025-04-16T04:59:18Z

**Summary**: Recent large language models (LLMs) face increasing inference latency as input context length and model size continue to grow. In particular, the retrieval-augmented generation (RAG) technique, which enhances LLM responses by incorporating external knowledge, exacerbates this issue by significantly increasing the number of input tokens. This expansion in token length leads to a substantial rise in computational overhead, particularly during the prefill stage, resulting in prolonged time-to-first-token (TTFT). To address this issue, this paper proposes a method to reduce TTFT by leveraging a disk-based key-value (KV) cache to lessen the computational burden during the prefill stage. We also introduce a disk-based shared KV cache management system, called Shared RAG-DCache, for multi-instance LLM RAG service environments. This system, together with an optimal system configuration, improves both throughput and latency under given resource constraints. Shared RAG-DCache exploits the locality of documents related to user queries in RAG, as well as the queueing delay in LLM inference services. It proactively generates and stores disk KV caches for query-related documents and shares them across multiple LLM instances to enhance inference performance. In experiments on a single host equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in throughput and up to a 12~65% reduction in latency, depending on the resource configuration.

**Link**: [arxiv](http://arxiv.org/abs/2504.11765v1),  [pdf](http://arxiv.org/pdf/2504.11765v1)

**Tags**: cs.AI 



### EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G   Networks
**Authors**: Jiahong Ning, Pengyan Zhu, Ce Zheng, Gary Lee, Sumei Sun, Tingting Yang

**Updated**: 2025-04-16T03:07:07Z

**Summary**: As sixth-generation (6G) networks advance, large language models (LLMs) are increasingly integrated into 6G infrastructure to enhance network management and intelligence. However, traditional LLMs architecture struggle to meet the stringent latency and security requirements of 6G, especially as the increasing in sequence length leads to greater task complexity. This paper proposes Edge-Prompt, a cloud-edge collaborative framework based on a hierarchical attention splicing mechanism. EdgePrompt employs distributed key-value (KV) pair optimization techniques to accelerate inference and adapt to network conditions. Additionally, to reduce the risk of data leakage, EdgePrompt incorporates a privacy preserving strategy by isolating sensitive information during processing. Experiments on public dataset show that EdgePrompt effectively improves the inference throughput and reduces the latency, which provides a reliable solution for LLMs deployment in 6G environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.11729v1),  [pdf](http://arxiv.org/pdf/2504.11729v1)

**Tags**: eess.SP 



### Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues
**Authors**: Marvin Williams, Peter Sanders

**Updated**: 2025-04-15T22:38:54Z

**Summary**: Priority queues are used in a wide range of applications, including prioritized online scheduling, discrete event simulation, and greedy algorithms. In parallel settings, classical priority queues often become a severe bottleneck, resulting in low throughput. Consequently, there has been significant interest in concurrent priority queues with relaxed semantics. In this article, we present the MultiQueue, a flexible approach to relaxed priority queues that uses multiple internal sequential priority queues. The scalability of the MultiQueue is enhanced by buffering elements, batching operations on the internal queues, and optimizing access patterns for high cache locality. We investigate the complementary quality criteria of rank error, which measures how close deleted elements are to the global minimum, and delay, which quantifies how many smaller elements were deleted before a given element. Extensive experimental evaluation shows that the MultiQueue outperforms competing approaches across several benchmarks. This includes shortest-path and branch-and-bound benchmarks that resemble real applications. Moreover, the MultiQueue can be configured easily to balance throughput and quality according to the application's requirements. We employ a seemingly paradoxical technique of wait-free locking that might be of broader interest for converting sequential data structures into relaxed concurrent data structures.

**Link**: [arxiv](http://arxiv.org/abs/2504.11652v1),  [pdf](http://arxiv.org/pdf/2504.11652v1)

**Tags**: cs.DS cs.DC cs.PF 



### Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory   Constraints
**Authors**: Ruicheng Ao, Gan Luo, David Simchi-Levi, Xinshang Wang

**Updated**: 2025-04-15T16:00:21Z

**Summary**: Large Language Models (LLMs) are indispensable in today's applications, but their inference procedure -- generating responses by processing text in segments and using a memory-heavy Key-Value (KV) cache -- demands significant computational resources, particularly under memory constraints. This paper formulates LLM inference optimization as a multi-stage online scheduling problem where sequential prompt arrivals and KV cache growth render conventional scheduling ineffective. We develop a fluid dynamics approximation to provide a tractable benchmark that guides algorithm design. Building on this, we propose the Waiting for Accumulated Inference Threshold (WAIT) algorithm, which uses multiple thresholds to schedule incoming prompts optimally when output lengths are known, and extend it to Nested WAIT for cases with unknown output lengths. Theoretical analysis shows that both algorithms achieve near-optimal performance against the fluid benchmark in heavy traffic conditions, balancing throughput, latency, and Time to First Token (TTFT). Experiments with the Llama-7B model on an A100 GPU using both synthetic and real-world datasets demonstrate improved throughput and reduced latency relative to established baselines like vLLM and Sarathi. This work bridges operations research and machine learning, offering a rigorous framework for the efficient deployment of LLMs under memory constraints.

**Link**: [arxiv](http://arxiv.org/abs/2504.11320v1),  [pdf](http://arxiv.org/pdf/2504.11320v1)

**Tags**: cs.LG cs.AI cs.DC math.OC stat.ML 



### Performant Automatic BLAS Offloading on Unified Memory Architecture with   OpenMP First-Touch Style Data Movement
**Authors**: Junjie Li

**Updated**: 2025-04-15T15:40:25Z

**Summary**: BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.

**Link**: [arxiv](http://arxiv.org/abs/2501.00279v3),  [pdf](http://arxiv.org/pdf/2501.00279v3)

**Tags**: cs.DC cs.MS cs.PF cs.SE 



### Automatic BLAS Offloading on Unified Memory Architecture: A Study on   NVIDIA Grace-Hopper
**Authors**: Junjie Li, Yinzhi Wang, Xiao Liang, Hang Liu

**Updated**: 2025-04-15T15:37:58Z

**Summary**: Porting codes to GPU often requires major efforts. While several tools exist for automatically offload numerical libraries such as BLAS and LAPACK, they often prove impractical due to the high cost of mandatory data transfer. The new unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth cache-coherent memory access of all memory from both CPU and GPU, potentially eliminating bottleneck faced in conventional architecture. This breakthrough opens up new avenues for application development and porting strategies. In this study, we introduce a new tool for automatic BLAS offload, the tool leverages the high speed cache coherent NVLink C2C interconnect in Grace-Hopper, and enables performant GPU offload for BLAS heavy applications with no code changes or recompilation. The tool was tested on two quantum chemistry or physics codes, great performance benefits were observed.

**Link**: [arxiv](http://arxiv.org/abs/2404.13195v5),  [pdf](http://arxiv.org/pdf/2404.13195v5)

**Tags**: cs.DC 



### Morphing-based Compression for Data-centric ML Pipelines
**Authors**: Sebastian Baunsgaard, Matthias Boehm

**Updated**: 2025-04-15T11:02:34Z

**Summary**: Data-centric ML pipelines extend traditional machine learning (ML) pipelines -- of feature transformations and ML model training -- by outer loops for data cleaning, augmentation, and feature engineering to create high-quality input data. Existing lossless matrix compression applies lightweight compression schemes to numeric matrices and performs linear algebra operations such as matrix-vector multiplications directly on the compressed representation but struggles to efficiently rediscover structural data redundancy. Compressed operations are effective at fitting data in available memory, reducing I/O across the storage-memory-cache hierarchy, and improving instruction parallelism. The applied data cleaning, augmentation, and feature transformations provide a rich source of information about data characteristics such as distinct items, column sparsity, and column correlations. In this paper, we introduce BWARE -- an extension of AWARE for workload-aware lossless matrix compression -- that pushes compression through feature transformations and engineering to leverage information about structural transformations. Besides compressed feature transformations, we introduce a novel technique for lightweight morphing of a compressed representation into workload-optimized compressed representations without decompression. BWARE shows substantial end-to-end runtime improvements, reducing the execution time for training data-centric ML pipelines from days to hours.

**Link**: [arxiv](http://arxiv.org/abs/2504.11067v1),  [pdf](http://arxiv.org/pdf/2504.11067v1)

**Tags**: cs.DB cs.DC cs.LG 



### AlayaDB: The Data Foundation for Efficient and Effective Long-context   LLM Inference
**Authors**: Yangshen Deng, Zhengxin You, Long Xiang, Qilong Li, Peiqi Yuan, Zhaoyang Hong, Yitao Zheng, Wanting Li, Runzhong Li, Haotian Liu, Kyriakos Mouratidis, Man Lung Yiu, Huan Li, Qiaomu Shen, Rui Mao, Bo Tang

**Updated**: 2025-04-14T15:34:26Z

**Summary**: AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.10326v1),  [pdf](http://arxiv.org/pdf/2504.10326v1)

**Tags**: cs.AI cs.DB cs.IR H.3.1; H.3.2; H.3.3; H.3.4 



### Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing   Obfuscation
**Authors**: Kartik Ramkrishnan, Antonia Zhai, Stephen McCamant, Pen Chung Yew

**Updated**: 2025-04-14T15:27:32Z

**Summary**: Microarchitectural attacks are a significant concern, leading to many hardware-based defense proposals. However, different defenses target different classes of attacks, and their impact on each other has not been fully considered. To raise awareness of this problem, we study an interaction between two state-of-the art defenses in this paper, timing obfuscations of remote cache lines (TORC) and delaying speculative changes to remote cache lines (DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative coherence state change attacks.   We observe that DSRC enables coherence information to be retrieved into the processor core, where it is out of the reach of timing obfuscations to protect. This creates an unforeseen consequence that redo operations can be triggered within the core to detect the presence or absence of remote cache lines, which constitutes a security vulnerability. We demonstrate that a new covert channel attack is possible using this vulnerability. We propose two ways to mitigate the attack, whose performance varies depending on an application's cache usage. One way is to never send remote exclusive coherence state (E) information to the core even if it is created. The other way is to never create a remote E state, which is responsible for triggering redos.   We demonstrate the timing difference caused by this microarchitectural defense assumption violation using GEM5 simulations. Performance evaluation on SPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\% average overhead across both sets of benchmarks. The repair which prevented the creation of remote E state had less than 2.8% average overhead.

**Link**: [arxiv](http://arxiv.org/abs/2504.10318v1),  [pdf](http://arxiv.org/pdf/2504.10318v1)

**Tags**: cs.CR cs.AR 



### A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis
**Authors**: Zahid Javid, Firdous Ul Nazir, Wentao Zhu, Diptargha Chakravorty, Ahmed Aboushady, Mohamed Galeela

**Updated**: 2025-04-14T12:34:20Z

**Summary**: The fault characteristics of inverter-based resources (IBRs) are different from conventional synchronous generators. The fault response of IBRs is non-linear due to saturation states and mainly determined by fault ride through (FRT) strategies of the associated voltage source converter (VSC). This results in prohibitively large solution times for power flows considering these short circuit characteristics, especially when the power system states change fast due to uncertainty in IBR generations. To overcome this, a phasor-domain steady state (SS) short circuit (SC) solver for IBR dominated power systems is proposed in this paper, and subsequently the developed IBR models are incorporated with a novel Jacobian-based Power Flow (PF) solver. In this multiphase PF solver, any power system components can be modeled by considering their original non-linear or linear mathematical representations. Moreover, two novel FRT strategies are proposed to fully utilize the converter capacity and to comply with IEEE-2800 2022 std and German grid code. The results are compared with the Electromagnetic Transient (EMT) simulation on the IEEE 34 test network and the 120 kV EPRI benchmark system. The developed IBR sequence domain PF model demonstrates more accurate behavior compared to the classical IBR generator model. The error in calculating the short circuit current with the proposed SC solver is less than 3%, while achieving significant speed improvements of three order of magnitudes.

**Link**: [arxiv](http://arxiv.org/abs/2504.10181v1),  [pdf](http://arxiv.org/pdf/2504.10181v1)

**Tags**: eess.SY cs.SY 



### Diversity in Network-Friendly Recommendations
**Authors**: Evangelia Tzimpimpaki, Thrasyvoulos Spyropoulos

**Updated**: 2025-04-14T11:20:56Z

**Summary**: In recent years, the Internet has been dominated by content-rich platforms, employing recommendation systems to provide users with more appealing content (e.g., videos in YouTube, movies in Netflix). While traditional content recommendations are oblivious to network conditions, the paradigm of Network-Friendly Recommendations (NFR) has recently emerged, favoring content that improves network performance (e.g. cached near the user), while still being appealing to the user. However, NFR algorithms sometimes achieve their goal by shrinking the pool of content recommended to users. The undesirable side-effect is reduced content diversity, a phenomenon known as ``content/filter bubble''. This reduced diversity is problematic for both users, who are prevented from exploring a broader range of content, and content creators (e.g. YouTubers) whose content may be recommended less frequently, leading to perceived unfairness. In this paper, we first investigate - using real data and state-of-the-art NFR schemes - the extent of this phenomenon. We then formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly recommendations with - sufficient - content diversity), and through a series of transformation steps, we manage to reduce it to a linear program that can be solved fast and optimally. Our findings show that Diverse-NFR can achieve high network gains (comparable to non-diverse NFR) while maintaining diversity constraints. To our best knowledge, this is the first work that incorporates diversity issues into network-friendly recommendation algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2411.00601v3),  [pdf](http://arxiv.org/pdf/2411.00601v3)

**Tags**: cs.PF 



### On Precomputation and Caching in Information Retrieval Experiments with   Pipeline Architectures
**Authors**: Sean MacAvaney, Craig Macdonald

**Updated**: 2025-04-14T08:51:35Z

**Summary**: Modern information retrieval systems often rely on multiple components executed in a pipeline. In a research setting, this can lead to substantial redundant computations (e.g., retrieving the same query multiple times for evaluating different downstream rerankers). To overcome this, researchers take cached "result" files as inputs, which represent the output of another pipeline. However, these result files can be brittle and can cause a disconnect between the conceptual design of the pipeline and its logical implementation. To overcome both the redundancy problem (when executing complete pipelines) and the disconnect problem (when relying on intermediate result files), we describe our recent efforts to improve the caching capabilities in the open-source PyTerrier IR platform. We focus on two main directions: (1) automatic implicit caching of common pipeline prefixes when comparing systems and (2) explicit caching of operations through a new extension package, pyterrier-caching. These approaches allow for the best of both worlds: pipelines can be fully expressed end-to-end, while also avoiding redundant computations between pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2504.09984v1),  [pdf](http://arxiv.org/pdf/2504.09984v1)

**Tags**: cs.IR 



### Secrecy and Privacy in Multi-Access Combinatorial Topology
**Authors**: Mallikharjuna Chinnapadamala, B. Sundar Rajan

**Updated**: 2025-04-14T07:30:03Z

**Summary**: In this work, we consider the multi-access combinatorial topology with $C$ caches where each user accesses a unique set of $r$ caches. For this setup, we consider secrecy, where each user should not know anything about the files it did not request, and demand privacy, where each user's demand must be kept private from other non-colluding users. We propose a scheme satisfying both conditions and derive a lower bound based on cut-set arguments. Also, we prove that our scheme is optimal when $r\geq C-1$, and it is order-optimal when the cache memory size $M$ is greater than or equal to a certain threshold for $r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same rate as the one given by the secretive scheme for the dedicated cache setup by Ravindrakumar et al. ( 'Private Coded Caching,' in \textit{IEEE Transactions on Information Forensics and Security}, 2018), while satisfying both secrecy and demand privacy conditions.

**Link**: [arxiv](http://arxiv.org/abs/2504.09952v1),  [pdf](http://arxiv.org/pdf/2504.09952v1)

**Tags**: cs.IT math.IT 



### KeepKV: Eliminating Output Perturbation in KV Cache Compression for   Efficient LLMs Inference
**Authors**: Yuxuan Tian, Zihan Wang, Yebo Peng, Aomufei Yuan, Zhiming Wang, Bairen Yi, Xin Liu, Yong Cui, Tong Yang

**Updated**: 2025-04-14T06:58:00Z

**Summary**: Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.

**Link**: [arxiv](http://arxiv.org/abs/2504.09936v1),  [pdf](http://arxiv.org/pdf/2504.09936v1)

**Tags**: cs.LG cs.AI cs.CL 



### Plato: Plan to Efficiently Decode for Large Language Model Inference
**Authors**: Shuowei Jin, Xueshen Liu, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Matthew Lentz, Danyang Zhuo, Feng Qian, Z. Morley Mao

**Updated**: 2025-04-13T14:17:57Z

**Summary**: Large language models (LLMs) have achieved remarkable success in natural language tasks, but their inference incurs substantial computational and memory overhead. To improve efficiency, parallel decoding methods like Skeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent processing. However, these methods significantly compromise answer quality by treating semantically linked sub-problems as independent. We propose Plato, a novel approach that co-designs algorithms and systems for semantic-aware parallel decoding. Plato leverages LLMs to organize sub-problems into a dependency graph based on logical and causal relationships, enabling concurrent decoding of non-dependent nodes while preserving answer coherence and quality. To further enhance efficiency, Plato pipelines planning and node decoding stages, implements a global context cache, and carefully structures node inference prompts to maximize key-value cache reuse and minimize overhead. Our evaluations show that Plato improves throughput by 68% over autoregressive decoding while achieving a 40% net win rate in answer quality. Compared to SoT, Plato demonstrates a remarkable 90% quality net-win rate. Ablation studies reveal that our pipeline design improves speedup by 29%, while our KV cache reuse optimization reduces overhead by 75%.

**Link**: [arxiv](http://arxiv.org/abs/2402.12280v2),  [pdf](http://arxiv.org/pdf/2402.12280v2)

**Tags**: cs.CL cs.AI 



### Efficient LLM Serving on Hybrid Real-time and Best-effort Requests
**Authors**: Wan Borui, Zhao Juntao, Jiang Chenyu, Guo Chuanxiong, Wu Chuan

**Updated**: 2025-04-13T14:16:57Z

**Summary**: Recent breakthroughs in large Language Models (LLMs) have enabled various generative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT [27]) powered by an LLM often concurrently support latency-critical requests for interactive applications (e.g., question-answering systems, referred to as real-time or RT requests) and throughput-oriented requests for back-of-house processing (e.g., documents batch processing [28], referred to best-effort or BE requests), with complex hybrid inference workloads to the underlying model. State-of-the-art (SOTA) LLM serving systems dedicate machines to each type of request, towards either low inference latency or high serving throughput, respectively. This practice simplifies request scheduling and management but suffers from poor resource utilization. We propose BROS, a hybrid LLM serving system that aims to collocate RT/BE requests, meeting RT requests' latency requirements while maintaining BE requests' throughput. BROS formulates the problem of hybrid RT/BE request scheduling and solves it with a dynamic priority-based algorithm. BROS designs a bidirectional KV cache management mechanism, allowing RT requests to share KV memory with BE requests to remove the scheduling restrictions caused by insufficient KV memory and improve utilization. Extensive experiments validate that BROS achieves a good trade-off when serving hybrid RT and BE requests. It significantly reduces the latency of RT requests (up to 74.20%), improving their fine-grained service level objectives (SLOs) attainments (up to 36.38x), with negligible throughput reduction for BE requests, showing significant advantages over SOTA systems like vLLM and TGI.

**Link**: [arxiv](http://arxiv.org/abs/2504.09590v1),  [pdf](http://arxiv.org/pdf/2504.09590v1)

**Tags**: cs.AI 



### Block-Attention for Efficient Prefilling
**Authors**: Dongyang Ma, Yan Wang, Lan Tian

**Updated**: 2025-04-13T14:02:47Z

**Summary**: We introduce Block-attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. Traditional approaches often encode the entire context in an auto-regressive manner. Instead, Block-attention divides retrieved documents into discrete blocks, with each block independently calculating key-value (KV) states except for the final block. In RAG scenarios, by defining each passage as a block, Block-attention enables us to reuse the KV states of passages that have been seen before, thereby significantly reducing the latency and the computation overhead during inference. The implementation of Block-attention involves block segmentation, position re-encoding, and fine-tuning the LLM to adapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks, including RAG, ICL, and general domains, demonstrate that after block fine-tuning, the Block-attention model not only achieves performance comparable to that of full-attention models, but can also seamlessly switch between the block and full attention modes without any performance loss. Notably, Block-attention significantly reduces the time to first token (TTFT) and floating point operations (FLOPs) to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared to the full-attention models, the TTFT and corresponding FLOPs are reduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we elaborate on how Block-attention is applied in Game AI scenario and the substantial potential benefits it entails. We strongly suggest researchers in the gaming field not to overlook this section.

**Link**: [arxiv](http://arxiv.org/abs/2409.15355v5),  [pdf](http://arxiv.org/pdf/2409.15355v5)

**Tags**: cs.LG cs.AI cs.CL 



### AB-Cache: Training-Free Acceleration of Diffusion Models via   Adams-Bashforth Cached Feature Reuse
**Authors**: Zichao Yu, Zhen Zou, Guojiang Shao, Chengwei Zhang, Shengze Xu, Jie Huang, Feng Zhao, Xiaodong Cun, Wenyi Zhang

**Updated**: 2025-04-13T08:29:58Z

**Summary**: Diffusion models have demonstrated remarkable success in generative tasks, yet their iterative denoising process results in slow inference, limiting their practicality. While existing acceleration methods exploit the well-known U-shaped similarity pattern between adjacent steps through caching mechanisms, they lack theoretical foundation and rely on simplistic computation reuse, often leading to performance degradation. In this work, we provide a theoretical understanding by analyzing the denoising process through the second-order Adams-Bashforth method, revealing a linear relationship between the outputs of consecutive steps. This analysis explains why the outputs of adjacent steps exhibit a U-shaped pattern. Furthermore, extending Adams-Bashforth method to higher order, we propose a novel caching-based acceleration approach for diffusion models, instead of directly reusing cached results, with a truncation error bound of only \(O(h^k)\) where $h$ is the step size. Extensive validation across diverse image and video diffusion models (including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates our method's effectiveness in achieving nearly $3\times$ speedup while maintaining original performance levels, offering a practical real-time solution without compromising generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2504.10540v1),  [pdf](http://arxiv.org/pdf/2504.10540v1)

**Tags**: stat.ML cs.AI cs.LG 



### Sub-nanosecond in-plane magnetization switching induced by field-like   spin-orbit torques from ferromagnets
**Authors**: Hanying Zhang, Ziqian Cui, Baiqing Jiang, Yuan Wang, C. Bi

**Updated**: 2025-04-13T04:46:02Z

**Summary**: Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures are classified as damping-like (DL) and field-like (FL) torques for current-driven magnetization switching. It is well known that both DL- and FL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven switching process while FL-SOT contributes limitedly, resulting in an incubation time (several nanoseconds) during collinear magnetization switching with the spin polarization because of the DL attributes. Here we report a FL-SOT originated from the ferromagnet, different from the origin of DL-SOT, and demonstrate that it dominates the collinear magnetization switching. We show that the FL-SOT and resultant collinear switching can be modulated, one order of magnitude and sign reversal, by controlling the ferromagnet. Because of no incubation time and higher charge-to-spin efficiencies in the FL switching, we further show that the switching time can be down to 200 ps with one order lower critical switching current density compared to DL switching. These results indicate that the FL switching may provide a practical solution for magnetic memory in speed-priority cache applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.09431v1),  [pdf](http://arxiv.org/pdf/2504.09431v1)

**Tags**: physics.app-ph 



## Keyword: LLM Inference 
 ### MINERVA: Evaluating Complex Video Reasoning
**Authors**: Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, Cordelia Schmid, Tobias Weyand

**Updated**: 2025-05-01T17:41:49Z

**Summary**: Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called MINERVA for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.

**Link**: [arxiv](http://arxiv.org/abs/2505.00681v1),  [pdf](http://arxiv.org/pdf/2505.00681v1)

**Tags**: cs.LG cs.CV 



### Steering Large Language Models with Register Analysis for Arbitrary   Style Transfer
**Authors**: Xinchen Yang, Marine Carpuat

**Updated**: 2025-05-01T17:39:02Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities in rewriting text across various styles. However, effectively leveraging this ability for example-based arbitrary style transfer, where an input text is rewritten to match the style of a given exemplar, remains an open challenge. A key question is how to describe the style of the exemplar to guide LLMs toward high-quality rewrites. In this work, we propose a prompting method based on register analysis to guide LLMs to perform this task. Empirical evaluations across multiple style transfer tasks show that our prompting approach enhances style transfer strength while preserving meaning more effectively than existing prompting strategies.

**Link**: [arxiv](http://arxiv.org/abs/2505.00679v1),  [pdf](http://arxiv.org/pdf/2505.00679v1)

**Tags**: cs.CL 



### Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future   Directions
**Authors**: Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan

**Updated**: 2025-05-01T17:31:33Z

**Summary**: Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.

**Link**: [arxiv](http://arxiv.org/abs/2505.00675v1),  [pdf](http://arxiv.org/pdf/2505.00675v1)

**Tags**: cs.CL 



### Learning An Active Inference Model of Driver Perception and Control:   Application to Vehicle Car-Following
**Authors**: Ran Wei, Anthony D. McDonald, Alfredo Garcia, Gustav Markkula, Johan Engstrom, Matthew O'Kelly

**Updated**: 2025-05-01T17:28:57Z

**Summary**: In this paper we introduce a general estimation methodology for learning a model of human perception and control in a sensorimotor control task based upon a finite set of demonstrations. The model's structure consists of i the agent's internal representation of how the environment and associated observations evolve as a result of control actions and ii the agent's preferences over observable outcomes. We consider a model's structure specification consistent with active inference, a theory of human perception and behavior from cognitive science. According to active inference, the agent acts upon the world so as to minimize surprise defined as a measure of the extent to which an agent's current sensory observations differ from its preferred sensory observations. We propose a bi-level optimization approach to estimation which relies on a structural assumption on prior distributions that parameterize the statistical accuracy of the human agent's model of the environment. To illustrate the proposed methodology, we present the estimation of a model for car-following behavior based upon a naturalistic dataset. Overall, the results indicate that learning active inference models of human perception and control from data is a promising alternative to black-box models of driving.

**Link**: [arxiv](http://arxiv.org/abs/2303.15201v2),  [pdf](http://arxiv.org/pdf/2303.15201v2)

**Tags**: cs.LG cs.AI cs.RO 



### Gaussian Mixture Flow Matching Models
**Authors**: Hansheng Chen, Kai Zhang, Hao Tan, Zexiang Xu, Fujun Luan, Leonidas Guibas, Gordon Wetzstein, Sai Bi

**Updated**: 2025-05-01T17:23:22Z

**Summary**: Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\times$256.

**Link**: [arxiv](http://arxiv.org/abs/2504.05304v2),  [pdf](http://arxiv.org/pdf/2504.05304v2)

**Tags**: cs.LG cs.CV 



### Generative Predictive Control: Flow Matching Policies for Dynamic and   Difficult-to-Demonstrate Tasks
**Authors**: Vince Kurtz, Joel W. Burdick

**Updated**: 2025-05-01T17:23:06Z

**Summary**: Generative control policies have recently unlocked major progress in robotics. These methods produce action sequences via diffusion or flow matching, with training data provided by demonstrations. But existing methods come with two key limitations: they require expert demonstrations, which can be difficult to obtain, and they are limited to relatively slow, quasi-static tasks. In this paper, we leverage a tight connection between sampling-based predictive control and generative modeling to address each of these issues. In particular, we introduce generative predictive control, a supervised learning framework for tasks with fast dynamics that are easy to simulate but difficult to demonstrate. We then show how trained flow-matching policies can be warm-started at inference time, maintaining temporal consistency and enabling high-frequency feedback. We believe that generative predictive control offers a complementary approach to existing behavior cloning methods, and hope that it paves the way toward generalist policies that extend beyond quasi-static demonstration-oriented tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.13406v2),  [pdf](http://arxiv.org/pdf/2502.13406v2)

**Tags**: cs.RO cs.AI cs.SY eess.SY 



### Artificial Scientific Discovery
**Authors**: Antonio Norelli

**Updated**: 2025-05-01T17:09:17Z

**Summary**: Rooted in the explosion of deep learning over the past decade, this thesis spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts needed to realize the vision of an artificial scientist: a machine with the capacity to autonomously generate original research and contribute to the expansion of human knowledge. The investigation begins with Olivaw, an AlphaGo Zero-like agent that discovers Othello knowledge from scratch but is unable to communicate it. This realization leads to the development of the Explanatory Learning (EL) framework, a formalization of the problem faced by a scientist when trying to explain a new phenomenon to their peers. The effective EL prescriptions allow us to crack Zendo, a popular board game simulating the scientific endeavor. This success comes with a fundamental insight: an artificial scientist must develop its own interpretation of the language used to explain its findings, and not rely on a rigid existing interpreter. Questioning the very process of learning an interpreter, we turn our attention to the inner functioning of modern multimodal models. This culminates in a simple idea to build CLIP-like models where interpretation and perception are explicitly disentangled: a cost-effective approach that couples two unimodal models using little multimodal data and no further training. Finally, we discuss what ChatGPT and its siblings are still missing to become artificial scientists, and introduce the Big-Bench Symbol Interpretation Task, a benchmark about interpreting Zendo-like explanations that sees LLMs going no further than random chance while being instead fully solved by humans.

**Link**: [arxiv](http://arxiv.org/abs/2411.11672v2),  [pdf](http://arxiv.org/pdf/2411.11672v2)

**Tags**: cs.AI cs.LG I.2 



### DeepCritic: Deliberate Critique with Large Language Models
**Authors**: Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen

**Updated**: 2025-05-01T17:03:17Z

**Summary**: As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.

**Link**: [arxiv](http://arxiv.org/abs/2505.00662v1),  [pdf](http://arxiv.org/pdf/2505.00662v1)

**Tags**: cs.CL cs.AI cs.LG 



### On the generalization of language models from in-context learning and   finetuning: a controlled study
**Authors**: Andrew K. Lampinen, Arslan Chaudhry, Stephanie C. Y. Chan, Cody Wild, Diane Wan, Alex Ku, Jörg Bornschein, Razvan Pascanu, Murray Shanahan, James L. McClelland

**Updated**: 2025-05-01T17:02:27Z

**Summary**: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.00661v1),  [pdf](http://arxiv.org/pdf/2505.00661v1)

**Tags**: cs.CL cs.AI cs.LG 



### Joint inference for gravitational wave signals and glitches using a   data-informed glitch model
**Authors**: Ann-Kristin Malz, John Veitch

**Updated**: 2025-05-01T16:59:36Z

**Summary**: Gravitational wave data are often contaminated by non-Gaussian noise transients, glitches, which can bias the inference of astrophysical signal parameters. Traditional approaches either subtract glitches in a pre-processing step, or a glitch model can be included from an agnostic wavelet basis (e.g. BayesWave). In this work, we introduce a machine-learning-based approach to build a parameterised model of glitches. We train a normalising flow on known glitches from the Gravity Spy catalogue, constructing an informative prior on the glitch model. By incorporating this model into the Bayesian inference analysis with Bilby, we estimate glitch and signal parameters simultaneously. We demonstrate the performance of our method through bias reduction, glitch identification and Bayesian model selection on real glitches. Our results show that this approach effectively removes glitches from the data, significantly improving source parameter estimation and reducing bias.

**Link**: [arxiv](http://arxiv.org/abs/2505.00657v1),  [pdf](http://arxiv.org/pdf/2505.00657v1)

**Tags**: gr-qc astro-ph.HE astro-ph.IM 



### Large Language Models Understanding: an Inherent Ambiguity Barrier
**Authors**: Daniel N. Nissani

**Updated**: 2025-05-01T16:55:44Z

**Summary**: A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.

**Link**: [arxiv](http://arxiv.org/abs/2505.00654v1),  [pdf](http://arxiv.org/pdf/2505.00654v1)

**Tags**: cs.CL cs.AI 



### Open-Source LLM-Driven Federated Transformer for Predictive IoV   Management
**Authors**: Yazan Otoum, Arghavan Asad, Ishtiaq Ahmad

**Updated**: 2025-05-01T16:54:21Z

**Summary**: The proliferation of connected vehicles within the Internet of Vehicles (IoV) ecosystem presents critical challenges in ensuring scalable, real-time, and privacy-preserving traffic management. Existing centralized IoV solutions often suffer from high latency, limited scalability, and reliance on proprietary Artificial Intelligence (AI) models, creating significant barriers to widespread deployment, particularly in dynamic and privacy-sensitive environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular systems remains underexplored, especially concerning prompt optimization and effective utilization in federated contexts. To address these challenges, we propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel framework that leverages open-source LLMs for predictive IoV management. FPoTT introduces a dynamic prompt optimization mechanism that iteratively refines textual prompts to enhance trajectory prediction. The architecture employs a dual-layer federated learning paradigm, combining lightweight edge models for real-time inference with cloud-based LLMs to retain global intelligence. A Transformer-driven synthetic data generator is incorporated to augment training with diverse, high-fidelity traffic scenarios in the Next Generation Simulation (NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data while maintaining high performance on synthetic datasets. These results underscore the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management, offering a promising alternative to proprietary solutions in smart mobility ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00651v1),  [pdf](http://arxiv.org/pdf/2505.00651v1)

**Tags**: cs.AI cs.ET cs.LG 



### Investigating Task Arithmetic for Zero-Shot Information Retrieval
**Authors**: Marco Braga, Pranav Kasela, Alessandro Raganato, Gabriella Pasi

**Updated**: 2025-05-01T16:48:37Z

**Summary**: Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.

**Link**: [arxiv](http://arxiv.org/abs/2505.00649v1),  [pdf](http://arxiv.org/pdf/2505.00649v1)

**Tags**: cs.IR cs.CL cs.LG 



### Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and   Neural Networks
**Authors**: Erin Carson, Xinye Chen

**Updated**: 2025-05-01T16:35:03Z

**Summary**: Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.   In this paper, we offer a comprehensive exposition of the design, implementation, validation, and practical application of Pychop, establishing it as a foundational tool for advancing efficient mixed-precision algorithms. Furthermore, we present empirical results on low-precision emulation for image classification and object detection using published datasets, illustrating the sensitivity of the use of low precision and offering valuable insights into its impact. Pychop enables in-depth investigations into the effects of numerical precision, facilitates the development of novel hardware accelerators, and integrates seamlessly into existing deep learning workflows. Software and experimental code are publicly available at https://github.com/inEXASCALE/pychop.

**Link**: [arxiv](http://arxiv.org/abs/2504.07835v4),  [pdf](http://arxiv.org/pdf/2504.07835v4)

**Tags**: cs.LG cs.NA math.NA 



### Automated Review Generation Method Based on Large Language Models
**Authors**: Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Changying Du, Zhi-Jian Zhao, Jinlong Gong

**Updated**: 2025-05-01T16:24:05Z

**Summary**: Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\% with 95\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.

**Link**: [arxiv](http://arxiv.org/abs/2407.20906v5),  [pdf](http://arxiv.org/pdf/2407.20906v5)

**Tags**: cs.CL cs.AI physics.data-an 



### Variational Self-Supervised Learning
**Authors**: Mehmet Can Yavuz, Berrin Yanikoglu

**Updated**: 2025-05-01T16:21:49Z

**Summary**: We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.

**Link**: [arxiv](http://arxiv.org/abs/2504.04318v3),  [pdf](http://arxiv.org/pdf/2504.04318v3)

**Tags**: cs.LG cs.CV 



### SOMA: a novel sampler for exchangeable variables
**Authors**: Yifei Xiong, Nianqiao Phyllis Ju

**Updated**: 2025-05-01T16:20:16Z

**Summary**: The problem of sampling exchangeable random variables arises in many Bayesian inference tasks, especially in data imputation given a privatized summary statistics. These permutation-invariant joint distributions often have dependency structures that make sampling challenging. Component-wise sampling strategies, such as Metropolis-within-Gibbs, can mix slowly because they consider only comparing a proposed point with one component at a time. In this work, we propose a novel Single-Offer-Multiple-Attempts (SOMA) sampler that is tailored to sampling permutation invariant distributions. The core intuition of SOMA is that a proposed point unsuitable to replace one component might still be a good candidate to replace some other component in the joint distribution. SOMA first makes a singer offer, and then simultaneously considers attempts to replace each component of the current state with the single offer, before making the final acceptance or rejection decision. We provide an acceptance lower bound of SOMA and, using a coupling method, derive the convergence rate upper bound of SOMA in the two-dimensional case. We validate theoretical findings with numerical simulations, including a demonstration on differentially private Bayesian linear regression.

**Link**: [arxiv](http://arxiv.org/abs/2505.00635v1),  [pdf](http://arxiv.org/pdf/2505.00635v1)

**Tags**: stat.ME 



### Reward-Augmented Data Enhances Direct Preference Alignment of LLMs
**Authors**: Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang

**Updated**: 2025-05-01T16:20:11Z

**Summary**: Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion. Our code is available at https://github.com/shenao-zhang/reward-augmented-preference.

**Link**: [arxiv](http://arxiv.org/abs/2410.08067v4),  [pdf](http://arxiv.org/pdf/2410.08067v4)

**Tags**: cs.LG cs.AI 



### OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation   Learning
**Authors**: Anirudhan Badrinath, Alex Yang, Kousik Rajesh, Prabhat Agarwal, Jaewon Yang, Haoyu Chen, Jiajing Xu, Charles Rosenberg

**Updated**: 2025-05-01T16:17:50Z

**Summary**: Representation learning, a task of learning latent vectors to represent entities, is a key task in improving search and recommender systems in web applications. Various representation learning methods have been developed, including graph-based approaches for relationships among entities, sequence-based methods for capturing the temporal evolution of user activities, and content-based models for leveraging text and visual content. However, the development of a unifying framework that integrates these diverse techniques to support multiple applications remains a significant challenge. This paper presents OmniSage, a large-scale representation framework that learns universal representations for a variety of applications at Pinterest. OmniSage integrates graph neural networks with content-based models and user sequence models by employing multiple contrastive learning tasks to effectively process graph data, user sequence data, and content signals. To support the training and inference of OmniSage, we developed an efficient infrastructure capable of supporting Pinterest graphs with billions of nodes. The universal representations generated by OmniSage have significantly enhanced user experiences on Pinterest, leading to an approximate 2.5% increase in sitewide repins (saves) across five applications. This paper highlights the impact of unifying representation learning methods, and we will open source the OmniSage code by the time of publication.

**Link**: [arxiv](http://arxiv.org/abs/2504.17811v2),  [pdf](http://arxiv.org/pdf/2504.17811v2)

**Tags**: cs.IR cs.LG 



### Detecting Modeling Bias with Continuous Time Flow Models on Weak Lensing   Maps
**Authors**: Kangning Diao, Biwei Dai, Uros Seljak

**Updated**: 2025-05-01T16:16:47Z

**Summary**: Simulation-based inference provides a powerful framework for extracting rich information from nonlinear scales in current and upcoming cosmological surveys, and ensuring its robustness requires stringent validation of forward models. In this work, we recast forward model validation as an out-of-distribution (OoD) detection problem within the framework of machine learning (ML)-based simulation-based inference (SBI). We employ probability density as the metric for OoD detection, and compare various density estimation techniques, demonstrating that field-level probability density estimation via continuous time flow models (CTFM) significantly outperforms feature-level approaches that combine scattering transform (ST) or convolutional neural networks (CNN) with normalizing flows (NFs), as well as NF-based field-level estimators, as quantified by the area under the receiver operating characteristic curve (AUROC). Our analysis shows that CTFM not only excels in detecting OoD samples but also provides a robust metric for model selection. Additionally, we verified CTFM maintains consistent efficacy across different cosmologies while mitigating the inductive biases inherent in NF architectures. Although our proof-of-concept study employs simplified forward modeling and noise settings, our framework establishes a promising pathway for identifying unknown systematics in the cosmology datasets.

**Link**: [arxiv](http://arxiv.org/abs/2505.00632v1),  [pdf](http://arxiv.org/pdf/2505.00632v1)

**Tags**: astro-ph.CO astro-ph.IM 



### The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning   (and How to Fix Them)
**Authors**: Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang

**Updated**: 2025-05-01T16:06:16Z

**Summary**: Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.

**Link**: [arxiv](http://arxiv.org/abs/2505.00626v1),  [pdf](http://arxiv.org/pdf/2505.00626v1)

**Tags**: cs.CL cs.AI 68T50 I.2 



### FineScope : Precision Pruning for Domain-Specialized Large Language   Models Using SAE-Guided Self-Data Cultivation
**Authors**: Chaitali Bhattacharyya, Yeseong Kim

**Updated**: 2025-05-01T16:05:08Z

**Summary**: Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2505.00624v1),  [pdf](http://arxiv.org/pdf/2505.00624v1)

**Tags**: cs.CL cs.AI 



### Locally minimax optimal and dimension-agnostic discrete argmin inference
**Authors**: Ilmun Kim, Aaditya Ramdas

**Updated**: 2025-05-01T15:51:08Z

**Summary**: This paper tackles a fundamental inference problem: given $n$ observations from a $d$ dimensional vector with unknown mean $\boldsymbol{\mu}$, we must form a confidence set for the index (or indices) corresponding to the smallest component of $\boldsymbol{\mu}$. By duality, we reduce this to testing, for each $r$ in $1,\ldots,d$, whether $\mu_r$ is the smallest. Based on the sample splitting and self-normalization approach of Kim and Ramdas (2024), we propose "dimension-agnostic" tests that maintain validity regardless of how $d$ scales with $n$, and regardless of arbitrary ties in $\boldsymbol{\mu}$. Notably, our validity holds under mild moment conditions, requiring little more than finiteness of a second moment, and permitting possibly strong dependence between coordinates. In addition, we establish the local minimax separation rate for this problem, which adapts to the cardinality of a confusion set, and show that the proposed tests attain this rate. Furthermore, we develop robust variants that continue to achieve the same minimax rate under heavy-tailed distributions with only finite second moments. Empirical results on simulated and real data illustrate the strong performance of our approach in terms of type I error control and power compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2503.21639v2),  [pdf](http://arxiv.org/pdf/2503.21639v2)

**Tags**: math.ST stat.ME stat.ML stat.TH 



### Combining LLMs with Logic-Based Framework to Explain MCTS
**Authors**: Ziyan An, Xia Wang, Hendrik Baier, Zirong Chen, Abhishek Dubey, Taylor T. Johnson, Jonathan Sprinkle, Ayan Mukhopadhyay, Meiyi Ma

**Updated**: 2025-05-01T15:40:58Z

**Summary**: In response to the lack of trust in Artificial Intelligence (AI) for sequential planning, we design a Computational Tree Logic-guided large language model (LLM)-based natural language explanation framework designed for the Monte Carlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to interpret due to the complexity of its search trees, but our framework is flexible enough to handle a wide range of free-form post-hoc queries and knowledge-based inquiries centered around MCTS and the Markov Decision Process (MDP) of the application domain. By transforming user queries into logic and variable statements, our framework ensures that the evidence obtained from the search tree remains factually consistent with the underlying environmental dynamics and any constraints in the actual stochastic control process. We evaluate the framework rigorously through quantitative assessments, where it demonstrates strong performance in terms of accuracy and factual consistency.

**Link**: [arxiv](http://arxiv.org/abs/2505.00610v1),  [pdf](http://arxiv.org/pdf/2505.00610v1)

**Tags**: cs.AI 



### Can LLMs Help Improve Analogical Reasoning For Strategic Decisions?   Experimental Evidence from Humans and GPT-4
**Authors**: Phanish Puranam, Prothit Sen, Maciej Workiewicz

**Updated**: 2025-05-01T15:35:01Z

**Summary**: This study investigates whether large language models, specifically GPT4, can match human capabilities in analogical reasoning within strategic decision making contexts. Using a novel experimental design involving source to target matching, we find that GPT4 achieves high recall by retrieving all plausible analogies but suffers from low precision, frequently applying incorrect analogies based on superficial similarities. In contrast, human participants exhibit high precision but low recall, selecting fewer analogies yet with stronger causal alignment. These findings advance theory by identifying matching, the evaluative phase of analogical reasoning, as a distinct step that requires accurate causal mapping beyond simple retrieval. While current LLMs are proficient in generating candidate analogies, humans maintain a comparative advantage in recognizing deep structural similarities across domains. Error analysis reveals that AI errors arise from surface level matching, whereas human errors stem from misinterpretations of causal structure. Taken together, the results suggest a productive division of labor in AI assisted organizational decision making where LLMs may serve as broad analogy generators, while humans act as critical evaluators, applying the most contextually appropriate analogies to strategic problems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00603v1),  [pdf](http://arxiv.org/pdf/2505.00603v1)

**Tags**: cs.AI cs.HC 



### Explainable AI in Spatial Analysis
**Authors**: Ziqi Li

**Updated**: 2025-05-01T15:25:23Z

**Summary**: This chapter discusses the opportunities of eXplainable Artificial Intelligence (XAI) within the realm of spatial analysis. A key objective in spatial analysis is to model spatial relationships and infer spatial processes to generate knowledge from spatial data, which has been largely based on spatial statistical methods. More recently, machine learning offers scalable and flexible approaches that complement traditional methods and has been increasingly applied in spatial data science. Despite its advantages, machine learning is often criticized for being a black box, which limits our understanding of model behavior and output. Recognizing this limitation, XAI has emerged as a pivotal field in AI that provides methods to explain the output of machine learning models to enhance transparency and understanding. These methods are crucial for model diagnosis, bias detection, and ensuring the reliability of results obtained from machine learning models. This chapter introduces key concepts and methods in XAI with a focus on Shapley value-based approaches, which is arguably the most popular XAI method, and their integration with spatial analysis. An empirical example of county-level voting behaviors in the 2020 Presidential election is presented to demonstrate the use of Shapley values and spatial analysis with a comparison to multi-scale geographically weighted regression. The chapter concludes with a discussion on the challenges and limitations of current XAI techniques and proposes new directions.

**Link**: [arxiv](http://arxiv.org/abs/2505.00591v1),  [pdf](http://arxiv.org/pdf/2505.00591v1)

**Tags**: cs.LG econ.EM 



### AMUN: Adversarial Machine UNlearning
**Authors**: Ali Ebrahimpour-Boroojeny, Hari Sundaram, Varun Chandrasekaran

**Updated**: 2025-05-01T15:21:54Z

**Summary**: Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on ``exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, ``approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random $10\%$ of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing.

**Link**: [arxiv](http://arxiv.org/abs/2503.00917v2),  [pdf](http://arxiv.org/pdf/2503.00917v2)

**Tags**: cs.LG cs.CR 



### Block Circulant Adapter for Large Language Models
**Authors**: Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang

**Updated**: 2025-05-01T15:14:32Z

**Summary**: Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2505.00582v1),  [pdf](http://arxiv.org/pdf/2505.00582v1)

**Tags**: cs.CL cs.LG 



### AI-in-the-Loop Planning for Transportation Electrification: Case Studies   from Austin, Texas
**Authors**: Seung Jun Choi

**Updated**: 2025-05-01T15:07:50Z

**Summary**: This study explores the integration of AI in transportation electrification planning in Austin, TX, focusing on the use of Geospatial AI (GeoAI), Generative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site selection, localized GenAI models support meta-level estimations, and LLMs enable scenario simulations. These AI applications require human oversight. GeoAI outputs must be evaluated with land use data, GenAI models are not always accurate, and LLMs are prone to hallucinations. To ensure accountable planning, human planners must work alongside AI agents. Establishing a community feedback loop is essential to audit automated decisions. Planners should place Community Experience (CX) at the center of Urban Planning AI.

**Link**: [arxiv](http://arxiv.org/abs/2504.21185v2),  [pdf](http://arxiv.org/pdf/2504.21185v2)

**Tags**: cs.CY 



### Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase   Transition in Large Language Models
**Authors**: Makoto Sato

**Updated**: 2025-05-01T14:58:32Z

**Summary**: What underlies intuitive human thinking? One approach to this question is to compare the cognitive dynamics of humans and large language models (LLMs). However, such a comparison requires a method to quantitatively analyze AI cognitive behavior under controlled conditions. While anecdotal observations suggest that certain prompts can dramatically change LLM behavior, these observations have remained largely qualitative. Here, we propose a two-part framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP) that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying Prompt (TQP) that evaluates this change using a separate LLM. Through controlled experiments, we examined how LLMs react to prompts embedding two semantically distant concepts (e.g., mathematical aperiodicity and traditional crafts)-either fused together or presented separately-by changing their linguistic quality and affective tone. Whereas humans tend to experience heightened engagement when such concepts are meaningfully blended producing a novel concept-a form of conceptual fusion-current LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts. This suggests that LLMs may not yet replicate the conceptual integration processes seen in human intuition. Our method enables fine-grained, reproducible measurement of cognitive responsiveness, and may help illuminate key differences in how intuition and conceptual leaps emerge in artificial versus human minds.

**Link**: [arxiv](http://arxiv.org/abs/2504.21012v2),  [pdf](http://arxiv.org/pdf/2504.21012v2)

**Tags**: cs.CL cs.AI 



### Hypothesis-free discovery from epidemiological data by automatic   detection and local inference for tree-based nonlinearities and interactions
**Authors**: Giorgio Spadaccini, Marjolein Fokkema, Mark A. van de Wiel

**Updated**: 2025-05-01T14:55:22Z

**Summary**: In epidemiological settings, Machine Learning (ML) is gaining popularity for hypothesis-free discovery of risk (or protective) factors. Although ML is strong at discovering non-linearities and interactions, this power is currently compromised by a lack of reliable inference. Although local measures of feature effect can be combined with tree ensembles, uncertainty quantifications for these measures remain only partially available and oftentimes unsatisfactory. We propose RuleSHAP, a framework for using rule-based, hypothesis-free discovery that combines sparse Bayesian regression, tree ensembles and Shapley values in a one-step procedure that both detects and tests complex patterns at the individual level. To ease computation, we derive a formula that computes marginal Shapley values more efficiently for our setting. We demonstrate the validity of our framework on simulated data. To illustrate, we apply our machinery to data from an epidemiological cohort to detect and infer several effects for high cholesterol and blood pressure, such as nonlinear interaction effects between features like age, sex, ethnicity, BMI and glucose level.

**Link**: [arxiv](http://arxiv.org/abs/2505.00571v1),  [pdf](http://arxiv.org/pdf/2505.00571v1)

**Tags**: stat.ML cs.LG 



### UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models   for Multilingual Multimodal Idiomaticity Representation
**Authors**: Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang

**Updated**: 2025-05-01T14:54:16Z

**Summary**: SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.

**Link**: [arxiv](http://arxiv.org/abs/2502.20984v3),  [pdf](http://arxiv.org/pdf/2502.20984v3)

**Tags**: cs.CL cs.AI 



### FreqKV: Frequency Domain Key-Value Compression for Efficient Context   Window Extension
**Authors**: Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin

**Updated**: 2025-05-01T14:53:12Z

**Summary**: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2505.00570v1),  [pdf](http://arxiv.org/pdf/2505.00570v1)

**Tags**: cs.CL cs.AI 



### TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching
**Authors**: Yue Meng, Chuchu Fan

**Updated**: 2025-05-01T14:40:07Z

**Summary**: Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF

**Link**: [arxiv](http://arxiv.org/abs/2505.00562v1),  [pdf](http://arxiv.org/pdf/2505.00562v1)

**Tags**: cs.RO cs.AI cs.FL cs.LG 



### Efficient Recommendation with Millions of Items by Dynamic Pruning of   Sub-Item Embeddings
**Authors**: Aleksandr V. Petrov, Craig Macdonald, Nicola Tonellotto

**Updated**: 2025-05-01T14:36:33Z

**Summary**: A large item catalogue is a major challenge for deploying modern sequential recommender models, since it makes the memory footprint of the model large and increases inference latency. One promising approach to address this is RecJPQ, which replaces item embeddings with sub-item embeddings. However, slow inference remains problematic because finding the top highest-scored items usually requires scoring all items in the catalogue, which may not be feasible for large catalogues. By adapting dynamic pruning concepts from document retrieval, we propose the RecJPQPrune dynamic pruning algorithm to efficiently find the top highest-scored items without computing the scores of all items in the catalogue. Our RecJPQPrune algorithm is safe-up-to-rank K since it theoretically guarantees that no potentially high-scored item is excluded from the final top K recommendation list, thereby ensuring no impact on effectiveness. Our experiments on two large datasets and three recommendation models demonstrate the efficiency achievable using RecJPQPrune: for instance, on the Tmall dataset with 2.2M items, we can reduce the median model scoring time by 64 times compared to the Transformer Default baseline, and 5.3 times compared to a recent scoring approach called PQTopK. Overall, this paper demonstrates the effective and efficient inference of Transformer-based recommendation models at catalogue scales not previously reported in the literature. Indeed, our RecJPQPrune algorithm can score 2 million items in under 10 milliseconds without GPUs, and without relying on Approximate Nearest Neighbour (ANN) techniques.

**Link**: [arxiv](http://arxiv.org/abs/2505.00560v1),  [pdf](http://arxiv.org/pdf/2505.00560v1)

**Tags**: cs.IR 



### Triggering Hallucinations in LLMs: A Quantitative Study of   Prompt-Induced Hallucination in Large Language Models
**Authors**: Makoto Sato

**Updated**: 2025-05-01T14:33:47Z

**Summary**: Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.

**Link**: [arxiv](http://arxiv.org/abs/2505.00557v1),  [pdf](http://arxiv.org/pdf/2505.00557v1)

**Tags**: cs.CL cs.AI 



### On the Mechanistic Interpretability of Neural Networks for Causality in   Bio-statistics
**Authors**: Jean-Baptiste A. Conan

**Updated**: 2025-05-01T14:30:34Z

**Summary**: Interpretable insights from predictive models remain critical in bio-statistics, particularly when assessing causality, where classical statistical and machine learning methods often provide inherent clarity. While Neural Networks (NNs) offer powerful capabilities for modeling complex biological data, their traditional "black-box" nature presents challenges for validation and trust in high-stakes health applications. Recent advances in Mechanistic Interpretability (MI) aim to decipher the internal computations learned by these networks. This work investigates the application of MI techniques to NNs within the context of causal inference for bio-statistics.   We demonstrate that MI tools can be leveraged to: (1) probe and validate the internal representations learned by NNs, such as those estimating nuisance functions in frameworks like Targeted Minimum Loss-based Estimation (TMLE); (2) discover and visualize the distinct computational pathways employed by the network to process different types of inputs, potentially revealing how confounders and treatments are handled; and (3) provide methodologies for comparing the learned mechanisms and extracted insights across statistical, machine learning, and NN models, fostering a deeper understanding of their respective strengths and weaknesses for causal bio-statistical analysis.

**Link**: [arxiv](http://arxiv.org/abs/2505.00555v1),  [pdf](http://arxiv.org/pdf/2505.00555v1)

**Tags**: stat.AP cs.AI 



### Graph Spectral Filtering with Chebyshev Interpolation for Recommendation
**Authors**: Chanwoo Kim, Jinkyu Sung, Yebonn Han, Joonseok Lee

**Updated**: 2025-05-01T14:28:44Z

**Summary**: Graph convolutional networks have recently gained prominence in collaborative filtering (CF) for recommendations. However, we identify potential bottlenecks in two foundational components. First, the embedding layer leads to a latent space with limited capacity, overlooking locally observed but potentially valuable preference patterns. Also, the widely-used neighborhood aggregation is limited in its ability to leverage diverse preference patterns in a fine-grained manner. Building on spectral graph theory, we reveal that these limitations stem from graph filtering with a cut-off in the frequency spectrum and a restricted linear form. To address these issues, we introduce ChebyCF, a CF framework based on graph spectral filtering. Instead of a learned embedding, it takes a user's raw interaction history to utilize the full spectrum of signals contained in it. Also, it adopts Chebyshev interpolation to effectively approximate a flexible non-linear graph filter, and further enhances it by using an additional ideal pass filter and degree-based normalization. Through extensive experiments, we verify that ChebyCF overcomes the aforementioned bottlenecks and achieves state-of-the-art performance across multiple benchmarks and reasonably fast inference. Our code is available at https://github.com/chanwoo0806/ChebyCF.

**Link**: [arxiv](http://arxiv.org/abs/2505.00552v1),  [pdf](http://arxiv.org/pdf/2505.00552v1)

**Tags**: cs.IR cs.LG 



### Reducing Student Distraction Through Fuzzy Logic Based Seating   Arrangements
**Authors**: Garrett Olges, Kelly Cohen

**Updated**: 2025-05-01T14:20:45Z

**Summary**: A crucial skill for primary school teachers is maintaining efficient classroom management. Teachers use classroom seating arrangements to help maintain this efficiency. However, developing classroom seating arrangements is both time-consuming and often non-optimal for distraction mitigation. Fuzzy logic-based approaches for the development of classroom seating arrangements can reduce development time and minimize classroom distraction. In this study, an original fuzzy logic-based software package named "CUB" is introduced and applied to a modern classroom using "cluster" seating arrangements. The combination of fuzzy inference systems, fuzzy c-means clustering, sequential, and iterative processes produce ready-to-use seating arrangements for the classroom in this study. The seating arrangements are compared with an existing set of seating arrangements to validate the results. The author's findings show that CUB is successful in generating applicable seating arrangements with a small liklihood of replicating arrangements. The findings also suggest that fuzz logic-based approaches may be successful in other styles of classroom arrangement.

**Link**: [arxiv](http://arxiv.org/abs/2505.00545v1),  [pdf](http://arxiv.org/pdf/2505.00545v1)

**Tags**: cs.LO 



### FLAMINGO: combining kinetic SZ effect and galaxy-galaxy lensing   measurements to gauge the impact of feedback on large-scale structure
**Authors**: Ian G. McCarthy, Alexandra Amon, Joop Schaye, Emmanuel Schaan, Raul E. Angulo, Jaime Salcido, Matthieu Schaller, Leah Bigwood, Willem Elbers, Roi Kugel, John C. Helly, Victor J. Forouhar Moreno, Carlos S. Frenk, Robert J. McGibbon, Lurdes Ondaro-Mallea, Marcel P. van Daalen

**Updated**: 2025-05-01T14:17:28Z

**Summary**: Energetic feedback processes associated with accreting supermassive black holes can expel gas from massive haloes and significantly alter various measures of clustering on ~Mpc scales, potentially biasing the values of cosmological parameters inferred from analyses of large-scale structure (LSS) if not modelled accurately. Here we use the state-of-the-art FLAMINGO suite of cosmological hydrodynamical simulations to gauge the impact of feedback on large-scale structure by comparing to Planck + ACT stacking measurements of the kinetic Sunyaev-Zel'dovich (kSZ) effect of SDSS BOSS galaxies. We make careful like-with-like comparisons to the observations, aided by high precision KiDS and DES galaxy-galaxy lensing measurements of the BOSS galaxies to inform the selection of the simulated galaxies. In qualitative agreement with several recent studies using dark matter only simulations corrected for baryonic effects, we find that the kSZ effect measurements prefer stronger feedback than predicted by simulations which have been calibrated to reproduce the gas fractions of low redshift X-ray-selected groups and clusters. We find that the increased feedback can help to reduce the so-called S8 tension between the observed and CMB-predicted clustering on small scales as probed by cosmic shear (although at the expense of agreement with the X-ray group measurements). However, the increased feedback is only marginally effective at reducing the reported offsets between the predicted and observed clustering as probed by the thermal SZ (tSZ) effect power spectrum and tSZ effect--weak lensing cross-spectrum, both of which are sensitive to higher halo masses than cosmic shear.

**Link**: [arxiv](http://arxiv.org/abs/2410.19905v2),  [pdf](http://arxiv.org/pdf/2410.19905v2)

**Tags**: astro-ph.CO 



### Efficiency and Effectiveness of LLM-Based Summarization of Evidence in   Crowdsourced Fact-Checking
**Authors**: Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro

**Updated**: 2025-05-01T14:05:16Z

**Summary**: Evaluating the truthfulness of online content is critical for combating misinformation. This study examines the efficiency and effectiveness of crowdsourced truthfulness assessments through a comparative analysis of two approaches: one involving full-length webpages as evidence for each claim, and another using summaries for each evidence document generated with a large language model. Using an A/B testing setting, we engage a diverse pool of participants tasked with evaluating the truthfulness of statements under these conditions. Our analysis explores both the quality of assessments and the behavioral patterns of participants. The results reveal that relying on summarized evidence offers comparable accuracy and error metrics to the Standard modality while significantly improving efficiency. Workers in the Summary setting complete a significantly higher number of assessments, reducing task duration and costs. Additionally, the Summary modality maximizes internal agreement and maintains consistent reliance on and perceived usefulness of evidence, demonstrating its potential to streamline large-scale truthfulness evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2501.18265v2),  [pdf](http://arxiv.org/pdf/2501.18265v2)

**Tags**: cs.IR cs.CL cs.HC 



### Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks
**Authors**: Xinyu Wang, Jinbo Bi, Minghu Song

**Updated**: 2025-05-01T13:57:20Z

**Summary**: SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.

**Link**: [arxiv](http://arxiv.org/abs/2505.00530v1),  [pdf](http://arxiv.org/pdf/2505.00530v1)

**Tags**: cs.LG cs.CE q-bio.BM 



### DeCo: Task Decomposition and Skill Composition for Zero-Shot   Generalization in Long-Horizon 3D Manipulation
**Authors**: Zixuan Chen, Junhui Yin, Yangtao Chen, Jing Huo, Pinzhuo Tian, Jieqi Shi, Yiwen Hou, Yinchuan Li, Yang Gao

**Updated**: 2025-05-01T13:52:19Z

**Summary**: Generalizing language-conditioned multi-task imitation learning (IL) models to novel long-horizon 3D manipulation tasks remains a significant challenge. To address this, we propose DeCo (Task Decomposition and Skill Composition), a model-agnostic framework compatible with various multi-task IL models, designed to enhance their zero-shot generalization to novel, compositional, long-horizon 3D manipulation tasks. DeCo first decomposes IL demonstrations into a set of modular atomic tasks based on the physical interaction between the gripper and objects, and constructs an atomic training dataset that enables models to learn a diverse set of reusable atomic skills during imitation learning. At inference time, DeCo leverages a vision-language model (VLM) to parse high-level instructions for novel long-horizon tasks, retrieve the relevant atomic skills, and dynamically schedule their execution; a spatially-aware skill-chaining module then ensures smooth, collision-free transitions between sequential skills. We evaluate DeCo in simulation using DeCoBench, a benchmark specifically designed to assess zero-shot generalization of multi-task IL models in compositional long-horizon 3D manipulation. Across three representative multi-task IL models (RVT-2, 3DDA, and ARP), DeCo achieves success rate improvements of 66.67%, 21.53%, and 57.92%, respectively, on 12 novel compositional tasks. Moreover, in real-world experiments, a DeCo-enhanced model trained on only 6 atomic tasks successfully completes 9 novel long-horizon tasks, yielding an average success rate improvement of 53.33% over the base multi-task IL model. Video demonstrations are available at: https://deco226.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2505.00527v1),  [pdf](http://arxiv.org/pdf/2505.00527v1)

**Tags**: cs.RO 



### HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World   Hallucination Detection
**Authors**: Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu

**Updated**: 2025-05-01T13:22:45Z

**Summary**: As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.

**Link**: [arxiv](http://arxiv.org/abs/2505.00506v1),  [pdf](http://arxiv.org/pdf/2505.00506v1)

**Tags**: cs.CL cs.AI 



### Distilling Calibration via Conformalized Credal Inference
**Authors**: Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone

**Updated**: 2025-05-01T13:04:11Z

**Summary**: Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.

**Link**: [arxiv](http://arxiv.org/abs/2501.06066v3),  [pdf](http://arxiv.org/pdf/2501.06066v3)

**Tags**: cs.LG cs.AI eess.SP 



### Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and   Datasets
**Authors**: Lorenz Brehme, Thomas Ströhle, Ruth Breu

**Updated**: 2025-05-01T13:03:37Z

**Summary**: Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2504.20119v2),  [pdf](http://arxiv.org/pdf/2504.20119v2)

**Tags**: cs.IR cs.AI 



### Intracluster light is a biased tracer of the dark matter distribution in   clusters
**Authors**: J. Butler, G. Martin, N. A. Hatch, F. Pearce, S. Brough, Y. Dubois

**Updated**: 2025-05-01T12:47:52Z

**Summary**: The diffuse stellar component of galaxy clusters known as intracluster light (ICL) has been proposed as an observable tracer of the cluster's dark matter (DM) halo. Assessing its reliability as a DM tracer requires understanding how the intracluster stars are energetically linked to the underlying DM distribution, which we investigate at $z\approx0$ in 12 galaxy clusters with $M_{178} = 1.18 - 3.71 \times 10^{14}\,\textrm{M}_\odot$ from the Horizon-AGN simulation. We quantify the orbital energies of these components by their mean specific energies ${\langle \varepsilon \rangle}$, and find that this quantity is $\approx$ 25 per cent lower for the intracluster stars than the DM, whilst the energetics of the satellite galaxies (a standard DM tracer) are only marginally ($\approx$ 5 per cent) higher than the DM. Importantly, the lower ${\langle \varepsilon \rangle}$ of the intracluster stars compared to the DM is robust against the precise separation between the brightest cluster galaxy (BCG) and the ICL. The specific energy distribution of ICL stars is concentrated towards lower energies and poorly samples the higher energies, where much of the DM resides. Consequently, the intracluster stars have velocity distributions with lower typical speeds and a more centrally-concentrated density profile than the DM. We also find that intracluster stars have more radially-biased orbits than the DM, indicating these components have distinct orbital distributions. This study demonstrates that although the morphology of the ICL may match the DM halo, the ICL is a biased tracer of DM, and these biases must be understood in order to infer properties of the DM from the ICL.

**Link**: [arxiv](http://arxiv.org/abs/2504.03518v2),  [pdf](http://arxiv.org/pdf/2504.03518v2)

**Tags**: astro-ph.GA astro-ph.CO 



### (Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration   for Translating Ultra-Long Literary Texts
**Authors**: Minghao Wu, Jiahao Xu, Yulin Yuan, Gholamreza Haffari, Longyue Wang, Weihua Luo, Kaifu Zhang

**Updated**: 2025-05-01T12:02:02Z

**Summary**: Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.

**Link**: [arxiv](http://arxiv.org/abs/2405.11804v2),  [pdf](http://arxiv.org/pdf/2405.11804v2)

**Tags**: cs.CL 



### Rule-based Classifier Models
**Authors**: Cecilia Di Florio, Huimin Dong, Antonino Rotolo

**Updated**: 2025-05-01T11:59:16Z

**Summary**: We extend the formal framework of classifier models used in the legal domain. While the existing classifier framework characterises cases solely through the facts involved, legal reasoning fundamentally relies on both facts and rules, particularly the ratio decidendi. This paper presents an initial approach to incorporating sets of rules within a classifier. Our work is built on the work of Canavotto et al. (2023), which has developed the rule-based reason model of precedential constraint within a hierarchy of factors. We demonstrate how decisions for new cases can be inferred using this enriched rule-based classifier framework. Additionally, we provide an example of how the time element and the hierarchy of courts can be used in the new classifier framework.

**Link**: [arxiv](http://arxiv.org/abs/2505.00474v1),  [pdf](http://arxiv.org/pdf/2505.00474v1)

**Tags**: cs.AI 



### EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful   Prompt Optimizers
**Authors**: Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang

**Updated**: 2025-05-01T11:56:52Z

**Summary**: Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2309.08532v3),  [pdf](http://arxiv.org/pdf/2309.08532v3)

**Tags**: cs.CL cs.AI 



### UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces
**Authors**: Alaa Saleh, Sasu Tarkoma, Praveen Kumar Donta, Naser Hossein Motlagh, Schahram Dustdar, Susanna Pirttikangas, Lauri Lovén

**Updated**: 2025-05-01T11:54:49Z

**Summary**: Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.

**Link**: [arxiv](http://arxiv.org/abs/2505.00472v1),  [pdf](http://arxiv.org/pdf/2505.00472v1)

**Tags**: cs.AI cs.DC cs.MA cs.NI 



### Red Teaming Large Language Models for Healthcare
**Authors**: Vahid Balazadeh, Michael Cooper, David Pellow, Atousa Assadi, Jennifer Bell, Jim Fackler, Gabriel Funingana, Spencer Gable-Cook, Anirudh Gangadhar, Abhishek Jaiswal, Sumanth Kaja, Christopher Khoury, Randy Lin, Kaden McKeen, Sara Naimimohasses, Khashayar Namdar, Aviraj Newatia, Allan Pang, Anshul Pattoo, Sameer Peesapati, Diana Prepelita, Bogdana Rakova, Saba Sadatamin, Rafael Schulman, Ajay Shah, Syed Azhar Shah, Syed Ahmar Shah, Babak Taati, Balagopal Unnikrishnan, Stephanie Williams, Rahul G Krishnan

**Updated**: 2025-05-01T11:43:27Z

**Summary**: We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.

**Link**: [arxiv](http://arxiv.org/abs/2505.00467v1),  [pdf](http://arxiv.org/pdf/2505.00467v1)

**Tags**: cs.CL cs.AI 



### Bayesian Model Averaging in Causal Instrumental Variable Models
**Authors**: Gregor Steiner, Mark Steel

**Updated**: 2025-05-01T11:09:37Z

**Summary**: Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.

**Link**: [arxiv](http://arxiv.org/abs/2504.13520v2),  [pdf](http://arxiv.org/pdf/2504.13520v2)

**Tags**: stat.ME econ.EM math.ST stat.TH 



### The iterated Dirichlet process and applications to Bayesian inference
**Authors**: Evan Donald, Jason Swanson

**Updated**: 2025-05-01T10:53:49Z

**Summary**: Consider an i.i.d. sequence of random variables, taking values in some space $S$, whose underlying distribution is unknown. In problems of Bayesian inference, one models this unknown distribution as a random measure, and the law of this random measure is the prior. When $S = \{0, 1\}$, a commonly used prior is the uniform distribution on $[0, 1]$, or more generally, the beta distribution. When $S$ is finite, the analogous choice is the Dirichlet distribution. For a general space $S$, we are led naturally to the Dirichlet process (see [Ferguson, 1973]).   Here, we consider an array of random variables, and in so doing are led to what we call the iterated Dirichlet process (IDP). We define the IDP and then show how to compute the posterior distribution, given a finite set of observations, using the method of sequential imputation. Ordinarily, this method requires the existence of certain joint density functions, which the IDP lacks. We therefore present a new, more general proof of the validity of sequential imputation, and show that the hypotheses of our proof are satisfied by the IDP.

**Link**: [arxiv](http://arxiv.org/abs/2505.00451v1),  [pdf](http://arxiv.org/pdf/2505.00451v1)

**Tags**: math.ST math.PR stat.TH 62G05 (Primary) 60G57, 62D10, 62M20 (Secondary) 



### Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion
**Authors**: Hao Wen, Zehuan Huang, Yaohui Wang, Xinyuan Chen, Lu Sheng

**Updated**: 2025-05-01T10:43:02Z

**Summary**: Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction. However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results. We introduce a unified 3D generation framework, named Ouroboros3D, which integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process. In our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference. During the multi-view denoising process, the multi-view diffusion model uses the 3D-aware maps rendered by the reconstruction module at the previous timestep as additional conditions. The recursive diffusion framework with 3D-aware feedback unites the entire process and improves geometric consistency.Experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase. Project page: https://costwen.github.io/Ouroboros3D/

**Link**: [arxiv](http://arxiv.org/abs/2406.03184v2),  [pdf](http://arxiv.org/pdf/2406.03184v2)

**Tags**: cs.CV 



### Distributed Retrieval-Augmented Generation
**Authors**: Chenhao Xu, Longxiang Gao, Yuan Miao, Xi Zheng

**Updated**: 2025-05-01T10:37:06Z

**Summary**: As large language models (LLMs) become increasingly adopted on edge devices, Retrieval-Augmented Generation (RAG) is gaining prominence as a solution to address factual deficiencies and hallucinations by integrating external knowledge. However, centralized RAG architectures face significant challenges in data privacy and scalability. For instance, smart healthcare services often rely on collecting sensitive patient data and building a centralized knowledge base to provide better diagnosis and treatment advice, while privacy concerns significantly impede this process. Besides, maintaining a comprehensive and continuously updated knowledge base is costly, particularly in response to regional epidemics and rapidly mutating viruses. To address these challenges, this paper introduces Distributed Retrieval-Augmented Generation (DRAG), a novel framework that improves data privacy by eliminating the need for a centralized knowledge base and restoring data control to owners. DRAG incorporates a Topic-Aware Random Walk (TARW) algorithm that leverages LLMs to extract query topics and facilitate targeted peer discovery within a peer-to-peer network, enabling efficient knowledge retrieval in decentralized environments. Extensive experiments across three diverse datasets and LLMs demonstrate that DRAG with TARW achieves near-centralized RAG performance by using half as many messages as flooding. The code is available at https://github.com/xuchenhao001/DRAG.

**Link**: [arxiv](http://arxiv.org/abs/2505.00443v1),  [pdf](http://arxiv.org/pdf/2505.00443v1)

**Tags**: cs.DC 



### Can Differentially Private Fine-tuning LLMs Protect Against Privacy   Attacks?
**Authors**: Hao Du, Shang Liu, Yang Cao

**Updated**: 2025-05-01T10:10:01Z

**Summary**: Fine-tuning large language models (LLMs) has become an essential strategy for adapting them to specialized tasks; however, this process introduces significant privacy challenges, as sensitive training data may be inadvertently memorized and exposed. Although differential privacy (DP) offers strong theoretical guarantees against such leakage, its empirical privacy effectiveness on LLMs remains unclear, especially under different fine-tuning methods. In this paper, we systematically investigate the impact of DP across fine-tuning methods and privacy budgets, using both data extraction and membership inference attacks to assess empirical privacy risks. Our main findings are as follows: (1) Differential privacy reduces model utility, but its impact varies significantly across different fine-tuning methods. (2) Without DP, the privacy risks of models fine-tuned with different approaches differ considerably. (3) When DP is applied, even a relatively high privacy budget can substantially lower privacy risk. (4) The privacy-utility trade-off under DP training differs greatly among fine-tuning methods, with some methods being unsuitable for DP due to severe utility degradation. Our results provide practical guidance for privacy-conscious deployment of LLMs and pave the way for future research on optimizing the privacy-utility trade-off in fine-tuning methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2504.21036v2),  [pdf](http://arxiv.org/pdf/2504.21036v2)

**Tags**: cs.CR cs.AI cs.LG 



### A Neural Network Mode for PX4 on Embedded Flight Controllers
**Authors**: Sindre M. Hegre, Welf Rehberg, Mihir Kulkarni, Kostas Alexis

**Updated**: 2025-05-01T10:01:43Z

**Summary**: This paper contributes an open-sourced implementation of a neural-network based controller framework within the PX4 stack. We develop a custom module for inference on the microcontroller while retaining all of the functionality of the PX4 autopilot. Policies trained in the Aerial Gym Simulator are converted to the TensorFlow Lite format and then built together with PX4 and flashed to the flight controller. The policies substitute the control-cascade within PX4 to offer an end-to-end position-setpoint tracking controller directly providing normalized motor RPM setpoints. Experiments conducted in simulation and the real-world show similar tracking performance. We thus provide a flight-ready pipeline for testing neural control policies in the real world. The pipeline simplifies the deployment of neural networks on embedded flight controller hardware thereby accelerating research on learning-based control. Both the Aerial Gym Simulator and the PX4 module are open-sourced at https://github.com/ntnu-arl/aerial_gym_simulator and https://github.com/SindreMHegre/PX4-Autopilot-public/tree/for_paper. Video: https://youtu.be/lY1OKz_UOqM?si=VtzL243BAY3lblTJ.

**Link**: [arxiv](http://arxiv.org/abs/2505.00432v1),  [pdf](http://arxiv.org/pdf/2505.00432v1)

**Tags**: cs.RO 



### Over-the-Air Inference over Multi-hop MIMO Networks
**Authors**: Chenghong Bian, Meng Hua, Deniz Gunduz

**Updated**: 2025-05-01T09:59:32Z

**Summary**: A novel over-the-air machine learning framework over multi-hop multiple-input and multiple-output (MIMO) networks is proposed. The core idea is to imitate fully connected (FC) neural network layers using multiple MIMO channels by carefully designing the precoding matrices at the transmitting nodes. A neural network dubbed PrototypeNet is employed consisting of multiple FC layers, with the number of neurons of each layer equal to the number of antennas of the corresponding terminal. To achieve satisfactory performance, we train PrototypeNet based on a customized loss function consisting of classification error and the power of latent vectors to satisfy transmit power constraints, with noise injection during training. Precoding matrices for each hop are then obtained by solving an optimization problem. We also propose a multiple-block extension when the number of antennas is limited. Numerical results verify that the proposed over-the-air transmission scheme can achieve satisfactory classification accuracy under a power constraint. The results also show that higher classification accuracy can be achieved with an increasing number of hops at a modest signal-to-noise ratio (SNR).

**Link**: [arxiv](http://arxiv.org/abs/2505.00430v1),  [pdf](http://arxiv.org/pdf/2505.00430v1)

**Tags**: eess.SP cs.LG 



### Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts   Models
**Authors**: Keisuke Kamahori, Tian Tang, Yile Gu, Kan Zhu, Baris Kasikci

**Updated**: 2025-05-01T09:58:34Z

**Summary**: Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from the significant overhead of frequently moving data between CPU and GPU, or fail to consider distinct characteristics of CPUs and GPUs. This paper proposes Fiddler, a resource-efficient inference system for MoE models with limited GPU resources. Fiddler strategically utilizes CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Fiddler performs better in all scenarios. Compared against different baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference. The code of Fiddler is publicly available at https://github.com/efeslab/fiddler.

**Link**: [arxiv](http://arxiv.org/abs/2402.07033v3),  [pdf](http://arxiv.org/pdf/2402.07033v3)

**Tags**: cs.LG cs.AI cs.DC cs.OS 



### Network-aided Efficient LLM Services With Denoising-inspired Prompt   Compression
**Authors**: Feiran You, Hongyang Du, Kaibin Huang, Abbas Jamalipour

**Updated**: 2025-05-01T09:47:43Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, leading to their increasing adoption in diverse services delivered through wireless networks. There is a growing trend toward longer prompts to better leverage LLMs' capabilities and address difficult tasks. However, longer prompts not only increase data transmission costs but also require more computing resources and processing time, which impacts overall system efficiency and user experience. To address this challenge, we propose Joint Power and Prompt Optimization (JPPO), a framework that combines Small Language Model (SLM)-based prompt compression with wireless power allocation optimization. By deploying SLM at edge devices for prompt compression and employing Deep Reinforcement Learning (DRL) for joint optimization of compression ratio and transmission power, JPPO effectively balances service quality with resource efficiency. Furthermore, inspired by denoising diffusion models, we design a denoising-inspired prompt compression approach that iteratively compresses prompts by gradually removing non-critical information, further enhancing the framework's performance. Experimental results with long prompt tokens demonstrate that our framework achieves high service fidelity while optimizing power usage in wireless LLM services, significantly reducing the total service response time. With our DRL-based JPPO, the framework maintains fidelity comparable to the no-compression baseline while still achieving a 17% service time reduction through adaptive compression.

**Link**: [arxiv](http://arxiv.org/abs/2412.03621v3),  [pdf](http://arxiv.org/pdf/2412.03621v3)

**Tags**: cs.NI 



### $PINN - a Domain Decomposition Method for Bayesian Physics-Informed   Neural Networks
**Authors**: Júlia Vicens Figueres, Juliette Vanderhaeghen, Federica Bragone, Kateryna Morozovska, Khemraj Shukla

**Updated**: 2025-05-01T09:26:03Z

**Summary**: Physics-Informed Neural Networks (PINNs) are a novel computational approach for solving partial differential equations (PDEs) with noisy and sparse initial and boundary data. Although, efficient quantification of epistemic and aleatoric uncertainties in big multi-scale problems remains challenging. We propose \$PINN a novel method of computing global uncertainty in PDEs using a Bayesian framework, by combining local Bayesian Physics-Informed Neural Networks (BPINN) with domain decomposition. The solution continuity across subdomains is obtained by imposing the flux continuity across the interface of neighboring subdomains. To demonstrate the effectiveness of \$PINN, we conduct a series of computational experiments on PDEs in 1D and 2D spatial domains. Although we have adopted conservative PINNs (cPINNs), the method can be seamlessly extended to other domain decomposition techniques. The results infer that the proposed method recovers the global uncertainty by computing the local uncertainty exactly more efficiently as the uncertainty in each subdomain can be computed concurrently. The robustness of \$PINN is verified by adding uncorrelated random noise to the training data up to 15% and testing for different domain sizes.

**Link**: [arxiv](http://arxiv.org/abs/2504.19013v3),  [pdf](http://arxiv.org/pdf/2504.19013v3)

**Tags**: cs.LG cs.AI math.AP 



### GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for   Task-Oriented Grasping
**Authors**: Teli Ma, Zifan Wang, Jiaming Zhou, Mengmeng Wang, Junwei Liang

**Updated**: 2025-05-01T09:13:09Z

**Summary**: Inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. Current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3D radiance modeling, restricting real-time, open-vocabulary interactions with objects. To address these limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary Affordance Reasoning framework, which fine-tunes the Large Language Models (LLMs) to predict the visual affordance of graspable object parts within RGB feature space. We compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. GLOVER inherits world knowledge and common-sense reasoning from LLMs, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. To enable effective real-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. In evaluations across 30 table-top real-world scenes, GLOVER achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 29 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art. We also validate the generalization across embodiments, showing effectiveness in humanoid robots with dexterous hands.

**Link**: [arxiv](http://arxiv.org/abs/2411.12286v2),  [pdf](http://arxiv.org/pdf/2411.12286v2)

**Tags**: cs.RO cs.CV 



### HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in   Large Language Models
**Authors**: Paul Darm, Annalisa Riccardi

**Updated**: 2025-05-01T09:03:35Z

**Summary**: Robust alignment guardrails for large language models are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination for Llama 2. Our method applies fine-grained interventions at specific model subcomponents, particularly attention heads, using a simple binary choice probing strategy. These interventions then generalise to the open-ended generation setting effectively circumventing safety guardrails. We show that probing single attention heads is more effective than intervening on full layers and intervening on only four attention heads is comparable to supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. Our findings highlight the shortcomings of current alignment techniques. In addition, our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviors. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.

**Link**: [arxiv](http://arxiv.org/abs/2502.05945v2),  [pdf](http://arxiv.org/pdf/2502.05945v2)

**Tags**: cs.CL cs.AI I.2.7 



### Fitness Landscape of Large Language Model-Assisted Automated Algorithm   Search
**Authors**: Fei Liu, Qingfu Zhang, Xialiang Tong, Kun Mao, Mingxuan Yuan

**Updated**: 2025-05-01T08:33:32Z

**Summary**: Large Language Models (LLMs) have demonstrated significant potential in algorithm design. However, when integrated into search frameworks for iterative algorithm search, the underlying fitness landscape--critical for understanding search behaviou--remains underexplored. In this paper, we illustrate and analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, where nodes represent algorithms and edges denote transitions between them. We conduct extensive evaluations across six algorithm design tasks and six commonly used LLMs. Our findings reveal that LAS landscapes are highly multimodal and rugged, particularly in combinatorial optimization tasks, with distinct structural variations across tasks and LLMs. For instance, heuristic design tasks exhibit dense clusters of high-performing algorithms, while symbolic regression tasks show sparse, scattered distributions. Additionally, we demonstrate how population size influences exploration-exploitation trade-offs and the evolving trajectory of elite algorithms. These insights not only advance our understanding of LAS landscapes but also provide practical guidance for designing more effective LAS methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.19636v2),  [pdf](http://arxiv.org/pdf/2504.19636v2)

**Tags**: cs.AI cs.NE 



### Dynamic Parametric Retrieval Augmented Generation for Test-time   Knowledge Enhancement
**Authors**: Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu

**Updated**: 2025-05-01T08:03:11Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

**Link**: [arxiv](http://arxiv.org/abs/2503.23895v2),  [pdf](http://arxiv.org/pdf/2503.23895v2)

**Tags**: cs.CL cs.AI 



### Constraints on the state of the IGM at $z\sim 8-10$ using redshifted   21-cm observations with LOFAR
**Authors**: R. Ghara, S. Zaroubi, B. Ciardi, G. Mellema, S. K. Giri, F. G. Mertens, M. Mevius, L. V. E. Koopmans, I. T. Iliev, A. Acharya, S. A. Brackenhoff, E. Ceccotti, K. Chege, I. Georgiev, S. Ghosh, I. Hothi, C. Höfer, Q. Ma, S. Munshi, A. R. Offringa, A. K. Shaw, V. N. Pandey, S. Yatawatta, M. Choudhury

**Updated**: 2025-05-01T07:56:08Z

**Summary**: The power spectra of the redshifted 21-cm signal from the Epoch of Reionization (EoR) contain information about the ionization and thermal states of the intergalactic medium (IGM), and depend on the properties of the EoR sources. Recently, Mertens et al 2025 has analysed 10 nights of LOFAR high-band data and estimated upper limits on the 21-cm power spectrum at redshifts 8.3, 9.1 and 10.1. Here we use these upper limit results to constrain the properties of the IGM at those redshifts. We focus on the properties of the ionized and heated regions where the temperature is larger than that of the CMB. We model the 21-cm power spectrum with the code GRIZZLY, and use a Bayesian inference framework to explore the source parameters for uniform priors on their ranges. The framework also provides information about the IGM properties in the form of derived parameters. In a model which includes a radio background in excess of the CMB, the 95 (68) per cent credible intervals of disfavoured models at redshift 9.1 for the chosen priors correspond to IGM states with averaged ionization and heated fraction below 0.46 ($\lesssim 0.05$), an average gas temperature below 44 K (4 K), and a characteristic size of the heated region $\lesssim 14 ~h^{-1} ~\mathrm{Mpc}$ ($\lesssim 3 ~h^{-1} ~\mathrm{Mpc}$). The 68 per cent credible interval suggests an excess radio background which is more than 100 per cent of the CMB at 1.42 GHz, while the 95 per cent credible interval of the radio background efficiency parameter spans the entire prior range. The behaviour of the credible intervals is similar at all redshifts. The models disfavoured by the LOFAR upper limits are extreme ones, as they are mainly driven by rare and large ionized or heated regions.

**Link**: [arxiv](http://arxiv.org/abs/2505.00373v1),  [pdf](http://arxiv.org/pdf/2505.00373v1)

**Tags**: astro-ph.CO 



### Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic   Approach
**Authors**: Ahmed R. Sadik, Muhammad Ashfaq, Niko Mäkitalo, Tommi Mikkonen

**Updated**: 2025-05-01T07:39:11Z

**Summary**: Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces challenges in system architecture, planning, task management, and execution. Traditional architectural approaches struggle with scalability, adaptability, and seamless resource integration within dynamic and complex environments. This paper presents an intelligent holonic architecture that incorporates Large Language Model (LLM) to manage the complexities of UAM. Holons function semi autonomously, allowing for real time coordination among air taxis, ground transport, and vertiports. LLMs process natural language inputs, generate adaptive plans, and manage disruptions such as weather changes or airspace closures.Through a case study of multimodal transportation with electric scooters and air taxis, we demonstrate how this architecture enables dynamic resource allocation, real time replanning, and autonomous adaptation without centralized control, creating more resilient and efficient urban transportation networks. By advancing decentralized control and AI driven adaptability, this work lays the groundwork for resilient, human centric UAM ecosystems, with future efforts targeting hybrid AI integration and real world validation.

**Link**: [arxiv](http://arxiv.org/abs/2505.00368v1),  [pdf](http://arxiv.org/pdf/2505.00368v1)

**Tags**: cs.AI cs.ET cs.MA cs.RO 



### KoACD: The First Korean Adolescent Dataset for Cognitive Distortion   Analysis
**Authors**: JunSeo Kim, HyeHyeon Kim

**Updated**: 2025-05-01T07:37:18Z

**Summary**: Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.

**Link**: [arxiv](http://arxiv.org/abs/2505.00367v1),  [pdf](http://arxiv.org/pdf/2505.00367v1)

**Tags**: cs.CL cs.AI 



### Domain-Specific Translation with Open-Source Large Language Models:   Resource-Oriented Analysis
**Authors**: Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem

**Updated**: 2025-05-01T07:36:13Z

**Summary**: In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language directions with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant quality gap in specialized translation compared to multilingual encoder-decoder MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms all evaluated LLMs in the 7-8B parameter range across three out of the four language directions. While fine-tuning improves the performance of LLMs such as Mistral and Llama, these models still underperform compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve high-quality domain-specific translation, especially in medium-resource and low-resource settings. Moreover, the superior performance of larger LLMs over their 8B variants suggests potential value in pre-training domain-specific medium-sized language models, employing targeted data selection and knowledge distillation approaches to enhance both quality and efficiency in specialized translation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.05862v3),  [pdf](http://arxiv.org/pdf/2412.05862v3)

**Tags**: cs.CL 



### Reproducing NevIR: Negation in Neural Information Retrieval
**Authors**: Coen van den Elsen, Francien Barkhof, Thijmen Nijdam, Simon Lupart, Mohammad Aliannejadi

**Updated**: 2025-05-01T07:27:34Z

**Summary**: Negation is a fundamental aspect of human communication, yet it remains a challenge for Language Models (LMs) in Information Retrieval (IR). Despite the heavy reliance of modern neural IR systems on LMs, little attention has been given to their handling of negation. In this study, we reproduce and extend the findings of NevIR, a benchmark study that revealed most IR models perform at or below the level of random ranking when dealing with negation. We replicate NevIR's original experiments and evaluate newly developed state-of-the-art IR models. Our findings show that a recently emerging category-listwise Large Language Model (LLM) re-rankers-outperforms other models but still underperforms human performance. Additionally, we leverage ExcluIR, a benchmark dataset designed for exclusionary queries with extensive negation, to assess the generalisability of negation understanding. Our findings suggest that fine-tuning on one dataset does not reliably improve performance on the other, indicating notable differences in their data distributions. Furthermore, we observe that only cross-encoders and listwise LLM re-rankers achieve reasonable performance across both negation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.13506v3),  [pdf](http://arxiv.org/pdf/2502.13506v3)

**Tags**: cs.IR 



### Matrix Healy Plot: A Practical Tool for Visual Assessment of   Matrix-Variate Normality
**Authors**: Fen Jiang, Jianhua Zhao, Changchun Shang, Xuan Ma, Yue Wang, Ye Tao

**Updated**: 2025-05-01T07:20:25Z

**Summary**: Matrix-valued data, where each observation is represented as a matrix, frequently arises in various scientific disciplines. Modeling such data often relies on matrix-variate normal distributions, making matrix-variate normality testing crucial for valid statistical inference. Recently, the Distance-Distance (DD) plot has been introduced as a graphical tool for visually assessing matrix-variate normality. However, the Mahalanobis squared distances (MSD) used in the DD plot require vectorizing matrix observations, restricting its applicability to cases where the dimension of the vectorized data does not exceed the sample size. To address this limitation, we propose a novel graphical method called the Matrix Healy (MHealy) plot, an extension of the Healy plot for vector-valued data. This new plot is based on more accurate matrix-based MSD that leverages the inherent structure of matrix data. Consequently, it offers a more reliable visual assessment. Importantly, the MHealy plot eliminates the sample size restriction of the DD plot and hence more applicable to matrix-valued data. Empirical results demonstrate its effectiveness and practicality compared to the DD plot across various scenarios, particularly in cases where the DD plot is not available due to limited sample sizes.

**Link**: [arxiv](http://arxiv.org/abs/2505.00361v1),  [pdf](http://arxiv.org/pdf/2505.00361v1)

**Tags**: stat.ME 



### LLMPrism: Black-box Performance Diagnosis for Production LLM Training   Platforms
**Authors**: Zhihan Jiang, Rui Ren, Guangba Yu, Yulun Wu, Wenwei Gu, Yichen Li, Yujie Huang, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael R. Lyu

**Updated**: 2025-05-01T06:38:52Z

**Summary**: Large Language Models (LLMs) have brought about revolutionary changes in diverse fields, rendering LLM training of utmost importance for modern enterprises. To meet this demand, multi-tenant large-scale LLM training platforms have been built to offer LLM training services. Nevertheless, due to the complexity and synchronous nature of LLM training process, performance issues occur frequently and can result in substantial resource wastage. The limited visibility from the perspective of platform providers impedes existing profiling methods and poses challenges to the monitoring and diagnosis of the performance of LLM training jobs. For the first time, this paper proposes the utilization of underlying network flow data to reconstruct the training timelines of jobs based on the distinct characteristics in the LLM training procedure. We design LLMPrism, the first black-box performance diagnosis system for LLM training platforms. By progressively recognizing LLM training jobs, identifying their parallelism strategies, and reconstructing the training timelines, LLMPrism achieves non-intrusive, lightweight, and continuous monitoring of LLM training systems. Leveraging this monitoring capability, it further effectively diagnoses potential performance issues. Since Oct. 2024, LLMPrism has been deployed on our large-scale production Platform-X, in which the evaluations and deployment experiences demonstrate that LLMPrism can achieve accurate timeline reconstruction with an error within 0.3% and effectively diagnose various performance issues.

**Link**: [arxiv](http://arxiv.org/abs/2505.00342v1),  [pdf](http://arxiv.org/pdf/2505.00342v1)

**Tags**: cs.SE 



### Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale   AI Models
**Authors**: Bumjun Kim, Wan Choi

**Updated**: 2025-05-01T06:15:38Z

**Summary**: Transformer-based large language models (LLMs) have achieved remarkable success across various tasks. Yet, fine-tuning such massive models in federated learning (FL) settings poses significant challenges due to resource constraints and communication overhead. Low-Rank Adaptation (LoRA) addresses these issues by training compact, low-rank matrices instead of fully fine-tuning large models. This paper introduces a wireless federated LoRA fine-tuning framework that optimizes both learning performance and communication efficiency. We provide a novel convergence analysis, revealing how LoRA rank and covariance effects influence FL training dynamics. Leveraging these insights, we propose Sparsified Orthogonal Fine-Tuning (\textbf{SOFT}), an adaptive sparsification method that streamlines parameter updates without expensive matrix multiplications and singular value decomposition (SVD) operations. Additionally, we present a Two Stage Federated Algorithm (\textbf{TSFA}) algorithm that pre-determines key parameters offline and dynamically adjusts bandwidth and sparsification online, ensuring efficient training under latency constraints. Experiments on benchmark datasets show that our approach achieves accuracy comparable to ideal scenario models while significantly reducing communication overhead. Our framework thus enables scalable, resource-efficient deployment of large models in real-world wireless FL scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.00333v1),  [pdf](http://arxiv.org/pdf/2505.00333v1)

**Tags**: cs.LG eess.SP 



### MotionGlot: A Multi-Embodied Motion Generation Model
**Authors**: Sudarshan Harithas, Srinath Sridhar

**Updated**: 2025-05-01T06:13:42Z

**Summary**: This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motionrelated tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert-controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2410.16623v2),  [pdf](http://arxiv.org/pdf/2410.16623v2)

**Tags**: cs.RO 



### Geodesic Synthetic Control Methods for Random Objects and Functional   Data
**Authors**: Daisuke Kurisu, Yidong Zhou, Taisuke Otsu, Hans-Georg Müller

**Updated**: 2025-05-01T06:12:35Z

**Summary**: We introduce a geodesic synthetic control method for causal inference that extends existing synthetic control methods to scenarios where outcomes are elements in a geodesic metric space rather than scalars. Examples of such outcomes include distributions, compositions, networks, trees and functional data, among other data types that can be viewed as elements of a geodesic metric space given a suitable metric. We extend this further to geodesic synthetic difference-in-differences that builds on the established synthetic difference-in-differences for Euclidean outcomes. This estimator generalizes both the geodesic synthetic control method and a previously proposed geodesic difference-in-differences method and exhibits a double robustness property. The proposed geodesic synthetic control method is illustrated through comprehensive simulation studies and applications to the employment composition changes following the 2011 Great East Japan Earthquake, and the impact of abortion liberalization policy on fertility patterns in East Germany. We illustrate the proposed geodesic synthetic difference-in-differences by studying the consequences of the Soviet Union's collapse on age-at-death distributions for males and females.

**Link**: [arxiv](http://arxiv.org/abs/2505.00331v1),  [pdf](http://arxiv.org/pdf/2505.00331v1)

**Tags**: stat.ME 62D20, 62R20 



### A Framework for Testing and Adapting REST APIs as LLM Tools
**Authors**: Jayachandu Bandlamudi, Ritwik Chaudhuri, Neelamadhav Gantayat, Kushal Mukherjee, Prerna Agarwal, Renuka Sindhgatta, Sameep Mehta

**Updated**: 2025-05-01T05:50:45Z

**Summary**: Large Language Models (LLMs) are enabling autonomous agents to perform complex workflows using external tools or functions, often provided via REST APIs in enterprise systems. However, directly utilizing these APIs as tools poses challenges due to their complex input schemas, elaborate responses, and often ambiguous documentation. Current benchmarks for tool testing do not adequately address these complexities, leading to a critical gap in evaluating API readiness for agent-driven automation. In this work, we present a novel testing framework aimed at evaluating and enhancing the readiness of REST APIs to function as tools for LLM-based agents. Our framework transforms apis as tools, generates comprehensive test cases for the APIs, translates tests cases into natural language instructions suitable for agents, enriches tool definitions and evaluates the agent's ability t correctly invoke the API and process its inputs and responses. To provide actionable insights, we analyze the outcomes of 750 test cases, presenting a detailed taxonomy of errors, including input misinterpretation, output handling inconsistencies, and schema mismatches. Additionally, we classify these test cases to streamline debugging and refinement of tool integrations. This work offers a foundational step toward enabling enterprise APIs as tools, improving their usability in agent-based applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.15546v2),  [pdf](http://arxiv.org/pdf/2504.15546v2)

**Tags**: cs.SE cs.AI I.2.7 



### Recursive Algorithms for Sparse Parameter Identification of Multivariate   Stochastic Systems with Non-stationary Observations
**Authors**: Yanxin Fu, Wenxiao Zhao

**Updated**: 2025-05-01T05:47:17Z

**Summary**: The classical sparse parameter identification methods are usually based on the iterative basis selection such as greedy algorithms, or the numerical optimization of regularized cost functions such as LASSO and Bayesian posterior probability distribution, etc., which, however, are not suitable for online sparsity inference when data arrive sequentially. This paper presents recursive algorithms for sparse parameter identification of multivariate stochastic systems with non-stationary observations. First, a new bivariate criterion function is presented by introducing an auxiliary variable matrix into a weighted $L_1$ regularization criterion. The new criterion function is subsequently decomposed into two solvable subproblems via alternating optimization of the two variable matrices, for which the optimizers can be explicitly formulated into recursive equations. Second, under the non-stationary and non-persistent excitation conditions on the systems, theoretical properties of the recursive algorithms are established. That is, the estimates are proved to be with (i) set convergence, i.e., the accurate estimation of the sparse index set of the unknown parameter matrix, and (ii) parameter convergence, i.e., the consistent estimation for values of the non-zero elements of the unknown parameter matrix. Finally, numerical examples are given to support the theoretical analysis.

**Link**: [arxiv](http://arxiv.org/abs/2505.00323v1),  [pdf](http://arxiv.org/pdf/2505.00323v1)

**Tags**: eess.SY cs.SY 



### AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis   with Vehicle Dynamics
**Authors**: Keshu Wu, Zihao Li, Sixu Li, Xinyue Ye, Dominique Lord, Yang Zhou

**Updated**: 2025-05-01T05:46:34Z

**Summary**: This paper introduces an AI-enabled, interaction-aware active safety analysis framework that accounts for groupwise vehicle interactions. Specifically, the framework employs a bicycle model-augmented with road gradient considerations-to accurately capture vehicle dynamics. In parallel, a hypergraph-based AI model is developed to predict probabilistic trajectories of ambient traffic. By integrating these two components, the framework derives vehicle intra-spacing over a 3D road surface as the solution of a stochastic ordinary differential equation, yielding high-fidelity surrogate safety measures such as time-to-collision (TTC). To demonstrate its effectiveness, the framework is analyzed using stochastic numerical methods comprising 4th-order Runge-Kutta integration and AI inference, generating probability-weighted high-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent maneuvers and behavioral uncertainties. Evaluated with HF-TTC against traditional constant-velocity TTC and non-interaction-aware approaches on highway datasets, the proposed framework offers a systematic methodology for active safety analysis with enhanced potential for improving safety perception in complex traffic environments.

**Link**: [arxiv](http://arxiv.org/abs/2505.00322v1),  [pdf](http://arxiv.org/pdf/2505.00322v1)

**Tags**: cs.RO cs.AI 



### Edge Large AI Models: Revolutionizing 6G Networks
**Authors**: Zixin Wang, Yuanming Shi, Yong Zhou, Jingyang Zhu, Khaled. B. Letaief

**Updated**: 2025-05-01T05:44:00Z

**Summary**: Large artificial intelligence models (LAMs) possess human-like abilities to solve a wide range of real-world problems, exemplifying the potential of experts in various domains and modalities. By leveraging the communication and computation capabilities of geographically dispersed edge devices, edge LAM emerges as an enabling technology to empower the delivery of various real-time intelligent services in 6G. Unlike traditional edge artificial intelligence (AI) that primarily supports a single task using small models, edge LAM is featured by the need of the decomposition and distributed deployment of large models, and the ability to support highly generalized and diverse tasks. However, due to limited communication, computation, and storage resources over wireless networks, the vast number of trainable neurons and the substantial communication overhead pose a formidable hurdle to the practical deployment of edge LAMs. In this paper, we investigate the opportunities and challenges of edge LAMs from the perspectives of model decomposition and resource management. Specifically, we propose collaborative fine-tuning and full-parameter training frameworks, alongside a microservice-assisted inference architecture, to enhance the deployment of edge LAM over wireless networks. Additionally, we investigate the application of edge LAM in air-interface designs, focusing on channel prediction and beamforming. These innovative frameworks and applications offer valuable insights and solutions for advancing 6G technology.

**Link**: [arxiv](http://arxiv.org/abs/2505.00321v1),  [pdf](http://arxiv.org/pdf/2505.00321v1)

**Tags**: cs.NI cs.LG eess.SP 



### SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference
**Authors**: Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, Jianfei Chen

**Updated**: 2025-05-01T05:38:46Z

**Summary**: An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.

**Link**: [arxiv](http://arxiv.org/abs/2502.18137v2),  [pdf](http://arxiv.org/pdf/2502.18137v2)

**Tags**: cs.LG cs.AI cs.CV cs.PF 



### FedEMA: Federated Exponential Moving Averaging with Negative Entropy   Regularizer in Autonomous Driving
**Authors**: Wei-Bin Kou, Guangxu Zhu, Bingyang Cheng, Shuai Wang, Ming Tang, Yik-Chung Wu

**Updated**: 2025-05-01T05:37:43Z

**Summary**: Street Scene Semantic Understanding (denoted as S3U) is a crucial but complex task for autonomous driving (AD) vehicles. Their inference models typically face poor generalization due to domain-shift. Federated Learning (FL) has emerged as a promising paradigm for enhancing the generalization of AD models through privacy-preserving distributed learning. However, these FL AD models face significant temporal catastrophic forgetting when deployed in dynamically evolving environments, where continuous adaptation causes abrupt erosion of historical knowledge. This paper proposes Federated Exponential Moving Average (FedEMA), a novel framework that addresses this challenge through two integral innovations: (I) Server-side model's historical fitting capability preservation via fusing current FL round's aggregation model and a proposed previous FL round's exponential moving average (EMA) model; (II) Vehicle-side negative entropy regularization to prevent FL models' possible overfitting to EMA-introduced temporal patterns. Above two strategies empower FedEMA a dual-objective optimization that balances model generalization and adaptability. In addition, we conduct theoretical convergence analysis for the proposed FedEMA. Extensive experiments both on Cityscapes dataset and Camvid dataset demonstrate FedEMA's superiority over existing approaches, showing 7.12% higher mean Intersection-over-Union (mIoU).

**Link**: [arxiv](http://arxiv.org/abs/2505.00318v1),  [pdf](http://arxiv.org/pdf/2505.00318v1)

**Tags**: cs.RO cs.LG 



### TerEffic: Highly Efficient Ternary LLM Inference on FPGA
**Authors**: Chenyang Yin, Zhenyu Bai, Pranav Venkatram, Shivam Aggarwal, Zhaoying Li, Tulika Mitra

**Updated**: 2025-05-01T05:05:03Z

**Summary**: Deploying Large Language Models (LLMs) efficiently on edge devices is often constrained by limited memory capacity and high power consumption. Low-bit quantization methods, particularly ternary quantization, have demonstrated significant potential in preserving model accuracy while substantially decreasing memory footprint and computational costs. However, existing general-purpose architectures and accelerators have not fully exploited the advantages of low-bit quantization due to insufficient specialized hardware support. We introduce TerEffic, an FPGA-based architecture tailored for ternary-quantized LLM inference. The proposed system offers flexibility through reconfigurable hardware to meet various system requirements. We evaluated two representative configurations: a fully on-chip design that stores all weights within on-chip memories, scaling out using multiple FPGAs, and an HBM-assisted design capable of accommodating larger models on a single FPGA board. Experimental results demonstrate significant performance and energy efficiency improvements. For single-batch inference on a 370 M-parameter model, our fully on-chip architecture achieves 16,300 tokens/second, delivering a throughput 192 times higher than NVIDIA Jetson Orin Nano with a power efficiency of 455 tokens/second/W, marking a 19-fold improvement. The HBM-assisted architecture processes 727 tokens/second for a larger 2.7B-parameter model, which is 3 times of the throughput of NVIDIA A100, while consuming only 46W, resulting in a power efficiency of 16 tokens/second/W, an 8-fold improvement over the A100.

**Link**: [arxiv](http://arxiv.org/abs/2502.16473v2),  [pdf](http://arxiv.org/pdf/2502.16473v2)

**Tags**: cs.AR 



### BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language   Models in Chinese
**Authors**: Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, Yining Hua

**Updated**: 2025-05-01T05:02:57Z

**Summary**: As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.

**Link**: [arxiv](http://arxiv.org/abs/2504.19314v2),  [pdf](http://arxiv.org/pdf/2504.19314v2)

**Tags**: cs.CL 



### Gateformer: Advancing Multivariate Time Series Forecasting through   Temporal and Variate-Wise Attention with Gated Representations
**Authors**: Yu-Hsiang Lan, Anton Alyakin, Eric K. Oermann

**Updated**: 2025-05-01T04:59:05Z

**Summary**: There has been a recent surge of interest in time series modeling using the Transformer architecture. However, forecasting multivariate time series with Transformer presents a unique challenge as it requires modeling both temporal (cross-time) and variate (cross-variate) dependencies. While Transformer-based models have gained popularity for their flexibility in capturing both sequential and cross-variate relationships, it is unclear how to best integrate these two sources of information in the context of the Transformer architecture while optimizing for both performance and efficiency. We re-purpose the Transformer architecture to effectively model both cross-time and cross-variate dependencies. Our approach begins by embedding each variate independently into a variate-wise representation that captures its cross-time dynamics, and then models cross-variate dependencies through attention mechanisms on these learned embeddings. Gating operations in both cross-time and cross-variate modeling phases regulate information flow, allowing the model to focus on the most relevant features for accurate predictions. Our method achieves state-of-the-art performance across 13 real-world datasets and can be seamlessly integrated into other Transformer-based and LLM-based forecasters, delivering performance improvements up to 20.7\% over original models. Code is available at this repository: https://github.com/nyuolab/Gateformer.

**Link**: [arxiv](http://arxiv.org/abs/2505.00307v1),  [pdf](http://arxiv.org/pdf/2505.00307v1)

**Tags**: cs.LG 



### Evaluation and Verification of Physics-Informed Neural Models of the   Grad-Shafranov Equation
**Authors**: Fauzan Nazranda Rizqan, Matthew Hole, Charles Gretton

**Updated**: 2025-05-01T04:26:16Z

**Summary**: Our contributions are motivated by fusion reactors that rely on maintaining magnetohydrodynamic (MHD) equilibrium, where the balance between plasma pressure and confining magnetic fields is required for stable operation. In axisymmetric tokamak reactors in particular, and under the assumption of toroidal symmetry, this equilibrium can be mathematically modelled using the Grad-Shafranov Equation (GSE). Recent works have demonstrated the potential of using Physics-Informed Neural Networks (PINNs) to model the GSE. Existing studies did not examine realistic scenarios in which a single network generalizes to a variety of boundary conditions. Addressing that limitation, we evaluate a PINN architecture that incorporates boundary points as network inputs. Additionally, we compare PINN model accuracy and inference speeds with a Fourier Neural Operator (FNO) model. Finding the PINN model to be the most performant, and accurate in our setting, we use the network verification tool Marabou to perform a range of verification tasks. Although we find some discrepancies between evaluations of the networks natively in PyTorch, compared to via Marabou, we are able to demonstrate useful and practical verification workflows. Our study is the first investigation of verification of such networks.

**Link**: [arxiv](http://arxiv.org/abs/2504.21155v2),  [pdf](http://arxiv.org/pdf/2504.21155v2)

**Tags**: physics.plasm-ph cs.AI cs.NE 



### LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous   Driving
**Authors**: Zhijie Qiao, Haowei Li, Zhong Cao, Henry X. Liu

**Updated**: 2025-05-01T04:12:41Z

**Summary**: Vision-Language Models (VLMs) have demonstrated significant potential for end-to-end autonomous driving. However, fully exploiting their capabilities for safe and reliable vehicle control remains an open research challenge. To systematically examine advances and limitations of VLMs in driving tasks, we introduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous driving. LightEMMA provides a unified, VLM-based autonomous driving framework without ad hoc customizations, enabling easy integration and evaluation of evolving state-of-the-art commercial and open-source models. We construct twelve autonomous driving agents using various VLMs and evaluate their performance on the nuScenes prediction task, comprehensively assessing metrics such as inference time, computational cost, and predictive accuracy. Illustrative examples highlight that, despite their strong scenario interpretation capabilities, VLMs' practical performance in autonomous driving tasks remains concerning, emphasizing the need for further improvements. The code is available at https://github.com/michigan-traffic-lab/LightEMMA.

**Link**: [arxiv](http://arxiv.org/abs/2505.00284v1),  [pdf](http://arxiv.org/pdf/2505.00284v1)

**Tags**: cs.RO cs.AI 



### A Unifying Framework for Robust and Efficient Inference with   Unstructured Data
**Authors**: Jacob Carlson, Melissa Dell

**Updated**: 2025-05-01T04:11:25Z

**Summary**: This paper presents a general framework for conducting efficient and robust inference on parameters derived from unstructured data, which include text, images, audio, and video. Economists have long incorporated data extracted from texts and images into their analyses, a practice that has accelerated with advancements in deep neural networks. However, neural networks do not generically produce unbiased predictions, potentially propagating bias to estimators that use their outputs. To address this challenge, we reframe inference with unstructured data as a missing structured data problem, where structured data are imputed from unstructured inputs using deep neural networks. This perspective allows us to apply classic results from semiparametric inference, yielding valid, efficient, and robust estimators based on unstructured data. We formalize this approach with MARS (Missing At Random Structured Data), a unifying framework that integrates and extends existing methods for debiased inference using machine learning predictions, linking them to a variety of older, familiar problems such as causal inference. We develop robust and efficient estimators for both descriptive and causal estimands and address challenges such as inference using aggregated and transformed predictions from unstructured data. Importantly, MARS applies to common empirical settings that have received limited attention in the existing literature. Finally, we reanalyze prominent studies that use unstructured data, demonstrating the practical value of MARS.

**Link**: [arxiv](http://arxiv.org/abs/2505.00282v1),  [pdf](http://arxiv.org/pdf/2505.00282v1)

**Tags**: econ.EM cs.LG 



### Abundance Measurements of the Metal-poor M subdwarf LHS 174 Using   High-resolution Optical Spectroscopy
**Authors**: Neda Hejazi, Sebastien Lepine, Thomas Nordlander, Wei-Chun Jao, David R. Coria, Kathryn V. Lester

**Updated**: 2025-05-01T04:01:35Z

**Summary**: Metal-poor M subdwarfs are among the oldest stellar populations and carry valuable information about the chemical enrichment history of the Milky Way. The measurements of chemical abundances of these stars therefore provide essential insights into the nucleosynthesis in the early stages of the Galaxy's formation. We present the detailed spectroscopic analysis of a nearby metal-poor M subdwarf, LHS 174 from its high-resolution optical spectrum, and apply our previously developed spectral fitting code, \texttt{AutoSpecFit}, to measure the abundances of five elements:[O/H]=$-$0.519$\pm$0.081, [Ca/H]=$-$0.753$\pm$0.177, [Ti/H]=$-$0.711$\pm$0.144, [V/H]=$-$1.026$\pm$0.077, and [Fe/H]=$-$1.170$\pm$0.135. We compare the abundances of O, Ti, and Fe derived from this work and those from previous studies and demonstrate the observed data is clearly better matched with the synthetic model generated based on our abundances than those from the other analyses. The accuracy of inferred stellar abundances strongly depends on the accuracy of physical parameters, which motivates us to develop a reliable technique to determine the parameters of low-mass M dwarfs more accurately ever than before and infer abundances with smaller uncertainties.

**Link**: [arxiv](http://arxiv.org/abs/2504.20255v2),  [pdf](http://arxiv.org/pdf/2504.20255v2)

**Tags**: astro-ph.SR astro-ph.GA 



### Integrating Dynamical Systems Modeling with Spatiotemporal scRNA-seq   Data Analysis
**Authors**: Zhenyi Zhang, Yuhao Sun, Qiangwei Peng, Tiejun Li, Peijie Zhou

**Updated**: 2025-05-01T03:59:41Z

**Summary**: Understanding the dynamic nature of biological systems is fundamental to deciphering cellular behavior, developmental processes, and disease progression. Single-cell RNA sequencing (scRNA-seq) has provided static snapshots of gene expression, offering valuable insights into cellular states at a single time point. Recent advancements in temporally resolved scRNA-seq, spatial transcriptomics (ST), and time-series spatial transcriptomics (temporal-ST) have further revolutionized our ability to study the spatiotemporal dynamics of individual cells. These technologies, when combined with computational frameworks such as Markov chains, stochastic differential equations (SDEs), and generative models like optimal transport and Schr\"odinger bridges, enable the reconstruction of dynamic cellular trajectories and cell fate decisions. This review discusses how these dynamical system approaches offer new opportunities to model and infer cellular dynamics from a systematic perspective.

**Link**: [arxiv](http://arxiv.org/abs/2503.11347v2),  [pdf](http://arxiv.org/pdf/2503.11347v2)

**Tags**: q-bio.QM cs.LG physics.bio-ph 



### Topological State Space Inference for Dynamical Systems
**Authors**: Mishal Assif P K, Yuliy Baryshnikov

**Updated**: 2025-05-01T03:53:29Z

**Summary**: We present a computational pipe aiming at recovery of the topology of the underlying phase space from observation of an output function along a sample of trajectories of a dynamical system.

**Link**: [arxiv](http://arxiv.org/abs/2505.00276v1),  [pdf](http://arxiv.org/pdf/2505.00276v1)

**Tags**: math.DS cs.SY eess.SY math.AT 



### AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor   Long-Term Medication Adherence and Care
**Authors**: Md Asaduzzaman Jabin, Hanqi Jiang, Yiwei Li, Patrick Kaggwa, Eugene Douglass, Juliet N. Sekandi, Tianming Liu

**Updated**: 2025-05-01T03:48:12Z

**Summary**: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based multimodal large vision language model (LVLM) aimed at visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2505.00275v1),  [pdf](http://arxiv.org/pdf/2505.00275v1)

**Tags**: cs.CV 



### Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets   Collective Intelligence
**Authors**: Haiyan Qin, Jiahao Feng, Xiaotong Feng, Wei W. Xing, Wang Kang

**Updated**: 2025-05-01T03:43:28Z

**Summary**: Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\%--1075\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.

**Link**: [arxiv](http://arxiv.org/abs/2504.14625v3),  [pdf](http://arxiv.org/pdf/2504.14625v3)

**Tags**: cs.AR cs.AI 



### Large Language Models as AI Agents for Digital Atoms and Molecules:   Catalyzing a New Era in Computational Biophysics
**Authors**: Yijie Xia, Xiaohan Lin, Zicheng Ma, Jinyuan Hu, Yanheng Li, Zhaoxin Xie, Hao Li, Li Yang, Zhiqiang Zhao, Lijiang Yang, Zhenyu Chen, Yi Qin Gao

**Updated**: 2025-05-01T03:33:57Z

**Summary**: In computational biophysics, where molecular data is expanding rapidly and system complexity is increasing exponentially, large language models (LLMs) and agent-based systems are fundamentally reshaping the field. This perspective article examines the recent advances at the intersection of LLMs, intelligent agents, and scientific computation, with a focus on biophysical computation. Building on these advancements, we introduce ADAM (Agent for Digital Atoms and Molecules), an innovative multi-agent LLM-based framework. ADAM employs cutting-edge AI architectures to reshape scientific workflows through a modular design. It adopts a hybrid neural-symbolic architecture that combines LLM-driven semantic tools with deterministic symbolic computations. Moreover, its ADAM Tool Protocol (ATP) enables asynchronous, database-centric tool orchestration, fostering community-driven extensibility. Despite the significant progress made, ongoing challenges call for further efforts in establishing benchmarking standards, optimizing foundational models and agents, and building an open collaborative ecosystem. ADAM is accessible at https://sidereus-ai.com.

**Link**: [arxiv](http://arxiv.org/abs/2505.00270v1),  [pdf](http://arxiv.org/pdf/2505.00270v1)

**Tags**: physics.comp-ph physics.bio-ph 



### A Comprehensive Survey on Integrating Large Language Models with   Knowledge-Based Methods
**Authors**: Wenli Yang, Lilian Some, Michael Bain, Byeong Kang

**Updated**: 2025-05-01T03:29:50Z

**Summary**: The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.

**Link**: [arxiv](http://arxiv.org/abs/2501.13947v3),  [pdf](http://arxiv.org/pdf/2501.13947v3)

**Tags**: cs.CL cs.AI 



### EnronQA: Towards Personalized RAG over Private Documents
**Authors**: Michael J. Ryan, Danmei Xu, Chris Nivera, Daniel Campos

**Updated**: 2025-05-01T03:07:30Z

**Summary**: Retrieval Augmented Generation (RAG) has become one of the most popular methods for bringing knowledge-intensive context to large language models (LLM) because of its ability to bring local context at inference time without the cost or data leakage risks associated with fine-tuning. A clear separation of private information from the LLM training has made RAG the basis for many enterprise LLM workloads as it allows the company to augment LLM's understanding using customers' private documents. Despite its popularity for private documents in enterprise deployments, current RAG benchmarks for validating and optimizing RAG pipelines draw their corpora from public data such as Wikipedia or generic web pages and offer little to no personal context. Seeking to empower more personal and private RAG we release the EnronQA benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs across 150 different user inboxes. EnronQA enables better benchmarking of RAG pipelines over private data and allows for experimentation on the introduction of personalized retrieval settings over realistic data. Finally, we use EnronQA to explore the tradeoff in memorization and retrieval when reasoning over private documents.

**Link**: [arxiv](http://arxiv.org/abs/2505.00263v1),  [pdf](http://arxiv.org/pdf/2505.00263v1)

**Tags**: cs.IR cs.CL 



### ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model
**Authors**: Haiyan Qin, Zhiwei Xie, Jingjing Li, Liangchen Li, Xiaotong Feng, Junzhan Liu, Wang Kang

**Updated**: 2025-05-01T03:06:06Z

**Summary**: Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75\% while preserving performance. Experimental results demonstrate ReasoningV's effectiveness with a pass@1 accuracy of 57.8\% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5\%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at https://github.com/BUAA-CLab/ReasoningV.

**Link**: [arxiv](http://arxiv.org/abs/2504.14560v3),  [pdf](http://arxiv.org/pdf/2504.14560v3)

**Tags**: cs.AR cs.AI 



### Policy Learning with $α$-Expected Welfare
**Authors**: Yanqin Fan, Yuan Qi, Gaoqian Xu

**Updated**: 2025-05-01T02:42:13Z

**Summary**: This paper proposes an optimal policy that targets the average welfare of the worst-off $\alpha$-fraction of the post-treatment outcome distribution. We refer to this policy as the $\alpha$-Expected Welfare Maximization ($\alpha$-EWM) rule, where $\alpha \in (0,1]$ denotes the size of the subpopulation of interest. The $\alpha$-EWM rule interpolates between the expected welfare ($\alpha=1$) and the Rawlsian welfare ($\alpha\rightarrow 0$). For $\alpha\in (0,1)$, an $\alpha$-EWM rule can be interpreted as a distributionally robust EWM rule that allows the target population to have a different distribution than the study population. Using the dual formulation of our $\alpha$-expected welfare function, we propose a debiased estimator for the optimal policy and establish its asymptotic upper regret bounds. In addition, we develop asymptotically valid inference for the optimal welfare based on the proposed debiased estimator. We examine the finite sample performance of the debiased estimator and inference via both real and synthetic data.

**Link**: [arxiv](http://arxiv.org/abs/2505.00256v1),  [pdf](http://arxiv.org/pdf/2505.00256v1)

**Tags**: econ.EM 



### Meta-rater: A Multi-dimensional Data Selection Method for Pre-training   Language Models
**Authors**: Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He

**Updated**: 2025-05-01T02:37:14Z

**Summary**: The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose PRRC to evaluate data quality across Professionalism, Readability, Reasoning, and Cleanliness. We further introduce Meta-rater, a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with scalable benefits observed in 3.3B models trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B dataset, labeled across 25 quality metrics (including PRRC), to advance research in data-centric LLM development. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability.

**Link**: [arxiv](http://arxiv.org/abs/2504.14194v2),  [pdf](http://arxiv.org/pdf/2504.14194v2)

**Tags**: cs.CL 



### From Effectiveness to Efficiency: Uncovering Linguistic Bias in Large   Language Model-based Code Generation
**Authors**: Weipeng Jiang, Xuanqi Gao, Juan Zhai, Shiqing Ma, Xiaoyu Zhang, Ziyan Lei, Chao Shen

**Updated**: 2025-05-01T02:34:03Z

**Summary**: Large Language Models (LLMs) have demonstrated promising capabilities for code generation. While existing benchmarks evaluate the correctness and efficiency of LLM-generated code, the potential linguistic bias - where code quality varies based on the natural language used to describe programming tasks - remains underexplored. In this paper, we aim to investigate this linguistic bias through the lens of English and Chinese. To facilitate our investigation, we present a unified evaluation framework comprising a curated dataset of 52 Python programming questions with parallel bilingual task descriptions, automated correctness verification, and efficiency quantification tools based on runtime complexity estimation. Based on this framework, we conduct the first empirical study towards the linguistic bias in LLM-generated code on eight popular LCGMs, as well as GPT-3.5-Turbo and GPT-4. We observe that these LCGM-generated code show different correctness on an average of 12% bilingual programming tasks, where 39% also exhibits diverse efficiency. Our findings indicate that LLMs commonly exhibit linguistic bias for code generation.

**Link**: [arxiv](http://arxiv.org/abs/2406.00602v2),  [pdf](http://arxiv.org/pdf/2406.00602v2)

**Tags**: cs.SE cs.PL 



## Keyword: LLM Deployment 
 ### Robotic Visual Instruction
**Authors**: Yanbang Li, Ziyang Gong, Haoyang Li, Haoyang Li, Xiaoqi Huang, Haolan Kang, Guangping Bai, Xianzheng Ma

**Updated**: 2025-05-01T17:55:05Z

**Summary**: Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision for robotic control introduces challenges such as ambiguity and verbosity. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment, enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Code and Datasets in this paper will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2505.00693v1),  [pdf](http://arxiv.org/pdf/2505.00693v1)

**Tags**: cs.RO cs.AI cs.CV 



### MINERVA: Evaluating Complex Video Reasoning
**Authors**: Arsha Nagrani, Sachit Menon, Ahmet Iscen, Shyamal Buch, Ramin Mehran, Nilpa Jha, Anja Hauth, Yukun Zhu, Carl Vondrick, Mikhail Sirotenko, Cordelia Schmid, Tobias Weyand

**Updated**: 2025-05-01T17:41:49Z

**Summary**: Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called MINERVA for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva.

**Link**: [arxiv](http://arxiv.org/abs/2505.00681v1),  [pdf](http://arxiv.org/pdf/2505.00681v1)

**Tags**: cs.LG cs.CV 



### Steering Large Language Models with Register Analysis for Arbitrary   Style Transfer
**Authors**: Xinchen Yang, Marine Carpuat

**Updated**: 2025-05-01T17:39:02Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities in rewriting text across various styles. However, effectively leveraging this ability for example-based arbitrary style transfer, where an input text is rewritten to match the style of a given exemplar, remains an open challenge. A key question is how to describe the style of the exemplar to guide LLMs toward high-quality rewrites. In this work, we propose a prompting method based on register analysis to guide LLMs to perform this task. Empirical evaluations across multiple style transfer tasks show that our prompting approach enhances style transfer strength while preserving meaning more effectively than existing prompting strategies.

**Link**: [arxiv](http://arxiv.org/abs/2505.00679v1),  [pdf](http://arxiv.org/pdf/2505.00679v1)

**Tags**: cs.CL 



### Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future   Directions
**Authors**: Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, Jeff Z. Pan

**Updated**: 2025-05-01T17:31:33Z

**Summary**: Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\footnote{The paper list, datasets, methods and tools are available at \href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\_Memory\_in\_AI}.}.

**Link**: [arxiv](http://arxiv.org/abs/2505.00675v1),  [pdf](http://arxiv.org/pdf/2505.00675v1)

**Tags**: cs.CL 



### Multi-Constraint Safe Reinforcement Learning via Closed-form Solution   for Log-Sum-Exp Approximation of Control Barrier Functions
**Authors**: Chenggang Wang, Xinyi Wang, Yutong Dong, Lei Song, Xinping Guan

**Updated**: 2025-05-01T17:22:11Z

**Summary**: The safety of training task policies and their subsequent application using reinforcement learning (RL) methods has become a focal point in the field of safe RL. A central challenge in this area remains the establishment of theoretical guarantees for safety during both the learning and deployment processes. Given the successful implementation of Control Barrier Function (CBF)-based safety strategies in a range of control-affine robotic systems, CBF-based safe RL demonstrates significant promise for practical applications in real-world scenarios. However, integrating these two approaches presents several challenges. First, embedding safety optimization within the RL training pipeline requires that the optimization outputs be differentiable with respect to the input parameters, a condition commonly referred to as differentiable optimization, which is non-trivial to solve. Second, the differentiable optimization framework confronts significant efficiency issues, especially when dealing with multi-constraint problems. To address these challenges, this paper presents a CBF-based safe RL architecture that effectively mitigates the issues outlined above. The proposed approach constructs a continuous AND logic approximation for the multiple constraints using a single composite CBF. By leveraging this approximation, a close-form solution of the quadratic programming is derived for the policy network in RL, thereby circumventing the need for differentiable optimization within the end-to-end safe RL pipeline. This strategy significantly reduces computational complexity because of the closed-form solution while maintaining safety guarantees. Simulation results demonstrate that, in comparison to existing approaches relying on differentiable optimization, the proposed method significantly reduces training computational costs while ensuring provable safety throughout the training process.

**Link**: [arxiv](http://arxiv.org/abs/2505.00671v1),  [pdf](http://arxiv.org/pdf/2505.00671v1)

**Tags**: cs.RO cs.SY eess.SY 



### Deep Reinforcement Learning for Urban Air Quality Management:   Multi-Objective Optimization of Pollution Mitigation Booth Placement in   Metropolitan Environments
**Authors**: Kirtan Rajesh, Suvidha Rupesh Kumar

**Updated**: 2025-05-01T17:19:48Z

**Summary**: Urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like Delhi, where exposure to harmful pollutants severely impacts public health. Delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. Traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. This study presents a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths to improve the air quality index (AQI) in the city of Delhi. We employ Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. Our approach is benchmarked against conventional placement strategies, including random and greedy AQI-based methods, using multi-dimensional performance evaluation metrics such as AQI improvement, spatial coverage, population and traffic impact, and spatial entropy. Experimental results demonstrate that the RL-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. Notably, the DRL framework achieves an optimal trade-off between AQI reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. The findings underscore the potential of AI-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management.

**Link**: [arxiv](http://arxiv.org/abs/2505.00668v1),  [pdf](http://arxiv.org/pdf/2505.00668v1)

**Tags**: cs.CV cs.AI cs.LG 



### Artificial Scientific Discovery
**Authors**: Antonio Norelli

**Updated**: 2025-05-01T17:09:17Z

**Summary**: Rooted in the explosion of deep learning over the past decade, this thesis spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts needed to realize the vision of an artificial scientist: a machine with the capacity to autonomously generate original research and contribute to the expansion of human knowledge. The investigation begins with Olivaw, an AlphaGo Zero-like agent that discovers Othello knowledge from scratch but is unable to communicate it. This realization leads to the development of the Explanatory Learning (EL) framework, a formalization of the problem faced by a scientist when trying to explain a new phenomenon to their peers. The effective EL prescriptions allow us to crack Zendo, a popular board game simulating the scientific endeavor. This success comes with a fundamental insight: an artificial scientist must develop its own interpretation of the language used to explain its findings, and not rely on a rigid existing interpreter. Questioning the very process of learning an interpreter, we turn our attention to the inner functioning of modern multimodal models. This culminates in a simple idea to build CLIP-like models where interpretation and perception are explicitly disentangled: a cost-effective approach that couples two unimodal models using little multimodal data and no further training. Finally, we discuss what ChatGPT and its siblings are still missing to become artificial scientists, and introduce the Big-Bench Symbol Interpretation Task, a benchmark about interpreting Zendo-like explanations that sees LLMs going no further than random chance while being instead fully solved by humans.

**Link**: [arxiv](http://arxiv.org/abs/2411.11672v2),  [pdf](http://arxiv.org/pdf/2411.11672v2)

**Tags**: cs.AI cs.LG I.2 



### DeepCritic: Deliberate Critique with Large Language Models
**Authors**: Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen

**Updated**: 2025-05-01T17:03:17Z

**Summary**: As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.

**Link**: [arxiv](http://arxiv.org/abs/2505.00662v1),  [pdf](http://arxiv.org/pdf/2505.00662v1)

**Tags**: cs.CL cs.AI cs.LG 



### Large Language Models Understanding: an Inherent Ambiguity Barrier
**Authors**: Daniel N. Nissani

**Updated**: 2025-05-01T16:55:44Z

**Summary**: A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.

**Link**: [arxiv](http://arxiv.org/abs/2505.00654v1),  [pdf](http://arxiv.org/pdf/2505.00654v1)

**Tags**: cs.CL cs.AI 



### Open-Source LLM-Driven Federated Transformer for Predictive IoV   Management
**Authors**: Yazan Otoum, Arghavan Asad, Ishtiaq Ahmad

**Updated**: 2025-05-01T16:54:21Z

**Summary**: The proliferation of connected vehicles within the Internet of Vehicles (IoV) ecosystem presents critical challenges in ensuring scalable, real-time, and privacy-preserving traffic management. Existing centralized IoV solutions often suffer from high latency, limited scalability, and reliance on proprietary Artificial Intelligence (AI) models, creating significant barriers to widespread deployment, particularly in dynamic and privacy-sensitive environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular systems remains underexplored, especially concerning prompt optimization and effective utilization in federated contexts. To address these challenges, we propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel framework that leverages open-source LLMs for predictive IoV management. FPoTT introduces a dynamic prompt optimization mechanism that iteratively refines textual prompts to enhance trajectory prediction. The architecture employs a dual-layer federated learning paradigm, combining lightweight edge models for real-time inference with cloud-based LLMs to retain global intelligence. A Transformer-driven synthetic data generator is incorporated to augment training with diverse, high-fidelity traffic scenarios in the Next Generation Simulation (NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data while maintaining high performance on synthetic datasets. These results underscore the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management, offering a promising alternative to proprietary solutions in smart mobility ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00651v1),  [pdf](http://arxiv.org/pdf/2505.00651v1)

**Tags**: cs.AI cs.ET cs.LG 



### Investigating Task Arithmetic for Zero-Shot Information Retrieval
**Authors**: Marco Braga, Pranav Kasela, Alessandro Raganato, Gabriella Pasi

**Updated**: 2025-05-01T16:48:37Z

**Summary**: Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.

**Link**: [arxiv](http://arxiv.org/abs/2505.00649v1),  [pdf](http://arxiv.org/pdf/2505.00649v1)

**Tags**: cs.IR cs.CL cs.LG 



### Automated Review Generation Method Based on Large Language Models
**Authors**: Shican Wu, Xiao Ma, Dehui Luo, Lulu Li, Xiangcheng Shi, Xin Chang, Xiaoyun Lin, Ran Luo, Chunlei Pei, Changying Du, Zhi-Jian Zhao, Jinlong Gong

**Updated**: 2025-05-01T16:24:05Z

**Summary**: Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\% with 95\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.

**Link**: [arxiv](http://arxiv.org/abs/2407.20906v5),  [pdf](http://arxiv.org/pdf/2407.20906v5)

**Tags**: cs.CL cs.AI physics.data-an 



### Fully passive quantum random number generation with untrusted light
**Authors**: KaiWei Qiu, Yu Cai, Nelly H. Y. Ng, Jing Yan Haw

**Updated**: 2025-05-01T16:21:50Z

**Summary**: Quantum random number generators (QRNGs) harness the inherent unpredictability of quantum mechanics to produce true randomness. Yet, in many optical implementations, the light source remains a potential vulnerability - susceptible to deviations from ideal behavior and even adversarial eavesdropping. Source-device-independent (SDI) protocols address this with a pragmatic strategy, by removing trust assumptions on the source, and instead rely on realistic modelling and characterization of the measurement device. In this work, we enhance an existing SDI-QRNG protocol by eliminating the need for a perfectly balanced beam splitter within the trusted measurement device, which is an idealized assumption made for the simplification of security analysis. We demonstrate that certified randomness can still be reliably extracted across a wide range of beam-splitting ratios, significantly improving the protocol's practicality and robustness. Using only off-the-shelf components, our implementation achieves real-time randomness generation rates of 0.347 Gbps. We also experimentally validate the protocol's resilience against adversarial attacks and highlight its self-testing capabilities. These advances mark a significant step toward practical, lightweight, high-performance, fully-passive, and composably secure QRNGs suitable for real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2505.00636v1),  [pdf](http://arxiv.org/pdf/2505.00636v1)

**Tags**: quant-ph 



### Reward-Augmented Data Enhances Direct Preference Alignment of LLMs
**Authors**: Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang

**Updated**: 2025-05-01T16:20:11Z

**Summary**: Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion. Our code is available at https://github.com/shenao-zhang/reward-augmented-preference.

**Link**: [arxiv](http://arxiv.org/abs/2410.08067v4),  [pdf](http://arxiv.org/pdf/2410.08067v4)

**Tags**: cs.LG cs.AI 



### The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning   (and How to Fix Them)
**Authors**: Zihao Wang, Yibo Jiang, Jiahao Yu, Heqing Huang

**Updated**: 2025-05-01T16:06:16Z

**Summary**: Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.

**Link**: [arxiv](http://arxiv.org/abs/2505.00626v1),  [pdf](http://arxiv.org/pdf/2505.00626v1)

**Tags**: cs.CL cs.AI 68T50 I.2 



### FineScope : Precision Pruning for Domain-Specialized Large Language   Models Using SAE-Guided Self-Data Cultivation
**Authors**: Chaitali Bhattacharyya, Yeseong Kim

**Updated**: 2025-05-01T16:05:08Z

**Summary**: Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2505.00624v1),  [pdf](http://arxiv.org/pdf/2505.00624v1)

**Tags**: cs.CL cs.AI 



### Combining LLMs with Logic-Based Framework to Explain MCTS
**Authors**: Ziyan An, Xia Wang, Hendrik Baier, Zirong Chen, Abhishek Dubey, Taylor T. Johnson, Jonathan Sprinkle, Ayan Mukhopadhyay, Meiyi Ma

**Updated**: 2025-05-01T15:40:58Z

**Summary**: In response to the lack of trust in Artificial Intelligence (AI) for sequential planning, we design a Computational Tree Logic-guided large language model (LLM)-based natural language explanation framework designed for the Monte Carlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to interpret due to the complexity of its search trees, but our framework is flexible enough to handle a wide range of free-form post-hoc queries and knowledge-based inquiries centered around MCTS and the Markov Decision Process (MDP) of the application domain. By transforming user queries into logic and variable statements, our framework ensures that the evidence obtained from the search tree remains factually consistent with the underlying environmental dynamics and any constraints in the actual stochastic control process. We evaluate the framework rigorously through quantitative assessments, where it demonstrates strong performance in terms of accuracy and factual consistency.

**Link**: [arxiv](http://arxiv.org/abs/2505.00610v1),  [pdf](http://arxiv.org/pdf/2505.00610v1)

**Tags**: cs.AI 



### Surviving the Storm: The Impacts of Open RAN Disaggregation on Latency   and Resilience
**Authors**: Sotiris Chatzimiltis, Mohammad Shojafar, Mahdi Boloursaz Mashhadi, Rahim Tafazolli

**Updated**: 2025-05-01T15:35:31Z

**Summary**: The development of Open Radio Access Networks (Open RAN), with their disaggregated architectures and virtualization of network functions, has brought considerable flexibility and cost savings to mobile networks. However, these architectural advancements introduce additional latency during the initial attachment procedure of User Equipment (UE), increasing the risk of signaling storms. This paper investigates the latency impact due to disaggregation of the Base-band Unit (BBU) into the Central Unit (CU) and Distributed Unit (DU). Specifically, we model the delays induced due to disaggregation on UE attachment, analyzing the performance under varying load conditions, and sensitivity to processing times. We demonstrate that while both monolithic and Open RAN architectures experience performance degradation under high-load conditions, Open RAN's added overheads can increase its susceptibility to congestion and signaling storms. However, Open RAN's inherent flexibility, enabled by disaggregation and virtualization, allows efficient deployment of resources, faster service deployment, and adaptive congestion control mechanisms to mitigate these risks and enhance overall system resilience. Thereby, we quantify resilience by introducing a new utility function and propose a novel adaptation mechanism to reinforce Open RAN's robustness against signaling storms. Our results show that the proposed adaptive mechanism significantly enhances resilience, achieving improvements of up to 286% over fixed configurations, with resilience scores approaching 0.96 under optimal conditions. While simulation results show that Open RAN disaggregation increases attachment latency and susceptibility to signaling congestion, they also highlight that its architectural flexibility can mitigate these effects, improving resilience under high-load conditions.

**Link**: [arxiv](http://arxiv.org/abs/2505.00605v1),  [pdf](http://arxiv.org/pdf/2505.00605v1)

**Tags**: cs.NI 



### Can LLMs Help Improve Analogical Reasoning For Strategic Decisions?   Experimental Evidence from Humans and GPT-4
**Authors**: Phanish Puranam, Prothit Sen, Maciej Workiewicz

**Updated**: 2025-05-01T15:35:01Z

**Summary**: This study investigates whether large language models, specifically GPT4, can match human capabilities in analogical reasoning within strategic decision making contexts. Using a novel experimental design involving source to target matching, we find that GPT4 achieves high recall by retrieving all plausible analogies but suffers from low precision, frequently applying incorrect analogies based on superficial similarities. In contrast, human participants exhibit high precision but low recall, selecting fewer analogies yet with stronger causal alignment. These findings advance theory by identifying matching, the evaluative phase of analogical reasoning, as a distinct step that requires accurate causal mapping beyond simple retrieval. While current LLMs are proficient in generating candidate analogies, humans maintain a comparative advantage in recognizing deep structural similarities across domains. Error analysis reveals that AI errors arise from surface level matching, whereas human errors stem from misinterpretations of causal structure. Taken together, the results suggest a productive division of labor in AI assisted organizational decision making where LLMs may serve as broad analogy generators, while humans act as critical evaluators, applying the most contextually appropriate analogies to strategic problems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00603v1),  [pdf](http://arxiv.org/pdf/2505.00603v1)

**Tags**: cs.AI cs.HC 



### A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security   and Privacy in IoT and Edge Networks
**Authors**: Muhammad Shahbaz Khan, Ahmed Al-Dubai, Jawad Ahmad, Nikolaos Pitropakis, Baraq Ghaleb

**Updated**: 2025-05-01T15:26:48Z

**Summary**: The security of image data in the Internet of Things (IoT) and edge networks is crucial due to the increasing deployment of intelligent systems for real-time decision-making. Traditional encryption algorithms such as AES and RSA are computationally expensive for resource-constrained IoT devices and ineffective for large-volume image data, leading to inefficiencies in privacy-preserving distributed learning applications. To address these concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic Chain Permutation and Confusion mechanisms to enhance security while maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS, which extracts and reorganizes pixels based on high and low edge intensity features for correlation disruption; (2) Chaotic Chain Permutation, which employs a logistic chaotic map with SHA-256-based dynamically updated keys for block-wise permutation; and (3) Chaotic chain Confusion, which utilises dynamically generated chaotic seed matrices for bitwise XOR operations. Extensive security and performance evaluations demonstrate that the proposed scheme significantly reduces pixel correlation -- almost zero, achieves high entropy values close to 8, and resists differential cryptographic attacks. The optimum design of the proposed scheme makes it suitable for real-time deployment in resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2505.00593v1),  [pdf](http://arxiv.org/pdf/2505.00593v1)

**Tags**: cs.CR 



### Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced   Disease Grading
**Authors**: Shuo Tong, Shangde Gao, Ke Liu, Zihang Huang, Hongxia Xu, Haochao Ying, Jian Wu

**Updated**: 2025-05-01T15:26:23Z

**Summary**: Automatic disease image grading is a significant application of artificial intelligence for healthcare, enabling faster and more accurate patient assessments. However, domain shifts, which are exacerbated by data imbalance, introduce bias into the model, posing deployment difficulties in clinical applications. To address the problem, we propose a novel \textbf{U}ncertainty-aware \textbf{M}ulti-experts \textbf{K}nowledge \textbf{D}istillation (UMKD) framework to transfer knowledge from multiple expert models to a single student model. Specifically, to extract discriminative features, UMKD decouples task-agnostic and task-specific features with shallow and compact feature alignment in the feature space. At the output space, an uncertainty-aware decoupled distillation (UDD) mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust and reliable distillation. Additionally, UMKD also tackles the problems of model architecture heterogeneity and distribution discrepancies between source and target domains, which are inadequately tackled by previous KD approaches. Extensive experiments on histology prostate grading (\textit{SICAPv2}) and fundus image grading (\textit{APTOS}) demonstrate that UMKD achieves a new state-of-the-art in both source-imbalanced and target-imbalanced scenarios, offering a robust and practical solution for real-world disease image grading.

**Link**: [arxiv](http://arxiv.org/abs/2505.00592v1),  [pdf](http://arxiv.org/pdf/2505.00592v1)

**Tags**: cs.CV cs.LG 



### Block Circulant Adapter for Large Language Models
**Authors**: Xinyu Ding, Meiqi Wang, Siyu Liao, Zhongfeng Wang

**Updated**: 2025-05-01T15:14:32Z

**Summary**: Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\times$ less number of parameters than VeRA, $16\times$ smaller than LoRA and $32\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2505.00582v1),  [pdf](http://arxiv.org/pdf/2505.00582v1)

**Tags**: cs.CL cs.LG 



### AI-in-the-Loop Planning for Transportation Electrification: Case Studies   from Austin, Texas
**Authors**: Seung Jun Choi

**Updated**: 2025-05-01T15:07:50Z

**Summary**: This study explores the integration of AI in transportation electrification planning in Austin, TX, focusing on the use of Geospatial AI (GeoAI), Generative AI (GenAI), and Large Language Models (LLMs). GeoAI enhances site selection, localized GenAI models support meta-level estimations, and LLMs enable scenario simulations. These AI applications require human oversight. GeoAI outputs must be evaluated with land use data, GenAI models are not always accurate, and LLMs are prone to hallucinations. To ensure accountable planning, human planners must work alongside AI agents. Establishing a community feedback loop is essential to audit automated decisions. Planners should place Community Experience (CX) at the center of Urban Planning AI.

**Link**: [arxiv](http://arxiv.org/abs/2504.21185v2),  [pdf](http://arxiv.org/pdf/2504.21185v2)

**Tags**: cs.CY 



### Belief Roadmaps with Uncertain Landmark Evanescence
**Authors**: Erick Fuentes, Jared Strader, Ethan Fahnestock, Nicholas Roy

**Updated**: 2025-05-01T15:03:04Z

**Summary**: We would like a robot to navigate to a goal location while minimizing state uncertainty. To aid the robot in this endeavor, maps provide a prior belief over the location of objects and regions of interest. To localize itself within the map, a robot identifies mapped landmarks using its sensors. However, as the time between map creation and robot deployment increases, portions of the map can become stale, and landmarks, once believed to be permanent, may disappear. We refer to the propensity of a landmark to disappear as landmark evanescence. Reasoning about landmark evanescence during path planning, and the associated impact on localization accuracy, requires analyzing the presence or absence of each landmark, leading to an exponential number of possible outcomes of a given motion plan. To address this complexity, we develop BRULE, an extension of the Belief Roadmap. During planning, we replace the belief over future robot poses with a Gaussian mixture which is able to capture the effects of landmark evanescence. Furthermore, we show that belief updates can be made efficient, and that maintaining a random subset of mixture components is sufficient to find high quality solutions. We demonstrate performance in simulated and real-world experiments. Software is available at https://bit.ly/BRULE.

**Link**: [arxiv](http://arxiv.org/abs/2501.17982v2),  [pdf](http://arxiv.org/pdf/2501.17982v2)

**Tags**: cs.RO cs.AI 



### Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase   Transition in Large Language Models
**Authors**: Makoto Sato

**Updated**: 2025-05-01T14:58:32Z

**Summary**: What underlies intuitive human thinking? One approach to this question is to compare the cognitive dynamics of humans and large language models (LLMs). However, such a comparison requires a method to quantitatively analyze AI cognitive behavior under controlled conditions. While anecdotal observations suggest that certain prompts can dramatically change LLM behavior, these observations have remained largely qualitative. Here, we propose a two-part framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP) that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying Prompt (TQP) that evaluates this change using a separate LLM. Through controlled experiments, we examined how LLMs react to prompts embedding two semantically distant concepts (e.g., mathematical aperiodicity and traditional crafts)-either fused together or presented separately-by changing their linguistic quality and affective tone. Whereas humans tend to experience heightened engagement when such concepts are meaningfully blended producing a novel concept-a form of conceptual fusion-current LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts. This suggests that LLMs may not yet replicate the conceptual integration processes seen in human intuition. Our method enables fine-grained, reproducible measurement of cognitive responsiveness, and may help illuminate key differences in how intuition and conceptual leaps emerge in artificial versus human minds.

**Link**: [arxiv](http://arxiv.org/abs/2504.21012v2),  [pdf](http://arxiv.org/pdf/2504.21012v2)

**Tags**: cs.CL cs.AI 



### UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models   for Multilingual Multimodal Idiomaticity Representation
**Authors**: Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang

**Updated**: 2025-05-01T14:54:16Z

**Summary**: SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.

**Link**: [arxiv](http://arxiv.org/abs/2502.20984v3),  [pdf](http://arxiv.org/pdf/2502.20984v3)

**Tags**: cs.CL cs.AI 



### FreqKV: Frequency Domain Key-Value Compression for Efficient Context   Window Extension
**Authors**: Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin

**Updated**: 2025-05-01T14:53:12Z

**Summary**: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2505.00570v1),  [pdf](http://arxiv.org/pdf/2505.00570v1)

**Tags**: cs.CL cs.AI 



### Triggering Hallucinations in LLMs: A Quantitative Study of   Prompt-Induced Hallucination in Large Language Models
**Authors**: Makoto Sato

**Updated**: 2025-05-01T14:33:47Z

**Summary**: Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.

**Link**: [arxiv](http://arxiv.org/abs/2505.00557v1),  [pdf](http://arxiv.org/pdf/2505.00557v1)

**Tags**: cs.CL cs.AI 



### Efficiency and Effectiveness of LLM-Based Summarization of Evidence in   Crowdsourced Fact-Checking
**Authors**: Kevin Roitero, Dustin Wright, Michael Soprano, Isabelle Augenstein, Stefano Mizzaro

**Updated**: 2025-05-01T14:05:16Z

**Summary**: Evaluating the truthfulness of online content is critical for combating misinformation. This study examines the efficiency and effectiveness of crowdsourced truthfulness assessments through a comparative analysis of two approaches: one involving full-length webpages as evidence for each claim, and another using summaries for each evidence document generated with a large language model. Using an A/B testing setting, we engage a diverse pool of participants tasked with evaluating the truthfulness of statements under these conditions. Our analysis explores both the quality of assessments and the behavioral patterns of participants. The results reveal that relying on summarized evidence offers comparable accuracy and error metrics to the Standard modality while significantly improving efficiency. Workers in the Summary setting complete a significantly higher number of assessments, reducing task duration and costs. Additionally, the Summary modality maximizes internal agreement and maintains consistent reliance on and perceived usefulness of evidence, demonstrating its potential to streamline large-scale truthfulness evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2501.18265v2),  [pdf](http://arxiv.org/pdf/2501.18265v2)

**Tags**: cs.IR cs.CL cs.HC 



### Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in   Reinforcement Learning Frameworks
**Authors**: Xinyu Wang, Jinbo Bi, Minghu Song

**Updated**: 2025-05-01T13:57:20Z

**Summary**: SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.

**Link**: [arxiv](http://arxiv.org/abs/2505.00530v1),  [pdf](http://arxiv.org/pdf/2505.00530v1)

**Tags**: cs.LG cs.CE q-bio.BM 



### HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World   Hallucination Detection
**Authors**: Deanna Emery, Michael Goitia, Freddie Vargus, Iulia Neagu

**Updated**: 2025-05-01T13:22:45Z

**Summary**: As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\unicode{x2013}$text that is not grounded in supporting evidence$\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\unicode{x2013}$both open and closed source$\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.

**Link**: [arxiv](http://arxiv.org/abs/2505.00506v1),  [pdf](http://arxiv.org/pdf/2505.00506v1)

**Tags**: cs.CL cs.AI 



### Distilling Calibration via Conformalized Credal Inference
**Authors**: Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone

**Updated**: 2025-05-01T13:04:11Z

**Summary**: Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.

**Link**: [arxiv](http://arxiv.org/abs/2501.06066v3),  [pdf](http://arxiv.org/pdf/2501.06066v3)

**Tags**: cs.LG cs.AI eess.SP 



### Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and   Datasets
**Authors**: Lorenz Brehme, Thomas Ströhle, Ruth Breu

**Updated**: 2025-05-01T13:03:37Z

**Summary**: Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2504.20119v2),  [pdf](http://arxiv.org/pdf/2504.20119v2)

**Tags**: cs.IR cs.AI 



### MULE: Multi-terrain and Unknown Load Adaptation for Effective   Quadrupedal Locomotion
**Authors**: Vamshi Kumar Kurva, Shishir Kolathaya

**Updated**: 2025-05-01T12:41:35Z

**Summary**: Quadrupedal robots are increasingly deployed for load-carrying tasks across diverse terrains. While Model Predictive Control (MPC)-based methods can account for payload variations, they often depend on predefined gait schedules or trajectory generators, limiting their adaptability in unstructured environments. To address these limitations, we propose an Adaptive Reinforcement Learning (RL) framework that enables quadrupedal robots to dynamically adapt to both varying payloads and diverse terrains. The framework consists of a nominal policy responsible for baseline locomotion and an adaptive policy that learns corrective actions to preserve stability and improve command tracking under payload variations. We validate the proposed approach through large-scale simulation experiments in Isaac Gym and real-world hardware deployment on a Unitree Go1 quadruped. The controller was tested on flat ground, slopes, and stairs under both static and dynamic payload changes. Across all settings, our adaptive controller consistently outperformed the controller in tracking body height and velocity commands, demonstrating enhanced robustness and adaptability without requiring explicit gait design or manual tuning.

**Link**: [arxiv](http://arxiv.org/abs/2505.00488v1),  [pdf](http://arxiv.org/pdf/2505.00488v1)

**Tags**: cs.RO cs.AI 



### (Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration   for Translating Ultra-Long Literary Texts
**Authors**: Minghao Wu, Jiahao Xu, Yulin Yuan, Gholamreza Haffari, Longyue Wang, Weihua Luo, Kaifu Zhang

**Updated**: 2025-05-01T12:02:02Z

**Summary**: Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.

**Link**: [arxiv](http://arxiv.org/abs/2405.11804v2),  [pdf](http://arxiv.org/pdf/2405.11804v2)

**Tags**: cs.CL 



### EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful   Prompt Optimizers
**Authors**: Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, Yujiu Yang

**Updated**: 2025-05-01T11:56:52Z

**Summary**: Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2309.08532v3),  [pdf](http://arxiv.org/pdf/2309.08532v3)

**Tags**: cs.CL cs.AI 



### UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces
**Authors**: Alaa Saleh, Sasu Tarkoma, Praveen Kumar Donta, Naser Hossein Motlagh, Schahram Dustdar, Susanna Pirttikangas, Lauri Lovén

**Updated**: 2025-05-01T11:54:49Z

**Summary**: Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.

**Link**: [arxiv](http://arxiv.org/abs/2505.00472v1),  [pdf](http://arxiv.org/pdf/2505.00472v1)

**Tags**: cs.AI cs.DC cs.MA cs.NI 



### Red Teaming Large Language Models for Healthcare
**Authors**: Vahid Balazadeh, Michael Cooper, David Pellow, Atousa Assadi, Jennifer Bell, Jim Fackler, Gabriel Funingana, Spencer Gable-Cook, Anirudh Gangadhar, Abhishek Jaiswal, Sumanth Kaja, Christopher Khoury, Randy Lin, Kaden McKeen, Sara Naimimohasses, Khashayar Namdar, Aviraj Newatia, Allan Pang, Anshul Pattoo, Sameer Peesapati, Diana Prepelita, Bogdana Rakova, Saba Sadatamin, Rafael Schulman, Ajay Shah, Syed Azhar Shah, Syed Ahmar Shah, Babak Taati, Balagopal Unnikrishnan, Stephanie Williams, Rahul G Krishnan

**Updated**: 2025-05-01T11:43:27Z

**Summary**: We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.

**Link**: [arxiv](http://arxiv.org/abs/2505.00467v1),  [pdf](http://arxiv.org/pdf/2505.00467v1)

**Tags**: cs.CL cs.AI 



### Distributed Retrieval-Augmented Generation
**Authors**: Chenhao Xu, Longxiang Gao, Yuan Miao, Xi Zheng

**Updated**: 2025-05-01T10:37:06Z

**Summary**: As large language models (LLMs) become increasingly adopted on edge devices, Retrieval-Augmented Generation (RAG) is gaining prominence as a solution to address factual deficiencies and hallucinations by integrating external knowledge. However, centralized RAG architectures face significant challenges in data privacy and scalability. For instance, smart healthcare services often rely on collecting sensitive patient data and building a centralized knowledge base to provide better diagnosis and treatment advice, while privacy concerns significantly impede this process. Besides, maintaining a comprehensive and continuously updated knowledge base is costly, particularly in response to regional epidemics and rapidly mutating viruses. To address these challenges, this paper introduces Distributed Retrieval-Augmented Generation (DRAG), a novel framework that improves data privacy by eliminating the need for a centralized knowledge base and restoring data control to owners. DRAG incorporates a Topic-Aware Random Walk (TARW) algorithm that leverages LLMs to extract query topics and facilitate targeted peer discovery within a peer-to-peer network, enabling efficient knowledge retrieval in decentralized environments. Extensive experiments across three diverse datasets and LLMs demonstrate that DRAG with TARW achieves near-centralized RAG performance by using half as many messages as flooding. The code is available at https://github.com/xuchenhao001/DRAG.

**Link**: [arxiv](http://arxiv.org/abs/2505.00443v1),  [pdf](http://arxiv.org/pdf/2505.00443v1)

**Tags**: cs.DC 



### Can Differentially Private Fine-tuning LLMs Protect Against Privacy   Attacks?
**Authors**: Hao Du, Shang Liu, Yang Cao

**Updated**: 2025-05-01T10:10:01Z

**Summary**: Fine-tuning large language models (LLMs) has become an essential strategy for adapting them to specialized tasks; however, this process introduces significant privacy challenges, as sensitive training data may be inadvertently memorized and exposed. Although differential privacy (DP) offers strong theoretical guarantees against such leakage, its empirical privacy effectiveness on LLMs remains unclear, especially under different fine-tuning methods. In this paper, we systematically investigate the impact of DP across fine-tuning methods and privacy budgets, using both data extraction and membership inference attacks to assess empirical privacy risks. Our main findings are as follows: (1) Differential privacy reduces model utility, but its impact varies significantly across different fine-tuning methods. (2) Without DP, the privacy risks of models fine-tuned with different approaches differ considerably. (3) When DP is applied, even a relatively high privacy budget can substantially lower privacy risk. (4) The privacy-utility trade-off under DP training differs greatly among fine-tuning methods, with some methods being unsuitable for DP due to severe utility degradation. Our results provide practical guidance for privacy-conscious deployment of LLMs and pave the way for future research on optimizing the privacy-utility trade-off in fine-tuning methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2504.21036v2),  [pdf](http://arxiv.org/pdf/2504.21036v2)

**Tags**: cs.CR cs.AI cs.LG 



### A Neural Network Mode for PX4 on Embedded Flight Controllers
**Authors**: Sindre M. Hegre, Welf Rehberg, Mihir Kulkarni, Kostas Alexis

**Updated**: 2025-05-01T10:01:43Z

**Summary**: This paper contributes an open-sourced implementation of a neural-network based controller framework within the PX4 stack. We develop a custom module for inference on the microcontroller while retaining all of the functionality of the PX4 autopilot. Policies trained in the Aerial Gym Simulator are converted to the TensorFlow Lite format and then built together with PX4 and flashed to the flight controller. The policies substitute the control-cascade within PX4 to offer an end-to-end position-setpoint tracking controller directly providing normalized motor RPM setpoints. Experiments conducted in simulation and the real-world show similar tracking performance. We thus provide a flight-ready pipeline for testing neural control policies in the real world. The pipeline simplifies the deployment of neural networks on embedded flight controller hardware thereby accelerating research on learning-based control. Both the Aerial Gym Simulator and the PX4 module are open-sourced at https://github.com/ntnu-arl/aerial_gym_simulator and https://github.com/SindreMHegre/PX4-Autopilot-public/tree/for_paper. Video: https://youtu.be/lY1OKz_UOqM?si=VtzL243BAY3lblTJ.

**Link**: [arxiv](http://arxiv.org/abs/2505.00432v1),  [pdf](http://arxiv.org/pdf/2505.00432v1)

**Tags**: cs.RO 



### Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts   Models
**Authors**: Keisuke Kamahori, Tian Tang, Yile Gu, Kan Zhu, Baris Kasikci

**Updated**: 2025-05-01T09:58:34Z

**Summary**: Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from the significant overhead of frequently moving data between CPU and GPU, or fail to consider distinct characteristics of CPUs and GPUs. This paper proposes Fiddler, a resource-efficient inference system for MoE models with limited GPU resources. Fiddler strategically utilizes CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Fiddler performs better in all scenarios. Compared against different baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference. The code of Fiddler is publicly available at https://github.com/efeslab/fiddler.

**Link**: [arxiv](http://arxiv.org/abs/2402.07033v3),  [pdf](http://arxiv.org/pdf/2402.07033v3)

**Tags**: cs.LG cs.AI cs.DC cs.OS 



### Network-aided Efficient LLM Services With Denoising-inspired Prompt   Compression
**Authors**: Feiran You, Hongyang Du, Kaibin Huang, Abbas Jamalipour

**Updated**: 2025-05-01T09:47:43Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, leading to their increasing adoption in diverse services delivered through wireless networks. There is a growing trend toward longer prompts to better leverage LLMs' capabilities and address difficult tasks. However, longer prompts not only increase data transmission costs but also require more computing resources and processing time, which impacts overall system efficiency and user experience. To address this challenge, we propose Joint Power and Prompt Optimization (JPPO), a framework that combines Small Language Model (SLM)-based prompt compression with wireless power allocation optimization. By deploying SLM at edge devices for prompt compression and employing Deep Reinforcement Learning (DRL) for joint optimization of compression ratio and transmission power, JPPO effectively balances service quality with resource efficiency. Furthermore, inspired by denoising diffusion models, we design a denoising-inspired prompt compression approach that iteratively compresses prompts by gradually removing non-critical information, further enhancing the framework's performance. Experimental results with long prompt tokens demonstrate that our framework achieves high service fidelity while optimizing power usage in wireless LLM services, significantly reducing the total service response time. With our DRL-based JPPO, the framework maintains fidelity comparable to the no-compression baseline while still achieving a 17% service time reduction through adaptive compression.

**Link**: [arxiv](http://arxiv.org/abs/2412.03621v3),  [pdf](http://arxiv.org/pdf/2412.03621v3)

**Tags**: cs.NI 



### GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for   Task-Oriented Grasping
**Authors**: Teli Ma, Zifan Wang, Jiaming Zhou, Mengmeng Wang, Junwei Liang

**Updated**: 2025-05-01T09:13:09Z

**Summary**: Inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. Current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3D radiance modeling, restricting real-time, open-vocabulary interactions with objects. To address these limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary Affordance Reasoning framework, which fine-tunes the Large Language Models (LLMs) to predict the visual affordance of graspable object parts within RGB feature space. We compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. GLOVER inherits world knowledge and common-sense reasoning from LLMs, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. To enable effective real-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. In evaluations across 30 table-top real-world scenes, GLOVER achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 29 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art. We also validate the generalization across embodiments, showing effectiveness in humanoid robots with dexterous hands.

**Link**: [arxiv](http://arxiv.org/abs/2411.12286v2),  [pdf](http://arxiv.org/pdf/2411.12286v2)

**Tags**: cs.RO cs.CV 



### HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in   Large Language Models
**Authors**: Paul Darm, Annalisa Riccardi

**Updated**: 2025-05-01T09:03:35Z

**Summary**: Robust alignment guardrails for large language models are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination for Llama 2. Our method applies fine-grained interventions at specific model subcomponents, particularly attention heads, using a simple binary choice probing strategy. These interventions then generalise to the open-ended generation setting effectively circumventing safety guardrails. We show that probing single attention heads is more effective than intervening on full layers and intervening on only four attention heads is comparable to supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. Our findings highlight the shortcomings of current alignment techniques. In addition, our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviors. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.

**Link**: [arxiv](http://arxiv.org/abs/2502.05945v2),  [pdf](http://arxiv.org/pdf/2502.05945v2)

**Tags**: cs.CL cs.AI I.2.7 



### SNR-aware Semantic Image Transmission with Deep Learning-based Channel   Estimation in Fading Channels
**Authors**: Mahmoud M. Salim, Mohamed S. Abdalzaher, Ali H. Muqaibel, Hussein A. Elsayed, Inkyu Lee

**Updated**: 2025-05-01T08:56:23Z

**Summary**: Semantic communications (SCs) play a central role in shaping the future of the sixth generation (6G) wireless systems, which leverage rapid advances in deep learning (DL). In this regard, end-to-end optimized DL-based joint source-channel coding (JSCC) has been adopted to achieve SCs, particularly in image transmission. Utilizing vision transformers in the encoder/decoder design has enabled significant advancements in image semantic extraction, surpassing traditional convolutional neural networks (CNNs). In this paper, we propose a new JSCC paradigm for image transmission, namely Swin semantic image transmission (SwinSIT), based on the Swin transformer. The Swin transformer is employed to construct both the semantic encoder and decoder for efficient image semantic extraction and reconstruction. Inspired by the squeezing-and-excitation (SE) network, we introduce a signal-to-noise-ratio (SNR)-aware module that utilizes SNR feedback to adaptively perform a double-phase enhancement for the encoder-extracted semantic map and its noisy version at the decoder. Additionally, a CNN-based channel estimator and compensator (CEAC) module repurposes an image-denoising CNN to mitigate fading channel effects. To optimize deployment in resource-constrained IoT devices, a joint pruning and quantization scheme compresses the SwinSIT model. Simulations evaluate the SwinSIT performance against conventional benchmarks demonstrating its effectiveness. Moreover, the model's compressed version substantially reduces its size while maintaining favorable PSNR performance.

**Link**: [arxiv](http://arxiv.org/abs/2504.20557v2),  [pdf](http://arxiv.org/pdf/2504.20557v2)

**Tags**: cs.IT math.IT 



### Aligning Beam with Imbalanced Multi-modality: A Generative Federated   Learning Approach
**Authors**: Jiahui Liang, Miaowen Wen, Shuoyao Wang, Yuxuan Liang, Shijian Gao

**Updated**: 2025-05-01T08:36:04Z

**Summary**: As vehicle intelligence advances, multi-modal sensing-aided communication emerges as a key enabler for reliable Vehicle-to-Everything (V2X) connectivity through precise environmental characterization. As centralized learning may suffer from data privacy, model heterogeneity and communication overhead issues, federated learning (FL) has been introduced to support V2X. However, the practical deployment of FL faces critical challenges: model performance degradation from label imbalance across vehicles and training instability induced by modality disparities in sensor-equipped agents. To overcome these limitations, we propose a generative FL approach for beam selection (GFL4BS). Our solution features two core innovations: 1) An adaptive zero-shot multi-modal generator coupled with spectral-regularized loss functions to enhance the expressiveness of synthetic data compensating for both label scarcity and missing modalities; 2) A hybrid training paradigm integrating feature fusion with decentralized optimization to ensure training resilience while minimizing communication costs. Experimental evaluations demonstrate significant improvements over baselines achieving 16.2% higher accuracy than the current state-of-the-art under severe label imbalance conditions while maintaining over 70% successful rate even when two agents lack both LiDAR and RGB camera inputs.

**Link**: [arxiv](http://arxiv.org/abs/2504.14835v2),  [pdf](http://arxiv.org/pdf/2504.14835v2)

**Tags**: eess.SP 



### Fitness Landscape of Large Language Model-Assisted Automated Algorithm   Search
**Authors**: Fei Liu, Qingfu Zhang, Xialiang Tong, Kun Mao, Mingxuan Yuan

**Updated**: 2025-05-01T08:33:32Z

**Summary**: Large Language Models (LLMs) have demonstrated significant potential in algorithm design. However, when integrated into search frameworks for iterative algorithm search, the underlying fitness landscape--critical for understanding search behaviou--remains underexplored. In this paper, we illustrate and analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, where nodes represent algorithms and edges denote transitions between them. We conduct extensive evaluations across six algorithm design tasks and six commonly used LLMs. Our findings reveal that LAS landscapes are highly multimodal and rugged, particularly in combinatorial optimization tasks, with distinct structural variations across tasks and LLMs. For instance, heuristic design tasks exhibit dense clusters of high-performing algorithms, while symbolic regression tasks show sparse, scattered distributions. Additionally, we demonstrate how population size influences exploration-exploitation trade-offs and the evolving trajectory of elite algorithms. These insights not only advance our understanding of LAS landscapes but also provide practical guidance for designing more effective LAS methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.19636v2),  [pdf](http://arxiv.org/pdf/2504.19636v2)

**Tags**: cs.AI cs.NE 



### Dynamic Parametric Retrieval Augmented Generation for Test-time   Knowledge Enhancement
**Authors**: Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu

**Updated**: 2025-05-01T08:03:11Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.

**Link**: [arxiv](http://arxiv.org/abs/2503.23895v2),  [pdf](http://arxiv.org/pdf/2503.23895v2)

**Tags**: cs.CL cs.AI 



### Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic   Approach
**Authors**: Ahmed R. Sadik, Muhammad Ashfaq, Niko Mäkitalo, Tommi Mikkonen

**Updated**: 2025-05-01T07:39:11Z

**Summary**: Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces challenges in system architecture, planning, task management, and execution. Traditional architectural approaches struggle with scalability, adaptability, and seamless resource integration within dynamic and complex environments. This paper presents an intelligent holonic architecture that incorporates Large Language Model (LLM) to manage the complexities of UAM. Holons function semi autonomously, allowing for real time coordination among air taxis, ground transport, and vertiports. LLMs process natural language inputs, generate adaptive plans, and manage disruptions such as weather changes or airspace closures.Through a case study of multimodal transportation with electric scooters and air taxis, we demonstrate how this architecture enables dynamic resource allocation, real time replanning, and autonomous adaptation without centralized control, creating more resilient and efficient urban transportation networks. By advancing decentralized control and AI driven adaptability, this work lays the groundwork for resilient, human centric UAM ecosystems, with future efforts targeting hybrid AI integration and real world validation.

**Link**: [arxiv](http://arxiv.org/abs/2505.00368v1),  [pdf](http://arxiv.org/pdf/2505.00368v1)

**Tags**: cs.AI cs.ET cs.MA cs.RO 



### KoACD: The First Korean Adolescent Dataset for Cognitive Distortion   Analysis
**Authors**: JunSeo Kim, HyeHyeon Kim

**Updated**: 2025-05-01T07:37:18Z

**Summary**: Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.

**Link**: [arxiv](http://arxiv.org/abs/2505.00367v1),  [pdf](http://arxiv.org/pdf/2505.00367v1)

**Tags**: cs.CL cs.AI 



### Domain-Specific Translation with Open-Source Large Language Models:   Resource-Oriented Analysis
**Authors**: Aman Kassahun Wassie, Mahdi Molaei, Yasmin Moslem

**Updated**: 2025-05-01T07:36:13Z

**Summary**: In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language directions with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant quality gap in specialized translation compared to multilingual encoder-decoder MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms all evaluated LLMs in the 7-8B parameter range across three out of the four language directions. While fine-tuning improves the performance of LLMs such as Mistral and Llama, these models still underperform compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve high-quality domain-specific translation, especially in medium-resource and low-resource settings. Moreover, the superior performance of larger LLMs over their 8B variants suggests potential value in pre-training domain-specific medium-sized language models, employing targeted data selection and knowledge distillation approaches to enhance both quality and efficiency in specialized translation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.05862v3),  [pdf](http://arxiv.org/pdf/2412.05862v3)

**Tags**: cs.CL 



### Reproducing NevIR: Negation in Neural Information Retrieval
**Authors**: Coen van den Elsen, Francien Barkhof, Thijmen Nijdam, Simon Lupart, Mohammad Aliannejadi

**Updated**: 2025-05-01T07:27:34Z

**Summary**: Negation is a fundamental aspect of human communication, yet it remains a challenge for Language Models (LMs) in Information Retrieval (IR). Despite the heavy reliance of modern neural IR systems on LMs, little attention has been given to their handling of negation. In this study, we reproduce and extend the findings of NevIR, a benchmark study that revealed most IR models perform at or below the level of random ranking when dealing with negation. We replicate NevIR's original experiments and evaluate newly developed state-of-the-art IR models. Our findings show that a recently emerging category-listwise Large Language Model (LLM) re-rankers-outperforms other models but still underperforms human performance. Additionally, we leverage ExcluIR, a benchmark dataset designed for exclusionary queries with extensive negation, to assess the generalisability of negation understanding. Our findings suggest that fine-tuning on one dataset does not reliably improve performance on the other, indicating notable differences in their data distributions. Furthermore, we observe that only cross-encoders and listwise LLM re-rankers achieve reasonable performance across both negation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.13506v3),  [pdf](http://arxiv.org/pdf/2502.13506v3)

**Tags**: cs.IR 



### Confidential Serverless Computing
**Authors**: Patrick Sabanic, Masanori Misono, Teofil Bodea, Julian Pritzi, Michael Hackl, Dimitrios Stavrakakis, Pramod Bhatotia

**Updated**: 2025-05-01T07:09:22Z

**Summary**: Although serverless computing offers compelling cost and deployment simplicity advantages, a significant challenge remains in securely managing sensitive data as it flows through the network of ephemeral function executions in serverless computing environments within untrusted clouds. While Confidential Virtual Machines (CVMs) offer a promising secure execution environment, their integration with serverless architectures currently faces fundamental limitations in key areas: security, performance, and resource efficiency. We present Hacher, a confidential computing system for secure serverless deployments to overcome these limitations. By employing nested confidential execution and a decoupled guest OS within CVMs, Hacher runs each function in a minimal "trustlet", significantly improving security through a reduced Trusted Computing Base (TCB). Furthermore, by leveraging a data-centric I/O architecture built upon a lightweight LibOS, Hacher optimizes network communication to address performance and resource efficiency challenges. Our evaluation shows that compared to CVM-based deployments, Hacher has 4.3x smaller TCB, improves end-to-end latency (15-93%), achieves higher function density (up to 907x), and reduces inter-function communication (up to 27x) and function chaining latency (16.7-30.2x); thus, Hacher offers a practical system for confidential serverless computing.

**Link**: [arxiv](http://arxiv.org/abs/2504.21518v2),  [pdf](http://arxiv.org/pdf/2504.21518v2)

**Tags**: cs.CR cs.OS 



### Optimizing Deep Neural Networks using Safety-Guided Self Compression
**Authors**: Mohammad Zbeeb, Mariam Salman, Mohammad Bazzi, Ammar Mohanna

**Updated**: 2025-05-01T06:50:30Z

**Summary**: The deployment of deep neural networks on resource-constrained devices necessitates effective model com- pression strategies that judiciously balance the reduction of model size with the preservation of performance. This study introduces a novel safety-driven quantization framework that leverages preservation sets to systematically prune and quantize neural network weights, thereby optimizing model complexity without compromising accuracy. The proposed methodology is rigorously evaluated on both a convolutional neural network (CNN) and an attention-based language model, demonstrating its applicability across diverse architectural paradigms. Experimental results reveal that our framework achieves up to a 2.5% enhancement in test accuracy relative to the original unquantized models while maintaining 60% of the initial model size. In comparison to conventional quantization techniques, our approach not only augments generalization by eliminating parameter noise and retaining essential weights but also reduces variance, thereby ensuring the retention of critical model features. These findings underscore the efficacy of safety-driven quantization as a robust and reliable strategy for the efficient optimization of deep learn- ing models. The implementation and comprehensive experimental evaluations of our framework are publicly accessible at GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2505.00350v1),  [pdf](http://arxiv.org/pdf/2505.00350v1)

**Tags**: cs.LG cs.AI 



### LLMPrism: Black-box Performance Diagnosis for Production LLM Training   Platforms
**Authors**: Zhihan Jiang, Rui Ren, Guangba Yu, Yulun Wu, Wenwei Gu, Yichen Li, Yujie Huang, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael R. Lyu

**Updated**: 2025-05-01T06:38:52Z

**Summary**: Large Language Models (LLMs) have brought about revolutionary changes in diverse fields, rendering LLM training of utmost importance for modern enterprises. To meet this demand, multi-tenant large-scale LLM training platforms have been built to offer LLM training services. Nevertheless, due to the complexity and synchronous nature of LLM training process, performance issues occur frequently and can result in substantial resource wastage. The limited visibility from the perspective of platform providers impedes existing profiling methods and poses challenges to the monitoring and diagnosis of the performance of LLM training jobs. For the first time, this paper proposes the utilization of underlying network flow data to reconstruct the training timelines of jobs based on the distinct characteristics in the LLM training procedure. We design LLMPrism, the first black-box performance diagnosis system for LLM training platforms. By progressively recognizing LLM training jobs, identifying their parallelism strategies, and reconstructing the training timelines, LLMPrism achieves non-intrusive, lightweight, and continuous monitoring of LLM training systems. Leveraging this monitoring capability, it further effectively diagnoses potential performance issues. Since Oct. 2024, LLMPrism has been deployed on our large-scale production Platform-X, in which the evaluations and deployment experiences demonstrate that LLMPrism can achieve accurate timeline reconstruction with an error within 0.3% and effectively diagnose various performance issues.

**Link**: [arxiv](http://arxiv.org/abs/2505.00342v1),  [pdf](http://arxiv.org/pdf/2505.00342v1)

**Tags**: cs.SE 



### Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale   AI Models
**Authors**: Bumjun Kim, Wan Choi

**Updated**: 2025-05-01T06:15:38Z

**Summary**: Transformer-based large language models (LLMs) have achieved remarkable success across various tasks. Yet, fine-tuning such massive models in federated learning (FL) settings poses significant challenges due to resource constraints and communication overhead. Low-Rank Adaptation (LoRA) addresses these issues by training compact, low-rank matrices instead of fully fine-tuning large models. This paper introduces a wireless federated LoRA fine-tuning framework that optimizes both learning performance and communication efficiency. We provide a novel convergence analysis, revealing how LoRA rank and covariance effects influence FL training dynamics. Leveraging these insights, we propose Sparsified Orthogonal Fine-Tuning (\textbf{SOFT}), an adaptive sparsification method that streamlines parameter updates without expensive matrix multiplications and singular value decomposition (SVD) operations. Additionally, we present a Two Stage Federated Algorithm (\textbf{TSFA}) algorithm that pre-determines key parameters offline and dynamically adjusts bandwidth and sparsification online, ensuring efficient training under latency constraints. Experiments on benchmark datasets show that our approach achieves accuracy comparable to ideal scenario models while significantly reducing communication overhead. Our framework thus enables scalable, resource-efficient deployment of large models in real-world wireless FL scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.00333v1),  [pdf](http://arxiv.org/pdf/2505.00333v1)

**Tags**: cs.LG eess.SP 



### MotionGlot: A Multi-Embodied Motion Generation Model
**Authors**: Sudarshan Harithas, Srinath Sridhar

**Updated**: 2025-05-01T06:13:42Z

**Summary**: This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motionrelated tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert-controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2410.16623v2),  [pdf](http://arxiv.org/pdf/2410.16623v2)

**Tags**: cs.RO 



### A Framework for Testing and Adapting REST APIs as LLM Tools
**Authors**: Jayachandu Bandlamudi, Ritwik Chaudhuri, Neelamadhav Gantayat, Kushal Mukherjee, Prerna Agarwal, Renuka Sindhgatta, Sameep Mehta

**Updated**: 2025-05-01T05:50:45Z

**Summary**: Large Language Models (LLMs) are enabling autonomous agents to perform complex workflows using external tools or functions, often provided via REST APIs in enterprise systems. However, directly utilizing these APIs as tools poses challenges due to their complex input schemas, elaborate responses, and often ambiguous documentation. Current benchmarks for tool testing do not adequately address these complexities, leading to a critical gap in evaluating API readiness for agent-driven automation. In this work, we present a novel testing framework aimed at evaluating and enhancing the readiness of REST APIs to function as tools for LLM-based agents. Our framework transforms apis as tools, generates comprehensive test cases for the APIs, translates tests cases into natural language instructions suitable for agents, enriches tool definitions and evaluates the agent's ability t correctly invoke the API and process its inputs and responses. To provide actionable insights, we analyze the outcomes of 750 test cases, presenting a detailed taxonomy of errors, including input misinterpretation, output handling inconsistencies, and schema mismatches. Additionally, we classify these test cases to streamline debugging and refinement of tool integrations. This work offers a foundational step toward enabling enterprise APIs as tools, improving their usability in agent-based applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.15546v2),  [pdf](http://arxiv.org/pdf/2504.15546v2)

**Tags**: cs.SE cs.AI I.2.7 



### Edge Large AI Models: Revolutionizing 6G Networks
**Authors**: Zixin Wang, Yuanming Shi, Yong Zhou, Jingyang Zhu, Khaled. B. Letaief

**Updated**: 2025-05-01T05:44:00Z

**Summary**: Large artificial intelligence models (LAMs) possess human-like abilities to solve a wide range of real-world problems, exemplifying the potential of experts in various domains and modalities. By leveraging the communication and computation capabilities of geographically dispersed edge devices, edge LAM emerges as an enabling technology to empower the delivery of various real-time intelligent services in 6G. Unlike traditional edge artificial intelligence (AI) that primarily supports a single task using small models, edge LAM is featured by the need of the decomposition and distributed deployment of large models, and the ability to support highly generalized and diverse tasks. However, due to limited communication, computation, and storage resources over wireless networks, the vast number of trainable neurons and the substantial communication overhead pose a formidable hurdle to the practical deployment of edge LAMs. In this paper, we investigate the opportunities and challenges of edge LAMs from the perspectives of model decomposition and resource management. Specifically, we propose collaborative fine-tuning and full-parameter training frameworks, alongside a microservice-assisted inference architecture, to enhance the deployment of edge LAM over wireless networks. Additionally, we investigate the application of edge LAM in air-interface designs, focusing on channel prediction and beamforming. These innovative frameworks and applications offer valuable insights and solutions for advancing 6G technology.

**Link**: [arxiv](http://arxiv.org/abs/2505.00321v1),  [pdf](http://arxiv.org/pdf/2505.00321v1)

**Tags**: cs.NI cs.LG eess.SP 



### TerEffic: Highly Efficient Ternary LLM Inference on FPGA
**Authors**: Chenyang Yin, Zhenyu Bai, Pranav Venkatram, Shivam Aggarwal, Zhaoying Li, Tulika Mitra

**Updated**: 2025-05-01T05:05:03Z

**Summary**: Deploying Large Language Models (LLMs) efficiently on edge devices is often constrained by limited memory capacity and high power consumption. Low-bit quantization methods, particularly ternary quantization, have demonstrated significant potential in preserving model accuracy while substantially decreasing memory footprint and computational costs. However, existing general-purpose architectures and accelerators have not fully exploited the advantages of low-bit quantization due to insufficient specialized hardware support. We introduce TerEffic, an FPGA-based architecture tailored for ternary-quantized LLM inference. The proposed system offers flexibility through reconfigurable hardware to meet various system requirements. We evaluated two representative configurations: a fully on-chip design that stores all weights within on-chip memories, scaling out using multiple FPGAs, and an HBM-assisted design capable of accommodating larger models on a single FPGA board. Experimental results demonstrate significant performance and energy efficiency improvements. For single-batch inference on a 370 M-parameter model, our fully on-chip architecture achieves 16,300 tokens/second, delivering a throughput 192 times higher than NVIDIA Jetson Orin Nano with a power efficiency of 455 tokens/second/W, marking a 19-fold improvement. The HBM-assisted architecture processes 727 tokens/second for a larger 2.7B-parameter model, which is 3 times of the throughput of NVIDIA A100, while consuming only 46W, resulting in a power efficiency of 16 tokens/second/W, an 8-fold improvement over the A100.

**Link**: [arxiv](http://arxiv.org/abs/2502.16473v2),  [pdf](http://arxiv.org/pdf/2502.16473v2)

**Tags**: cs.AR 



### BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language   Models in Chinese
**Authors**: Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, Yining Hua

**Updated**: 2025-05-01T05:02:57Z

**Summary**: As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.

**Link**: [arxiv](http://arxiv.org/abs/2504.19314v2),  [pdf](http://arxiv.org/pdf/2504.19314v2)

**Tags**: cs.CL 



### Gateformer: Advancing Multivariate Time Series Forecasting through   Temporal and Variate-Wise Attention with Gated Representations
**Authors**: Yu-Hsiang Lan, Anton Alyakin, Eric K. Oermann

**Updated**: 2025-05-01T04:59:05Z

**Summary**: There has been a recent surge of interest in time series modeling using the Transformer architecture. However, forecasting multivariate time series with Transformer presents a unique challenge as it requires modeling both temporal (cross-time) and variate (cross-variate) dependencies. While Transformer-based models have gained popularity for their flexibility in capturing both sequential and cross-variate relationships, it is unclear how to best integrate these two sources of information in the context of the Transformer architecture while optimizing for both performance and efficiency. We re-purpose the Transformer architecture to effectively model both cross-time and cross-variate dependencies. Our approach begins by embedding each variate independently into a variate-wise representation that captures its cross-time dynamics, and then models cross-variate dependencies through attention mechanisms on these learned embeddings. Gating operations in both cross-time and cross-variate modeling phases regulate information flow, allowing the model to focus on the most relevant features for accurate predictions. Our method achieves state-of-the-art performance across 13 real-world datasets and can be seamlessly integrated into other Transformer-based and LLM-based forecasters, delivering performance improvements up to 20.7\% over original models. Code is available at this repository: https://github.com/nyuolab/Gateformer.

**Link**: [arxiv](http://arxiv.org/abs/2505.00307v1),  [pdf](http://arxiv.org/pdf/2505.00307v1)

**Tags**: cs.LG 



### AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor   Long-Term Medication Adherence and Care
**Authors**: Md Asaduzzaman Jabin, Hanqi Jiang, Yiwei Li, Patrick Kaggwa, Eugene Douglass, Juliet N. Sekandi, Tianming Liu

**Updated**: 2025-05-01T03:48:12Z

**Summary**: Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based multimodal large vision language model (LVLM) aimed at visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2505.00275v1),  [pdf](http://arxiv.org/pdf/2505.00275v1)

**Tags**: cs.CV 



### Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets   Collective Intelligence
**Authors**: Haiyan Qin, Jiahao Feng, Xiaotong Feng, Wei W. Xing, Wang Kang

**Updated**: 2025-05-01T03:43:28Z

**Summary**: Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\%--1075\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.

**Link**: [arxiv](http://arxiv.org/abs/2504.14625v3),  [pdf](http://arxiv.org/pdf/2504.14625v3)

**Tags**: cs.AR cs.AI 



### Large Language Models as AI Agents for Digital Atoms and Molecules:   Catalyzing a New Era in Computational Biophysics
**Authors**: Yijie Xia, Xiaohan Lin, Zicheng Ma, Jinyuan Hu, Yanheng Li, Zhaoxin Xie, Hao Li, Li Yang, Zhiqiang Zhao, Lijiang Yang, Zhenyu Chen, Yi Qin Gao

**Updated**: 2025-05-01T03:33:57Z

**Summary**: In computational biophysics, where molecular data is expanding rapidly and system complexity is increasing exponentially, large language models (LLMs) and agent-based systems are fundamentally reshaping the field. This perspective article examines the recent advances at the intersection of LLMs, intelligent agents, and scientific computation, with a focus on biophysical computation. Building on these advancements, we introduce ADAM (Agent for Digital Atoms and Molecules), an innovative multi-agent LLM-based framework. ADAM employs cutting-edge AI architectures to reshape scientific workflows through a modular design. It adopts a hybrid neural-symbolic architecture that combines LLM-driven semantic tools with deterministic symbolic computations. Moreover, its ADAM Tool Protocol (ATP) enables asynchronous, database-centric tool orchestration, fostering community-driven extensibility. Despite the significant progress made, ongoing challenges call for further efforts in establishing benchmarking standards, optimizing foundational models and agents, and building an open collaborative ecosystem. ADAM is accessible at https://sidereus-ai.com.

**Link**: [arxiv](http://arxiv.org/abs/2505.00270v1),  [pdf](http://arxiv.org/pdf/2505.00270v1)

**Tags**: physics.comp-ph physics.bio-ph 



### A Comprehensive Survey on Integrating Large Language Models with   Knowledge-Based Methods
**Authors**: Wenli Yang, Lilian Some, Michael Bain, Byeong Kang

**Updated**: 2025-05-01T03:29:50Z

**Summary**: The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.

**Link**: [arxiv](http://arxiv.org/abs/2501.13947v3),  [pdf](http://arxiv.org/pdf/2501.13947v3)

**Tags**: cs.CL cs.AI 



### EnronQA: Towards Personalized RAG over Private Documents
**Authors**: Michael J. Ryan, Danmei Xu, Chris Nivera, Daniel Campos

**Updated**: 2025-05-01T03:07:30Z

**Summary**: Retrieval Augmented Generation (RAG) has become one of the most popular methods for bringing knowledge-intensive context to large language models (LLM) because of its ability to bring local context at inference time without the cost or data leakage risks associated with fine-tuning. A clear separation of private information from the LLM training has made RAG the basis for many enterprise LLM workloads as it allows the company to augment LLM's understanding using customers' private documents. Despite its popularity for private documents in enterprise deployments, current RAG benchmarks for validating and optimizing RAG pipelines draw their corpora from public data such as Wikipedia or generic web pages and offer little to no personal context. Seeking to empower more personal and private RAG we release the EnronQA benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs across 150 different user inboxes. EnronQA enables better benchmarking of RAG pipelines over private data and allows for experimentation on the introduction of personalized retrieval settings over realistic data. Finally, we use EnronQA to explore the tradeoff in memorization and retrieval when reasoning over private documents.

**Link**: [arxiv](http://arxiv.org/abs/2505.00263v1),  [pdf](http://arxiv.org/pdf/2505.00263v1)

**Tags**: cs.IR cs.CL 



### ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid   Reasoning Model
**Authors**: Haiyan Qin, Zhiwei Xie, Jingjing Li, Liangchen Li, Xiaotong Feng, Junzhan Liu, Wang Kang

**Updated**: 2025-05-01T03:06:06Z

**Summary**: Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75\% while preserving performance. Experimental results demonstrate ReasoningV's effectiveness with a pass@1 accuracy of 57.8\% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5\%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at https://github.com/BUAA-CLab/ReasoningV.

**Link**: [arxiv](http://arxiv.org/abs/2504.14560v3),  [pdf](http://arxiv.org/pdf/2504.14560v3)

**Tags**: cs.AR cs.AI 



### Meta-rater: A Multi-dimensional Data Selection Method for Pre-training   Language Models
**Authors**: Xinlin Zhuang, Jiahui Peng, Ren Ma, Yinfan Wang, Tianyi Bai, Xingjian Wei, Jiantao Qiu, Chi Zhang, Ying Qian, Conghui He

**Updated**: 2025-05-01T02:37:14Z

**Summary**: The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose PRRC to evaluate data quality across Professionalism, Readability, Reasoning, and Cleanliness. We further introduce Meta-rater, a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with scalable benefits observed in 3.3B models trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B dataset, labeled across 25 quality metrics (including PRRC), to advance research in data-centric LLM development. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability.

**Link**: [arxiv](http://arxiv.org/abs/2504.14194v2),  [pdf](http://arxiv.org/pdf/2504.14194v2)

**Tags**: cs.CL 



### From Effectiveness to Efficiency: Uncovering Linguistic Bias in Large   Language Model-based Code Generation
**Authors**: Weipeng Jiang, Xuanqi Gao, Juan Zhai, Shiqing Ma, Xiaoyu Zhang, Ziyan Lei, Chao Shen

**Updated**: 2025-05-01T02:34:03Z

**Summary**: Large Language Models (LLMs) have demonstrated promising capabilities for code generation. While existing benchmarks evaluate the correctness and efficiency of LLM-generated code, the potential linguistic bias - where code quality varies based on the natural language used to describe programming tasks - remains underexplored. In this paper, we aim to investigate this linguistic bias through the lens of English and Chinese. To facilitate our investigation, we present a unified evaluation framework comprising a curated dataset of 52 Python programming questions with parallel bilingual task descriptions, automated correctness verification, and efficiency quantification tools based on runtime complexity estimation. Based on this framework, we conduct the first empirical study towards the linguistic bias in LLM-generated code on eight popular LCGMs, as well as GPT-3.5-Turbo and GPT-4. We observe that these LCGM-generated code show different correctness on an average of 12% bilingual programming tasks, where 39% also exhibits diverse efficiency. Our findings indicate that LLMs commonly exhibit linguistic bias for code generation.

**Link**: [arxiv](http://arxiv.org/abs/2406.00602v2),  [pdf](http://arxiv.org/pdf/2406.00602v2)

**Tags**: cs.SE cs.PL 



### BRIDGE: Benchmarking Large Language Models for Understanding Real-world   Clinical Practice Text
**Authors**: Jiageng Wu, Bowen Gu, Ren Zhou, Kevin Xie, Doug Snyder, Yixing Jiang, Valentina Carducci, Richard Wyss, Rishi J Desai, Emily Alsentzer, Leo Anthony Celi, Adam Rodman, Sebastian Schneeweiss, Jonathan H. Chen, Santiago Romero-Brufau, Kueiyu Joshua Lin, Jie Yang

**Updated**: 2025-05-01T02:21:09Z

**Summary**: Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding.   The BRIDGE leaderboard: https://huggingface.co/spaces/YLab-Open/BRIDGE-Medical-Leaderboard

**Link**: [arxiv](http://arxiv.org/abs/2504.19467v2),  [pdf](http://arxiv.org/pdf/2504.19467v2)

**Tags**: cs.CL cs.AI 



### QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM   Serving
**Authors**: Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han

**Updated**: 2025-05-01T02:14:05Z

**Summary**: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.

**Link**: [arxiv](http://arxiv.org/abs/2405.04532v3),  [pdf](http://arxiv.org/pdf/2405.04532v3)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems
**Authors**: Yazan Otoum, Arghavan Asad, Amiya Nayak

**Updated**: 2025-05-01T01:18:54Z

**Summary**: The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00240v1),  [pdf](http://arxiv.org/pdf/2505.00240v1)

**Tags**: cs.CR cs.AI cs.ET cs.LG 



### "Reasoning" with Rhetoric: On the Style-Evidence Tradeoff in   LLM-Generated Counter-Arguments
**Authors**: Preetika Verma, Kokil Jaidka, Svetlana Churina

**Updated**: 2025-05-01T01:00:25Z

**Summary**: Large language models (LLMs) play a key role in generating evidence-based and stylistic counter-arguments, yet their effectiveness in real-world applications has been underexplored. Previous research often neglects the balance between evidentiality and style, which are crucial for persuasive arguments. To address this, we evaluated the effectiveness of stylized evidence-based counter-argument generation in Counterfire, a new dataset of 38,000 counter-arguments generated by revising counter-arguments to Reddit's ChangeMyView community to follow different discursive styles. We evaluated generic and stylized counter-arguments from basic and fine-tuned models such as GPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku, LLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings reveal that humans prefer stylized counter-arguments over the original outputs, with GPT-3.5 Turbo performing well, though still not reaching human standards of rhetorical quality nor persuasiveness. Additionally, our work created a novel argument triplets dataset for studying style control, with human preference labels that provide insights into the tradeoffs between evidence integration and argument quality.

**Link**: [arxiv](http://arxiv.org/abs/2402.08498v5),  [pdf](http://arxiv.org/pdf/2402.08498v5)

**Tags**: cs.CL 



### Self-Generated In-Context Examples Improve LLM Agents for Sequential   Decision-Making Tasks
**Authors**: Vishnu Sarukkai, Zhiqiang Xie, Kayvon Fatahalian

**Updated**: 2025-05-01T00:48:12Z

**Summary**: Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.

**Link**: [arxiv](http://arxiv.org/abs/2505.00234v1),  [pdf](http://arxiv.org/pdf/2505.00234v1)

**Tags**: cs.LG cs.CL 



### Scaling On-Device GPU Inference for Large Generative Models
**Authors**: Jiuqiang Tang, Raman Sarokin, Ekaterina Ignasheva, Grant Jensen, Lin Chen, Juhyun Lee, Andrei Kulik, Matthias Grundmann

**Updated**: 2025-05-01T00:44:13Z

**Summary**: Driven by the advancements in generative AI, large machine learning models have revolutionized domains such as image processing, audio synthesis, and speech recognition. While server-based deployments remain the locus of peak performance, the imperative for on-device inference, necessitated by privacy and efficiency considerations, persists. Recognizing GPUs as the on-device ML accelerator with the widest reach, we present ML Drift--an optimized framework that extends the capabilities of state-of-the-art GPU-accelerated inference engines. ML Drift enables on-device execution of generative AI workloads which contain 10 to 100x more parameters than existing on-device generative AI models. ML Drift addresses intricate engineering challenges associated with cross-GPU API development, and ensures broad compatibility across mobile and desktop/laptop platforms, thereby facilitating the deployment of significantly more complex models on resource-constrained devices. Our GPU-accelerated ML/AI inference engine achieves an order-of-magnitude performance improvement relative to existing open-source GPU inference engines.

**Link**: [arxiv](http://arxiv.org/abs/2505.00232v1),  [pdf](http://arxiv.org/pdf/2505.00232v1)

**Tags**: cs.LG cs.AI 



### Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?
**Authors**: Hye Sun Yun, Karen Y. C. Zhang, Ramez Kouzy, Iain J. Marshall, Junyi Jessy Li, Byron C. Wallace

**Updated**: 2025-05-01T00:09:30Z

**Summary**: Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present "positive" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs.

**Link**: [arxiv](http://arxiv.org/abs/2502.07963v2),  [pdf](http://arxiv.org/pdf/2502.07963v2)

**Tags**: cs.CL cs.AI 



### Which Agent Causes Task Failures and When? On Automated Failure   Attribution of LLM Multi-Agent Systems
**Authors**: Shaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu, Zhiguang Han, Jingyang Zhang, Beibin Li, Chi Wang, Huazheng Wang, Yiran Chen, Qingyun Wu

**Updated**: 2025-04-30T23:09:44Z

**Summary**: Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution

**Link**: [arxiv](http://arxiv.org/abs/2505.00212v1),  [pdf](http://arxiv.org/pdf/2505.00212v1)

**Tags**: cs.MA cs.CL 



### LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem
**Authors**: Hongyi Liu, Shaochen Zhong, Xintong Sun, Minghao Tian, Mohsen Hariri, Zirui Liu, Ruixiang Tang, Zhimeng Jiang, Jiayi Yuan, Yu-Neng Chuang, Li Li, Soo-Hyun Choi, Rui Chen, Vipin Chaudhary, Xia Hu

**Updated**: 2025-04-30T22:51:42Z

**Summary**: Finetuning LLMs with LoRA has gained significant popularity due to its simplicity and effectiveness. Often, users may even find pluggable, community-shared LoRAs to enhance their base models for a specific downstream task of interest; enjoying a powerful, efficient, yet customized LLM experience with negligible investment. However, this convenient share-and-play ecosystem also introduces a new attack surface, where attackers can distribute malicious LoRAs to a community eager to try out shared assets. Despite the high-risk potential, no prior art has comprehensively explored LoRA's attack surface under the downstream-enhancing share-and-play context. In this paper, we investigate how backdoors can be injected into task-enhancing LoRAs and examine the mechanisms of such infections. We find that with a simple, efficient, yet specific recipe, a backdoor LoRA can be trained once and then seamlessly merged (in a training-free fashion) with multiple task-enhancing LoRAs, retaining both its malicious backdoor and benign downstream capabilities. This allows attackers to scale the distribution of compromised LoRAs with minimal effort by leveraging the rich pool of existing shared LoRA assets. We note that such merged LoRAs are particularly infectious -- because their malicious intent is cleverly concealed behind improved downstream capabilities, creating a strong incentive for voluntary download -- and dangerous -- because under local deployment, no safety measures exist to intervene when things go wrong. Our work is among the first to study this new threat model of training-free distribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting the urgent need for heightened security awareness in the LoRA ecosystem. Warning: This paper contains offensive content and involves a real-life tragedy.

**Link**: [arxiv](http://arxiv.org/abs/2403.00108v2),  [pdf](http://arxiv.org/pdf/2403.00108v2)

**Tags**: cs.CR cs.AI cs.CL 



### SoK: Security and Privacy Risks of Healthcare AI
**Authors**: Yuanhaur Chang, Han Liu, Chenyang Lu, Ning Zhang

**Updated**: 2025-04-30T22:27:30Z

**Summary**: The integration of artificial intelligence (AI) and machine learning (ML) into healthcare systems holds great promise for enhancing patient care and care delivery efficiency; however, it also exposes sensitive data and system integrity to potential cyberattacks. Current security and privacy (S&P) research on healthcare AI is highly unbalanced in terms of healthcare deployment scenarios and threat models, and has a disconnected focus with the biomedical research community. This hinders a comprehensive understanding of the risks that healthcare AI entails. To address this gap, this paper takes a thorough examination of existing healthcare AI S&P research, providing a unified framework that allows the identification of under-explored areas. Our survey presents a systematic overview of healthcare AI attacks and defenses, and points out challenges and research opportunities for each AI-driven healthcare application domain. Through our experimental analysis of different threat models and feasibility studies on under-explored adversarial attacks, we provide compelling insights into the pressing need for cybersecurity research in the rapidly evolving field of healthcare AI.

**Link**: [arxiv](http://arxiv.org/abs/2409.07415v2),  [pdf](http://arxiv.org/pdf/2409.07415v2)

**Tags**: cs.CR cs.AI cs.LG 



### Process-Supervised LLM Recommenders via Flow-guided Tuning
**Authors**: Chongming Gao, Mengyao Gao, Chenxiao Fan, Shuai Yuan, Wentao Shi, Xiangnan He

**Updated**: 2025-04-30T22:26:32Z

**Summary**: While large language models (LLMs) are increasingly adapted for recommendation systems via supervised fine-tuning (SFT), this approach amplifies popularity bias due to its likelihood maximization objective, compromising recommendation diversity and fairness. To address this, we present Flow-guided fine-tuning recommender (Flower), which replaces SFT with a Generative Flow Network (GFlowNet) framework that enacts process supervision through token-level reward propagation. Flower's key innovation lies in decomposing item-level rewards into constituent token rewards, enabling direct alignment between token generation probabilities and their reward signals. This mechanism achieves three critical advancements: (1) popularity bias mitigation and fairness enhancement through empirical distribution matching, (2) preservation of diversity through GFlowNet's proportional sampling, and (3) flexible integration of personalized preferences via adaptable token rewards. Experiments demonstrate Flower's superior distribution-fitting capability and its significant advantages over traditional SFT in terms of accuracy, fairness, and diversity, highlighting its potential to improve LLM-based recommendation systems. The implementation is available via https://github.com/MrPeach0301/Flower

**Link**: [arxiv](http://arxiv.org/abs/2503.07377v2),  [pdf](http://arxiv.org/pdf/2503.07377v2)

**Tags**: cs.IR 



### Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the   effect of Epistemic Markers on LLM-based Evaluation
**Authors**: Dongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk Park, Kyomin Jung

**Updated**: 2025-04-30T22:19:35Z

**Summary**: In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using EMBER, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the presence of epistemic markers. Specifically, we observe a negative bias toward epistemic markers, with a stronger bias against markers expressing uncertainty. This suggests that LLM-judges are influenced by the presence of these markers and do not focus solely on the correctness of the content.

**Link**: [arxiv](http://arxiv.org/abs/2410.20774v2),  [pdf](http://arxiv.org/pdf/2410.20774v2)

**Tags**: cs.CL 



### A Domain-Agnostic Scalable AI Safety Ensuring Framework
**Authors**: Beomjun Kim, Kangyeon Kim, Sunwoo Kim, Heejin Ahn

**Updated**: 2025-04-30T22:06:09Z

**Summary**: Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts. We propose a novel AI safety framework that ensures AI systems comply with any user-defined constraint, with any desired probability, and across various domains. In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose internal test data, a supplementary set of safety-labeled data, and a conservative testing methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training. We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms for up to several order of magnitudes existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data.

**Link**: [arxiv](http://arxiv.org/abs/2504.20924v2),  [pdf](http://arxiv.org/pdf/2504.20924v2)

**Tags**: cs.AI 



### RAIL in the Wild: Operationalizing Responsible AI Evaluation Using   Anthropic's Value Dataset
**Authors**: Sumit Verma, Pritam Prasun, Arpit Jaiswal, Pritish Kumar

**Updated**: 2025-04-30T22:03:26Z

**Summary**: As AI systems become embedded in real-world applications, ensuring they meet ethical standards is crucial. While existing AI ethics frameworks emphasize fairness, transparency, and accountability, they often lack actionable evaluation methods. This paper introduces a systematic approach using the Responsible AI Labs (RAIL) framework, which includes eight measurable dimensions to assess the normative behavior of large language models (LLMs). We apply this framework to Anthropic's "Values in the Wild" dataset, containing over 308,000 anonymized conversations with Claude and more than 3,000 annotated value expressions. Our study maps these values to RAIL dimensions, computes synthetic scores, and provides insights into the ethical behavior of LLMs in real-world use.

**Link**: [arxiv](http://arxiv.org/abs/2505.00204v1),  [pdf](http://arxiv.org/pdf/2505.00204v1)

**Tags**: cs.AI 



### GenTorrent: Scaling Large Language Model Serving with An Overley Network
**Authors**: Fei Fang, Yifan Hua, Shengze Wang, Ruilin Zhou, Yi Liu, Chen Qian, Xiaoxue Zhang

**Updated**: 2025-04-30T21:24:19Z

**Summary**: While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2504.20101v2),  [pdf](http://arxiv.org/pdf/2504.20101v2)

**Tags**: cs.DC cs.AI 



### Real-World Gaps in AI Governance Research
**Authors**: Ilan Strauss, Isobel Moure, Tim O'Reilly, Sruly Rosenblat

**Updated**: 2025-04-30T20:44:42Z

**Summary**: Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.

**Link**: [arxiv](http://arxiv.org/abs/2505.00174v1),  [pdf](http://arxiv.org/pdf/2505.00174v1)

**Tags**: cs.AI 



### Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge   Distillation Through Question Decomposition
**Authors**: Tyler McDonald, Ali Emami

**Updated**: 2025-04-30T20:44:09Z

**Summary**: Knowledge distillation allows smaller neural networks to emulate the performance of larger, teacher models with reduced computational demands. Traditional methods for Large Language Models (LLMs) often necessitate extensive fine-tuning, which limits their accessibility. To address this, we introduce Trace-of-Thought Prompting, a novel framework designed to distill critical reasoning capabilities from high-resource teacher models (over 8 billion parameters) to low-resource student models (up to 8 billion parameters). This approach leverages problem decomposition to enhance interpretability and facilitate human-in-the-loop interventions. Empirical evaluations on the GSM8K and MATH datasets show that student models achieve accuracy gains of up to 113% on GSM8K and 21% on MATH, with significant improvements particularly notable in smaller models like Llama 2 and Zephyr. Our results suggest a promising pathway for open-source, low-resource models to eventually serve both as both students and teachers, potentially reducing our reliance on high-resource, proprietary models.

**Link**: [arxiv](http://arxiv.org/abs/2504.20946v2),  [pdf](http://arxiv.org/pdf/2504.20946v2)

**Tags**: cs.CL cs.AI 



### Deep Reinforcement Learning Policies for Underactuated Satellite   Attitude Control
**Authors**: Matteo El Hariry, Andrea Cini, Giacomo Mellone, Alessandro Balossino

**Updated**: 2025-04-30T20:24:49Z

**Summary**: Autonomy is a key challenge for future space exploration endeavours. Deep Reinforcement Learning holds the promises for developing agents able to learn complex behaviours simply by interacting with their environment. This paper investigates the use of Reinforcement Learning for the satellite attitude control problem, namely the angular reorientation of a spacecraft with respect to an in- ertial frame of reference. In the proposed approach, a set of control policies are implemented as neural networks trained with a custom version of the Proximal Policy Optimization algorithm to maneuver a small satellite from a random starting angle to a given pointing target. In particular, we address the problem for two working conditions: the nominal case, in which all the actuators (a set of 3 reac- tion wheels) are working properly, and the underactuated case, where an actuator failure is simulated randomly along with one of the axes. We show that the agents learn to effectively perform large-angle slew maneuvers with fast convergence and industry-standard pointing accuracy. Furthermore, we test the proposed method on representative hardware, showing that by taking adequate measures controllers trained in simulation can perform well in real systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00165v1),  [pdf](http://arxiv.org/pdf/2505.00165v1)

**Tags**: cs.RO 



### V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving
**Authors**: Jannik Lübberstedt, Esteban Rivera, Nico Uhlemann, Markus Lienkamp

**Updated**: 2025-04-30T20:00:37Z

**Summary**: Large Vision Language Models (LVLMs) have shown strong capabilities in understanding and analyzing visual scenes across various domains. However, in the context of autonomous driving, their limited comprehension of 3D environments restricts their effectiveness in achieving a complete and safe understanding of dynamic surroundings. To address this, we introduce V3LMA, a novel approach that enhances 3D scene understanding by integrating Large Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions generated from object detections and video inputs, significantly boosting performance without requiring fine-tuning. Through a dedicated preprocessing pipeline that extracts 3D object data, our method improves situational awareness and decision-making in complex traffic scenarios, achieving a score of 0.56 on the LingoQA benchmark. We further explore different fusion strategies and token combinations with the goal of advancing the interpretation of traffic scenes, ultimately enabling safer autonomous driving systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.00156v1),  [pdf](http://arxiv.org/pdf/2505.00156v1)

**Tags**: cs.CV 



### Leveraging LLMs for Influence Path Planning in Proactive Recommendation
**Authors**: Mingze Wang, Shuxian Bi, Wenjie Wang, Chongming Gao, Yangyang Li, Fuli Feng

**Updated**: 2025-04-30T19:55:56Z

**Summary**: Recommender systems are pivotal in Internet social platforms, yet they often cater to users' historical interests, leading to critical issues like echo chambers. To broaden user horizons, proactive recommender systems aim to guide user interest to gradually like a target item beyond historical interests through an influence path,i.e., a sequence of recommended items. As a representative, Influential Recommender System (IRS) designs a sequential model for influence path planning but faces issues of lacking target item inclusion and path coherence. To address the issues, we leverage the advanced planning capabilities of Large Language Models (LLMs) and propose an LLM-based Influence Path Planning (LLM-IPP) method. LLM-IPP generates coherent and effective influence paths by capturing user interest shifts and item characteristics. We introduce novel evaluation metrics and user simulators to benchmark LLM-IPP against traditional methods. Our experiments demonstrate that LLM-IPP significantly enhances user acceptability and path coherence, outperforming existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.04827v2),  [pdf](http://arxiv.org/pdf/2409.04827v2)

**Tags**: cs.IR 



### Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired   Individuals
**Authors**: Bhanuja Ainary

**Updated**: 2025-04-30T19:55:19Z

**Summary**: Visually impaired people face significant challenges when attempting to interact with and understand complex environments, and traditional assistive technologies often struggle to quickly provide necessary contextual understanding and interactive intelligence. This thesis presents Audo-Sight, a state-of-the-art assistive system that seamlessly integrates Multimodal Large Language Models (MLLMs) to provide expedient, context-aware interactions for Blind and Visually Impaired (BVI) individuals. The system operates in two different modalities: personalized interaction through user identification and public access in common spaces like museums and shopping malls. In tailored environments, the system adjusts its output to conform to the preferences of individual users, thus enhancing accessibility through a user-aware form of interaction. In shared environments, Audo-Sight employs a shared architecture that adapts to its current user with no manual reconfiguration required. To facilitate appropriate interactions with the LLM, the public Audo-Sight solution includes an Age-Range Determiner and Safe Query Filter. Additionally, the system ensures that responses are respectful to BVI users through NeMo Guardrails. By utilizing multimodal reasoning, BVI-cognizant response editing, and safeguarding features, this work represents a major leap in AI-driven accessibility technology capable of increasing autonomy, safety, and interaction for people with visual impairments in social settings. Finally, we present the integration of Audo-Sight and SmartSight, which enables enhanced situational awareness for BVI individuals. This integration takes advantage of the real-time visual analysis of SmartSight, combined with the extensive reasoning and interactive capabilities of Audo-Sight, and goes beyond object identification to provide context-driven, voice-controlled assistance in dynamic environments.

**Link**: [arxiv](http://arxiv.org/abs/2505.00153v1),  [pdf](http://arxiv.org/pdf/2505.00153v1)

**Tags**: cs.HC cs.DC 



### AdaptMI: Adaptive Skill-based In-context Math Instruction for Small   Language Models
**Authors**: Yinghui He, Abhishek Panigrahi, Yong Lin, Sanjeev Arora

**Updated**: 2025-04-30T19:35:46Z

**Summary**: In-context learning (ICL) allows a language model to improve its problem-solving capability when provided with suitable information in context. Since the choice of in-context information can be determined based on the problem itself, in-context learning is analogous to human learning from teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that ICL performance can be improved by leveraging a frontier large language model's (LLM) ability to predict required skills to solve a problem, popularly referred to as an LLM's metacognition, and using the recommended skills to construct necessary in-context examples. While this skill-based strategy boosts ICL performance in larger models, its gains on small language models (SLMs) have been minimal, highlighting a performance gap in ICL capabilities. We investigate this gap and show that skill-based prompting can hurt SLM performance on easy questions by introducing unnecessary information, akin to cognitive overload. To address this, we introduce AdaptMI, an adaptive approach to selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory from human pedagogy, our method only introduces skill-based examples when the model performs poorly. We further propose AdaptMI+, which adds examples targeted to the specific skills missing from the model's responses. On 5-shot evaluations across popular math benchmarks and five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies.

**Link**: [arxiv](http://arxiv.org/abs/2505.00147v1),  [pdf](http://arxiv.org/pdf/2505.00147v1)

**Tags**: cs.CL 



### Efficient Reinforcement Finetuning via Adaptive Curriculum Learning
**Authors**: Taiwei Shi, Yiyang Wu, Linxin Song, Tianyi Zhou, Jieyu Zhao

**Updated**: 2025-04-30T19:01:00Z

**Summary**: Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.

**Link**: [arxiv](http://arxiv.org/abs/2504.05520v2),  [pdf](http://arxiv.org/pdf/2504.05520v2)

**Tags**: cs.LG cs.CL 



### Between Underthinking and Overthinking: An Empirical Study of Reasoning   Length and correctness in LLMs
**Authors**: Jinyan Su, Jennifer Healey, Preslav Nakov, Claire Cardie

**Updated**: 2025-04-30T18:48:06Z

**Summary**: Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.

**Link**: [arxiv](http://arxiv.org/abs/2505.00127v1),  [pdf](http://arxiv.org/pdf/2505.00127v1)

**Tags**: cs.CL cs.AI 



### Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of   Lebanese
**Authors**: Silvana Yakhni, Ali Chehab

**Updated**: 2025-04-30T18:33:53Z

**Summary**: This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the "More Data is Better" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.

**Link**: [arxiv](http://arxiv.org/abs/2505.00114v1),  [pdf](http://arxiv.org/pdf/2505.00114v1)

**Tags**: cs.CL cs.AI 



### Large Language Model Agent as a Mechanical Designer
**Authors**: Yayati Jadhav, Amir Barati Farimani

**Updated**: 2025-04-30T18:23:36Z

**Summary**: Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.

**Link**: [arxiv](http://arxiv.org/abs/2404.17525v3),  [pdf](http://arxiv.org/pdf/2404.17525v3)

**Tags**: cs.LG cs.AI cs.CL 



### CoordField: Coordination Field for Agentic UAV Task Allocation In   Low-altitude Urban Scenarios
**Authors**: Tengchao Zhang, Yonglin Tian, Fei Lin, Jun Huang, Rui Qin, Fei-Yue Wang

**Updated**: 2025-04-30T18:02:45Z

**Summary**: With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing approaches, this paper proposes coordination field agentic system for coordinating heterogeneous UAV swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.

**Link**: [arxiv](http://arxiv.org/abs/2505.00091v1),  [pdf](http://arxiv.org/pdf/2505.00091v1)

**Tags**: cs.RO cs.AI 



### Can We Trust Embodied Agents? Exploring Backdoor Attacks against   Embodied LLM-based Decision-Making Systems
**Authors**: Ruochen Jiao, Shaoyuan Xie, Justin Yue, Takami Sato, Lixu Wang, Yixuan Wang, Qi Alfred Chen, Qi Zhu

**Updated**: 2025-04-30T17:59:57Z

**Summary**: Large Language Models (LLMs) have shown significant promise in real-world decision-making tasks for embodied artificial intelligence, especially when fine-tuned to leverage their inherent common sense and reasoning abilities while being tailored to specific applications. However, this fine-tuning process introduces considerable safety and security vulnerabilities, especially in safety-critical cyber-physical systems. In this work, we propose the first comprehensive framework for Backdoor Attacks against LLM-based Decision-making systems (BALD) in embodied AI, systematically exploring the attack surfaces and trigger mechanisms. Specifically, we propose three distinct attack mechanisms: word injection, scenario manipulation, and knowledge injection, targeting various components in the LLM-based decision-making pipeline. We perform extensive experiments on representative LLMs (GPT-3.5, LLaMA2, PaLM2) in autonomous driving and home robot tasks, demonstrating the effectiveness and stealthiness of our backdoor triggers across various attack channels, with cases like vehicles accelerating toward obstacles and robots placing knives on beds. Our word and knowledge injection attacks achieve nearly 100% success rate across multiple models and datasets while requiring only limited access to the system. Our scenario manipulation attack yields success rates exceeding 65%, reaching up to 90%, and does not require any runtime system intrusion. We also assess the robustness of these attacks against defenses, revealing their resilience. Our findings highlight critical security vulnerabilities in embodied LLM systems and emphasize the urgent need for safeguarding these systems to mitigate potential risks.

**Link**: [arxiv](http://arxiv.org/abs/2405.20774v3),  [pdf](http://arxiv.org/pdf/2405.20774v3)

**Tags**: cs.CR cs.AI 



### TRUST: An LLM-Based Dialogue System for Trauma Understanding and   Structured Assessments
**Authors**: Sichang Tu, Abigail Powers, Stephen Doogan, Jinho D. Choi

**Updated**: 2025-04-30T17:58:06Z

**Summary**: Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.

**Link**: [arxiv](http://arxiv.org/abs/2504.21851v1),  [pdf](http://arxiv.org/pdf/2504.21851v1)

**Tags**: cs.CL cs.AI 



