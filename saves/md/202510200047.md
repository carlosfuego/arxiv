# Arxiv Results
## Keyword: kv cache 
 ### Attention Is All You Need for KV Cache in Diffusion LLMs
**Authors**: Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen

**Updated**: 2025-10-16T17:59:48Z

**Summary**: This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences, and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14973v1),  [pdf](http://arxiv.org/pdf/2510.14973v1)

**Tags**: cs.CL cs.AI cs.LG 



### A Performance Portable Matrix Free Dense MTTKRP in GenTen
**Authors**: Gabriel Kosmacher, Eric T. Phipps, Sivasankaran Rajamanickam

**Updated**: 2025-10-16T17:10:03Z

**Summary**: We extend the GenTen tensor decomposition package by introducing an accelerated dense matricized tensor times Khatri-Rao product (MTTKRP), the workhorse kernel for canonical polyadic (CP) tensor decompositions, that is portable and performant on modern CPU and GPU architectures. In contrast to the state-of-the-art matrix multiply based MTTKRP kernels used by Tensor Toolbox, TensorLy, etc., that explicitly form Khatri-Rao matrices, we develop a matrix-free element-wise parallelization approach whose memory cost grows with the rank R like the sum of the tensor shape O(R(n+m+k)), compared to matrix-based methods whose memory cost grows like the product of the tensor shape O(R(mnk)). For the largest problem we study, a rank 2000 MTTKRP, the smaller growth rate yields a matrix-free memory cost of just 2% of the matrix-based methods, a 50x improvement. In practice, the reduced memory impact means our matrix-free MTTKRP can compute a rank 2000 tensor decomposition on a single NVIDIA H100 instead of six H100s using a matrix-based MTTKRP. We also compare our optimized matrix-free MTTKRP to baseline matrix-free implementations on different devices, showing a 3x single-device speedup on an Intel 8480+ CPU and an 11x speedup on a H100 GPU. In addition to numerical results, we provide fine grained performance models for an ideal multi-level cache machine, compare analytical performance predictions to empirical results, and provide a motivated heuristic selection for selecting an algorithmic hyperparameter.

**Link**: [arxiv](http://arxiv.org/abs/2510.14891v1),  [pdf](http://arxiv.org/pdf/2510.14891v1)

**Tags**: cs.MS G.2 



### xLLM Technical Report
**Authors**: Tongxuan Liu, Tao Peng, Peijun Yang, Xiaoyang Zhao, Xiusheng Lu, Weizhe Huang, Zirui Liu, Xiaoyu Chen, Zhiwei Liang, Jun Xiong, Donghe Jin, Minchao Zhang, Jinrong Guo, Yingxu Deng, Xu Zhang, Xianzhe Dong, Siqi Wang, Siyu Wu, Yu Wu, Zihan Tang, Yuting Zeng, Yanshu Wang, Jinguang Liu, Meng Kang, Menxin Li, Yunlong Wang, Yiming Liu, Xiaolong Ma, Yifan Wang, Yichen Zhang, Jinrun Yin, Keyang Zheng, Jiawei Yin, Jun Zhang, Ziyue Wang, Xiaobo Lin, Liangyu Liu, Liwei Lan, Yang Liu, Chunhua Peng, Han Liu, Songcheng Ren, Xuezhu Wang, Yunheng Shen, Yi Wang, Guyue Liu, Hui Chen, Tong Yang, Hailong Yang, Jing Li, Guiguang Ding, Ke Zhang

**Updated**: 2025-10-16T13:53:47Z

**Summary**: We introduce xLLM, an intelligent and efficient Large Language Model (LLM) inference framework designed for high-performance, large-scale enterprise-grade serving, with deep optimizations for diverse AI accelerators. To address these challenges, xLLM builds a novel decoupled service-engine architecture. At the service layer, xLLM-Service features an intelligent scheduling module that efficiently processes multimodal requests and co-locates online and offline tasks through unified elastic scheduling to maximize cluster utilization. This module also relies on a workload-adaptive dynamic Prefill-Decode (PD) disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation policy designed for multimodal inputs. Furthermore, it incorporates a distributed architecture to provide global KV Cache management and robust fault-tolerant capabilities for high availability. At the engine layer, xLLM-Engine co-optimizes system and algorithm designs to fully saturate computing resources. This is achieved through comprehensive multi-layer execution pipeline optimizations, an adaptive graph mode and an xTensor memory management. xLLM-Engine also further integrates algorithmic enhancements such as optimized speculative decoding and dynamic EPLB, collectively serving to substantially boost throughput and inference efficiency. Extensive evaluations demonstrate that xLLM delivers significantly superior performance and resource efficiency. Under identical TPOT constraints, xLLM achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining an average throughput of 1.7x that of MindIE with Deepseek-series models. xLLM framework is publicly available at https://github.com/jd-opensource/xllm and https://github.com/jd-opensource/xllm-service.

**Link**: [arxiv](http://arxiv.org/abs/2510.14686v1),  [pdf](http://arxiv.org/pdf/2510.14686v1)

**Tags**: cs.DC cs.AI 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2025-10-16T13:25:38Z

**Summary**: Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods. Our code is available at https://github.com/FFY0/AdaKV.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v5),  [pdf](http://arxiv.org/pdf/2407.11550v5)

**Tags**: cs.CL cs.AI 



### MPI-over-CXL: Enhancing Communication Efficiency in Distributed HPC   Systems
**Authors**: Miryeong Kwon, Donghyun Gouk, Hyein Woo, Junhee Kim, Jinwoo Baek, Kyungkuk Nam, Sangyoon Ji, Jiseon Kim, Hanyeoreum Bae, Junhyeok Jang, Hyunwoo You, Junseok Moon, Myoungsoo Jung

**Updated**: 2025-10-16T12:32:51Z

**Summary**: MPI implementations commonly rely on explicit memory-copy operations, incurring overhead from redundant data movement and buffer management. This overhead notably impacts HPC workloads involving intensive inter-processor communication. In response, we introduce MPI-over-CXL, a novel MPI communication paradigm leveraging CXL, which provides cache-coherent shared memory across multiple hosts. MPI-over-CXL replaces traditional data-copy methods with direct shared memory access, significantly reducing communication latency and memory bandwidth usage. By mapping shared memory regions directly into the virtual address spaces of MPI processes, our design enables efficient pointer-based communication, eliminating redundant copying operations. To validate this approach, we implement a comprehensive hardware and software environment, including a custom CXL 3.2 controller, FPGA-based multi-host emulation, and dedicated software stack. Our evaluations using representative benchmarks demonstrate substantial performance improvements over conventional MPI systems, underscoring MPI-over-CXL's potential to enhance efficiency and scalability in large-scale HPC environments.

**Link**: [arxiv](http://arxiv.org/abs/2510.14622v1),  [pdf](http://arxiv.org/pdf/2510.14622v1)

**Tags**: cs.DC 



### Design and simulation of a 4H-SiC low gain avalanche diode with   trench-isolation
**Authors**: Sebastian Onder, Philipp Gaggl, Jürgen Burin, Andreas Gsponer, Matthias Knopf, Simon Waid, Neil Moffat, Giulio Pellegrini, Thomas Bergauer

**Updated**: 2025-10-16T10:21:35Z

**Summary**: We present the design and simulation of a 30 $\mathrm{\mu m}$ thick 4H-SiC Low Gain Avalanche Diode (LGAD) optimized for high-voltage operation. A 2.4 $\mathrm{\mu m}$ thick epitaxially grown gain layer enables controlled internal amplification up to 1 kV reverse bias, while maintaining full depletion below 500 V. Electrical characteristics, including I-V, C-V, and gain behavior, were simulated in Synopsys Sentaurus Technology Computer-Aided Design (TCAD) using a quasi-1D geometry and verified across process-related variations in gain layer parameters. To ensure high-voltage stability and proper edge termination, a guard structure combining deep etched trenches and deep $p^+$ junction termination extension (JTE) implants was designed. TCAD simulations varying the guard structure dimensions yielded an optimized design with a breakdown voltage above 2.4 kV. A corresponding wafer run is currently processed at IMB-CNM, Barcelona.

**Link**: [arxiv](http://arxiv.org/abs/2510.14531v1),  [pdf](http://arxiv.org/pdf/2510.14531v1)

**Tags**: physics.ins-det 



### Cortex: Workflow-Aware Resource Pooling and Scheduling for Agentic   Serving
**Authors**: Nikos Pagonas, Yeounoh Chung, Kostis Kaffes, Arvind Krishnamurthy

**Updated**: 2025-10-15T21:49:38Z

**Summary**: We introduce Cortex, a prototype workflow-aware serving platform designed for agentic workloads. The core principle of Cortex is stage isolation: it provisions dedicated resource pools for each distinct stage of an agentic workflow. This simple yet powerful strategy mitigates inter-stage interference in compute and memory, leading to better KV cache utilization, higher throughput, and more predictable performance. By customizing resource allocation and scheduling within each distinct stage of agentic workflows, Cortex lays the groundwork for more advanced, agent-native serving paradigms, including malleable resource management, speculative execution of workflow branches, and a shared, multi-tiered cache for "agentic state."

**Link**: [arxiv](http://arxiv.org/abs/2510.14126v1),  [pdf](http://arxiv.org/pdf/2510.14126v1)

**Tags**: cs.DC 



### Less is More: Improving LLM Reasoning with Minimal Test-Time   Intervention
**Authors**: Zhen Yang, Mingyang Zhang, Feng Chen, Ganggui Ding, Liang Hou, Xin Tao, Pengfei Wan, Ying-Cong Chen

**Updated**: 2025-10-15T17:59:45Z

**Summary**: Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining highly efficient.

**Link**: [arxiv](http://arxiv.org/abs/2510.13940v1),  [pdf](http://arxiv.org/pdf/2510.13940v1)

**Tags**: cs.CL cs.AI 



### Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression   Beacons
**Authors**: Giovanni Monea, Yair Feldman, Shankar Padmanabhan, Kianté Brantley, Yoav Artzi

**Updated**: 2025-10-15T17:57:21Z

**Summary**: The scalability of large language models for long-context reasoning is severely constrained by the linear growth of their Transformer key-value cache, which incurs significant memory and computational costs. We posit that as a model generates reasoning tokens, the informational value of past generated tokens diminishes, creating an opportunity for compression. In this work, we propose to periodically compress the generation KV cache with a learned, special-purpose token and evict compressed entries. We train the model to perform this compression via a modified joint distillation and reinforcement learning (RL) framework. Our training method minimizes overhead over the conventional RL process, as it leverages RL outputs for distillation. Empirically, our method achieves a superior memory-accuracy Pareto frontier compared to both the model without cache compression and training-free compression techniques.

**Link**: [arxiv](http://arxiv.org/abs/2510.13797v1),  [pdf](http://arxiv.org/pdf/2510.13797v1)

**Tags**: cs.CL 



### Quantize What Counts: More for Keys, Less for Values
**Authors**: Mohsen Hariri, Alan Luo, Weicong Chen, Shaochen Zhong, Tianyi Zhang, Qifan Wang, Xia Hu, Xiaotian Han, Vipin Chaudhary

**Updated**: 2025-10-15T16:03:13Z

**Summary**: Large Language Models (LLMs) suffer inference-time memory bottlenecks dominated by the attention Key-Value (KV) cache, which scales with model size and context length. While KV-cache quantization alleviates this cost, bit allocation between keys and values is often tuned heuristically, lacking theoretical grounding and generalizability. This paper proposes two theorems that anchor mixed-precision KV quantization in the intrinsic geometry of Transformer models. First, key projections systematically have larger spectral and Frobenius norms than value matrices, implying higher information density along the key path. Second, for any given memory budget, prioritizing precision for keys over values strictly reduces quantization error and better preserves accuracy. Empirical evaluations across various prominent LLMs and benchmarks show that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to 98.3\% accuracy compared to uniform allocations (e.g., 4-bit for both), while conserving memory. These results transform bit allocation from ad hoc tuning into a theoretically grounded, geometry-driven design principle for efficient LLM inference. Source code is available at https://github.com/mohsenhariri/spectral-kv.

**Link**: [arxiv](http://arxiv.org/abs/2502.15075v3),  [pdf](http://arxiv.org/pdf/2502.15075v3)

**Tags**: cs.LG 



### NOSA: Native and Offloadable Sparse Attention
**Authors**: Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu

**Updated**: 2025-10-15T14:33:16Z

**Summary**: Trainable sparse attention has emerged as a promising solution to address the decoding efficiency bottleneck of LLMs in long-context processing, significantly saving memory accesses while minimally impacting task performance. However, existing sparse attention methods leave a crucial limitation unresolved: the size of the key-value (KV) cache remains unreduced, which constrains on-GPU batch sizes and throttles decoding throughput, especially in large-scale batched inference. In this paper, we show that trainable sparse attention naturally exhibits strong locality in token selection across adjacent decoding steps, thereby enabling KV cache offloading without altering the underlying attention computation. However, the inherent locality remains insufficient to achieve efficient offloading, as the transfer of selected KV pairs between the CPU and GPU continues to dominate the overall decoding cost. Building on this insight, we present NOSA, a trainable sparse attention framework designed to natively support KV cache offloading. NOSA introduces explicit locality constraints by decomposing token selection into query-aware and query-agnostic components, thereby reducing KV transfers while preserving the same attention computation as used during training. We pretrain a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that it preserves near-lossless performance while achieving up to a 2.3x improvement in decoding throughput compared with the vanilla trainable sparse attention baseline (InfLLM-V2).

**Link**: [arxiv](http://arxiv.org/abs/2510.13602v1),  [pdf](http://arxiv.org/pdf/2510.13602v1)

**Tags**: cs.CL cs.AI cs.LG 



### F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs
**Authors**: Jude Haris, José Cano

**Updated**: 2025-10-15T10:56:37Z

**Summary**: Large Language Models (LLMs) have become increasingly prominent for daily tasks, from improving sound-totext translation to generating additional frames for the latest video games. With the help of LLM inference frameworks, such as llama.cpp, which support optimizations such as KV-caching and quantization, it is now easier than ever to deploy LLMs on edge devices. Quantization is fundamental to enable LLMs on resource-constrained edge devices, and llama.cpp utilizes block floating point (BFP) quantization to drastically reduce the bit width of weights and input tensors, the memory footprint, and the computational power required to run LLMs. LLMs are typically quantized with mixed BFP quantization across the model layers to reduce the loss of model accuracy due to quantization. Therefore, to efficiently accelerate across the layers of BFP-quantized LLMs, specialized accelerators need to support different BFP variants without reconfiguration. To address this issue, we propose a Flexible Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically switch between two BFP quantization variants and perform matrix multiplication (MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD Kria board, reduces inference time by 1.4x on average over the Arm NEON-based CPU execution across three BFP quantized LLMs while achieving 5.2 tokens per second (~3.9 words per second).

**Link**: [arxiv](http://arxiv.org/abs/2510.13401v1),  [pdf](http://arxiv.org/pdf/2510.13401v1)

**Tags**: cs.AR cs.DC cs.LG 



### Taming the Fragility of KV Cache Eviction in LLM Inference
**Authors**: Yuan Feng, Haoyu Guo, JunLin Lv, S. Kevin Zhou, Xike Xie

**Updated**: 2025-10-15T09:18:58Z

**Summary**: Large language models have revolutionized natural language processing, yet their deployment remains hampered by the substantial memory and runtime overhead of the transformer's Key-Value cache. To mitigate this, recent methods employ a scoring-aggregation framework to evict unimportant cache entries, based on the stability assumption-that a fixed subset of entries remains consistently important during generation. However, prior work has largely focused on refining importance indicators for scoring, while defaulting to mean aggregation due to a faithful trust in the stability assumption. In this work, we argue that this underlying assumption is inherently fragile, making mean aggregation highly vulnerable in extreme cases. To counter this, we propose a simple yet elegant defensive aggregation strategy: a two-step, linear-time approach that controls worst-case risk, thereby defending against extreme cases with negligible computational overhead. Embodying this strategy, we propose a novel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV, which incorporates layer-wise budget allocation. Across seven task domains (18 datasets), our methods reduce generation quality loss by 2.3x and 4.3x respectively, versus the strongest baseline under a 20% cache size. These results set new performance benchmarks and pioneer a promising direction for optimizing cache eviction against underlying fragility through worst-case risk management. Our code is available at https://github.com/FFY0/DefensiveKV.

**Link**: [arxiv](http://arxiv.org/abs/2510.13334v1),  [pdf](http://arxiv.org/pdf/2510.13334v1)

**Tags**: cs.CL 



### Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution   Time
**Authors**: Fuma Omori, Atsushi Yano, Takuya Azumi

**Updated**: 2025-10-15T08:25:13Z

**Summary**: Autonomous driving systems, critical for safety, require real-time guarantees and can be modeled as DAGs. Their acceleration features, such as caches and pipelining, often result in execution times below the worst-case. Thus, a probabilistic approach ensuring constraint satisfaction within a probability threshold is more suitable than worst-case guarantees for these systems. This paper considers probabilistic guarantees for DAG tasks by utilizing the results of probabilistic guarantees for single processors, which have been relatively more advanced than those for multi-core processors. This paper proposes a task set partitioning method that guarantees schedulability under the partitioned scheduling. The evaluation on randomly generated DAG task sets demonstrates that the proposed method schedules more task sets with a smaller mean analysis time compared to existing probabilistic schedulability analysis for DAGs. The evaluation also compares four bin-packing heuristics, revealing Item-Centric Worst-Fit-Decreasing schedules the most task sets.

**Link**: [arxiv](http://arxiv.org/abs/2510.13279v1),  [pdf](http://arxiv.org/pdf/2510.13279v1)

**Tags**: eess.SY cs.SY 



### BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing   Disaggregated LLM Serving in AI Infrastructure
**Authors**: Yiyuan He, Minxian Xu, Jingfeng Wu, Jianmin Hu, Chong Ma, Min Shen, Le Chen, Chengzhong Xu, Lin Qu, Kejiang Ye

**Updated**: 2025-10-15T07:20:14Z

**Summary**: Large language models (LLMs) are increasingly deployed in AI infrastructure, driving the need for high throughput, resource efficient serving systems. Disaggregated LLM serving, which separates prompt prefill from auto-regressive decode, has emerged as a promising architecture by isolating their heterogeneous compute and memory demands. However, current disaggregated systems face three key limitations: (i) static resource allocation cannot adapt to highly dynamic workloads, causing over-provisioning that wastes resources or under-provisioning that violates service level objectives (SLOs); (ii) inherent load imbalance between prefill and decode stages, where prefill is compute-bound and decode is memory-bound, causes under-utilization in one tier while the other becomes a bottleneck; and (iii) prefix cache aware routing skews load distribution, as high cache hit rate prefill nodes attract disproportionately more requests, further degrading balance and efficiency. To address these issues, we present BanaServe, a dynamic orchestration framework that continuously rebalances computational and memory resources across prefill and decode instances while eliminating hotspots induced by cache. BanaServe introduces layer level weight migration, attention level Key Value Cache (KV Cache) migration, and Global KV Cache Store sharing with layer wise overlapped transmission, enabling both coarse grained (layer level) and fine grained (attention level) load redistribution with minimal latency overhead. These mechanisms allow routers to perform purely load aware scheduling, unconstrained by cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher throughput with 3.9%-78.4% lower total processing time, and outperforms DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.

**Link**: [arxiv](http://arxiv.org/abs/2510.13223v1),  [pdf](http://arxiv.org/pdf/2510.13223v1)

**Tags**: cs.DC 



### Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar   Propagation
**Authors**: Yi Zuo, Zitao Wang, Lingling Li, Xu Liu, Fang Liu, Licheng Jiao

**Updated**: 2025-10-15T01:55:32Z

**Summary**: Text-to-image (T2I) diffusion models have recently demonstrated significant progress in video editing.   However, existing video editing methods are severely limited by their high computational overhead and memory consumption.   Furthermore, these approaches often sacrifice visual fidelity, leading to undesirable temporal inconsistencies and artifacts such as blurring and pronounced mosaic-like patterns.   We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video editing method.   Edit-Your-Interest introduces a spatio-temporal feature memory to cache features from previous frames, significantly reducing computational overhead compared to full-sequence spatio-temporal modeling approaches.   Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM), which is designed to efficiently cache and retain the crucial image tokens processed by spatial attention.   Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP propagates the most relevant tokens from previous frames to subsequent ones, preserving temporal consistency.   Finally, we introduce an SFM update algorithm that continuously refreshes the cached features, ensuring their long-term relevance and effectiveness throughout the video sequence.   Furthermore, we leverage cross-attention maps to automatically extract masks for the instances of interest.   These masks are seamlessly integrated into the diffusion denoising process, enabling fine-grained control over target objects and allowing Edit-Your-Interest to perform highly accurate edits while robustly preserving the background integrity.   Extensive experiments decisively demonstrate that the proposed Edit-Your-Interest outperforms state-of-the-art methods in both efficiency and visual fidelity, validating its superior effectiveness and practicality.

**Link**: [arxiv](http://arxiv.org/abs/2510.13084v1),  [pdf](http://arxiv.org/pdf/2510.13084v1)

**Tags**: cs.CV 



### LazyEviction: Lagged KV Eviction with Attention Pattern Observation for   Efficient Long Reasoning
**Authors**: Haoyue Zhang, Hualei Zhang, Xiaosong Ma, Jie Zhang, Song Guo

**Updated**: 2025-10-15T01:55:31Z

**Summary**: Large Language Models (LLMs) exhibit enhanced capabilities by Chain-of-Thought reasoning. However, the extended reasoning sequences introduce significant GPU memory overhead due to increased key-value (KV) cache. Existing KV cache compression methods mitigate memory bottlenecks but struggle in long reasoning tasks. In this paper, we analyze attention patterns in reasoning tasks and reveal a Token Importance Recurrence phenomenon: a large proportion of tokens regain high attention after multiple decoding steps, which is failed to capture by existing works and may lead to unpredictable eviction on such periodically critical tokens. To address this, we propose LazyEviction, an observation window-based lagged eviction framework retaining latent recurring tokens by prioritized eviction based on tokens' recurrence patterns. Extensive experiments demonstrate that LazyEviction reduces KV cache by 50%~70% while maintaining comparable accuracy, outperforming existing KV cache compression baselines. Our implementation code can be found at https://github.com/Halo-949/LazyEviction.

**Link**: [arxiv](http://arxiv.org/abs/2506.15969v3),  [pdf](http://arxiv.org/pdf/2506.15969v3)

**Tags**: cs.LG cs.CL 



### Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching   for Heterogeneous Tasks and Clusters
**Authors**: Wei Da, Evangelia Kalyvianaki

**Updated**: 2025-10-14T18:04:00Z

**Summary**: This paper introduces Dodoor, an efficient randomized decentralized scheduler designed for task scheduling in modern data centers. Dodoor leverages advanced research on the weighted balls-into-bins model with b-batched setting. Unlike other decentralized schedulers that rely on real-time probing of remote servers, Dodoor makes scheduling decisions based on cached server information, which is updated in batches, to reduce communication overheads. To schedule tasks with dynamic, multidimensional resource requirements in heterogeneous cluster, Dodoor uses a novel load score to measure servers' loads for each scheduled task. This score captures the anti-affinity between servers and tasks in contrast to the commonly used heuristic of counting pending tasks to balance load. On a 101-node heterogeneous cluster, Dodoor is evaluated using two workloads: (i) simulated Azure virtual machines placements and (ii) real serverless Python functions executions in Docker. The evaluation shows that Dodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can also increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency by 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two workloads.

**Link**: [arxiv](http://arxiv.org/abs/2510.12889v1),  [pdf](http://arxiv.org/pdf/2510.12889v1)

**Tags**: cs.DC C.2.4 



### KVCOMM: Online Cross-context KV-cache Communication for Efficient   LLM-based Multi-agent Systems
**Authors**: Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, Yiran Chen

**Updated**: 2025-10-14T18:00:01Z

**Summary**: Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

**Link**: [arxiv](http://arxiv.org/abs/2510.12872v1),  [pdf](http://arxiv.org/pdf/2510.12872v1)

**Tags**: cs.MA cs.AI stat.ML 



### A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization   of Banded Matrices
**Authors**: Evelyne Ringoot, Rabab Alomairy, Alan Edelman

**Updated**: 2025-10-14T16:39:29Z

**Summary**: The reduction of a banded matrix to a bidiagonal form is a crucial step in the Singular Value Decomposition (SVD), a cornerstone of scientific computing and AI. Despite being a highly parallel algorithm, it was previously believed to be unsuitable for GPU computation because it is memory bandwidth-bound. Recent developments in GPU hardware, including larger L1 memory per Streaming Multiprocessor/Compute Unit, have changed that. We present the first GPU algorithm for reducing a banded matrix to bidiagonal form as part of the NextLA$.$jl open-source software package. Our algorithm is based on previous CPU-based multicore parallel cache-efficient bulge chasing algorithms and adapted to optimize for GPU throughput. We leverage Julia Language's Array abstractions and KernelAbstractions to implement a single hardware- and data precision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for half, single, and double precision, and examine performance optimization across hardware architectures and data precision. We also develop a hardware-aware performance model and identify key hyperparameters, such as inner tilewidth and block concurrency, that govern optimal GPU execution for bandwidth-bound workloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU can outperform CPU-based implementations: the GPU algorithm outperforms multithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size 1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition, the performance of the algorithm increases linearly with matrix bandwidth size, making faster reduction of larger matrix bandwidths now also possible. With this work, we break memory bandwidth barriers, as well as matrix bandwidth barriers, resulting in orders-of-magnitude faster algorithms for the reduction of banded matrices to bidiagonal form on the GPU.

**Link**: [arxiv](http://arxiv.org/abs/2510.12705v1),  [pdf](http://arxiv.org/pdf/2510.12705v1)

**Tags**: cs.DC cs.MS 



### OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device   Speculative Decoding
**Authors**: Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang

**Updated**: 2025-10-14T16:05:11Z

**Summary**: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2507.02659v3),  [pdf](http://arxiv.org/pdf/2507.02659v3)

**Tags**: cs.LG cs.CL 



### ChunkKV: Semantic-Preserving KV Cache Compression for Efficient   Long-Context LLM Inference
**Authors**: Xiang Liu, Zhenheng Tang, Peijie Dong, Zeyu Li, Yue Liu, Bo Li, Xuming Hu, Xiaowen Chu

**Updated**: 2025-10-14T15:42:41Z

**Summary**: Large Language Models (LLMs) require significant GPU memory when processing long texts, with the key value (KV) cache consuming up to 70\% of total memory during inference. Although existing compression methods reduce memory by evaluating the importance of individual tokens, they overlook critical semantic relationships between tokens, resulting in fragmented context and degraded performance. We introduce ChunkKV, which fundamentally reimagines KV cache compression by treating semantic chunks - rather than isolated tokens - as basic compression units. This approach preserves complete linguistic structures and contextual integrity, ensuring that essential meaning is retained even under aggressive compression. Our innovation includes a novel layer-wise index reuse technique that exploits the higher cross-layer similarity of preserved indices in ChunkKV, reducing computational overhead and improving throughput by 26.5\%. Comprehensive evaluations on challenging benchmarks: LongBench, Needle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV outperforms state-of-the-art methods by up to 8.7\% in precision while maintaining the same compression ratio. These results confirm that semantic-aware compression significantly enhances both efficiency and performance for long-context LLM inference, providing a simple yet effective solution to the memory bottleneck problem. The code is available at \href{https://github.com/NVIDIA/kvpress}{link}.

**Link**: [arxiv](http://arxiv.org/abs/2502.00299v5),  [pdf](http://arxiv.org/pdf/2502.00299v5)

**Tags**: cs.CL 



### Aixel: A Unified, Adaptive and Extensible System for AI-powered Data   Analysis
**Authors**: Meihui Zhang, Liming Wang, Chi Zhang, Zhaojing Luo

**Updated**: 2025-10-14T15:34:35Z

**Summary**: A growing trend in modern data analysis is the integration of data management with learning, guided by accuracy, latency, and cost requirements. In practice, applications draw data of different formats from many sources. In the meanwhile, the objectives and budgets change over time. Existing systems handle these applications across databases, analysis libraries, and tuning services. Such fragmentation leads to complex user interaction, limited adaptability, suboptimal performance, and poor extensibility across components. To address these challenges, we present Aixel, a unified, adaptive, and extensible system for AI-powered data analysis. The system organizes work across four layers: application, task, model, and data. The task layer provides a declarative interface to capture user intent, which is parsed into an executable operator plan. An optimizer compiles and schedules this plan to meet specified goals in accuracy, latency, and cost. The task layer coordinates the execution of data and model operators, with built-in support for reuse and caching to improve efficiency. The model layer offers versioned storage for index, metadata, tensors, and model artifacts. It supports adaptive construction, task-aligned drift detection, and safe updates that reuse shared components. The data layer provides unified data management capabilities, including indexing, constraint-aware discovery, task-aligned selection, and comprehensive feature management. With the above designed layers, Aixel delivers a user friendly, adaptive, efficient, and extensible system.

**Link**: [arxiv](http://arxiv.org/abs/2510.12642v1),  [pdf](http://arxiv.org/pdf/2510.12642v1)

**Tags**: cs.DB cs.AI 



### Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in   Containerized Clouds
**Authors**: Gunwoo Kim, Taejune Park, Jinwoo Kim

**Updated**: 2025-10-14T15:26:09Z

**Summary**: In modern containerized cloud environments, the adoption of RDMA (Remote Direct Memory Access) has expanded to reduce CPU overhead and enable high-performance data exchange. Achieving this requires strong performance isolation to ensure that one container's RDMA workload does not degrade the performance of others, thereby maintaining critical security assurances. However, existing isolation techniques are difficult to apply effectively due to the complexity of microarchitectural resource management within RDMA NICs (RNICs). This paper experimentally analyzes two types of resource exhaustion attacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline saturation attacks. Our results show that state saturation attacks can cause up to a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in cache misses for victim containers, while pipeline saturation attacks lead to severe link-level congestion and significant amplification, where small verb requests result in disproportionately high resource consumption. To mitigate these threats and restore predictable security assurances, we propose HT-Verbs, a threshold-driven framework based on real-time per-container RDMA verb telemetry and adaptive resource classification that partitions RNIC resources into hot, warm, and cold tiers and throttles abusive workloads without requiring hardware modifications.

**Link**: [arxiv](http://arxiv.org/abs/2510.12629v1),  [pdf](http://arxiv.org/pdf/2510.12629v1)

**Tags**: cs.CR cs.NI 



### Analysis and Evaluation of Using Microsecond-Latency Memory for   In-Memory Indices and Caches in SSD-Based Key-Value Stores
**Authors**: Yosuke Bando, Akinobu Mita, Kazuhiro Hiwada, Shintaro Sano, Tomoya Suzuki, Yu Nakanishi, Kazutaka Tomida, Hirotsugu Kajihara, Akiyuki Kaneko, Daisuke Taki, Yukimasa Miyamoto, Tomokazu Yoshida, Tatsuo Shiozawa

**Updated**: 2025-10-14T08:34:09Z

**Summary**: When key-value (KV) stores use SSDs for storing a large number of items, oftentimes they also require large in-memory data structures including indices and caches to be traversed to reduce IOs. This paper considers offloading most of such data structures from the costly host DRAM to secondary memory whose latency is in the microsecond range, an order of magnitude longer than those of currently available DIMM-mounted or CXL memory devices. While emerging microsecond-latency memory is likely to cost much less than DRAM, it can significantly slow down SSD-based KV stores if naively employed. This paper analyzes and evaluates the impact of microsecond-level memory latency on the KV operation throughput. Our analysis finds that a well-known latency-hiding technique of software prefetching for long-latency memory from user-level threads is effective. The novelty of our analysis lies in modeling how the interplay between prefetching and IO affects performance, from which we derive an equation that well explains the throughput degradation due to long memory latency. The model tells us that the presence of IO significantly enhances the tolerance to memory latency, leading to a finding that SSD-based KV stores can be made latency-tolerant without devising new techniques for microsecond-latency memory. To confirm this, we design a microbenchmark as well as modify existing SSD-based KV stores so that they issue prefetches from user-level threads, and run them while placing most of in-memory data structures on FPGA-based memory with adjustable microsecond latency. The results demonstrate that their KV operation throughputs can be well explained by our model, and the modified KV stores achieve near-DRAM throughputs for up to a memory latency of 5 microseconds. This suggests the possibility that SSD-based KV stores can use microsecond-latency memory as a cost-effective alternative to the host DRAM.

**Link**: [arxiv](http://arxiv.org/abs/2510.12280v1),  [pdf](http://arxiv.org/pdf/2510.12280v1)

**Tags**: cs.PF cs.DB 



### RoVer: Robot Reward Model as Test-Time Verifier for   Vision-Language-Action Model
**Authors**: Mingtong Dai, Lingbo Liu, Yongjie Bai, Yang Liu, Zhouxia Wang, Rui SU, Chunjie Chen, Liang Lin, Xinyu Wu

**Updated**: 2025-10-14T07:41:47Z

**Summary**: Vision-Language-Action (VLA) models have become a prominent paradigm for embodied intelligence, yet further performance improvements typically rely on scaling up training data and model size -- an approach that is prohibitively expensive for robotics and fundamentally limited by data collection costs. We address this limitation with $\mathbf{RoVer}$, an embodied test-time scaling framework that uses a $\mathbf{Ro}$bot Process Reward Model (PRM) as a Test-Time $\mathbf{Ver}$ifier to enhance the capabilities of existing VLA models without modifying their architectures or weights. Specifically, RoVer (i) assigns scalar-based process rewards to evaluate the reliability of candidate actions, and (ii) predicts an action-space direction for candidate expansion/refinement. During inference, RoVer generates multiple candidate actions concurrently from the base policy, expands them along PRM-predicted directions, and then scores all candidates with PRM to select the optimal action for execution. Notably, by caching shared perception features, it can amortize perception cost and evaluate more candidates under the same test-time computational budget. Essentially, our approach effectively transforms available computing resources into better action decision-making, realizing the benefits of test-time scaling without extra training overhead. Our contributions are threefold: (1) a general, plug-and-play test-time scaling framework for VLAs; (2) a PRM that jointly provides scalar process rewards and an action-space direction to guide exploration; and (3) an efficient direction-guided sampling strategy that leverages a shared perception cache to enable scalable candidate generation and selection during inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.10975v2),  [pdf](http://arxiv.org/pdf/2510.10975v2)

**Tags**: cs.RO 



### AndesVL Technical Report: An Efficient Mobile-side Multimodal Large   Language Model
**Authors**: Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu

**Updated**: 2025-10-14T05:05:14Z

**Summary**: In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.

**Link**: [arxiv](http://arxiv.org/abs/2510.11496v2),  [pdf](http://arxiv.org/pdf/2510.11496v2)

**Tags**: cs.CV cs.AI 



### APCE: Adaptive Progressive Context Expansion for Long Context Processing
**Authors**: Baisub Lee, Sanghyun Byun, Mohanad Odema, Jung Guack, Jacob Song, Woo Seong Chung

**Updated**: 2025-10-14T01:26:36Z

**Summary**: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing two key challenges: (1) A growing memory footprint due to quadratic self-attention and linear KV-cache scaling in memory as sequence length increases; (2) the ContextRot phenomena where empirical evidence suggests that transformer architecture's performance degrades with increasing context length. Given the shared dependency on the input, a natural question arises: Can we surgically select the most important input chunks for processing to synergistically (a) reduce the memory footprint, and (b) mitigate the ContextRot effects? In this paper, we answer this question in the affirmative for long-context summarization tasks. We propose APCE as a context-aware solution to select the most important input chunks through low-dimensional semantic similarity matching with the current query. By directly operating on the input, APCE decouples from strict dependency on underlying hardware or CUDA environments, promising a compatible solution scalable to different deployment systems. Our empirical evaluations have demonstrated superior or on-par summarization performance for APCE compared to the full dense baseline using a fraction (50%-70%) of the input sequence resulting in KV-cache and self-attention memory efficiency improvements. We hope our findings inspire further research on context-aware efficiency solutions for LCTMs geared towards other relevant long-context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.12051v1),  [pdf](http://arxiv.org/pdf/2510.12051v1)

**Tags**: cs.CL cs.AI 



### LongLive: Real-time Interactive Long Video Generation
**Authors**: Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen

**Updated**: 2025-10-13T22:41:26Z

**Summary**: We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.

**Link**: [arxiv](http://arxiv.org/abs/2509.22622v2),  [pdf](http://arxiv.org/pdf/2509.22622v2)

**Tags**: cs.CV 



### FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline   Refactoring in Fragmented Serverless Clusters
**Authors**: Yanying Lin, Shijie Peng, Chengzhi Lu, Chengzhong Xu, Kejiang Ye

**Updated**: 2025-10-13T21:01:40Z

**Summary**: Serving Large Language Models (LLMs) in production faces significant challenges from highly variable request patterns and severe resource fragmentation in serverless clusters. Current systems rely on static pipeline configurations that struggle to adapt to dynamic workload conditions, leading to substantial inefficiencies. We present FlexPipe, a novel system that dynamically reconfigures pipeline architectures during runtime to address these fundamental limitations. FlexPipe decomposes models into fine-grained stages and intelligently adjusts pipeline granularity based on real-time request pattern analysis, implementing three key innovations: fine-grained model partitioning with preserved computational graph constraints, inflight pipeline refactoring with consistent cache transitions, and topology-aware resource allocation that navigates GPU fragmentation. Comprehensive evaluation on an 82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource efficiency while maintaining 38.3% lower latency compared to state-of-the-art systems, reducing GPU reservation requirements from 75% to 30% of peak capacity.

**Link**: [arxiv](http://arxiv.org/abs/2510.11938v1),  [pdf](http://arxiv.org/pdf/2510.11938v1)

**Tags**: cs.DC 



### LLMBridge: Reducing Costs in a Prompt-Centric Internet
**Authors**: Noah Martin, Abdullah Bin Faisal, Hiba Eltigani, Rukhshan Haroon, Swaminathan Lamelas, Fahad Dogar

**Updated**: 2025-10-13T20:40:32Z

**Summary**: Today's Internet infrastructure is centered around content retrieval over HTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in performance, security, and cost-effectiveness. We envision a future where Internet communication will be dominated by "prompts" sent to generative AI models. For this, we will need proxies that provide similar functions to HTTP proxies (e.g., caching, routing, compression) while dealing with unique challenges and opportunities of prompt-based communication. As a first step toward supporting prompt-based communication, we present LLMBridge, an LLM proxy designed for cost-conscious users, such as those in developing regions and education (e.g., students, instructors). LLMBridge supports three key optimizations: model selection (routing prompts to the most suitable model), context management (intelligently reducing the amount of context), and semantic caching (serving prompts using local models and vector databases). These optimizations introduce trade-offs between cost and quality, which applications navigate through a high-level, bidirectional interface. As case studies, we deploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service and a university classroom environment. The WhatsApp service has been live for over twelve months, serving 100+ users and handling more than 14.7K requests. In parallel, we exposed LLMBridge to students across three computer science courses over a semester, where it supported diverse LLM-powered applications - such as reasoning agents and chatbots - and handled an average of 500 requests per day. We report on deployment experiences across both settings and use the collected workloads to benchmark the effectiveness of various cost-optimization strategies, analyzing their trade-offs in cost, latency, and response quality.

**Link**: [arxiv](http://arxiv.org/abs/2410.11857v2),  [pdf](http://arxiv.org/pdf/2410.11857v2)

**Tags**: cs.DC cs.LG 



### StreamAgent: Towards Anticipatory Agents for Streaming Video   Understanding
**Authors**: Haolin Yang, Feilong Tang, Lingxiao Zhao, Xiang An, Ming Hu, Huifa Li, Xinlin Zhuang, Yifan Lu, Xiaofeng Zhang, Abdalla Swikir, Junjun He, Zongyuan Ge, Imran Razzak

**Updated**: 2025-10-13T17:15:14Z

**Summary**: Real-time streaming video understanding in domains such as autonomous driving and intelligent surveillance poses challenges beyond conventional offline video processing, requiring continuous perception, proactive decision making, and responsive interaction based on dynamically evolving visual content. However, existing methods rely on alternating perception-reaction or asynchronous triggers, lacking task-driven planning and future anticipation, which limits their real-time responsiveness and proactive decision making in evolving video streams. To this end, we propose a StreamAgent that anticipates the temporal intervals and spatial regions expected to contain future task-relevant information to enable proactive and goal-driven responses. Specifically, we integrate question semantics and historical observations through prompting the anticipatory agent to anticipate the temporal progression of key events, align current observations with the expected future evidence, and subsequently adjust the perception action (e.g., attending to task-relevant regions or continuously tracking in subsequent frames). To enable efficient inference, we design a streaming KV-cache memory mechanism that constructs a hierarchical memory structure for selective recall of relevant tokens, enabling efficient semantic retrieval while reducing the overhead of storing all tokens in the traditional KV-cache. Extensive experiments on streaming and long video understanding tasks demonstrate that our method outperforms existing methods in response accuracy and real-time efficiency, highlighting its practical value for real-world streaming scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2508.01875v3),  [pdf](http://arxiv.org/pdf/2508.01875v3)

**Tags**: cs.CV 



### MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with   RoE
**Authors**: Soheil Zibakhsh, Mohammad Samragh, Kumari Nishu, Lauren Hannah, Arnav Kundu, Minsik Cho

**Updated**: 2025-10-13T16:48:37Z

**Summary**: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction. To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.

**Link**: [arxiv](http://arxiv.org/abs/2509.17238v2),  [pdf](http://arxiv.org/pdf/2509.17238v2)

**Tags**: cs.AI cs.CL cs.LG 



### LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences
**Authors**: Wenbo Wu, Qingyi Si, Xiurui Pan, Ye Wang, Jie Zhang

**Updated**: 2025-10-13T11:28:30Z

**Summary**: While Key-Value (KV) cache succeeds in reducing redundant computations in auto-regressive models, it introduces significant memory overhead, limiting its practical deployment in long-sequence scenarios. Existing KV retrieval methods mitigate this by dynamically retaining only a subset of KV entries on the GPU. However, they still suffer from notable efficiency and accuracy bottlenecks due to per-token retrieval and coarse-grained page-level KV management, especially in long-output reasoning scenarios. With the emergence of large reasoning models, efficiently handling such scenarios has become increasingly important. To address this issue, we present two key observations: (1) critical KVs exhibit strong temporal locality during decoding, and (2) these KVs exhibit distinct distribution patterns across the input prompt and generated output. Building on these observations, we propose LouisKV, an efficient KV cache retrieval framework designed for various long-sequence scenarios. Specifically, LouisKV introduces a semantic-aware retrieval strategy leveraging temporal locality to trigger retrieval only at semantic boundaries, drastically reducing computation and data transfer overhead. LouisKV also designs a decoupled, fine-grained management scheme that tailors differentiated strategies for input and output sequences to create retrieval units that better match the model's attention patterns, enabling precise identification of critical KVs. Furthermore, to boost efficiency, LouisKV incorporates several kernel-level optimizations, including custom Triton and CUDA kernels to accelerate the KV clustering and retrieval. Evaluations show that LouisKV achieves up to 4.7$\times$ speedup over state-of-the-art KV retrieval methods while maintaining near-lossless accuracy across diverse long-sequence tasks, including long-input short-output, short-input long-output, and long-input long-output scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2510.11292v1),  [pdf](http://arxiv.org/pdf/2510.11292v1)

**Tags**: cs.LG cs.AI 



### Low-noise environment for probing fundamental symmetries
**Authors**: F. J. Collings, N. J. Fitch, R. A. Jenkins, J. M. Dyne, E. Wursten, M. T. Ziemba, X. S. Zheng, F. Castellini, J. Lim, B. E. Sauer, M. R. Tarbutt

**Updated**: 2025-10-13T11:21:00Z

**Summary**: We present the design and characterization of a low-noise environment for measuring the electron's electric dipole moment (EDM) with a beam of molecules. To minimize magnetic Johnson noise from metals, the design features ceramic electric field plates housed in a glass vacuum chamber. To suppress external magnetic noise the apparatus is enclosed within a cylindrical four-layer mu-metal shield with a shielding factor exceeding $10^6$ in one radial direction and $10^5$ in the other. Finite element modelling shows that the difference between these shielding factors is due to imperfect joints between sections of mu-metal. Using atomic magnetometers to monitor the magnetic field inside the shield, we measure noise below 40 fT/$\sqrt{{\rm Hz}}$ at 1 Hz and above, rising to 500 fT/$\sqrt{{\rm Hz}}$ at 0.1 Hz. Analytical and numerical studies show that residual magnetic Johnson noise contributes approximately 13 fT/$\sqrt{{\rm Hz}}$. The background magnetic field averaged along the beamline is maintained below 3 pT, with typical gradients of a few nT/m. An electric field of 20 kV/cm is applied without discharges and with leakage currents below 1 nA. Each magnetometer measures the magnetic field correlated with the direction of the applied electric field with a precision of 0.11 fT in 104 hours of data. These results demonstrate that the apparatus is suitable for measuring the electron EDM with precision at the $10^{-31}$ e cm level. The design principles and characterization techniques presented here are broadly applicable to precision measurements probing fundamental symmetries in molecules, atoms, and neutrons.

**Link**: [arxiv](http://arxiv.org/abs/2503.21725v2),  [pdf](http://arxiv.org/pdf/2503.21725v2)

**Tags**: physics.atom-ph 



### dInfer: An Efficient Inference Framework for Diffusion Language Models
**Authors**: Yuxin Ma, Lun Du, Lanning Wei, Kun Chen, Qian Xu, Kangyu Wang, Guofeng Feng, Guoshan Lu, Lin Liu, Xiaojing Qi, Xinyuan Zhang, Zhen Tao, Haibo Feng, Ziyun Jiang, Ying Xu, Zenan Huang, Yihong Zhuang, Haokai Xu, Jiaqi Hu, Zhenzhong Lan, Junbo Zhao, Jianguo Li, Da Zheng

**Updated**: 2025-10-13T10:39:59Z

**Summary**: Diffusion-based large language models (dLLMs) have emerged as a promising alternative to autoregressive (AR) LLMs, leveraging denoising-based generation to enable inherent parallelism. Even more and more open-sourced dLLM models emerge, yet their widespread adoption remains constrained by the lack of a standardized and efficient inference framework. We present dInfer, an efficient and extensible framework for dLLM inference. dInfer decomposes the inference pipeline into four modular components--model, diffusion iteration manager, decoding strategy, and KV-cache manager--and integrates novel algorithms for each component alongside system-level optimizations. Through this combination of algorithmic innovations and system enhancements, dInfer achieves substantial efficiency gains without compromising output quality on LLaDA-MoE. At batch size 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800 tokens per second across six benchmarks on $8\times$ H800 GPUs. Compared to prior systems, dInfer delivers a $10\times$ speedup over Fast-dLLM while maintaining similar model performance. Even compared to the AR model (with a comparable number of activation parameters and performance) QWen2.5-3B, which is highly optimized with the latest vLLM inference engine, dInfer still delivers a $2$-$3\times$ speedup. The implementation of dInfer is open-sourced at https://github.com/inclusionAI/dInfer.

**Link**: [arxiv](http://arxiv.org/abs/2510.08666v2),  [pdf](http://arxiv.org/pdf/2510.08666v2)

**Tags**: cs.CL cs.AI 



### TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for   Vision-Language-Action Models
**Authors**: Chenghao Liu, Jiachen Zhang, Chengxuan Li, Zhimu Zhou, Shixin Wu, Songfang Huang, Huiling Duan

**Updated**: 2025-10-13T10:18:34Z

**Summary**: Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.

**Link**: [arxiv](http://arxiv.org/abs/2508.19257v2),  [pdf](http://arxiv.org/pdf/2508.19257v2)

**Tags**: cs.CV cs.AI cs.LG cs.RO 



### XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer   Compression
**Authors**: Haoqi Yang, Yao Yao, Zuchao Li, Baoyuan Qi, Guoming Liu, Hai Zhao

**Updated**: 2025-10-13T10:17:21Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse natural language processing tasks. However, their extensive memory requirements, particularly due to KV cache growth during long-text understanding and generation, present significant challenges for deployment in resource-constrained environments. Quantization has emerged as a promising solution to reduce memory consumption while preserving historical information. We propose XQuant, a training-free and plug-and-play framework that achieves ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key innovations: a computationally negligible data-free calibration method and cross-layer KV cache compression, enabling quantization to sub-1.4 bits. Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by achieving lower bit-width while maintaining superior performance, establishing a better trade-off between memory efficiency and model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2510.11236v1),  [pdf](http://arxiv.org/pdf/2510.11236v1)

**Tags**: cs.CL 



### SANA-Video: Efficient Video Generation with Block Linear Diffusion   Transformer
**Authors**: Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-10-13T09:12:27Z

**Summary**: We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.24695v2),  [pdf](http://arxiv.org/pdf/2509.24695v2)

**Tags**: cs.CV cs.AI 



### Autoencoding-Free Context Compression for LLMs via Contextual Semantic   Anchors
**Authors**: Xin Liu, Runsong Zhao, Pengcheng Huang, Xinyu Liu, Junyi Xiao, Chunyang Xiao, Tong Xiao, Shengxiang Gao, Zhengtao Yu, Jingbo Zhu

**Updated**: 2025-10-13T08:26:21Z

**Summary**: Context compression presents a promising approach for accelerating large language model (LLM) inference by compressing long contexts into compact representations. Current context compression methods predominantly rely on autoencoding tasks to train context-agnostic compression tokens to compress contextual semantics. While autoencoding tasks enable compression tokens to acquire compression capabilities, compression via autoencoding tasks creates a fundamental mismatch: the models are optimized for reconstruction that diverge from actual downstream tasks, thereby weakening the features more beneficial for real-world usage. We propose Semantic-Anchor Compression (SAC), a novel method that shifts from autoencoding task based compression to an architecture that is equipped with this compression capability \textit{a priori}. Instead of training models to compress contexts through autoencoding tasks, SAC directly selects so-called anchor tokens from the original context and aggregates contextual information into their key-value (KV) representations. By deriving representations directly from the contextual tokens, SAC eliminates the need for autoencoding training. To ensure compression performance while directly leveraging anchor tokens, SAC incorporates two key designs: (1) anchor embeddings that enable the compressor to identify critical tokens, and (2) bidirectional attention modification that allows anchor tokens to capture information from the entire context. Experimental results demonstrate that SAC consistently outperforms existing context compression methods across various compression ratios. On out-of-distribution evaluation using MRQA, SAC achieves 1 EM improvement at 5x compression over strong baselines, with increasing advantages at higher compression ratios.

**Link**: [arxiv](http://arxiv.org/abs/2510.08907v2),  [pdf](http://arxiv.org/pdf/2510.08907v2)

**Tags**: cs.CL 



### Refining Hybrid Genetic Search for CVRP via Reinforcement   Learning-Finetuned LLM
**Authors**: Rongjie Zhu, Cong Zhang, Zhiguang Cao

**Updated**: 2025-10-13T08:08:58Z

**Summary**: While large language models (LLMs) are increasingly used as automated heuristic designers for vehicle routing problems (VRPs), current state-of-the-art methods predominantly rely on prompting massive, general-purpose models like GPT-4. This work challenges that paradigm by demonstrating that a smaller, specialized LLM, when meticulously fine-tuned, can generate components that surpass expert-crafted heuristics within advanced solvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for Fine-Tuning a small LLM to generate high-performance crossover operators for the Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP). Our method employs a multi-tiered, curriculum-based reward function that progressively guides the LLM to master generating first compilable, then executable, and finally, superior-performing operators that exceed human expert designs. This is coupled with an operator caching mechanism that discourages plagiarism and promotes diversity during training. Comprehensive experiments show that our fine-tuned LLM produces crossover operators which significantly outperform the expert-designed ones in HGS. The performance advantage remains consistent, generalizing from small-scale instances to large-scale problems with up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading neuro-combinatorial baselines, prompt-based methods, and commercial LLMs such as GPT-4o and GPT-4o-mini.

**Link**: [arxiv](http://arxiv.org/abs/2510.11121v1),  [pdf](http://arxiv.org/pdf/2510.11121v1)

**Tags**: cs.LG 



### GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable   Transactional and Analytical Workloads
**Authors**: Farzaneh Zirak, Farhana Choudhury, Renata Borovica-Gajic

**Updated**: 2025-10-13T05:03:23Z

**Summary**: Data prefetching--loading data into the cache before it is requested--is essential for reducing I/O overhead and improving database performance. While traditional prefetchers focus on sequential patterns, recent learning-based approaches, especially those leveraging data semantics, achieve higher accuracy for complex access patterns. However, these methods often struggle with today's dynamic, ever-growing datasets and require frequent, timely fine-tuning. Privacy constraints may also restrict access to complete datasets, necessitating prefetchers that can learn effectively from samples. To address these challenges, we present GrASP, a learning-based prefetcher designed for both analytical and transactional workloads. GrASP enhances prefetching accuracy and scalability by leveraging logical block address deltas and combining query representations with result encodings. It frames prefetching as a context-aware multi-label classification task, using multi-layer LSTMs to predict delta patterns from embedded context. This delta modeling approach enables GrASP to generalize predictions from small samples to larger, dynamic datasets without requiring extensive retraining. Experiments on real-world datasets and industrial benchmarks demonstrate that GrASP generalizes to datasets 250 times larger than the training data, achieving up to 45% higher hit ratios, 60% lower I/O time, and 55% lower end-to-end query execution latency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a 90.8% I/O time reduction, and a 57.1% execution latency reduction.

**Link**: [arxiv](http://arxiv.org/abs/2510.11011v1),  [pdf](http://arxiv.org/pdf/2510.11011v1)

**Tags**: cs.DB cs.LG 



### ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP   Architecture and Paired Weight Sharing
**Authors**: Shivanshu Kumar, Gopalakrishnan Srinivasan

**Updated**: 2025-10-13T04:04:54Z

**Summary**: While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint.

**Link**: [arxiv](http://arxiv.org/abs/2510.13860v1),  [pdf](http://arxiv.org/pdf/2510.13860v1)

**Tags**: cs.CL cs.AI 



### Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies   for Reasoning Models
**Authors**: Junhyuck Kim, Ethan Ewer, Taehong Moon, Jongho Park, Dimitris Papailiopoulos

**Updated**: 2025-10-13T03:14:28Z

**Summary**: While 4-bit quantization has emerged as a memory-optimal choice for non-reasoning models and zero-shot tasks across scales, we show that this universal prescription fails for reasoning models, where the KV cache rather than model size can dominate memory. Through systematic experiments across 1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent trade-off: models with an effective size below 8-bit 4B parameters achieve better accuracy by allocating memory to more weights rather than longer generation, while larger models achieve better accuracy by allocating memory to longer generations. This scale threshold also determines when parallel scaling becomes memory-efficient and whether KV cache eviction outperforms KV quantization. Our findings show that memory optimization for LLMs cannot be scale-agnostic, while providing principled guidelines: for small reasoning models, prioritize model capacity over test-time compute, while for larger ones, maximize test-time compute. Our results suggest that optimizing reasoning models for deployment requires fundamentally different strategies from those established for non-reasoning models.

**Link**: [arxiv](http://arxiv.org/abs/2510.10964v1),  [pdf](http://arxiv.org/pdf/2510.10964v1)

**Tags**: cs.LG 



### A Joint Learning Approach to Hardware Caching and Prefetching
**Authors**: Samuel Yuan, Divyanshu Saxena, Jiayi Chen, Nihal Sharma, Aditya Akella

**Updated**: 2025-10-13T00:11:02Z

**Summary**: Several learned policies have been proposed to replace heuristics for scheduling, caching, and other system components in modern systems. By leveraging diverse features, learning from historical trends, and predicting future behaviors, such models promise to keep pace with ever-increasing workload dynamism and continuous hardware evolution. However, policies trained in isolation may still achieve suboptimal performance when placed together. In this paper, we inspect one such instance in the domain of hardware caching -- for the policies of cache replacement and prefetching. We argue that these two policies are bidirectionally interdependent and make the case for training the two jointly. We propose a joint learning approach based on developing shared representations for the features used by the two policies. We present two approaches to develop these shared representations, one based on a joint encoder and another based on contrastive learning of the embeddings, and demonstrate promising preliminary results for both of these. Finally, we lay down an agenda for future research in this direction.

**Link**: [arxiv](http://arxiv.org/abs/2510.10862v1),  [pdf](http://arxiv.org/pdf/2510.10862v1)

**Tags**: cs.LG cs.AR 



### DriftBench: Defining and Generating Data and Query Workload Drift for   Benchmarking
**Authors**: Guanli Liu, Renata Borovica-Gajic

**Updated**: 2025-10-12T23:46:04Z

**Summary**: Data and workload drift are key to evaluating database components such as caching, cardinality estimation, indexing, and query optimization. Yet, existing benchmarks are static, offering little to no support for modeling drift. This limitation stems from the lack of clear definitions and tools for generating data and workload drift. Motivated by this gap, we propose a unified taxonomy for data and workload drift, grounded in observations from both academia and industry. Building on this foundation, we introduce DriftBench, a lightweight and extensible framework for generating data and workload drift in benchmark inputs. Together, the taxonomy and DriftBench provide a standardized vocabulary and mechanism for modeling and generating drift in benchmarking. We demonstrate their effectiveness through case studies involving data drift, workload drift, and drift-aware cardinality estimation.

**Link**: [arxiv](http://arxiv.org/abs/2510.10858v1),  [pdf](http://arxiv.org/pdf/2510.10858v1)

**Tags**: cs.DB 



### Streamlining Image Editing with Layered Diffusion Brushes
**Authors**: Peyman Gholami, Robert Xiao

**Updated**: 2025-10-12T23:17:39Z

**Summary**: Denoising diffusion models have emerged as powerful tools for image manipulation, yet interactive, localized editing workflows remain underdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel training-free framework that enables interactive, layer-based editing using standard diffusion models. LDB defines each "layer" as a self-contained set of parameters guiding the generative process, enabling independent, non-destructive, and fine-grained prompt-guided edits, even in overlapping regions. LDB leverages a unique intermediate latent caching approach to reduce each edit to only a few denoising steps, achieving 140~ms per edit on consumer GPUs. An editor implementing LDB, incorporating familiar layer concepts, was evaluated via user study and quantitative metrics. Results demonstrate LDB's superior speed alongside comparable or improved image quality, background preservation, and edit fidelity relative to state-of-the-art methods across various sequential image manipulation tasks. The findings highlight LDB's ability to significantly enhance creative workflows by providing an intuitive and efficient approach to diffusion-based image editing and its potential for expansion into related subdomains, such as video editing.

**Link**: [arxiv](http://arxiv.org/abs/2405.00313v2),  [pdf](http://arxiv.org/pdf/2405.00313v2)

**Tags**: cs.CV 



### A Simple and Better Baseline for Visual Grounding
**Authors**: Jingchao Wang, Wenlong Zhang, Dingjiang Huang, Hong Wang, Yefeng Zheng

**Updated**: 2025-10-12T13:06:59Z

**Summary**: Visual grounding aims to predict the locations of target objects specified by textual descriptions. For this task with linguistic and visual modalities, there is a latest research line that focuses on only selecting the linguistic-relevant visual regions for object localization to reduce the computational overhead. Albeit achieving impressive performance, it is iteratively performed on different image scales, and at every iteration, linguistic features and visual features need to be stored in a cache, incurring extra overhead. To facilitate the implementation, in this paper, we propose a feature selection-based simple yet effective baseline for visual grounding, called FSVG. Specifically, we directly encapsulate the linguistic and visual modalities into an overall network architecture without complicated iterative procedures, and utilize the language in parallel as guidance to facilitate the interaction between linguistic modal and visual modal for extracting effective visual features. Furthermore, to reduce the computational cost, during the visual feature learning, we introduce a similarity-based feature selection mechanism to only exploit language-related visual features for faster prediction. Extensive experiments conducted on several benchmark datasets comprehensively substantiate that the proposed FSVG achieves a better balance between accuracy and efficiency beyond the current state-of-the-art methods. Code is available at https://github.com/jcwang0602/FSVG.

**Link**: [arxiv](http://arxiv.org/abs/2510.10587v1),  [pdf](http://arxiv.org/pdf/2510.10587v1)

**Tags**: cs.CV 



### VORTA: Efficient Video Diffusion via Routing Sparse Attention
**Authors**: Wenhao Sun, Rong-Cheng Tu, Yifu Ding, Zhao Jin, Jingyi Liao, Shunyu Liu, Dacheng Tao

**Updated**: 2025-10-12T10:09:53Z

**Summary**: Video diffusion transformers have achieved remarkable progress in high-quality video generation, but remain computationally expensive due to the quadratic complexity of attention over high-dimensional video sequences. Recent acceleration methods enhance the efficiency by exploiting the local sparsity of attention scores; yet they often struggle with accelerating the long-range computation. To address this problem, we propose VORTA, an acceleration framework with two novel components: 1) a sparse attention mechanism that efficiently captures long-range dependencies, and 2) a routing strategy that adaptively replaces full 3D attention with specialized sparse attention variants. VORTA achieves an end-to-end speedup $1.76\times$ without loss of quality on VBench. Furthermore, it can seamlessly integrate with various other acceleration methods, such as model caching and step distillation, reaching up to speedup $14.41\times$ with negligible performance degradation. VORTA demonstrates its efficiency and enhances the practicality of video diffusion transformers in real-world settings. Codes and weights are available at https://github.com/wenhao728/VORTA.

**Link**: [arxiv](http://arxiv.org/abs/2505.18809v2),  [pdf](http://arxiv.org/pdf/2505.18809v2)

**Tags**: cs.CV 



### REFRAG: Rethinking RAG based Decoding
**Authors**: Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, Vijai Mohan

**Updated**: 2025-10-12T04:46:48Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, we demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.

**Link**: [arxiv](http://arxiv.org/abs/2509.01092v2),  [pdf](http://arxiv.org/pdf/2509.01092v2)

**Tags**: cs.CL cs.AI cs.LG 



### Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation   Linking
**Authors**: Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren

**Updated**: 2025-10-12T04:04:34Z

**Summary**: Large Language Models (LLMs) have achieved remarkable success across various domains, yet deploying them on mobile devices remains an arduous challenge due to their extensive computational and memory demands. While lightweight LLMs have been developed to fit mobile environments, they suffer from degraded model accuracy. In contrast, sparsity-based techniques minimize DRAM usage by selectively transferring only relevant neurons to DRAM while retaining the full model in external storage, such as flash. However, such approaches are critically limited by numerous I/O operations, particularly on smartphones with severe IOPS constraints.   In this paper, we propose Neuralink, a novel approach that accelerates LLM inference on smartphones by optimizing neuron placement in flash memory. Neuralink leverages the concept of Neuron Co-Activation, where neurons frequently activated together are linked to facilitate continuous read access and optimize I/O efficiency. Our approach incorporates a two-stage solution: an offline stage that reorganizes neuron placement based on co-activation patterns, and an online stage that employs tailored data access and caching strategies to align well with hardware characteristics. Evaluations conducted on a variety of smartphones and LLMs demonstrate that Neuralink achieves on average $1.49\times$ improvements in end-to-end latency compared to the state-of-the-art. As the first solution to optimize storage placement under sparsity, Neuralink explores a new optimization space at the intersection of sparsity-driven algorithm and storage-level system co-design for LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2410.19274v3),  [pdf](http://arxiv.org/pdf/2410.19274v3)

**Tags**: cs.LG cs.AI cs.OS cs.PF 



### Grounded AI for Code Review: Resource-Efficient Large-Model Serving in   Enterprise Pipelines
**Authors**: Sayan Mandal, Hua Jiang

**Updated**: 2025-10-11T17:08:45Z

**Summary**: Automated code review adoption lags in compliance-heavy settings, where static analyzers produce high-volume, low-rationale outputs, and naive LLM use risks hallucination and incurring cost overhead. We present a production system for grounded, PR-native review that pairs static-analysis findings with AST-guided context extraction and a single-GPU, on-demand serving stack (quantized open-weight model, multi-tier caching) to deliver concise explanations and remediation guidance. Evaluated on safety-oriented C/C++ standards, the approach achieves sub-minute median first-feedback (offline p50 build+LLM 59.8s) while maintaining competitive violation reduction and lower violation rates versus larger proprietary models. The architecture is decoupled: teams can adopt the grounding/prompting layer or the serving layer independently. A small internal survey (n=8) provides directional signals of reduced triage effort and moderate perceived grounding, with participants reporting fewer human review iterations. We outline operational lessons and limitations, emphasizing reproducibility, auditability, and pathways to broader standards and assisted patching.

**Link**: [arxiv](http://arxiv.org/abs/2510.10290v1),  [pdf](http://arxiv.org/pdf/2510.10290v1)

**Tags**: cs.SE cs.LG 



### Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc
**Authors**: Ruihao Li, Lizy K. John, Neeraja J. Yadwadkar

**Updated**: 2025-10-11T13:52:48Z

**Summary**: Memory allocators hide beneath nearly every application stack, yet their performance footprint extends far beyond their code size. Even small inefficiencies in the allocators ripple through caches and the rest of the memory hierarchy, collectively imposing what operators often call a "datacenter tax". At hyperscale, even a 1% improvement in allocator efficiency can unlock millions of dollars in savings and measurable reductions in datacenter energy consumption. Modern memory allocators are designed to optimize allocation speed and memory fragmentation in multi-threaded environments, relying on complex metadata and control logic to achieve high performance. However, the overhead introduced by this complexity prompts a reevaluation of allocator design. Notably, such overhead can be avoided in single-threaded scenarios, which continue to be widely used across diverse application domains.   In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built for single-threaded applications. By specializing for single-threaded execution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control flow, thereby reducing overhead and improving allocation efficiency. Its core design features include a centralized heap, a single free-block list, and a balanced strategy for memory commitment and relocation. Additionally, Exgen-Malloc incorporates design principles in modern multi-threaded allocators, which do not exist in legacy single-threaded allocators such as dlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both systems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over dlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In addition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory savings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2510.10219v1),  [pdf](http://arxiv.org/pdf/2510.10219v1)

**Tags**: cs.PL 



### CacheClip: Accelerating RAG with Effective KV Cache Reuse
**Authors**: Bin Yang, Qiuyu Leng, Jun Zeng, Zhenhua Wu

**Updated**: 2025-10-11T09:28:26Z

**Summary**: Retrieval-Augmented Generation (RAG) systems suffer from severe time-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV cache reuse methods face a fundamental trade-off: prefix caching requires identical prefixes that rarely occur in RAG scenarios, while direct precomputation sacrifices quality due to missing inter-chunk attention and repeated attention sinks. Recent methods like APE and CacheBlend partially address these issues but remain inadequate for robust RAG applications. This paper presents CacheClip, a novel framework that achieves both fast TTFT and high generation quality. Our key insight is that small auxiliary LLMs exhibit similar last-layer attention distributions to primary LLMs (the target model for generation), enabling efficient identification of tokens critical for restoring inter-chunk attention, thereby significantly improving response quality on cross-chunk reasoning tasks. CacheClip integrates three techniques: (1) auxiliary-model-guided token selection for selective KV cache recomputation, where the auxiliary model is finetuned to improve selection accuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3) grouping strategy to maintain local coherence during partial KV cache updates. Experiments show CacheClip retains up to 94.8% and 85.0% of full-attention performance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2% and 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM inference by up to 1.92x in prefill time, providing a practical solution to the efficiency-quality trade-off in RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.10129v1),  [pdf](http://arxiv.org/pdf/2510.10129v1)

**Tags**: cs.LG cs.AI 



### EpiCache: Episodic KV Cache Management for Long Conversational Question   Answering
**Authors**: Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho

**Updated**: 2025-10-11T09:04:23Z

**Summary**: Modern large language models (LLMs) extend context lengths to millions of tokens, enabling coherent, personalized responses grounded in long conversational histories. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly becomes the bottleneck in resource-constrained environments. An active line of research for reducing memory bottleneck is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting the KV cache after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to failure cases in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x compression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient multi-turn interaction under strict resource limits. Our code is available at https://github.com/apple/ml-epicache.

**Link**: [arxiv](http://arxiv.org/abs/2509.17396v3),  [pdf](http://arxiv.org/pdf/2509.17396v3)

**Tags**: cs.CL 



### PANTHER: Generative Pretraining Beyond Language for Sequential User   Behavior Modeling
**Authors**: Guilin Li, Yun Zhang, Xiuyuan Chen, Chengqi Li, Bo Wang, Linghe Kong, Wenjia Wang, Weiran Huang, Matthias Hwai Yong Tan

**Updated**: 2025-10-11T08:24:19Z

**Summary**: Large language models (LLMs) have shown that generative pretraining can distill vast world knowledge into compact token representations. While LLMs encapsulate extensive world knowledge, they remain limited in modeling the behavioral knowledge contained within user interaction histories. User behavior forms a distinct modality, where each action, defined by multi-dimensional attributes such as time, context, and transaction type, constitutes a behavioral token. Modeling these high-cardinality sequences is challenging, and discriminative models often falter under limited supervision. To bridge this gap, we extend generative pretraining to user behavior, learning transferable representations from unlabeled behavioral data analogous to how LLMs learn from text. We present PANTHER, a hybrid generative-discriminative framework that unifies user behavior pretraining and downstream adaptation, enabling large-scale sequential user representation learning and real-time inference. PANTHER introduces: (1) Structured Tokenization to compress multi-dimensional transaction attributes into an interpretable vocabulary; (2) Sequence Pattern Recognition Module (SPRM) for modeling periodic transaction motifs; (3) a Unified User-Profile Embedding that fuses static demographics with dynamic transaction histories; and (4) Real-time scalability enabled by offline caching of pretrained embeddings for millisecond-level inference. Fully deployed and operational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in next-transaction prediction HitRate@1 and a 38.6 percent relative improvement in fraud detection recall over baselines. Cross-domain evaluations on public benchmarks show strong generalization, achieving up to 21 percent HitRate@1 gains over transformer baselines, establishing PANTHER as a scalable, high-performance framework for industrial sequential user behavior modeling.

**Link**: [arxiv](http://arxiv.org/abs/2510.10102v1),  [pdf](http://arxiv.org/pdf/2510.10102v1)

**Tags**: cs.LG 



### HTTP Request Synchronization Defeats Discrepancy Attacks
**Authors**: Cem Topcuoglu, Kaan Onarlioglu, Steven Sprecher, Engin Kirda

**Updated**: 2025-10-11T01:42:38Z

**Summary**: Contemporary web application architectures involve many layers of proxy services that process traffic. Due to the complexity of HTTP and vendor design decisions, these proxies sometimes process a given request in different ways. Attackers can exploit these processing discrepancies to launch damaging attacks including web cache poisoning and request smuggling. Discrepancy attacks are surging, yet, there exists no systemic defense.   In this work, we propose the first comprehensive defense to address this problem, called HTTP Request Synchronization. Our scheme uses standard HTTP extension mechanisms to augment each request with a complete processing history. It propagates this context through the traffic path detailing how each server hop has processed said request. Using this history, every proxy server can validate that their processing is consistent with all previous hops, eliminating discrepancy attacks. We implement our scheme for 5 popular proxy technologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating its practical impact.

**Link**: [arxiv](http://arxiv.org/abs/2510.09952v1),  [pdf](http://arxiv.org/pdf/2510.09952v1)

**Tags**: cs.CR 



### Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem
**Authors**: Muhammad Maaz, Liam DeVoe, Zac Hatfield-Dodds, Nicholas Carlini

**Updated**: 2025-10-10T22:43:54Z

**Summary**: Property-based testing (PBT) is a lightweight formal method, typically implemented as a randomized testing framework. Users specify the input domain for their test using combinators supplied by the PBT framework, and the expected properties or invariants as a unit-test function. The framework then searches for a counterexample, e.g. by generating inputs and calling the test function. In this work, we demonstrate an LLM-based agent which analyzes Python modules, infers function-specific and cross-function properties from code and documentation, synthesizes and executes PBTs, reflects on outputs of these tests to confirm true bugs, and finally outputs actionable bug reports for the developer. We perform an extensive evaluation of our agent across 100 popular Python packages. Of the bug reports generated by the agent, we found after manual review that 56\% were valid bugs and 32\% were valid bugs that we would report to maintainers. We then developed a ranking rubric to surface high-priority valid bugs to developers, and found that of the 21 top-scoring bugs, 86\% were valid and 81\% we would report. The bugs span diverse failure modes from serialization failures to numerical precision errors to flawed cache implementations. We reported 5 bugs, 4 with patches, including to NumPy and cloud computing SDKs, with 3 patches merged successfully. Our results suggest that LLMs with PBT provides a rigorous and scalable method for autonomously testing software. Our code and artifacts are available at: https://github.com/mmaaz-git/agentic-pbt.

**Link**: [arxiv](http://arxiv.org/abs/2510.09907v1),  [pdf](http://arxiv.org/pdf/2510.09907v1)

**Tags**: cs.SE cs.AI 



### DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context   Reasoning
**Authors**: Hossein Entezari Zarch, Lei Gao, Chaoyi Jiang, Murali Annavarm

**Updated**: 2025-10-10T21:37:49Z

**Summary**: Large reasoning models (LRMs) achieve state-of-the-art performance on challenging benchmarks by generating long chains of intermediate steps, but their inference cost is dominated by decoding, where each new token must attend to the entire growing sequence. Existing sparse attention methods reduce computation by pruning the key-value (KV) cache, yet they suffer from severe accuracy degradation on reasoning tasks due to cumulative selection errors and the dynamic importance of tokens over long derivations. We present \textbf{DELTA}, a training-free sparse attention mechanism that achieves computational efficiency without sacrificing model accuracy. DELTA partitions transformer layers into three groups: initial layers that use full attention, a small set of \emph{selection layers} that identify salient tokens via aggregated head-level attention scores, and subsequent \emph{sparse-attention layers} that attend only to the selected subset. This design preserves the full KV cache in GPU memory for accuracy, while avoiding expensive full-attention computation over many layers. On reasoning benchmarks such as AIME and GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while reducing the number of attended tokens by up to $5\times$ and delivering $1.5\times$ end-to-end speedup. Our results show that selective reuse of intermediate attention maps offers a robust path toward efficient long-context reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.09883v1),  [pdf](http://arxiv.org/pdf/2510.09883v1)

**Tags**: cs.CL cs.LG 



### THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware   Resource Scheduling
**Authors**: Said Muhammad, Lahlou Laaziz, Nadjia Kara, Phat Tan Nguyen, Timothy Murphy

**Updated**: 2025-10-10T20:19:44Z

**Summary**: The dynamic adaptation of resource levels enables the system to enhance energy efficiency while maintaining the necessary computational resources, particularly in scenarios where workloads fluctuate significantly over time. The proposed approach can play a crucial role in heterogeneous systems where workload characteristics are not uniformly distributed, such as non-pinning tasks. The deployed THEAS algorithm in this research work ensures a balance between performance and power consumption, making it suitable for a wide range of real-time applications. A comparative analysis of the proposed THEAS algorithm with well-known scheduling techniques such as Completely Fair Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling (HeteroSched), and Utility-Based Scheduling is presented in Table III. Each scheme is compared based on adaptability, core selection criteria, performance scaling, cache awareness, overhead, and real-time suitability.

**Link**: [arxiv](http://arxiv.org/abs/2510.09847v1),  [pdf](http://arxiv.org/pdf/2510.09847v1)

**Tags**: cs.DC cs.PF 



### StreamingVLM: Real-Time Understanding for Infinite Video Streams
**Authors**: Ruyi Xu, Guangxuan Xiao, Yukang Chen, Liuning He, Kelly Peng, Yao Lu, Song Han

**Updated**: 2025-10-10T17:59:58Z

**Summary**: Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.

**Link**: [arxiv](http://arxiv.org/abs/2510.09608v1),  [pdf](http://arxiv.org/pdf/2510.09608v1)

**Tags**: cs.CV cs.AI cs.CL 



### DiskJoin: Large-scale Vector Similarity Join with SSD
**Authors**: Yanqi Chen, Xiao Yan, Alexandra Meliou, Eric Lo

**Updated**: 2025-10-10T16:56:23Z

**Summary**: Similarity join--a widely used operation in data science--finds all pairs of items that have distance smaller than a threshold. Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin, the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine. DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation. Our evaluation on real-world, large-scale datasets shows that DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x.

**Link**: [arxiv](http://arxiv.org/abs/2508.18494v2),  [pdf](http://arxiv.org/pdf/2508.18494v2)

**Tags**: cs.DB 



### Memory- and compute-optimized geometric multigrid GMGPolar for   curvilinear coordinate representations -- Applications to fusion plasma
**Authors**: Julian Litz, Philippe Leleux, Carola Kruse, Joscha Gedicke, Martin J. Kühn

**Updated**: 2025-10-10T16:08:26Z

**Summary**: Tokamak fusion reactors are actively studied as a means of realizing energy production from plasma fusion. However, due to the substantial cost and time required to construct fusion reactors and run physical experiments, numerical experiments are indispensable for understanding plasma physics inside tokamaks, supporting the design and engineering phase, and optimizing future reactor designs. Geometric multigrid methods are optimal solvers for many problems that arise from the discretization of partial differential equations. It has been shown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson equation in linear complexity and with only small memory requirements compared to other state-of-the-art solvers. In this paper, we present a completely refactored and object-oriented version of GMGPolar which offers two different matrix-free implementations. Among other things, we leverage the Sherman-Morrison formula to solve cyclic tridiagonal systems from circular line solvers without additional fill-in and we apply reordering to optimize cache access of circular and radial smoothing operations. With the Give approach, memory requirements are further reduced and speedups of four to seven are obtained for usual test cases. For the Take approach, speedups of 16 to 18 can be attained. In an additionally experimental setup of using GMGPolar as a preconditioner for conjugate gradients, this speedup could even be increased to factors between 25 and 37.

**Link**: [arxiv](http://arxiv.org/abs/2507.03812v2),  [pdf](http://arxiv.org/pdf/2507.03812v2)

**Tags**: cs.MS physics.plasm-ph 68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99 



### Efficient Autoregressive Inference for Transformer Probabilistic Models
**Authors**: Conor Hassan, Nasrulloh Loka, Cen-You Li, Daolang Huang, Paul E. Chang, Yang Yang, Francesco Silvestrin, Samuel Kaski, Luigi Acerbi

**Updated**: 2025-10-10T15:32:58Z

**Summary**: Transformer-based models for amortized probabilistic inference, such as neural processes, prior-fitted networks, and tabular foundation models, excel at single-pass marginal prediction. However, many real-world applications, from signal interpolation to multi-column tabular predictions, require coherent joint distributions that capture dependencies between predictions. While purely autoregressive architectures efficiently generate such distributions, they sacrifice the flexible set-conditioning that makes these models powerful for meta-learning. Conversely, the standard approach to obtain joint distributions from set-based models requires expensive re-encoding of the entire augmented conditioning set at each autoregressive step. We introduce a causal autoregressive buffer that preserves the advantages of both paradigms. Our approach decouples context encoding from updating the conditioning set. The model processes the context once and caches it. A dynamic buffer then captures target dependencies: as targets are incorporated, they enter the buffer and attend to both the cached context and previously buffered targets. This enables efficient batched autoregressive generation and one-pass joint log-likelihood evaluation. A unified training strategy allows seamless integration of set-based and autoregressive modes at minimal additional cost. Across synthetic functions, EEG signals, cognitive models, and tabular data, our method matches predictive accuracy of strong baselines while delivering up to 20 times faster joint sampling. Our approach combines the efficiency of autoregressive generative models with the representational power of set-based conditioning, making joint prediction practical for transformer-based probabilistic models.

**Link**: [arxiv](http://arxiv.org/abs/2510.09477v1),  [pdf](http://arxiv.org/pdf/2510.09477v1)

**Tags**: stat.ML cs.LG 



### 3C Resources Joint Allocation for Time-Deterministic Remote Sensing   Image Backhaul in the Space-Ground Integrated Network
**Authors**: Chongxiao Cai, Yan Zhu, Min Sheng, Jiandong Li, Yan Shi, Di Zhou, Ziwen Xie, Chen Zhang

**Updated**: 2025-10-10T14:03:42Z

**Summary**: Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to compress and backhaul more time-determined images (TDI) has become a new paradigm, which is used to enhance the timeout caused by the limited computing resources of OSs. However, how to capture the time-varying and dynamic characteristics of multi-dimensional resources is challenging for efficient collaborative scheduling. Motivated by this factor, we design a highly succinct multi-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically, by employing a slots division mechanism and introducing an external virtual node, the time-varying communication, caching, and computing (3C) resources are depicted in low complexity by the link weights within, between, and outside the slots. Based on the MDR-TEG, the maximizing successful transmission ratio of TDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem. Which further relaxed decomposed into two tractable sub-problems: maximizing the successful transmission rate of images (MSTRI) and ensuring the timeliness problem (ETP). Subsequently, an efficient subgradient of relaxation computing constraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI are obtained by solving the two subproblems and the dual problem (DP), and the direction of the next iteration is obtained by feedback. Furthermore, arranging the sending sequences of images to improve the quality of the solution. The approximate optimal solution of MSTR-TDI is eventually obtained through repeated iterations. The simulation results verify the superiority of the proposed MDR-TEG model and the effectiveness of the SRCC.

**Link**: [arxiv](http://arxiv.org/abs/2510.09409v1),  [pdf](http://arxiv.org/pdf/2510.09409v1)

**Tags**: eess.SY cs.IT cs.SY math.IT 



### Systematic Assessment of Cache Timing Vulnerabilities on RISC-V   Processors
**Authors**: Cédrick Austa, Jan Tobias Mühlberg, Jean-Michel Dricot

**Updated**: 2025-10-10T13:15:40Z

**Summary**: While interest in the open RISC-V instruction set architecture is growing, tools to assess the security of concrete processor implementations are lacking. There are dedicated tools and benchmarks for common microarchitectural side-channel vulnerabilities for popular processor families such as Intel x86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in porting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities to RISC-V. We then use this benchmark to evaluate the security of three commercially available RISC-V processors, the T-Head C910 and the SiFive U54 and U74 cores. We observe that the C910 processor exhibits more distinct timing types than the other processors, leading to the assumption that code running on the C910 would be exposed to more microarchitectural vulnerability sources. In addition, our evaluation reveals that $65.9\%$ of the vulnerabilities covered by the benchmark exist in all processors, while only $6.8\%$ are absent from all cores. Our work, in particular the ported benchmark, aims to support RISC-V processor designers to identify leakage sources early in their designs and to support the development of countermeasures.

**Link**: [arxiv](http://arxiv.org/abs/2510.08272v2),  [pdf](http://arxiv.org/pdf/2510.08272v2)

**Tags**: cs.CR 



### Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments   with a Hierarchical Spatial-Cognition Long-Short Memory System
**Authors**: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li

**Updated**: 2025-10-10T13:08:39Z

**Summary**: Vision-and-Language Navigation (VLN) in large-scale urban environments requires embodied agents to ground linguistic instructions in complex scenes and recall relevant experiences over extended time horizons. Prior modular pipelines offer interpretability but lack unified memory, while end-to-end (M)LLM agents excel at fusing vision and language yet remain constrained by fixed context windows and implicit spatial reasoning. We introduce \textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system that can augment any VLN backbone. Mem4Nav fuses a sparse octree for fine-grained voxel indexing with a semantic topology graph for high-level landmark connectivity, storing both in trainable memory tokens embedded via a reversible Transformer. Long-term memory (LTM) compresses and retains historical observations at both octree and graph nodes, while short-term memory (STM) caches recent multimodal entries in relative coordinates for real-time obstacle avoidance and local planning. At each step, STM retrieval sharply prunes dynamic context, and, when deeper history is needed, LTM tokens are decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13 pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW improvement. Ablations confirm the indispensability of both the hierarchical map and dual memory modules. Our codes are open-sourced via https://github.com/tsinghua-fib-lab/Mem4Nav.

**Link**: [arxiv](http://arxiv.org/abs/2506.19433v2),  [pdf](http://arxiv.org/pdf/2506.19433v2)

**Tags**: cs.CV cs.AI cs.CL 



### Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM   Inference
**Authors**: Jianuo Huang, Yaojie Zhang, Yicun Yang, Benhao Huang, Biqing Qi, Dongrui Liu, Linfeng Zhang

**Updated**: 2025-10-10T12:01:16Z

**Summary**: Diffusion large language models (dLLMs) present a promising alternative to dominant autoregressive models (ARMs) by the ability of parallel decoding at the expense of substantial computation and memory costs. Specifically, the cache mechanism for bidirectional attention in dLLMs demands large memory footprint, restricting their ability to handle long contexts under resource-limited settings. Existing cache eviction strategies are designed for ARMs and ignore the unique characteristics of dLLMs, thus leading to unsatisfactory performance. To address these challenges, we introduce MaskKV, a training-free cache eviction framework tailored to dLLMs, focusing on the effect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a mask-query guided scoring mechanism that leverages attention weights to identify and evict less critical prompt tokens for each head; (2) an adaptive cache budgeting strategy that improves efficiency by reducing allocation in intermediate layers and concentrating resources on prompt-preferring heads. On LLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of tokens) retains 94% of the full-cache performance on LongBench and achieves up to 31x acceleration at 32k prompt length. The code is publicly available at: https://github.com/jianuo-huang/MaskKV

**Link**: [arxiv](http://arxiv.org/abs/2510.09309v1),  [pdf](http://arxiv.org/pdf/2510.09309v1)

**Tags**: cs.CL 



### Online Video Depth Anything: Temporally-Consistent Depth Prediction with   Low Memory Consumption
**Authors**: Johann-Friedrich Feiden, Tim Küchler, Denis Zavadski, Bogdan Savchynskyy, Carsten Rother

**Updated**: 2025-10-10T09:24:53Z

**Summary**: Depth estimation from monocular video has become a key component of many real-world computer vision systems. Recently, Video Depth Anything (VDA) has demonstrated strong performance on long video sequences. However, it relies on batch-processing which prohibits its use in an online setting. In this work, we overcome this limitation and introduce online VDA (oVDA). The key innovation is to employ techniques from Large Language Models (LLMs), namely, caching latent features during inference and masking frames at training. Our oVDA method outperforms all competing online video depth estimation methods in both accuracy and VRAM usage. Low VRAM usage is particularly important for deployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an NVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release both, code and compilation scripts, making oVDA easy to deploy on low-power hardware.

**Link**: [arxiv](http://arxiv.org/abs/2510.09182v1),  [pdf](http://arxiv.org/pdf/2510.09182v1)

**Tags**: cs.CV 



### Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for   High-Power Applications
**Authors**: Tanjim Rahman, Trupti Ranjan Lenka

**Updated**: 2025-10-10T08:57:16Z

**Summary**: High Electron Mobility Transistors (HEMTs) are most suitable for harsh environments as they operate reliably under extreme conditions such as high voltages, high temperatures, radiation exposure and corrosive atmospheres. In this article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for achieving high breakdown voltage to reliably operate in harsh environments. The Al0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas) density of the order of 1013 cm-2 obtained from the self-consistent solution of Schr\"odinger and Poisson equations. The device has undergone DC and breakdown simulations which result in threshold voltage of -5.5 V, drain saturation current of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows excellent RF characteristics which include cut-off frequency (ft) of 28 GHz and maximum frequency of oscillation (fmax) of 38 GHz. The proposed gate field-plated HEMT is stable up to 40 GHz and suitable for high-voltage and high-power RF operation during harsh environment applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.09154v1),  [pdf](http://arxiv.org/pdf/2510.09154v1)

**Tags**: eess.SP 



### Lizard: An Efficient Linearization Framework for Large Language Models
**Authors**: Chien Van Nguyen, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Viet Dac Lai, Haoliang Wang, Jayakumar Subramanian, Ryan A. Rossi, Trung Bui, Nikos Vlassis, Franck Dernoncourt, Thien Huu Nguyen

**Updated**: 2025-10-09T20:37:43Z

**Summary**: We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into subquadratic architectures. Transformers faces severe computational and memory bottlenecks with long sequences due to the quadratic complexity of softmax attention and the growing Key-Value (KV) cache that makes inference memory-bound by context length. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving model quality. Unlike prior linearization methods constrained by fixed, non-adaptive structures, Lizard augments the architecture with compact, learnable modules that enable adaptive memory control and robust length generalization. Moreover, we introduce a hardwareaware algorithm that solves numerical instability in gated attention to accelerate training. Extensive experiments show that Lizard achieves near-lossless recovery of its teacher model's performance, significantly outperforming previous methods by up to 9.4 - 24.5 points on the 5-shot MMLU benchmark and demonstrating superior associative recall.

**Link**: [arxiv](http://arxiv.org/abs/2507.09025v3),  [pdf](http://arxiv.org/pdf/2507.09025v3)

**Tags**: cs.CL cs.LG 



### Man-Made Heuristics Are Dead. Long Live Code Generators!
**Authors**: Rohit Dwivedula, Divyanshu Saxena, Aditya Akella, Swarat Chaudhuri, Daehyeok Kim

**Updated**: 2025-10-09T20:35:00Z

**Summary**: Policy design for various systems controllers has conventionally been a manual process, with domain experts carefully tailoring heuristics for the specific instance in which the policy will be deployed. In this paper, we re-imagine policy design via a novel automated search technique fueled by recent advances in generative models, specifically Large Language Model (LLM)-driven code generation. We outline the design and implementation of PolicySmith, a framework that applies LLMs to synthesize instance-optimal heuristics. We apply PolicySmith to two long-standing systems policies - web caching and congestion control, highlighting the opportunities unraveled by this LLM-driven heuristic search. For caching, PolicySmith discovers heuristics that outperform established baselines on standard open-source traces. For congestion control, we show that PolicySmith can generate safe policies that integrate directly into the Linux kernel.

**Link**: [arxiv](http://arxiv.org/abs/2510.08803v1),  [pdf](http://arxiv.org/pdf/2510.08803v1)

**Tags**: cs.OS cs.DC cs.LG cs.NE 



### Struc-EMB: The Potential of Structure-Aware Encoding in Language   Embeddings
**Authors**: Shikun Liu, Haoyu Wang, Mufei Li, Pan Li

**Updated**: 2025-10-09T19:45:54Z

**Summary**: Text embeddings from Large Language Models (LLMs) have become foundational for numerous applications. However, these models typically operate on raw text, overlooking the rich structural information, such as hyperlinks or citations, that provides crucial context in many real-world datasets. This paper introduces and systematically evaluates a new paradigm for generating structure-aware text embeddings by integrating these structural relations directly into the LLM's internal encoding process, rather than relying on traditional post-hoc aggregation. We investigate two primary in-process methods: sequential concatenation and parallel caching. Through extensive zero-shot experiments across retrieval, clustering, classification, and recommendation tasks, we demonstrate that our structure-aware approaches consistently outperform both text-only and post-hoc baselines. Our analysis reveals critical trade-offs: sequential concatenation excels with noisy, moderate-length contexts, while parallel caching scales more effectively to long, high-signal contexts but is more susceptible to distractors. To address the challenge of noisy structural data, we also introduce and validate two effective techniques: Context Distillation and Semantic Balancing. This work provides the first comprehensive analysis of in-process structure-aware encoding, offering a blueprint for building more powerful and contextually aware embedding models.

**Link**: [arxiv](http://arxiv.org/abs/2510.08774v1),  [pdf](http://arxiv.org/pdf/2510.08774v1)

**Tags**: cs.LG cs.AI cs.CL 



### Which Heads Matter for Reasoning? RL-Guided KV Cache Compression
**Authors**: Wenjie Du, Li Jiang, Keda Tao, Xue Liu, Huan Wang

**Updated**: 2025-10-09T17:50:00Z

**Summary**: Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results.

**Link**: [arxiv](http://arxiv.org/abs/2510.08525v1),  [pdf](http://arxiv.org/pdf/2510.08525v1)

**Tags**: cs.CL 



### Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM   Step-Provers
**Authors**: Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, Xia Xiao

**Updated**: 2025-10-09T17:45:50Z

**Summary**: The integration of Large Language Models (LLMs) into automated theorem proving has shown immense promise, yet is fundamentally constrained by challenges in scaling up both training-time reinforcement learning (RL) and inference-time compute. This paper introduces \texttt{BFS-Prover-V2}, a system designed to address this dual scaling problem. We present two primary innovations. The first is a novel multi-turn off-policy RL framework for continually improving the performance of LLM step-prover at training time. This framework, inspired by the principles of AlphaZero, utilizes a multi-stage expert iteration pipeline featuring adaptive tactic-level data filtering and periodic retraining to surmount the performance plateaus that typically curtail long-term RL in LLM-based agents. The second innovation is a planner-enhanced multi-agent search architecture that scales reasoning capabilities at inference time. This architecture employs a general reasoning model as a high-level planner to iteratively decompose complex theorems into a sequence of simpler subgoals. This hierarchical approach substantially reduces the search space, enabling a team of parallel prover agents to collaborate efficiently by leveraging a shared proof cache. We demonstrate that this dual approach to scaling yields state-of-the-art results on established formal mathematics benchmarks. \texttt{BFS-Prover-V2} achieves 95.08\% and 41.4\% on the MiniF2F and ProofNet test sets respectively. While demonstrated in the domain of formal mathematics, the RL and inference techniques presented in this work are of broader interest and may be applied to other domains requiring long-horizon multi-turn reasoning and complex search.

**Link**: [arxiv](http://arxiv.org/abs/2509.06493v2),  [pdf](http://arxiv.org/pdf/2509.06493v2)

**Tags**: cs.AI 



### Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative   Decoding
**Authors**: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli

**Updated**: 2025-10-09T17:38:52Z

**Summary**: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.

**Link**: [arxiv](http://arxiv.org/abs/2509.18085v2),  [pdf](http://arxiv.org/pdf/2509.18085v2)

**Tags**: cs.LG cs.AI cs.CL 



### FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching
**Authors**: Jiacheng Liu, Peiliang Cai, Qinming Zhou, Yuqi Lin, Deyang Kong, Benhao Huang, Yupei Pan, Haowen Xu, Chang Zou, Junshu Tang, Shikang Zheng, Linfeng Zhang

**Updated**: 2025-10-09T17:22:23Z

**Summary**: The application of diffusion transformers is suffering from their significant inference costs. Recently, feature caching has been proposed to solve this problem by reusing features from previous timesteps, thereby skipping computation in future timesteps. However, previous feature caching assumes that features in adjacent timesteps are similar or continuous, which does not always hold in all settings. To investigate this, this paper begins with an analysis from the frequency domain, which reveal that different frequency bands in the features of diffusion models exhibit different dynamics across timesteps. Concretely, low-frequency components, which decide the structure of images, exhibit higher similarity but poor continuity. In contrast, the high-frequency bands, which decode the details of images, show significant continuity but poor similarity. These interesting observations motivate us to propose Frequency-aware Caching (FreqCa)   which directly reuses features of low-frequency components based on their similarity, while using a second-order Hermite interpolator to predict the volatile high-frequency ones based on its continuity.   Besides, we further propose to cache Cumulative Residual Feature (CRF) instead of the features in all the layers, which reduces the memory footprint of feature caching by 99%.   Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and Qwen-Image-Edit demonstrate its effectiveness in both generation and editing. Codes are available in the supplementary materials and will be released on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2510.08669v1),  [pdf](http://arxiv.org/pdf/2510.08669v1)

**Tags**: cs.LG cs.AI cs.CV 



### FMCache: File-System Metadata Caching in Programmable Switches
**Authors**: Qingxiu Liu, Jiazhen Cai, Siyuan Sheng, Yuhui Chen, Lu Tang, Zhirong Shen, Patrick P. C. Lee

**Updated**: 2025-10-09T15:38:13Z

**Summary**: Fast and scalable metadata management across multiple metadata servers is crucial for distributed file systems to handle numerous files and directories. Client-side caching of frequently accessed metadata can mitigate server loads, but incurs significant overhead and complexity in maintaining cache consistency when the number of clients increases. We propose FMCache, an in-switch file-system metadata caching framework that leverages programmable switches to serve file-system metadata requests from multiple clients directly in the switch data plane. Unlike prior in-switch key-value caching approaches, FMCache addresses file-system-specific path dependencies under stringent switch resource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on a Tofino-switch testbed using real-world file-system metadata workloads. FMCache achieves up to 181.6% higher throughput than vanilla HDFS and complements client-side caching with additional throughput gains of up to 139.6%. It also incurs low latencies and limited switch resource usage.

**Link**: [arxiv](http://arxiv.org/abs/2510.08351v1),  [pdf](http://arxiv.org/pdf/2510.08351v1)

**Tags**: cs.AR 



### Towards Energy-Efficient Serverless Computing with Hardware Isolation
**Authors**: Natalie Carl, Tobias Pfandzelter, David Bermbach

**Updated**: 2025-10-09T13:06:16Z

**Summary**: Serverless computing provides just-in-time infrastructure provisioning with rapid elasticity and a finely-grained pricing model. As full control of resource allocation is in the hands of the cloud provider and applications only consume resources when they actually perform work, we believe that serverless computing is uniquely positioned to maximize energy efficiency.   However, the focus of current serverless platforms is to run hundreds or thousands of serverless functions from different tenants on traditional server hardware, requiring expensive software isolation mechanisms and a high degree of overprovisioning, i.e., idle servers, to anticipate load spikes. With shared caches, high clock frequencies, and many-core architectures, servers today are optimized for large, singular workloads but not to run thousands of isolated functions.   We propose rethinking the serverless hardware architecture to align it with the requirements of serverless software. Specifically, we propose using hardware isolation with individual processors per function instead of software isolation resulting in a serverless hardware stack that consumes energy only when an application actually performs work. In preliminary evaluation with real hardware and a typical serverless workload we find that this could reduce energy consumption overheads by 90.63% or an average 70.8MW.

**Link**: [arxiv](http://arxiv.org/abs/2510.08180v1),  [pdf](http://arxiv.org/pdf/2510.08180v1)

**Tags**: cs.DC 



### TASP: Topology-aware Sequence Parallelism
**Authors**: Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang

**Updated**: 2025-10-09T13:03:29Z

**Summary**: Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.

**Link**: [arxiv](http://arxiv.org/abs/2509.26541v2),  [pdf](http://arxiv.org/pdf/2509.26541v2)

**Tags**: cs.LG cs.DC 



### TokenSelect: Efficient Long-Context Inference and Length Extrapolation   for LLMs via Dynamic Token-Level KV Cache Selection
**Authors**: Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong

**Updated**: 2025-10-09T12:05:04Z

**Summary**: Rapid advances in Large Language Models (LLMs) have spurred demand for processing extended context sequences in contemporary applications. However, this progress faces two challenges: performance degradation due to sequence lengths out-of-distribution, and excessively long inference times caused by the quadratic computational complexity of attention. These issues limit LLMs in long-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free method for efficient and accurate long-context inference. TokenSelect builds upon the observation of non-contiguous attention sparsity, using QK dot products to measure per-head KV Cache criticality at token-level. By per-head soft voting mechanism, TokenSelect selectively involves a few critical KV cache tokens in attention calculation without sacrificing accuracy. To further accelerate TokenSelect, we design the Selection Cache based on observations of consecutive Query similarity and implemented the efficient Paged Dot Product Kernel, significantly reducing the selection overhead. A comprehensive evaluation of TokenSelect demonstrates up to $23.84\times$ speedup in attention computation and up to $2.28\times$ acceleration in end-to-end latency, while providing superior performance compared to state-of-the-art long-context inference methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.02886v4),  [pdf](http://arxiv.org/pdf/2411.02886v4)

**Tags**: cs.CL cs.AI cs.LG 



### Panorama: Fast-Track Nearest Neighbors
**Authors**: Vansh Ramani, Alexis Schlomer, Akash Nayar, Panagiotis Karras, Sayan Ranu, Jignesh M. Patel

**Updated**: 2025-10-09T12:01:20Z

**Summary**: Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99\% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90\% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\times$ end-to-end speedup with no recall loss.

**Link**: [arxiv](http://arxiv.org/abs/2510.00566v2),  [pdf](http://arxiv.org/pdf/2510.00566v2)

**Tags**: cs.LG cs.AI cs.DB 



### Spotlight Attention: Towards Efficient LLM Generation via Non-linear   Hashing-based KV Cache Retrieval
**Authors**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji

**Updated**: 2025-10-09T09:33:47Z

**Summary**: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.

**Link**: [arxiv](http://arxiv.org/abs/2508.19740v4),  [pdf](http://arxiv.org/pdf/2508.19740v4)

**Tags**: cs.CL 



### Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent   Metasurfaces
**Authors**: Geng Sun, Mingzhe Fan, Lei Zhang, Hongyang Pan, Jiahui Li, Chuang Zhang, Linyao Li, Changyuan Zhao, Chau Yuen

**Updated**: 2025-10-09T09:14:43Z

**Summary**: Wireless communication systems face challenges in meeting the demand for higher data rates and reliable connectivity in complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for advanced wave-domain signal processing, where mobile SIMs can outperform fixed counterparts. In this paper, we propose a novel unmanned aerial vehicle (UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude economy (LAE) networks, where UAVs act as both cache-enabled base stations and mobile SIM carriers to enhance uplink transmissions. To maximize network capacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that integrates user association, UAV-SIM three-dimensional positioning, and multi-layer SIM phase shift design. Due to the non-convexity and NP-hardness of USBJOP, we decompose it into three subproblems, which are the association between UAV-SIMs and users optimization problem (AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase shifts optimization problem (USPSOP). Then, we solve them through an alternating optimization strategy. Specifically, AUUOP and ULOP are transformed into convex forms solvable via the CVX tool, while USPSOP is addressed by a generative artificial intelligence (GAI)-based hybrid optimization algorithm. Simulation results show that the proposed approach achieves approximately 1.5 times higher network capacity compared with suboptimal schemes, effectively mitigates multi-user interference with increasing SIM layers and meta-atoms, and reduces runtime by 10\% while maintaining solution quality, thereby demonstrating its practicality for real-world deployments.

**Link**: [arxiv](http://arxiv.org/abs/2506.23488v2),  [pdf](http://arxiv.org/pdf/2506.23488v2)

**Tags**: cs.NI 



### LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs
**Authors**: Fatemeh Hadadi, Qinghua Xu, Domenico Bianculli, Lionel Briand

**Updated**: 2025-10-09T02:37:26Z

**Summary**: Most log-based anomaly detectors assume logs are stable, though logs are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that combines ML models -- decision tree, k-nearest neighbors, and a feedforward neural network -- with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we configured four datasets for \task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points (pp) in F1 score while using much less labeled data (62.87 pp reduction). When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 pp increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications, except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog's key components: cache, RAG and ensemble learning.

**Link**: [arxiv](http://arxiv.org/abs/2406.07467v4),  [pdf](http://arxiv.org/pdf/2406.07467v4)

**Tags**: cs.SE 



### FlashDLM: Accelerating Diffusion Language Model Inference via Efficient   KV Caching and Guided Diffusion
**Authors**: Zhanqiu Hu, Jian Meng, Yash Akhauri, Mohamed S. Abdelfattah, Jae-sun Seo, Zhiru Zhang, Udit Gupta

**Updated**: 2025-10-09T01:43:02Z

**Summary**: Diffusion language models offer parallel token generation and inherent bidirectionality, promising more efficient and powerful sequence modeling compared to autoregressive approaches. However, state-of-the-art diffusion models (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match the quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B, Llama3 8B), their iterative denoising requires multiple full-sequence forward passes, resulting in high computational costs and latency, particularly for long input prompts and long-context scenarios. Furthermore, parallel token generation introduces token incoherence problems, and current sampling heuristics suffer from significant quality drops with decreasing denoising steps. We address these limitations with two training-free techniques. First, we propose FreeCache, a Key-Value (KV) approximation caching technique that reuses stable KV projections across denoising steps, effectively reducing the computational cost of DLM inference. Second, we introduce Guided Diffusion, a training-free method that uses a lightweight pretrained autoregressive model to supervise token unmasking, dramatically reducing the total number of denoising iterations without sacrificing quality. We conduct extensive evaluations on open-source reasoning benchmarks, and our combined methods deliver an average of 12.14x end-to-end speedup across various tasks with negligible accuracy degradation. For the first time, diffusion language models achieve a comparable and even faster latency as the widely adopted autoregressive models. Our work successfully paved the way for scaling up the diffusion language model to a broader scope of applications across different domains.

**Link**: [arxiv](http://arxiv.org/abs/2505.21467v2),  [pdf](http://arxiv.org/pdf/2505.21467v2)

**Tags**: cs.CL 



### An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit   Data Reuse Strategies
**Authors**: Binzhe Yuan, Xiangyu Zhang, Zeyu Zheng, Yuefeng Zhang, Haochuan Wan, Zhechen Yuan, Junsheng Chen, Yunxiang He, Junran Ding, Xiaoming Zhang, Chaolin Rao, Wenyan Su, Pingqiang Zhou, Jingyi Yu, Xin Lou

**Updated**: 2025-10-09T01:40:39Z

**Summary**: Neural radiance fields (NeRF) have transformed 3D reconstruction and rendering, facilitating photorealistic image synthesis from sparse viewpoints. This work introduces an explicit data reuse neural rendering (EDR-NR) architecture, which reduces frequent external memory accesses (EMAs) and cache misses by exploiting the spatial locality from three phases, including rays, ray packets (RPs), and samples. The EDR-NR architecture features a four-stage scheduler that clusters rays on the basis of Z-order, prioritize lagging rays when ray divergence happens, reorders RPs based on spatial proximity, and issues samples out-of-orderly (OoO) according to the availability of on-chip feature data. In addition, a four-tier hierarchical RP marching (HRM) technique is integrated with an axis-aligned bounding box (AABB) to facilitate spatial skipping (SS), reducing redundant computations and improving throughput. Moreover, a balanced allocation strategy for feature storage is proposed to mitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area of 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized energy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X increase in normalized throughput, and a 53.42% reduction in on-chip SRAM consumption compared to state-of-the-art accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2510.07667v1),  [pdf](http://arxiv.org/pdf/2510.07667v1)

**Tags**: eess.IV 



### OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM   Inference
**Authors**: Yuzhe Gu, Xiyu Liang, Jiaojiao Zhao, Enmao Diao

**Updated**: 2025-10-09T00:58:28Z

**Summary**: Large language models (LLMs) with extended context windows enable powerful downstream applications but impose significant memory overhead, as caching all key-value (KV) states scales linearly with sequence length and batch size. Existing cache eviction methods address this by exploiting attention sparsity, yet they typically rank tokens heuristically using accumulated attention weights without considering their true impact on attention outputs. We propose Optimal Brain Cache (OBCache), a principled framework that formulates cache eviction as a layer-wise structured pruning problem. Building upon the Optimal Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the perturbation in attention outputs induced by pruning tokens, with closed-form scores derived for isolated keys, isolated values, and joint key-value pairs. Our scores account not only for attention weights but also for information from value states and attention outputs, thereby enhancing existing eviction strategies with output-aware signals. Experiments on LLaMA and Qwen models demonstrate that replacing the heuristic scores in existing works, which estimate token saliency across different query positions, with OBCache's output-aware scores consistently improves long-context accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2510.07651v1),  [pdf](http://arxiv.org/pdf/2510.07651v1)

**Tags**: cs.CL cs.AI 



### When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs
**Authors**: Soyeong Jeong, Taehee Jung, Sung Ju Hwang, Joo-Kyung Kim, Dongyeop Kang

**Updated**: 2025-10-08T19:52:35Z

**Summary**: Recent Long-Context Language Models (LCLMs) can process hundreds of thousands of tokens in a single prompt, enabling new opportunities for knowledge-intensive multi-hop reasoning by integrating large sets of retrieved documents or, in some cases, directly all necessary information. However, simply feeding more documents into the context window fails to capture how evidence should be connected. We address this gap with thought templates, which recast reasoning as reusable thought caches, derived from prior problem solving traces, structuring how evidence is combined and guiding multi-hop inference with factual documents. To keep these templates effective, we propose an update strategy that iteratively refines templates derived from training data through natural-language feedback. Across diverse benchmarks and LCLM families, our approach delivers consistent gains over strong baselines in both retrieval-based and retrieval-free settings. Furthermore, we show that optimized templates can be distilled into smaller open-source models, demonstrating its broad applicability and transparent reasoning reuse. We refer to our framework as Thought Template Augmented LCLMs (ToTAL).

**Link**: [arxiv](http://arxiv.org/abs/2510.07499v1),  [pdf](http://arxiv.org/pdf/2510.07499v1)

**Tags**: cs.CL cs.AI cs.LG 



### AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse   Decoding
**Authors**: Shuqing Luo, Yilin Guan, Pingzhi Li, Hanrui Wang, Tianlong Chen

**Updated**: 2025-10-08T19:36:11Z

**Summary**: Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT), but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM decoding. Query-aware page-level sparse decoding can achieve state-of-the-art performance under constrained FLOPs budgets, but is limited by both sequential-dependent page filtering and coarse-grained token selection, hampering serving efficiency and model performance on TTS tasks under high concurrency and long CoT scenarios (consuming even higher runtime than the forward pipeline itself). In this paper, we first find that the current-step query state can be accurately approximated in a unified manner from a short window of recent queries, enabling training-free query-aware sparsity without waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework for efficient TTS built on two core components: (1) a novel light-weight temporal-regressive module that predicts the next-token query state; (2) an asynchronous and disaggregated framework that decouples the KV cache filtering from the auto-regressive decoding loop, overlapping the token-level KV selection with the forward inference computation through asynchronism. To our knowledge, AsyncSpade is the first to eliminate the sequential dependence without sacrificing model performance. We validate the effectiveness of AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade fully overlaps KV-cache operations with the inference pipeline, achieving theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and at least 50% TPOT reduction compared to full attention on Qwen3-8B and Qwen3-32B models, while matching or surpassing their accuracy on various TTS benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

**Link**: [arxiv](http://arxiv.org/abs/2510.07486v1),  [pdf](http://arxiv.org/pdf/2510.07486v1)

**Tags**: cs.CL 



### Speculate Deep and Accurate: Lossless and Training-Free Acceleration for   Offloaded LLMs via Substitute Speculative Decoding
**Authors**: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu

**Updated**: 2025-10-08T18:16:04Z

**Summary**: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

**Link**: [arxiv](http://arxiv.org/abs/2509.18344v2),  [pdf](http://arxiv.org/pdf/2509.18344v2)

**Tags**: cs.CL 



### Artificial Hippocampus Networks for Efficient Long-Context Modeling
**Authors**: Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei

**Updated**: 2025-10-08T17:59:55Z

**Summary**: Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.

**Link**: [arxiv](http://arxiv.org/abs/2510.07318v1),  [pdf](http://arxiv.org/pdf/2510.07318v1)

**Tags**: cs.CL cs.AI cs.LG 



### Agentic generative AI for media content discovery at the national   football league
**Authors**: Henry Wang, Md Sirajus Salekin, Jake Lee, Ross Claytor, Shinan Zhang, Michael Chi

**Updated**: 2025-10-08T17:51:34Z

**Summary**: Generative AI has unlocked new possibilities in content discovery and management. Through collaboration with the National Football League (NFL), we demonstrate how a generative-AI based workflow enables media researchers and analysts to query relevant historical plays using natural language rather than traditional filter-and-click interfaces. The agentic workflow takes a user query as input, breaks it into elements, and translates them into the underlying database query language. Accuracy and latency are further improved through carefully designed semantic caching. The solution achieves over 95 percent accuracy and reduces the average time to find relevant videos from 10 minutes to 30 seconds, significantly increasing the NFL's operational efficiency and allowing users to focus on producing creative content and engaging storylines.

**Link**: [arxiv](http://arxiv.org/abs/2510.07297v1),  [pdf](http://arxiv.org/pdf/2510.07297v1)

**Tags**: cs.AI 



### AudioMarathon: A Comprehensive Benchmark for Long-Context Audio   Understanding and Efficiency in Audio LLMs
**Authors**: Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang

**Updated**: 2025-10-08T17:50:16Z

**Summary**: Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.

**Link**: [arxiv](http://arxiv.org/abs/2510.07293v1),  [pdf](http://arxiv.org/pdf/2510.07293v1)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference
**Authors**: Yihua Cheng, Yuhan Liu, Jiayi Yao, Yuwei An, Xiaokun Chen, Shaoting Feng, Yuyang Huang, Samuel Shen, Kuntai Du, Junchen Jiang

**Updated**: 2025-10-08T00:15:04Z

**Summary**: Today's LLM inference systems treat individual engines and queries independently for simplicity, but this causes significant resource inefficiencies. While there are proposals to avoid redundant computation by reusing KV caches across queries and to increase GPU utilization by disaggregating a single query to different engines, their promises cannot be realized without efficiently offloading and communicating KV cache across LLM inference engines and queries.   We present LMCache, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) and shares the KV caches across engines and queries. LMCache exposes KV caches in the LLM engine interface, effectively transforming LLM engines from individual token processors to a collection of engines with KV cache as the storage and communication medium. In particular, it supports both cache offloading (prefix reuse across queries) and prefill-decode disaggregation (cross-engine cache transfer). LMCache's high performance and wide adoption stem from the following contributions: highly optimized KV cache data movement with performance optimizations including batched data movement operations, compute and I/O pipelining; a modular KV cache connector component, decoupling LMCache from the rapid evolution of inference engines; a first-class control API, such as pinning, lookup, cleanup, movement, and compression, for flexible cache orchestration across GPU, CPU, storage, and network layers. Evaluation shows that combining LMCache with vLLM achieves up to 15x improvement in throughput across diverse workloads. With a growing community, LMCache has seen dramatic growth in adoption by enterprise inference systems, which provides valuable lessons for future KV caching solutions. The source code of LMCache is at: https://github.com/LMCache/LMCache.

**Link**: [arxiv](http://arxiv.org/abs/2510.09665v1),  [pdf](http://arxiv.org/pdf/2510.09665v1)

**Tags**: cs.LG 



### FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via   Isolated Key-Value Cache Management
**Authors**: Xiang Liu, Hong Chen, Xuming Hu, Xiaowen Chu

**Updated**: 2025-10-08T00:06:52Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in multi-turn conversational applications, where the management of the Key-Value (KV) Cache presents a significant bottleneck. The linear growth of the KV Cache with dialogue history imposes substantial computational costs, and existing eviction strategies often degrade performance by repeatedly compressing early conversational context, leading to information loss and context forgetting. This paper introduces FlowKV, a novel \textbf{multi-turn isolation mechanism} for KV Cache management, which can be applied to any KV Cache compression method without training. FlowKV's core innovation is a multi-turn isolation mechanism that preserves the accumulated compressed KV cache from past turns. Compression is then strategically applied only to the newly generated KV pairs of the latest completed turn, effectively preventing the re-compression of older context and thereby mitigating catastrophic forgetting. Our results demonstrate that FlowKV consistently and significantly outperforms baseline strategies in maintaining instruction-following accuracy and user preference retention from 10.90\% to 75.40\%, particularly in later conversational turns.

**Link**: [arxiv](http://arxiv.org/abs/2505.15347v2),  [pdf](http://arxiv.org/pdf/2505.15347v2)

**Tags**: cs.CL 



### SuffixDecoding: Extreme Speculative Decoding for Emerging AI   Applications
**Authors**: Gabriele Oliaro, Zhihao Jia, Daniel Campos, Aurick Qiao

**Updated**: 2025-10-07T22:07:44Z

**Summary**: Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 5.3$\times$, outperforming state-of-the-art methods -- 2.8$\times$ faster than model-based approaches like EAGLE-2/3 and 1.9$\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced at https://github.com/snowflakedb/ArcticInference

**Link**: [arxiv](http://arxiv.org/abs/2411.04975v3),  [pdf](http://arxiv.org/pdf/2411.04975v3)

**Tags**: cs.CL cs.AI cs.DC cs.LG 



### Enhanced Breakdown Voltage in $β$-Ga$_2$O$_3$ Schottky Barrier   Diodes via Fast Neutron Irradiation and Electrothermal Annealing
**Authors**: Saleh Ahmed Khan, Sudipto Saha, Ahmed Ibreljic, Stephen Margiotta, Jiawei Liu, Walid Amir, Surajit Chakraborty, Uttam Singisetti, A F M Anhar Uddin Bhuiyan

**Updated**: 2025-10-07T19:50:52Z

**Summary**: This study investigates the impact of fast neutron irradiation and post-radiation electro-thermal annealing on the electrical performance of $\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation, including a drastic reduction in on-current and an increase in on-resistance. Electrothermal testing, conducted through simultaneous current-voltage (J-V) measurements and thermal annealing, resulted in significant recovery. After four cycles of electro-thermal testing, the devices demonstrated significant improvements in performance, with a substantial recovery of on-current and a reduction in on-resistance compared to the post-radiation condition, approaching pre-radiation levels. Most recovery occurred during the first two cycles, with diminishing improvements in later cycles, indicating that most thermally recoverable traps were mitigated early. Capacitance-voltage (C-V) measurements revealed a substantial reduction in carrier concentration, decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first electro-thermal testing cycle, indicating an over 82% reduction. Following the third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3, reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a ~325% improvement) after the first electro-thermal testing, attributed to the reduction in carrier concentration by compensating radiation-induced traps. Subsequent testing reduced breakdown voltage slightly to 940 V due to partial recovery of carrier concentration, but it remained significantly higher than pre-radiation levels, highlighting the promise of $\beta$-Ga$_2$O$_3$ power devices for high-power applications in radiation-intense environments.

**Link**: [arxiv](http://arxiv.org/abs/2510.06415v1),  [pdf](http://arxiv.org/pdf/2510.06415v1)

**Tags**: physics.app-ph 



### VecInfer: Efficient LLM Inference with Low-Bit KV Cache via   Outlier-Suppressed Vector Quantization
**Authors**: Dingyu Yao, Chenxu Yang, Zhengyang Tong, Zheng Lin, Wei Liu, Jian Luan, Weiping Wang

**Updated**: 2025-10-07T17:35:28Z

**Summary**: The Key-Value (KV) cache introduces substantial memory overhead during large language model (LLM) inference. Although existing vector quantization (VQ) methods reduce KV cache usage and provide flexible representational capacity across bit-widths, they suffer severe performance degradation at ultra-low bit-widths due to key cache outliers that hinder effective codebook utilization. To address this challenge, we propose VecInfer, a novel VQ method for aggressive KV cache compression while enabling efficient inference. By applying smooth and Hadamard transformations, VecInfer suppresses outliers in the key cache, enabling the codebook to comprehensively cover the original data distribution and thereby reducing quantization difficulty. To facilitate efficient deployment, we design an optimized CUDA kernel that fuses computation with dequantization to minimize memory access overhead. Extensive evaluations demonstrate that VecInfer consistently outperforms existing quantization baselines across both long-context understanding and mathematical reasoning tasks. With only 2-bit quantization, VecInfer achieves performance comparable to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.

**Link**: [arxiv](http://arxiv.org/abs/2510.06175v1),  [pdf](http://arxiv.org/pdf/2510.06175v1)

**Tags**: cs.CL 



### On Enhancing Delay SLAs in TCP Networks through Joint Routing and   Transport Assistant Deployment
**Authors**: José Gómez-delaHiz, Mohamed Faten Zhani, Jaime Galán-Jiménez, John Kaippallimalil

**Updated**: 2025-10-07T08:43:07Z

**Summary**: The Transport Control Protocol has long been the primary transport protocol for applications requiring performance and reliability over the Internet. Unfortunately, due its retransmission mechanism, TCP incurs high packet delivery delays when segments are lost. To address this issue, previous research proposed to use a novel network function, namely Transport Assistant, deployed within the network to cache and retransmit lost packets, thus reducing retransmission delays. In this paper, we propose to jointly route the flows and deploy TAs in order to minimize packet delivery delays in best-effort networks (scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based networks (scenario 2). We hence formulate the joint routing and TA deployment problem as Integer Linear Program for the two scenarios and propose a heuristic solution for large-scale instances of the problem. Through extensive simulations, we demonstrate the benefits of performing joint routing flows and TA deployment in reducing packet delivery delays (up to 16.4%) while minimizing deployment costs (up to 60.98%).

**Link**: [arxiv](http://arxiv.org/abs/2510.05686v1),  [pdf](http://arxiv.org/pdf/2510.05686v1)

**Tags**: cs.NI 



## Keyword: LLM Inference 
 ### Coupled Diffusion Sampling for Training-Free Multi-View Image Editing
**Authors**: Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu

**Updated**: 2025-10-16T17:59:59Z

**Summary**: We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.

**Link**: [arxiv](http://arxiv.org/abs/2510.14981v1),  [pdf](http://arxiv.org/pdf/2510.14981v1)

**Tags**: cs.CV cs.AI 



### Agentic Design of Compositional Machines
**Authors**: Wenqian Zhang, Weiyang Liu, Zhen Liu

**Updated**: 2025-10-16T17:59:58Z

**Summary**: The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.14980v1),  [pdf](http://arxiv.org/pdf/2510.14980v1)

**Tags**: cs.AI cs.CL cs.CV cs.GR cs.LG 



### Ponimator: Unfolding Interactive Pose for Versatile Human-human   Interaction Animation
**Authors**: Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang

**Updated**: 2025-10-16T17:59:56Z

**Summary**: Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.

**Link**: [arxiv](http://arxiv.org/abs/2510.14976v1),  [pdf](http://arxiv.org/pdf/2510.14976v1)

**Tags**: cs.CV cs.GR cs.RO 



### Attention Is All You Need for KV Cache in Diffusion LLMs
**Authors**: Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen

**Updated**: 2025-10-16T17:59:48Z

**Summary**: This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences, and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14973v1),  [pdf](http://arxiv.org/pdf/2510.14973v1)

**Tags**: cs.CL cs.AI cs.LG 



### TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar
**Authors**: Yinxi Li, Yuntian Deng, Pengyu Nie

**Updated**: 2025-10-16T17:59:45Z

**Summary**: Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14972v1),  [pdf](http://arxiv.org/pdf/2510.14972v1)

**Tags**: cs.CL cs.AI cs.LG cs.PL cs.SE 



### LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent   Training
**Authors**: Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang

**Updated**: 2025-10-16T17:59:38Z

**Summary**: Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.

**Link**: [arxiv](http://arxiv.org/abs/2510.14969v1),  [pdf](http://arxiv.org/pdf/2510.14969v1)

**Tags**: cs.CL cs.AI cs.LG 



### Biology-informed neural networks learn nonlinear representations from   omics data to improve genomic prediction and interpretability
**Authors**: Katiana Kontolati, Rini Jasmine Gladstone, Ian Davis, Ethan Pickering

**Updated**: 2025-10-16T17:59:38Z

**Summary**: We extend biologically-informed neural networks (BINNs) for genomic prediction (GP) and selection (GS) in crops by integrating thousands of single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior biological knowledge. Traditional genotype-to-phenotype (G2P) models depend heavily on direct mappings that achieve only modest accuracy, forcing breeders to conduct large, costly field trials to maintain or marginally improve genetic gain. Models that incorporate intermediate molecular phenotypes such as gene expression can achieve higher predictive fit, but they remain impractical for GS since such data are unavailable at deployment or design time. BINNs overcome this limitation by encoding pathway-level inductive biases and leveraging multi-omics data only during training, while using genotype data alone during inference. Applied to maize gene-expression and multi-environment field-trial data, BINN improves rank-correlation accuracy by up to 56% within and across subpopulations under sparse-data conditions and nonlinearly identifies genes that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic metabolomics benchmark, BINN reduces prediction error by 75% relative to conventional neural nets and correctly identifies the most important nonlinear pathway. Importantly, both cases show highly sensitive BINN latent variables correlate with the experimental quantities they represent, despite not being trained on them. This suggests BINNs learn biologically-relevant representations, nonlinear or linear, from genotype to phenotype. Together, BINNs establish a framework that leverages intermediate domain information to improve genomic prediction accuracy and reveal nonlinear biological relationships that can guide genomic selection, candidate gene selection, pathway enrichment, and gene-editing prioritization.

**Link**: [arxiv](http://arxiv.org/abs/2510.14970v1),  [pdf](http://arxiv.org/pdf/2510.14970v1)

**Tags**: cs.LG 



### Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents
**Authors**: Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying

**Updated**: 2025-10-16T17:59:32Z

**Summary**: Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2510.14967v1),  [pdf](http://arxiv.org/pdf/2510.14967v1)

**Tags**: cs.CL cs.AI cs.LG 



### Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity   in TVD-MI Scores
**Authors**: Zachary Robertson

**Updated**: 2025-10-16T17:59:25Z

**Summary**: Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MI's binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\rho = 0.872$) and consistent identity-link advantage. TVD-MI's geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains.

**Link**: [arxiv](http://arxiv.org/abs/2510.14966v1),  [pdf](http://arxiv.org/pdf/2510.14966v1)

**Tags**: cs.LG stat.ML 



### Efficient Parallel Samplers for Recurrent-Depth Models and Their   Connection to Diffusion Language Models
**Authors**: Jonas Geiping, Xinyu Yang, Guinan Su

**Updated**: 2025-10-16T17:59:07Z

**Summary**: Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.

**Link**: [arxiv](http://arxiv.org/abs/2510.14961v1),  [pdf](http://arxiv.org/pdf/2510.14961v1)

**Tags**: cs.LG cs.CL 



### MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning
**Authors**: Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li

**Updated**: 2025-10-16T17:58:58Z

**Summary**: While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2510.14958v1),  [pdf](http://arxiv.org/pdf/2510.14958v1)

**Tags**: cs.CV cs.CL 



### EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge   Devices
**Authors**: Romina Aalishah, Mozhgan Navardi, Tinoosh Mohsenin

**Updated**: 2025-10-16T17:55:56Z

**Summary**: Deployment of efficient and accurate Deep Learning models has long been a challenge in autonomous navigation, particularly for real-time applications on resource-constrained edge devices. Edge devices are limited in computing power and memory, making model efficiency and compression essential. In this work, we propose EdgeNavMamba, a reinforcement learning-based framework for goal-directed navigation using an efficient Mamba object detection model. To train and evaluate the detector, we introduce a custom shape detection dataset collected in diverse indoor settings, reflecting visual cues common in real-world navigation. The object detector serves as a pre-processing module, extracting bounding boxes (BBOX) from visual input, which are then passed to an RL policy to control goal-oriented navigation. Experimental results show that the student model achieved a reduction of 67% in size, and up to 73% in energy per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5, while keeping the same performance as the teacher model. EdgeNavMamba also maintains high detection accuracy in MiniWorld and IsaacLab simulators while reducing parameters by 31% compared to the baseline. In the MiniWorld simulator, the navigation policy achieves over 90% success across environments of varying complexity.

**Link**: [arxiv](http://arxiv.org/abs/2510.14946v1),  [pdf](http://arxiv.org/pdf/2510.14946v1)

**Tags**: eess.IV cs.RO 



### MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics
**Authors**: Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang

**Updated**: 2025-10-16T17:55:14Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.

**Link**: [arxiv](http://arxiv.org/abs/2510.14944v1),  [pdf](http://arxiv.org/pdf/2510.14944v1)

**Tags**: cs.CL cs.AI cs.CE 



### LaSeR: Reinforcement Learning with Last-Token Self-Rewarding
**Authors**: Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin

**Updated**: 2025-10-16T17:55:11Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.14943v1),  [pdf](http://arxiv.org/pdf/2510.14943v1)

**Tags**: cs.CL cs.AI cs.LG 



### GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning
**Authors**: Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp

**Updated**: 2025-10-16T17:54:07Z

**Summary**: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.14942v1),  [pdf](http://arxiv.org/pdf/2510.14942v1)

**Tags**: cs.AI 



### The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement   and Optimization
**Authors**: Jae-Won Chung, Jeff J. Ma, Ruofan Wu, Jiachen Liu, Oh Jun Kweon, Yuxuan Xia, Zhiyu Wu, Mosharaf Chowdhury

**Updated**: 2025-10-16T17:51:15Z

**Summary**: As the adoption of Generative AI in real-world services grow explosively, energy has emerged as a critical bottleneck resource. However, energy remains a metric that is often overlooked, under-explored, or poorly understood in the context of building ML systems. We present the ML$.$ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML$.$ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services. In this paper, we explain four key design principles for benchmarking ML energy we have acquired over time, and then describe how they are implemented in the ML$.$ENERGY Benchmark. We then highlight results from the early 2025 iteration of the benchmark, including energy measurements of 40 widely used model architectures across 6 different tasks, case studies of how ML design choices impact energy consumption, and how automated optimization recommendations can lead to significant (sometimes more than 40%) energy savings without changing what is being computed by the model. The ML$.$ENERGY Benchmark is open-source and can be easily extended to various customized models and application scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.06371v2),  [pdf](http://arxiv.org/pdf/2505.06371v2)

**Tags**: cs.LG cs.AI 



### AI-Powered Early Diagnosis of Mental Health Disorders from Real-World   Clinical Conversations
**Authors**: Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin

**Updated**: 2025-10-16T17:50:04Z

**Summary**: Mental health disorders remain among the leading cause of disability worldwide, yet conditions such as depression, anxiety, and Post-Traumatic Stress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to subjective assessments, limited clinical resources, and stigma and low awareness. In primary care settings, studies show that providers misidentify depression or anxiety in over 60% of cases, highlighting the urgent need for scalable, accessible, and context-aware diagnostic tools that can support early detection and intervention. In this study, we evaluate the effectiveness of machine learning models for mental health screening using a unique dataset of 553 real-world, semistructured interviews, each paried with ground-truth diagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We benchmark multiple model classes, including zero-shot prompting with GPT-4.1 Mini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank Adaptation (LoRA). Our models achieve over 80% accuracy across diagnostic categories, with especially strongperformance on PTSD (up to 89% accuracy and 98% recall). We also find that using shorter context, focused context segments improves recall, suggesting that focused narrative cues enhance detection sensitivity. LoRA fine-tuning proves both efficient and effective, with lower-rank configurations (e.g., rank 8 and 16) maintaining competitive performance across evaluation metrics. Our results demonstrate that LLM-based models can offer substantial improvements over traditional self-report screening tools, providing a path toward low-barrier, AI-powerd early diagnosis. This work lays the groundwork for integrating machine learning into real-world clinical workflows, particularly in low-resource or high-stigma environments where access to timely mental health care is most limited.

**Link**: [arxiv](http://arxiv.org/abs/2510.14937v1),  [pdf](http://arxiv.org/pdf/2510.14937v1)

**Tags**: cs.CL 



### Circuit Insights: Towards Interpretability Beyond Activations
**Authors**: Elena Golimblevskaia, Aakriti Jain, Bruno Puri, Ammar Ibrahim, Wojciech Samek, Sebastian Lapuschkin

**Updated**: 2025-10-16T17:49:41Z

**Summary**: The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.

**Link**: [arxiv](http://arxiv.org/abs/2510.14936v1),  [pdf](http://arxiv.org/pdf/2510.14936v1)

**Tags**: cs.LG cs.AI cs.CL 



### Ctrl-VI: Controllable Video Synthesis via Variational Inference
**Authors**: Haoyi Duan, Yunzhi Zhang, Yilun Du, Jiajun Wu

**Updated**: 2025-10-16T17:48:29Z

**Summary**: Many video workflows benefit from a mixture of user controls with varying granularity, from exact 4D object trajectories and camera paths to coarse text prompts, while existing video generative models are typically trained for fixed input formats. We develop Ctrl-VI, a video synthesis method that addresses this need and generates samples with high controllability for specified elements while maintaining diversity for under-specified ones. We cast the task as variational inference to approximate a composed distribution, leveraging multiple video generation backbones to account for all task constraints collectively. To address the optimization challenge, we break down the problem into step-wise KL divergence minimization over an annealed sequence of distributions, and further propose a context-conditioned factorization technique that reduces modes in the solution space to circumvent local optima. Experiments suggest that our method produces samples with improved controllability, diversity, and 3D consistency compared to prior works.

**Link**: [arxiv](http://arxiv.org/abs/2510.07670v2),  [pdf](http://arxiv.org/pdf/2510.07670v2)

**Tags**: cs.CV cs.AI 



### Why is Your Language Model a Poor Implicit Reward Model?
**Authors**: Noam Razin, Yong Lin, Jiarui Yao, Sanjeev Arora

**Updated**: 2025-10-16T17:45:44Z

**Summary**: Reward models are key to language model post-training and inference pipelines. Conveniently, recent work showed that every language model defines an implicit reward model (IM-RM), without requiring any architectural changes. However, such IM-RMs tend to generalize worse, especially out-of-distribution, compared to explicit reward models (EX-RMs) that apply a dedicated linear head over the hidden representations of a language model. The existence of a generalization gap is puzzling, as EX-RMs and IM-RMs are nearly identical. They can be trained using the same data, loss function, and language model, and differ only in how the reward is computed. Toward a fundamental understanding of the implicit biases underlying different reward model types, we investigate the root cause of this gap. Our main finding, backed by theory and experiments, is that IM-RMs rely more heavily on superficial token-level cues. Consequently, they often generalize worse than EX-RMs under token-level distribution shifts, as well as in-distribution. Furthermore, we provide evidence against alternative hypotheses for the generalization gap. Most notably, we challenge the intuitive claim that IM-RMs struggle in tasks where generation is harder than verification because they can operate both as a verifier and a generator. Taken together, our results highlight that seemingly minor design choices can substantially impact the generalization behavior of reward models.

**Link**: [arxiv](http://arxiv.org/abs/2507.07981v2),  [pdf](http://arxiv.org/pdf/2507.07981v2)

**Tags**: cs.CL cs.AI cs.LG stat.ML 



### VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular   Design
**Authors**: Malikussaid, Hilal Hudan Nuha, Isman Kurniawan

**Updated**: 2025-10-16T17:43:31Z

**Summary**: Large Language Models demonstrate substantial promise for advancing scientific discovery, yet their deployment in disciplines demanding factual precision and specialized domain constraints presents significant challenges. Within molecular design for pharmaceutical development, these models can propose innovative molecular modifications but frequently generate chemically infeasible structures. We introduce VALID-Mol, a comprehensive framework that integrates chemical validation with LLM-driven molecular design, achieving an improvement in valid chemical structure generation from 3% to 83%. Our methodology synthesizes systematic prompt optimization, automated chemical verification, and domain-adapted fine-tuning to ensure dependable generation of synthesizable molecules with enhanced properties. Our contribution extends beyond implementation details to provide a transferable methodology for scientifically-constrained LLM applications with measurable reliability enhancements. Computational analyses indicate our framework generates promising synthesis candidates with up to 17-fold predicted improvements in target binding affinity while preserving synthetic feasibility.

**Link**: [arxiv](http://arxiv.org/abs/2506.23339v2),  [pdf](http://arxiv.org/pdf/2506.23339v2)

**Tags**: cs.LG cs.AI physics.chem-ph q-bio.QM 68T50 (Primary) 92E10, 68T07 (Secondary) I.2.7; J.3; I.2.1; I.2.6 



### NIRPS and TESS reveal a peculiar system around the M dwarf TOI-756: A   transiting sub-Neptune and a cold eccentric giant
**Authors**: Léna Parc, François Bouchy, Neil J. Cook, Nolan Grieves, Étienne Artigau, Alexandrine L'Heureux, René Doyon, Yuri S. Messias, Frédérique Baron, Susana C. C. Barros, Björn Benneke, Xavier Bonfils, Marta Bryan, Bruno L. Canto Martins, Ryan Cloutier, Nicolas B. Cowan, Daniel Brito de Freitas, Jose Renan De Medeiros, Xavier Delfosse, Elisa Delgado-Mena, Xavier Dumusque, David Ehrenreich, Pedro Figueira, Jonay I. González Hernández, David Lafrenière, Izan de Castro Leão, Christophe Lovis, Lison Malo, Claudio Melo, Lucile Mignon, Christoph Mordasini, Francesco Pepe, Rafael Rebolo, Jason Rowe, Nuno C. Santos, Damien Ségransan, Alejandro Suárez Mascareño, Stéphane Udry, Diana Valencia, Gregg Wade, Manuel Abreu, José L. A. Aguiar, Khaled Al Moulla, Guillaume Allain, Romain Allart, Jose Manuel Almenara, Tomy Arial, Hugues Auger, Luc Bazinet, Nicolas Blind, David Bohlender, Isabelle Boisse, Anne Boucher, Vincent Bourrier, Sébastien Bovay, Pedro Branco, Christopher Broeg, Denis Brousseau, Alexandre Cabral, Charles Cadieux, Andres Carmona, Yann Carteret, Zalpha Challita, David Charbonneau, Bruno Chazelas, Catherine A. Clark, João Coelho, Marion Cointepas, Karen A. Collins, Kevin I. Collins, Uriel Conod, Eduardo Cristo, Ana Rita Costa Silva, Antoine Darveau-Bernier, Laurie Dauplaise, Jean-Baptiste Delisle, Roseane de Lima Gomes, João Faria, Dasaev O. Fontinele, Thierry Forveille, Yolanda G. C. Frensch, Jonathan Gagné, Frédéric Genest, Ludovic Genolet, João Gomes da Silva, Félix Gracia Témich, Nicole Gromek, Olivier Hernandez, Melissa J. Hobson, Jens H. Hoeijmakers, Norbert Hubin, Marziye Jafariyazani, Farbod Jahandar, Ray Jayawardhana, Hans-Ulrich Käufl, Dan Kerley, Johann Kolb, Vigneshwaran Krishnamurthy, Benjamin Kung, Pierrot Lamontagne, Pierre Larue, Henry Leath, Olivia Lim, Gaspare Lo Curto, Allan M. Martins, Elisabeth C. Matthews, Jaymie Matthews, Jean-Sébastien Mayer, Stan Metchev, Lina Messamah, Leslie Moranta, Dany Mounzer, Nicola Nari, Louise D. Nielsen, Ares Osborn, Mathieu Ouellet, Jon Otegi, Luca Pasquini, Vera M. Passegger, Stefan Pelletier, Céline Peroux, Caroline Piaulet-Ghorayeb, Mykhaylo Plotnykov, Emanuela Pompei, Anne-Sophie Poulin-Girard, José Luis Rasilla, Vladimir Reshetov, Jonathan Saint-Antoine, Mirsad Sarajlic, Ivo Saviane, Robin Schnell, Alex Segovia, Julia Seidel, Armin Silber, Peter Sinclair, Michael Sordet, Danuta Sosnowska, Avidaan Srivastava, Atanas K. Stefanov, Márcio A. Teixeira, Simon Thibault, Philippe Vallée, Thomas Vandal, Valentina Vaulato, Joost P. Wardenier, Bachar Wehbe, Drew Weisserman, Ivan Wevers, François Wildi, Vincent Yariv, Gérard Zins

**Updated**: 2025-10-16T17:40:55Z

**Summary**: The Near InfraRed Planet Searcher (NIRPS) joined HARPS on the 3.6-m ESO telescope at La Silla Observatory in April 2023, dedicating part of its Guaranteed Time Observations (GTO) program to the radial velocity follow-up of TESS planet candidates to confirm and characterize transiting planets around M dwarfs. We report the first results of this program with the characterization of the TOI-756 system, which consists of TOI-756 b, a transiting sub-Neptune candidate detected by TESS, as well as TOI-756 c, an additional non-transiting planet discovered by NIRPS and HARPS. TOI-756 b is a 1.24-day period sub-Neptune with a radius of 2.81 $\pm$ 0.10 $R_\oplus$ and a mass of 9.8$^{+1.8}_{-1.6}$ $M_\oplus$. TOI-756 c is a cold eccentric (e$_c$ = 0.45 $\pm$ 0.01) giant planet orbiting with a period of 149.6 days around its star with a minimum mass of 4.05 $\pm$ 0.11 $M_\mathrm{jup}$. Additionally, a linear trend of 146$~\mathrm{m\,s}^{-1}\,\mathrm{yr}^{-1}$ is visible in the radial velocities, hinting at a third component, possibly in the planetary or brown dwarf regime. This system is unique in the exoplanet landscape, standing as the first confirmed example of such a planetary architecture around an M dwarf. With a density of 2.42 $\pm$ 0.49 g cm$^{-3}$, the inner planet, TOI-756 b, is a volatile-rich sub-Neptune. Assuming a pure H/He envelope, we inferred an atmospheric mass fraction of 0.023 and a core mass fraction of 0.27, which is well constrained by stellar refractory abundances derived from NIRPS spectra. It falls within the still poorly explored radius cliff and at the lower boundary of the Neptune desert, making it a prime target for a future atmospheric characterization with JWST to improve our understanding of this population.

**Link**: [arxiv](http://arxiv.org/abs/2510.14927v1),  [pdf](http://arxiv.org/pdf/2510.14927v1)

**Tags**: astro-ph.EP 



### Stable but Miscalibrated: A Kantian View on Overconfidence from Filters   to Large Language Models
**Authors**: Akira Okutomi

**Updated**: 2025-10-16T17:40:28Z

**Summary**: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we find that fragile internal dynamics correlate with miscalibration and hallucination, while critique-style prompts show mixed effects on calibration and hallucination. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens for diagnosing -- and selectively reducing -- overconfidence in reasoning systems. This is a preliminary version; supplementary experiments and broader replication will be reported in a future revision.

**Link**: [arxiv](http://arxiv.org/abs/2510.14925v1),  [pdf](http://arxiv.org/pdf/2510.14925v1)

**Tags**: cs.AI cs.CL cs.LG 



### Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments   on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents
**Authors**: Wugeng Zheng, Guohou Shan

**Updated**: 2025-10-16T17:39:20Z

**Summary**: Online freelance marketplaces, a rapidly growing part of the global labor market, are creating a fair environment where professional skills are the main factor for hiring. While these platforms can reduce bias from traditional hiring, the personal information in user profiles raises concerns about ongoing discrimination. Past studies on this topic have mostly used existing data, which makes it hard to control for other factors and clearly see the effect of things like gender or race. To solve these problems, this paper presents a new method that uses Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) to create realistic, artificial freelancer profiles for controlled experiments. This approach effectively separates individual factors, enabling a clearer statistical analysis of how different variables influence the freelancer project process. In addition to analyzing extracted data with traditional statistical methods for post-project stage analysis, our research utilizes a dataset with highly controlled variables, generated by an RAG-LLM, to conduct a simulated hiring experiment for pre-project stage analysis. The results of our experiments show that, regarding gender, while no significant preference emerged in initial hiring decisions, female freelancers are substantially more likely to receive imperfect ratings post-project stage. Regarding regional bias, a strong and consistent preference favoring US-based freelancers shows that people are more likely to be selected in the simulated experiments, perceived as more leader-like, and receive higher ratings on the live platform.

**Link**: [arxiv](http://arxiv.org/abs/2510.13091v2),  [pdf](http://arxiv.org/pdf/2510.13091v2)

**Tags**: cs.HC 



### REX: Causal discovery based on machine learning and explainability   techniques
**Authors**: Jesus Renero, Idoia Ochoa, Roberto Maestre

**Updated**: 2025-10-16T17:38:53Z

**Summary**: Explainable Artificial Intelligence (XAI) techniques hold significant potential for enhancing the causal discovery process, which is crucial for understanding complex systems in areas like healthcare, economics, and artificial intelligence. However, no causal discovery methods currently incorporate explainability into their models to derive the causal graphs. Thus, in this paper we explore this innovative approach, as it offers substantial potential and represents a promising new direction worth investigating. Specifically, we introduce ReX, a causal discovery method that leverages machine learning (ML) models coupled with explainability techniques, specifically Shapley values, to identify and interpret significant causal relationships among variables. Comparative evaluations on synthetic datasets comprising continuous tabular data reveal that ReX outperforms state-of-the-art causal discovery methods across diverse data generation processes, including non-linear and additive noise models. Moreover, ReX was tested on the Sachs single-cell protein-signaling dataset, achieving a precision of 0.952 and recovering key causal relationships with no incorrect edges. Taking together, these results showcase ReX's effectiveness in accurately recovering true causal structures while minimizing false positive predictions, its robustness across diverse datasets, and its applicability to real-world problems. By combining ML and explainability techniques with causal discovery, ReX bridges the gap between predictive modeling and causal inference, offering an effective tool for understanding complex causal structures.

**Link**: [arxiv](http://arxiv.org/abs/2501.12706v2),  [pdf](http://arxiv.org/pdf/2501.12706v2)

**Tags**: cs.LG 



### Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for   Adaptive Policies
**Authors**: Jing Wang, Weiting Peng, Jing Tang, Zeyu Gong, Xihua Wang, Bo Tao, Li Cheng

**Updated**: 2025-10-16T17:37:59Z

**Summary**: Existing imitation learning methods decouple perception and action, which overlooks the causal reciprocity between sensory representations and action execution that humans naturally leverage for adaptive behaviors. To bridge this gap, we introduce Action-Guided Diffusion Policy (DP-AG), a unified representation learning that explicitly models a dynamic interplay between perception and action through probabilistic latent dynamics. DP-AG encodes latent observations into a Gaussian posterior via variational inference and evolves them using an action-guided SDE, where the Vector-Jacobian Product (VJP) of the diffusion policy's noise predictions serves as a structured stochastic force driving latent updates. To promote bidirectional learning between perception and action, we introduce a cycle-consistent contrastive loss that organizes the gradient flow of the noise predictor into a coherent perception-action loop, enforcing mutually consistent transitions in both latent updates and action refinements. Theoretically, we derive a variational lower bound for the action-guided SDE, and prove that the contrastive objective enhances continuity in both latent and action trajectories. Empirically, DP-AG significantly outperforms state-of-the-art methods across simulation benchmarks and real-world UR5 manipulation tasks. As a result, our DP-AG offers a promising step toward bridging biological adaptability and artificial policy learning.

**Link**: [arxiv](http://arxiv.org/abs/2509.25822v3),  [pdf](http://arxiv.org/pdf/2509.25822v3)

**Tags**: cs.RO 



### Predicting Task Performance with Context-aware Scaling Laws
**Authors**: Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang

**Updated**: 2025-10-16T17:35:18Z

**Summary**: Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.

**Link**: [arxiv](http://arxiv.org/abs/2510.14919v1),  [pdf](http://arxiv.org/pdf/2510.14919v1)

**Tags**: cs.CL cs.AI cs.LG 



### Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent   Generation
**Authors**: Xujun Peng, Anoop Kumar, Jingyu Wu, Parker Glenn, Daben Liu

**Updated**: 2025-10-16T17:30:28Z

**Summary**: Retrieval-Augmented Generation (RAG) systems leverage Large Language Models (LLMs) to generate accurate and reliable responses that are grounded in retrieved context. However, LLMs often generate inconsistent outputs for semantically equivalent inputs, a problem compounded by the scarcity of consistency-focused training data and the limitations of current fine-tuning techniques in enhancing output consistency. We propose a new approach combining systematic synthetic data generation, triplet loss for better embeddings, and a novel layer-wise model merging approach. Using consistency-aware weights derived from intermediate layer activations, our method effectively integrates knowledge from specialized models. Experimental results how that our merged model significantly enhances output consistency, achieving a ~47.5\% improvement in response similarity over the baseline, thus offering a practical solution for increasing the reliability of an industrial RAG system.

**Link**: [arxiv](http://arxiv.org/abs/2510.14915v1),  [pdf](http://arxiv.org/pdf/2510.14915v1)

**Tags**: cs.CL 



### Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning
**Authors**: Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong

**Updated**: 2025-10-16T17:30:21Z

**Summary**: Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach suffers from catastrophic forgetting: second-stage RL gradually loses SFT-acquired behaviors and inefficiently explores new patterns. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2509.06948v2),  [pdf](http://arxiv.org/pdf/2509.06948v2)

**Tags**: cs.CL 



### Is Misinformation More Open? A Study of robots.txt Gatekeeping on the   Web
**Authors**: Nicolas Steinacker-Olsztyn, Devashish Gosain, Ha Dao

**Updated**: 2025-10-16T17:26:58Z

**Summary**: Large Language Models (LLMs) are increasingly relying on web crawling to stay up to date and accurately answer user queries. These crawlers are expected to honor robots.txt files, which govern automated access. In this study, for the first time, we investigate whether reputable news websites and misinformation sites differ in how they configure these files, particularly in relation to AI crawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of reputable sites disallow at least one AI crawler, compared to just 9.1% of misinformation sites in their robots.txt files. Reputable sites forbid an average of 15.5 AI user agents, while misinformation sites prohibit fewer than one. We then measure active blocking behavior, where websites refuse to return content when HTTP requests include AI crawler user agents, and reveal that both categories of websites utilize it. Notably, the behavior of reputable news websites in this regard aligns more closely with their declared robots.txt directive than that of misinformation websites. Finally, our longitudinal analysis reveals that this gap has widened over time, with AI-blocking by reputable sites rising from 23% in September 2023 to nearly 60% by May 2025. Our findings highlight a growing asymmetry in content accessibility that may shape the training data available to LLMs, raising essential questions for web transparency, data ethics, and the future of AI training practices.

**Link**: [arxiv](http://arxiv.org/abs/2510.10315v3),  [pdf](http://arxiv.org/pdf/2510.10315v3)

**Tags**: cs.CY 



### Reasoning with Sampling: Your Base Model is Smarter Than You Think
**Authors**: Aayush Karan, Yilun Du

**Updated**: 2025-10-16T17:18:11Z

**Summary**: Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.

**Link**: [arxiv](http://arxiv.org/abs/2510.14901v1),  [pdf](http://arxiv.org/pdf/2510.14901v1)

**Tags**: cs.LG cs.AI cs.CL 



### Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent   That Improves Without Labels or Model Updates
**Authors**: Wen-Kwang Tsao, Yao-Ching Yu, Chien-Ming Huang

**Updated**: 2025-10-16T17:17:00Z

**Summary**: The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.

**Link**: [arxiv](http://arxiv.org/abs/2510.14900v1),  [pdf](http://arxiv.org/pdf/2510.14900v1)

**Tags**: cs.AI cs.CR 



### Leveraging Multimodal LLM Descriptions of Activity for Explainable   Semi-Supervised Video Anomaly Detection
**Authors**: Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz

**Updated**: 2025-10-16T17:13:33Z

**Summary**: Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.

**Link**: [arxiv](http://arxiv.org/abs/2510.14896v1),  [pdf](http://arxiv.org/pdf/2510.14896v1)

**Tags**: cs.CV 



### You May Speak Freely: Improving the Fine-Grained Visual Recognition   Capabilities of Multimodal Large Language Models with Answer Extraction
**Authors**: Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn

**Updated**: 2025-10-16T17:04:25Z

**Summary**: Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.

**Link**: [arxiv](http://arxiv.org/abs/2510.14885v1),  [pdf](http://arxiv.org/pdf/2510.14885v1)

**Tags**: cs.CV cs.CL 



### ScaleWeaver: Weaving Efficient Controllable T2I Generation with   Multi-Scale Reference Attention
**Authors**: Keli Liu, Zhendong Wang, Wengang Zhou, Shaodong Xu, Ruixiao Dong, Houqiang Li

**Updated**: 2025-10-16T17:00:59Z

**Summary**: Text-to-image generation with visual autoregressive~(VAR) models has recently achieved impressive advances in generation fidelity and inference efficiency. While control mechanisms have been explored for diffusion models, enabling precise and flexible control within VAR paradigm remains underexplored. To bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel framework designed to achieve high-fidelity, controllable generation upon advanced VAR models through parameter-efficient fine-tuning. The core module in ScaleWeaver is the improved MMDiT block with the proposed Reference Attention module, which efficiently and effectively incorporates conditional information. Different from MM Attention, the proposed Reference Attention module discards the unnecessary attention from image$\rightarrow$condition, reducing computational cost while stabilizing control injection. Besides, it strategically emphasizes parameter reuse, leveraging the capability of the VAR backbone itself with a few introduced parameters to process control information, and equipping a zero-initialized linear projection to ensure that control signals are incorporated effectively without disrupting the generative capability of the base model. Extensive experiments show that ScaleWeaver delivers high-quality generation and precise control while attaining superior efficiency over diffusion-based methods, making ScaleWeaver a practical and effective solution for controllable text-to-image generation within the visual autoregressive paradigm. Code and models will be released.

**Link**: [arxiv](http://arxiv.org/abs/2510.14882v1),  [pdf](http://arxiv.org/pdf/2510.14882v1)

**Tags**: cs.CV 



### The Gatekeeper Knows Enough
**Authors**: Fikresilase Wondmeneh Abebayew

**Updated**: 2025-10-16T17:00:42Z

**Summary**: Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity "latent state" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.

**Link**: [arxiv](http://arxiv.org/abs/2510.14881v1),  [pdf](http://arxiv.org/pdf/2510.14881v1)

**Tags**: cs.AI cs.IT math.IT 



### Multi-wavelength analysis of the progenitor of GRB 230307A via Bayesian   model comparison
**Authors**: V. Alfradique, R. da Mata, J. C. Rodríguez-Ramírez, C. R. Bom

**Updated**: 2025-10-16T17:00:02Z

**Summary**: GRB 230307A is one of the brightest long-duration gamma-ray bursts (GRBs) ever detected, yet its progenitor remains uncertain due to the variety of plausible astrophysical scenarios. In this work, we investigate four possible progenitors for GRB 230307A: a binary neutron star (BNS), a neutron star--white dwarf (NS--WD) system, a neutron star--black hole (NS--BH) merger, and a tidal disruption event (TDE) involving a white dwarf and a supermassive black hole. Additionally, we explore three distinct central engine models powering the kilonova associated with the BNS: radioactive decay of $r$-process nuclei in a two-component ejecta model, a magnetar-driven model including magnetic dipole spin-down, and a combined model of magnetar spin-down with ${}^{56}$Ni radioactive decay. We perform Bayesian multi-wavelength light-curve analyses using physically motivated models and priors, and evaluate model performance through Bayes factors and leave-one-out cross-validation (LOO) scores. Our results show a statistical preference for a BNS or NS--WD progenitor producing a kilonova powered by a magnetar and ${}^{56}$Ni decay, characterized by a ${}^{56}$Ni mass of $\sim4\times10^{-4}\,M_{\odot}$ and an ejecta mass of $0.06\,M_{\odot}$. Furthermore, under the assumption of a BNS origin within this model, we infer binary component masses of $m_{1} = 1.81^{+0.46}_{-0.61}\,M_{\odot}$ and $m_{2} = 1.61^{+0.65}_{-0.41}\,M_{\odot}$, with a dimensionless tidal deformability of $\tilde{\Lambda} = 471^{+318}_{-395}$. From the component mass posteriors, we infer that the observed offset can be explained by a natal kick as long as the systemic velocity is nearly aligned with the pre-kick orbital motion. In this case, the required kick velocity (co-moving frame) and binary separation range within $v'_{\mathrm{k}}\sim100$--$150~\mathrm{km\,s^{-1}}$, and $a_0\sim2$--$3~R_{\odot}$, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2510.14879v1),  [pdf](http://arxiv.org/pdf/2510.14879v1)

**Tags**: astro-ph.HE astro-ph.GA 



### AREPO-RSG: Aspherical Circumstellar Material and Winds from Pulsating   Dusty Red Supergiants in Global 3D Radiation Hydrodynamic Simulations
**Authors**: Jing-Ze Ma, Stephen Justham, Ruediger Pakmor, Andrea Chiavassa, Taeho Ryu, Selma de Mink

**Updated**: 2025-10-16T16:54:29Z

**Summary**: Recent observations have revealed a surprisingly large fraction of hydrogen-rich supernovae (SNe) interacting with dense confined circumstellar material (CSM), whose origin is heavily debated. Exploiting our recent implementation of a sophisticated radiation transport scheme in the moving-mesh code AREPO, we perform full-sphere 3D radiation hydrodynamic simulations of red supergiant envelopes. For $10\, M_\odot$ and $20\, M_\odot$ core-carbon-burning stars, we find that large-amplitude radial pulsations lift the surface material of density $10^{-14}$-$10^{-12}\; \mathrm{g\; cm^{-3}}$ to the circumstellar environment up to $3\times10^{14}$ cm, consistent with the inferred density for the interacting SN 2013fs. There, radiation acts on dust to drive highly anisotropic outflows of $10^{-6}$-$10^{-5}\, M_\odot\, \mathrm{yr^{-1}}$. The total CSM masses for both simulations are $\sim 0.01\, M_\odot$. Due to convection, the CSM density structure has order-of-magnitude angular variations, dominated by large-scale asymmetries. We suggest that (1) the CSM around the progenitor is bound material instead of a widely-assumed steady wind, (2) highly aspherical CSM is common and can be created by surface convection rather than only from binary interactions, and (3) 3D effects need to be incorporated in 1D SN modeling, potentially via effective clumping. Based on our simulations, we propose a 1D analytical CSM model to be directly used for SN observable modeling. We predict that progenitor pulsations (seen in SN 2023ixf) and highly-confined CSM (seen in SN 2013fs) should be common among most hydrogen-rich SNe. This can be tested with progenitor monitoring using Rubin Observatory and near-future high-cadence surveys such as ULTRASAT and UVEX.

**Link**: [arxiv](http://arxiv.org/abs/2510.14875v1),  [pdf](http://arxiv.org/pdf/2510.14875v1)

**Tags**: astro-ph.SR astro-ph.GA astro-ph.HE 



### Robust Counterfactual Inference in Markov Decision Processes
**Authors**: Jessica Lally, Milad Kazemi, Nicola Paoletti

**Updated**: 2025-10-16T16:51:12Z

**Summary**: This paper addresses a key limitation in existing counterfactual inference methods for Markov Decision Processes (MDPs). Current approaches assume a specific causal model to make counterfactuals identifiable. However, there are usually many causal models that align with the observational and interventional distributions of an MDP, each yielding different counterfactual distributions, so fixing a particular causal model limits the validity (and usefulness) of counterfactual inference. We propose a novel non-parametric approach that computes tight bounds on counterfactual transition probabilities across all compatible causal models. Unlike previous methods that require solving prohibitively large optimisation problems (with variables that grow exponentially in the size of the MDP), our approach provides closed-form expressions for these bounds, making computation highly efficient and scalable for non-trivial MDPs. Once such an interval counterfactual MDP is constructed, our method identifies robust counterfactual policies that optimise the worst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate our method on various case studies, demonstrating improved robustness over existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2502.13731v3),  [pdf](http://arxiv.org/pdf/2502.13731v3)

**Tags**: cs.AI 



### Chiplet-Based RISC-V SoC with Modular AI Acceleration
**Authors**: P. Ramkumar, S. S. Bharadwaj

**Updated**: 2025-10-16T16:44:51Z

**Summary**: Achieving high performance, energy efficiency, and cost-effectiveness while maintaining architectural flexibility is a critical challenge in the development and deployment of edge AI devices. Monolithic SoC designs struggle with this complex balance mainly due to low manufacturing yields (below 16%) at advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based RISC-V SoC architecture that addresses these limitations through modular AI acceleration and intelligent system level optimization. Our proposed design integrates 4 different key innovations in a 30mm x 30mm silicon interposer: adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring streaming flow control units and compression-aware transfers; distributed cryptographic security across heterogeneous chiplets; and intelligent sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory stacks, and dedicated power management controllers. Experimental results across industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video processing demonstrate significant performance improvements. The AI-optimized configuration achieves ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power reduction compared to previous basic chiplet implementations. These improvements collectively translate to a 40.1% efficiency gain corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while maintaining sub-5ms real-time capability across all experimented workloads. These performance upgrades demonstrate that modular chiplet designs can achieve near-monolithic computational density while enabling cost efficiency, scalability and upgradeability, crucial for next-generation edge AI device applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.18355v2),  [pdf](http://arxiv.org/pdf/2509.18355v2)

**Tags**: cs.AR cs.AI 



### SteeringSafety: A Systematic Safety Evaluation Framework of   Representation Steering in LLMs
**Authors**: Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang

**Updated**: 2025-10-16T16:44:31Z

**Summary**: We introduce SteeringSafety, a systematic framework for evaluating representation steering methods across seven safety perspectives spanning 17 datasets. While prior work highlights general capabilities of representation steering, we systematically explore safety perspectives including bias, harmfulness, hallucination, social behaviors, reasoning, epistemic integrity, and normative judgment. Our framework provides modularized building blocks for state-of-the-art steering methods, enabling unified implementation of DIM, ACE, CAA, PCA, and LAT with recent enhancements like conditional steering. Results on Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering performance depends critically on pairing of method, model, and specific perspective. DIM shows consistent effectiveness, but all methods exhibit substantial entanglement: social behaviors show highest vulnerability (reaching degradation as high as 76%), jailbreaking often compromises normative judgment, and hallucination steering unpredictably shifts political views. Our findings underscore the critical need for holistic safety evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2509.13450v2),  [pdf](http://arxiv.org/pdf/2509.13450v2)

**Tags**: cs.AI cs.CL cs.LG 



### Machine Unlearning Meets Adversarial Robustness via Constrained   Interventions on LLMs
**Authors**: Fatmazohra Rezkellah, Ramzi Dakhmouche

**Updated**: 2025-10-16T16:42:58Z

**Summary**: With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.

**Link**: [arxiv](http://arxiv.org/abs/2510.03567v3),  [pdf](http://arxiv.org/pdf/2510.03567v3)

**Tags**: cs.LG cs.CL cs.CR cs.CY math.OC 



### AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective
**Authors**: Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han

**Updated**: 2025-10-16T16:37:59Z

**Summary**: Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. In addition, to ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text/audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized Phoneme-Frame Causal Attention Mask to support multi-step parallel acceleration. Extensive experiments conducted on both Chinese (CMLR) and English (HDTF) datasets demonstrate that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.

**Link**: [arxiv](http://arxiv.org/abs/2509.12052v2),  [pdf](http://arxiv.org/pdf/2509.12052v2)

**Tags**: cs.CV 



### Deuterated water ice on the satellites of Saturn
**Authors**: Michael E. Brown, Samantha K. Trumbo, M. Ryleigh Davis, Swaroop Chandra

**Updated**: 2025-10-16T16:33:34Z

**Summary**: The deuterium to hydrogen ratio in water ice in a planetary body carries important information on the history of water processing and delivery in the protostellar nebula. For a giant planet satellite, the D/H ratio is also affected by the processes and temperatures of the circumplanetary or circumstellar environment in which the satellites formed. Here we present robust JWST spectroscopic detections of the 4.14 $\mu$m O-D stretch absorption line (analogous to the 3 $\mu$m water O-H stretch) on the mid-sized Saturnian satellites and use these detections to infer a D/H ratio on each satellite. Within the limitations of the technique, we find that all of the satellites are consistent with having a D/H ratio of about $1.5 \times$ Vienna Standard Mean Ocean Water (VSMOW), which is about an order of magnitude higher than the value of the atmosphere of Saturn. A much higher previously reported D/H ratio for Phoebe is ruled out at the 10$\sigma$ level, and a 3$\sigma$ upper limit of 2.3 $\times$ VSMOW is obtained. The elevated D/H ratios demonstrate that the solid planetesimals and pebbles that built the satellites never sublimed and re-equilibrated with the gaseous circumplanetary disk. The similarity of the D/H measurements across all satellites suggest that the D/H ratio of water ice in the vicinity of Saturn at the time of satellite formation was also approximately 1.5 $\times$ VSMOW.

**Link**: [arxiv](http://arxiv.org/abs/2510.14859v1),  [pdf](http://arxiv.org/pdf/2510.14859v1)

**Tags**: astro-ph.EP 



### TinyGraphEstimator: Adapting Lightweight Language Models for Graph   Structure Inference
**Authors**: Michal Podstawski

**Updated**: 2025-10-16T16:29:36Z

**Summary**: Graphs provide a universal framework for representing complex relational systems, and inferring their structural properties is a core challenge in graph analysis and reasoning. While large language models have recently demonstrated emerging abilities to perform symbolic and numerical reasoning, the potential of smaller, resource-efficient models in this context remains largely unexplored. This paper investigates whether compact transformer-based language models can infer graph-theoretic parameters directly from graph representations. To enable systematic evaluation, we introduce the TinyGraphEstimator dataset - a balanced collection of connected graphs generated from multiple random graph models and annotated with detailed structural metadata. We evaluate several small open models on their ability to predict key graph parameters such as density, clustering, and chromatic number. Furthermore, we apply lightweight fine-tuning using the Low-Rank Adaptation (LoRA) technique, achieving consistent improvements across all evaluated metrics. The results demonstrate that small language models possess non-trivial reasoning capacity over graph-structured data and can be effectively adapted for structural inference tasks through efficient parameter tuning.

**Link**: [arxiv](http://arxiv.org/abs/2510.08808v3),  [pdf](http://arxiv.org/pdf/2510.08808v3)

**Tags**: cs.LG 



### Thinker: Learning to Think Fast and Slow
**Authors**: Stephen Chung, Wenyu Du, Jie Fu

**Updated**: 2025-10-16T16:20:17Z

**Summary**: Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training. Additionally, we have open-sourced both the trained models and the source code.

**Link**: [arxiv](http://arxiv.org/abs/2505.21097v2),  [pdf](http://arxiv.org/pdf/2505.21097v2)

**Tags**: cs.CL cs.AI cs.LG I.2.6; I.2.8; I.5.1 



### ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond   Semantic Dependency Constraints
**Authors**: Meiqi Wu, Jiashu Zhu, Xiaokun Feng, Chubin Chen, Chen Zhu, Bingze Song, Fangyuan Mao, Jiahong Wu, Xiangxiang Chu, Kaiqi Huang

**Updated**: 2025-10-16T16:19:13Z

**Summary**: Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.14847v1),  [pdf](http://arxiv.org/pdf/2510.14847v1)

**Tags**: cs.CV 



### Where to Search: Measure the Prior-Structured Search Space of LLM Agents
**Authors**: Zhuo-Yang Song

**Updated**: 2025-10-16T16:18:37Z

**Summary**: The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14846v1),  [pdf](http://arxiv.org/pdf/2510.14846v1)

**Tags**: cs.AI cs.CL cs.LO 



### Boosting Instruction Following at Scale
**Authors**: Ben Elder, Evelyn Duesterwald, Vinod Muthusamy

**Updated**: 2025-10-16T16:15:58Z

**Summary**: A typical approach developers follow to influence an LLM's behavior in an application is through careful manipulation of the prompt, such as by adding or modifying instructions. However, merely adding more instructions provides little assurance that they will actually be followed. We introduce Instruction Boosting as a post-generation method to increase the reliability of LLM prompt instructions. We show that Instruction Boosting improves the instruction following rate by up to 7 points for two instructions and up to 4 points for ten instructions. To demonstrate these results we introduce SCALEDIF, a benchmark with a scaled instruction volume of up to ten instructions per data sample. We also present an analysis of the commonly observed trend that performance degrades as more instructions are added. We show that an important factor contributing to this trend is the degree of tension and conflict that arises as the number of instructions is increased. We contribute a quantitative conflict scoring tool that explains the observed performance trends and provides feedback to developers on the impact that additional prompt instructions have on a model's performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.14842v1),  [pdf](http://arxiv.org/pdf/2510.14842v1)

**Tags**: cs.AI 



### Programmatic Representation Learning with Language Models
**Authors**: Gabriel Poesia, Georgia Gabriela Sampaio

**Updated**: 2025-10-16T16:02:42Z

**Summary**: Classical models for supervised machine learning, such as decision trees, are efficient and interpretable predictors, but their quality is highly dependent on the particular choice of input features. Although neural networks can learn useful representations directly from raw data (e.g., images or text), this comes at the expense of interpretability and the need for specialized hardware to run them efficiently. In this paper, we explore a hypothesis class we call Learned Programmatic Representations (LeaPR) models, which stack arbitrary features represented as code (functions from data points to scalars) and decision tree predictors. We synthesize feature functions using Large Language Models (LLMs), which have rich prior knowledge in a wide range of domains and a remarkable ability to write code using existing domain-specific libraries. We propose two algorithms to learn LeaPR models from supervised data. First, we design an adaptation of FunSearch to learn features rather than directly generate predictors. Then, we develop a novel variant of the classical ID3 algorithm for decision tree learning, where new features are generated on demand when splitting leaf nodes. In experiments from chess position evaluation to image and text classification, our methods learn high-quality, neural network-free predictors often competitive with neural networks. Our work suggests a flexible paradigm for learning interpretable representations end-to-end where features and predictions can be readily inspected and understood.

**Link**: [arxiv](http://arxiv.org/abs/2510.14825v1),  [pdf](http://arxiv.org/pdf/2510.14825v1)

**Tags**: cs.LG 



### Supervised Fine-Tuning or Contrastive Learning? Towards Better   Multimodal LLM Reranking
**Authors**: Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang

**Updated**: 2025-10-16T16:02:27Z

**Summary**: In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.

**Link**: [arxiv](http://arxiv.org/abs/2510.14824v1),  [pdf](http://arxiv.org/pdf/2510.14824v1)

**Tags**: cs.CL cs.CV cs.IR 



### Exploring a cosmic ray inverse-Compton origin to the SZ-to-X-ray   pressure deficit in the cool core cluster ZwCl 3146
**Authors**: Emily M. Silich, Jack Sayers, Philip F. Hopkins, Charles Romero, Brian Mason, John Orlowski-Scherer, Craig L. Sarazin

**Updated**: 2025-10-16T15:55:54Z

**Summary**: We explore the possibility that inverse-Compton (IC) scattering of cosmic microwave background photons by $\sim$GeV cosmic rays (CRs) injected by the central active galactic nucleus (AGN) in cool core (CC) clusters produces a non-negligible continuum-like X-ray signal that is easily misinterpreted as intracluster medium (ICM) thermal bremsstrahlung continuum. This is particularly relevant to the cooling flow problem--the lack of star formation relative to X-ray-inferred ICM cooling rates. Using ZwCl 3146, a relaxed CC system at $z = 0.291$, we compare pressure profiles derived via X-rays and the thermal Sunyaev-Zel'dovich (SZ) effect. While SZ measurements probe only thermal ICM electrons, additional CR-IC emission would appear to boost the X-ray-inferred pressure. Relative to unity, we measure a $\simeq30\%$ decrement in $P_{SZ}/P_X$ within 100 kpc of the ZwCl 3146 center at a statistical significance of $\simeq 3.3\sigma$, consistent with predicted deficits from CR-IC contamination in reasonable models of central AGN-driven CR injection. X-ray spectral fits of a two-component model with thermal ICM and CR-IC emission are consistent with CR-IC as the cause of this deficit. We test alternative explanations and systematics that could drive such a decrement, with the leading order systematics associated with halo triaxiality. Collectively, these systematics are unlikely to produce a $P_{SZ}/P_X$ decrement $\gtrsim10\%$. While our results establish that non-negligible CR-IC emission is plausible in ZwCl 3146, we stress that more detailed studies of larger cluster samples are required to robustly assess whether CR-IC is relevant to the cooling flow problem.

**Link**: [arxiv](http://arxiv.org/abs/2510.14820v1),  [pdf](http://arxiv.org/pdf/2510.14820v1)

**Tags**: astro-ph.HE astro-ph.CO 



### Efficient Dynamic Structured Sparse Training with Learned Shuffles
**Authors**: Abhishek Tyagi, Arjun Iyer, Liam Young, William H Renninger, Christopher Kanan, Yuhao Zhu

**Updated**: 2025-10-16T15:48:17Z

**Summary**: Structured sparsity accelerates training and inference on modern GPUs, yet it still trails unstructured dynamic sparse training (DST) in accuracy. The shortfall stems from a loss of expressivity: whereas a dense layer can realize every possible mask obtained by choosing any $w$ active weights out of $n$, a fixed block or N:M layout explores only a subset of those possibilities. We propose to close this gap by learning, for each layer, a single permutation matrix jointly with the structured weight matrix. Applied to three canonical structures -- block, N:M, and diagonals -- we show that permutation-augmented DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\% sparsity on ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\times$ and infers up to $2.9\times$ faster. The results position structure + learned permutation as a sweet spot between accuracy and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2510.14812v1),  [pdf](http://arxiv.org/pdf/2510.14812v1)

**Tags**: cs.LG 



### Agentic NL2SQL to Reduce Computational Costs
**Authors**: Dominik Jehle, Lennart Purucker, Frank Hutter

**Updated**: 2025-10-16T15:42:28Z

**Summary**: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87\% and thus allows for substantial cost reductions while maintaining competitive performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.14808v1),  [pdf](http://arxiv.org/pdf/2510.14808v1)

**Tags**: cs.AI cs.LG 



### SimKO: Simple Pass@K Policy Optimization
**Authors**: Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen

**Updated**: 2025-10-16T15:40:49Z

**Summary**: Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.

**Link**: [arxiv](http://arxiv.org/abs/2510.14807v1),  [pdf](http://arxiv.org/pdf/2510.14807v1)

**Tags**: cs.AI 



### A new photometric ephemeris for the 2M1510 AB double brown dwarf   eclipsing binary system
**Authors**: Seb T. Millward, Vedad Kunovac

**Updated**: 2025-10-16T15:33:20Z

**Summary**: Eclipsing brown dwarfs are important calibrators of sub-stellar evolution models used to infer the characteristics of directly imaged brown dwarfs and giant exoplanets. Only two double brown dwarf eclipsing binary systems are known, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a poorly constrained orbital period. Here we analyse TESS full-frame image (FFI) photometry of this faint ($T=15.9$) binary and detect a significant (${>}10\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with previous data. We refine the orbital period to $20.897782 \pm 0.000036$ d, reducing its present-day uncertainty from 18 h to 8 min. Our work is crucial for scheduling follow-up observations of this system for detailed study with other photometric facilities. We also find that a recent orbital solution from Doppler data is inconsistent with existing photometry. A timing offset in the Doppler data may have produced a spurious signal mimicking retrograde apsidal precession, from which the claimed circumbinary planet 2M1510 ABb was inferred. From our best attempt at correcting the data we were unable to reconcile the radial velocity data with the photometry, suggesting that the radial velocity uncertainties are underestimated, and that the circumbinary planet 2M1510 ABb may be a false positive.

**Link**: [arxiv](http://arxiv.org/abs/2510.14801v1),  [pdf](http://arxiv.org/pdf/2510.14801v1)

**Tags**: astro-ph.SR astro-ph.EP 



### LLM-guided Chemical Process Optimization with a Multi-Agent Approach
**Authors**: Tong Zeng, Srivathsan Badrinarayanan, Janghoon Ock, Cheng-Kai Lai, Amir Barati Farimani

**Updated**: 2025-10-16T15:31:07Z

**Summary**: Chemical process optimization maximizes production efficiency and economic performance, but optimization algorithms, including gradient-based solvers, numerical methods, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable. We present a multi-agent LLM framework that autonomously infers operating constraints from minimal process descriptions, then collaboratively guides optimization. Our AutoGen-based framework employs OpenAI's o3 model with specialized agents for constraint generation, parameter validation, simulation, and optimization guidance. Through autonomous constraint generation and iterative multi-agent optimization, the framework eliminates the need for predefined operational bounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost ratio metrics, the framework achieved competitive performance with conventional methods while reducing wall-time 31-fold relative to grid search, converging in under 20 minutes. The reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. Unlike conventional methods requiring predefined constraints, our approach uniquely combines autonomous constraint generation with interpretable parameter exploration. Model comparison reveals reasoning-capable architectures (o3, o1) are essential for successful optimization, while standard models fail to converge. This approach is particularly valuable for emerging processes and retrofit applications where operational constraints are poorly characterized or unavailable.

**Link**: [arxiv](http://arxiv.org/abs/2506.20921v2),  [pdf](http://arxiv.org/pdf/2506.20921v2)

**Tags**: cs.LG cs.AI cs.CE 



### Cross-Scenario Unified Modeling of User Interests at Billion Scale
**Authors**: Manjie Xu, Cheng Chen, Xin Jia, Jingyi Zhou, Yongji Wu, Zejian Wang, Chi Zhang, Kai Zuo, Yibo Chen, Xu Tang, Yao Hu, Yixin Zhu

**Updated**: 2025-10-16T15:20:49Z

**Summary**: User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.

**Link**: [arxiv](http://arxiv.org/abs/2510.14788v1),  [pdf](http://arxiv.org/pdf/2510.14788v1)

**Tags**: cs.IR cs.AI 



### The dark side of early galaxies: $\texttt{geko}$ uncovers dark-matter   fractions at $z\sim4-6$
**Authors**: A. Lola Danhaive, Sandro Tacchella, Andrew J. Bunker, Emma Curtis-Lake, Anna de Graaff, Francesco D'Eugenio, Qiao Duan, Eiichi Egami, Daniel J. Eisenstein, Benjamin D. Johnson, Roberto Maiolino, William McClymont, Marcia Rieke, Brant Robertson, Fengwu Sun, Christopher N. A. Willmer, Zihao Wu, Yongda Zhu

**Updated**: 2025-10-16T15:15:06Z

**Summary**: JWST/NIRCam slitless spectroscopy enables dynamical mass measurements for typical star-forming galaxies only a billion years after the Big Bang. We model the H$\alpha$ morpho-kinematics of 163 galaxies at redshift $z\approx4$-6 from FRESCO and CONGRESS (with JADES imaging), using the $\texttt{geko}$ code, and infer rotational velocities and dispersions within $r_{\rm e}$. Our sample spans $\log M_{\star}\approx7$-10 and $\log M_{\rm dyn}\approx9$-11. Gas masses are estimated via scaling relations, yielding baryonic masses and dark-matter (DM) fractions $f_{\rm DM}(r<r_{\rm e})$ within the H$\alpha$ half-light radius. We find high median fractions of $\langle f_{\rm gas}\rangle=0.77$ and $\langle f_{\rm DM}\rangle=0.73$, where $f_{\rm gas}$ is measured with respect to the baryonic mass and $f_{\rm DM}$ with respect to the DM+baryonic mass. About two-thirds of systems are DM-dominated within $r_{\rm e}\sim0.5-1$ kpc. Both $f_{\rm gas}$ and $f_{\rm DM}$ decrease with stellar mass, consistent with simulations. The stellar Tully-Fisher relation shows a tentative offset to higher $v_{\rm circ}$ at fixed $M_{\star}$ and substantial intrinsic scatter, suggesting that the relation is only beginning to emerge at $z\sim5$. We measure a negative correlation between $f_{\rm DM}$ and baryonic surface density $\Sigma_{\rm bar}$, weaker but broadly consistent with trends at cosmic noon and at $z\sim0$. Qualitatively comparing with modified NFW profiles coupled to an empirical stellar-to-halo mass relation suggests that the lowest $f_{\rm DM}$ ($\lesssim0.4$) require cored inner DM profiles, while the highest fractions favour cuspier profiles, potentially reflecting adiabatic contraction. Overall, the elevated $f_{\rm gas}$ and $f_{\rm DM}$ at $z\gtrsim4$ are compatible with progenitors of baryon-dominated systems at $z\sim2$ and naturally anticipate overmassive black holes at fixed $M_{\star}$.

**Link**: [arxiv](http://arxiv.org/abs/2510.14779v1),  [pdf](http://arxiv.org/pdf/2510.14779v1)

**Tags**: astro-ph.GA 



### Finding Answers in Thought Matters: Revisiting Evaluation on Large   Language Models with Reasoning
**Authors**: Hwiyeol Jo, Joosung Lee, Jaehone Lee, Sang-Woo Lee, Joonsuk Park, Kang Min Yoo

**Updated**: 2025-10-16T15:09:22Z

**Summary**: Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt "Answer:". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2510.14773v1),  [pdf](http://arxiv.org/pdf/2510.14773v1)

**Tags**: cs.CL cs.AI 



### Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method   Refactoring
**Authors**: Abhiram Bellur, Fraol Batole, Mohammed Raihan Ullah, Malinda Dilhara, Yaroslav Zharov, Timofey Bryksin, Kai Ishikawa, Haifeng Chen, Masaharu Morimoto, Shota Motoura, Takeo Hosomi, Tien N. Nguyen, Hridesh Rajan, Nikolaos Tsantalis, Danny Dig

**Updated**: 2025-10-16T15:08:16Z

**Summary**: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools that recommend which methods to move and where, these recommendations do not align with how expert developers perform MOVEMETHOD. Given the extensive training of Large Language Models and their reliance upon naturalness of code, they should expertly recommend which methods are misplaced in a given class and which classes are better hosts. Our formative study of 2016 LLM recommendations revealed that LLMs give expert suggestions, yet they are unreliable: up to 80% of the suggestions are hallucinations. We introduce the first LLM fully powered assistant for MOVEMETHOD refactoring that automates its whole end-to-end lifecycle, from recommendation to execution. We designed novel solutions that automatically filter LLM hallucinations using static analysis from IDEs and a novel workflow that requires LLMs to be self-consistent, critique, and rank refactoring suggestions. As MOVEMETHOD refactoring requires global, projectlevel reasoning, we solved the limited context size of LLMs by employing refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist, synergistically combines the strengths of the LLM, IDE, static analysis, and semantic relevance. In our thorough, multi-methodology empirical evaluation, we compare MM-assist with the previous state-of-the-art approaches. MM-assist significantly outperforms them: (i) on a benchmark widely used by other researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a corpus of 210 recent refactorings from Open-source software, our Recall rates improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced participants who used MM-assist to refactor their own code for one week. They rated 82.8% of MM-assist recommendations positively. This shows that MM-assist is both effective and useful.

**Link**: [arxiv](http://arxiv.org/abs/2503.20934v2),  [pdf](http://arxiv.org/pdf/2503.20934v2)

**Tags**: cs.SE cs.AI 



### Predicting the Subhalo Mass Functions in Simulations from Galaxy Images
**Authors**: Andreas Filipp, Tri Nguyen, Laurence Perreault-Levasseur, Jonah Rose, Chris Lovell, Nicolas Payot, Francisco Villaescusa-Navarro, Yashar Hezaveh

**Updated**: 2025-10-16T15:02:42Z

**Summary**: Strong gravitational lensing provides a powerful tool to directly infer the dark matter (DM) subhalo mass function (SHMF) in lens galaxies. However, comparing observationally inferred SHMFs to theoretical predictions remains challenging, as the predicted SHMF can vary significantly between galaxies - even within the same cosmological model - due to differences in the properties and environment of individual galaxies. We present a machine learning framework to infer the galaxy-specific predicted SHMF from galaxy images, conditioned on the assumed inverse warm DM particle mass $M^{-1}_{\rm DM}$. To train the model, we use 1024 high-resolution hydrodynamical zoom-in simulations from the DREAMS suite. Mock observations are generated using Synthesizer, excluding gas particle contributions, and SHMFs are computed with the Rockstar halo finder. Our neural network takes as input both the galaxy images and the inverse DM mass. This method enables scalable, image-based predictions for the theoretical DM SHMFs of individual galaxies, facilitating direct comparisons with observational measurements.

**Link**: [arxiv](http://arxiv.org/abs/2510.14766v1),  [pdf](http://arxiv.org/pdf/2510.14766v1)

**Tags**: astro-ph.CO astro-ph.GA 



### Benchmarking Adversarial Robustness to Bias Elicitation in Large   Language Models: Scalable Automated Assessment with LLM-as-a-Judge
**Authors**: Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, Domenico Talia

**Updated**: 2025-10-16T14:59:50Z

**Summary**: The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities. To facilitate systematic benchmarking, we release a curated dataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying DeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is uneven, with age, disability, and intersectional biases among the most prominent. Some small models outperform larger ones in safety, suggesting that training and architecture may matter more than scale. However, no model is fully robust to adversarial elicitation, with jailbreak attacks using low-resource languages or refusal suppression proving effective across model families. We also find that successive LLM generations exhibit slight safety gains, while models fine-tuned for the medical domain tend to be less safe than their general-purpose counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2504.07887v2),  [pdf](http://arxiv.org/pdf/2504.07887v2)

**Tags**: cs.CL cs.AI 



### Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware   Code
**Authors**: Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda

**Updated**: 2025-10-16T14:57:01Z

**Summary**: Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\%, delay efficiency of 65.9\%, and power efficiency of 64.0\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.

**Link**: [arxiv](http://arxiv.org/abs/2510.14756v1),  [pdf](http://arxiv.org/pdf/2510.14756v1)

**Tags**: cs.CL 



### ConDiSim: Conditional Diffusion Models for Simulation Based Inference
**Authors**: Mayank Nautiyal, Andreas Hellander, Prashant Singh

**Updated**: 2025-10-16T14:53:05Z

**Summary**: We present a conditional diffusion model - ConDiSim, for simulation-based inference of complex systems with intractable likelihoods. ConDiSim leverages denoising diffusion probabilistic models to approximate posterior distributions, consisting of a forward process that adds Gaussian noise to parameters, and a reverse process learning to denoise, conditioned on observed data. This approach effectively captures complex dependencies and multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark problems and two real-world test problems, where it demonstrates effective posterior approximation accuracy while maintaining computational efficiency and stability in model training. ConDiSim offers a robust and extensible framework for simulation-based inference, particularly suitable for parameter inference workflows requiring fast inference methods.

**Link**: [arxiv](http://arxiv.org/abs/2505.08403v2),  [pdf](http://arxiv.org/pdf/2505.08403v2)

**Tags**: cs.LG cs.AI stat.ML 



### Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries
**Authors**: Divyat Mahajan, Sachin Goyal, Badr Youbi Idrissi, Mohammad Pezeshki, Ioannis Mitliagkas, David Lopez-Paz, Kartik Ahuja

**Updated**: 2025-10-16T14:52:52Z

**Summary**: Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. We propose future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. We explore two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2510.14751v1),  [pdf](http://arxiv.org/pdf/2510.14751v1)

**Tags**: cs.LG cs.AI 



### Beyond Linear Probes: Dynamic Safety Monitoring for Language Models
**Authors**: James Oldfield, Philip Torr, Ioannis Patras, Adel Bibi, Fazl Barez

**Updated**: 2025-10-16T14:51:42Z

**Summary**: Monitoring large language models' (LLMs) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. TPCs provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can "buy" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with up to 30B parameters, we show that TPCs compete with or outperform MLP-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at http://github.com/james-oldfield/tpc.

**Link**: [arxiv](http://arxiv.org/abs/2509.26238v2),  [pdf](http://arxiv.org/pdf/2509.26238v2)

**Tags**: cs.LG 



### Treatment Effects Inference with High-Dimensional Instruments and   Control Variables
**Authors**: Xiduo Chen, Xingdong Feng, Antonio F. Galvao, Yeheng Ge

**Updated**: 2025-10-16T14:45:36Z

**Summary**: Obtaining valid treatment effect inference remains a challenging problem when dealing with numerous instruments and non-sparse control variables. In this paper, we propose a novel ridge regularization-based instrumental variables method for estimation and inference in the presence of both high-dimensional instrumental variables and high-dimensional control variables. These methods are applicable both with and without sparsity assumptions. To remove the estimation bias, we introduce a two-step procedure employing a ridge regression coupled with data-splitting in the first step, and a ridge style projection matrix with a simple least squares regression in the second. We establish statistical properties of the estimator, including consistency and asymptotic normality. Furthermore, we develop practical statistical inference procedures by providing a consistent estimator for the asymptotic variance of the estimator. The finite sample performance of the proposed methods is evaluated through numerical simulations. Results indicate that the new estimator consistently outperforms existing sparsity-based approaches across various settings, offering valuable insights for complex scenarios. Finally, we provide an empirical application estimating the causal effect of schooling on earnings addressing potential endogeneity through the use of high-dimensional instrumental variables and high-dimensional covariates.

**Link**: [arxiv](http://arxiv.org/abs/2503.20149v2),  [pdf](http://arxiv.org/pdf/2503.20149v2)

**Tags**: econ.EM 



### Knowledge Prompting: How Knowledge Engineers Use Large Language Models
**Authors**: Elisavet Koutsiana, Johanna Walker, Michelle Nwachukwu, Albert Meroño-Peñuela, Elena Simperl

**Updated**: 2025-10-16T14:34:41Z

**Summary**: Despite many advances in knowledge engineering (KE), challenges remain in areas such as engineering knowledge graphs (KGs) at scale, keeping up with evolving domain knowledge, multilingualism, and multimodality. Recently, KE has used LLMs to support semi-automatic tasks, but the most effective use of LLMs to support knowledge engineers across the KE activites is still in its infancy. To explore the vision of LLM copilots for KE and change existing KE practices, we conducted a multimethod study during a KE hackathon. We investigated participants' views on the use of LLMs, the challenges they face, the skills they may need to integrate LLMs into their practices, and how they use LLMs responsibly. We found participants felt LLMs could contribute to improving efficiency when engineering KGs, but presented increased challenges around the already complex issues of evaluating the KE tasks. We discovered prompting to be a useful but undervalued skill for knowledge engineers working with LLMs, and note that natural language processing skills may become more relevant across more roles in KG construction. Integrating LLMs into KE tasks needs to be mindful of potential risks and harms related to responsible AI. Given the limited ethical training, most knowledge engineers receive solutions such as our suggested `KG cards' based on data cards could be a useful guide for KG construction. Our findings can support designers of KE AI copilots, KE researchers, and practitioners using advanced AI to develop trustworthy applications, propose new methodologies for KE and operate new technologies responsibly.

**Link**: [arxiv](http://arxiv.org/abs/2408.08878v2),  [pdf](http://arxiv.org/pdf/2408.08878v2)

**Tags**: cs.HC 



### MoLink: Distributed and Efficient Serving Framework for Large Models
**Authors**: Lewei Jin, Yongqi Chen, Kui Zhang, Yifan Zhuo, Yi Gao, Bowei Yang, Zhengong Cai, Wei Dong

**Updated**: 2025-10-16T14:21:45Z

**Summary**: Large language models represent a groundbreaking shift in generative AI. Yet, these advances come with a significant challenge: the high cost of model serving. To mitigate these costs, consumer-grade GPUs emerge as a more affordable alternative. This presents an opportunity for more cost-efficient LLM serving by leveraging these GPUs.   However, it is non-trivial to achieve high-efficiency LLM serving on consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often deployed in limited network conditions; 2) these GPUs often exhibit heterogeneity in host systems. To address these challenges, we present MoLink, a distributed LLM serving system for large models. It incorporates several key techniques, enabling efficient LLM serving on heterogeneous and weakly connected consumer-grade GPUs. Our experiments demonstrate that it achieves throughput improvements of up to 458\% and cost-profit margin improvements of up to 151\%, compared to state-of-the-art systems. MoLink allows users on Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a few lines of code over Ethernet or public networks. Currently, it supports 18 mainstream architectures of open-source large language models. The source code is publicly available https://github.com/oldcpple/MoLink.

**Link**: [arxiv](http://arxiv.org/abs/2507.05043v2),  [pdf](http://arxiv.org/pdf/2507.05043v2)

**Tags**: cs.DC 



### Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous   References
**Authors**: Hongzheng Chen, Bin Fan, Alexander Collins, Bastian Hagedorn, Evghenii Gaburov, Masahiro Masuda, Matthew Brookhart, Chris Sullivan, Jason Knight, Zhiru Zhang, Vinod Grover

**Updated**: 2025-10-16T14:20:00Z

**Summary**: Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT programming model is fundamentally misaligned with this task-parallel hardware, creating a significant programmability gap. While hardware-level warp specialization is the key to unlocking peak performance, it forces developers to manually orchestrate complex, low-level communication and software pipelines--a process that is labor-intensive, error-prone, and unsustainable. To address this challenge, we present Tawa, an automated compiler that systematically generates high-performance, warp-specialized code from a high-level, tile-based program. Central to our approach is a novel IR abstraction, asynchronous references (aref), which expresses warp-level communication without exposing low-level hardware details. Using this abstraction, Tawa automatically partitions programs into producer-consumer roles and manages the intricate dataflow pipeline, relieving developers of invasive kernel rewriting. Evaluation on NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers high hardware utilization, achieving up to 1.1$\times$ speedup over highly optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains 1.2$\times$ speedup over Triton and matches the performance of the hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming effort.

**Link**: [arxiv](http://arxiv.org/abs/2510.14719v1),  [pdf](http://arxiv.org/pdf/2510.14719v1)

**Tags**: cs.LG cs.AR cs.PL 



### Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via   Internal Representations
**Authors**: Peng Lai, Jianjie Zheng, Sijie Cheng, Yun Chen, Peng Li, Yang Liu, Guanhua Chen

**Updated**: 2025-10-16T14:16:18Z

**Summary**: The growing scale of evaluation tasks has led to the widespread adoption of automated evaluation using LLMs, a paradigm known as "LLM-as-a-judge". However, improving its alignment with human preferences without complex prompts or fine-tuning remains challenging. Previous studies mainly optimize based on shallow outputs, overlooking rich cross-layer representations. In this work, motivated by preliminary findings that middle-to-upper layers encode semantically and task-relevant representations that are often more aligned with human judgments than the final layer, we propose LAGER, a post-hoc, plug-and-play framework for improving the alignment of LLM-as-a-Judge point-wise evaluations with human scores by leveraging internal representations. LAGER produces fine-grained judgment scores by aggregating cross-layer score-token logits and computing the expected score from a softmax-based distribution, while keeping the LLM backbone frozen and ensuring no impact on the inference process. LAGER fully leverages the complementary information across different layers, overcoming the limitations of relying solely on the final layer. We evaluate our method on the standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find that LAGER achieves improvements of up to 7.5% over the best baseline across these benchmarks. Without reasoning steps, LAGER matches or outperforms reasoning-based methods. Experiments on downstream applications, such as data selection and emotional understanding, further show the generalization of LAGER.

**Link**: [arxiv](http://arxiv.org/abs/2508.03550v3),  [pdf](http://arxiv.org/pdf/2508.03550v3)

**Tags**: cs.CL 



### Numerical Studies on the Radio Afterglows in TDE (I): Forward Shock
**Authors**: Guobin Mou

**Updated**: 2025-10-16T14:15:01Z

**Summary**: Recent long-term radio monitoring of tidal disruption events (TDEs) suggests that radio afterglows are common. Most studies argue that these afterglows may arise from forward shocks (FS) produced by the interaction between the TDE outflow and the hot, diffuse circumnuclear medium (CNM). Current theoretical models do not model the evolution of relativistic electrons in space, which introduces uncertainties. Here we conducted hydrodynamic simulations to study the hydrodynamic evolution of relativistic electrons, and calculated the synchrotron spectra via radiative transfer. We focus on the FS scenario with non-relativistic outflows, and various parameters of the outflow and CNM are explored. A moderate outflow with kinetic energy of several $10^{50}$ erg in a Galactic center - like CNM can produce mJy-level radio afterglows at a distance of 100 Mpc. The self-absorption frequency exhibits a slow decline at early times and a rapid decrease at late times. We derived the temporal evolution of the high-frequency radio flux, revealing its characteristic rise and decline pattern. We also find that: (1) the radio spectra for narrow outflows are clearly anisotropic along different sight lines; (2) the FS parameters inferred from radio spectra using conventional analytical formulas deviate significantly from those in simulations, in which the inferred shock radii are half of those from simulations, and the inferred energies are an order of magnitude lower.

**Link**: [arxiv](http://arxiv.org/abs/2510.14715v1),  [pdf](http://arxiv.org/pdf/2510.14715v1)

**Tags**: astro-ph.HE astro-ph.GA 



### Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in   Mathematics and Quantum Physics
**Authors**: Marco Del Tredici, Jacob McCarran, Benjamin Breen, Javier Aspuru Mijares, Weichen Winston Yin, Jacob M. Taylor, Frank H. L. Koppens, Dirk Englund

**Updated**: 2025-10-16T14:10:07Z

**Summary**: We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.

**Link**: [arxiv](http://arxiv.org/abs/2510.12787v2),  [pdf](http://arxiv.org/pdf/2510.12787v2)

**Tags**: cs.AI cs.MA 



### ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for   Function Calling
**Authors**: Jianghao Lin, Yuanyuan Shi, Xin Peng, Renjie Ding, Hairui Wang, Yuxuan Peng, Bizhe Bai, Weixi Song, Fengshuo Bai, Huacan Chai, Weinan Zhang, Fei Huang, Ying Wen

**Updated**: 2025-10-16T14:06:03Z

**Summary**: Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: "explore more but retain less" due to the unrecoverability characteristics of structured function calling generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.14703v1),  [pdf](http://arxiv.org/pdf/2510.14703v1)

**Tags**: cs.AI 



### The Last Dependency Crusade: Solving Python Dependency Conflicts with   LLMs
**Authors**: Antony Bartlett, Cynthia Liem, Annibale Panichella

**Updated**: 2025-10-16T14:05:53Z

**Summary**: Resolving Python dependency issues remains a tedious and error-prone process, forcing developers to manually trial compatible module versions and interpreter configurations. Existing automated solutions, such as knowledge-graph-based and database-driven methods, face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies. This paper investigates the use of Large Language Models (LLMs) to automatically repair dependency issues in Python programs. We propose PLLM (pronounced "plum"), a novel retrieval-augmented generation (RAG) approach that iteratively infers missing or incorrect dependencies. PLLM builds a test environment where the LLM proposes module combinations, observes execution feedback, and refines its predictions using natural language processing (NLP) to parse error messages. We evaluate PLLM on the Gistable HG2.9K dataset, a curated collection of real-world Python programs. Using this benchmark, we explore multiple PLLM configurations, including six open-source LLMs evaluated both with and without RAG. Our findings show that RAG consistently improves fix rates, with the best performance achieved by Gemma-2 9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo and ReadPyE, PLLM achieves significantly higher fix rates; +15.97\% more than ReadPyE and +21.58\% more than PyEGo. Further analysis shows that PLLM is especially effective for projects with numerous dependencies and those using specialized numerical or machine-learning libraries.

**Link**: [arxiv](http://arxiv.org/abs/2501.16191v2),  [pdf](http://arxiv.org/pdf/2501.16191v2)

**Tags**: cs.SE cs.AI 



### Cognitive-Aligned Spatio-Temporal Large Language Models For Next   Point-of-Interest Prediction
**Authors**: Penglong Zhai, Jie Li, Fanyi Di, Yue Liu, Yifang Yuan, Jie Huang, Peng Wu, Sicong Wang, Mingyang Yin, Tingting Hu, Yao Xu, Xin Li

**Updated**: 2025-10-16T14:05:28Z

**Summary**: The next point-of-interest (POI) recommendation task aims to predict the users' immediate next destinations based on their preferences and historical check-ins, holding significant value in location-based services. Recently, large language models (LLMs) have shown great potential in recommender systems, which treat the next POI prediction in a generative manner. However, these LLMs, pretrained primarily on vast corpora of unstructured text, lack the native understanding of structured geographical entities and sequential mobility patterns required for next POI prediction tasks. Moreover, in industrial-scale POI prediction applications, incorporating world knowledge and alignment of human cognition, such as seasons, weather conditions, holidays, and users' profiles (such as habits, occupation, and preferences), can enhance the user experience while improving recommendation performance. To address these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a framework employing natural language as an interface, allowing for the incorporation of world knowledge, spatio-temporal trajectory patterns, profiles, and situational information. Specifically, CoAST mainly comprises of 2 stages: (1) Recommendation Knowledge Acquisition through continued pretraining on the enriched spatial-temporal trajectory data of the desensitized users; (2) Cognitive Alignment to align cognitive judgments with human preferences using enriched training data through Supervised Fine-Tuning (SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline experiments on various real-world datasets and online experiments deployed in "Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of CoAST.

**Link**: [arxiv](http://arxiv.org/abs/2510.14702v1),  [pdf](http://arxiv.org/pdf/2510.14702v1)

**Tags**: cs.AI 



### LLM Agents for Automated Web Vulnerability Reproduction: Are We There   Yet?
**Authors**: Bin Liu, Yanjie Zhao, Guoai Xu, Haoyu Wang

**Updated**: 2025-10-16T14:04:46Z

**Summary**: Large language model (LLM) agents have demonstrated remarkable capabilities in software engineering and cybersecurity tasks, including code generation, vulnerability discovery, and automated testing. One critical but underexplored application is automated web vulnerability reproduction, which transforms vulnerability reports into working exploits. Although recent advances suggest promising potential, challenges remain in applying LLM agents to real-world web vulnerability reproduction scenarios. In this paper, we present the first comprehensive evaluation of state-of-the-art LLM agents for automated web vulnerability reproduction. We systematically assess 20 agents from software engineering, cybersecurity, and general domains across 16 dimensions, including technical capabilities, environment adaptability, and user experience factors, on 3 representative web vulnerabilities. Based on the results, we select three top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies. Our results reveal that while LLM agents achieve reasonable success on simple library-based vulnerabilities, they consistently fail on complex service-based vulnerabilities requiring multi-component environments. Complex environment configurations and authentication barriers create a gap where agents can execute exploit code but fail to trigger actual vulnerabilities. We observe high sensitivity to input guidance, with performance degrading by over 33% under incomplete authentication information. Our findings highlight the significant gap between current LLM agent capabilities and the demands of reliable automated vulnerability reproduction, emphasizing the need for advances in environmental adaptation and autonomous problem-solving capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2510.14700v1),  [pdf](http://arxiv.org/pdf/2510.14700v1)

**Tags**: cs.SE cs.CR 



### Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in   LLMs Continual Learning
**Authors**: Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin

**Updated**: 2025-10-16T14:03:22Z

**Summary**: Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in an end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment.To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter achieves higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark, and maintains its advantages on two larger benchmarks comprising 15 tasks.

**Link**: [arxiv](http://arxiv.org/abs/2505.22358v2),  [pdf](http://arxiv.org/pdf/2505.22358v2)

**Tags**: cs.LG cs.AI 



### Role-Aware Multi-modal federated learning system for detecting phishing   webpages
**Authors**: Bo Wang, Imran Khan, Martin White, Natalia Beloff

**Updated**: 2025-10-16T14:00:39Z

**Summary**: We present a federated, multi-modal phishing website detector that supports URL, HTML, and IMAGE inputs without binding clients to a fixed modality at inference: any client can invoke any modality head trained elsewhere. Methodologically, we propose role-aware bucket aggregation on top of FedProx, inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling separate aggregation of modality-specific parameters to isolate cross-embedding conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc 97.5% with FPR 2.4% across two data types; on the image subset (ablation) it attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc 96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results indicate that bucket aggregation with hard-gated experts enables stable federated training under strict privacy, while improving the usability and flexibility of multi-modal phishing detection.

**Link**: [arxiv](http://arxiv.org/abs/2509.22369v2),  [pdf](http://arxiv.org/pdf/2509.22369v2)

**Tags**: cs.LG cs.DC 



### Confidence-Based Response Abstinence: Improving LLM Trustworthiness via   Activation-Based Uncertainty Estimation
**Authors**: Zhiqi Huang, Vivek Datla, Chenyang Zhu, Alfy Samuel, Daben Liu, Anoop Kumar, Ritesh Soni

**Updated**: 2025-10-16T13:58:27Z

**Summary**: We propose a method for confidence estimation in retrieval-augmented generation (RAG) systems that aligns closely with the correctness of large language model (LLM) outputs. Confidence estimation is especially critical in high-stakes domains such as finance and healthcare, where the cost of an incorrect answer outweighs that of not answering the question. Our approach extends prior uncertainty quantification methods by leveraging raw feed-forward network (FFN) activations as auto-regressive signals, avoiding the information loss inherent in token logits and probabilities after projection and softmax normalization. We model confidence prediction as a sequence classification task, and regularize training with a Huber loss term to improve robustness against noisy supervision. Applied in a real-world financial industry customer-support setting with complex knowledge bases, our method outperforms strong baselines and maintains high accuracy under strict latency constraints. Experiments on Llama 3.1 8B model show that using activations from only the 16th layer preserves accuracy while reducing response latency. Our results demonstrate that activation-based confidence modeling offers a scalable, architecture-aware path toward trustworthy RAG deployment.

**Link**: [arxiv](http://arxiv.org/abs/2510.13750v2),  [pdf](http://arxiv.org/pdf/2510.13750v2)

**Tags**: cs.CL 



### xLLM Technical Report
**Authors**: Tongxuan Liu, Tao Peng, Peijun Yang, Xiaoyang Zhao, Xiusheng Lu, Weizhe Huang, Zirui Liu, Xiaoyu Chen, Zhiwei Liang, Jun Xiong, Donghe Jin, Minchao Zhang, Jinrong Guo, Yingxu Deng, Xu Zhang, Xianzhe Dong, Siqi Wang, Siyu Wu, Yu Wu, Zihan Tang, Yuting Zeng, Yanshu Wang, Jinguang Liu, Meng Kang, Menxin Li, Yunlong Wang, Yiming Liu, Xiaolong Ma, Yifan Wang, Yichen Zhang, Jinrun Yin, Keyang Zheng, Jiawei Yin, Jun Zhang, Ziyue Wang, Xiaobo Lin, Liangyu Liu, Liwei Lan, Yang Liu, Chunhua Peng, Han Liu, Songcheng Ren, Xuezhu Wang, Yunheng Shen, Yi Wang, Guyue Liu, Hui Chen, Tong Yang, Hailong Yang, Jing Li, Guiguang Ding, Ke Zhang

**Updated**: 2025-10-16T13:53:47Z

**Summary**: We introduce xLLM, an intelligent and efficient Large Language Model (LLM) inference framework designed for high-performance, large-scale enterprise-grade serving, with deep optimizations for diverse AI accelerators. To address these challenges, xLLM builds a novel decoupled service-engine architecture. At the service layer, xLLM-Service features an intelligent scheduling module that efficiently processes multimodal requests and co-locates online and offline tasks through unified elastic scheduling to maximize cluster utilization. This module also relies on a workload-adaptive dynamic Prefill-Decode (PD) disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation policy designed for multimodal inputs. Furthermore, it incorporates a distributed architecture to provide global KV Cache management and robust fault-tolerant capabilities for high availability. At the engine layer, xLLM-Engine co-optimizes system and algorithm designs to fully saturate computing resources. This is achieved through comprehensive multi-layer execution pipeline optimizations, an adaptive graph mode and an xTensor memory management. xLLM-Engine also further integrates algorithmic enhancements such as optimized speculative decoding and dynamic EPLB, collectively serving to substantially boost throughput and inference efficiency. Extensive evaluations demonstrate that xLLM delivers significantly superior performance and resource efficiency. Under identical TPOT constraints, xLLM achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining an average throughput of 1.7x that of MindIE with Deepseek-series models. xLLM framework is publicly available at https://github.com/jd-opensource/xllm and https://github.com/jd-opensource/xllm-service.

**Link**: [arxiv](http://arxiv.org/abs/2510.14686v1),  [pdf](http://arxiv.org/pdf/2510.14686v1)

**Tags**: cs.DC cs.AI 



### Hierarchical shot-noise Cox process mixtures for clustering across   groups
**Authors**: Alessandro Carminati, Mario Beraha, Federico Camerlenghi, Alessandra Guglielmi

**Updated**: 2025-10-16T13:46:45Z

**Summary**: Clustering observations across partially exchangeable groups of data is a routine task in Bayesian nonparametrics. Previously proposed models allow for clustering across groups by sharing atoms in the group-specific mixing measures. However, exact atom sharing can be overly rigid when groups differ subtly, introducing a trade-off between clustering and density estimates and fragmenting across-group clusters, particularly at larger sample sizes. We introduce the hierarchical shot-noise Cox process (HSNCP) mixture model, where group-specific atoms concentrate around shared centers through a kernel. This enables accurate density estimation within groups and flexible borrowing across groups, overcoming the density-clustering trade-off of previous approaches. Our construction, built on the shot-noise Cox process, remains analytically tractable: we derive closed-form prior moments and an inter-group correlation, obtain the marginal law and predictive distribution for latent parameters, as well as the posterior of the mixing measures given the latent parameters. We develop an efficient conditional MCMC algorithm for posterior inference. We assess the performance of the HSNCP model through simulations and an application to a large galaxy dataset, demonstrating balanced across-group clusters and improved density estimates compared with the hierarchical Dirichlet process, including under model misspecification.

**Link**: [arxiv](http://arxiv.org/abs/2510.14681v1),  [pdf](http://arxiv.org/pdf/2510.14681v1)

**Tags**: stat.ME 



### The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes   in Large Language Models
**Authors**: Konrad Löhr, Shuzhou Yuan, Michael Färber

**Updated**: 2025-10-16T13:44:28Z

**Summary**: Large Language Models (LLMs) are increasingly integral to information dissemination and decision-making processes. Given their growing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propagation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the inherent political leanings of these models. Subsequently, persona prompting with the PCT is used to explore explicit stereotypes across various social dimensions. In a final step, implicit stereotypes are uncovered by evaluating models with multilingual versions of the PCT. Key findings reveal a consistent left-leaning political alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those identified via explicit persona prompting. Interestingly, for most models, implicit and explicit stereotypes show a notable alignment, suggesting a degree of transparency or "awareness" regarding their inherent biases. This study underscores the complex interplay of political bias and stereotypes in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.08236v2),  [pdf](http://arxiv.org/pdf/2510.08236v2)

**Tags**: cs.LG cs.AI 



### On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification
**Authors**: Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, Xu Yang

**Updated**: 2025-10-16T13:40:55Z

**Summary**: We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.

**Link**: [arxiv](http://arxiv.org/abs/2508.05629v2),  [pdf](http://arxiv.org/pdf/2508.05629v2)

**Tags**: cs.LG 



### Augmenting Smart Contract Decompiler Output through Fine-grained   Dependency Analysis and LLM-facilitated Semantic Recovery
**Authors**: Zeqin Liao, Yuhong Nan, Zixu Gao, Henglong Liang, Sicheng Hao, Peifan Reng, Zibin Zheng

**Updated**: 2025-10-16T13:40:44Z

**Summary**: Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes.

**Link**: [arxiv](http://arxiv.org/abs/2501.08670v2),  [pdf](http://arxiv.org/pdf/2501.08670v2)

**Tags**: cs.SE 



### The simulation of judgment in LLMs
**Authors**: Edoardo Loru, Jacopo Nudo, Niccolò Di Marco, Alessandro Santirocchi, Roberto Atzeni, Matteo Cinelli, Vincenzo Cestari, Clelia Rossi-Arnaud, Walter Quattrociocchi

**Updated**: 2025-10-16T13:38:08Z

**Summary**: Large Language Models (LLMs) are increasingly embedded in evaluative processes, from information filtering to assessing and addressing knowledge gaps through explanation and credibility judgments. This raises the need to examine how such evaluations are built, what assumptions they rely on, and how their strategies diverge from those of humans. We benchmark six LLMs against expert ratings--NewsGuard and Media Bias/Fact Check--and against human judgments collected through a controlled experiment. We use news domains purely as a controlled benchmark for evaluative tasks, focusing on the underlying mechanisms rather than on news classification per se. To enable direct comparison, we implement a structured agentic framework in which both models and nonexpert participants follow the same evaluation procedure: selecting criteria, retrieving content, and producing justifications. Despite output alignment, our findings show consistent differences in the observable criteria guiding model evaluations, suggesting that lexical associations and statistical priors could influence evaluations in ways that differ from contextual reasoning. This reliance is associated with systematic effects: political asymmetries and a tendency to confuse linguistic form with epistemic reliability--a dynamic we term epistemia, the illusion of knowledge that emerges when surface plausibility replaces verification. Indeed, delegating judgment to such systems may affect the heuristics underlying evaluative processes, suggesting a shift from normative reasoning toward pattern-based approximation and raising open questions about the role of LLMs in evaluative processes.

**Link**: [arxiv](http://arxiv.org/abs/2502.04426v3),  [pdf](http://arxiv.org/pdf/2502.04426v3)

**Tags**: cs.CL cs.AI cs.CY 



### NAEL: Non-Anthropocentric Ethical Logic
**Authors**: Bianca Maria Lerma, Rafael Peñaloza

**Updated**: 2025-10-16T13:33:10Z

**Summary**: We introduce NAEL (Non-Anthropocentric Ethical Logic), a novel ethical framework for artificial agents grounded in active inference and symbolic reasoning. Departing from conventional, human-centred approaches to AI ethics, NAEL formalizes ethical behaviour as an emergent property of intelligent systems minimizing global expected free energy in dynamic, multi-agent environments. We propose a neuro-symbolic architecture to allow agents to evaluate the ethical consequences of their actions in uncertain settings. The proposed system addresses the limitations of existing ethical models by allowing agents to develop context-sensitive, adaptive, and relational ethical behaviour without presupposing anthropomorphic moral intuitions. A case study involving ethical resource distribution illustrates NAEL's dynamic balancing of self-preservation, epistemic learning, and collective welfare.

**Link**: [arxiv](http://arxiv.org/abs/2510.14676v1),  [pdf](http://arxiv.org/pdf/2510.14676v1)

**Tags**: cs.AI 



### Analysis of Hyperparameter Optimization Effects on Lightweight Deep   Models for Real-Time Image Classification
**Authors**: Vineet Kumar Rakesh, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das

**Updated**: 2025-10-16T13:29:58Z

**Summary**: Lightweight convolutional and transformer-based networks are increasingly preferred for real-time image classification, especially on resource-constrained devices. This study evaluates the impact of hyperparameter optimization on the accuracy and deployment feasibility of seven modern lightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L, MobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced subset of 90,000 images from ImageNet-1K. Under standardized training settings, this paper investigates the influence of learning rate schedules, augmentation, optimizers, and initialization on model performance. Inference benchmarks are performed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512, capturing latency and throughput in real-time conditions. This work demonstrates that controlled hyperparameter variation significantly alters convergence dynamics in lightweight CNN and transformer backbones, providing insight into stability regions and deployment feasibility in edge artificial intelligence. Our results reveal that tuning alone leads to a top-1 accuracy improvement of 1.5 to 3.5 percent over baselines, and select models (e.g., RepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800 frames per second, making them ideal for edge deployment. This work provides reproducible, subset-based insights into lightweight hyperparameter tuning and its role in balancing speed and accuracy. The code and logs may be seen at: https://vineetkumarrakesh.github.io/lcnn-opt

**Link**: [arxiv](http://arxiv.org/abs/2507.23315v2),  [pdf](http://arxiv.org/pdf/2507.23315v2)

**Tags**: cs.CV cs.AI cs.LG 



### VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning
**Authors**: Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma

**Updated**: 2025-10-16T13:29:02Z

**Summary**: In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: https://vtimecot.github.io

**Link**: [arxiv](http://arxiv.org/abs/2510.14672v1),  [pdf](http://arxiv.org/pdf/2510.14672v1)

**Tags**: cs.CV 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2025-10-16T13:25:38Z

**Summary**: Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods. Our code is available at https://github.com/FFY0/AdaKV.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v5),  [pdf](http://arxiv.org/pdf/2407.11550v5)

**Tags**: cs.CL cs.AI 



### Beyond Hallucinations: The Illusion of Understanding in Large Language   Models
**Authors**: Rikard Rosenbacke, Carl Rosenbacke, Victor Rosenbacke, Martin McKee

**Updated**: 2025-10-16T13:19:44Z

**Summary**: Large language models (LLMs) are becoming deeply embedded in human communication and decision-making, yet they inherit the ambiguity, bias, and lack of direct access to truth inherent in language itself. While their outputs are fluent, emotionally resonant, and coherent, they are generated through statistical prediction rather than grounded reasoning. This creates the risk of hallucination, responses that sound convincing but lack factual validity. Building on Geoffrey Hinton's observation that AI mirrors human intuition rather than reasoning, this paper argues that LLMs operationalize System 1 cognition at scale: fast, associative, and persuasive, but without reflection or falsification. To address this, we introduce the Rose-Frame, a three-dimensional framework for diagnosing cognitive and epistemic drift in human-AI interaction. The three axes are: (i) Map vs. Territory, which distinguishes representations of reality (epistemology) from reality itself (ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to separate fast, emotional judgments from slow, reflective thinking; and (iii) Conflict vs. Confirmation, which examines whether ideas are critically tested through disagreement or simply reinforced through mutual validation. Each dimension captures a distinct failure mode, and their combination amplifies misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules. Instead, it offers a reflective tool that makes both the model's limitations and the user's assumptions visible, enabling more transparent and critically aware AI deployment. It reframes alignment as cognitive governance: intuition, whether human or artificial, must remain governed by human reason. Only by embedding reflective, falsifiable oversight can we align machine fluency with human understanding.

**Link**: [arxiv](http://arxiv.org/abs/2510.14665v1),  [pdf](http://arxiv.org/pdf/2510.14665v1)

**Tags**: cs.AI cs.HC 



### SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality   Evaluation
**Authors**: Hui Wang, Jinghua Zhao, Yifan Yang, Shujie Liu, Junyang Chen, Yanzhe Zhang, Shiwan Zhao, Jinyu Li, Jiaming Zhou, Haoqin Sun, Yan Lu, Yong Qin

**Updated**: 2025-10-16T13:19:07Z

**Summary**: Generative speech technologies are progressing rapidly, but evaluating the perceptual quality of synthetic speech remains a core challenge. Existing methods typically rely on scalar scores or binary decisions, which lack interpretability and generalization across tasks and languages. We present SpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs) to conduct structured and explanation-based speech quality evaluation. To support this direction, we introduce SpeechEval, a large-scale dataset containing 32,207 multilingual speech clips and 128,754 annotations spanning four tasks: quality assessment, pairwise comparison, improvement suggestion, and deepfake detection. Based on this resource, we develop SQ-LLM, a speech-quality-aware LLM trained with chain-of-thought reasoning and reward optimization to improve capability. Experimental results show that SQ-LLM delivers strong performance across tasks and languages, revealing the potential of this paradigm for advancing speech quality evaluation. Relevant resources will be open-sourced.

**Link**: [arxiv](http://arxiv.org/abs/2510.14664v1),  [pdf](http://arxiv.org/pdf/2510.14664v1)

**Tags**: cs.SD eess.AS 



### Provable Mixed-Noise Learning with Flow-Matching
**Authors**: Paul Hagemann, Robert Gruhlke, Bernhard Stankewitz, Claudia Schillings, Gabriele Steidl

**Updated**: 2025-10-16T13:17:05Z

**Summary**: We study Bayesian inverse problems with mixed noise, modeled as a combination of additive and multiplicative Gaussian components. While traditional inference methods often assume fixed or known noise characteristics, real-world applications, particularly in physics and chemistry, frequently involve noise with unknown and heterogeneous structure. Motivated by recent advances in flow-based generative modeling, we propose a novel inference framework based on conditional flow matching embedded within an Expectation-Maximization (EM) algorithm to jointly estimate posterior samplers and noise parameters. To enable high-dimensional inference and improve scalability, we use simulation-free ODE-based flow matching as the generative model in the E-step of the EM algorithm. We prove that, under suitable assumptions, the EM updates converge to the true noise parameters in the population limit of infinite observations. Our numerical results illustrate the effectiveness of combining EM inference with flow matching for mixed-noise Bayesian inverse problems.

**Link**: [arxiv](http://arxiv.org/abs/2508.18122v2),  [pdf](http://arxiv.org/pdf/2508.18122v2)

**Tags**: cs.LG math.OC 



### An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs
**Authors**: Linyue Ma, Yilong Xu, Xiang Long, Zhi Zheng

**Updated**: 2025-10-16T13:15:40Z

**Summary**: Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, "nugget-as-rubric", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14660v1),  [pdf](http://arxiv.org/pdf/2510.14660v1)

**Tags**: cs.CL cs.AI cs.IR 



### Parameter Identification for Partial Differential Equation with Jump   Discontinuities in Coefficients by Markov Switching Model and   Physics-Informed Machine Learning
**Authors**: Zhikun Zhang, Guanyu Pan, Xiangjun Wang, Yong Xu, Guangtao Zhang

**Updated**: 2025-10-16T13:12:26Z

**Summary**: Inverse problems involving partial differential equations (PDEs) with discontinuous coefficients are fundamental challenges in modeling complex spatiotemporal systems with heterogeneous structures and uncertain dynamics. Traditional numerical and machine learning approaches often face limitations in addressing these problems due to high dimensionality, inherent nonlinearity, and discontinuous parameter spaces. In this work, we propose a novel computational framework that synergistically integrates physics-informed deep learning with Bayesian inference for accurate parameter identification in PDEs with jump discontinuities in coefficients. The core innovation of our framework lies in a dual-network architecture employing a gradient-adaptive weighting strategy: a main network approximates PDE solutions while a sub network samples its coefficients. To effectively identify mixture structures in parameter spaces, we employ Markovian dynamics methods to capture hidden state transitions of complex spatiotemporal systems. The framework has applications in reconstruction of solutions and identification of parameter-varying regions. Comprehensive numerical experiments on various PDEs with jump-varying coefficients demonstrate the framework's exceptional adaptability, accuracy, and robustness compared to existing methods. This study provides a generalizable computational approach of parameter identification for PDEs with discontinuous parameter structures, particularly in non-stationary or heterogeneous systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.14656v1),  [pdf](http://arxiv.org/pdf/2510.14656v1)

**Tags**: stat.ML cs.LG 



### O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis
**Authors**: Ayush Khaitan, Vijay Ganesh

**Updated**: 2025-10-16T13:07:41Z

**Summary**: Large language models have recently demonstrated advanced capabilities in solving IMO and Putnam problems; yet their role in research mathematics has remained fairly limited. The key difficulty is verification: suggested proofs may look plausible, but cannot be trusted without rigorous checking. We present a framework, called LLM+CAS, and an associated tool, O-Forge, that couples frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic Feedback loop to produce proofs that are both creative and symbolically verified. Our focus is on asymptotic inequalities, a topic that often involves difficult proofs and appropriate decomposition of the domain into the "right" subdomains. Many mathematicians, including Terry Tao, have suggested that using AI tools to find the right decompositions can be very useful for research-level asymptotic analysis. In this paper, we show that our framework LLM+CAS turns out to be remarkably effective at proposing such decompositions via a combination of a frontier LLM and a CAS. More precisely, we use an LLM to suggest domain decomposition, and a CAS (such as Mathematica) that provides a verification of each piece axiomatically. Using this loop, we answer a question posed by Terence Tao: whether LLMs coupled with a verifier can be used to help prove intricate asymptotic inequalities. More broadly, we show how AI can move beyond contest math towards research-level tools for professional mathematicians.

**Link**: [arxiv](http://arxiv.org/abs/2510.12350v2),  [pdf](http://arxiv.org/pdf/2510.12350v2)

**Tags**: cs.AI 03B35, 68W30, 68T05 



### Spatially anchored Tactile Awareness for Robust Dexterous Manipulation
**Authors**: Jialei Huang, Yang Ye, Yuanqing Gong, Xuezhou Zhu, Yang Gao, Kaifeng Zhang

**Updated**: 2025-10-16T12:59:34Z

**Summary**: Dexterous manipulation requires precise geometric reasoning, yet existing visuo-tactile learning methods struggle with sub-millimeter precision tasks that are routine for traditional model-based approaches. We identify a key limitation: while tactile sensors provide rich contact information, current learning frameworks fail to effectively leverage both the perceptual richness of tactile signals and their spatial relationship with hand kinematics. We believe an ideal tactile representation should explicitly ground contact measurements in a stable reference frame while preserving detailed sensory information, enabling policies to not only detect contact occurrence but also precisely infer object geometry in the hand's coordinate system. We introduce SaTA (Spatially-anchored Tactile Awareness for dexterous manipulation), an end-to-end policy framework that explicitly anchors tactile features to the hand's kinematic frame through forward kinematics, enabling accurate geometric reasoning without requiring object models or explicit pose estimation. Our key insight is that spatially grounded tactile representations allow policies to not only detect contact occurrence but also precisely infer object geometry in the hand's coordinate system. We validate SaTA on challenging dexterous manipulation tasks, including bimanual USB-C mating in free space, a task demanding sub-millimeter alignment precision, as well as light bulb installation requiring precise thread engagement and rotational control, and card sliding that demands delicate force modulation and angular precision. These tasks represent significant challenges for learning-based methods due to their stringent precision requirements. Across multiple benchmarks, SaTA significantly outperforms strong visuo-tactile baselines, improving success rates by up to 30 percentage while reducing task completion times by 27 percentage.

**Link**: [arxiv](http://arxiv.org/abs/2510.14647v1),  [pdf](http://arxiv.org/pdf/2510.14647v1)

**Tags**: cs.RO 



### The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon   Blockchain
**Authors**: Andrei Seoev, Leonid Gremyachikh, Anastasiia Smirnova, Yash Madhwal, Alisa Kalacheva, Dmitry Belousov, Ilia Zubov, Aleksei Smirnov, Denis Fedyanin, Vladimir Gorgadze, Yury Yanovich

**Updated**: 2025-10-16T12:54:53Z

**Summary**: In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\% of available profits when deployed alongside existing searchers and 81\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.

**Link**: [arxiv](http://arxiv.org/abs/2510.14642v1),  [pdf](http://arxiv.org/pdf/2510.14642v1)

**Tags**: cs.GT cs.AI cs.DC 



### Intent Clustering with Shared Pseudo-Labels
**Authors**: I-Fan Lin, Faegheh Hasibi, Suzan Verberne

**Updated**: 2025-10-17T11:18:40Z

**Summary**: In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2510.14640v2),  [pdf](http://arxiv.org/pdf/2510.14640v2)

**Tags**: cs.CL cs.IR 



## Keyword: LLM Deployment 
 ### Agentic Design of Compositional Machines
**Authors**: Wenqian Zhang, Weiyang Liu, Zhen Liu

**Updated**: 2025-10-16T17:59:58Z

**Summary**: The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.14980v1),  [pdf](http://arxiv.org/pdf/2510.14980v1)

**Tags**: cs.AI cs.CL cs.CV cs.GR cs.LG 



### Attention Is All You Need for KV Cache in Diffusion LLMs
**Authors**: Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen

**Updated**: 2025-10-16T17:59:48Z

**Summary**: This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences, and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14973v1),  [pdf](http://arxiv.org/pdf/2510.14973v1)

**Tags**: cs.CL cs.AI cs.LG 



### TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar
**Authors**: Yinxi Li, Yuntian Deng, Pengyu Nie

**Updated**: 2025-10-16T17:59:45Z

**Summary**: Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14972v1),  [pdf](http://arxiv.org/pdf/2510.14972v1)

**Tags**: cs.CL cs.AI cs.LG cs.PL cs.SE 



### LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent   Training
**Authors**: Yiming Wang, Da Yin, Yuedong Cui, Ruichen Zheng, Zhiqian Li, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Yu Zhou, Kai-Wei Chang

**Updated**: 2025-10-16T17:59:38Z

**Summary**: Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.

**Link**: [arxiv](http://arxiv.org/abs/2510.14969v1),  [pdf](http://arxiv.org/pdf/2510.14969v1)

**Tags**: cs.CL cs.AI cs.LG 



### Biology-informed neural networks learn nonlinear representations from   omics data to improve genomic prediction and interpretability
**Authors**: Katiana Kontolati, Rini Jasmine Gladstone, Ian Davis, Ethan Pickering

**Updated**: 2025-10-16T17:59:38Z

**Summary**: We extend biologically-informed neural networks (BINNs) for genomic prediction (GP) and selection (GS) in crops by integrating thousands of single-nucleotide polymorphisms (SNPs) with multi-omics measurements and prior biological knowledge. Traditional genotype-to-phenotype (G2P) models depend heavily on direct mappings that achieve only modest accuracy, forcing breeders to conduct large, costly field trials to maintain or marginally improve genetic gain. Models that incorporate intermediate molecular phenotypes such as gene expression can achieve higher predictive fit, but they remain impractical for GS since such data are unavailable at deployment or design time. BINNs overcome this limitation by encoding pathway-level inductive biases and leveraging multi-omics data only during training, while using genotype data alone during inference. Applied to maize gene-expression and multi-environment field-trial data, BINN improves rank-correlation accuracy by up to 56% within and across subpopulations under sparse-data conditions and nonlinearly identifies genes that GWAS/TWAS fail to uncover. With complete domain knowledge for a synthetic metabolomics benchmark, BINN reduces prediction error by 75% relative to conventional neural nets and correctly identifies the most important nonlinear pathway. Importantly, both cases show highly sensitive BINN latent variables correlate with the experimental quantities they represent, despite not being trained on them. This suggests BINNs learn biologically-relevant representations, nonlinear or linear, from genotype to phenotype. Together, BINNs establish a framework that leverages intermediate domain information to improve genomic prediction accuracy and reveal nonlinear biological relationships that can guide genomic selection, candidate gene selection, pathway enrichment, and gene-editing prioritization.

**Link**: [arxiv](http://arxiv.org/abs/2510.14970v1),  [pdf](http://arxiv.org/pdf/2510.14970v1)

**Tags**: cs.LG 



### Information Gain-based Policy Optimization: A Simple and Effective   Approach for Multi-Turn LLM Agents
**Authors**: Guoqing Wang, Sunhao Dai, Guangze Ye, Zeyu Gan, Wei Yao, Yong Deng, Xiaofeng Wu, Zhenzhe Ying

**Updated**: 2025-10-16T17:59:32Z

**Summary**: Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2510.14967v1),  [pdf](http://arxiv.org/pdf/2510.14967v1)

**Tags**: cs.CL cs.AI cs.LG 



### Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity   in TVD-MI Scores
**Authors**: Zachary Robertson

**Updated**: 2025-10-16T17:59:25Z

**Summary**: Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MI's binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \pm 0.008$ while preserving agent rankings (Spearman $\rho = 0.972 \pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\rho = 0.872$) and consistent identity-link advantage. TVD-MI's geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains.

**Link**: [arxiv](http://arxiv.org/abs/2510.14966v1),  [pdf](http://arxiv.org/pdf/2510.14966v1)

**Tags**: cs.LG stat.ML 



### ChangingGrounding: 3D Visual Grounding in Changing Scenes
**Authors**: Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu

**Updated**: 2025-10-16T17:59:16Z

**Summary**: Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .

**Link**: [arxiv](http://arxiv.org/abs/2510.14965v1),  [pdf](http://arxiv.org/pdf/2510.14965v1)

**Tags**: cs.CV 



### MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal   Mathematical Reasoning
**Authors**: Weikang Shi, Aldrich Yu, Rongyao Fang, Houxing Ren, Ke Wang, Aojun Zhou, Changyao Tian, Xinyu Fu, Yuxuan Hu, Zimu Lu, Linjiang Huang, Si Liu, Rui Liu, Hongsheng Li

**Updated**: 2025-10-16T17:58:58Z

**Summary**: While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2510.14958v1),  [pdf](http://arxiv.org/pdf/2510.14958v1)

**Tags**: cs.CV cs.CL 



### CBF-RL: Safety Filtering Reinforcement Learning in Training with Control   Barrier Functions
**Authors**: Lizhi Yang, Blake Werner, Massimiliano de Sa Aaron D. Ames

**Updated**: 2025-10-16T17:58:58Z

**Summary**: Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter.

**Link**: [arxiv](http://arxiv.org/abs/2510.14959v1),  [pdf](http://arxiv.org/pdf/2510.14959v1)

**Tags**: cs.RO cs.AI cs.LG cs.SY eess.SY 



### From Language to Locomotion: Retargeting-free Humanoid Control via   Motion Latent Guidance
**Authors**: Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, Chang Xu

**Updated**: 2025-10-16T17:57:47Z

**Summary**: Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.

**Link**: [arxiv](http://arxiv.org/abs/2510.14952v1),  [pdf](http://arxiv.org/pdf/2510.14952v1)

**Tags**: cs.RO cs.CV 



### EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge   Devices
**Authors**: Romina Aalishah, Mozhgan Navardi, Tinoosh Mohsenin

**Updated**: 2025-10-16T17:55:56Z

**Summary**: Deployment of efficient and accurate Deep Learning models has long been a challenge in autonomous navigation, particularly for real-time applications on resource-constrained edge devices. Edge devices are limited in computing power and memory, making model efficiency and compression essential. In this work, we propose EdgeNavMamba, a reinforcement learning-based framework for goal-directed navigation using an efficient Mamba object detection model. To train and evaluate the detector, we introduce a custom shape detection dataset collected in diverse indoor settings, reflecting visual cues common in real-world navigation. The object detector serves as a pre-processing module, extracting bounding boxes (BBOX) from visual input, which are then passed to an RL policy to control goal-oriented navigation. Experimental results show that the student model achieved a reduction of 67% in size, and up to 73% in energy per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5, while keeping the same performance as the teacher model. EdgeNavMamba also maintains high detection accuracy in MiniWorld and IsaacLab simulators while reducing parameters by 31% compared to the baseline. In the MiniWorld simulator, the navigation policy achieves over 90% success across environments of varying complexity.

**Link**: [arxiv](http://arxiv.org/abs/2510.14946v1),  [pdf](http://arxiv.org/pdf/2510.14946v1)

**Tags**: eess.IV cs.RO 



### MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics
**Authors**: Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang

**Updated**: 2025-10-16T17:55:14Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.

**Link**: [arxiv](http://arxiv.org/abs/2510.14944v1),  [pdf](http://arxiv.org/pdf/2510.14944v1)

**Tags**: cs.CL cs.AI cs.CE 



### LaSeR: Reinforcement Learning with Last-Token Self-Rewarding
**Authors**: Wenkai Yang, Weijie Liu, Ruobing Xie, Yiju Guo, Lulu Wu, Saiyong Yang, Yankai Lin

**Updated**: 2025-10-16T17:55:11Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.14943v1),  [pdf](http://arxiv.org/pdf/2510.14943v1)

**Tags**: cs.CL cs.AI cs.LG 



### GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for   Step-Level Reasoning
**Authors**: Yao Zhang, Yu Wu, Haowei Zhang, Weiguo Li, Haokun Chen, Jingpei Wu, Guohao Li, Zhen Han, Volker Tresp

**Updated**: 2025-10-16T17:54:07Z

**Summary**: Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2510.14942v1),  [pdf](http://arxiv.org/pdf/2510.14942v1)

**Tags**: cs.AI 



### AI-Powered Early Diagnosis of Mental Health Disorders from Real-World   Clinical Conversations
**Authors**: Jianfeng Zhu, Julina Maharjan, Xinyu Li, Karin G. Coifman, Ruoming Jin

**Updated**: 2025-10-16T17:50:04Z

**Summary**: Mental health disorders remain among the leading cause of disability worldwide, yet conditions such as depression, anxiety, and Post-Traumatic Stress Disorder (PTSD) are frequently underdiagnosed or misdiagnosed due to subjective assessments, limited clinical resources, and stigma and low awareness. In primary care settings, studies show that providers misidentify depression or anxiety in over 60% of cases, highlighting the urgent need for scalable, accessible, and context-aware diagnostic tools that can support early detection and intervention. In this study, we evaluate the effectiveness of machine learning models for mental health screening using a unique dataset of 553 real-world, semistructured interviews, each paried with ground-truth diagnoses for major depressive episodes (MDE), anxiety disorders, and PTSD. We benchmark multiple model classes, including zero-shot prompting with GPT-4.1 Mini and MetaLLaMA, as well as fine-tuned RoBERTa models using LowRank Adaptation (LoRA). Our models achieve over 80% accuracy across diagnostic categories, with especially strongperformance on PTSD (up to 89% accuracy and 98% recall). We also find that using shorter context, focused context segments improves recall, suggesting that focused narrative cues enhance detection sensitivity. LoRA fine-tuning proves both efficient and effective, with lower-rank configurations (e.g., rank 8 and 16) maintaining competitive performance across evaluation metrics. Our results demonstrate that LLM-based models can offer substantial improvements over traditional self-report screening tools, providing a path toward low-barrier, AI-powerd early diagnosis. This work lays the groundwork for integrating machine learning into real-world clinical workflows, particularly in low-resource or high-stigma environments where access to timely mental health care is most limited.

**Link**: [arxiv](http://arxiv.org/abs/2510.14937v1),  [pdf](http://arxiv.org/pdf/2510.14937v1)

**Tags**: cs.CL 



### Circuit Insights: Towards Interpretability Beyond Activations
**Authors**: Elena Golimblevskaia, Aakriti Jain, Bruno Puri, Ammar Ibrahim, Wojciech Samek, Sebastian Lapuschkin

**Updated**: 2025-10-16T17:49:41Z

**Summary**: The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.

**Link**: [arxiv](http://arxiv.org/abs/2510.14936v1),  [pdf](http://arxiv.org/pdf/2510.14936v1)

**Tags**: cs.LG cs.AI cs.CL 



### VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular   Design
**Authors**: Malikussaid, Hilal Hudan Nuha, Isman Kurniawan

**Updated**: 2025-10-16T17:43:31Z

**Summary**: Large Language Models demonstrate substantial promise for advancing scientific discovery, yet their deployment in disciplines demanding factual precision and specialized domain constraints presents significant challenges. Within molecular design for pharmaceutical development, these models can propose innovative molecular modifications but frequently generate chemically infeasible structures. We introduce VALID-Mol, a comprehensive framework that integrates chemical validation with LLM-driven molecular design, achieving an improvement in valid chemical structure generation from 3% to 83%. Our methodology synthesizes systematic prompt optimization, automated chemical verification, and domain-adapted fine-tuning to ensure dependable generation of synthesizable molecules with enhanced properties. Our contribution extends beyond implementation details to provide a transferable methodology for scientifically-constrained LLM applications with measurable reliability enhancements. Computational analyses indicate our framework generates promising synthesis candidates with up to 17-fold predicted improvements in target binding affinity while preserving synthetic feasibility.

**Link**: [arxiv](http://arxiv.org/abs/2506.23339v2),  [pdf](http://arxiv.org/pdf/2506.23339v2)

**Tags**: cs.LG cs.AI physics.chem-ph q-bio.QM 68T50 (Primary) 92E10, 68T07 (Secondary) I.2.7; J.3; I.2.1; I.2.6 



### Stable but Miscalibrated: A Kantian View on Overconfidence from Filters   to Large Language Models
**Authors**: Akira Okutomi

**Updated**: 2025-10-16T17:40:28Z

**Summary**: We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we find that fragile internal dynamics correlate with miscalibration and hallucination, while critique-style prompts show mixed effects on calibration and hallucination. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens for diagnosing -- and selectively reducing -- overconfidence in reasoning systems. This is a preliminary version; supplementary experiments and broader replication will be reported in a future revision.

**Link**: [arxiv](http://arxiv.org/abs/2510.14925v1),  [pdf](http://arxiv.org/pdf/2510.14925v1)

**Tags**: cs.AI cs.CL cs.LG 



### Unmasking Hiring Bias: Platform Data Analysis and Controlled Experiments   on Bias in Online Freelance Marketplaces via RAG-LLM Generated Contents
**Authors**: Wugeng Zheng, Guohou Shan

**Updated**: 2025-10-16T17:39:20Z

**Summary**: Online freelance marketplaces, a rapidly growing part of the global labor market, are creating a fair environment where professional skills are the main factor for hiring. While these platforms can reduce bias from traditional hiring, the personal information in user profiles raises concerns about ongoing discrimination. Past studies on this topic have mostly used existing data, which makes it hard to control for other factors and clearly see the effect of things like gender or race. To solve these problems, this paper presents a new method that uses Retrieval-Augmented Generation (RAG) with a Large Language Model (LLM) to create realistic, artificial freelancer profiles for controlled experiments. This approach effectively separates individual factors, enabling a clearer statistical analysis of how different variables influence the freelancer project process. In addition to analyzing extracted data with traditional statistical methods for post-project stage analysis, our research utilizes a dataset with highly controlled variables, generated by an RAG-LLM, to conduct a simulated hiring experiment for pre-project stage analysis. The results of our experiments show that, regarding gender, while no significant preference emerged in initial hiring decisions, female freelancers are substantially more likely to receive imperfect ratings post-project stage. Regarding regional bias, a strong and consistent preference favoring US-based freelancers shows that people are more likely to be selected in the simulated experiments, perceived as more leader-like, and receive higher ratings on the live platform.

**Link**: [arxiv](http://arxiv.org/abs/2510.13091v2),  [pdf](http://arxiv.org/pdf/2510.13091v2)

**Tags**: cs.HC 



### Predicting Task Performance with Context-aware Scaling Laws
**Authors**: Kyle Montgomery, David Park, Jianhong Tu, Michael Bendersky, Beliz Gunel, Dawn Song, Chenguang Wang

**Updated**: 2025-10-16T17:35:18Z

**Summary**: Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.

**Link**: [arxiv](http://arxiv.org/abs/2510.14919v1),  [pdf](http://arxiv.org/pdf/2510.14919v1)

**Tags**: cs.CL cs.AI cs.LG 



### Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent   Generation
**Authors**: Xujun Peng, Anoop Kumar, Jingyu Wu, Parker Glenn, Daben Liu

**Updated**: 2025-10-16T17:30:28Z

**Summary**: Retrieval-Augmented Generation (RAG) systems leverage Large Language Models (LLMs) to generate accurate and reliable responses that are grounded in retrieved context. However, LLMs often generate inconsistent outputs for semantically equivalent inputs, a problem compounded by the scarcity of consistency-focused training data and the limitations of current fine-tuning techniques in enhancing output consistency. We propose a new approach combining systematic synthetic data generation, triplet loss for better embeddings, and a novel layer-wise model merging approach. Using consistency-aware weights derived from intermediate layer activations, our method effectively integrates knowledge from specialized models. Experimental results how that our merged model significantly enhances output consistency, achieving a ~47.5\% improvement in response similarity over the baseline, thus offering a practical solution for increasing the reliability of an industrial RAG system.

**Link**: [arxiv](http://arxiv.org/abs/2510.14915v1),  [pdf](http://arxiv.org/pdf/2510.14915v1)

**Tags**: cs.CL 



### Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning
**Authors**: Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong

**Updated**: 2025-10-16T17:30:21Z

**Summary**: Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach suffers from catastrophic forgetting: second-stage RL gradually loses SFT-acquired behaviors and inefficiently explores new patterns. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2509.06948v2),  [pdf](http://arxiv.org/pdf/2509.06948v2)

**Tags**: cs.CL 



### Is Misinformation More Open? A Study of robots.txt Gatekeeping on the   Web
**Authors**: Nicolas Steinacker-Olsztyn, Devashish Gosain, Ha Dao

**Updated**: 2025-10-16T17:26:58Z

**Summary**: Large Language Models (LLMs) are increasingly relying on web crawling to stay up to date and accurately answer user queries. These crawlers are expected to honor robots.txt files, which govern automated access. In this study, for the first time, we investigate whether reputable news websites and misinformation sites differ in how they configure these files, particularly in relation to AI crawlers. Analyzing a curated dataset, we find a stark contrast: 60.0% of reputable sites disallow at least one AI crawler, compared to just 9.1% of misinformation sites in their robots.txt files. Reputable sites forbid an average of 15.5 AI user agents, while misinformation sites prohibit fewer than one. We then measure active blocking behavior, where websites refuse to return content when HTTP requests include AI crawler user agents, and reveal that both categories of websites utilize it. Notably, the behavior of reputable news websites in this regard aligns more closely with their declared robots.txt directive than that of misinformation websites. Finally, our longitudinal analysis reveals that this gap has widened over time, with AI-blocking by reputable sites rising from 23% in September 2023 to nearly 60% by May 2025. Our findings highlight a growing asymmetry in content accessibility that may shape the training data available to LLMs, raising essential questions for web transparency, data ethics, and the future of AI training practices.

**Link**: [arxiv](http://arxiv.org/abs/2510.10315v3),  [pdf](http://arxiv.org/pdf/2510.10315v3)

**Tags**: cs.CY 



### Reasoning with Sampling: Your Base Model is Smarter Than You Think
**Authors**: Aayush Karan, Yilun Du

**Updated**: 2025-10-16T17:18:11Z

**Summary**: Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.

**Link**: [arxiv](http://arxiv.org/abs/2510.14901v1),  [pdf](http://arxiv.org/pdf/2510.14901v1)

**Tags**: cs.LG cs.AI cs.CL 



### Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent   That Improves Without Labels or Model Updates
**Authors**: Wen-Kwang Tsao, Yao-Ching Yu, Chien-Ming Huang

**Updated**: 2025-10-16T17:17:00Z

**Summary**: The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\%(LLM-only) to 72.73\%(RAG) to 93.94\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.

**Link**: [arxiv](http://arxiv.org/abs/2510.14900v1),  [pdf](http://arxiv.org/pdf/2510.14900v1)

**Tags**: cs.AI cs.CR 



### Leveraging Multimodal LLM Descriptions of Activity for Explainable   Semi-Supervised Video Anomaly Detection
**Authors**: Furkan Mumcu, Michael J. Jones, Anoop Cherian, Yasin Yilmaz

**Updated**: 2025-10-16T17:13:33Z

**Summary**: Existing semi-supervised video anomaly detection (VAD) methods often struggle with detecting complex anomalies involving object interactions and generally lack explainability. To overcome these limitations, we propose a novel VAD framework leveraging Multimodal Large Language Models (MLLMs). Unlike previous MLLM-based approaches that make direct anomaly judgments at the frame level, our method focuses on extracting and interpreting object activity and interactions over time. By querying an MLLM with visual inputs of object pairs at different moments, we generate textual descriptions of the activity and interactions from nominal videos. These textual descriptions serve as a high-level representation of the activity and interactions of objects in a video. They are used to detect anomalies during test time by comparing them to textual descriptions found in nominal training videos. Our approach inherently provides explainability and can be combined with many traditional VAD methods to further enhance their interpretability. Extensive experiments on benchmark datasets demonstrate that our method not only detects complex interaction-based anomalies effectively but also achieves state-of-the-art performance on datasets without interaction anomalies.

**Link**: [arxiv](http://arxiv.org/abs/2510.14896v1),  [pdf](http://arxiv.org/pdf/2510.14896v1)

**Tags**: cs.CV 



### You May Speak Freely: Improving the Fine-Grained Visual Recognition   Capabilities of Multimodal Large Language Models with Answer Extraction
**Authors**: Logan Lawrence, Oindrila Saha, Megan Wei, Chen Sun, Subhransu Maji, Grant Van Horn

**Updated**: 2025-10-16T17:04:25Z

**Summary**: Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.

**Link**: [arxiv](http://arxiv.org/abs/2510.14885v1),  [pdf](http://arxiv.org/pdf/2510.14885v1)

**Tags**: cs.CV cs.CL 



### The Gatekeeper Knows Enough
**Authors**: Fikresilase Wondmeneh Abebayew

**Updated**: 2025-10-16T17:00:42Z

**Summary**: Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity "latent state" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.

**Link**: [arxiv](http://arxiv.org/abs/2510.14881v1),  [pdf](http://arxiv.org/pdf/2510.14881v1)

**Tags**: cs.AI cs.IT math.IT 



### BADAS: Context Aware Collision Prediction Using Real-World Dashcam Data
**Authors**: Roni Goldshmidt, Hamish Scott, Lorenzo Niccolini, Shizhan Zhu, Daniel Moura, Orly Zvitia

**Updated**: 2025-10-16T16:55:30Z

**Summary**: Existing collision prediction methods often fail to distinguish between ego-vehicle threats and random accidents not involving the ego vehicle, leading to excessive false alerts in real-world deployment. We present BADAS, a family of collision prediction models trained on Nexar's real-world dashcam collision dataset -- the first benchmark designed explicitly for ego-centric evaluation. We re-annotate major benchmarks to identify ego involvement, add consensus alert-time labels, and synthesize negatives where needed, enabling fair AP/AUC and temporal evaluation. BADAS uses a V-JEPA2 backbone trained end-to-end and comes in two variants: BADAS-Open (trained on our 1.5k public videos) and BADAS1.0 (trained on 40k proprietary videos). Across DAD, DADA-2000, DoTA, and Nexar, BADAS achieves state-of-the-art AP/AUC and outperforms a forward-collision ADAS baseline while producing more realistic time-to-accident estimates. We release our BADAS-Open model weights and code, along with re-annotations of all evaluation datasets to promote ego-centric collision prediction research.

**Link**: [arxiv](http://arxiv.org/abs/2510.14876v1),  [pdf](http://arxiv.org/pdf/2510.14876v1)

**Tags**: cs.CV 



### NEFT: A Unified Transformer Framework for Efficient Near-Field CSI   Feedback in XL-MIMO Systems
**Authors**: Haiyang Li, Tianqi Mao, Pengyu Wang, Ruiqi Liu, Shunyu Li, Zhaocheng Wang

**Updated**: 2025-10-16T16:53:39Z

**Summary**: Extremely large-scale multiple-input multiple-output (XL-MIMO) systems, operating in the near-field region due to their massive antenna arrays, are key enablers of next-generation wireless communications but face significant challenges in channel state information (CSI) feedback. Deep learning has emerged as a powerful tool by learning compact CSI representations for feedback. However, existing methods struggle to capture the intricate structure of near-field CSI and incur prohibitive computational overhead on practical mobile devices.   To overcome these limitations, we propose the Near-Field Efficient Feedback Transformer (NEFT) family for accurate and efficient near-field CSI feedback across diverse hardware platforms. Built on a hierarchical Vision Transformer backbone, NEFT is extended with lightweight variants to meet various deployment constraints: NEFT-Compact applies multi-level knowledge distillation (KD) to reduce complexity while maintaining accuracy, whereas NEFT-Hybrid and NEFT-Edge address encoder- and edge-constrained scenarios via attention-free encoding and KD.   Extensive simulations show that NEFT achieves a 15--21 dB improvement in normalized mean-squared error (NMSE) over state-of-the-art methods, while NEFT-Compact and NEFT-Edge reduce total FLOPs by 25--36% with negligible accuracy loss. Moreover, NEFT-Hybrid reduces encoder-side complexity by up to 64%, enabling deployment in highly asymmetric device scenarios. These results establish NEFT as a practical and scalable solution for near-field CSI feedback in XL-MIMO systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.12748v2),  [pdf](http://arxiv.org/pdf/2509.12748v2)

**Tags**: eess.SP 



### Chiplet-Based RISC-V SoC with Modular AI Acceleration
**Authors**: P. Ramkumar, S. S. Bharadwaj

**Updated**: 2025-10-16T16:44:51Z

**Summary**: Achieving high performance, energy efficiency, and cost-effectiveness while maintaining architectural flexibility is a critical challenge in the development and deployment of edge AI devices. Monolithic SoC designs struggle with this complex balance mainly due to low manufacturing yields (below 16%) at advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based RISC-V SoC architecture that addresses these limitations through modular AI acceleration and intelligent system level optimization. Our proposed design integrates 4 different key innovations in a 30mm x 30mm silicon interposer: adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring streaming flow control units and compression-aware transfers; distributed cryptographic security across heterogeneous chiplets; and intelligent sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory stacks, and dedicated power management controllers. Experimental results across industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video processing demonstrate significant performance improvements. The AI-optimized configuration achieves ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power reduction compared to previous basic chiplet implementations. These improvements collectively translate to a 40.1% efficiency gain corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while maintaining sub-5ms real-time capability across all experimented workloads. These performance upgrades demonstrate that modular chiplet designs can achieve near-monolithic computational density while enabling cost efficiency, scalability and upgradeability, crucial for next-generation edge AI device applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.18355v2),  [pdf](http://arxiv.org/pdf/2509.18355v2)

**Tags**: cs.AR cs.AI 



### SteeringSafety: A Systematic Safety Evaluation Framework of   Representation Steering in LLMs
**Authors**: Vincent Siu, Nicholas Crispino, David Park, Nathan W. Henry, Zhun Wang, Yang Liu, Dawn Song, Chenguang Wang

**Updated**: 2025-10-16T16:44:31Z

**Summary**: We introduce SteeringSafety, a systematic framework for evaluating representation steering methods across seven safety perspectives spanning 17 datasets. While prior work highlights general capabilities of representation steering, we systematically explore safety perspectives including bias, harmfulness, hallucination, social behaviors, reasoning, epistemic integrity, and normative judgment. Our framework provides modularized building blocks for state-of-the-art steering methods, enabling unified implementation of DIM, ACE, CAA, PCA, and LAT with recent enhancements like conditional steering. Results on Gemma-2-2B, Llama-3.1-8B, and Qwen-2.5-7B reveal that strong steering performance depends critically on pairing of method, model, and specific perspective. DIM shows consistent effectiveness, but all methods exhibit substantial entanglement: social behaviors show highest vulnerability (reaching degradation as high as 76%), jailbreaking often compromises normative judgment, and hallucination steering unpredictably shifts political views. Our findings underscore the critical need for holistic safety evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2509.13450v2),  [pdf](http://arxiv.org/pdf/2509.13450v2)

**Tags**: cs.AI cs.CL cs.LG 



### Machine Unlearning Meets Adversarial Robustness via Constrained   Interventions on LLMs
**Authors**: Fatmazohra Rezkellah, Ramzi Dakhmouche

**Updated**: 2025-10-16T16:42:58Z

**Summary**: With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.

**Link**: [arxiv](http://arxiv.org/abs/2510.03567v3),  [pdf](http://arxiv.org/pdf/2510.03567v3)

**Tags**: cs.LG cs.CL cs.CR cs.CY math.OC 



### AvatarSync: Rethinking Talking-Head Animation through Phoneme-Guided   Autoregressive Perspective
**Authors**: Yuchen Deng, Xiuyang Wu, Hai-Tao Zheng, Suiyang Zhang, Yi He, Yuxing Han

**Updated**: 2025-10-16T16:37:59Z

**Summary**: Talking-head animation focuses on generating realistic facial videos from audio input. Following Generative Adversarial Networks (GANs), diffusion models have become the mainstream, owing to their robust generative capacities. However, inherent limitations of the diffusion process often lead to inter-frame flicker and slow inference, restricting their practical deployment. To address this, we introduce AvatarSync, an autoregressive framework on phoneme representations that generates realistic and controllable talking-head animations from a single reference image, driven directly by text or audio input. To mitigate flicker and ensure continuity, AvatarSync leverages an autoregressive pipeline that enhances temporal modeling. In addition, to ensure controllability, we introduce phonemes, which are the basic units of speech sounds, and construct a many-to-one mapping from text/audio to phonemes, enabling precise phoneme-to-visual alignment. Additionally, to further accelerate inference, we adopt a two-stage generation strategy that decouples semantic modeling from visual dynamics, and incorporate a customized Phoneme-Frame Causal Attention Mask to support multi-step parallel acceleration. Extensive experiments conducted on both Chinese (CMLR) and English (HDTF) datasets demonstrate that AvatarSync outperforms existing talking-head animation methods in visual fidelity, temporal consistency, and computational efficiency, providing a scalable and controllable solution.

**Link**: [arxiv](http://arxiv.org/abs/2509.12052v2),  [pdf](http://arxiv.org/pdf/2509.12052v2)

**Tags**: cs.CV 



### Rewiring Experts on the Fly:Continuous Rerouting for Better Online   Adaptation in Mixture-of-Expert models
**Authors**: Guinan Su, Yanwu Yang, Li Shen, Lu Yin, Shiwei Liu, Jonas Geiping

**Updated**: 2025-10-16T16:24:36Z

**Summary**: Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite.

**Link**: [arxiv](http://arxiv.org/abs/2510.14853v1),  [pdf](http://arxiv.org/pdf/2510.14853v1)

**Tags**: cs.CL 



### Thinker: Learning to Think Fast and Slow
**Authors**: Stephen Chung, Wenyu Du, Jie Fu

**Updated**: 2025-10-16T16:20:17Z

**Summary**: Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 25.6% to 27.3% for Qwen2.5-1.5B, and from 45.9% to 51.0% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 25.2% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training. Additionally, we have open-sourced both the trained models and the source code.

**Link**: [arxiv](http://arxiv.org/abs/2505.21097v2),  [pdf](http://arxiv.org/pdf/2505.21097v2)

**Tags**: cs.CL cs.AI cs.LG I.2.6; I.2.8; I.5.1 



### Where to Search: Measure the Prior-Structured Search Space of LLM Agents
**Authors**: Zhuo-Yang Song

**Updated**: 2025-10-16T16:18:37Z

**Summary**: The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14846v1),  [pdf](http://arxiv.org/pdf/2510.14846v1)

**Tags**: cs.AI cs.CL cs.LO 



### Boosting Instruction Following at Scale
**Authors**: Ben Elder, Evelyn Duesterwald, Vinod Muthusamy

**Updated**: 2025-10-16T16:15:58Z

**Summary**: A typical approach developers follow to influence an LLM's behavior in an application is through careful manipulation of the prompt, such as by adding or modifying instructions. However, merely adding more instructions provides little assurance that they will actually be followed. We introduce Instruction Boosting as a post-generation method to increase the reliability of LLM prompt instructions. We show that Instruction Boosting improves the instruction following rate by up to 7 points for two instructions and up to 4 points for ten instructions. To demonstrate these results we introduce SCALEDIF, a benchmark with a scaled instruction volume of up to ten instructions per data sample. We also present an analysis of the commonly observed trend that performance degrades as more instructions are added. We show that an important factor contributing to this trend is the degree of tension and conflict that arises as the number of instructions is increased. We contribute a quantitative conflict scoring tool that explains the observed performance trends and provides feedback to developers on the impact that additional prompt instructions have on a model's performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.14842v1),  [pdf](http://arxiv.org/pdf/2510.14842v1)

**Tags**: cs.AI 



### Intelligent Dynamic Handover via AI-assisted Signal Quality Prediction   in 6G Multi-RAT Networks
**Authors**: Maria Lamprini A. Bartsioka, Anastasios Giannopoulos, Sotirios Spantideas

**Updated**: 2025-10-16T16:08:14Z

**Summary**: The emerging paradigm of 6G multiple Radio Access Technology (multi-RAT) networks, where cellular and Wireless Fidelity (WiFi) transmitters coexist, requires mobility decisions that remain reliable under fast channel dynamics, interference, and heterogeneous coverage. Handover in multi-RAT deployments is still highly reactive and event-triggered, relying on instantaneous measurements and threshold events. This work proposes a Machine Learning (ML)-assisted Predictive Conditional Handover (P-CHO) framework based on a model-driven and short-horizon signal quality forecasts. We present a generalized P-CHO sequence workflow orchestrated by a RAT Steering Controller, which standardizes data collection, parallel per-RAT predictions, decision logic with hysteresis-based conditions, and CHO execution. Considering a realistic multi-RAT environment, we train RAT-aware Long Short Term Memory (LSTM) networks to forecast the signal quality indicators of mobile users along randomized trajectories. The proposed P-CHO models are trained and evaluated under different channel models for cellular and IEEE 802.11 WiFi integrated coverage. We study the impact of hyperparameter tuning of LSTM models under different system settings, and compare direct multi-step versus recursive P-CHO variants. Comparisons against baseline predictors are also carried out. Finally, the proposed P-CHO is tested under soft and hard handover settings, showing that hysteresis-enabled P-CHO scheme is able to reduce handover failures and ping-pong events. Overall, the proposed P-CHO framework can enable accurate, low-latency, and proactive handovers suitable for ML-assisted handover steering in 6G multi-RAT deployments.

**Link**: [arxiv](http://arxiv.org/abs/2510.14832v1),  [pdf](http://arxiv.org/pdf/2510.14832v1)

**Tags**: cs.LG cs.NI 



### Programmatic Representation Learning with Language Models
**Authors**: Gabriel Poesia, Georgia Gabriela Sampaio

**Updated**: 2025-10-16T16:02:42Z

**Summary**: Classical models for supervised machine learning, such as decision trees, are efficient and interpretable predictors, but their quality is highly dependent on the particular choice of input features. Although neural networks can learn useful representations directly from raw data (e.g., images or text), this comes at the expense of interpretability and the need for specialized hardware to run them efficiently. In this paper, we explore a hypothesis class we call Learned Programmatic Representations (LeaPR) models, which stack arbitrary features represented as code (functions from data points to scalars) and decision tree predictors. We synthesize feature functions using Large Language Models (LLMs), which have rich prior knowledge in a wide range of domains and a remarkable ability to write code using existing domain-specific libraries. We propose two algorithms to learn LeaPR models from supervised data. First, we design an adaptation of FunSearch to learn features rather than directly generate predictors. Then, we develop a novel variant of the classical ID3 algorithm for decision tree learning, where new features are generated on demand when splitting leaf nodes. In experiments from chess position evaluation to image and text classification, our methods learn high-quality, neural network-free predictors often competitive with neural networks. Our work suggests a flexible paradigm for learning interpretable representations end-to-end where features and predictions can be readily inspected and understood.

**Link**: [arxiv](http://arxiv.org/abs/2510.14825v1),  [pdf](http://arxiv.org/pdf/2510.14825v1)

**Tags**: cs.LG 



### Supervised Fine-Tuning or Contrastive Learning? Towards Better   Multimodal LLM Reranking
**Authors**: Ziqi Dai, Xin Zhang, Mingxin Li, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Wenjie Li, Min Zhang

**Updated**: 2025-10-16T16:02:27Z

**Summary**: In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.

**Link**: [arxiv](http://arxiv.org/abs/2510.14824v1),  [pdf](http://arxiv.org/pdf/2510.14824v1)

**Tags**: cs.CL cs.CV cs.IR 



### Agentic NL2SQL to Reduce Computational Costs
**Authors**: Dominik Jehle, Lennart Purucker, Frank Hutter

**Updated**: 2025-10-16T15:42:28Z

**Summary**: Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87\% and thus allows for substantial cost reductions while maintaining competitive performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.14808v1),  [pdf](http://arxiv.org/pdf/2510.14808v1)

**Tags**: cs.AI cs.LG 



### SimKO: Simple Pass@K Policy Optimization
**Authors**: Ruotian Peng, Yi Ren, Zhouliang Yu, Weiyang Liu, Yandong Wen

**Updated**: 2025-10-16T15:40:49Z

**Summary**: Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.

**Link**: [arxiv](http://arxiv.org/abs/2510.14807v1),  [pdf](http://arxiv.org/pdf/2510.14807v1)

**Tags**: cs.AI 



### Deep Anomaly Detection for Active Attacks on the Receiver in Quantum Key   Distribution
**Authors**: Junxuan Liu, Bingcheng Huang, Jialei Su, Qingquan Peng, Anqi Huang

**Updated**: 2025-10-16T15:34:29Z

**Summary**: Traditional countermeasures against attacks targeting the receiver in quantum key distribution (QKD) systems often suffer from poor compatibility with deployed infrastructure, the risk of introducing new vulnerabilities, and limited applicability to specific types of active attacks. In this work, we propose an anomaly detection (AD) model based on one-class machine learning to address active attacks targeting the receiver. By constructing a dataset from the QKD system's operational states, the AD model learns the characteristics of normal behavior under secure conditions. When an active attack occurs, the system's state deviates from the learned normal patterns and is identified as anomalous by the model. Experimental results show that the AD model achieves an area under the curve (AUC) exceeding 99%, effectively safeguarding the receiver of the QKD system. Compared to traditional approaches, our model can be deployed with minimal cost in existing QKD networks without requiring additional optical or electrical components, thus avoiding the introduction of new side channels. Furthermore, unlike multi-class machine learning algorithms, our approach does not rely on prior knowledge of specific attack types and is potentially able to detect unknown active attacks. These advantages-generality, ease of deployment, low cost, and high accuracy-make our model a practical and effective tool for protecting the receiver of QKD systems against active attacks.

**Link**: [arxiv](http://arxiv.org/abs/2508.12749v2),  [pdf](http://arxiv.org/pdf/2508.12749v2)

**Tags**: quant-ph 



### LLM-guided Chemical Process Optimization with a Multi-Agent Approach
**Authors**: Tong Zeng, Srivathsan Badrinarayanan, Janghoon Ock, Cheng-Kai Lai, Amir Barati Farimani

**Updated**: 2025-10-16T15:31:07Z

**Summary**: Chemical process optimization maximizes production efficiency and economic performance, but optimization algorithms, including gradient-based solvers, numerical methods, and parameter grid searches, become impractical when operating constraints are ill-defined or unavailable. We present a multi-agent LLM framework that autonomously infers operating constraints from minimal process descriptions, then collaboratively guides optimization. Our AutoGen-based framework employs OpenAI's o3 model with specialized agents for constraint generation, parameter validation, simulation, and optimization guidance. Through autonomous constraint generation and iterative multi-agent optimization, the framework eliminates the need for predefined operational bounds. Validated on hydrodealkylation across cost, yield, and yield-to-cost ratio metrics, the framework achieved competitive performance with conventional methods while reducing wall-time 31-fold relative to grid search, converging in under 20 minutes. The reasoning-guided search demonstrates sophisticated process understanding, correctly identifying utility trade-offs and applying domain-informed heuristics. Unlike conventional methods requiring predefined constraints, our approach uniquely combines autonomous constraint generation with interpretable parameter exploration. Model comparison reveals reasoning-capable architectures (o3, o1) are essential for successful optimization, while standard models fail to converge. This approach is particularly valuable for emerging processes and retrofit applications where operational constraints are poorly characterized or unavailable.

**Link**: [arxiv](http://arxiv.org/abs/2506.20921v2),  [pdf](http://arxiv.org/pdf/2506.20921v2)

**Tags**: cs.LG cs.AI cs.CE 



### Cross-Scenario Unified Modeling of User Interests at Billion Scale
**Authors**: Manjie Xu, Cheng Chen, Xin Jia, Jingyi Zhou, Yongji Wu, Zejian Wang, Chi Zhang, Kai Zuo, Yibo Chen, Xu Tang, Yao Hu, Yixin Zhu

**Updated**: 2025-10-16T15:20:49Z

**Summary**: User interests on content platforms are inherently diverse, manifesting through complex behavioral patterns across heterogeneous scenarios such as search, feed browsing, and content discovery. Traditional recommendation systems typically prioritize business metric optimization within isolated specific scenarios, neglecting cross-scenario behavioral signals and struggling to integrate advanced techniques like LLMs at billion-scale deployments, which finally limits their ability to capture holistic user interests across platform touchpoints. We propose RED-Rec, an LLM-enhanced hierarchical Recommender Engine for Diversified scenarios, tailored for industry-level content recommendation systems. RED-Rec unifies user interest representations across multiple behavioral contexts by aggregating and synthesizing actions from varied scenarios, resulting in comprehensive item and user modeling. At its core, a two-tower LLM-powered framework enables nuanced, multifaceted representations with deployment efficiency, and a scenario-aware dense mixing and querying policy effectively fuses diverse behavioral signals to capture cross-scenario user intent patterns and express fine-grained, context-specific intents during serving. We validate RED-Rec through online A/B testing on hundreds of millions of users in RedNote through online A/B testing, showing substantial performance gains in both content recommendation and advertisement targeting tasks. We further introduce a million-scale sequential recommendation dataset, RED-MMU, for comprehensive offline training and evaluation. Our work advances unified user modeling, unlocking deeper personalization and fostering more meaningful user engagement in large-scale UGC platforms.

**Link**: [arxiv](http://arxiv.org/abs/2510.14788v1),  [pdf](http://arxiv.org/pdf/2510.14788v1)

**Tags**: cs.IR cs.AI 



### SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with   Model-Based Reinforcement Learning
**Authors**: Aderik Verraest, Stavrow Bahnam, Robin Ferede, Guido de Croon, Christophe De Wagter

**Updated**: 2025-10-16T15:16:26Z

**Summary**: Autonomous drone racing (ADR) systems have recently achieved champion-level performance, yet remain highly specific to drone racing. While end-to-end vision-based methods promise broader applicability, no system to date simultaneously achieves full sim-to-real transfer, onboard execution, and champion-level performance. In this work, we present SkyDreamer, to the best of our knowledge, the first end-to-end vision-based ADR policy that maps directly from pixel-level representations to motor commands. SkyDreamer builds on informed Dreamer, a model-based reinforcement learning approach where the world model decodes to privileged information only available during training. By extending this concept to end-to-end vision-based ADR, the world model effectively functions as an implicit state and parameter estimator, greatly improving interpretability. SkyDreamer runs fully onboard without external aid, resolves visual ambiguities by tracking progress using the state decoded from the world model's hidden state, and requires no extrinsic camera calibration, enabling rapid deployment across different drones without retraining. Real-world experiments show that SkyDreamer achieves robust, high-speed flight, executing tight maneuvers such as an inverted loop, a split-S and a ladder, reaching speeds of up to 21 m/s and accelerations of up to 6 g. It further demonstrates a non-trivial visual sim-to-real transfer by operating on poor-quality segmentation masks, and exhibits robustness to battery depletion by accurately estimating the maximum attainable motor RPM and adjusting its flight path in real-time. These results highlight SkyDreamer's adaptability to important aspects of the reality gap, bringing robustness while still achieving extremely high-speed, agile flight.

**Link**: [arxiv](http://arxiv.org/abs/2510.14783v1),  [pdf](http://arxiv.org/pdf/2510.14783v1)

**Tags**: cs.RO 



### Sensing-Secure ISAC: Ambiguity Function Engineering for Impairing   Unauthorized Sensing
**Authors**: Kawon Han, Kaitao Meng, Christos Masouros

**Updated**: 2025-10-16T15:15:17Z

**Summary**: The deployment of integrated sensing and communication (ISAC) brings along unprecedented vulnerabilities to authorized sensing, necessitating the development of secure solutions. Sensing parameters are embedded within the target-reflected signal leaked to unauthorized passive radar sensing eavesdroppers (Eve), implying that they can silently extract sensory information without prior knowledge of the information data. To overcome this limitation, we propose a sensing-secure ISAC framework that ensures secure target detection and estimation for the legitimate system, while obfuscating unauthorized sensing without requiring any prior knowledge of Eve. By introducing artificial imperfections into the ambiguity function (AF) of ISAC signals, we introduce artificial targets into Eve's range profile which increase its range estimation ambiguity. In contrast, the legitimate sensing receiver (Alice) can suppress these AF artifacts using mismatched filtering, albeit at the expense of signal-to-noise ratio (SNR) loss. Employing an OFDM signal, a structured subcarrier power allocation scheme is designed to shape the secure autocorrelation function (ACF), inserting periodic peaks to mislead Eve's range estimation and degrade target detection performance. To quantify the sensing security, we introduce peak sidelobe level (PSL) and integrated sidelobe level (ISL) as key performance metrics. Then, we analyze the three-way trade-offs between communication, legitimate sensing, and sensing security, highlighting the impact of the proposed sensing-secure ISAC signaling on system performance. We formulate a convex optimization problem to maximize ISAC performance while guaranteeing a certain sensing security level. Numerical results validate the effectiveness of the proposed sensing-secure ISAC signaling, demonstrating its ability to degrade Eve's target estimation while preserving Alice's performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.02103v2),  [pdf](http://arxiv.org/pdf/2510.02103v2)

**Tags**: eess.SP 



### Finding Answers in Thought Matters: Revisiting Evaluation on Large   Language Models with Reasoning
**Authors**: Hwiyeol Jo, Joosung Lee, Jaehone Lee, Sang-Woo Lee, Joonsuk Park, Kang Min Yoo

**Updated**: 2025-10-16T15:09:22Z

**Summary**: Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt "Answer:". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2510.14773v1),  [pdf](http://arxiv.org/pdf/2510.14773v1)

**Tags**: cs.CL cs.AI 



### Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method   Refactoring
**Authors**: Abhiram Bellur, Fraol Batole, Mohammed Raihan Ullah, Malinda Dilhara, Yaroslav Zharov, Timofey Bryksin, Kai Ishikawa, Haifeng Chen, Masaharu Morimoto, Shota Motoura, Takeo Hosomi, Tien N. Nguyen, Hridesh Rajan, Nikolaos Tsantalis, Danny Dig

**Updated**: 2025-10-16T15:08:16Z

**Summary**: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools that recommend which methods to move and where, these recommendations do not align with how expert developers perform MOVEMETHOD. Given the extensive training of Large Language Models and their reliance upon naturalness of code, they should expertly recommend which methods are misplaced in a given class and which classes are better hosts. Our formative study of 2016 LLM recommendations revealed that LLMs give expert suggestions, yet they are unreliable: up to 80% of the suggestions are hallucinations. We introduce the first LLM fully powered assistant for MOVEMETHOD refactoring that automates its whole end-to-end lifecycle, from recommendation to execution. We designed novel solutions that automatically filter LLM hallucinations using static analysis from IDEs and a novel workflow that requires LLMs to be self-consistent, critique, and rank refactoring suggestions. As MOVEMETHOD refactoring requires global, projectlevel reasoning, we solved the limited context size of LLMs by employing refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist, synergistically combines the strengths of the LLM, IDE, static analysis, and semantic relevance. In our thorough, multi-methodology empirical evaluation, we compare MM-assist with the previous state-of-the-art approaches. MM-assist significantly outperforms them: (i) on a benchmark widely used by other researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a corpus of 210 recent refactorings from Open-source software, our Recall rates improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced participants who used MM-assist to refactor their own code for one week. They rated 82.8% of MM-assist recommendations positively. This shows that MM-assist is both effective and useful.

**Link**: [arxiv](http://arxiv.org/abs/2503.20934v2),  [pdf](http://arxiv.org/pdf/2503.20934v2)

**Tags**: cs.SE cs.AI 



### Auction-based Adaptive Resource Allocation Optimization in Dense and   Heterogeneous IoT Networks
**Authors**: Nirmal D. Wickramasinghe, John Dooley, Dirk Pesch, Indrakshi Dey

**Updated**: 2025-10-16T15:05:07Z

**Summary**: Efficient and reliable resource allocation within densely-deployed massive IoT networks remains a key challenge due to resource constraints among low-size, weight, and power (SWaP) IoT devices and within the network and limitations of conventional centralized methods under incomplete information. We propose a novel auction-based framework for adaptive resource allocation, combining space-time-frequency spreading (STFS) techniques with Bayesian Game approaches. We introduce novel modified Simultaneous Ascending Auction (mSAA) mechanism tailored to densely-deployed and low-complexity IoT networks, enabling distributed computation and reduced power consumption. By incorporating Bayesian game-based bidding strategies and optimizing dispersion matrices for signal transmission, the proposed approach ensures enhanced channel throughput and energy efficiency. Comparative analysis against traditional auction types, including First-Price and Second-Price Sealed-Bid Auctions, as well as the Vickery-Clarke-Groves (VCG) mechanism, demonstrates the superiority of mSAA in terms of surplus maximization, revenue efficiency, and robustness in risk-prone bidding environments. Simulation results validate the model's adaptability to heterogeneous IoT nodes and its potential for dense deployment across different environments and verticals.

**Link**: [arxiv](http://arxiv.org/abs/2409.17843v2),  [pdf](http://arxiv.org/pdf/2409.17843v2)

**Tags**: cs.GT 



### Benchmarking Adversarial Robustness to Bias Elicitation in Large   Language Models: Scalable Automated Assessment with LLM-as-a-Judge
**Authors**: Riccardo Cantini, Alessio Orsino, Massimo Ruggiero, Domenico Talia

**Updated**: 2025-10-16T14:59:50Z

**Summary**: The growing integration of Large Language Models (LLMs) into critical societal domains has raised concerns about embedded biases that can perpetuate stereotypes and undermine fairness. Such biases may stem from historical inequalities in training data, linguistic imbalances, or adversarial manipulation. Despite mitigation efforts, recent studies show that LLMs remain vulnerable to adversarial attacks that elicit biased outputs. This work proposes a scalable benchmarking framework to assess LLM robustness to adversarial bias elicitation. Our methodology involves: (i) systematically probing models across multiple tasks targeting diverse sociocultural biases, (ii) quantifying robustness through safety scores using an LLM-as-a-Judge approach, and (iii) employing jailbreak techniques to reveal safety vulnerabilities. To facilitate systematic benchmarking, we release a curated dataset of bias-related prompts, named CLEAR-Bias. Our analysis, identifying DeepSeek V3 as the most reliable judge LLM, reveals that bias resilience is uneven, with age, disability, and intersectional biases among the most prominent. Some small models outperform larger ones in safety, suggesting that training and architecture may matter more than scale. However, no model is fully robust to adversarial elicitation, with jailbreak attacks using low-resource languages or refusal suppression proving effective across model families. We also find that successive LLM generations exhibit slight safety gains, while models fine-tuned for the medical domain tend to be less safe than their general-purpose counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2504.07887v2),  [pdf](http://arxiv.org/pdf/2504.07887v2)

**Tags**: cs.CL cs.AI 



### Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware   Code
**Authors**: Manar Abdelatty, Maryam Nouh, Jacob K. Rosenstein, Sherief Reda

**Updated**: 2025-10-16T14:57:01Z

**Summary**: Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\%, delay efficiency of 65.9\%, and power efficiency of 64.0\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.

**Link**: [arxiv](http://arxiv.org/abs/2510.14756v1),  [pdf](http://arxiv.org/pdf/2510.14756v1)

**Tags**: cs.CL 



### Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries
**Authors**: Divyat Mahajan, Sachin Goyal, Badr Youbi Idrissi, Mohammad Pezeshki, Ioannis Mitliagkas, David Lopez-Paz, Kartik Ahuja

**Updated**: 2025-10-16T14:52:52Z

**Summary**: Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. We propose future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. We explore two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2510.14751v1),  [pdf](http://arxiv.org/pdf/2510.14751v1)

**Tags**: cs.LG cs.AI 



### Beyond Linear Probes: Dynamic Safety Monitoring for Language Models
**Authors**: James Oldfield, Philip Torr, Ioannis Patras, Adel Bibi, Fazl Barez

**Updated**: 2025-10-16T14:51:42Z

**Summary**: Monitoring large language models' (LLMs) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. TPCs provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can "buy" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with up to 30B parameters, we show that TPCs compete with or outperform MLP-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at http://github.com/james-oldfield/tpc.

**Link**: [arxiv](http://arxiv.org/abs/2509.26238v2),  [pdf](http://arxiv.org/pdf/2509.26238v2)

**Tags**: cs.LG 



### Knowledge Prompting: How Knowledge Engineers Use Large Language Models
**Authors**: Elisavet Koutsiana, Johanna Walker, Michelle Nwachukwu, Albert Meroño-Peñuela, Elena Simperl

**Updated**: 2025-10-16T14:34:41Z

**Summary**: Despite many advances in knowledge engineering (KE), challenges remain in areas such as engineering knowledge graphs (KGs) at scale, keeping up with evolving domain knowledge, multilingualism, and multimodality. Recently, KE has used LLMs to support semi-automatic tasks, but the most effective use of LLMs to support knowledge engineers across the KE activites is still in its infancy. To explore the vision of LLM copilots for KE and change existing KE practices, we conducted a multimethod study during a KE hackathon. We investigated participants' views on the use of LLMs, the challenges they face, the skills they may need to integrate LLMs into their practices, and how they use LLMs responsibly. We found participants felt LLMs could contribute to improving efficiency when engineering KGs, but presented increased challenges around the already complex issues of evaluating the KE tasks. We discovered prompting to be a useful but undervalued skill for knowledge engineers working with LLMs, and note that natural language processing skills may become more relevant across more roles in KG construction. Integrating LLMs into KE tasks needs to be mindful of potential risks and harms related to responsible AI. Given the limited ethical training, most knowledge engineers receive solutions such as our suggested `KG cards' based on data cards could be a useful guide for KG construction. Our findings can support designers of KE AI copilots, KE researchers, and practitioners using advanced AI to develop trustworthy applications, propose new methodologies for KE and operate new technologies responsibly.

**Link**: [arxiv](http://arxiv.org/abs/2408.08878v2),  [pdf](http://arxiv.org/pdf/2408.08878v2)

**Tags**: cs.HC 



### MoLink: Distributed and Efficient Serving Framework for Large Models
**Authors**: Lewei Jin, Yongqi Chen, Kui Zhang, Yifan Zhuo, Yi Gao, Bowei Yang, Zhengong Cai, Wei Dong

**Updated**: 2025-10-16T14:21:45Z

**Summary**: Large language models represent a groundbreaking shift in generative AI. Yet, these advances come with a significant challenge: the high cost of model serving. To mitigate these costs, consumer-grade GPUs emerge as a more affordable alternative. This presents an opportunity for more cost-efficient LLM serving by leveraging these GPUs.   However, it is non-trivial to achieve high-efficiency LLM serving on consumer-grade GPUs, mainly due to two challenges: 1) these GPUs are often deployed in limited network conditions; 2) these GPUs often exhibit heterogeneity in host systems. To address these challenges, we present MoLink, a distributed LLM serving system for large models. It incorporates several key techniques, enabling efficient LLM serving on heterogeneous and weakly connected consumer-grade GPUs. Our experiments demonstrate that it achieves throughput improvements of up to 458\% and cost-profit margin improvements of up to 151\%, compared to state-of-the-art systems. MoLink allows users on Windows, Linux, and containerized VMs to seamlessly integrate GPUs with just a few lines of code over Ethernet or public networks. Currently, it supports 18 mainstream architectures of open-source large language models. The source code is publicly available https://github.com/oldcpple/MoLink.

**Link**: [arxiv](http://arxiv.org/abs/2507.05043v2),  [pdf](http://arxiv.org/pdf/2507.05043v2)

**Tags**: cs.DC 



### Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous   References
**Authors**: Hongzheng Chen, Bin Fan, Alexander Collins, Bastian Hagedorn, Evghenii Gaburov, Masahiro Masuda, Matthew Brookhart, Chris Sullivan, Jason Knight, Zhiru Zhang, Vinod Grover

**Updated**: 2025-10-16T14:20:00Z

**Summary**: Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT programming model is fundamentally misaligned with this task-parallel hardware, creating a significant programmability gap. While hardware-level warp specialization is the key to unlocking peak performance, it forces developers to manually orchestrate complex, low-level communication and software pipelines--a process that is labor-intensive, error-prone, and unsustainable. To address this challenge, we present Tawa, an automated compiler that systematically generates high-performance, warp-specialized code from a high-level, tile-based program. Central to our approach is a novel IR abstraction, asynchronous references (aref), which expresses warp-level communication without exposing low-level hardware details. Using this abstraction, Tawa automatically partitions programs into producer-consumer roles and manages the intricate dataflow pipeline, relieving developers of invasive kernel rewriting. Evaluation on NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers high hardware utilization, achieving up to 1.1$\times$ speedup over highly optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains 1.2$\times$ speedup over Triton and matches the performance of the hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming effort.

**Link**: [arxiv](http://arxiv.org/abs/2510.14719v1),  [pdf](http://arxiv.org/pdf/2510.14719v1)

**Tags**: cs.LG cs.AR cs.PL 



### Speculative Model Risk in Healthcare AI: Using Storytelling to Surface   Unintended Harms
**Authors**: Xingmeng Zhao, Dan Schumacher, Veronica Rammouz, Anthony Rios

**Updated**: 2025-10-16T14:18:31Z

**Summary**: Artificial intelligence (AI) is rapidly transforming healthcare, enabling fast development of tools like stress monitors, wellness trackers, and mental health chatbots. However, rapid and low-barrier development can introduce risks of bias, privacy violations, and unequal access, especially when systems ignore real-world contexts and diverse user needs. Many recent methods use AI to detect risks automatically, but this can reduce human engagement in understanding how harms arise and who they affect. We present a human-centered framework that generates user stories and supports multi-agent discussions to help people think creatively about potential benefits and harms before deployment. In a user study, participants who read stories recognized a broader range of harms, distributing their responses more evenly across all 13 harm types. In contrast, those who did not read stories focused primarily on privacy and well-being (58.3%). Our findings show that storytelling helped participants speculate about a broader range of harms and benefits and think more creatively about AI's impact on users.

**Link**: [arxiv](http://arxiv.org/abs/2510.14718v1),  [pdf](http://arxiv.org/pdf/2510.14718v1)

**Tags**: cs.CL 



### Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via   Internal Representations
**Authors**: Peng Lai, Jianjie Zheng, Sijie Cheng, Yun Chen, Peng Li, Yang Liu, Guanhua Chen

**Updated**: 2025-10-16T14:16:18Z

**Summary**: The growing scale of evaluation tasks has led to the widespread adoption of automated evaluation using LLMs, a paradigm known as "LLM-as-a-judge". However, improving its alignment with human preferences without complex prompts or fine-tuning remains challenging. Previous studies mainly optimize based on shallow outputs, overlooking rich cross-layer representations. In this work, motivated by preliminary findings that middle-to-upper layers encode semantically and task-relevant representations that are often more aligned with human judgments than the final layer, we propose LAGER, a post-hoc, plug-and-play framework for improving the alignment of LLM-as-a-Judge point-wise evaluations with human scores by leveraging internal representations. LAGER produces fine-grained judgment scores by aggregating cross-layer score-token logits and computing the expected score from a softmax-based distribution, while keeping the LLM backbone frozen and ensuring no impact on the inference process. LAGER fully leverages the complementary information across different layers, overcoming the limitations of relying solely on the final layer. We evaluate our method on the standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find that LAGER achieves improvements of up to 7.5% over the best baseline across these benchmarks. Without reasoning steps, LAGER matches or outperforms reasoning-based methods. Experiments on downstream applications, such as data selection and emotional understanding, further show the generalization of LAGER.

**Link**: [arxiv](http://arxiv.org/abs/2508.03550v3),  [pdf](http://arxiv.org/pdf/2508.03550v3)

**Tags**: cs.CL 



### Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in   Mathematics and Quantum Physics
**Authors**: Marco Del Tredici, Jacob McCarran, Benjamin Breen, Javier Aspuru Mijares, Weichen Winston Yin, Jacob M. Taylor, Frank H. L. Koppens, Dirk Englund

**Updated**: 2025-10-16T14:10:07Z

**Summary**: We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperforms them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.

**Link**: [arxiv](http://arxiv.org/abs/2510.12787v2),  [pdf](http://arxiv.org/pdf/2510.12787v2)

**Tags**: cs.AI cs.MA 



### ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for   Function Calling
**Authors**: Jianghao Lin, Yuanyuan Shi, Xin Peng, Renjie Ding, Hairui Wang, Yuxuan Peng, Bizhe Bai, Weixi Song, Fengshuo Bai, Huacan Chai, Weinan Zhang, Fei Huang, Ying Wen

**Updated**: 2025-10-16T14:06:03Z

**Summary**: Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: "explore more but retain less" due to the unrecoverability characteristics of structured function calling generation.

**Link**: [arxiv](http://arxiv.org/abs/2510.14703v1),  [pdf](http://arxiv.org/pdf/2510.14703v1)

**Tags**: cs.AI 



### The Last Dependency Crusade: Solving Python Dependency Conflicts with   LLMs
**Authors**: Antony Bartlett, Cynthia Liem, Annibale Panichella

**Updated**: 2025-10-16T14:05:53Z

**Summary**: Resolving Python dependency issues remains a tedious and error-prone process, forcing developers to manually trial compatible module versions and interpreter configurations. Existing automated solutions, such as knowledge-graph-based and database-driven methods, face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies. This paper investigates the use of Large Language Models (LLMs) to automatically repair dependency issues in Python programs. We propose PLLM (pronounced "plum"), a novel retrieval-augmented generation (RAG) approach that iteratively infers missing or incorrect dependencies. PLLM builds a test environment where the LLM proposes module combinations, observes execution feedback, and refines its predictions using natural language processing (NLP) to parse error messages. We evaluate PLLM on the Gistable HG2.9K dataset, a curated collection of real-world Python programs. Using this benchmark, we explore multiple PLLM configurations, including six open-source LLMs evaluated both with and without RAG. Our findings show that RAG consistently improves fix rates, with the best performance achieved by Gemma-2 9B when combined with RAG. Compared to two state-of-the-art baselines, PyEGo and ReadPyE, PLLM achieves significantly higher fix rates; +15.97\% more than ReadPyE and +21.58\% more than PyEGo. Further analysis shows that PLLM is especially effective for projects with numerous dependencies and those using specialized numerical or machine-learning libraries.

**Link**: [arxiv](http://arxiv.org/abs/2501.16191v2),  [pdf](http://arxiv.org/pdf/2501.16191v2)

**Tags**: cs.SE cs.AI 



### Cognitive-Aligned Spatio-Temporal Large Language Models For Next   Point-of-Interest Prediction
**Authors**: Penglong Zhai, Jie Li, Fanyi Di, Yue Liu, Yifang Yuan, Jie Huang, Peng Wu, Sicong Wang, Mingyang Yin, Tingting Hu, Yao Xu, Xin Li

**Updated**: 2025-10-16T14:05:28Z

**Summary**: The next point-of-interest (POI) recommendation task aims to predict the users' immediate next destinations based on their preferences and historical check-ins, holding significant value in location-based services. Recently, large language models (LLMs) have shown great potential in recommender systems, which treat the next POI prediction in a generative manner. However, these LLMs, pretrained primarily on vast corpora of unstructured text, lack the native understanding of structured geographical entities and sequential mobility patterns required for next POI prediction tasks. Moreover, in industrial-scale POI prediction applications, incorporating world knowledge and alignment of human cognition, such as seasons, weather conditions, holidays, and users' profiles (such as habits, occupation, and preferences), can enhance the user experience while improving recommendation performance. To address these issues, we propose CoAST (Cognitive-Aligned Spatial-Temporal LLMs), a framework employing natural language as an interface, allowing for the incorporation of world knowledge, spatio-temporal trajectory patterns, profiles, and situational information. Specifically, CoAST mainly comprises of 2 stages: (1) Recommendation Knowledge Acquisition through continued pretraining on the enriched spatial-temporal trajectory data of the desensitized users; (2) Cognitive Alignment to align cognitive judgments with human preferences using enriched training data through Supervised Fine-Tuning (SFT) and a subsequent Reinforcement Learning (RL) phase. Extensive offline experiments on various real-world datasets and online experiments deployed in "Guess Where You Go" of AMAP App homepage demonstrate the effectiveness of CoAST.

**Link**: [arxiv](http://arxiv.org/abs/2510.14702v1),  [pdf](http://arxiv.org/pdf/2510.14702v1)

**Tags**: cs.AI 



### LLM Agents for Automated Web Vulnerability Reproduction: Are We There   Yet?
**Authors**: Bin Liu, Yanjie Zhao, Guoai Xu, Haoyu Wang

**Updated**: 2025-10-16T14:04:46Z

**Summary**: Large language model (LLM) agents have demonstrated remarkable capabilities in software engineering and cybersecurity tasks, including code generation, vulnerability discovery, and automated testing. One critical but underexplored application is automated web vulnerability reproduction, which transforms vulnerability reports into working exploits. Although recent advances suggest promising potential, challenges remain in applying LLM agents to real-world web vulnerability reproduction scenarios. In this paper, we present the first comprehensive evaluation of state-of-the-art LLM agents for automated web vulnerability reproduction. We systematically assess 20 agents from software engineering, cybersecurity, and general domains across 16 dimensions, including technical capabilities, environment adaptability, and user experience factors, on 3 representative web vulnerabilities. Based on the results, we select three top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies. Our results reveal that while LLM agents achieve reasonable success on simple library-based vulnerabilities, they consistently fail on complex service-based vulnerabilities requiring multi-component environments. Complex environment configurations and authentication barriers create a gap where agents can execute exploit code but fail to trigger actual vulnerabilities. We observe high sensitivity to input guidance, with performance degrading by over 33% under incomplete authentication information. Our findings highlight the significant gap between current LLM agent capabilities and the demands of reliable automated vulnerability reproduction, emphasizing the need for advances in environmental adaptation and autonomous problem-solving capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2510.14700v1),  [pdf](http://arxiv.org/pdf/2510.14700v1)

**Tags**: cs.SE cs.CR 



### Adaptive Budget Allocation for Orthogonal-Subspace Adapter Tuning in   LLMs Continual Learning
**Authors**: Zhiyi Wan, Wanrou Du, Liang Li, Miao Pan, Xiaoqi Qin

**Updated**: 2025-10-16T14:03:22Z

**Summary**: Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in an end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment.To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency. OA-Adapter achieves higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark, and maintains its advantages on two larger benchmarks comprising 15 tasks.

**Link**: [arxiv](http://arxiv.org/abs/2505.22358v2),  [pdf](http://arxiv.org/pdf/2505.22358v2)

**Tags**: cs.LG cs.AI 



### Confidence-Based Response Abstinence: Improving LLM Trustworthiness via   Activation-Based Uncertainty Estimation
**Authors**: Zhiqi Huang, Vivek Datla, Chenyang Zhu, Alfy Samuel, Daben Liu, Anoop Kumar, Ritesh Soni

**Updated**: 2025-10-16T13:58:27Z

**Summary**: We propose a method for confidence estimation in retrieval-augmented generation (RAG) systems that aligns closely with the correctness of large language model (LLM) outputs. Confidence estimation is especially critical in high-stakes domains such as finance and healthcare, where the cost of an incorrect answer outweighs that of not answering the question. Our approach extends prior uncertainty quantification methods by leveraging raw feed-forward network (FFN) activations as auto-regressive signals, avoiding the information loss inherent in token logits and probabilities after projection and softmax normalization. We model confidence prediction as a sequence classification task, and regularize training with a Huber loss term to improve robustness against noisy supervision. Applied in a real-world financial industry customer-support setting with complex knowledge bases, our method outperforms strong baselines and maintains high accuracy under strict latency constraints. Experiments on Llama 3.1 8B model show that using activations from only the 16th layer preserves accuracy while reducing response latency. Our results demonstrate that activation-based confidence modeling offers a scalable, architecture-aware path toward trustworthy RAG deployment.

**Link**: [arxiv](http://arxiv.org/abs/2510.13750v2),  [pdf](http://arxiv.org/pdf/2510.13750v2)

**Tags**: cs.CL 



### xLLM Technical Report
**Authors**: Tongxuan Liu, Tao Peng, Peijun Yang, Xiaoyang Zhao, Xiusheng Lu, Weizhe Huang, Zirui Liu, Xiaoyu Chen, Zhiwei Liang, Jun Xiong, Donghe Jin, Minchao Zhang, Jinrong Guo, Yingxu Deng, Xu Zhang, Xianzhe Dong, Siqi Wang, Siyu Wu, Yu Wu, Zihan Tang, Yuting Zeng, Yanshu Wang, Jinguang Liu, Meng Kang, Menxin Li, Yunlong Wang, Yiming Liu, Xiaolong Ma, Yifan Wang, Yichen Zhang, Jinrun Yin, Keyang Zheng, Jiawei Yin, Jun Zhang, Ziyue Wang, Xiaobo Lin, Liangyu Liu, Liwei Lan, Yang Liu, Chunhua Peng, Han Liu, Songcheng Ren, Xuezhu Wang, Yunheng Shen, Yi Wang, Guyue Liu, Hui Chen, Tong Yang, Hailong Yang, Jing Li, Guiguang Ding, Ke Zhang

**Updated**: 2025-10-16T13:53:47Z

**Summary**: We introduce xLLM, an intelligent and efficient Large Language Model (LLM) inference framework designed for high-performance, large-scale enterprise-grade serving, with deep optimizations for diverse AI accelerators. To address these challenges, xLLM builds a novel decoupled service-engine architecture. At the service layer, xLLM-Service features an intelligent scheduling module that efficiently processes multimodal requests and co-locates online and offline tasks through unified elastic scheduling to maximize cluster utilization. This module also relies on a workload-adaptive dynamic Prefill-Decode (PD) disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation policy designed for multimodal inputs. Furthermore, it incorporates a distributed architecture to provide global KV Cache management and robust fault-tolerant capabilities for high availability. At the engine layer, xLLM-Engine co-optimizes system and algorithm designs to fully saturate computing resources. This is achieved through comprehensive multi-layer execution pipeline optimizations, an adaptive graph mode and an xTensor memory management. xLLM-Engine also further integrates algorithmic enhancements such as optimized speculative decoding and dynamic EPLB, collectively serving to substantially boost throughput and inference efficiency. Extensive evaluations demonstrate that xLLM delivers significantly superior performance and resource efficiency. Under identical TPOT constraints, xLLM achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining an average throughput of 1.7x that of MindIE with Deepseek-series models. xLLM framework is publicly available at https://github.com/jd-opensource/xllm and https://github.com/jd-opensource/xllm-service.

**Link**: [arxiv](http://arxiv.org/abs/2510.14686v1),  [pdf](http://arxiv.org/pdf/2510.14686v1)

**Tags**: cs.DC cs.AI 



### The Hidden Bias: A Study on Explicit and Implicit Political Stereotypes   in Large Language Models
**Authors**: Konrad Löhr, Shuzhou Yuan, Michael Färber

**Updated**: 2025-10-16T13:44:28Z

**Summary**: Large Language Models (LLMs) are increasingly integral to information dissemination and decision-making processes. Given their growing societal influence, understanding potential biases, particularly within the political domain, is crucial to prevent undue influence on public opinion and democratic processes. This work investigates political bias and stereotype propagation across eight prominent LLMs using the two-dimensional Political Compass Test (PCT). Initially, the PCT is employed to assess the inherent political leanings of these models. Subsequently, persona prompting with the PCT is used to explore explicit stereotypes across various social dimensions. In a final step, implicit stereotypes are uncovered by evaluating models with multilingual versions of the PCT. Key findings reveal a consistent left-leaning political alignment across all investigated models. Furthermore, while the nature and extent of stereotypes vary considerably between models, implicit stereotypes elicited through language variation are more pronounced than those identified via explicit persona prompting. Interestingly, for most models, implicit and explicit stereotypes show a notable alignment, suggesting a degree of transparency or "awareness" regarding their inherent biases. This study underscores the complex interplay of political bias and stereotypes in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.08236v2),  [pdf](http://arxiv.org/pdf/2510.08236v2)

**Tags**: cs.LG cs.AI 



### On the Generalization of SFT: A Reinforcement Learning Perspective with   Reward Rectification
**Authors**: Yongliang Wu, Yizhou Zhou, Zhou Ziheng, Yingzhe Peng, Xinyu Ye, Xinting Hu, Wenbo Zhu, Lu Qi, Ming-Hsuan Yang, Xu Yang

**Updated**: 2025-10-16T13:40:55Z

**Summary**: We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT.

**Link**: [arxiv](http://arxiv.org/abs/2508.05629v2),  [pdf](http://arxiv.org/pdf/2508.05629v2)

**Tags**: cs.LG 



### Augmenting Smart Contract Decompiler Output through Fine-grained   Dependency Analysis and LLM-facilitated Semantic Recovery
**Authors**: Zeqin Liao, Yuhong Nan, Zixu Gao, Henglong Liang, Sicheng Hao, Peifan Reng, Zibin Zheng

**Updated**: 2025-10-16T13:40:44Z

**Summary**: Decompiler is a specialized type of reverse engineering tool extensively employed in program analysis tasks, particularly in program comprehension and vulnerability detection. However, current Solidity smart contract decompilers face significant limitations in reconstructing the original source code. In particular, the bottleneck of SOTA decompilers lies in inaccurate method identification, incorrect variable type recovery, and missing contract attributes. These deficiencies hinder downstream tasks and understanding of the program logic. To address these challenges, we propose SmartHalo, a new framework that enhances decompiler output by combining static analysis (SA) and large language models (LLM). SmartHalo leverages the complementary strengths of SA's accuracy in control and data flow analysis and LLM's capability in semantic prediction. More specifically, \system{} constructs a new data structure - Dependency Graph (DG), to extract semantic dependencies via static analysis. Then, it takes DG to create prompts for LLM optimization. Finally, the correctness of LLM outputs is validated through symbolic execution and formal verification. Evaluation on a dataset consisting of 465 randomly selected smart contract methods shows that SmartHalo significantly improves the quality of the decompiled code, compared to SOTA decompilers (e.g., Gigahorse). Notably, integrating GPT-4o with SmartHalo further enhances its performance, achieving precision rates of 87.39% for method boundaries, 90.39% for variable types, and 80.65% for contract attributes.

**Link**: [arxiv](http://arxiv.org/abs/2501.08670v2),  [pdf](http://arxiv.org/pdf/2501.08670v2)

**Tags**: cs.SE 



### The simulation of judgment in LLMs
**Authors**: Edoardo Loru, Jacopo Nudo, Niccolò Di Marco, Alessandro Santirocchi, Roberto Atzeni, Matteo Cinelli, Vincenzo Cestari, Clelia Rossi-Arnaud, Walter Quattrociocchi

**Updated**: 2025-10-16T13:38:08Z

**Summary**: Large Language Models (LLMs) are increasingly embedded in evaluative processes, from information filtering to assessing and addressing knowledge gaps through explanation and credibility judgments. This raises the need to examine how such evaluations are built, what assumptions they rely on, and how their strategies diverge from those of humans. We benchmark six LLMs against expert ratings--NewsGuard and Media Bias/Fact Check--and against human judgments collected through a controlled experiment. We use news domains purely as a controlled benchmark for evaluative tasks, focusing on the underlying mechanisms rather than on news classification per se. To enable direct comparison, we implement a structured agentic framework in which both models and nonexpert participants follow the same evaluation procedure: selecting criteria, retrieving content, and producing justifications. Despite output alignment, our findings show consistent differences in the observable criteria guiding model evaluations, suggesting that lexical associations and statistical priors could influence evaluations in ways that differ from contextual reasoning. This reliance is associated with systematic effects: political asymmetries and a tendency to confuse linguistic form with epistemic reliability--a dynamic we term epistemia, the illusion of knowledge that emerges when surface plausibility replaces verification. Indeed, delegating judgment to such systems may affect the heuristics underlying evaluative processes, suggesting a shift from normative reasoning toward pattern-based approximation and raising open questions about the role of LLMs in evaluative processes.

**Link**: [arxiv](http://arxiv.org/abs/2502.04426v3),  [pdf](http://arxiv.org/pdf/2502.04426v3)

**Tags**: cs.CL cs.AI cs.CY 



### Analysis of Hyperparameter Optimization Effects on Lightweight Deep   Models for Real-Time Image Classification
**Authors**: Vineet Kumar Rakesh, Soumya Mazumdar, Tapas Samanta, Hemendra Kumar Pandey, Amitabha Das

**Updated**: 2025-10-16T13:29:58Z

**Summary**: Lightweight convolutional and transformer-based networks are increasingly preferred for real-time image classification, especially on resource-constrained devices. This study evaluates the impact of hyperparameter optimization on the accuracy and deployment feasibility of seven modern lightweight architectures: ConvNeXt-T, EfficientNetV2-S, MobileNetV3-L, MobileViT v2 (S/XS), RepVGG-A2, and TinyViT-21M, trained on a class-balanced subset of 90,000 images from ImageNet-1K. Under standardized training settings, this paper investigates the influence of learning rate schedules, augmentation, optimizers, and initialization on model performance. Inference benchmarks are performed using an NVIDIA L40s GPU with batch sizes ranging from 1 to 512, capturing latency and throughput in real-time conditions. This work demonstrates that controlled hyperparameter variation significantly alters convergence dynamics in lightweight CNN and transformer backbones, providing insight into stability regions and deployment feasibility in edge artificial intelligence. Our results reveal that tuning alone leads to a top-1 accuracy improvement of 1.5 to 3.5 percent over baselines, and select models (e.g., RepVGG-A2, MobileNetV3-L) deliver latency under 5 milliseconds and over 9,800 frames per second, making them ideal for edge deployment. This work provides reproducible, subset-based insights into lightweight hyperparameter tuning and its role in balancing speed and accuracy. The code and logs may be seen at: https://vineetkumarrakesh.github.io/lcnn-opt

**Link**: [arxiv](http://arxiv.org/abs/2507.23315v2),  [pdf](http://arxiv.org/pdf/2507.23315v2)

**Tags**: cs.CV cs.AI cs.LG 



### VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning
**Authors**: Jinglei Zhang, Yuanfan Guo, Rolandos Alexandros Potamias, Jiankang Deng, Hang Xu, Chao Ma

**Updated**: 2025-10-16T13:29:02Z

**Summary**: In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: https://vtimecot.github.io

**Link**: [arxiv](http://arxiv.org/abs/2510.14672v1),  [pdf](http://arxiv.org/pdf/2510.14672v1)

**Tags**: cs.CV 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2025-10-16T13:25:38Z

**Summary**: Large Language Models have excelled in various domains but face efficiency challenges due to the growing Key-Value (KV) cache required for long-sequence inference. Recent efforts aim to reduce KV cache size by evicting vast non-critical cache elements during runtime while preserving generation quality. However, these methods typically allocate compression budgets uniformly across all attention heads, ignoring the unique attention patterns of each head. In this paper, we establish a theoretical loss upper bound between pre- and post-eviction attention output, explaining the optimization target of prior cache eviction methods, while guiding the optimization of adaptive budget allocation. Base on this, we propose {\it Ada-KV}, the first head-wise adaptive budget allocation strategy. It offers plug-and-play benefits, enabling seamless integration with prior cache eviction methods. Extensive evaluations on 13 datasets from Ruler and 16 datasets from LongBench, all conducted under both question-aware and question-agnostic scenarios, demonstrate substantial quality improvements over existing methods. Our code is available at https://github.com/FFY0/AdaKV.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v5),  [pdf](http://arxiv.org/pdf/2407.11550v5)

**Tags**: cs.CL cs.AI 



### Beyond Hallucinations: The Illusion of Understanding in Large Language   Models
**Authors**: Rikard Rosenbacke, Carl Rosenbacke, Victor Rosenbacke, Martin McKee

**Updated**: 2025-10-16T13:19:44Z

**Summary**: Large language models (LLMs) are becoming deeply embedded in human communication and decision-making, yet they inherit the ambiguity, bias, and lack of direct access to truth inherent in language itself. While their outputs are fluent, emotionally resonant, and coherent, they are generated through statistical prediction rather than grounded reasoning. This creates the risk of hallucination, responses that sound convincing but lack factual validity. Building on Geoffrey Hinton's observation that AI mirrors human intuition rather than reasoning, this paper argues that LLMs operationalize System 1 cognition at scale: fast, associative, and persuasive, but without reflection or falsification. To address this, we introduce the Rose-Frame, a three-dimensional framework for diagnosing cognitive and epistemic drift in human-AI interaction. The three axes are: (i) Map vs. Territory, which distinguishes representations of reality (epistemology) from reality itself (ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to separate fast, emotional judgments from slow, reflective thinking; and (iii) Conflict vs. Confirmation, which examines whether ideas are critically tested through disagreement or simply reinforced through mutual validation. Each dimension captures a distinct failure mode, and their combination amplifies misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules. Instead, it offers a reflective tool that makes both the model's limitations and the user's assumptions visible, enabling more transparent and critically aware AI deployment. It reframes alignment as cognitive governance: intuition, whether human or artificial, must remain governed by human reason. Only by embedding reflective, falsifiable oversight can we align machine fluency with human understanding.

**Link**: [arxiv](http://arxiv.org/abs/2510.14665v1),  [pdf](http://arxiv.org/pdf/2510.14665v1)

**Tags**: cs.AI cs.HC 



### SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality   Evaluation
**Authors**: Hui Wang, Jinghua Zhao, Yifan Yang, Shujie Liu, Junyang Chen, Yanzhe Zhang, Shiwan Zhao, Jinyu Li, Jiaming Zhou, Haoqin Sun, Yan Lu, Yong Qin

**Updated**: 2025-10-16T13:19:07Z

**Summary**: Generative speech technologies are progressing rapidly, but evaluating the perceptual quality of synthetic speech remains a core challenge. Existing methods typically rely on scalar scores or binary decisions, which lack interpretability and generalization across tasks and languages. We present SpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs) to conduct structured and explanation-based speech quality evaluation. To support this direction, we introduce SpeechEval, a large-scale dataset containing 32,207 multilingual speech clips and 128,754 annotations spanning four tasks: quality assessment, pairwise comparison, improvement suggestion, and deepfake detection. Based on this resource, we develop SQ-LLM, a speech-quality-aware LLM trained with chain-of-thought reasoning and reward optimization to improve capability. Experimental results show that SQ-LLM delivers strong performance across tasks and languages, revealing the potential of this paradigm for advancing speech quality evaluation. Relevant resources will be open-sourced.

**Link**: [arxiv](http://arxiv.org/abs/2510.14664v1),  [pdf](http://arxiv.org/pdf/2510.14664v1)

**Tags**: cs.SD eess.AS 



### An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs
**Authors**: Linyue Ma, Yilong Xu, Xiang Long, Zhi Zheng

**Updated**: 2025-10-16T13:15:40Z

**Summary**: Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, "nugget-as-rubric", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.14660v1),  [pdf](http://arxiv.org/pdf/2510.14660v1)

**Tags**: cs.CL cs.AI cs.IR 



### O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis
**Authors**: Ayush Khaitan, Vijay Ganesh

**Updated**: 2025-10-16T13:07:41Z

**Summary**: Large language models have recently demonstrated advanced capabilities in solving IMO and Putnam problems; yet their role in research mathematics has remained fairly limited. The key difficulty is verification: suggested proofs may look plausible, but cannot be trusted without rigorous checking. We present a framework, called LLM+CAS, and an associated tool, O-Forge, that couples frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic Feedback loop to produce proofs that are both creative and symbolically verified. Our focus is on asymptotic inequalities, a topic that often involves difficult proofs and appropriate decomposition of the domain into the "right" subdomains. Many mathematicians, including Terry Tao, have suggested that using AI tools to find the right decompositions can be very useful for research-level asymptotic analysis. In this paper, we show that our framework LLM+CAS turns out to be remarkably effective at proposing such decompositions via a combination of a frontier LLM and a CAS. More precisely, we use an LLM to suggest domain decomposition, and a CAS (such as Mathematica) that provides a verification of each piece axiomatically. Using this loop, we answer a question posed by Terence Tao: whether LLMs coupled with a verifier can be used to help prove intricate asymptotic inequalities. More broadly, we show how AI can move beyond contest math towards research-level tools for professional mathematicians.

**Link**: [arxiv](http://arxiv.org/abs/2510.12350v2),  [pdf](http://arxiv.org/pdf/2510.12350v2)

**Tags**: cs.AI 03B35, 68W30, 68T05 



### Intent Clustering with Shared Pseudo-Labels
**Authors**: I-Fan Lin, Faegheh Hasibi, Suzan Verberne

**Updated**: 2025-10-17T11:18:40Z

**Summary**: In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2510.14640v2),  [pdf](http://arxiv.org/pdf/2510.14640v2)

**Tags**: cs.CL cs.IR 



### Performance of the Prototype Station of the IceCube Surface Array   Enhancement
**Authors**: S. Shefali for the IceCube Collaboration

**Updated**: 2025-10-16T12:50:40Z

**Summary**: The prototype station of the Surface Array Enhancement at the IceCube Neutrino Observatory has been taking data in its final design since 2023. This station is part of the planned extension within the footprint of the existing surface array, IceTop. One station consists of 8 scintillator detectors, 3 radio antennas, and a central DAQ. The final upgrade of the scintillation detectors and their firmware at the prototype station has extended the dynamic range and increased the data-taking up-time, thereby expanding the observation window for air showers. This contribution will discuss the performance of the upgraded prototype station after commissioning and its angular resolution capabilities when observing air showers with the scintillation detectors and in coincidence with IceTop. Furthermore, the integration of additional stations during the most recent deployment will be discussed.

**Link**: [arxiv](http://arxiv.org/abs/2510.14636v1),  [pdf](http://arxiv.org/pdf/2510.14636v1)

**Tags**: astro-ph.HE hep-ex 



### ATGen: Adversarial Reinforcement Learning for Test Case Generation
**Authors**: Qingyao Li, Xinyi Dai, Weiwen Liu, Xiangyang Li, Yasheng Wang, Ruiming Tang, Yong Yu, Weinan Zhang

**Updated**: 2025-10-16T12:49:25Z

**Summary**: Large Language Models (LLMs) excel at code generation, yet their outputs often contain subtle bugs, for which effective test cases are a critical bottleneck. Existing test generation methods, whether based on prompting or supervised fine-tuning, rely on static datasets. This imposes a ``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover novel or more complex bugs beyond their training scope. To overcome this, we introduce ATGen, a framework that trains a test case generator via adversarial reinforcement learning. ATGen pits a test generator against an adversarial code generator that continuously crafts harder bugs to evade the current policy. This dynamic loop creates a curriculum of increasing difficulty challenging current policy. The test generator is optimized via Reinforcement Learning (RL) to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to learn a progressively stronger policy that breaks the fixed-difficulty ceiling of static training. Extensive experiments demonstrate that ATGen significantly outperforms state-of-the-art baselines. We further validate its practical utility, showing it serves as both a more effective filter for Best-of-N inference and a higher-quality reward source for training code generation models. Our work establishes a new, dynamic paradigm for improving the reliability of LLM-generated code.

**Link**: [arxiv](http://arxiv.org/abs/2510.14635v1),  [pdf](http://arxiv.org/pdf/2510.14635v1)

**Tags**: cs.SE 



### MR.Rec: Synergizing Memory and Reasoning for Personalized Recommendation   Assistant with LLMs
**Authors**: Jiani Huang, Xingchen Zou, Lianghao Xia, Qing Li

**Updated**: 2025-10-16T12:40:48Z

**Summary**: The application of Large Language Models (LLMs) in recommender systems faces key challenges in delivering deep personalization and intelligent reasoning, especially for interactive scenarios. Current methods are often constrained by limited context windows and single-turn reasoning, hindering their ability to capture dynamic user preferences and proactively reason over recommendation contexts. To address these limitations, we propose MR.Rec, a novel framework that synergizes memory and reasoning for LLM-based recommendations. To achieve personalization, we develop a comprehensive Retrieval-Augmented Generation (RAG) system that efficiently indexes and retrieves relevant external memory to enhance LLM personalization capabilities. Furthermore, to enable the synergy between memory and reasoning, our RAG system goes beyond conventional query-based retrieval by integrating reasoning enhanced memory retrieval. Finally, we design a reinforcement learning framework that trains the LLM to autonomously learn effective strategies for both memory utilization and reasoning refinement. By combining dynamic memory retrieval with adaptive reasoning, this approach ensures more accurate, context-aware, and highly personalized recommendations. Extensive experiments demonstrate that MR.Rec significantly outperforms state-of-the-art baselines across multiple metrics, validating its efficacy in delivering intelligent and personalized recommendations. We will release code and data upon paper notification.

**Link**: [arxiv](http://arxiv.org/abs/2510.14629v1),  [pdf](http://arxiv.org/pdf/2510.14629v1)

**Tags**: cs.IR 



### RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF
**Authors**: Qing Yang, Zhenghao Liu, Junxin Wang, Yangfan Du, Pengcheng Huang, Tong Xiao

**Updated**: 2025-10-16T12:40:37Z

**Summary**: Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2510.14628v1),  [pdf](http://arxiv.org/pdf/2510.14628v1)

**Tags**: cs.CL cs.AI 



### Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster   VLM Inference
**Authors**: Natan Bagrov, Eugene Khvedchenia, Borys Tymchenko, Shay Aharon, Lior Kadoch, Tomer Keren, Ofri Masad, Yonatan Geifman, Ran Zilberstein, Tuomas Rintamaki, Matthieu Le, Andrew Tao

**Updated**: 2025-10-16T12:34:38Z

**Summary**: Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.

**Link**: [arxiv](http://arxiv.org/abs/2510.14624v1),  [pdf](http://arxiv.org/pdf/2510.14624v1)

**Tags**: cs.CV 



### Large Language Models Enable Design of Personalized Nudges across   Cultures
**Authors**: Vladimir Maksimenko, Qingyao Xin, Prateek Gupta, Bin Zhang, Prateek Bansal

**Updated**: 2025-10-16T12:33:13Z

**Summary**: Nudge strategies are effective tools for influencing behaviour, but their impact depends on individual preferences. Strategies that work for some individuals may be counterproductive for others. We hypothesize that large language models (LLMs) can facilitate the design of individual-specific nudges without the need for costly and time-intensive behavioural data collection and modelling. To test this, we use LLMs to design personalized decoy-based nudges tailored to individual profiles and cultural contexts, aimed at encouraging air travellers to voluntarily offset CO$_2$ emissions from flights. We evaluate their effectiveness through a large-scale survey experiment ($n=3495$) conducted across five countries. Results show that LLM-informed personalized nudges are more effective than uniform settings, raising offsetting rates by 3-7$\%$ in Germany, Singapore, and the US, though not in China or India. Our study highlights the potential of LLM as a low-cost testbed for piloting nudge strategies. At the same time, cultural heterogeneity constrains their generalizability underscoring the need for combining LLM-based simulations with targeted empirical validation.

**Link**: [arxiv](http://arxiv.org/abs/2508.12045v2),  [pdf](http://arxiv.org/pdf/2508.12045v2)

**Tags**: cs.CY cs.AI 



### Checkpoint-GCG: Auditing and Attacking Fine-Tuning-Based Prompt   Injection Defenses
**Authors**: Xiaoxue Yang, Bozhidar Stevanoski, Matthieu Meeus, Yves-Alexandre de Montjoye

**Updated**: 2025-10-16T12:31:18Z

**Summary**: Large language models (LLMs) are increasingly deployed in real-world applications ranging from chatbots to agentic systems, where they are expected to process untrusted data and follow trusted instructions. Failure to distinguish between the two poses significant security risks, exploited by prompt injection attacks, which inject malicious instructions into the data to control model outputs. Model-level defenses have been proposed to mitigate prompt injection attacks. These defenses fine-tune LLMs to ignore injected instructions in untrusted data. We introduce Checkpoint-GCG, a white-box attack against fine-tuning-based defenses. Checkpoint-GCG enhances the Greedy Coordinate Gradient (GCG) attack by leveraging intermediate model checkpoints produced during fine-tuning to initialize GCG, with each checkpoint acting as a stepping stone for the next one to continuously improve attacks. First, we instantiate Checkpoint-GCG to evaluate the robustness of the state-of-the-art defenses in an auditing setup, assuming both (a) full knowledge of the model input and (b) access to intermediate model checkpoints. We show Checkpoint-GCG to achieve up to $96\%$ attack success rate (ASR) against the strongest defense. Second, we relax the first assumption by searching for a universal suffix that would work on unseen inputs, and obtain up to $89.9\%$ ASR against the strongest defense. Finally, we relax both assumptions by searching for a universal suffix that would transfer to similar black-box models and defenses, achieving an ASR of $63.9\%$ against a newly released defended model from Meta.

**Link**: [arxiv](http://arxiv.org/abs/2505.15738v2),  [pdf](http://arxiv.org/pdf/2505.15738v2)

**Tags**: cs.CR cs.AI cs.CL cs.LG 



### Code-driven Number Sequence Calculation: Enhancing the inductive   Reasoning Abilities of Large Language Models
**Authors**: Kedi Chen, Zhikai Lei, Xu Guo, Xuecheng Wu, Siyuan Zeng, Jianghao Yin, Yinqi Zhang, Qin Chen, Jie Zhou, Liang He, Qipeng Guo, Kai Chen, Wei Zhang

**Updated**: 2025-10-16T12:29:40Z

**Summary**: Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.14620v1),  [pdf](http://arxiv.org/pdf/2510.14620v1)

**Tags**: cs.CL cs.AI 



### Evaluating Arabic Large Language Models: A Survey of Benchmarks,   Methods, and Gaps
**Authors**: Ahmed Alzubaidi, Shaikha Alsuwaidi, Basma El Amel Boussaha, Leen AlQadi, Omar Alkaabi, Mohammed Alyafeai, Hamza Alobeidli, Hakim Hacid

**Updated**: 2025-10-16T12:22:13Z

**Summary**: This survey provides the first systematic review of Arabic LLM benchmarks, analyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains, cultural understanding, and specialized capabilities. We propose a taxonomy organizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and Dialects, and Target-Specific evaluations. Our analysis reveals significant progress in benchmark diversity while identifying critical gaps: limited temporal evaluation, insufficient multi-turn dialogue assessment, and cultural misalignment in translated datasets. We examine three primary approaches: native collection, translation, and synthetic generation discussing their trade-offs regarding authenticity, scale, and cost. This work serves as a comprehensive reference for Arabic NLP researchers, providing insights into benchmark methodologies, reproducibility standards, and evaluation metrics while offering recommendations for future development.

**Link**: [arxiv](http://arxiv.org/abs/2510.13430v2),  [pdf](http://arxiv.org/pdf/2510.13430v2)

**Tags**: cs.CL 



### A Methodology for Assessing the Risk of Metric Failure in LLMs Within   the Financial Domain
**Authors**: William Flanagan, Mukunda Das, Rajitha Ramanayake, Swanuja Maslekar, Meghana Mangipudi, Joong Ho Choi, Shruti Nair, Shambhavi Bhusan, Sanjana Dulam, Mouni Pendharkar, Nidhi Singh, Vashisth Doshi, Sachi Shah Paresh

**Updated**: 2025-10-16T12:21:22Z

**Summary**: As Generative Artificial Intelligence is adopted across the financial services industry, a significant barrier to adoption and usage is measuring model performance. Historical machine learning metrics can oftentimes fail to generalize to GenAI workloads and are often supplemented using Subject Matter Expert (SME) Evaluation. Even in this combination, many projects fail to account for various unique risks present in choosing specific metrics. Additionally, many widespread benchmarks created by foundational research labs and educational institutions fail to generalize to industrial use. This paper explains these challenges and provides a Risk Assessment Framework to allow for better application of SME and machine learning Metrics

**Link**: [arxiv](http://arxiv.org/abs/2510.13524v2),  [pdf](http://arxiv.org/pdf/2510.13524v2)

**Tags**: cs.AI 



### SoK: Evaluating Jailbreak Guardrails for Large Language Models
**Authors**: Xunguang Wang, Zhenlan Ji, Wenxuan Wang, Zongjie Li, Daoyuan Wu, Shuai Wang

**Updated**: 2025-10-16T12:15:42Z

**Summary**: Large Language Models (LLMs) have achieved remarkable progress, but their deployment has exposed critical vulnerabilities, particularly to jailbreak attacks that circumvent safety alignments. Guardrails--external defense mechanisms that monitor and control LLM interactions--have emerged as a promising solution. However, the current landscape of LLM guardrails is fragmented, lacking a unified taxonomy and comprehensive evaluation framework. In this Systematization of Knowledge (SoK) paper, we present the first holistic analysis of jailbreak guardrails for LLMs. We propose a novel, multi-dimensional taxonomy that categorizes guardrails along six key dimensions, and introduce a Security-Efficiency-Utility evaluation framework to assess their practical effectiveness. Through extensive analysis and experiments, we identify the strengths and limitations of existing guardrail approaches, provide insights into optimizing their defense mechanisms, and explore their universality across attack types. Our work offers a structured foundation for future research and development, aiming to guide the principled advancement and deployment of robust LLM guardrails. The code is available at https://github.com/xunguangwang/SoK4JailbreakGuardrails.

**Link**: [arxiv](http://arxiv.org/abs/2506.10597v2),  [pdf](http://arxiv.org/pdf/2506.10597v2)

**Tags**: cs.CR cs.AI 



### JASDA: Introducing Job-Aware Scheduling in Scheduler-Driven Job   Atomization
**Authors**: Michal Konopa, Jan Fesl, Ladislav Ber ánek

**Updated**: 2025-10-16T12:04:27Z

**Summary**: The increasing complexity and temporal variability of workloads on MIG-enabled GPUs challenge the scalability of traditional centralized scheduling. Building upon the SJA concept, this paper introduces JASDA-a novel paradigm that extends SJA from a largely centralized scheduling model toward a fully decentralized negotiation process. In JASDA, jobs actively generate and score feasible subjobs in response to scheduler-announced execution windows, while the scheduler performs policy-driven clearing that balances utilization, fairness, and temporal responsiveness. This bidirectional, iterative interaction embeds feedback, calibration, and probabilistic safety directly into the scheduling loop, enabling adaptive and transparent decision-making. By coupling principles from auction theory and online optimization with the temporal granularity of GPU workloads, JASDA provides a scalable foundation for market-aware and fairness-driven resource management-bridging theoretical scheduling models with practical deployment in modern MIG-enabled environments relevant to Artificial Intelligence and Agriculture 4.0.

**Link**: [arxiv](http://arxiv.org/abs/2510.14599v1),  [pdf](http://arxiv.org/pdf/2510.14599v1)

**Tags**: cs.DC D.4.1; C.4; C.1.4; D.1.3 



### HyCoVAD: A Hybrid SSL-LLM Model for Complex Video Anomaly Detection
**Authors**: Mohammad Mahdi Hemmatyar, Mahdi Jafari, Mohammad Amin Yousefi, Mohammad Reza Nemati, Mobin Azadani, Hamid Reza Rastad, Amirmohammad Akbari

**Updated**: 2025-10-16T12:00:43Z

**Summary**: Video anomaly detection (VAD) is crucial for intelligent surveillance, but a significant challenge lies in identifying complex anomalies, which are events defined by intricate relationships and temporal dependencies among multiple entities rather than by isolated actions. While self-supervised learning (SSL) methods effectively model low-level spatiotemporal patterns, they often struggle to grasp the semantic meaning of these interactions. Conversely, large language models (LLMs) offer powerful contextual reasoning but are computationally expensive for frame-by-frame analysis and lack fine-grained spatial localization. We introduce HyCoVAD, Hybrid Complex Video Anomaly Detection, a hybrid SSL-LLM model that combines a multi-task SSL temporal analyzer with LLM validator. The SSL module is built upon an nnFormer backbone which is a transformer-based model for image segmentation. It is trained with multiple proxy tasks, learns from video frames to identify those suspected of anomaly. The selected frames are then forwarded to the LLM, which enriches the analysis with semantic context by applying structured, rule-based reasoning to validate the presence of anomalies. Experiments on the challenging ComplexVAD dataset show that HyCoVAD achieves a 72.5% frame-level AUC, outperforming existing baselines by 12.5% while reducing LLM computation. We release our interaction anomaly taxonomy, adaptive thresholding protocol, and code to facilitate future research in complex VAD scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.22544v2),  [pdf](http://arxiv.org/pdf/2509.22544v2)

**Tags**: cs.CV 



### Leveraging Coordinate Momentum in SignSGD and Muon: Memory-Optimized   Zero-Order
**Authors**: Egor Petrov, Grigoriy Evseev, Aleksey Antonov, Andrey Veprikov, Nikolay Bushkov, Stanislav Moiseev, Aleksandr Beznosikov

**Updated**: 2025-10-16T11:58:47Z

**Summary**: Fine-tuning Large Language Models (LLMs) is essential for adapting pre-trained models to downstream tasks. Yet traditional first-order optimizers such as Stochastic Gradient Descent (SGD) and Adam incur prohibitive memory and computational costs that scale poorly with model size. In this paper, we investigate zero-order (ZO) optimization methods as a memory- and compute-efficient alternative, particularly in the context of parameter-efficient fine-tuning techniques like LoRA. We propose $\texttt{JAGUAR SignSGD}$, a ZO momentum-based algorithm that extends ZO SignSGD, requiring the same number of parameters as the standard ZO SGD and only $\mathcal{O}(1)$ function evaluations per iteration. To the best of our knowledge, this is the first study to establish rigorous convergence guarantees for SignSGD in the stochastic ZO case. We further propose $\texttt{JAGUAR Muon}$, a novel ZO extension of the Muon optimizer that leverages the matrix structure of model parameters, and we provide its convergence rate under arbitrary stochastic noise. Through extensive experiments on challenging LLM fine-tuning benchmarks, we demonstrate that the proposed algorithms meet or exceed the convergence quality of standard first-order methods, achieving significant memory reduction. Our theoretical and empirical results establish new ZO optimization methods as a practical and theoretically grounded approach for resource-constrained LLM adaptation. Our code is available at https://github.com/brain-mmo-lab/ZO_LLM

**Link**: [arxiv](http://arxiv.org/abs/2506.04430v4),  [pdf](http://arxiv.org/pdf/2506.04430v4)

**Tags**: cs.LG math.OC 



### Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era   of Large Language Models
**Authors**: Rajvee Sheth, Samridhi Raj Sinha, Mahavir Patil, Himanshu Beniwal, Mayank Singh

**Updated**: 2025-10-16T11:58:33Z

**Summary**: Code-switching (CSW), the alternation of languages and scripts within a single utterance, remains a fundamental challenge for multilingual NLP, even amidst the rapid advances of large language models (LLMs). Most LLMs still struggle with mixed-language inputs, limited CSW datasets, and evaluation biases, hindering deployment in multilingual societies. This survey provides the first comprehensive analysis of CSW-aware LLM research, reviewing 308 studies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+ languages. We classify recent advances by architecture, training strategy, and evaluation methodology, outlining how LLMs have reshaped CSW modeling and what challenges persist. The paper concludes with a roadmap emphasizing the need for inclusive datasets, fair evaluation, and linguistically grounded models to achieve truly multilingual intelligence. A curated collection of all resources is maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.

**Link**: [arxiv](http://arxiv.org/abs/2510.07037v3),  [pdf](http://arxiv.org/pdf/2510.07037v3)

**Tags**: cs.CL 



### Following the Autoregressive Nature of LLM Embeddings via Compression   and Alignment
**Authors**: Jingcheng Deng, Zhongtao Jiang, Liang Pang, Liwei Chen, Kun Xu, Zihao Wei, Huawei Shen, Xueqi Cheng

**Updated**: 2025-10-16T11:55:56Z

**Summary**: A new trend uses LLMs as dense text encoders via contrastive learning. However, since LLM embeddings predict the probability distribution of the next token, they are inherently generative and distributive, conflicting with contrastive learning, which requires embeddings to capture full-text semantics and align via cosine similarity. This discrepancy hinders the full utilization of LLMs' pre-training capabilities, resulting in inefficient learning. In response to this issue, we propose AutoRegEmbed, a new contrastive learning method built on embedding conditional probability distributions, which integrates two core tasks: information compression and conditional distribution alignment. The information compression task encodes text into the embedding space, ensuring that the embedding vectors capture global semantics. The conditional distribution alignment task focuses on aligning text embeddings with positive samples embeddings by leveraging the conditional distribution of embeddings while simultaneously reducing the likelihood of generating negative samples from text embeddings, thereby achieving embedding alignment and uniformity. Experimental results demonstrate that our method significantly outperforms traditional contrastive learning approaches and achieves performance comparable to state-of-the-art models when using the same amount of data.

**Link**: [arxiv](http://arxiv.org/abs/2502.11401v3),  [pdf](http://arxiv.org/pdf/2502.11401v3)

**Tags**: cs.CL 



### Just-In-Time Objectives: A General Approach for Specialized AI   Interactions
**Authors**: Michelle S. Lam, Omar Shaikh, Hallie Xu, Alice Guo, Diyi Yang, Jeffrey Heer, James A. Landay, Michael S. Bernstein

**Updated**: 2025-10-16T11:53:17Z

**Summary**: Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., "Clarify the abstract's research contribution") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.

**Link**: [arxiv](http://arxiv.org/abs/2510.14591v1),  [pdf](http://arxiv.org/pdf/2510.14591v1)

**Tags**: cs.HC cs.AI cs.CL 



### ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient   RAG
**Authors**: Yikuan Hu, Jifeng Zhu, Lanrui Tang, Chen Huang

**Updated**: 2025-10-16T11:41:38Z

**Summary**: Knowledge graphs (KGs), with their structured representation capabilities, offer promising avenue for enhancing Retrieval Augmented Generation (RAG) systems, leading to the development of KG-RAG systems. Nevertheless, existing methods often struggle to achieve effective synergy between system effectiveness and cost efficiency, leading to neither unsatisfying performance nor excessive LLM prompt tokens and inference time. To this end, this paper proposes REMINDRAG, which employs an LLM-guided graph traversal featuring node exploration, node exploitation, and, most notably, memory replay, to improve both system effectiveness and cost efficiency. Specifically, REMINDRAG memorizes traversal experience within KG edge embeddings, mirroring the way LLMs "memorize" world knowledge within their parameters, but in a train-free manner. We theoretically and experimentally confirm the effectiveness of REMINDRAG, demonstrating its superiority over existing baselines across various benchmark datasets and LLM backbones. Our code is available at https://github.com/kilgrims/ReMindRAG.

**Link**: [arxiv](http://arxiv.org/abs/2510.13193v2),  [pdf](http://arxiv.org/pdf/2510.13193v2)

**Tags**: cs.IR 



### Comparing Human and Language Models Sentence Processing Difficulties on   Complex Structures
**Authors**: Samuel Joseph Amouyal, Aya Meltzer-Asscher, Jonathan Berant

**Updated**: 2025-10-16T11:40:29Z

**Summary**: Large language models (LLMs) that fluently converse with humans are a reality - but do LLMs experience human-like processing difficulties? We systematically compare human and LLM sentence comprehension across seven challenging linguistic structures. We collect sentence comprehension data from humans and five families of state-of-the-art LLMs, varying in size and training procedure in a unified experimental framework. Our results show LLMs overall struggle on the target structures, but especially on garden path (GP) sentences. Indeed, while the strongest models achieve near perfect accuracy on non-GP structures (93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5). Additionally, when ranking structures based on average performance, rank correlation between humans and models increases with parameter count. For each target structure, we also collect data for their matched baseline without the difficult structure. Comparing performance on the target vs. baseline sentences, the performance gap observed in humans holds for LLMs, with two exceptions: for models that are too weak performance is uniformly low across both sentence types, and for models that are too strong the performance is uniformly high. Together, these reveal convergence and divergence in human and LLM sentence comprehension, offering new insights into the similarity of humans and LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.07141v2),  [pdf](http://arxiv.org/pdf/2510.07141v2)

**Tags**: cs.CL cs.AI 



