# Arxiv Results
## Keyword: kv cache 
 ### Iris: A Next Generation Digital Pathology Rendering Engine
**Authors**: Ryan Erik Landvater, Ulysses Balis

**Updated**: 2025-04-23T15:02:16Z

**Summary**: Digital pathology is a tool of rapidly evolving importance within the discipline of pathology. Whole slide imaging promises numerous advantages; however, adoption is limited by challenges in ease of use and speed of high-quality image rendering relative to the simplicity and visual quality of glass slides. We introduce Iris, a new high-performance digital pathology rendering system. Specifically, we outline and detail the performance metrics of Iris Core, the core rendering engine technology. Iris Core comprises machine code modules written from the ground up in C++ and using Vulkan, a low-level and low-overhead cross-platform graphical processing unit application program interface, and our novel rapid tile buffering algorithms. We provide a detailed explanation of Iris Core's system architecture, including the stateless isolation of core processes, interprocess communication paradigms, and explicit synchronization paradigms that provide powerful control over the graphical processing unit. Iris Core achieves slide rendering at the sustained maximum frame rate on all tested platforms and buffers an entire new slide field of, view without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is able to buffer and compute high-fidelity reduction-enhancements for viewing low-power cytology with increased visual quality at a rate of 100-160 us per slide tile, and with a cumulative median buffering rate of 1.36 GB of decompressed image data per second. This buffering rate allows for an entirely new field of view to be fully buffered and rendered in less than a single monitor refresh on a standard display, and high detail features within 2-3 monitor refresh frames. These metrics far exceed previously published specifications, beyond an order of magnitude in some contexts. The system shows no slowing with high use loads, but rather increases performance due to cache mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2504.15437v2),  [pdf](http://arxiv.org/pdf/2504.15437v2)

**Tags**: cs.GR 



### Fluctuated lattice-driven charge density wave far above the condensation   temperature in kagome superconductor KV3Sb5
**Authors**: Haoran Liu, Shaofeng Duan, Xiangqi Liu, Zhihua Liu, Shichong Wang, Lingxiao Gu, Jiongyu Huang, Wenxuan Yang, Jianzhe Liu, Dong Qian, Yanfeng Guo, Wentao Zhang

**Updated**: 2025-04-23T11:18:34Z

**Summary**: The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including an unconventional charge density wave (CDW). Elucidating the underlying mechanism behind the CDW transition is crucial for unraveling the complex interactions among these phases. However, the driving force of the CDW remains a topic of debate due to the intertwined interactions among the system's various excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by isolating the ultrafast electronic phase transition using time- and angleresolved photoemission spectroscopy. An ultrafast electronic phase transition was observed at a critical photoexcitation fluence, Fc, without reduction in CDW lattice-distortion-induced band folding. This folded band persisted up to 150 K under equilibrium heating, well above the CDW condensation temperature of Tc = 78 K. Notably, the pump-induced band shifts at Fc were comparable to those caused by thermal effects at Tc. These findings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges above 150 K, with out-of-plane electronic correlations leading to the $2\times2 \times 2$ CDW near Tc, offering key insights into the interplay between the electronic and structural dynamics in AV$_3$Sb$_5$.

**Link**: [arxiv](http://arxiv.org/abs/2504.16620v1),  [pdf](http://arxiv.org/pdf/2504.16620v1)

**Tags**: cond-mat.str-el cond-mat.mtrl-sci cond-mat.supr-con 



### The NIC should be part of the OS
**Authors**: Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-04-23T10:48:52Z

**Summary**: The network interface adapter (NIC) is a critical component of a cloud server occupying a unique position. Not only is network performance vital to efficient operation of the machine, but unlike compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency.   Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path.   However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernelbypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.10138v2),  [pdf](http://arxiv.org/pdf/2501.10138v2)

**Tags**: cs.OS cs.AR cs.NI 



### CAOTE: KV Caching through Attention Output Error based Token Eviction
**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T05:04:58Z

**Summary**: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.

**Link**: [arxiv](http://arxiv.org/abs/2504.14051v2),  [pdf](http://arxiv.org/pdf/2504.14051v2)

**Tags**: cs.LG 



### ML-based Adaptive Prefetching and Data Placement for US HEP Systems
**Authors**: Venkat Sai Suman Lamba Karanam, Sarat Sasank Barla, Byrav Ramamurthy, Derek Weitzel

**Updated**: 2025-04-23T04:21:49Z

**Summary**: Although benefits from caching in US HEP are well-known, current caching strategies are not adaptive i.e they do not adapt to changing cache access patterns. Newer developments such as the High-Luminosity - Large Hadron Collider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward streaming readout based Data Acquisition systems (DAQs) will increase the data production exponentially and hence burden the storage, compute & network infrastructures. Moreover, existing caching frameworks are optimized to reduce latency, but not optimized for storage. This, in combination with limited cache capacities relative to total data, makes it difficult to achieve data locality.   In this work, we present Machine Learning-aided (ML) caching strategies. Specifically, we first present a Long Short-Term Memory-based (LSTM) hourly and multi-step cache usage prediction. Second, we present an hourly file-level access prediction model based on CatboostRegressor. To date, most ML-based cache prediction strategies in HEP have focused on daily cache usage and limited works tackled hourly cache usage and even fewer strategies addressed hourly file-level access prediction. File-level access prediction allows for the design of intelligent prefetching and data placement strategies with fine-grained control. We validated our cache prediction strategies using data collected from SoCal MINI caches in August 2024. We are currently extending the WRENCH simulator to reflect the US HEP ecosystem at the storage, network and compute levels. We plan to deploy our cache prediction strategies into WRENCH and later perform extensive analysis with complex data access patterns and candidate infrastructure configurations.

**Link**: [arxiv](http://arxiv.org/abs/2503.06015v2),  [pdf](http://arxiv.org/pdf/2503.06015v2)

**Tags**: cs.DC 



### The Dawn of Disaggregation and the Coherence Conundrum: A Call for   Federated Coherence
**Authors**: Jaewan Hong, Marcos K. Aguilera, Emmanuel Amaro, Vincent Liu, Aurojit Panda, Ion Stoica

**Updated**: 2025-04-22T23:52:13Z

**Summary**: Disaggregated memory is an upcoming data center technology that will allow nodes (servers) to share data efficiently. Sharing data creates a debate on the level of cache coherence the system should provide. While current proposals aim to provide coherence for all or parts of the disaggregated memory, we argue that this approach is problematic, because of scalability limitations and hardware complexity. Instead, we propose and formally define federated coherence, a model that provides coherence only within nodes, not across nodes. Federated coherence can use current intra-node coherence provided by processors without requiring expensive mechanisms for inter-node coherence. Developers can use federated coherence with a few simple programming paradigms and a synchronization library. We sketch some potential applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16324v1),  [pdf](http://arxiv.org/pdf/2504.16324v1)

**Tags**: cs.DC cs.AR 



### Key, Value, Compress: A Systematic Exploration of KV Cache Compression   Techniques
**Authors**: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

**Updated**: 2025-04-22T17:34:34Z

**Summary**: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.

**Link**: [arxiv](http://arxiv.org/abs/2503.11816v3),  [pdf](http://arxiv.org/pdf/2503.11816v3)

**Tags**: cs.CL 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hoßfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-04-22T17:23:28Z

**Summary**: As AI workloads drive soaring memory requirements, there is a need for higher-density on-chip memory for domain-specific accelerators that goes beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, there has been little work in factoring dynamic application profiles into such design decisions. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and computes data lifetimes in domain-specific accelerators. By combining instrumentation and simulation across retargetable hardware backends, GainSight aligns heterogeneous memory designs with workload-specific traffic and lifetime metrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA H100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3) Up to 90% of GPU cache fetches are never reused, highlighting inefficiencies in terms of cache pollution. These insights that GainSight provides can be used to better understand the design spaces of both emerging on-chip memories and software algorithmic optimizations for the next generation of AI accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v2),  [pdf](http://arxiv.org/pdf/2504.14866v2)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### Optimizing SLO-oriented LLM Serving with PD-Multiplexing
**Authors**: Weihao Cui, Yukang Chen, Han Zhao, Ziyi Xu, Quan Chen, Xusheng Chen, Yangjie Zhou, Shixuan Sun, Minyi Guo

**Updated**: 2025-04-22T15:19:48Z

**Summary**: Modern LLM services demand high throughput and stringent SLO guarantees across two distinct inference phases-prefill and decode-and complex multi-turn workflows. However, current systems face a fundamental tradeoff: out-of-place compute partition enables per-phase SLO attainment, while in-place memory sharing maximizes throughput via KV cache reuse. Moreover, existing in-place compute partition also encounters low utilization and high overhead due to phase-coupling design. We present Drift, a new LLM serving framework that resolves this tension via PD multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs, while preserving in-place memory sharing. To fully leverage the multiplexing capability, Drift introduces an adaptive gang scheduling mechanism, a contention-free modeling method, and a SLO-aware dispatching policy. Evaluation shows that Drift achieves an average $5.1\times$ throughput improvement (up to $17.5\times$) over state-of-the-art baselines, while consistently meeting SLO targets under complex LLM workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.14489v2),  [pdf](http://arxiv.org/pdf/2504.14489v2)

**Tags**: cs.OS 



### SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large   Language Model Inference
**Authors**: Yihao Zhao, Jiadun Chen, Peng Sun, Lei Li, Xuanzhe Liu, Xin Jin

**Updated**: 2025-04-22T09:08:46Z

**Summary**: Large language models (LLMs) with different architectures and sizes have been developed. Serving each LLM with dedicated GPUs leads to resource waste and service inefficiency due to the varying demand of LLM requests. A common practice is to share multiple LLMs. However, existing sharing systems either do not consider the autoregressive pattern of LLM services, or only focus on improving the throughput, which impairs the sharing performance, especially the serving latency. We present SeaLLM, which enables service-aware and latency-optimized LLM sharing. SeaLLM improves the overall sharing performance by (1) a latency-optimized scheduling algorithm utilizing the characteristics of LLM services, (2) a placement algorithm to determine the placement plan and an adaptive replacement algorithm to decide the replacement interval, and (3) a unified key-value cache to share GPU memory among LLM services efficiently. Our evaluation under real-world traces and LLM services demonstrates that SeaLLM improves the normalized latency by up to $13.60\times$, the tail latency by up to $18.69\times$, and the SLO attainment by up to $3.64\times$ compared to existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2504.15720v1),  [pdf](http://arxiv.org/pdf/2504.15720v1)

**Tags**: cs.DC 



### Reimagining Memory Access for LLM Inference: Compression-Aware Memory   Controller Design
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-04-21T22:13:07Z

**Summary**: The efficiency of Large Language Model~(LLM) inference is often constrained by substantial memory bandwidth and capacity demands. Existing techniques, such as pruning, quantization, and mixture of experts/depth, reduce memory capacity and/or bandwidth consumption at the cost of slight degradation in inference quality. This paper introduces a design solution that further alleviates memory bottlenecks by enhancing the on-chip memory controller in AI accelerators to achieve two main objectives: (1) significantly reducing memory capacity and bandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of model weights and key-value (KV) cache without compromising inference quality, and (2) enabling memory bandwidth and energy consumption to scale proportionally with context-dependent dynamic quantization. These goals are accomplished by equipping the on-chip memory controller with mechanisms to improve fine-grained bit-level accessibility and compressibility of weights and KV cache through LLM-aware configuration of in-memory placement and representation. Experimental results on publicly available LLMs demonstrate the effectiveness of this approach, showing memory footprint reductions of 25.2\% for model weights and 46.9\% for KV cache. In addition, our hardware prototype at 4\,GHz and 32 lanes (7\,nm) achieves 8\,TB/s throughput with a modest area overhead (under 3.8\,mm\(^2\)), which underscores the viability of LLM-aware memory control as a key to efficient large-scale inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.18869v3),  [pdf](http://arxiv.org/pdf/2503.18869v3)

**Tags**: cs.AR 



### FlashInfer: Efficient and Customizable Attention Engine for LLM   Inference Serving
**Authors**: Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze

**Updated**: 2025-04-21T20:10:11Z

**Summary**: Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01005v2),  [pdf](http://arxiv.org/pdf/2501.01005v2)

**Tags**: cs.DC cs.AI cs.LG 



### KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM   Inference in Resource-Constrained Environments
**Authors**: Junyoung Park, Dalton Jones, Matt J Morse, Raghavv Goel, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T18:02:55Z

**Summary**: In this work, we demonstrate that distinctive keys during LLM inference tend to have high attention scores. We explore this phenomenon and propose KeyDiff, a training-free KV cache eviction method based on key similarity. This method facilitates the deployment of LLM-based application requiring long input prompts in resource-constrained environments with limited memory and compute budgets. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses. We demonstrate that KeyDiff computes the optimal solution to a KV cache selection problem that maximizes key diversity, providing a theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse tasks and models, illustrating a performance gap of less than 0.04\% with 8K cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

**Link**: [arxiv](http://arxiv.org/abs/2504.15364v2),  [pdf](http://arxiv.org/pdf/2504.15364v2)

**Tags**: cs.AI 



### Joint Knowledge and Power Management for Secure Semantic Communication   Networks
**Authors**: Xuesong Liu, Yansong Liu, Haoyu Tang, Fangzhou Zhao, Le Xia, Yao Sun

**Updated**: 2025-04-21T17:39:59Z

**Summary**: Recently, semantic communication (SemCom) has shown its great superiorities in resource savings and information exchanges. However, while its unique background knowledge guarantees accurate semantic reasoning and recovery, semantic information security-related concerns are introduced at the same time. Since the potential eavesdroppers may have the same background knowledge to accurately decrypt the private semantic information transmitted between legal SemCom users, this makes the knowledge management in SemCom networks rather challenging in joint consideration with the power control. To this end, this paper focuses on jointly addressing three core issues of power allocation, knowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in secure SemCom networks. We first develop a novel performance metric, namely semantic secrecy throughput (SST), to quantify the information security level that can be achieved at each pair of D2D SemCom users. Next, an SST maximization problem is formulated subject to secure SemCom-related delay and reliability constraints. Afterward, we propose a security-aware resource management solution using the Lagrange primal-dual method and a two-stage method. Simulation results demonstrate our proposed solution nearly doubles the SST performance and realizes less than half of the queuing delay performance compared to different benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.15260v1),  [pdf](http://arxiv.org/pdf/2504.15260v1)

**Tags**: eess.SP 



### Lance: Efficient Random Access in Columnar Storage through Adaptive   Structural Encodings
**Authors**: Weston Pace, Chang She, Lei Xu, Will Jones, Albert Lockett, Jun Wang, Raunak Shah

**Updated**: 2025-04-21T17:22:18Z

**Summary**: The growing interest in artificial intelligence has created workloads that require both sequential and random access. At the same time, NVMe-backed storage solutions have emerged, providing caching capability for large columnar datasets in cloud storage. Current columnar storage libraries fall short of effectively utilizing an NVMe device's capabilities, especially when it comes to random access. Historically, this has been assumed an implicit weakness in columnar storage formats, but this has not been sufficiently explored. In this paper, we examine the effectiveness of popular columnar formats such as Apache Arrow, Apache Parquet, and Lance in both random access and full scan tasks against NVMe storage.   We argue that effective encoding of a column's structure, such as the repetition and validity information, is the key to unlocking the disk's performance. We show that Parquet, when configured correctly, can achieve over 60x better random access performance than default settings. We also show that this high random access performance requires making minor trade-offs in scan performance and RAM utilization. We then describe the Lance structural encoding scheme, which alternates between two different structural encodings based on data width, and achieves better random access performance without making trade-offs in scan performance or RAM utilization.

**Link**: [arxiv](http://arxiv.org/abs/2504.15247v1),  [pdf](http://arxiv.org/pdf/2504.15247v1)

**Tags**: cs.DB H.3.2 



### A Unified Framework for Quantitative Cache Analysis
**Authors**: Sophie Kahlen, Jan Reineke

**Updated**: 2025-04-21T15:36:53Z

**Summary**: In this work we unify two existing lines of work towards cache analysis for non-LRU policies. To this end, we extend the notion of competitiveness to block competitiveness and systematically analyze the competitiveness and block competitiveness of FIFO and MRU relative to LRU for arbitrary associativities. We show how competitiveness and block competitiveness can be exploited in state-of-the-art WCET analysis based on the results of existing persistence analyses for LRU. Unlike prior work, our approach is applicable to microarchitectures that exhibit timing anomalies. We experimentally evaluate the precision and cost of our approach on benchmarks from TACLeBench. The experiments demonstrate that quantitative cache analysis for FIFO and MRU comes close to the precision of LRU.

**Link**: [arxiv](http://arxiv.org/abs/2503.16588v3),  [pdf](http://arxiv.org/pdf/2503.16588v3)

**Tags**: cs.PL 68 D.3.4 



### LServe: Efficient Long-sequence LLM Serving with Unified Sparse   Attention
**Authors**: Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han

**Updated**: 2025-04-21T15:13:44Z

**Summary**: Large language models (LLMs) have shown remarkable potential in processing long sequences and complex reasoning tasks, yet efficiently serving these models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context and reasoning capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.

**Link**: [arxiv](http://arxiv.org/abs/2502.14866v2),  [pdf](http://arxiv.org/pdf/2502.14866v2)

**Tags**: cs.CL cs.AI cs.DC cs.LG cs.PF 



### Is Intelligence the Right Direction in New OS Scheduling for Multiple   Resources in Cloud Environments?
**Authors**: Xinglei Dou, Lei Liu, Limin Xiao

**Updated**: 2025-04-21T11:09:43Z

**Summary**: Making it intelligent is a promising way in System/OS design. This paper proposes OSML+, a new ML-based resource scheduling mechanism for co-located cloud services. OSML+ intelligently schedules the cache and main memory bandwidth resources at the memory hierarchy and the computing core resources simultaneously. OSML+ uses a multi-model collaborative learning approach during its scheduling and thus can handle complicated cases, e.g., avoiding resource cliffs, sharing resources among applications, enabling different scheduling policies for applications with different priorities, etc. OSML+ can converge faster using ML models than previous studies. Moreover, OSML+ can automatically learn on the fly and handle dynamically changing workloads accordingly. Using transfer learning technologies, we show our design can work well across various cloud servers, including the latest off-the-shelf large-scale servers. Our experimental results show that OSML+ supports higher loads and meets QoS targets with lower overheads than previous studies.

**Link**: [arxiv](http://arxiv.org/abs/2504.15021v1),  [pdf](http://arxiv.org/pdf/2504.15021v1)

**Tags**: cs.DC cs.LG cs.PF 



### Efficient Pretraining Length Scaling
**Authors**: Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou

**Updated**: 2025-04-24T04:13:49Z

**Summary**: Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.14992v2),  [pdf](http://arxiv.org/pdf/2504.14992v2)

**Tags**: cs.CL 



### Context Parallelism for Scalable Million-Token Inference
**Authors**: Amy Yang, Jingyi Yang, Aya Ibrahim, Xinfeng Xie, Bangsheng Tang, Grigory Sizov, Jeremy Reizenstein, Jongsoo Park, Jianyu Huang

**Updated**: 2025-04-21T03:40:10Z

**Summary**: We present context parallelism for long-context large language model inference, which achieves near-linear scaling for long-context prefill latency with up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M context prefill with Llama3 405B model in 77s (93% parallelization efficiency, 63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two lossless exact ring attention variants: pass-KV and pass-Q to cover a wide range of use cases with the state-of-the-art performance: full prefill, persistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected with RDMA and TCP both show similar scalability for long-context prefill, demonstrating that our method scales well using common commercial data center with medium-to-low inter-host bandwidth.

**Link**: [arxiv](http://arxiv.org/abs/2411.01783v3),  [pdf](http://arxiv.org/pdf/2411.01783v3)

**Tags**: cs.DC cs.AI cs.LG 



### gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM   Serving with Token Throttling
**Authors**: Tianyu Guo, Xianwei Zhang, Jiangsu Du, Zhiguang Chen, Nong Xiao, Yutong Lu

**Updated**: 2025-04-21T00:07:49Z

**Summary**: Pipeline parallelism has emerged as a predominant approach for deploying large language models (LLMs) across distributed nodes, owing to its lower communication overhead compared to tensor parallelism. While demonstrating high throughput in request serving, pipeline parallelism often suffers from performance limitations caused by pipeline bubbles, which are primarily resulted from imbalanced computation delays across batches. Existing methods like Sarathi-Serve attempt to address this through hybrid scheduling of chunked prefill and decode tokens using a fixed token budget. However, such methods may experience significant fluctuations due to either insufficient prefill tokens or uneven distribution of decode tokens, ultimately leading to computational imbalance. To overcome these inefficiencies, we present gLLM, a globally balanced pipeline parallelism system incorporating Token Throttling to effectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a fine-grained scheduling policy that independently regulates the quantities of prefill and decode tokens, thus enabling balanced computation by leveraging global information from the inference system. Specifically, for decode tokens, gLLM maintains near-consistent token count across processing batches. For prefill tokens, it dynamically adjusts batch sizes based on both total pending tokens and the memory utilization rates of key-value cache (KV cache). Furthermore, gLLM runtime adopts an asynchronous execution and message passing architecture specifically optimized for pipeline parallelism characteristics. Experimental evaluations with representative LLMs show that gLLM achieves significant performance improvements, delivering 11% to 398% higher maximum throughput compared to state-of-the-art pipeline or tensor parallelism systems, while simultaneously maintaining lower latency.

**Link**: [arxiv](http://arxiv.org/abs/2504.14775v1),  [pdf](http://arxiv.org/pdf/2504.14775v1)

**Tags**: cs.DC 



### Star Attention: Efficient LLM Inference over Long Sequences
**Authors**: Shantanu Acharya, Fei Jia, Boris Ginsburg

**Updated**: 2025-04-20T21:50:03Z

**Summary**: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2411.17116v2),  [pdf](http://arxiv.org/pdf/2411.17116v2)

**Tags**: cs.CL cs.AI cs.LG 



### Understanding and Optimizing Multi-Stage AI Inference Pipelines
**Authors**: Abhimanyu Rajeshkumar Bambhaniya, Hanjiang Wu, Suvinay Subramanian, Sudarshan Srinivasan, Souvik Kundu, Amir Yazdanbakhsh, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna

**Updated**: 2025-04-20T19:57:16Z

**Summary**: The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.   To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.09775v3),  [pdf](http://arxiv.org/pdf/2504.09775v3)

**Tags**: cs.AR cs.AI cs.DC cs.LG 



### Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink   of an Eye
**Authors**: Bradley Morgan, Gal Horowitz, Sioli O'Connell, Stephan van Schaik, Chitchanok Chuengsatiansup, Daniel Genkin, Olaf Maennel, Paul Montague, Eyal Ronen, Yuval Yarom

**Updated**: 2025-04-20T07:53:09Z

**Summary**: An essential step for mounting cache attacks is finding eviction sets, collections of memory locations that contend on cache space. On Intel processors, one of the main challenges for identifying contending addresses is the sliced cache design, where the processor hashes the physical address to determine where in the cache a memory location is stored. While past works have demonstrated that the hash function can be reversed, they also showed that it depends on physical address bits that the adversary does not know.   In this work, we make three main contributions to the art of finding eviction sets. We first exploit microarchitectural races to compare memory access times and identify the cache slice to which an address maps. We then use the known hash function to both reduce the error rate in our slice identification method and to reduce the work by extrapolating slice mappings to untested memory addresses. Finally, we show how to propagate information on eviction sets across different page offsets for the hitherto unexplored case of non-linear hash functions.   Our contributions allow for entire LLC eviction set generation in 0.7 seconds on the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear functions. This represents a significant improvement compared to state-of-the-art techniques taking 9x and 10x longer, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2504.11208v2),  [pdf](http://arxiv.org/pdf/2504.11208v2)

**Tags**: cs.CR 



### Deuteronomy 2.0: Record Caching and Latch Freedom
**Authors**: David Lomet

**Updated**: 2025-04-20T00:49:27Z

**Summary**: The Deuteronomy transactional key-value store is unique architecturally in providing separation between transaction functionality -- its Transactional Component (TC) and data management -- its Data Component (DC). It is unique in technology by (1) supporting record caching, a smaller unit than the traditional page; and (2) protecting resources during concurrent execution using a latch-free approach. Both technologies are enabled by delta updating. This paper explains how record caching improves cache cost/performance. It also shows how a new latch-free approach makes implementation easier and improves performance.

**Link**: [arxiv](http://arxiv.org/abs/2504.14435v1),  [pdf](http://arxiv.org/pdf/2504.14435v1)

**Tags**: cs.DB 



### A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated   in a coupled reactive transport HPC simulation
**Authors**: Max Lübke, Marco De Lucia, Stefan Petri, Bettina Schnor

**Updated**: 2025-04-19T18:25:20Z

**Summary**: Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.

**Link**: [arxiv](http://arxiv.org/abs/2504.14374v1),  [pdf](http://arxiv.org/pdf/2504.14374v1)

**Tags**: cs.DC 



### Room-temperature high-average-power strong-field terahertz source based   on industrial high-repetition-rate femtosecond laser
**Authors**: Deyin Kong, Yichen Su, Cheng Song, Xiaojun Wu

**Updated**: 2025-04-19T06:18:56Z

**Summary**: Free-space strong-field terahertz (THz) pulses, generated via optical rectification of femtosecond lasers in nonlinear crystals, are pivotal in various applications. However, conventional Ti:sapphire lasers struggle to produce high-average-power THz due to their limited output power. While kilowatt ytterbium lasers are increasingly adopted, their application in THz generation faces challenges: low optical-to-THz conversion efficiency (attributed to long pulse durations and low energy) and crystal damage under high pumping power. Here, we report a high-average-power strong-field THz source using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ, 50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By systematically optimizing TPFP implementations and comparing grating- and echelon-type configurations, we achieve a THz source with 64.5 mW average power at 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at 0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in cobalt-iron ferromagnetic nanofilms. This high-repetition-rate, high-average-power THz system, combined with its potential capabilities in high signal-to-noise spectroscopy and imaging, promises transformative impacts in quantum matter manipulation, non-destructive testing, and biomedicine.

**Link**: [arxiv](http://arxiv.org/abs/2504.14196v1),  [pdf](http://arxiv.org/pdf/2504.14196v1)

**Tags**: physics.optics 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-04-19T05:57:44Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v5),  [pdf](http://arxiv.org/pdf/2411.10659v5)

**Tags**: cs.PL 



### LogicTree: Structured Proof Exploration for Coherent and Rigorous   Logical Reasoning with Large Language Models
**Authors**: Kang He, Kaushik Roy

**Updated**: 2025-04-18T22:10:02Z

**Summary**: Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average.

**Link**: [arxiv](http://arxiv.org/abs/2504.14089v1),  [pdf](http://arxiv.org/pdf/2504.14089v1)

**Tags**: cs.CL cs.AI cs.LG 



### Gradual Binary Search and Dimension Expansion : A general method for   activation quantization in LLMs
**Authors**: Lucas Maisonnave, Cyril Moineau, Olivier Bichler, Fabrice Rastello

**Updated**: 2025-04-18T13:46:58Z

**Summary**: Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40\% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley algorithm. We theoretically demonstrates the superiority of Hadamard matrices in reducing outliers.We achieved 3-bit quantization for weights, activations, and KV cache, significantly enhancing model performance. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.

**Link**: [arxiv](http://arxiv.org/abs/2504.13989v1),  [pdf](http://arxiv.org/pdf/2504.13989v1)

**Tags**: cs.LG cs.AI cs.CL 



### CacheFormer: High Attention-Based Segment Caching
**Authors**: Sushant Singh, Ausif Mahmood

**Updated**: 2025-04-18T06:34:57Z

**Summary**: Efficiently handling long contexts in transformer-based language models with low perplexity is an active area of research. Numerous recent approaches like Linformer, Longformer, Performer, and Structured state space models (SSMs)., have not fully resolved this problem. All these models strive to reduce the quadratic time complexity of the attention mechanism while minimizing the loss in quality due to the effective compression of the long context. Inspired by the cache and virtual memory principle in computers, where in case of a cache miss, not only the needed data is retrieved from the memory, but the adjacent data is also obtained, we apply this concept to handling long contexts by dividing it into small segments. In our design, we retrieve the nearby segments in an uncompressed form when high segment-level attention occurs at the compressed level. Our en-hancements for handling long context include aggregating four attention mechanisms consisting of short sliding window attention, long compressed segmented attention, dynamically retrieving top k high attention uncompressed segments, and overlapping segments in long segment attention to avoid segment fragmentation. These enhancements result in an architecture that outperforms ex-isting SOTA architectures with an average perplexity improvement of 8.5% over similar model sizes.

**Link**: [arxiv](http://arxiv.org/abs/2504.13981v1),  [pdf](http://arxiv.org/pdf/2504.13981v1)

**Tags**: cs.LG cs.AI 



### Towards Federated Multi-Armed Bandit Learning for Content Dissemination   using Swarm of UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-04-18T05:13:52Z

**Summary**: This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.

**Link**: [arxiv](http://arxiv.org/abs/2501.09146v2),  [pdf](http://arxiv.org/pdf/2501.09146v2)

**Tags**: cs.LG cs.NI I.2.11 



### HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM   Inference via GPU Co-processing
**Authors**: Myunghyun Rhee, Joonseop Sim, Taeyoung Ahn, Seungyong Lee, Daegun Yoon, Euiseok Kim, Kyoung Park, Youngpyo Joo, Hosik Kim

**Updated**: 2025-04-18T03:31:08Z

**Summary**: The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2504.16112v1),  [pdf](http://arxiv.org/pdf/2504.16112v1)

**Tags**: cs.AR cs.AI cs.CL cs.DC 



### EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for   Enhanced Cache Occupancy Attacks
**Authors**: Tianhong Xu, Aidong Adam Ding, Yunsi Fei

**Updated**: 2025-04-18T00:21:00Z

**Summary**: Cache occupancy attacks exploit the shared nature of cache hierarchies to infer a victim's activities by monitoring overall cache usage, unlike access-driven cache attacks that focus on specific cache lines or sets. There exists some prior work that target the last-level cache (LLC) of Intel processors, which is inclusive of higher-level caches, and L2 caches of ARM systems. In this paper, we target the System-Level Cache (SLC) of Apple M-series SoCs, which is exclusive to higher-level CPU caches. We address the challenges of the exclusiveness and propose a suite of SLC-cache occupancy attacks, the first of its kind, where an adversary can monitor GPU and other CPU cluster activities from their own CPU cluster. We first discover the structure of SLC in Apple M1 SOC and various policies pertaining to access and sharing through reverse engineering. We propose two attacks against websites. One is a coarse-grained fingerprinting attack, recognizing which website is accessed based on their different GPU memory access patterns monitored through the SLC occupancy channel. The other attack is a fine-grained pixel stealing attack, which precisely monitors the GPU memory usage for rendering different pixels, through the SLC occupancy channel. Third, we introduce a novel screen capturing attack which works beyond webpages, with the monitoring granularity of 57 rows of pixels (there are 1600 rows for the screen). This significantly expands the attack surface, allowing the adversary to retrieve any screen display, posing a substantial new threat to system security. Our findings reveal critical vulnerabilities in Apple's M-series SoCs and emphasize the urgent need for effective countermeasures against cache occupancy attacks in heterogeneous computing environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.13385v1),  [pdf](http://arxiv.org/pdf/2504.13385v1)

**Tags**: cs.CR cs.AR 



### Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN   Heterostructures
**Authors**: Seungheon Shin, Kyle Liddy, Yinxuan Zhu, Chandan Joishi, Brianna A. Klein, Andrew Armstrong, Andrew A. Allerman, Siddharth Rajan

**Updated**: 2025-04-17T23:45:51Z

**Summary**: We report on energy bands and breakdown characteristics of Al2O3 dielectrics on ultra-wide bandgap (UWBG) AlGaN heterostructures. Metal-dielectric-semiconductor structures are important to sustain high fields needed for future high-performance UWBG transistors. Using systematic experiments, we determined the fixed charge density (> 1013 cm-2), the dielectric/interface, and electric fields in the oxide of under flat-band conditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x 10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In lateral metal-semiconductor-insulator test structures, breakdown voltage exceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013 cm-2. The effective peak electric field and average breakdown field were estimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings demonstrate the potential of Al2O2 integration for enhancing the breakdown performance of UWBG AlGaN HEMTs.

**Link**: [arxiv](http://arxiv.org/abs/2504.01291v2),  [pdf](http://arxiv.org/pdf/2504.01291v2)

**Tags**: cond-mat.mtrl-sci physics.app-ph 



### ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM   Inference
**Authors**: Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen

**Updated**: 2025-04-17T21:19:19Z

**Summary**: With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.

**Link**: [arxiv](http://arxiv.org/abs/2410.21465v2),  [pdf](http://arxiv.org/pdf/2410.21465v2)

**Tags**: cs.LG cs.CL 



### Long-Context Autoregressive Video Modeling with Next-Frame Prediction
**Authors**: Yuchao Gu, Weijia Mao, Mike Zheng Shou

**Updated**: 2025-04-17T15:26:04Z

**Summary**: Long-context autoregressive modeling has significantly advanced language generation, but video generation still struggles to fully utilize extended temporal contexts. To investigate long-context video modeling, we introduce Frame AutoRegressive (FAR), a strong baseline for video autoregressive modeling. Just as language models learn causal dependencies between tokens (i.e., Token AR), FAR models temporal causal dependencies between continuous frames, achieving better convergence than Token AR and video diffusion transformers. Building on FAR, we observe that long-context video modeling faces challenges due to visual redundancy. Training on long videos is computationally expensive, as vision tokens grow much faster than language tokens. To tackle this issue, we propose balancing locality and long-range dependency through long short-term context modeling. A high-resolution short-term context window ensures fine-grained temporal consistency, while an unlimited long-term context window encodes long-range information using fewer tokens. With this approach, we can train on long video sequences with a manageable token context length, thereby significantly reducing training time and memory usage. Furthermore, we propose a multi-level KV cache designed to support the long short-term context modeling, which accelerating inference on long video sequences. We demonstrate that FAR achieves state-of-the-art performance in both short- and long-video generation, providing a simple yet effective baseline for video autoregressive modeling. The code is released at https://github.com/showlab/FAR.

**Link**: [arxiv](http://arxiv.org/abs/2503.19325v2),  [pdf](http://arxiv.org/pdf/2503.19325v2)

**Tags**: cs.CV 



### In-context KV-Cache Eviction for LLMs via Attention-Gate
**Authors**: Zihao Zeng, Bokai Lin, Tianqi Hou, Hao Zhang, Zhijie Deng

**Updated**: 2025-04-17T03:51:06Z

**Summary**: The KV-Cache technique has become the standard for the inference of large language models (LLMs). Yet, it is widely criticized that KV-Cache can become a bottleneck of the LLM inference system. This paper enables a novel dynamic KV-Cache eviction policy by injecting a lightweight module called Attention-Gate to the model. It accepts the global context as input and yields eviction flags for each token. The self-attention modules in the model proceed according to the flags and cache only a subset of the KV states for next token prediction. The Attention-Gates can yield various flags for different heads and layers and be easily tuned on top of a pre-trained LLM via continual pre-training or supervised fine-tuning. The computational and memory overhead introduced by Attention-Gates can be minimal. We empirically evaluate the proposed approach across multiple scenarios, showing that effective eviction of redundant tokens can not only improve efficiency but also enhance performance.

**Link**: [arxiv](http://arxiv.org/abs/2410.12876v3),  [pdf](http://arxiv.org/pdf/2410.12876v3)

**Tags**: cs.CL cs.LG 



### Demoting Security via Exploitation of Cache Demote Operation in Intel's   Latest ISA Extension
**Authors**: Taehun Kim, Hyerean Jang, Youngjoo Shin

**Updated**: 2025-04-17T00:38:24Z

**Summary**: ISA extensions are increasingly adopted to boost the performance of specialized workloads without requiring an entire architectural redesign. However, these enhancements can inadvertently expose new attack surfaces in the microarchitecture. In this paper, we investigate Intel's recently introduced cldemote extension, which promotes efficient data sharing by transferring cache lines from upper-level caches to the Last Level Cache (LLC). Despite its performance benefits, we uncover critical properties-unprivileged access, inter-cache state transition, and fault suppression-that render cldemote exploitable for microarchitectural attacks. We propose two new attack primitives, Flush+Demote and Demote+Time, built on our analysis. Flush+Demote constructs a covert channel with a bandwidth of 2.84 Mbps and a bit error rate of 0.018%, while Demote+Time derandomizes the kernel base address in 2.49 ms on Linux. Furthermore, we show that leveraging cldemote accelerates eviction set construction in non-inclusive LLC designs by obviating the need for helper threads or extensive cache conflicts, thereby reducing construction time by 36% yet retaining comparable success rates. Finally, we examine how ISA extensions contribute to broader microarchitectural attacks, identifying five key exploitable characteristics and categorizing four distinct attack types. We also discuss potential countermeasures, highlighting the far-reaching security implications of emerging ISA extensions.

**Link**: [arxiv](http://arxiv.org/abs/2503.10074v2),  [pdf](http://arxiv.org/pdf/2503.10074v2)

**Tags**: cs.CR 



### MOM: Memory-Efficient Offloaded Mini-Sequence Inference for Long Context   Language Models
**Authors**: Junyang Zhang, Tianyi Zhu, Cheng Luo, Anima Anandkumar

**Updated**: 2025-04-16T23:15:09Z

**Summary**: Long-context language models exhibit impressive performance but remain challenging to deploy due to high GPU memory demands during inference. We propose Memory-efficient Offloaded Mini-sequence Inference (MOM), a method that partitions critical layers into smaller "mini-sequences" and integrates seamlessly with KV cache offloading. Experiments on various Llama, Qwen, and Mistral models demonstrate that MOM reduces peak memory usage by over 50\% on average. On Meta-Llama-3.2-8B, MOM extends the maximum context length from 155k to 455k tokens on a single A100 80GB GPU, while keeping outputs identical and not compromising accuracy. MOM also maintains highly competitive throughput due to minimal computational overhead and efficient last-layer processing. Compared to traditional chunked prefill methods, MOM achieves a 35\% greater context length extension. More importantly, our method drastically reduces prefill memory consumption, eliminating it as the longstanding dominant memory bottleneck during inference. This breakthrough fundamentally changes research priorities, redirecting future efforts from prefill-stage optimizations to improving decode-stage residual KV cache efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2504.12526v1),  [pdf](http://arxiv.org/pdf/2504.12526v1)

**Tags**: cs.LG cs.AI cs.CL 



### Activated LoRA: Fine-tuned LLMs for Intrinsics
**Authors**: Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox

**Updated**: 2025-04-16T18:03:21Z

**Summary**: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is highly inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. highly specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We use aLoRA to train a set of intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits.

**Link**: [arxiv](http://arxiv.org/abs/2504.12397v1),  [pdf](http://arxiv.org/pdf/2504.12397v1)

**Tags**: cs.LG cs.AI 



### Cobra: Efficient Line Art COlorization with BRoAder References
**Authors**: Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan

**Updated**: 2025-04-16T16:45:19Z

**Summary**: The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.

**Link**: [arxiv](http://arxiv.org/abs/2504.12240v1),  [pdf](http://arxiv.org/pdf/2504.12240v1)

**Tags**: cs.CV 



### Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache   Offloading
**Authors**: Kihyun Kim, Jinwoo Kim, Hyunsun Chung, Myung-Hoon Cha, Hong-Yeon Kim, Youngjae Kim

**Updated**: 2025-04-16T07:02:38Z

**Summary**: LLM inference is essential for applications like text summarization, translation, and data analysis, but the high cost of GPU instances from Cloud Service Providers (CSPs) like AWS is a major burden. This paper proposes InferSave, a cost-efficient VM selection framework for cloud based LLM inference. InferSave optimizes KV cache offloading based on Service Level Objectives (SLOs) and workload charac teristics, estimating GPU memory needs, and recommending cost-effective VM instances. Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. Experiments on AWS GPU instances show that selecting lower-cost instances without KV cache offloading improves cost efficiency by up to 73.7% for online workloads, while KV cache offloading saves up to 20.19% for offline workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.11816v1),  [pdf](http://arxiv.org/pdf/2504.11816v1)

**Tags**: cs.LG cs.DC 



### Efficient Architecture for RISC-V Vector Memory Access
**Authors**: Hongyi Guan, Yichuan Gao, Chenlu Miao, Haoyang Wu, Hang Zhu, Mingfeng Lin, Huayue Liang

**Updated**: 2025-04-16T05:57:08Z

**Summary**: Vector processors frequently suffer from inefficient memory accesses, particularly for strided and segment patterns. While coalescing strided accesses is a natural solution, effectively gathering or scattering elements at fixed strides remains challenging. Naive approaches rely on high-overhead crossbars that remap any byte between memory and registers, leading to physical design issues. Segment operations require row-column transpositions, typically handled using either element-level in-place transposition (degrading performance) or large buffer-based bulk transposition (incurring high area overhead). In this paper, we present EARTH, a novel vector memory access architecture designed to overcome these challenges through shifting-based optimizations. For strided accesses, EARTH integrates specialized shift networks for gathering and scattering elements. After coalescing multiple accesses within the same cache line, data is routed between memory and registers through the shifting network with minimal overhead. For segment operations, EARTH employs a shifted register bank enabling direct column-wise access, eliminating dedicated segment buffers while providing high-performance, in-place bulk transposition. Implemented on FPGA with Chisel HDL based on an open-source RISC-V vector unit, EARTH enhances performance for strided memory accesses, achieving 4x-8x speedups in benchmarks dominated by strided operations. Compared to conventional designs, EARTH reduces hardware area by 9% and power consumption by 41%, significantly advancing both performance and efficiency of vector processors.

**Link**: [arxiv](http://arxiv.org/abs/2504.08334v3),  [pdf](http://arxiv.org/pdf/2504.08334v3)

**Tags**: cs.AR cs.DC 



### Shared Disk KV Cache Management for Efficient Multi-Instance Inference   in RAG-Powered LLMs
**Authors**: Hyungwoo Lee, Kihyun Kim, Jinwoo Kim, Jungmin So, Myung-Hoon Cha, Hong-Yeon Kim, James J. Kim, Youngjae Kim

**Updated**: 2025-04-16T04:59:18Z

**Summary**: Recent large language models (LLMs) face increasing inference latency as input context length and model size continue to grow. In particular, the retrieval-augmented generation (RAG) technique, which enhances LLM responses by incorporating external knowledge, exacerbates this issue by significantly increasing the number of input tokens. This expansion in token length leads to a substantial rise in computational overhead, particularly during the prefill stage, resulting in prolonged time-to-first-token (TTFT). To address this issue, this paper proposes a method to reduce TTFT by leveraging a disk-based key-value (KV) cache to lessen the computational burden during the prefill stage. We also introduce a disk-based shared KV cache management system, called Shared RAG-DCache, for multi-instance LLM RAG service environments. This system, together with an optimal system configuration, improves both throughput and latency under given resource constraints. Shared RAG-DCache exploits the locality of documents related to user queries in RAG, as well as the queueing delay in LLM inference services. It proactively generates and stores disk KV caches for query-related documents and shares them across multiple LLM instances to enhance inference performance. In experiments on a single host equipped with 2 GPUs and 1 CPU, Shared RAG-DCache achieved a 15~71% increase in throughput and up to a 12~65% reduction in latency, depending on the resource configuration.

**Link**: [arxiv](http://arxiv.org/abs/2504.11765v1),  [pdf](http://arxiv.org/pdf/2504.11765v1)

**Tags**: cs.AI 



### EdgePrompt: A Distributed Key-Value Inference Framework for LLMs in 6G   Networks
**Authors**: Jiahong Ning, Pengyan Zhu, Ce Zheng, Gary Lee, Sumei Sun, Tingting Yang

**Updated**: 2025-04-16T03:07:07Z

**Summary**: As sixth-generation (6G) networks advance, large language models (LLMs) are increasingly integrated into 6G infrastructure to enhance network management and intelligence. However, traditional LLMs architecture struggle to meet the stringent latency and security requirements of 6G, especially as the increasing in sequence length leads to greater task complexity. This paper proposes Edge-Prompt, a cloud-edge collaborative framework based on a hierarchical attention splicing mechanism. EdgePrompt employs distributed key-value (KV) pair optimization techniques to accelerate inference and adapt to network conditions. Additionally, to reduce the risk of data leakage, EdgePrompt incorporates a privacy preserving strategy by isolating sensitive information during processing. Experiments on public dataset show that EdgePrompt effectively improves the inference throughput and reduces the latency, which provides a reliable solution for LLMs deployment in 6G environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.11729v1),  [pdf](http://arxiv.org/pdf/2504.11729v1)

**Tags**: eess.SP 



### Engineering MultiQueues: Fast Relaxed Concurrent Priority Queues
**Authors**: Marvin Williams, Peter Sanders

**Updated**: 2025-04-15T22:38:54Z

**Summary**: Priority queues are used in a wide range of applications, including prioritized online scheduling, discrete event simulation, and greedy algorithms. In parallel settings, classical priority queues often become a severe bottleneck, resulting in low throughput. Consequently, there has been significant interest in concurrent priority queues with relaxed semantics. In this article, we present the MultiQueue, a flexible approach to relaxed priority queues that uses multiple internal sequential priority queues. The scalability of the MultiQueue is enhanced by buffering elements, batching operations on the internal queues, and optimizing access patterns for high cache locality. We investigate the complementary quality criteria of rank error, which measures how close deleted elements are to the global minimum, and delay, which quantifies how many smaller elements were deleted before a given element. Extensive experimental evaluation shows that the MultiQueue outperforms competing approaches across several benchmarks. This includes shortest-path and branch-and-bound benchmarks that resemble real applications. Moreover, the MultiQueue can be configured easily to balance throughput and quality according to the application's requirements. We employ a seemingly paradoxical technique of wait-free locking that might be of broader interest for converting sequential data structures into relaxed concurrent data structures.

**Link**: [arxiv](http://arxiv.org/abs/2504.11652v1),  [pdf](http://arxiv.org/pdf/2504.11652v1)

**Tags**: cs.DS cs.DC cs.PF 



### Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory   Constraints
**Authors**: Ruicheng Ao, Gan Luo, David Simchi-Levi, Xinshang Wang

**Updated**: 2025-04-15T16:00:21Z

**Summary**: Large Language Models (LLMs) are indispensable in today's applications, but their inference procedure -- generating responses by processing text in segments and using a memory-heavy Key-Value (KV) cache -- demands significant computational resources, particularly under memory constraints. This paper formulates LLM inference optimization as a multi-stage online scheduling problem where sequential prompt arrivals and KV cache growth render conventional scheduling ineffective. We develop a fluid dynamics approximation to provide a tractable benchmark that guides algorithm design. Building on this, we propose the Waiting for Accumulated Inference Threshold (WAIT) algorithm, which uses multiple thresholds to schedule incoming prompts optimally when output lengths are known, and extend it to Nested WAIT for cases with unknown output lengths. Theoretical analysis shows that both algorithms achieve near-optimal performance against the fluid benchmark in heavy traffic conditions, balancing throughput, latency, and Time to First Token (TTFT). Experiments with the Llama-7B model on an A100 GPU using both synthetic and real-world datasets demonstrate improved throughput and reduced latency relative to established baselines like vLLM and Sarathi. This work bridges operations research and machine learning, offering a rigorous framework for the efficient deployment of LLMs under memory constraints.

**Link**: [arxiv](http://arxiv.org/abs/2504.11320v1),  [pdf](http://arxiv.org/pdf/2504.11320v1)

**Tags**: cs.LG cs.AI cs.DC math.OC stat.ML 



### Performant Automatic BLAS Offloading on Unified Memory Architecture with   OpenMP First-Touch Style Data Movement
**Authors**: Junjie Li

**Updated**: 2025-04-15T15:40:25Z

**Summary**: BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.

**Link**: [arxiv](http://arxiv.org/abs/2501.00279v3),  [pdf](http://arxiv.org/pdf/2501.00279v3)

**Tags**: cs.DC cs.MS cs.PF cs.SE 



### Automatic BLAS Offloading on Unified Memory Architecture: A Study on   NVIDIA Grace-Hopper
**Authors**: Junjie Li, Yinzhi Wang, Xiao Liang, Hang Liu

**Updated**: 2025-04-15T15:37:58Z

**Summary**: Porting codes to GPU often requires major efforts. While several tools exist for automatically offload numerical libraries such as BLAS and LAPACK, they often prove impractical due to the high cost of mandatory data transfer. The new unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth cache-coherent memory access of all memory from both CPU and GPU, potentially eliminating bottleneck faced in conventional architecture. This breakthrough opens up new avenues for application development and porting strategies. In this study, we introduce a new tool for automatic BLAS offload, the tool leverages the high speed cache coherent NVLink C2C interconnect in Grace-Hopper, and enables performant GPU offload for BLAS heavy applications with no code changes or recompilation. The tool was tested on two quantum chemistry or physics codes, great performance benefits were observed.

**Link**: [arxiv](http://arxiv.org/abs/2404.13195v5),  [pdf](http://arxiv.org/pdf/2404.13195v5)

**Tags**: cs.DC 



### Morphing-based Compression for Data-centric ML Pipelines
**Authors**: Sebastian Baunsgaard, Matthias Boehm

**Updated**: 2025-04-15T11:02:34Z

**Summary**: Data-centric ML pipelines extend traditional machine learning (ML) pipelines -- of feature transformations and ML model training -- by outer loops for data cleaning, augmentation, and feature engineering to create high-quality input data. Existing lossless matrix compression applies lightweight compression schemes to numeric matrices and performs linear algebra operations such as matrix-vector multiplications directly on the compressed representation but struggles to efficiently rediscover structural data redundancy. Compressed operations are effective at fitting data in available memory, reducing I/O across the storage-memory-cache hierarchy, and improving instruction parallelism. The applied data cleaning, augmentation, and feature transformations provide a rich source of information about data characteristics such as distinct items, column sparsity, and column correlations. In this paper, we introduce BWARE -- an extension of AWARE for workload-aware lossless matrix compression -- that pushes compression through feature transformations and engineering to leverage information about structural transformations. Besides compressed feature transformations, we introduce a novel technique for lightweight morphing of a compressed representation into workload-optimized compressed representations without decompression. BWARE shows substantial end-to-end runtime improvements, reducing the execution time for training data-centric ML pipelines from days to hours.

**Link**: [arxiv](http://arxiv.org/abs/2504.11067v1),  [pdf](http://arxiv.org/pdf/2504.11067v1)

**Tags**: cs.DB cs.DC cs.LG 



### AlayaDB: The Data Foundation for Efficient and Effective Long-context   LLM Inference
**Authors**: Yangshen Deng, Zhengxin You, Long Xiang, Qilong Li, Peiqi Yuan, Zhaoyang Hong, Yitao Zheng, Wanting Li, Runzhong Li, Haotian Liu, Kyriakos Mouratidis, Man Lung Yiu, Huan Li, Qiaomu Shen, Rui Mao, Bo Tang

**Updated**: 2025-04-14T15:34:26Z

**Summary**: AlayaDB is a cutting-edge vector database system natively architected for efficient and effective long-context inference for Large Language Models (LLMs) at AlayaDB AI. Specifically, it decouples the KV cache and attention computation from the LLM inference systems, and encapsulates them into a novel vector database system. For the Model as a Service providers (MaaS), AlayaDB consumes fewer hardware resources and offers higher generation quality for various workloads with different kinds of Service Level Objectives (SLOs), when comparing with the existing alternative solutions (e.g., KV cache disaggregation, retrieval-based sparse attention). The crux of AlayaDB is that it abstracts the attention computation and cache management for LLM inference into a query processing procedure, and optimizes the performance via a native query optimizer. In this work, we demonstrate the effectiveness of AlayaDB via (i) three use cases from our industry partners, and (ii) extensive experimental results on LLM inference benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.10326v1),  [pdf](http://arxiv.org/pdf/2504.10326v1)

**Tags**: cs.AI cs.DB cs.IR H.3.1; H.3.2; H.3.3; H.3.4 



### Shield Bash: Abusing Defensive Coherence State Retrieval to Break Timing   Obfuscation
**Authors**: Kartik Ramkrishnan, Antonia Zhai, Stephen McCamant, Pen Chung Yew

**Updated**: 2025-04-14T15:27:32Z

**Summary**: Microarchitectural attacks are a significant concern, leading to many hardware-based defense proposals. However, different defenses target different classes of attacks, and their impact on each other has not been fully considered. To raise awareness of this problem, we study an interaction between two state-of-the art defenses in this paper, timing obfuscations of remote cache lines (TORC) and delaying speculative changes to remote cache lines (DSRC). TORC mitigates cache-hit based attacks and DSRC mitigates speculative coherence state change attacks.   We observe that DSRC enables coherence information to be retrieved into the processor core, where it is out of the reach of timing obfuscations to protect. This creates an unforeseen consequence that redo operations can be triggered within the core to detect the presence or absence of remote cache lines, which constitutes a security vulnerability. We demonstrate that a new covert channel attack is possible using this vulnerability. We propose two ways to mitigate the attack, whose performance varies depending on an application's cache usage. One way is to never send remote exclusive coherence state (E) information to the core even if it is created. The other way is to never create a remote E state, which is responsible for triggering redos.   We demonstrate the timing difference caused by this microarchitectural defense assumption violation using GEM5 simulations. Performance evaluation on SPECrate 2017 and PARSEC benchmarks of the two fixes show less than 32\% average overhead across both sets of benchmarks. The repair which prevented the creation of remote E state had less than 2.8% average overhead.

**Link**: [arxiv](http://arxiv.org/abs/2504.10318v1),  [pdf](http://arxiv.org/pdf/2504.10318v1)

**Tags**: cs.CR cs.AR 



### A New Paradigm in IBR Modeling for Power Flow and Short Circuit Analysis
**Authors**: Zahid Javid, Firdous Ul Nazir, Wentao Zhu, Diptargha Chakravorty, Ahmed Aboushady, Mohamed Galeela

**Updated**: 2025-04-14T12:34:20Z

**Summary**: The fault characteristics of inverter-based resources (IBRs) are different from conventional synchronous generators. The fault response of IBRs is non-linear due to saturation states and mainly determined by fault ride through (FRT) strategies of the associated voltage source converter (VSC). This results in prohibitively large solution times for power flows considering these short circuit characteristics, especially when the power system states change fast due to uncertainty in IBR generations. To overcome this, a phasor-domain steady state (SS) short circuit (SC) solver for IBR dominated power systems is proposed in this paper, and subsequently the developed IBR models are incorporated with a novel Jacobian-based Power Flow (PF) solver. In this multiphase PF solver, any power system components can be modeled by considering their original non-linear or linear mathematical representations. Moreover, two novel FRT strategies are proposed to fully utilize the converter capacity and to comply with IEEE-2800 2022 std and German grid code. The results are compared with the Electromagnetic Transient (EMT) simulation on the IEEE 34 test network and the 120 kV EPRI benchmark system. The developed IBR sequence domain PF model demonstrates more accurate behavior compared to the classical IBR generator model. The error in calculating the short circuit current with the proposed SC solver is less than 3%, while achieving significant speed improvements of three order of magnitudes.

**Link**: [arxiv](http://arxiv.org/abs/2504.10181v1),  [pdf](http://arxiv.org/pdf/2504.10181v1)

**Tags**: eess.SY cs.SY 



### Diversity in Network-Friendly Recommendations
**Authors**: Evangelia Tzimpimpaki, Thrasyvoulos Spyropoulos

**Updated**: 2025-04-14T11:20:56Z

**Summary**: In recent years, the Internet has been dominated by content-rich platforms, employing recommendation systems to provide users with more appealing content (e.g., videos in YouTube, movies in Netflix). While traditional content recommendations are oblivious to network conditions, the paradigm of Network-Friendly Recommendations (NFR) has recently emerged, favoring content that improves network performance (e.g. cached near the user), while still being appealing to the user. However, NFR algorithms sometimes achieve their goal by shrinking the pool of content recommended to users. The undesirable side-effect is reduced content diversity, a phenomenon known as ``content/filter bubble''. This reduced diversity is problematic for both users, who are prevented from exploring a broader range of content, and content creators (e.g. YouTubers) whose content may be recommended less frequently, leading to perceived unfairness. In this paper, we first investigate - using real data and state-of-the-art NFR schemes - the extent of this phenomenon. We then formulate a ``Diverse-NFR'' optimization problem (i.e., network-friendly recommendations with - sufficient - content diversity), and through a series of transformation steps, we manage to reduce it to a linear program that can be solved fast and optimally. Our findings show that Diverse-NFR can achieve high network gains (comparable to non-diverse NFR) while maintaining diversity constraints. To our best knowledge, this is the first work that incorporates diversity issues into network-friendly recommendation algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2411.00601v3),  [pdf](http://arxiv.org/pdf/2411.00601v3)

**Tags**: cs.PF 



### On Precomputation and Caching in Information Retrieval Experiments with   Pipeline Architectures
**Authors**: Sean MacAvaney, Craig Macdonald

**Updated**: 2025-04-14T08:51:35Z

**Summary**: Modern information retrieval systems often rely on multiple components executed in a pipeline. In a research setting, this can lead to substantial redundant computations (e.g., retrieving the same query multiple times for evaluating different downstream rerankers). To overcome this, researchers take cached "result" files as inputs, which represent the output of another pipeline. However, these result files can be brittle and can cause a disconnect between the conceptual design of the pipeline and its logical implementation. To overcome both the redundancy problem (when executing complete pipelines) and the disconnect problem (when relying on intermediate result files), we describe our recent efforts to improve the caching capabilities in the open-source PyTerrier IR platform. We focus on two main directions: (1) automatic implicit caching of common pipeline prefixes when comparing systems and (2) explicit caching of operations through a new extension package, pyterrier-caching. These approaches allow for the best of both worlds: pipelines can be fully expressed end-to-end, while also avoiding redundant computations between pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2504.09984v1),  [pdf](http://arxiv.org/pdf/2504.09984v1)

**Tags**: cs.IR 



### Secrecy and Privacy in Multi-Access Combinatorial Topology
**Authors**: Mallikharjuna Chinnapadamala, B. Sundar Rajan

**Updated**: 2025-04-14T07:30:03Z

**Summary**: In this work, we consider the multi-access combinatorial topology with $C$ caches where each user accesses a unique set of $r$ caches. For this setup, we consider secrecy, where each user should not know anything about the files it did not request, and demand privacy, where each user's demand must be kept private from other non-colluding users. We propose a scheme satisfying both conditions and derive a lower bound based on cut-set arguments. Also, we prove that our scheme is optimal when $r\geq C-1$, and it is order-optimal when the cache memory size $M$ is greater than or equal to a certain threshold for $r<C-1$. When $r=1$, in most of the memory region, our scheme achieves the same rate as the one given by the secretive scheme for the dedicated cache setup by Ravindrakumar et al. ( 'Private Coded Caching,' in \textit{IEEE Transactions on Information Forensics and Security}, 2018), while satisfying both secrecy and demand privacy conditions.

**Link**: [arxiv](http://arxiv.org/abs/2504.09952v1),  [pdf](http://arxiv.org/pdf/2504.09952v1)

**Tags**: cs.IT math.IT 



### KeepKV: Eliminating Output Perturbation in KV Cache Compression for   Efficient LLMs Inference
**Authors**: Yuxuan Tian, Zihan Wang, Yebo Peng, Aomufei Yuan, Zhiming Wang, Bairen Yi, Xin Liu, Yong Cui, Tong Yang

**Updated**: 2025-04-14T06:58:00Z

**Summary**: Efficient inference of large language models (LLMs) is hindered by an ever-growing key-value (KV) cache, making KV cache compression a critical research direction. Traditional methods selectively evict less important KV cache entries based on attention scores or position heuristics, which leads to information loss and hallucinations. Recently, merging-based strategies have been explored to retain more information by merging KV pairs that would be discarded; however, these existing approaches inevitably introduce inconsistencies in attention distributions before and after merging, causing output perturbation and degraded generation quality. To overcome this challenge, we propose KeepKV, a novel adaptive KV cache merging method designed to eliminate output perturbation while preserving performance under strict memory constraints. KeepKV introduces the Electoral Votes mechanism that records merging history and adaptively adjusts attention scores. Moreover, it further leverages a novel Zero Inference-Perturbation Merging methods, keeping attention consistency and compensating for attention loss resulting from cache merging. KeepKV successfully retains essential context information within a significantly compressed cache. Extensive experiments on various benchmarks and LLM architectures demonstrate that KeepKV substantially reduces memory usage, enhances inference throughput by more than 2x and keeps superior generation quality even with 10% KV cache budgets.

**Link**: [arxiv](http://arxiv.org/abs/2504.09936v1),  [pdf](http://arxiv.org/pdf/2504.09936v1)

**Tags**: cs.LG cs.AI cs.CL 



### Plato: Plan to Efficiently Decode for Large Language Model Inference
**Authors**: Shuowei Jin, Xueshen Liu, Yongji Wu, Haizhong Zheng, Qingzhao Zhang, Atul Prakash, Matthew Lentz, Danyang Zhuo, Feng Qian, Z. Morley Mao

**Updated**: 2025-04-13T14:17:57Z

**Summary**: Large language models (LLMs) have achieved remarkable success in natural language tasks, but their inference incurs substantial computational and memory overhead. To improve efficiency, parallel decoding methods like Skeleton-of-Thought (SoT) decompose prompts into sub-problems for concurrent processing. However, these methods significantly compromise answer quality by treating semantically linked sub-problems as independent. We propose Plato, a novel approach that co-designs algorithms and systems for semantic-aware parallel decoding. Plato leverages LLMs to organize sub-problems into a dependency graph based on logical and causal relationships, enabling concurrent decoding of non-dependent nodes while preserving answer coherence and quality. To further enhance efficiency, Plato pipelines planning and node decoding stages, implements a global context cache, and carefully structures node inference prompts to maximize key-value cache reuse and minimize overhead. Our evaluations show that Plato improves throughput by 68% over autoregressive decoding while achieving a 40% net win rate in answer quality. Compared to SoT, Plato demonstrates a remarkable 90% quality net-win rate. Ablation studies reveal that our pipeline design improves speedup by 29%, while our KV cache reuse optimization reduces overhead by 75%.

**Link**: [arxiv](http://arxiv.org/abs/2402.12280v2),  [pdf](http://arxiv.org/pdf/2402.12280v2)

**Tags**: cs.CL cs.AI 



### Efficient LLM Serving on Hybrid Real-time and Best-effort Requests
**Authors**: Wan Borui, Zhao Juntao, Jiang Chenyu, Guo Chuanxiong, Wu Chuan

**Updated**: 2025-04-13T14:16:57Z

**Summary**: Recent breakthroughs in large Language Models (LLMs) have enabled various generative tasks on a single model. Real-world services (e.g., OpenAI's ChatGPT [27]) powered by an LLM often concurrently support latency-critical requests for interactive applications (e.g., question-answering systems, referred to as real-time or RT requests) and throughput-oriented requests for back-of-house processing (e.g., documents batch processing [28], referred to best-effort or BE requests), with complex hybrid inference workloads to the underlying model. State-of-the-art (SOTA) LLM serving systems dedicate machines to each type of request, towards either low inference latency or high serving throughput, respectively. This practice simplifies request scheduling and management but suffers from poor resource utilization. We propose BROS, a hybrid LLM serving system that aims to collocate RT/BE requests, meeting RT requests' latency requirements while maintaining BE requests' throughput. BROS formulates the problem of hybrid RT/BE request scheduling and solves it with a dynamic priority-based algorithm. BROS designs a bidirectional KV cache management mechanism, allowing RT requests to share KV memory with BE requests to remove the scheduling restrictions caused by insufficient KV memory and improve utilization. Extensive experiments validate that BROS achieves a good trade-off when serving hybrid RT and BE requests. It significantly reduces the latency of RT requests (up to 74.20%), improving their fine-grained service level objectives (SLOs) attainments (up to 36.38x), with negligible throughput reduction for BE requests, showing significant advantages over SOTA systems like vLLM and TGI.

**Link**: [arxiv](http://arxiv.org/abs/2504.09590v1),  [pdf](http://arxiv.org/pdf/2504.09590v1)

**Tags**: cs.AI 



### Block-Attention for Efficient Prefilling
**Authors**: Dongyang Ma, Yan Wang, Lan Tian

**Updated**: 2025-04-13T14:02:47Z

**Summary**: We introduce Block-attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. Traditional approaches often encode the entire context in an auto-regressive manner. Instead, Block-attention divides retrieved documents into discrete blocks, with each block independently calculating key-value (KV) states except for the final block. In RAG scenarios, by defining each passage as a block, Block-attention enables us to reuse the KV states of passages that have been seen before, thereby significantly reducing the latency and the computation overhead during inference. The implementation of Block-attention involves block segmentation, position re-encoding, and fine-tuning the LLM to adapt to the Block-attention mechanism. Experiments on 11 diverse benchmarks, including RAG, ICL, and general domains, demonstrate that after block fine-tuning, the Block-attention model not only achieves performance comparable to that of full-attention models, but can also seamlessly switch between the block and full attention modes without any performance loss. Notably, Block-attention significantly reduces the time to first token (TTFT) and floating point operations (FLOPs) to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared to the full-attention models, the TTFT and corresponding FLOPs are reduced by 98.7% and 99.8%, respectively. Additionally, in Appendix A, we elaborate on how Block-attention is applied in Game AI scenario and the substantial potential benefits it entails. We strongly suggest researchers in the gaming field not to overlook this section.

**Link**: [arxiv](http://arxiv.org/abs/2409.15355v5),  [pdf](http://arxiv.org/pdf/2409.15355v5)

**Tags**: cs.LG cs.AI cs.CL 



### AB-Cache: Training-Free Acceleration of Diffusion Models via   Adams-Bashforth Cached Feature Reuse
**Authors**: Zichao Yu, Zhen Zou, Guojiang Shao, Chengwei Zhang, Shengze Xu, Jie Huang, Feng Zhao, Xiaodong Cun, Wenyi Zhang

**Updated**: 2025-04-13T08:29:58Z

**Summary**: Diffusion models have demonstrated remarkable success in generative tasks, yet their iterative denoising process results in slow inference, limiting their practicality. While existing acceleration methods exploit the well-known U-shaped similarity pattern between adjacent steps through caching mechanisms, they lack theoretical foundation and rely on simplistic computation reuse, often leading to performance degradation. In this work, we provide a theoretical understanding by analyzing the denoising process through the second-order Adams-Bashforth method, revealing a linear relationship between the outputs of consecutive steps. This analysis explains why the outputs of adjacent steps exhibit a U-shaped pattern. Furthermore, extending Adams-Bashforth method to higher order, we propose a novel caching-based acceleration approach for diffusion models, instead of directly reusing cached results, with a truncation error bound of only \(O(h^k)\) where $h$ is the step size. Extensive validation across diverse image and video diffusion models (including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates our method's effectiveness in achieving nearly $3\times$ speedup while maintaining original performance levels, offering a practical real-time solution without compromising generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2504.10540v1),  [pdf](http://arxiv.org/pdf/2504.10540v1)

**Tags**: stat.ML cs.AI cs.LG 



### Sub-nanosecond in-plane magnetization switching induced by field-like   spin-orbit torques from ferromagnets
**Authors**: Hanying Zhang, Ziqian Cui, Baiqing Jiang, Yuan Wang, C. Bi

**Updated**: 2025-04-13T04:46:02Z

**Summary**: Spin-orbit torques (SOTs) generated in SOT-material/ferromagnet structures are classified as damping-like (DL) and field-like (FL) torques for current-driven magnetization switching. It is well known that both DL- and FL-SOTs originate from the SOT-material and DL-SOT dominates the current-driven switching process while FL-SOT contributes limitedly, resulting in an incubation time (several nanoseconds) during collinear magnetization switching with the spin polarization because of the DL attributes. Here we report a FL-SOT originated from the ferromagnet, different from the origin of DL-SOT, and demonstrate that it dominates the collinear magnetization switching. We show that the FL-SOT and resultant collinear switching can be modulated, one order of magnitude and sign reversal, by controlling the ferromagnet. Because of no incubation time and higher charge-to-spin efficiencies in the FL switching, we further show that the switching time can be down to 200 ps with one order lower critical switching current density compared to DL switching. These results indicate that the FL switching may provide a practical solution for magnetic memory in speed-priority cache applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.09431v1),  [pdf](http://arxiv.org/pdf/2504.09431v1)

**Tags**: physics.app-ph 



### Head-Aware KV Cache Compression for Efficient Visual Autoregressive   Modeling
**Authors**: Ziran Qin, Youru Lv, Mingbao Lin, Zeren Zhang, Danping Zou, Weiyao Lin

**Updated**: 2025-04-12T15:42:17Z

**Summary**: Visual Autoregressive (VAR) models have emerged as a powerful approach for multi-modal content creation, offering high efficiency and quality across diverse multimedia applications. However, they face significant memory bottlenecks due to extensive KV cache accumulation during inference. Existing KV cache compression techniques for large language models are suboptimal for VAR models due to, as we identify in this paper, two distinct categories of attention heads in VAR models: Structural Heads, which preserve spatial coherence through diagonal attention patterns, and Contextual Heads, which maintain semantic consistency through vertical attention patterns. These differences render single-strategy KV compression techniques ineffective for VAR models. To address this, we propose HACK, a training-free Head-Aware Compression method for KV cache. HACK allocates asymmetric cache budgets and employs pattern-specific compression strategies tailored to the essential characteristics of each head category. Experiments on Infinity-2B, Infinity-8B, and VAR-d30 demonstrate its effectiveness in text-to-image and class-conditional generation tasks. HACK can hack down up to 50\% and 70\% of cache with minimal performance degradation for VAR-d30 and Infinity-8B, respectively. Even with 70\% and 90\% KV cache compression in VAR-d30 and Infinity-8B, HACK still maintains high-quality generation while reducing memory usage by 44.2\% and 58.9\%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2504.09261v1),  [pdf](http://arxiv.org/pdf/2504.09261v1)

**Tags**: cs.CV 



### WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent   Video Diffusion Model
**Authors**: Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, Li Yuan

**Updated**: 2025-04-11T12:31:07Z

**Summary**: Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.

**Link**: [arxiv](http://arxiv.org/abs/2411.17459v3),  [pdf](http://arxiv.org/pdf/2411.17459v3)

**Tags**: cs.CV cs.AI 



### Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and   Flash
**Authors**: Fucheng Jia, Zewen Wu, Shiqi Jiang, Huiqiang Jiang, Qianxi Zhang, Yuqing Yang, Yunxin Liu, Ju Ren, Deyu Zhang, Ting Cao

**Updated**: 2025-04-11T09:26:47Z

**Summary**: Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.08378v1),  [pdf](http://arxiv.org/pdf/2504.08378v1)

**Tags**: cs.LG 



### II-NVM: Enhancing Map Accuracy and Consistency with Normal   Vector-Assisted Mapping
**Authors**: Chengwei Zhao, Yixuan Li, Yina Jian, Jie Xu, Linji Wang, Yongxin Ma, Xinglai Jin

**Updated**: 2025-04-11T02:10:02Z

**Summary**: SLAM technology plays a crucial role in indoor mapping and localization. A common challenge in indoor environments is the "double-sided mapping issue", where closely positioned walls, doors, and other surfaces are mistakenly identified as a single plane, significantly hindering map accuracy and consistency. To address this issue this paper introduces a SLAM approach that ensures accurate mapping using normal vector consistency. We enhance the voxel map structure to store both point cloud data and normal vector information, enabling the system to evaluate consistency during nearest neighbor searches and map updates. This process distinguishes between the front and back sides of surfaces, preventing incorrect point-to-plane constraints. Moreover, we implement an adaptive radius KD-tree search method that dynamically adjusts the search radius based on the local density of the point cloud, thereby enhancing the accuracy of normal vector calculations. To further improve realtime performance and storage efficiency, we incorporate a Least Recently Used (LRU) cache strategy, which facilitates efficient incremental updates of the voxel map. The code is released as open-source and validated in both simulated environments and real indoor scenarios. Experimental results demonstrate that this approach effectively resolves the "double-sided mapping issue" and significantly improves mapping precision. Additionally, we have developed and open-sourced the first simulation and real world dataset specifically tailored for the "double-sided mapping issue".

**Link**: [arxiv](http://arxiv.org/abs/2504.08204v1),  [pdf](http://arxiv.org/pdf/2504.08204v1)

**Tags**: cs.RO 



### Boosting Universal LLM Reward Design through Heuristic Reward   Observation Space Evolution
**Authors**: Zen Kit Heng, Zimeng Zhao, Tianhao Wu, Yuanfei Wang, Mingdong Wu, Yangang Wang, Hao Dong

**Updated**: 2025-04-11T02:05:01Z

**Summary**: Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward.

**Link**: [arxiv](http://arxiv.org/abs/2504.07596v2),  [pdf](http://arxiv.org/pdf/2504.07596v2)

**Tags**: cs.AI 



### Memory-efficient Streaming VideoLLMs for Real-time Procedural Video   Understanding
**Authors**: Dibyadip Chatterjee, Edoardo Remelli, Yale Song, Bugra Tekin, Abhay Mittal, Bharat Bhatnagar, Necati Cihan Camgöz, Shreyas Hampali, Eric Sauser, Shugao Ma, Angela Yao, Fadime Sener

**Updated**: 2025-04-10T17:13:08Z

**Summary**: We introduce ProVideLLM, an end-to-end framework for real-time procedural video understanding. ProVideLLM integrates a multimodal cache configured to store two types of tokens - verbalized text tokens, which provide compressed textual summaries of long-term observations, and visual tokens, encoded with DETR-QFormer to capture fine-grained details from short-term observations. This design reduces token count by 22x over existing methods in representing one hour of long-term observations while effectively encoding fine-granularity of the present. By interleaving these tokens in our multimodal cache, ProVideLLM ensures sub-linear scaling of memory and compute with video length, enabling per-frame streaming inference at 10 FPS and streaming dialogue at 25 FPS, with a minimal 2GB GPU memory footprint. ProVideLLM also sets new state-of-the-art results on six procedural tasks across four datasets.

**Link**: [arxiv](http://arxiv.org/abs/2504.13915v1),  [pdf](http://arxiv.org/pdf/2504.13915v1)

**Tags**: cs.CV 



### Siren Federate: Bridging document, relational, and graph models for   exploratory graph analysis
**Authors**: Georgeta Bordea, Stephane Campinas, Matteo Catena, Renaud Delbru

**Updated**: 2025-04-10T14:52:03Z

**Summary**: Investigative workflows require interactive exploratory analysis on large heterogeneous knowledge graphs. Current databases show limitations in enabling such task. This paper discusses the architecture of Siren Federate, a system that efficiently supports exploratory graph analysis by bridging document-oriented, relational and graph models. Technical contributions include distributed join algorithms, adaptive query planning, query plan folding, semantic caching, and semi-join decomposition for path query. Semi-join decomposition addresses the exponential growth of intermediate results in path-based queries. Experiments show that Siren Federate exhibits low latency and scales well with the amount of data, the number of users, and the number of computing nodes.

**Link**: [arxiv](http://arxiv.org/abs/2504.07815v1),  [pdf](http://arxiv.org/pdf/2504.07815v1)

**Tags**: cs.IR D.2.11; E.1; H.2.4; H.3.3; H.3.4 



### Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based   Program Analysis
**Authors**: Rustam Sadykov, Azat Abdullin, Marat Akhin

**Updated**: 2025-04-10T10:43:42Z

**Summary**: Satisfiability Modulo Theories (SMT) solvers are integral to program analysis techniques like concolic and symbolic execution, where they help assess the satisfiability of logical formulae to explore execution paths of the program under test. However, frequent solver invocations are still the main performance bottleneck of these techniques. One way to mitigate this challenge is through optimizations such as caching and reusing solver results. While current methods typically focus on reusing results from fully equivalent or closely related formulas, they often miss broader opportunities for reuse. In this paper, we propose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable (unsat) results by systematically considering all possible variable substitutions. This enables more extensive reuse of results, thereby reducing the number of SMT solver invocations and improving the overall efficiency of concolic and symbolic execution. Our evaluation, conducted against the state-of-the-art Utopia solution using two benchmark sets, shows significant improvements, particularly with more complex formulas. Our method achieves up to 74% unsat core reuse, compared to Utopia's 41%, and significant increase in the time savings. These results demonstrate that, despite the additional computational complexity, the broader reuse of unsat results significantly enhances performance, offering valuable advancements for formal verification and program analysis.

**Link**: [arxiv](http://arxiv.org/abs/2504.07642v1),  [pdf](http://arxiv.org/pdf/2504.07642v1)

**Tags**: cs.SE 



### Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM   Inference Serving
**Authors**: Shihong Gao, Xin Zhang, Yanyan Shen, Lei Chen

**Updated**: 2025-04-10T06:51:23Z

**Summary**: Large language model (LLM) inference serving systems are essential to various LLM-based applications. As demand for LLM services continues to grow, scaling these systems to handle high request rates while meeting latency Service-Level Objectives (SLOs), referred to as effective throughput, becomes critical. However, existing systems often struggle to improve effective throughput, primarily due to a significant decline in Time To First Token (TTFT) SLO attainment. We identify two major causes of this bottleneck: (1) memory-intensive KV cache that limits batch size expansion under GPU memory constraints, and (2) rigid batch composition enforced by the default First-Come-First-Serve scheduling policy. In this paper, we introduce Apt-Serve, a scalable framework designed to enhance effective throughput in LLM inference serving. Apt-Serve features a new hybrid cache scheme that combines KV cache with a memory-efficient hidden cache for reusable input hidden state vectors, allowing large batch sizes and improving request concurrency. Based on the hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism that dynamically optimizes batch composition. We formally define the adaptive scheduling optimization problem and propose an efficient algorithm with theoretical guarantees. Extensive evaluations on three real-world datasets and LLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up to 8.8x improvement in effective throughput compared to the state-of-the-art inference serving systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.07494v1),  [pdf](http://arxiv.org/pdf/2504.07494v1)

**Tags**: cs.LG 



### UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache   Pruning for Efficient Long-Context LLM Inference
**Authors**: Weikai Xu, Wenxuan Zeng, Qianqian Huang, Meng Li, Ru Huang

**Updated**: 2025-04-10T06:13:30Z

**Summary**: Transformer-based large language models (LLMs) have achieved impressive performance in various natural language processing (NLP) applications. However, the high memory and computation cost induced by the KV cache limits the inference efficiency, especially for long input sequences. Compute-in-memory (CIM)-based accelerators have been proposed for LLM acceleration with KV cache pruning. However, as existing accelerators only support static pruning with a fixed pattern or dynamic pruning with primitive implementations, they suffer from either high accuracy degradation or low efficiency. In this paper, we propose a ferroelectric FET (FeFET)-based unified content addressable memory (CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous support for static and dynamic pruning with 3 computation modes: 1) in the CAM mode, UniCAIM enables approximate similarity measurement in O(1) time for dynamic KV cache pruning with high energy efficiency; 2) in the charge-domain CIM mode, static pruning can be supported based on accumulative similarity score, which is much more flexible compared to fixed patterns; 3) in the current-domain mode, exact attention computation can be conducted with a subset of selected KV cache. We further propose a novel CAM/CIM cell design that leverages the multi-level characteristics of FeFETs for signed multibit storage of the KV cache and in-place attention computation. With extensive experimental results, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP) by 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit level, along with high accuracy comparable with dense attention at the application level, showing its great potential for efficient long-context LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2504.07479v1),  [pdf](http://arxiv.org/pdf/2504.07479v1)

**Tags**: cs.AR 



### Marconi: Prefix Caching for the Era of Hybrid LLMs
**Authors**: Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali

**Updated**: 2025-04-10T05:06:29Z

**Summary**: Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints. Across diverse workloads and Hybrid models, Marconi achieves up to 34.4$\times$ higher token hit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix caching systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.19379v3),  [pdf](http://arxiv.org/pdf/2411.19379v3)

**Tags**: cs.DC cs.AI cs.LG 



### MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix   Multiplication on CPUs
**Authors**: Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini

**Updated**: 2025-04-09T21:47:31Z

**Summary**: Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation in many applications. Current multithreaded implementations are based on Gustavson's algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach. The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures. MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold. We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation. An OpenMP implementation of MAGNUS is compared with several baselines, including Intel MKL, for a variety of different matrices on three Intel architectures. For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is often an order of magnitude faster than at least one baseline. For massive random matrices, MAGNUS scales to the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.

**Link**: [arxiv](http://arxiv.org/abs/2501.07056v2),  [pdf](http://arxiv.org/pdf/2501.07056v2)

**Tags**: cs.DC 



### Tensor Product Attention Is All You Need
**Authors**: Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao

**Updated**: 2025-04-09T20:51:08Z

**Summary**: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPA's memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.

**Link**: [arxiv](http://arxiv.org/abs/2501.06425v3),  [pdf](http://arxiv.org/pdf/2501.06425v3)

**Tags**: cs.CL cs.AI cs.LG 



### Hogwild! Inference: Parallel LLM Generation via Concurrent Attention
**Authors**: Gleb Rodionov, Roman Garipov, Alina Shutova, George Yakushev, Vage Egiazarian, Anton Sinitsin, Denis Kuznedelev, Dan Alistarh

**Updated**: 2025-04-09T17:56:08Z

**Summary**: Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM "workers" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while "seeing" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with "instant" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2504.06261v2),  [pdf](http://arxiv.org/pdf/2504.06261v2)

**Tags**: cs.LG cs.CL 



### Saliency-driven Dynamic Token Pruning for Large Language Models
**Authors**: Yao Tao, Yehui Tang, Yun Wang, Mingjian Zhu, Hailin Hu, Yunhe Wang

**Updated**: 2025-04-09T14:36:19Z

**Summary**: Despite the recent success of large language models (LLMs), LLMs are particularly challenging in long-sequence inference scenarios due to the quadratic computational complexity of the attention mechanism. Inspired by the interpretability theory of feature attribution in neural network models, we observe that not all tokens have the same contribution. Based on this observation, we propose a novel token pruning framework, namely Saliency-driven Dynamic Token Pruning (SDTP), to gradually and dynamically prune redundant tokens based on the input context. Specifically, a lightweight saliency-driven prediction module is designed to estimate the importance score of each token with its hidden state, which is added to different layers of the LLM to hierarchically prune redundant tokens. Furthermore, a ranking-based optimization strategy is proposed to minimize the ranking divergence of the saliency score and the predicted importance score. Extensive experiments have shown that our framework is generalizable to various models and datasets. By hierarchically pruning 65\% of the input tokens, our method greatly reduces 33\% $\sim$ 47\% FLOPs and achieves speedup up to 1.75$\times$ during inference, while maintaining comparable performance. We further demonstrate that SDTP can be combined with KV cache compression method for further compression.

**Link**: [arxiv](http://arxiv.org/abs/2504.04514v2),  [pdf](http://arxiv.org/pdf/2504.04514v2)

**Tags**: cs.CL cs.AI 



### Introducing the Arm-membench Throughput Benchmark
**Authors**: Cyrill Burth, Markus Velten, Robert Schöne

**Updated**: 2025-04-09T12:07:26Z

**Summary**: Application performance of modern day processors is often limited by the memory subsystem rather than actual compute capabilities. Therefore, data throughput specifications play a key role in modeling application performance and determining possible bottlenecks. However, while peak instruction throughputs and bandwidths for local caches are often documented, the achievable throughput can also depend on the relation between memory access and compute instructions. In this paper, we present an Arm version of the well established x86-membench throughput benchmark, which we have adapted to support all current SIMD extensions of the Armv8 instruction set architecture. We describe aspects of the Armv8 ISA that need to be considered in the portable design of this benchmark. We use the benchmark to analyze the memory subsystem at a fine spatial granularity and to unveil microarchitectural details of three processors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the resulting performance information, we show that instruction fetch and decoder widths become a potential bottleneck for cache-bandwidth-sensitive workloads due to the load-store concept of the Arm ISA.

**Link**: [arxiv](http://arxiv.org/abs/2504.06813v1),  [pdf](http://arxiv.org/pdf/2504.06813v1)

**Tags**: cs.AR cs.PF 



### Optimizing LLM Queries in Relational Data Analytics Workloads
**Authors**: Shu Liu, Asim Biswal, Amog Kamsetty, Audrey Cheng, Luis Gaspar Schroeder, Liana Patel, Shiyi Cao, Xiangxi Mo, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia

**Updated**: 2025-04-09T10:23:39Z

**Summary**: Batch data analytics is a growing application for Large Language Models (LLMs). LLMs enable users to perform a wide range of natural language tasks, such as classification, entity extraction, and translation, over large datasets. However, LLM inference is highly costly and slow: for example, an NVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second, taking about a day to handle 15 GB of data; processing a similar amount of data costs around $10K on OpenAI's GPT-4o. In this paper, we propose novel techniques that can significantly reduce the cost of LLM calls for relational data analytics workloads. Our key contribution is developing efficient algorithms for reordering the rows and the fields within each row of an input table to maximize key-value (KV) cache reuse when performing LLM serving. As such, our approach can be easily applied to existing analytics systems and serving platforms. Our evaluation shows that our solution can yield up to 3.4x improvement in job completion time on a benchmark of diverse LLM-based queries using Llama 3 models. Our solution also achieves a 32% cost savings under OpenAI and Anthropic pricing models.

**Link**: [arxiv](http://arxiv.org/abs/2403.05821v2),  [pdf](http://arxiv.org/pdf/2403.05821v2)

**Tags**: cs.LG cs.DB 



### MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced   Retrieval Augmentation
**Authors**: Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, Tiejun Huang

**Updated**: 2025-04-09T09:09:37Z

**Summary**: Processing long contexts presents a significant challenge for large language models (LLMs). While recent advancements allow LLMs to handle much longer contexts than before (e.g., 32K or 128K tokens), it is computationally expensive and can still be insufficient for many applications. Retrieval-Augmented Generation (RAG) is considered a promising strategy to address this problem. However, conventional RAG methods face inherent limitations because of two underlying requirements: 1) explicitly stated queries, and 2) well-structured knowledge. These conditions, however, do not hold in general long-context processing tasks.   In this work, we propose MemoRAG, a novel RAG framework empowered by global memory-augmented retrieval. MemoRAG features a dual-system architecture. First, it employs a light but long-range system to create a global memory of the long context. Once a task is presented, it generates draft answers, providing useful clues for the retrieval tools to locate relevant information within the long context. Second, it leverages an expensive but expressive system, which generates the final answer based on the retrieved information. Building upon this fundamental framework, we realize the memory module in the form of KV compression, and reinforce its memorization and cluing capacity from the Generation quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG achieves superior performances across a variety of long-context evaluation tasks, not only complex scenarios where traditional RAG methods struggle, but also simpler ones where RAG is typically applied.

**Link**: [arxiv](http://arxiv.org/abs/2409.05591v3),  [pdf](http://arxiv.org/pdf/2409.05591v3)

**Tags**: cs.CL cs.AI 



### Dynamic Content Caching with Waiting Costs via Restless Multi-Armed   Bandits
**Authors**: Ankita Koley, Chandramani Singh

**Updated**: 2025-04-09T07:55:43Z

**Summary**: We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the exising policies.

**Link**: [arxiv](http://arxiv.org/abs/2410.18627v5),  [pdf](http://arxiv.org/pdf/2410.18627v5)

**Tags**: cs.NI 



### GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for   Dynamic Graph Processing
**Authors**: Hongfu Li, Qian Tao, Song Yu, Shufeng Gong, Yanfeng Zhang, Feng Yao, Wenyuan Yu, Ge Yu, Jingren Zhou

**Updated**: 2025-04-09T03:49:16Z

**Summary**: An efficient data structure is fundamental to meeting the growing demands in dynamic graph processing. However, the dual requirements for graph computation efficiency (with contiguous structures) and graph update efficiency (with linked list-like structures) present a conflict in the design principles of graph structures. After experimental studies of existing state-of-the-art dynamic graph structures, we observe that the overhead of cache misses accounts for a major portion of the graph computation time. This paper presents GastCoCo, a system with graph storage and coroutine-based prefetch co-design. By employing software prefetching via stackless coroutines and introducing a prefetch-friendly data structure CBList, GastCoCo significantly alleviates the performance degradation caused by cache misses. Our results show that GastCoCo outperforms state-of-the-art graph storage systems by 1.3x - 180x in graph updates and 1.4x - 41.1x in graph computation.

**Link**: [arxiv](http://arxiv.org/abs/2312.14396v5),  [pdf](http://arxiv.org/pdf/2312.14396v5)

**Tags**: cs.DB 



### SPIRe: Boosting LLM Inference Throughput with Speculative Decoding
**Authors**: Sanjit Neelam, Daniel Heinlein, Vaclav Cvicek, Akshay Mishra, Reiner Pope

**Updated**: 2025-04-08T20:39:20Z

**Summary**: Speculative decoding (SD) has been shown to reduce the latency of autoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing throughput and therefore reducing the cost per token requires decoding with large batch sizes. Recent work shows that SD can accelerate decoding with large batch sizes too if the context is sufficiently long and the draft model's KV cache is sparse. We introduce SPIRe, a draft model that combines static sparse attention, pruned initialization, and feedback memory to increase the modeled throughput of speculative decoding by over 100% compared to speculation with a much smaller draft model and by over 35% compared to the strong baseline of sparse self-speculation. Our approach is particularly effective when context lengths vary significantly across requests.

**Link**: [arxiv](http://arxiv.org/abs/2504.06419v1),  [pdf](http://arxiv.org/pdf/2504.06419v1)

**Tags**: cs.LG 



### Unifying Autoregressive and Diffusion-Based Sequence Generation
**Authors**: Nima Fathi, Torsten Scholak, Pierre-André Noël

**Updated**: 2025-04-08T20:32:10Z

**Summary**: We present significant extensions to diffusion-based sequence generation models, blurring the line with autoregressive language models. We introduce hyperschedules, which assign distinct noise schedules to individual token positions, generalizing both autoregressive models (e.g., GPT) and conventional diffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two hybrid token-wise noising processes that interpolate between absorbing and uniform processes, enabling the model to fix past mistakes, and we introduce a novel inference algorithm that leverages this new feature in a simplified context inspired from MDLM. To support efficient training and inference, we design attention masks compatible with KV-caching. Our methods achieve state-of-the-art perplexity and generate diverse, high-quality sequences across standard benchmarks, suggesting a promising path for autoregressive diffusion-based sequence generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.06416v1),  [pdf](http://arxiv.org/pdf/2504.06416v1)

**Tags**: cs.LG 



### Walking on Spheres and Talking to Neighbors: Variance Reduction for   Laplace's Equation
**Authors**: Michael Czekanski, Benjamin Faber, Margaret Fairborn, Adelle Wright, David Bindel

**Updated**: 2025-04-08T19:26:41Z

**Summary**: Walk on Spheres algorithms leverage properties of Brownian Motion to create Monte Carlo estimates of solutions to a class of elliptic partial differential equations. We propose a new caching strategy which leverages the continuity of paths of Brownian Motion. In the case of Laplace's equation with Dirichlet boundary conditions, our algorithm has improved asymptotic runtime compared to previous approaches. Until recently, estimates were constructed pointwise and did not use the relationship between solutions at nearby points within a domain. Instead, our results are achieved by passing information from a cache of fixed size. We also provide bounds on the performance of our algorithm and demonstrate its performance on example problems of increasing complexity.

**Link**: [arxiv](http://arxiv.org/abs/2404.17692v2),  [pdf](http://arxiv.org/pdf/2404.17692v2)

**Tags**: physics.comp-ph math.PR physics.app-ph 



### GPU-accelerated Evolutionary Many-objective Optimization Using   Tensorized NSGA-III
**Authors**: Hao Li, Zhenyu Liang, Ran Cheng

**Updated**: 2025-04-08T14:09:23Z

**Summary**: NSGA-III is one of the most widely adopted algorithms for tackling many-objective optimization problems. However, its CPU-based design severely limits scalability and computational efficiency. To address the limitations, we propose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that leverages GPU parallelism for large-scale many-objective optimization. Unlike conventional GPU-accelerated evolutionary algorithms that rely on heuristic approximations to improve efficiency, TensorNSGA-III maintains the exact selection and variation mechanisms of NSGA-III while achieving significant acceleration. By reformulating the selection process with tensorized data structures and an optimized caching strategy, our approach effectively eliminates computational bottlenecks inherent in traditional CPU-based and na\"ive GPU implementations. Experimental results on widely used numerical benchmarks show that TensorNSGA-III achieves speedups of up to $3629\times$ over the CPU version of NSGA-III. Additionally, we validate its effectiveness in multiobjective robotic control tasks, where it discovers diverse and high-quality behavioral solutions. Furthermore, we investigate the critical role of large population sizes in many-objective optimization and demonstrate the scalability of TensorNSGA-III in such scenarios. The source code is available at https://github.com/EMI-Group/evomo

**Link**: [arxiv](http://arxiv.org/abs/2504.06067v1),  [pdf](http://arxiv.org/pdf/2504.06067v1)

**Tags**: cs.NE 



### Unifying KV Cache Compression for Large Language Models with LeanKV
**Authors**: Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C. S. Lui, Haibo Chen

**Updated**: 2025-04-08T14:05:12Z

**Summary**: Large language models (LLMs) exhibit exceptional performance but incur significant serving costs due to their substantial memory requirements, with the key-value (KV) cache being a primary bottleneck. Existing KV cache compression techniques, such as quantization and pruning, apply uniform treatment to both keys and values, and discard unimportant tokens entirely, overlooking the fine-grained differences in significance of various components within the KV cache. To address these limitations, we introduce LeanKV, a framework that advances KV cache compression by exploiting three levels of differentiation in the KV cache: (1) the differing impact of keys and values on attention computation, (2) the varying importance of tokens, and (3) the diverse dynamic sparsity patterns across attention heads. At the core of LeanKV is an on-GPU memory manager that compacts fragmented free memory list into contiguous regions in parallel, effectively translating sparsity in the KV cache into performance gains. We evaluate LeanKV on several mainstream models, including the recent "thinking model". LeanKV is able to compress the KV cache by $2.7\times$ to $5.7\times$ with near-lossless accuracy on complex workloads requiring sophisticated reasoning and long-generation capabilities, and enhances throughput by $1.9\times$ to $5.4\times$.

**Link**: [arxiv](http://arxiv.org/abs/2412.03131v2),  [pdf](http://arxiv.org/pdf/2412.03131v2)

**Tags**: cs.LG cs.DC 



### Graph Federated Learning Based Proactive Content Caching in Edge   Computing
**Authors**: Rui Wang

**Updated**: 2025-04-08T12:46:45Z

**Summary**: With the rapid growth of mobile data traffic and the increasing prevalence of video streaming, proactive content caching in edge computing has become crucial for reducing latency and alleviating network congestion. However, traditional caching strategies such as FIFO, LRU, and LFU fail to effectively predict future content popularity, while existing proactive caching approaches often require users to upload data to a central server, raising concerns regarding privacy and scalability. To address these challenges, this paper proposes a Graph Federated Learning-based Proactive Content Caching (GFPCC) scheme that enhances caching efficiency while preserving user privacy. The proposed approach integrates federated learning and graph neural networks, enabling users to locally train Light Graph Convolutional Networks (LightGCN) to capture user-item relationships and predict content popularity. Instead of sharing raw data, only the trained model parameters are transmitted to the central server, where a federated averaging algorithm aggregates updates, refines the global model, and selects the most popular files for proactive caching. Experimental evaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC outperforms baseline caching algorithms by achieving higher cache efficiency through more accurate content popularity predictions. Moreover, the federated learning framework strengthens privacy protection while maintaining efficient model training; however, scalability remains a challenge in large-scale networks with dynamic user preferences.

**Link**: [arxiv](http://arxiv.org/abs/2502.04760v2),  [pdf](http://arxiv.org/pdf/2502.04760v2)

**Tags**: cs.LG cs.AI 



### HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient   MoE Inference
**Authors**: Shuzhang Zhong, Yanfan Sun, Ling Liang, Runsheng Wang, Ru Huang, Meng Li

**Updated**: 2025-04-08T10:47:37Z

**Summary**: The Mixture of Experts (MoE) architecture has demonstrated significant advantages as it enables to increase the model capacity without a proportional increase in computation. However, the large MoE model size still introduces substantial memory demands, which usually requires expert offloading on resource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU inference has been proposed to leverage CPU computation to reduce expert loading overhead but faces major challenges: on one hand, the expert activation patterns of MoE models are highly unstable, rendering the fixed mapping strategies in existing works inefficient; on the other hand, the hybrid CPU-GPU schedule for MoE is inherently complex due to the diverse expert sizes, structures, uneven workload distribution, etc. To address these challenges, in this paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that improves resource utilization through a novel CPU-GPU scheduling and cache management system. HybriMoE introduces (i) a dynamic intra-layer scheduling strategy to balance workloads across CPU and GPU, (ii) an impact-driven inter-layer prefetching algorithm, and (iii) a score-based caching algorithm to mitigate expert activation instability. We implement HybriMoE on top of the kTransformers framework and evaluate it on three widely used MoE-based LLMs. Experimental results demonstrate that HybriMoE achieves an average speedup of 1.33$\times$ in the prefill stage and 1.70$\times$ in the decode stage compared to state-of-the-art hybrid MoE inference framework. Our code is available at: https://github.com/PKU-SEC-Lab/HybriMoE.

**Link**: [arxiv](http://arxiv.org/abs/2504.05897v1),  [pdf](http://arxiv.org/pdf/2504.05897v1)

**Tags**: cs.LG cs.DC 



### Accelerating LLM Inference Throughput via Asynchronous KV Cache   Prefetching
**Authors**: Yanhao Dong, Yubo Miao, Weinan Li, Xiao Zheng, Chao Wang, Feng Lyu

**Updated**: 2025-04-08T09:17:35Z

**Summary**: Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.

**Link**: [arxiv](http://arxiv.org/abs/2504.06319v1),  [pdf](http://arxiv.org/pdf/2504.06319v1)

**Tags**: cs.LG cs.AI 



### Low-Complexity AoI-Optimal Status Update Control with Partial Battery   State Information in Energy Harvesting IoT Networks
**Authors**: Hao Wu, Shengtian Yang, Jun Chen, Chao Chen, Anding Wang

**Updated**: 2025-04-08T08:40:36Z

**Summary**: For a two-hop IoT system consisting of multiple energy harvesting sensors, a cache-enabled edge node, and multiple monitors, the status update control at the edge node, which has partial battery state information (pBSI) of the sensors, is formulated as a pBSI problem. The concept of inferred pBSI is introduced to reduce the noiseless single-sensor pBSI problem to a Markov decision process with a moderate state-space size, enabling the optimal policy to be obtained through a value iteration algorithm. A lower bound on the expected time-average on-demand age of information performance is established for the general single-sensor status update problem. For the single-sensor pBSI problem, a semi-closed-form policy called the current-next (CN) policy is proposed, along with an efficient post-update value iteration algorithm with a per-iteration time complexity proportional to the square of the battery capacity. A weighted-update-gain-competition (WUGC) approach is further leveraged to extend the CN policy to the multi-sensor case. Numerical results in the single-sensor case demonstrate the near-optimal performance of the CN policy across various energy arrival processes. Simulations for an IoT system with $100$ sensors reveal that the WUGC-CN policy outperforms the maximum-age-first policy and the random-scheduling-based CN policy under Bernoulli energy arrival processes.

**Link**: [arxiv](http://arxiv.org/abs/2504.05807v1),  [pdf](http://arxiv.org/pdf/2504.05807v1)

**Tags**: cs.IT cs.SY eess.SY math.IT 



### CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in   a 64-bit Application Class RISC-V Processor
**Authors**: Christopher Reinwardt, Robert Balas, Alessandro Ottaviano, Angelo Garofalo, Luca Benini

**Updated**: 2025-04-08T06:38:27Z

**Summary**: The increasing complexity of autonomous systems has driven a shift to integrated heterogeneous SoCs with real-time and safety demands. Ensuring deterministic WCETs and low-latency for critical tasks requires minimizing interference on shared resources like virtual memory. Existing techniques, such as software coloring and memory replication, introduce significant area and performance overhead, especially with virtualized memory where address translation adds latency uncertainty. To address these limitations, we propose CVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware support for predictability in virtual memory access with minimal area overhead. CVA6-VMRT features dynamically partitioned Translation Look-aside Buffers (TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows fine-grained per-thread control of resources, enabling the operating system to manage TLB replacements, including static overwrites, to ensure single-cycle address translation for critical memory regions. Additionally, CVA6-VMRT enables runtime partitioning of data and instruction caches into cache and SPM sections, providing low and predictable access times for critical data without impacting other accesses. In a virtualized setting, CVA6-VMRT enhances execution time determinism for critical guests by 94% during interference from non-critical guests, with minimal impact on their average absolute execution time compared to isolated execution of the critical guests only. This interference-aware behaviour is achieved with just a 4% area overhead and no timing penalty compared to the baseline CVA6 core.

**Link**: [arxiv](http://arxiv.org/abs/2504.05718v1),  [pdf](http://arxiv.org/pdf/2504.05718v1)

**Tags**: cs.AR 



### SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with   Sweep Reconstruction
**Authors**: Zikang Yuan, Ruiye Ming, Chengwei Zhao, Yonghao Tan, Pingcheng Dong, Hongcheng Luo, Yuzhong Jiao, Xin Yang, Kwang-Ting Cheng

**Updated**: 2025-04-08T05:27:15Z

**Summary**: Addressing the inherent low acquisition frequency limitation of 3D LiDAR to achieve high-frequency output has become a critical research focus in the LiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance, frequency-enhanced LIO systems must process each sweep within significantly reduced timeframe, which presents substantial challenges for deployment on low-computational-power platforms. To address these limitations, we introduce SR-LIO++, an innovative LIO system capable of achieving doubled output frequency relative to input frequency on resource-constrained hardware platforms, including the Raspberry Pi 4B. Our system employs a sweep reconstruction methodology to enhance LiDAR sweep frequency, generating high-frequency reconstructed sweeps. Building upon this foundation, we propose a caching mechanism for intermediate results (i.e., surface parameters) of the most recent segments, effectively minimizing redundant processing of common segments in adjacent reconstructed sweeps. This method decouples processing time from the traditionally linear dependence on reconstructed sweep frequency. Furthermore, we present a quantized map point management based on index table mapping, significantly reducing memory usage by converting global 3D point storage from 64-bit double precision to 8-bit char representation. This method also converts the computationally intensive Euclidean distance calculations in nearest neighbor searches from 64-bit double precision to 16-bit short and 32-bit integer formats, significantly reducing both memory and computational cost. Extensive experimental evaluations across three distinct computing platforms and four public datasets demonstrate that SR-LIO++ maintains state-of-the-art accuracy while substantially enhancing efficiency. Notably, our system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.

**Link**: [arxiv](http://arxiv.org/abs/2503.22926v2),  [pdf](http://arxiv.org/pdf/2503.22926v2)

**Tags**: cs.RO 



### MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV   Product Quantization
**Authors**: Zongwu Wang, Peng Xu, Fangxin Liu, Yiwei Hu, Qingxiao Sun, Gezi Li, Cheng Li, Xuan Wang, Li Jiang, Haibing Guan

**Updated**: 2025-04-08T04:34:44Z

**Summary**: Large language models (LLMs) are increasingly utilized for complex tasks requiring longer context lengths, with some models supporting up to 128K or 1M tokens. This trend, however, presents significant challenges in inference speed and memory management. Quantization emerges as a promising approach to address the widening gap between LLM size and memory capacity. However, traditional quantization schemes often yield suboptimal compression results for KV caches due to two key factors: i) On-the-fly quantization and de-quantization, causing significant performance overhead; ii) Prevalence of outliers in KV values, challenging low-bitwidth uniform quantization. To this end, we propose MILLION, a novel quantization framework achieving low-bitwidth KV cache through product quantization. First, we conduct a thorough analysis of KV cache distribution, revealing the limitations of existing quantization schemes. Second, we introduce a non-uniform quantization algorithm based on product quantization, which efficiently compresses data while preserving accuracy. Third, we develop a high-performance GPU inference framework with efficient attention kernel and pipeline design for MILLION that leverages sparse computation and asynchronous quantization, significantly enhancing inference speed. Comprehensive evaluation results demonstrate that MILLION can achieve 4 bits quantization with trivial perplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at 32K context length. Code is released at https://github.com/ZongwuWang/MILLION.

**Link**: [arxiv](http://arxiv.org/abs/2504.03661v2),  [pdf](http://arxiv.org/pdf/2504.03661v2)

**Tags**: cs.DC I.2.0 



### Lattice: Learning to Efficiently Compress the Memory
**Authors**: Mahdi Karami, Vahab Mirrokni

**Updated**: 2025-04-08T03:48:43Z

**Summary**: Attention mechanisms have revolutionized sequence learning but suffer from quadratic computational complexity. This paper introduces Lattice, a novel recurrent neural network (RNN) mechanism that leverages the inherent low-rank structure of K-V matrices to efficiently compress the cache into a fixed number of memory slots, achieving sub-quadratic complexity. We formulate this compression as an online optimization problem and derive a dynamic memory update rule based on a single gradient descent step. The resulting recurrence features a state- and input-dependent gating mechanism, offering an interpretable memory update process. The core innovation is the orthogonal update: each memory slot is updated exclusively with information orthogonal to its current state hence incorporation of only novel, non-redundant data, which minimizes the interference with previously stored information. The experimental results show that Lattice achieves the best perplexity compared to all baselines across diverse context lengths, with performance improvement becoming more pronounced as the context length increases.

**Link**: [arxiv](http://arxiv.org/abs/2504.05646v1),  [pdf](http://arxiv.org/pdf/2504.05646v1)

**Tags**: cs.LG cs.AI 



### ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions
**Authors**: Vincenzo Petrolo, Flavia Guella, Michele Caon, Pasquale Davide Schiavone, Guido Masera, Maurizio Martina

**Updated**: 2025-04-07T22:48:33Z

**Summary**: Modern data-driven applications expose limitations of von Neumann architectures - extensive data movement, low throughput, and poor energy efficiency. Accelerators improve performance but lack flexibility and require data transfers. Existing compute in- and near-memory solutions mitigate these issues but face usability challenges due to data placement constraints. We propose a novel cache architecture that doubles as a tightly-coupled compute-near-memory coprocessor. Our RISC-V cache controller executes custom instructions from the host CPU using vector operations dispatched to near-memory vector processing units within the cache memory subsystem. This architecture abstracts memory synchronization and data mapping from application software while offering software-based Instruction Set Architecture extensibility. Our implementation shows $30\times$ to $84\times$ performance improvement when operating on 8-bit data over the same system with a traditional cache when executing a worst-case 32-bit CNN workload, with only $41.3\%$ area overhead.

**Link**: [arxiv](http://arxiv.org/abs/2504.02533v3),  [pdf](http://arxiv.org/pdf/2504.02533v3)

**Tags**: cs.AR 



### LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs
**Authors**: Fatemeh Hadadi, Qinghua Xu, Domenico Bianculli, Lionel Briand

**Updated**: 2025-04-07T20:52:04Z

**Summary**: Most log-based anomaly detectors assume logs are stable, though logs are often unstable due to software or environmental changes. Anomaly detection on unstable logs (ULAD) is therefore a more realistic, yet under-investigated challenge. Current approaches predominantly employ machine learning (ML) models, which often require extensive labeled data for training. To mitigate data insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that combines ML models -- decision tree, k-nearest neighbors, and a feedforward neural network -- with a Large Language Model (Mistral) through ensemble learning. FlexLog also incorporates a cache and retrieval-augmented generation (RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we configured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and SYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points in F1 score while using 62.87 percentage points less labeled data. When trained on the same amount of data as the baselines, FlexLog achieves up to a 13 percentage points increase in F1 score on ADFA-U across varying training dataset sizes. Additionally, FlexLog maintains inference time under one second per log sequence, making it suitable for most applications except latency-sensitive systems. Further analysis reveals the positive impact of FlexLog's key components: cache, RAG and ensemble learning.

**Link**: [arxiv](http://arxiv.org/abs/2406.07467v2),  [pdf](http://arxiv.org/pdf/2406.07467v2)

**Tags**: cs.SE 



### State Tuning: State-based Test-Time Scaling on RWKV-7
**Authors**: Liu Xiao, Li Zhiyuan, Lin Yueyu

**Updated**: 2025-04-07T14:04:30Z

**Summary**: Test-time scaling has emerged as a prominent research direction in machine learning, enabling models to enhance their expressive capabilities during inference.Transformers, renowned for striking a delicate balance between efficiency and expressiveness, have benefited from test-time scaling techniques that leverage an expanding key-value (KV) cache to significantly improve performance.In this paper, we introduce a novel state-based approach to test-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7 model.By exploiting the unique strengths of RWKV-7, our method achieves state-of-the-art performance on the target task without altering the model's pre-trained weights. Our approach centers on three key innovations. First, we develop an observer framework that allows a smaller model to replicate and learn the state dynamics of the RWKV-7 model. Second, we employ a kernel method to dynamically upscale the state size, enhancing the model's capacity to capture intricate patterns. Third, we integrate Decorrelated Backpropagation (DBP) to optimize the upscaled state matrix, thereby improving convergence and expressivity. By tuning only the state matrix, we demonstrate that a smaller model can outperform larger models on the given task. This method preserves the efficiency of the original RWKV-7 architecture while harnessing the power of test-time scaling to deliver superior results. Our findings underscore the potential of state tuning as an effective strategy for advancing model performance in resource-constrained settings. Our code is https://github.com/TorchRWKV/flash-linear-attention.

**Link**: [arxiv](http://arxiv.org/abs/2504.05097v1),  [pdf](http://arxiv.org/pdf/2504.05097v1)

**Tags**: cs.CL cs.LG 



### Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning   Models
**Authors**: Ruikang Liu, Yuxuan Sun, Manyi Zhang, Haoli Bai, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou

**Updated**: 2025-04-07T08:22:45Z

**Summary**: Recent advancements in reasoning language models have demonstrated remarkable performance in complex tasks, but their extended chain-of-thought reasoning process increases inference overhead. While quantization has been widely adopted to reduce the inference cost of large language models, its impact on reasoning models remains understudied. In this study, we conduct the first systematic study on quantized reasoning models, evaluating the open-sourced DeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B parameters, and QwQ-32B. Our investigation covers weight, KV cache, and activation quantization using state-of-the-art algorithms at varying bit-widths, with extensive evaluation across mathematical (AIME, MATH-500), scientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our findings reveal that while lossless quantization can be achieved with W8A8 or W4A16 quantization, lower bit-widths introduce significant accuracy risks. We further identify model size, model origin, and task difficulty as critical determinants of performance. Contrary to expectations, quantized models do not exhibit increased output lengths. In addition, strategically scaling the model sizes or reasoning steps can effectively enhance the performance. All quantized models and codes will be open-sourced in https://github.com/ruikangliu/Quantized-Reasoning-Models.

**Link**: [arxiv](http://arxiv.org/abs/2504.04823v1),  [pdf](http://arxiv.org/pdf/2504.04823v1)

**Tags**: cs.CL cs.AI 



## Keyword: LLM Inference 
 ### Dispu$τ$able: the high cost of a low optical depth
**Authors**: Noah Sailer, Gerrit S. Farren, Simone Ferraro, Martin White

**Updated**: 2025-04-23T17:59:56Z

**Summary**: Recent Baryonic Acoustic Oscillation (BAO) measurements from the Dark Energy Spectroscopic Instrument (DESI) are mildly discrepant ($2.2\sigma$) with the Cosmic Microwave Background (CMB) when interpreted within $\Lambda$CDM. When analyzing these data with extended cosmologies this inconsistency manifests as a $\simeq3\sigma$ preference for sub-minimal neutrino mass or evolving dark energy. It is known that the preference for sub-minimal neutrino mass from the suppression of structure growth could be alleviated by increasing the optical depth to reionization $\tau$. We show that, because the CMB-inferred $\tau$ is negatively correlated with the matter fraction, a larger optical depth resolves a similar preference from geometric constraints. Optical depths large enough to resolve the neutrino mass tension ($\tau\sim0.09)$ also reduce the preference for evolving dark energy from $\simeq3\sigma$ to $\simeq1.5\sigma$. Conversely, within $\Lambda$CDM the combination of DESI BAO, high-$\ell$ CMB and CMB lensing yields $\tau = 0.090 \pm 0.012$. The required increase in $\tau$ is in $\simeq3-5\sigma$ tension with Planck low-$\ell$ polarization data when taken at face value. While there is no evidence for systematics in the large-scale Planck data, $\tau$ remains the least well-constrained $\Lambda$CDM parameter and is far from its cosmic variance limit. The importance of $\tau$ for several cosmological measurements strengthens the case for future large-scale CMB experiments as well as direct probes of the epoch of reionization.

**Link**: [arxiv](http://arxiv.org/abs/2504.16932v1),  [pdf](http://arxiv.org/pdf/2504.16932v1)

**Tags**: astro-ph.CO 



### IberBench: LLM Evaluation on Iberian Languages
**Authors**: José Ángel González, Ian Borrego Obrador, Álvaro Romo Herrero, Areg Mikael Sarvazyan, Mara Chinea-Ríos, Angelo Basile, Marc Franco-Salvador

**Updated**: 2025-04-23T17:48:25Z

**Summary**: Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.

**Link**: [arxiv](http://arxiv.org/abs/2504.16921v1),  [pdf](http://arxiv.org/pdf/2504.16921v1)

**Tags**: cs.CL 



### Stop Overthinking: A Survey on Efficient Reasoning for Large Language   Models
**Authors**: Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu

**Updated**: 2025-04-23T17:46:54Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.

**Link**: [arxiv](http://arxiv.org/abs/2503.16419v3),  [pdf](http://arxiv.org/pdf/2503.16419v3)

**Tags**: cs.CL 



### DiffArtist: Towards Structure and Appearance Controllable Image   Stylization
**Authors**: Ruixiang Jiang, Changwen Chen

**Updated**: 2025-04-23T17:46:08Z

**Summary**: Artistic style includes both structural and appearance elements. Existing neural stylization techniques primarily focus on transferring appearance features such as color and texture, often neglecting the equally crucial aspect of structural stylization. In this paper, we present a comprehensive study on the simultaneous stylization of structure and appearance of 2D images. Specifically, we introduce DiffArtist, which, to the best of our knowledge, is the first stylization method to allow for dual controllability over structure and appearance. Our key insight is to represent structure and appearance as separate diffusion processes to achieve complete disentanglement without requiring any training, thereby endowing users with unprecedented controllability for both components. The evaluation of stylization of both appearance and structure, however, remains challenging as it necessitates semantic understanding. To this end, we further propose a Multimodal LLM-based style evaluator, which better aligns with human preferences than metrics lacking semantic understanding. With this powerful evaluator, we conduct extensive analysis, demonstrating that DiffArtist achieves superior style fidelity, editability, and structure-appearance disentanglement. These merits make DiffArtist a highly versatile solution for creative applications. Project homepage: https://github.com/songrise/Artist.

**Link**: [arxiv](http://arxiv.org/abs/2407.15842v3),  [pdf](http://arxiv.org/pdf/2407.15842v3)

**Tags**: cs.CV cs.GR 



### OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents
**Authors**: Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang

**Updated**: 2025-04-23T17:45:05Z

**Summary**: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.

**Link**: [arxiv](http://arxiv.org/abs/2504.16918v1),  [pdf](http://arxiv.org/pdf/2504.16918v1)

**Tags**: cs.CL cs.AI 



### Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text
**Authors**: Shifali Agrahari, Sanasam Ranbir Singh

**Updated**: 2025-04-23T17:39:49Z

**Summary**: In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2504.16913v1),  [pdf](http://arxiv.org/pdf/2504.16913v1)

**Tags**: cs.CL cs.AI 



### A Measure Based Generalizable Approach to Understandability
**Authors**: Vikas Kushwaha, Sruti Srinivasa Ragavan, Subhajit Roy

**Updated**: 2025-04-23T17:39:20Z

**Summary**: Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).   In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future.

**Link**: [arxiv](http://arxiv.org/abs/2503.21615v2),  [pdf](http://arxiv.org/pdf/2503.21615v2)

**Tags**: cs.HC cs.AI cs.SE 



### Do Large Language Models know who did what to whom?
**Authors**: Joseph M. Denning, Xiaohan, Guo, Bryor Snefjella, Idan A. Blank

**Updated**: 2025-04-23T17:00:45Z

**Summary**: Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.

**Link**: [arxiv](http://arxiv.org/abs/2504.16884v1),  [pdf](http://arxiv.org/pdf/2504.16884v1)

**Tags**: cs.CL 



### Enhancing Critical Thinking with AI: A Tailored Warning System for RAG   Models
**Authors**: Xuyang Zhu, Sejoon Chang, Andrew Kuik

**Updated**: 2025-04-23T17:00:25Z

**Summary**: Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. However, fairness and reliability concerns persist, as hallucinations can emerge at both the retrieval and generation stages, affecting users' reasoning and decision-making. Our research explores how tailored warning messages -- whose content depends on the specific context of hallucination -- shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also introduce cognitive friction, leading to confusion and diminished trust in the system. By examining these interactions, this work contributes to the broader goal of AI-augmented reasoning: developing systems that actively support human reflection, critical thinking, and informed decision-making rather than passive information consumption.

**Link**: [arxiv](http://arxiv.org/abs/2504.16883v1),  [pdf](http://arxiv.org/pdf/2504.16883v1)

**Tags**: cs.HC 



### Context-Enhanced Vulnerability Detection Based on Large Language Model
**Authors**: Yixin Yang, Bowen Xu, Xiang Gao, Hailong Sun

**Updated**: 2025-04-23T16:54:16Z

**Summary**: Vulnerability detection is a critical aspect of software security. Accurate detection is essential to prevent potential security breaches and protect software systems from malicious attacks. Recently, vulnerability detection methods leveraging deep learning and large language models (LLMs) have garnered increasing attention. However, existing approaches often focus on analyzing individual files or functions, which limits their ability to gather sufficient contextual information. Analyzing entire repositories to gather context introduces significant noise and computational overhead. To address these challenges, we propose a context-enhanced vulnerability detection approach that combines program analysis with LLMs. Specifically, we use program analysis to extract contextual information at various levels of abstraction, thereby filtering out irrelevant noise. The abstracted context along with source code are provided to LLM for vulnerability detection. We investigate how different levels of contextual granularity improve LLM-based vulnerability detection performance. Our goal is to strike a balance between providing sufficient detail to accurately capture vulnerabilities and minimizing unnecessary complexity that could hinder model performance. Based on an extensive study using GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key findings includes: (1) incorporating abstracted context significantly enhances vulnerability detection effectiveness; (2) different models benefit from distinct levels of abstraction depending on their code understanding capabilities; and (3) capturing program behavior through program analysis for general LLM-based code analysis tasks can be a direction that requires further attention.

**Link**: [arxiv](http://arxiv.org/abs/2504.16877v1),  [pdf](http://arxiv.org/pdf/2504.16877v1)

**Tags**: cs.SE 



### aiXamine: Simplified LLM Safety and Security
**Authors**: Fatih Deniz, Dorde Popovic, Yazan Boshmaf, Euisuh Jeong, Minhaj Ahmad, Sanjay Chawla, Issa Khalil

**Updated**: 2025-04-23T16:52:54Z

**Summary**: Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats. To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security. aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment. The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations. We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations. Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0. Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness. Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices.

**Link**: [arxiv](http://arxiv.org/abs/2504.14985v2),  [pdf](http://arxiv.org/pdf/2504.14985v2)

**Tags**: cs.CR cs.AI 



### Natural Language Processing in the Patent Domain: A Survey
**Authors**: Lekang Jiang, Stephan Goetz

**Updated**: 2025-04-23T16:48:56Z

**Summary**: Patents, which encapsulate crucial technical and legal information in text form and referenced drawings, present a rich domain for natural language processing (NLP) applications. As NLP technologies evolve, large language models (LLMs) have demonstrated outstanding capabilities in general text processing and generation tasks. However, the application of LLMs in the patent domain remains under-explored and under-developed due to the complexity of patents, particularly their language and legal framework. Understanding the unique characteristics of patent documents and related research in the patent domain becomes essential for researchers to apply these tools effectively. Therefore, this paper aims to equip NLP researchers with the essential knowledge to navigate this complex domain efficiently. We introduce the relevant fundamental aspects of patents to provide solid background information. In addition, we systematically break down the structural and linguistic characteristics unique to patents and map out how NLP can be leveraged for patent analysis and generation. Moreover, we demonstrate the spectrum of text-based and multimodal patent-related tasks, including nine patent analysis and four patent generation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2403.04105v3),  [pdf](http://arxiv.org/pdf/2403.04105v3)

**Tags**: cs.AI 



### Exploring How LLMs Capture and Represent Domain-Specific Knowledge
**Authors**: Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan

**Updated**: 2025-04-23T16:46:06Z

**Summary**: We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the model's internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks

**Link**: [arxiv](http://arxiv.org/abs/2504.16871v1),  [pdf](http://arxiv.org/pdf/2504.16871v1)

**Tags**: cs.LG 



### Hint towards inconsistency between BAO and Supernovae Dataset: The   Evidence of Redshift Evolving Dark Energy from DESI DR2 is Absent
**Authors**: Samsuzzaman Afroz, Suvodip Mukherjee

**Updated**: 2025-04-23T16:44:24Z

**Summary**: The combination of independent cosmological datasets is a route towards precision and accurate inference of the cosmological parameters if these observations are not contaminated by systematic effects. However, the presence of unknown systematics present in differrent datasets can lead to a biased inference of the cosmological parameters. In this work, we test the consistency of the two independent tracers of the low-redshift cosmic expansion, namely the supernovae dataset from Pantheon$+$ and the BAO dataset from DESI DR2 using the distance duality relation which is a cornerstone relation in cosmology under the framework of General Relativity. We find that these datasets violate the distance duality relation and show a signature of redshift evolution, hinting toward unaccounted physical effects or observational artifacts. Coincidentally this effect mimics a redshift evolving dark energy scenario when supernovae dataset and DESI datasets are combined without accounting for this inconsistency. Accounting for this effect in the likelihood refutes the previous claim of evidence of non-cosmological constant as dark energy model from DESI DR2, and shows a result consistent with cosmological constant with $w_0= -0.92\pm 0.08$ and $w_a= -0.49^{+0.33}_{-0.36}$. This indicates that the current conclusion from DESI DR2 in combination with Pantheon$+$ is likely due to the combination of two inconsistent datasets resulting in precise but inaccurate inference of cosmological parameters. In the future, tests of this kind for the consistency between different cosmological datasets will be essential for robust inference of cosmological parameters and for deciphering unaccounted physical effects or observational artifacts from supernovae and BAO datasets.

**Link**: [arxiv](http://arxiv.org/abs/2504.16868v1),  [pdf](http://arxiv.org/pdf/2504.16868v1)

**Tags**: astro-ph.CO gr-qc 



### TALES: Text Adventure Learning Environment Suite
**Authors**: Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre Côté

**Updated**: 2025-04-24T02:50:28Z

**Summary**: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tale-suite.

**Link**: [arxiv](http://arxiv.org/abs/2504.14128v4),  [pdf](http://arxiv.org/pdf/2504.14128v4)

**Tags**: cs.AI cs.CL 



### Simulating biochemical reactions: The Linear Noise Approximation can   capture non-linear dynamics
**Authors**: Frederick Truman-Williams, Giorgos Minas

**Updated**: 2025-04-23T16:31:05Z

**Summary**: There is a plethora of highly stochastic non-linear dynamical systems in fields such as molecular biology, chemistry, epidemiology, and ecology. Yet, none of the currently available stochastic models are both accurate and computationally efficient for long-term predictions of large systems. The Linear Noise Approximation (LNA) model for biochemical reaction networks is analytically tractable, which makes it computationally efficient for simulation, analysis, and inference. However, it is only accurate for linear systems and short-time transitions. Other methods can achieve greater accuracy across a wider range of systems, including non-linear ones, but lack analytical tractability. This paper seeks to challenge the prevailing view by demonstrating that the Linear Noise Approximation can indeed capture non-linear dynamics after certain modifications. We introduce a new framework that utilises centre manifold theory allowing us to identify simple interventions to the LNA that do not significantly compromise its computational efficiency. We develop specific algorithms for systems that exhibit oscillations or bi-stability and demonstrate their accuracy and computational efficiency across multiple examples.

**Link**: [arxiv](http://arxiv.org/abs/2504.15166v2),  [pdf](http://arxiv.org/pdf/2504.15166v2)

**Tags**: q-bio.QM math.PR physics.chem-ph q-bio.MN 92C40, 82C31 (Primary) 60J20, 92C42 (Secondary) 



### Planning with Diffusion Models for Target-Oriented Dialogue Systems
**Authors**: Hanwen Du, Bo Peng, Xia Ning

**Updated**: 2025-04-23T16:27:15Z

**Summary**: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.

**Link**: [arxiv](http://arxiv.org/abs/2504.16858v1),  [pdf](http://arxiv.org/pdf/2504.16858v1)

**Tags**: cs.CL 



### Emo Pillars: Knowledge Distillation to Support Fine-Grained   Context-Aware and Context-Less Emotion Classification
**Authors**: Alexander Shvets

**Updated**: 2025-04-23T16:23:17Z

**Summary**: Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2504.16856v1),  [pdf](http://arxiv.org/pdf/2504.16856v1)

**Tags**: cs.CL 



### Monte Carlo Planning with Large Language Model for Text-Based Game   Agents
**Authors**: Zijing Shi, Meng Fang, Ling Chen

**Updated**: 2025-04-23T16:23:15Z

**Summary**: Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.16855v1),  [pdf](http://arxiv.org/pdf/2504.16855v1)

**Tags**: cs.CL 



### Comparative Performance Evaluation of Large Language Models for   Extracting Molecular Interactions and Pathway Knowledge
**Authors**: Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Shinjae Yoo, Shantenu Jha

**Updated**: 2025-04-23T16:14:18Z

**Summary**: Background: Identification of the interactions and regulatory relations between biomolecules play pivotal roles in understanding complex biological systems and the mechanisms underlying diverse biological functions. However, the collection of such molecular interactions has heavily relied on expert curation in the past, making it labor-intensive and time-consuming. To mitigate these challenges, we propose leveraging the capabilities of large language models (LLMs) to automate genome-scale extraction of this crucial knowledge.   Results: In this study, we investigate the efficacy of various LLMs in addressing biological tasks, such as the recognition of protein interactions, identification of genes linked to pathways affected by low-dose radiation, and the delineation of gene regulatory relationships. Overall, the larger models exhibited superior performance, indicating their potential for specific tasks that involve the extraction of complex interactions among genes and proteins. Although these models possessed detailed information for distinct gene and protein groups, they faced challenges in identifying groups with diverse functions and in recognizing highly correlated gene regulatory relationships.   Conclusions: By conducting a comprehensive assessment of the state-of-the-art models using well-established molecular interaction and pathway databases, our study reveals that LLMs can identify genes/proteins associated with pathways of interest and predict their interactions to a certain extent. Furthermore, these models can provide important insights, marking a noteworthy stride toward advancing our understanding of biological systems through AI-assisted knowledge discovery.

**Link**: [arxiv](http://arxiv.org/abs/2307.08813v4),  [pdf](http://arxiv.org/pdf/2307.08813v4)

**Tags**: cs.CL cs.LG 



### Certified Mitigation of Worst-Case LLM Copyright Infringement
**Authors**: Jingyu Zhang, Jiacan Yu, Marc Marone, Benjamin Van Durme, Daniel Khashabi

**Updated**: 2025-04-23T16:13:08Z

**Summary**: The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of "copyright takedown" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.

**Link**: [arxiv](http://arxiv.org/abs/2504.16046v2),  [pdf](http://arxiv.org/pdf/2504.16046v2)

**Tags**: cs.CL 



### An accreting dwarf star orbiting the S-type giant star pi1 Gru
**Authors**: M. Montargès, J. Malfait, M. Esseldeurs, A. de Koter, F. Baron, P. Kervella, T. Danilovich, A. M. S. Richards, R. Sahai, I. McDonald, T. Khouri, S. Shetye, A. Zijlstra, M. Van de Sande, I. El Mellah, F. Herpin, L. Siess, S. Etoka, D. Gobrecht, L. Marinho, S. H. J. Wallström, K. T. Wong, aJ. Yates

**Updated**: 2025-04-23T16:09:33Z

**Summary**: Aims. We aim to characterize the properties of the inner companion of the S-type AGB star pi1 Gru, and to identify plausible future evolution scenarios for this triple system. Methods. We observed pi1 Gru with ALMA and VLT/SPHERE. In addition, we collected archival photometry data and used the Hipparcos-Gaia proper motion anomaly. We derive the best orbital parameters from Bayesian inference. Results. The inner companion, pi1 Gru C was located at 37.4 +/- 2.0 mas from the primary in June-July 2019 (projected separation of 6.05 +/- 0.55 au at 161.7 +/- 11.7 pc). The best orbital solution gives a companion mass of 0.86 (+0.22/-0.20) Msun (using the derived mass of the primary), and a semi-major axis of 7.05 (+0.54/-0.57) au. This leads to an orbital period of 11.0 (+1.7/-1.5) yr. The best solution is an elliptical orbit with eccentricity e = 0.35 (+0.18/-0.17), but a circular orbit cannot be totally excluded. The close companion can either be a K1V (F9.5V/K7V) star or a white dwarf. The ultraviolet and millimeter continuum photometry are consistent with the presence of an accretion disk around the close companion. The ultraviolet emission could then either originate in hot spots in an overall cooler disk, or also from a hot disk in case the companion is a white dwarf. Conclusions. Though the close companion and the AGB star are interacting, and an accretion disk is observed around the companion, the mass-accretion rate is too low to cause a Ia supernova but could produce novae every ~900 yr. Short wavelength spatially resolved observations are needed to further constrain the nature of the C companion. Searches for close-in companions similar to this system will help to better understand the physics of mass- and angular-momentum transfer, and orbital evolution in the late evolutionary stages.

**Link**: [arxiv](http://arxiv.org/abs/2504.16845v1),  [pdf](http://arxiv.org/pdf/2504.16845v1)

**Tags**: astro-ph.SR 



### Building Real-time Awareness of Out-of-distribution in Trajectory   Prediction for Autonomous Vehicles
**Authors**: Tongfe Guo, Taposh Banerjee, Rui Liu, Lili Su

**Updated**: 2025-04-23T16:07:20Z

**Summary**: Accurate trajectory prediction is essential for the safe operation of autonomous vehicles in real-world environments. Even well-trained machine learning models may produce unreliable predictions due to discrepancies between training data and real-world conditions encountered during inference. In particular, the training dataset tends to overrepresent common scenes (e.g., straight lanes) while underrepresenting less frequent ones (e.g., traffic circles). In addition, it often overlooks unpredictable real-world events such as sudden braking or falling objects. To ensure safety, it is critical to detect in real-time when a model's predictions become unreliable. Leveraging the intuition that in-distribution (ID) scenes exhibit error patterns similar to training data, while out-of-distribution (OOD) scenes do not, we introduce a principled, real-time approach for OOD detection by framing it as a change-point detection problem. We address the challenging settings where the OOD scenes are deceptive, meaning that they are not easily detectable by human intuitions. Our lightweight solutions can handle the occurrence of OOD at any time during trajectory prediction inference. Experimental results on multiple real-world datasets using a benchmark trajectory prediction model demonstrate the effectiveness of our methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.17277v2),  [pdf](http://arxiv.org/pdf/2409.17277v2)

**Tags**: cs.RO cs.LG 



### On Benchmarking Code LLMs for Android Malware Analysis
**Authors**: Yiling He, Hongyu She, Xingzhi Qian, Xinran Zheng, Zhuo Chen, Zhan Qin, Lorenzo Cavallaro

**Updated**: 2025-04-23T16:07:20Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names. This paper presents CAMA, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. CAMA specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics (consistency, fidelity, and semantic relevance), enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use CAMA to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.

**Link**: [arxiv](http://arxiv.org/abs/2504.00694v2),  [pdf](http://arxiv.org/pdf/2504.00694v2)

**Tags**: cs.CR cs.LG 



### Improving Significant Wave Height Prediction Using Chronos Models
**Authors**: Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang

**Updated**: 2025-04-23T15:56:28Z

**Summary**: Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.

**Link**: [arxiv](http://arxiv.org/abs/2504.16834v1),  [pdf](http://arxiv.org/pdf/2504.16834v1)

**Tags**: cs.LG cs.AI physics.ao-ph 



### Regression-based proximal causal inference for right-censored   time-to-event data
**Authors**: Kendrick Li, George C. Linderman, Xu Shi, Eric J. Tchetgen Tchetgen

**Updated**: 2025-04-23T15:53:19Z

**Summary**: Unmeasured confounding is one of the major concerns in causal inference from observational data. Proximal causal inference (PCI) is an emerging methodological framework to detect and potentially account for confounding bias by carefully leveraging a pair of negative control exposure (NCE) and outcome (NCO) variables, also known as treatment and outcome confounding proxies. Although regression-based PCI is well developed for binary and continuous outcomes, analogous PCI regression methods for right-censored time-to-event outcomes are currently lacking. In this paper, we propose a novel two-stage regression PCI approach for right-censored survival data under an additive hazard structural model. We provide theoretical justification for the proposed approach tailored to different types of NCOs, including continuous, count, and right-censored time-to-event variables. We illustrate the approach with an evaluation of the effectiveness of right heart catheterization among critically ill patients using data from the SUPPORT study. Our method is implemented in the open-access R package 'pci2s'.

**Link**: [arxiv](http://arxiv.org/abs/2409.08924v4),  [pdf](http://arxiv.org/pdf/2409.08924v4)

**Tags**: stat.ME 



### LRASGen: LLM-based RESTful API Specification Generation
**Authors**: Sida Deng, Rubing Huang, Man Zhang, Chenhui Cui, Dave Towey, Rongcun Wang

**Updated**: 2025-04-23T15:52:50Z

**Summary**: REpresentation State Transfer (REST) is an architectural style for designing web applications that enable scalable, stateless communication between clients and servers via common HTTP techniques. Web APIs that employ the REST style are known as RESTful (or REST) APIs. When using or testing a RESTful API, developers may need to employ its specification, which is often defined by open-source standards such as the OpenAPI Specification (OAS). However, it can be very time-consuming and error-prone to write and update these specifications, which may negatively impact the use of RESTful APIs, especially when the software requirements change. Many tools and methods have been proposed to solve this problem, such as Respector and Swagger Core. OAS generation can be regarded as a common text-generation task that creates a formal description of API endpoints derived from the source code. A potential solution for this may involve using Large Language Models (LLMs), which have strong capabilities in both code understanding and text generation. Motivated by this, we propose a novel approach for generating the OASs of RESTful APIs using LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the best of our knowledge, this is the first use of LLMs and API source code to generate OASs for RESTful APIs. Compared with existing tools and methods, LRASGen can generate the OASs, even when the implementation is incomplete (with partial code, and/or missing annotations/comments, etc.). To evaluate the LRASGen performance, we conducted a series of empirical studies on 20 real-world RESTful APIs. The results show that two LLMs (GPT-4o mini and DeepSeek V3) can both support LARSGen to generate accurate specifications, and LRASGen-generated specifications cover an average of 48.85% more missed entities than the developer-provided specifications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16833v1),  [pdf](http://arxiv.org/pdf/2504.16833v1)

**Tags**: cs.SE 



### GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning
**Authors**: Luu Quy Tung, Hoang Quoc Viet, Vo Trong Thu

**Updated**: 2025-04-23T15:48:55Z

**Summary**: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.

**Link**: [arxiv](http://arxiv.org/abs/2504.16832v1),  [pdf](http://arxiv.org/pdf/2504.16832v1)

**Tags**: cs.CL 



### ChatDBG: Augmenting Debugging with Large Language Models
**Authors**: Kyla H. Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund

**Updated**: 2025-04-23T15:48:42Z

**Summary**: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.

**Link**: [arxiv](http://arxiv.org/abs/2403.16354v4),  [pdf](http://arxiv.org/pdf/2403.16354v4)

**Tags**: cs.SE cs.AI cs.LG cs.PL 



### Process Reward Models That Think
**Authors**: Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang

**Updated**: 2025-04-23T15:44:54Z

**Summary**: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.

**Link**: [arxiv](http://arxiv.org/abs/2504.16828v1),  [pdf](http://arxiv.org/pdf/2504.16828v1)

**Tags**: cs.LG cs.AI cs.CL 



### LLM-assisted Graph-RAG Information Extraction from IFC Data
**Authors**: Sima Iranmanesh, Hadeel Saadany, Edlira Vakaj

**Updated**: 2025-04-23T15:31:11Z

**Summary**: IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2504.16813v1),  [pdf](http://arxiv.org/pdf/2504.16813v1)

**Tags**: cs.CL 



### Decoupled Global-Local Alignment for Improving Compositional   Understanding
**Authors**: Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, Yupei Wang

**Updated**: 2025-04-23T15:20:53Z

**Summary**: Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA

**Link**: [arxiv](http://arxiv.org/abs/2504.16801v1),  [pdf](http://arxiv.org/pdf/2504.16801v1)

**Tags**: cs.CV 



### Array Partitioning Based Near-Field Attitude and Location Estimation
**Authors**: Mingchen Zhang, Xiaojun Yuan, Boyu Teng, Li Wang

**Updated**: 2025-04-23T15:20:39Z

**Summary**: This paper studies a passive source localization system, where a single base station (BS) is employed to estimate the positions and attitudes of multiple mobile stations (MSs). The BS and the MSs are equipped with uniform rectangular arrays, and the MSs are located in the near-field region of the BS array. To avoid the difficulty of tackling the problem directly based on the near-field signal model, we establish a subarray-wise far-field received signal model. In this model, the entire BS array is divided into multiple subarrays to ensure that each MS is in the far-field region of each BS subarray. By exploiting the angles of arrival (AoAs) of an MS antenna at different BS subarrays, we formulate the attitude and location estimation problem under the Bayesian inference framework. Based on the factor graph representation of the probabilistic problem model, a message passing algorithm named array partitioning based pose and location estimation (APPLE) is developed to solve this problem. An estimation-error lower bound is obtained as a performance benchmark of the proposed algorithm. Numerical results demonstrate that the proposed APPLE algorithm outperforms other baseline methods in the accuracy of position and attitude estimation.

**Link**: [arxiv](http://arxiv.org/abs/2504.16800v1),  [pdf](http://arxiv.org/pdf/2504.16800v1)

**Tags**: eess.SP 



### The extended adjoint state and nonlinearity in correlation-based passive   imaging
**Authors**: Tram Thi Ngoc Nguyen

**Updated**: 2025-04-23T15:18:34Z

**Summary**: This articles investigates physics-based passive imaging problem, wherein one infers an unknown medium using ambient noise and correlation of the noise signal. We develop a general backpropagation framework via the so-called extended adjoint state, suitable for any linear PDE; crucially, this approach reduces by half the number of required PDE solves. Applications to several different PDE models demonstrate the universality of our method. In addition, we analyze the nonlinearity of the correlated model, revealing a surprising tangential cone condition-like structure, thereby advancing the state of the art towards a convergence guarantee for regularized reconstruction in passive imaging.

**Link**: [arxiv](http://arxiv.org/abs/2504.16797v1),  [pdf](http://arxiv.org/pdf/2504.16797v1)

**Tags**: math.NA cs.NA 65M32, 65J22, 35R30 



### Random Long-Context Access for Mamba via Hardware-aligned Hierarchical   Sparse Attention
**Authors**: Xiang Hu, Jiaqi Leng, Jun Zhao, Kewei Tu, Wei Wu

**Updated**: 2025-04-23T15:15:06Z

**Summary**: A key advantage of Recurrent Neural Networks (RNNs) over Transformers is their linear computational and space complexity enables faster training and inference for long sequences. However, RNNs are fundamentally unable to randomly access historical context, and simply integrating attention mechanisms may undermine their efficiency advantages. To overcome this limitation, we propose \textbf{H}ierarchical \textbf{S}parse \textbf{A}ttention (HSA), a novel attention mechanism that enhances RNNs with long-range random access flexibility while preserving their merits in efficiency and length generalization. HSA divides inputs into chunks, selecting the top-$k$ chunks and hierarchically aggregates information. The core innovation lies in learning token-to-chunk relevance based on fine-grained token-level information inside each chunk. This approach enhances the precision of chunk selection across both in-domain and out-of-domain context lengths. To make HSA efficient, we further introduce a hardware-aligned kernel design. By combining HSA with Mamba, we introduce RAMba, which achieves perfect accuracy in passkey retrieval across 64 million contexts despite pre-training on only 4K-length contexts, and significant improvements on various downstream tasks, with nearly constant memory footprint. These results show RAMba's huge potential in long-context modeling.

**Link**: [arxiv](http://arxiv.org/abs/2504.16795v1),  [pdf](http://arxiv.org/pdf/2504.16795v1)

**Tags**: cs.CL cs.AI 



### Preemption Aware Task Scheduling for Priority and Deadline Constrained   DNN Inference Task Offloading in Homogeneous Mobile-Edge Networks
**Authors**: Jamie Cotter, Ignacio Castineiras, Donna O'Shea, Victor Cionca

**Updated**: 2025-04-23T15:10:55Z

**Summary**: This paper addresses the computational offloading of Deep Neural Networks (DNNs) to nearby devices with similar processing capabilities, to avoid the larger communication delays incurred for cloud offloading. We present a preemption aware scheduling approach for priority and deadline constrained task offloading in homogeneous edge networks. Our scheduling approach consists of two distinct scheduling algorithms, designed to accommodate the differing requirements of high and low priority tasks. To satisfy a task's deadline, our scheduling approach considers the availability of both communication and computational resources in the network when making placements in both the current time-slot and future time-slots. The scheduler implements a deadline-aware preemption mechanism to guarantee resource access to high priority tasks. When low-priority tasks are selected for preemption, the scheduler will attempt to reallocate them if possible before their deadline. We implement this scheduling approach into a task offloading system which we evaluate empirically in the real-world on a network of edge devices composed of four Raspberry Pi 2 Model B's. We evaluate this system under against a version without a task preemption mechanism as well as workstealing approaches to compare the impact on high priority task completion and the ability to complete overall frames. These solutions are evaluated under a workload of 1296 frames. Our findings show that our scheduling approach allows for 99\% of high-priority tasks to complete while also providing a 3 - 8\% increase in the number of frames fully classified end-to-end over both workstealing approaches and systems without a preemption mechanism.

**Link**: [arxiv](http://arxiv.org/abs/2504.16792v1),  [pdf](http://arxiv.org/pdf/2504.16792v1)

**Tags**: cs.DC 



### Inference with Many Weak Instruments and Heterogeneity
**Authors**: Luther Yap

**Updated**: 2025-04-23T15:10:34Z

**Summary**: This paper considers inference in a linear instrumental variable regression model with many potentially weak instruments, in the presence of heterogeneous treatment effects. I first show that existing test procedures, including those that are robust to either weak instruments or heterogeneous treatment effects, can be arbitrarily oversized. I propose a novel and valid test based on a score statistic and a ``leave-three-out" variance estimator. In the presence of heterogeneity and within the class of tests that are functions of the leave-one-out analog of a maximal invariant, this test is asymptotically the uniformly most powerful unbiased test. In two applications to judge and quarter-of-birth instruments, the proposed inference procedure also yields a bounded confidence set while some existing methods yield unbounded or empty confidence sets.

**Link**: [arxiv](http://arxiv.org/abs/2408.11193v3),  [pdf](http://arxiv.org/pdf/2408.11193v3)

**Tags**: econ.EM 



### MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating   Over-Smoothing and Incorporating Outlier Scores
**Authors**: Fengwei Zhou, Jiafei Song, Wenjin Jason Li, Gengjian Xue, Zhikang Zhao, Yichao Lu, Bailin Na

**Updated**: 2025-04-23T15:02:53Z

**Summary**: Recent advances in large language models have significantly improved their ability to process long-context input, but practical applications are challenged by increased inference time and resource consumption, particularly in resource-constrained environments. To address these challenges, we propose MOOSComp, a token-classification-based long-context compression method that enhances the performance of a BERT-based compressor by mitigating the over-smoothing problem and incorporating outlier scores. In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy. During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression. These scores are integrated with the classifier's output, making the compressor more generalizable to various tasks. Superior performance is achieved at various compression ratios on long-context understanding and reasoning benchmarks. Moreover, our method obtains a speedup of 3.3x at a 4x compression ratio on a resource-constrained mobile device.

**Link**: [arxiv](http://arxiv.org/abs/2504.16786v1),  [pdf](http://arxiv.org/pdf/2504.16786v1)

**Tags**: cs.CL cs.LG 



### OSDFace: One-Step Diffusion Model for Face Restoration
**Authors**: Jingkai Wang, Jue Gong, Lin Zhang, Zheng Chen, Xing Liu, Hong Gu, Yutong Liu, Yulun Zhang, Xiaokang Yang

**Updated**: 2025-04-23T14:42:19Z

**Summary**: Diffusion models have demonstrated impressive performance in face restoration. Yet, their multi-step inference process remains computationally intensive, limiting their applicability in real-world scenarios. Moreover, existing methods often struggle to generate face images that are harmonious, realistic, and consistent with the subject's identity. In this work, we propose OSDFace, a novel one-step diffusion model for face restoration. Specifically, we propose a visual representation embedder (VRE) to better capture prior information and understand the input face. In VRE, low-quality faces are processed by a visual tokenizer and subsequently embedded with a vector-quantized dictionary to generate visual prompts. Additionally, we incorporate a facial identity loss derived from face recognition to further ensure identity consistency. We further employ a generative adversarial network (GAN) as a guidance model to encourage distribution alignment between the restored face and the ground truth. Experimental results demonstrate that OSDFace surpasses current state-of-the-art (SOTA) methods in both visual quality and quantitative metrics, generating high-fidelity, natural face images with high identity consistency. The code and model will be released at https://github.com/jkwang28/OSDFace.

**Link**: [arxiv](http://arxiv.org/abs/2411.17163v2),  [pdf](http://arxiv.org/pdf/2411.17163v2)

**Tags**: cs.CV 



### How Effective are Generative Large Language Models in Performing   Requirements Classification?
**Authors**: Waad Alhoshan, Alessio Ferrari, Liping Zhao

**Updated**: 2025-04-23T14:41:11Z

**Summary**: In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.

**Link**: [arxiv](http://arxiv.org/abs/2504.16768v1),  [pdf](http://arxiv.org/pdf/2504.16768v1)

**Tags**: cs.CL cs.AI cs.SE 



### Lightweight Latent Verifiers for Efficient Meta-Generation Strategies
**Authors**: Bartosz Piotrowski, Witold Drzewakowski, Konrad Staniszewski, Piotr Miłoś

**Updated**: 2025-04-23T14:33:20Z

**Summary**: Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16760v1),  [pdf](http://arxiv.org/pdf/2504.16760v1)

**Tags**: cs.AI 



### HEMA : A Hippocampus-Inspired Extended Memory Architecture for   Long-Context AI Conversations
**Authors**: Kwangseob Ahn

**Updated**: 2025-04-23T14:27:12Z

**Summary**: Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.

**Link**: [arxiv](http://arxiv.org/abs/2504.16754v1),  [pdf](http://arxiv.org/pdf/2504.16754v1)

**Tags**: cs.CL cs.AI 



### Right Question is Already Half the Answer: Fully Unsupervised LLM   Reasoning Incentivization
**Authors**: Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian

**Updated**: 2025-04-23T14:25:51Z

**Summary**: While large language models (LLMs) have demonstrated exceptional capabilities in challenging tasks such as mathematical reasoning, existing methods to enhance reasoning ability predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data after pre-training. However, these approaches critically depend on external supervision--such as human-labelled reasoning traces, verified golden answers, or pre-trained reward models--which limits scalability and practical applicability. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. EMPO does not require any supervised information for incentivizing reasoning capabilities (i.e., neither verifiable reasoning traces, problems with golden answers, nor additional pre-trained reward models). By continuously minimizing the predictive entropy of LLMs on unlabeled user queries in a latent semantic space, EMPO enables purely self-supervised evolution of reasoning capabilities with strong flexibility and practicality. Our experiments demonstrate competitive performance of EMPO on both mathematical reasoning and free-form natural reasoning tasks. Specifically, without any supervised signals, \ours boosts the accuracy of Qwen2.5-Math-7B Base from 30.7\% to 48.1\% on mathematical benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1\% to 50.1\% on MMLU-Pro.

**Link**: [arxiv](http://arxiv.org/abs/2504.05812v2),  [pdf](http://arxiv.org/pdf/2504.05812v2)

**Tags**: cs.LG 



### SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from   Unstructured Clinical Narratives in Epilepsy
**Authors**: Meghal Dani, Muthu Jeyanthi Prakash, Zeynep Akata, Stefanie Liebe

**Updated**: 2025-04-23T14:25:46Z

**Summary**: Large Language Models (LLMs) have been shown to encode clinical knowledge. Many evaluations, however, rely on structured question-answer benchmarks, overlooking critical challenges of interpreting and reasoning about unstructured clinical narratives in real-world settings. Using free-text clinical descriptions, we present SemioLLM, an evaluation framework that benchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of 1,269 seizure descriptions, we show that most LLMs are able to accurately and confidently generate probabilistic predictions of seizure onset zones in the brain. Most models approach clinician-level performance after prompt engineering, with expert-guided chain-of-thought reasoning leading to the most consistent improvements. Performance was further strongly modulated by clinical in-context impersonation, narrative length and language context (13.7%, 32.7% and 14.2% performance variation, respectively). However, expert analysis of reasoning outputs revealed that correct prediction can be based on hallucinated knowledge and deficient source citation accuracy, underscoring the need to improve interpretability of LLMs in clinical use. Overall, SemioLLM provides a scalable, domain-adaptable framework for evaluating LLMs in clinical disciplines where unstructured verbal descriptions encode diagnostic information. By identifying both the strengths and limitations of state-of-the-art models, our work supports the development of clinically robust and globally applicable AI systems for healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2407.03004v2),  [pdf](http://arxiv.org/pdf/2407.03004v2)

**Tags**: cs.CL cs.AI 



### A Survey of AI Agent Protocols
**Authors**: Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, Weinan Zhang

**Updated**: 2025-04-23T14:07:26Z

**Summary**: The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide a systematic overview of existing communication protocols for LLM agents. We classify them into four main categories and make an analysis to help users and developers select the most suitable protocols for specific applications. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore future challenges, such as how protocols can adapt and survive in fast-evolving environments, and what qualities future protocols might need to support the next generation of LLM agent ecosystems. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.

**Link**: [arxiv](http://arxiv.org/abs/2504.16736v1),  [pdf](http://arxiv.org/pdf/2504.16736v1)

**Tags**: cs.AI 



### IRIS: Interactive Research Ideation System for Accelerating Scientific   Discovery
**Authors**: Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan

**Updated**: 2025-04-23T14:01:36Z

**Summary**: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

**Link**: [arxiv](http://arxiv.org/abs/2504.16728v1),  [pdf](http://arxiv.org/pdf/2504.16728v1)

**Tags**: cs.AI cs.CL 



### MEC Task Offloading in AIoT: A User-Centric DRL Model Splitting   Inference Scheme
**Authors**: Weixi Li, Rongzuo Guo, Yuning Wang, Fangying Chen

**Updated**: 2025-04-23T14:01:36Z

**Summary**: With the rapid development of the Artificial Intelligence of Things (AIoT), mobile edge computing (MEC) becomes an essential technology underpinning AIoT applications. However, multi-angle resource constraints, multi-user task competition, and the complexity of task offloading decisions in dynamic MEC environments present new technical challenges. Therefore, a user-centric deep reinforcement learning (DRL) model splitting inference scheme is proposed to address the problem. This scheme combines model splitting inference technology and designs a UCMS_MADDPG-based offloading algorithm to realize efficient model splitting inference responses in the dynamic MEC environment with multi-angle resource constraints. Specifically, we formulate a joint optimization problem that integrates resource allocation, server selection, and task offloading, aiming to minimize the weighted sum of task execution delay and energy consumption. We also introduce a user-server co-selection algorithm to address the selection issue between users and servers. Furthermore, we design an algorithm centered on user pre-decision to coordinate the outputs of continuous and discrete hybrid decisions, and introduce a priority sampling mechanism based on reward-error trade-off to optimize the experience replay mechanism of the network. Simulation results show that the proposed UCMS_MADDPG-based offloading algorithm demonstrates superior overall performance compared with other benchmark algorithms in dynamic environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.16729v1),  [pdf](http://arxiv.org/pdf/2504.16729v1)

**Tags**: cs.NI 



### Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical   Question Answering with Small-Scale LLMs
**Authors**: Yingjian Chen, Feiyang Li, Xingyu Song, Tianxiao Li, Zixin Xu, Xiujie Chen, Issey Sukeda, Irene Li

**Updated**: 2025-04-23T13:54:01Z

**Summary**: Large language models (LLMs) perform well in medical QA, but their effectiveness in Japanese contexts is limited due to privacy constraints that prevent the use of commercial models like GPT-4 in clinical settings. As a result, recent efforts focus on instruction-tuning open-source LLMs, though the potential of combining them with retrieval-augmented generation (RAG) remains underexplored. To bridge this gap, we are the first to explore a knowledge graph-based (KG) RAG framework for Japanese medical QA small-scale open-source LLMs. Experimental results show that KG-based RAG has only a limited impact on Japanese medical QA using small-scale open-source LLMs. Further case studies reveal that the effectiveness of the RAG is sensitive to the quality and relevance of the external retrieved content. These findings offer valuable insights into the challenges and potential of applying RAG in Japanese medical QA, while also serving as a reference for other low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.10982v4),  [pdf](http://arxiv.org/pdf/2504.10982v4)

**Tags**: cs.CL cs.AI 



### CF-CAM: Cluster Filter Class Activation Mapping for Reliable   Gradient-Based Interpretability
**Authors**: Hongjie He, Xu Pan, Yudong Yao

**Updated**: 2025-04-23T13:49:48Z

**Summary**: As deep learning continues to advance, the transparency of neural network decision-making remains a critical challenge, limiting trust and applicability in high-stakes domains. Class Activation Mapping (CAM) techniques have emerged as a key approach toward visualizing model decisions, yet existing methods face inherent trade-offs. Gradient-based CAM variants suffer from sensitivity to gradient perturbations due to gradient noise, leading to unstable and unreliable explanations. Conversely, gradient-free approaches mitigate gradient instability but incur significant computational overhead and inference latency. To address these limitations, we propose a Cluster Filter Class Activation Map (CF-CAM) technique, a novel framework that reintroduces gradient-based weighting while enhancing robustness against gradient noise. CF-CAM utilizes hierarchical importance weighting strategy to balance discriminative feature preservation and noise elimination. A density-aware channel clustering method via Density-Based Spatial Clustering of Applications with Noise (DBSCAN) groups semantically relevant feature channels and discard noise-prone activations. Additionally, cluster-conditioned gradient filtering leverages Gaussian filters to refine gradient signals, preserving edge-aware localization while suppressing noise impact. Experiment results demonstrate that CF-CAM achieves superior interpretability performance while enhancing computational efficiency, outperforming state-of-the-art CAM methods in faithfulness and robustness. By effectively mitigating gradient instability without excessive computational cost, CF-CAM provides a competitive solution for enhancing the interpretability of deep neural networks in critical applications such as autonomous driving and medical diagnosis.

**Link**: [arxiv](http://arxiv.org/abs/2504.00060v2),  [pdf](http://arxiv.org/pdf/2504.00060v2)

**Tags**: cs.LG cs.AI cs.CV 



### From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era   of LLMs
**Authors**: Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, Yong Liu

**Updated**: 2025-04-23T13:47:27Z

**Summary**: Memory is the process of encoding, storing, and retrieving information, allowing humans to retain experiences, knowledge, skills, and facts over time, and serving as the foundation for growth and effective interaction with the world. It plays a crucial role in shaping our identity, making decisions, learning from past experiences, building relationships, and adapting to changes. In the era of large language models (LLMs), memory refers to the ability of an AI system to retain, recall, and use information from past interactions to improve future responses and interactions. Although previous research and reviews have provided detailed descriptions of memory mechanisms, there is still a lack of a systematic review that summarizes and analyzes the relationship between the memory of LLM-driven AI systems and human memory, as well as how we can be inspired by human memory to construct more powerful memory systems. To achieve this, in this paper, we propose a comprehensive survey on the memory of LLM-driven AI systems. In particular, we first conduct a detailed analysis of the categories of human memory and relate them to the memory of AI systems. Second, we systematically organize existing memory-related work and propose a categorization method based on three dimensions (object, form, and time) and eight quadrants. Finally, we illustrate some open problems regarding the memory of current AI systems and outline possible future directions for memory in the era of large language models.

**Link**: [arxiv](http://arxiv.org/abs/2504.15965v2),  [pdf](http://arxiv.org/pdf/2504.15965v2)

**Tags**: cs.IR H.0 



### Random walks on random networks of cliques: Inferring the network   structure
**Authors**: Albano Nannini, Damián Zanette

**Updated**: 2025-04-23T13:45:34Z

**Summary**: We study the properties of discrete-time random walks on networks formed by randomly interconnected cliques, namely, random networks of cliques. Our purpose is to derive the parameters that define the network structure -- specifically, the distribution of clique size and the abundance of inter-clique links -- from the observation of selected statistical features along the random walk. To this end, we apply a Bayesian approach based on recording the times spent by the walker inside successively visited cliques. The procedure is illustrated with some numerical examples of diverse complexity, where the relevant structural parameters are successfully recovered.

**Link**: [arxiv](http://arxiv.org/abs/2504.16717v1),  [pdf](http://arxiv.org/pdf/2504.16717v1)

**Tags**: cond-mat.stat-mech 



### Surface to Seafloor: A Generative AI Framework for Decoding the Ocean   Interior State
**Authors**: Andre N. Souza, Simone Silvestri, Katherine Deck, Tobias Bischoff, Raffaele Ferrari, Glenn R. Flierl

**Updated**: 2025-04-23T13:36:33Z

**Summary**: Understanding subsurface ocean dynamics is essential for quantifying oceanic heat and mass transport, but direct observations at depth remain sparse due to logistical and technological constraints. In contrast, satellite missions provide rich surface datasets-such as sea surface height, temperature, and salinity-that offer indirect but potentially powerful constraints on the ocean interior. Here, we present a probabilistic framework based on score-based diffusion models to reconstruct three-dimensional subsurface velocity and buoyancy fields, including the energetic ocean eddy field, from surface observations. Using a 15-level primitive equation simulation of an idealized double-gyre system, we evaluate the skill of the model in inferring the mean circulation and the mesoscale variability at depth under varying levels of surface information. We find that the generative model successfully recovers key dynamical structures and provides physically meaningful uncertainty estimates, with predictive skill diminishing systematically as the surface resolution decreases or the inference depth increases. These results demonstrate the potential of generative approaches for ocean state estimation and uncertainty quantification, particularly in regimes where traditional deterministic methods are underconstrained or ill-posed.

**Link**: [arxiv](http://arxiv.org/abs/2504.15308v2),  [pdf](http://arxiv.org/pdf/2504.15308v2)

**Tags**: physics.geo-ph 



### Importance sampling-based gradient method for dimension reduction in   Poisson log-normal model
**Authors**: Bastien Batardière, Julien Chiquet, Joon Kwon, Julien Stoehr

**Updated**: 2025-04-23T13:25:10Z

**Summary**: High-dimensional count data poses significant challenges for statistical analysis, necessitating effective methods that also preserve explainability. We focus on a low rank constrained variant of the Poisson log-normal model, which relates the observed data to a latent low-dimensional multivariate Gaussian variable via a Poisson distribution. Variational inference methods have become a golden standard solution to infer such a model. While computationally efficient, they usually lack theoretical statistical properties with respect to the model. To address this issue we propose a projected stochastic gradient scheme that directly maximizes the log-likelihood. We prove the convergence of the proposed method when using importance sampling for estimating the gradient. Specifically, we obtain a rate of convergence of $O(T^{-1/2} + N^{-1})$ with $T$ the number of iterations and $N$ the number of Monte Carlo draws. The latter follows from a novel descent lemma for non convex $L$-smooth objective functions, and random biased gradient estimate. We also demonstrate numerically the efficiency of our solution compared to its variational competitor. Our method not only scales with respect to the number of observed samples but also provides access to the desirable properties of the maximum likelihood estimator.

**Link**: [arxiv](http://arxiv.org/abs/2410.00476v4),  [pdf](http://arxiv.org/pdf/2410.00476v4)

**Tags**: math.OC 



### Rethinking Vision Transformer for Large-Scale Fine-Grained Image   Retrieval
**Authors**: Xin Jiang, Hao Tang, Yonghua Pan, Zechao Li

**Updated**: 2025-04-23T13:23:56Z

**Summary**: Large-scale fine-grained image retrieval (FGIR) aims to retrieve images belonging to the same subcategory as a given query by capturing subtle differences in a large-scale setting. Recently, Vision Transformers (ViT) have been employed in FGIR due to their powerful self-attention mechanism for modeling long-range dependencies. However, most Transformer-based methods focus primarily on leveraging self-attention to distinguish fine-grained details, while overlooking the high computational complexity and redundant dependencies inherent to these models, limiting their scalability and effectiveness in large-scale FGIR. In this paper, we propose an Efficient and Effective ViT-based framework, termed \textbf{EET}, which integrates token pruning module with a discriminative transfer strategy to address these limitations. Specifically, we introduce a content-based token pruning scheme to enhance the efficiency of the vanilla ViT, progressively removing background or low-discriminative tokens at different stages by exploiting feature responses and self-attention mechanism. To ensure the resulting efficient ViT retains strong discriminative power, we further present a discriminative transfer strategy comprising both \textit{discriminative knowledge transfer} and \textit{discriminative region guidance}. Using a distillation paradigm, these components transfer knowledge from a larger ``teacher'' ViT to a more efficient ``student'' model, guiding the latter to focus on subtle yet crucial regions in a cost-free manner. Extensive experiments on two widely-used fine-grained datasets and four large-scale fine-grained datasets demonstrate the effectiveness of our method. Specifically, EET reduces the inference latency of ViT-Small by 42.7\% and boosts the retrieval performance of 16-bit hash codes by 5.15\% on the challenging NABirds dataset.

**Link**: [arxiv](http://arxiv.org/abs/2504.16691v1),  [pdf](http://arxiv.org/pdf/2504.16691v1)

**Tags**: cs.MM 



### Synthetic Lyrics Detection Across Languages and Genres
**Authors**: Yanis Labrak, Markus Frohmann, Gabriel Meseguer-Brocal, Elena V. Epure

**Updated**: 2025-04-24T07:21:44Z

**Summary**: In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.

**Link**: [arxiv](http://arxiv.org/abs/2406.15231v4),  [pdf](http://arxiv.org/pdf/2406.15231v4)

**Tags**: cs.CL cs.AI cs.LG 



### MCMC for Bayesian estimation of Differential Privacy from Membership   Inference Attacks
**Authors**: Ceren Yildirim, Kamer Kaya, Sinan Yildirim, Erkay Savas

**Updated**: 2025-04-23T13:10:37Z

**Summary**: We propose a new framework for Bayesian estimation of differential privacy, incorporating evidence from multiple membership inference attacks (MIA). Bayesian estimation is carried out via a Markov chain Monte Carlo (MCMC) algorithm, named MCMC-DP-Est, which provides an estimate of the full posterior distribution of the privacy parameter (e.g., instead of just credible intervals). Critically, the proposed method does not assume that privacy auditing is performed with the most powerful attack on the worst-case (dataset, challenge point) pair, which is typically unrealistic. Instead, MCMC-DP-Est jointly estimates the strengths of MIAs used and the privacy of the training algorithm, yielding a more cautious privacy analysis. We also present an economical way to generate measurements for the performance of an MIA that is to be used by the MCMC method to estimate privacy. We present the use of the methods with numerical examples with both artificial and real data.

**Link**: [arxiv](http://arxiv.org/abs/2504.16683v1),  [pdf](http://arxiv.org/pdf/2504.16683v1)

**Tags**: cs.LG stat.ML 



### NovelQA: Benchmarking Question Answering on Documents Exceeding 200K   Tokens
**Authors**: Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, Yue Zhang

**Updated**: 2025-04-23T12:52:18Z

**Summary**: Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.

**Link**: [arxiv](http://arxiv.org/abs/2403.12766v3),  [pdf](http://arxiv.org/pdf/2403.12766v3)

**Tags**: cs.CL 



### LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative   Analysis
**Authors**: Joel Oksanen, Andrés Lucero, Perttu Hämäläinen

**Updated**: 2025-04-23T12:39:06Z

**Summary**: The use of large language models (LLMs) in qualitative analysis offers enhanced efficiency but raises questions about their alignment with the contextual nature of research for design (RfD). This research examines the trustworthiness of LLM-driven design insights, using qualitative coding as a case study to explore the interpretive processes central to RfD. We introduce LLMCode, an open-source tool integrating two metrics, namely Intersection over Union (IoU) and Modified Hausdorff Distance, to assess the alignment between human and LLM-generated insights. Across two studies involving 26 designers, we find that while the model performs well with deductive coding, its ability to emulate a designer's deeper interpretive lens over the data is limited, emphasising the importance of human-AI collaboration. Our results highlight a reciprocal dynamic where users refine LLM outputs and adapt their own perspectives based on the model's suggestions. These findings underscore the importance of fostering appropriate reliance on LLMs by designing tools that preserve interpretive depth while facilitating intuitive collaboration between designers and AI.

**Link**: [arxiv](http://arxiv.org/abs/2504.16671v1),  [pdf](http://arxiv.org/pdf/2504.16671v1)

**Tags**: cs.HC 



### Binarity at LOw Metallicity (BLOeM): Bayesian inference of natal kicks   from inert black hole binaries
**Authors**: R. Willcox, P. Marchant, A. Vigna-Gómez, H. Sana, J. Bodensteiner, K. Deshmukh, M. Esseldeurs, M. Fabry, V. Hénault-Brunet, S. Janssens, L. Mahy, L. Patrick, D. Pauli, M. Renzo, A. A. C. Sander, T. Shenar, L. A. C. van Son, M. Stoop

**Updated**: 2025-04-23T12:36:32Z

**Summary**: Context. The emerging population of inert black hole binaries (BHBs) provides a unique opportunity to constrain black hole (BH) formation physics. These systems are composed of a stellar-mass BH in a wide orbit around a non-degenerate star with no observed Xray emission. Inert BHBs allow for narrow constraints to be inferred on the natal kick and mass loss during BH-forming core-collapse events. Aims. In anticipation of the upcoming BLOeM survey, we aim to provide tight constraints on BH natal kicks by exploiting the full parameter space obtained from combined spectroscopic and astrometric data to characterize the orbits of inert BHBs. Multi-epoch spectroscopy from the BLOeM project will provide measurements of periods, eccentricities, and radial velocities for inert BHBs in the SMC, which complements Gaia astrometric observations of proper motions. Methods. We present a Bayesian parameter estimation framework to infer natal kicks and mass loss during core-collapse from inert BHBs, accounting for all available observables, including the systemic velocity and its orientation relative to the orbital plane. The framework further allows for circumstances when some of the observables are unavailable, such as for the distant BLOeM sources which preclude resolved orbits. Results. With our new framework, we are able to distinguish between BH formation channels, even in the absence of a resolved orbit. In cases when the pre-explosion orbit can be assumed to be circular, we precisely recover the parameters of the core-collapse, highlighting the importance of understanding the eccentricity landscape of pre-explosion binaries, both theoretically and observationally. Treating the near-circular, inert BHB, VFTS 243, as a representative of the anticipated BLOeM systems, we constrain the natal kick to less than 27 km/s and the mass loss to less than 2.9 Msun within a 90% credible interval.

**Link**: [arxiv](http://arxiv.org/abs/2504.16669v1),  [pdf](http://arxiv.org/pdf/2504.16669v1)

**Tags**: astro-ph.SR astro-ph.GA astro-ph.HE 



### SEGA: Drivable 3D Gaussian Head Avatar from a Single Image
**Authors**: Chen Guo, Zhuo Su, Jian Wang, Shuang Li, Xu Chang, Zhaohu Li, Yang Zhao, Guidong Wang, Ruqi Huang

**Updated**: 2025-04-23T12:27:03Z

**Summary**: Creating photorealistic 3D head avatars from limited input has become increasingly important for applications in virtual reality, telepresence, and digital entertainment. While recent advances like neural rendering and 3D Gaussian splatting have enabled high-quality digital human avatar creation and animation, most methods rely on multiple images or multi-view inputs, limiting their practicality for real-world use. In this paper, we propose SEGA, a novel approach for Single-imagE-based 3D drivable Gaussian head Avatar creation that combines generalized prior models with a new hierarchical UV-space Gaussian Splatting framework. SEGA seamlessly combines priors derived from large-scale 2D datasets with 3D priors learned from multi-view, multi-expression, and multi-ID data, achieving robust generalization to unseen identities while ensuring 3D consistency across novel viewpoints and expressions. We further present a hierarchical UV-space Gaussian Splatting framework that leverages FLAME-based structural priors and employs a dual-branch architecture to disentangle dynamic and static facial components effectively. The dynamic branch encodes expression-driven fine details, while the static branch focuses on expression-invariant regions, enabling efficient parameter inference and precomputation. This design maximizes the utility of limited 3D data and achieves real-time performance for animation and rendering. Additionally, SEGA performs person-specific fine-tuning to further enhance the fidelity and realism of the generated avatars. Experiments show our method outperforms state-of-the-art approaches in generalization ability, identity preservation, and expression realism, advancing one-shot avatar creation for practical applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.14373v2),  [pdf](http://arxiv.org/pdf/2504.14373v2)

**Tags**: cs.GR cs.CV 



### Optimal Estimation and Uncertainty Quantification for Stochastic Inverse   Problems via Variational Bayesian Methods
**Authors**: Ruibiao Song, Liying Zhang

**Updated**: 2025-04-23T12:15:48Z

**Summary**: The Bayesian inversion method demonstrates significant potential for solving inverse problems, enabling both point estimation and uncertainty quantification. However, Bayesian maximum a posteriori (MAP) estimation may become unstable when handling data from diverse distributions (e.g., solutions of stochastic partial differential equations (SPDEs)). Additionally, Monte Carlo sampling methods are computationally expensive. To address these challenges, we propose a novel two-stage optimization method based on optimal control theory and variational Bayesian methods. This method not only achieves stable solutions for stochastic inverse problems but also efficiently quantifies the uncertainty of the solutions. In the first stage, we introduce a new weighting formulation to ensure the stability of the Bayesian MAP estimation. In the second stage, we derive the necessary condition to efficiently quantify the uncertainty of the solutions, by combining the new weighting formula with variational inference. Furthermore, we establish an error estimation theorem that relates the exact solution to the optimally estimated solution under different amounts of observed data. Finally, the efficiency of the proposed method is demonstrated through numerical examples.

**Link**: [arxiv](http://arxiv.org/abs/2503.10199v6),  [pdf](http://arxiv.org/pdf/2503.10199v6)

**Tags**: math.NA cs.NA 



### Expected Free Energy-based Planning as Variational Inference
**Authors**: Bert de Vries, Wouter Nuijten, Thijs van de Laar, Wouter Kouw, Sepideh Adamiat, Tim Nisslbeck, Mykola Lukashchuk, Hoang Minh Huu Nguyen, Marco Hidalgo Araya, Raphael Tresor, Thijs Jenneskens, Ivana Nikoloska, Raaja Ganapathy Subramanian, Bart van Erp, Dmitry Bagaev, Albert Podusenko

**Updated**: 2025-04-23T12:05:19Z

**Summary**: We address the problem of planning under uncertainty, where an agent must choose actions that not only achieve desired outcomes but also reduce uncertainty. Traditional methods often treat exploration and exploitation as separate objectives, lacking a unified inferential foundation. Active inference, grounded in the Free Energy Principle, provides such a foundation by minimizing Expected Free Energy (EFE), a cost function that combines utility with epistemic drives, such as ambiguity resolution and novelty seeking. However, the computational burden of EFE minimization had remained a significant obstacle to its scalability. In this paper, we show that EFE-based planning arises naturally from minimizing a variational free energy functional on a generative model augmented with preference and epistemic priors. This result reinforces theoretical consistency with the Free Energy Principle by casting planning under uncertainty itself as a form of variational inference. Our formulation yields policies that jointly support goal achievement and information gain, while incorporating a complexity term that accounts for bounded computational resources. This unifying framework connects and extends existing methods, enabling scalable, resource-aware implementations of active inference agents.

**Link**: [arxiv](http://arxiv.org/abs/2504.14898v2),  [pdf](http://arxiv.org/pdf/2504.14898v2)

**Tags**: stat.ML cs.LG 



### ParetoHqD: Fast Offline Multiobjective Alignment of Large Language   Models using Pareto High-quality Data
**Authors**: Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin

**Updated**: 2025-04-23T11:35:57Z

**Summary**: Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.16628v1),  [pdf](http://arxiv.org/pdf/2504.16628v1)

**Tags**: cs.LG cs.CL 



### TIFIN India at SemEval-2025: Harnessing Translation to Overcome   Multilingual IR Challenges in Fact-Checked Claim Retrieval
**Authors**: Prasanna Devadiga, Arya Suneesh, Pawan Kumar Rajpoot, Bharatdeep Hazarika, Aditya U Baliga

**Updated**: 2025-04-23T11:34:35Z

**Summary**: We address the challenge of retrieving previously fact-checked claims in monolingual and crosslingual settings - a critical task given the global prevalence of disinformation. Our approach follows a two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker. Our key contribution is demonstrating how LLM-based translation can overcome the hurdles of multilingual information retrieval. Additionally, we focus on ensuring that the bulk of the pipeline can be replicated on a consumer GPU. Our final integrated system achieved a success@10 score of 0.938 and 0.81025 on the monolingual and crosslingual test sets, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2504.16627v1),  [pdf](http://arxiv.org/pdf/2504.16627v1)

**Tags**: cs.CL 



### Compositional Active Learning of Synchronous Systems through Automated   Alphabet Refinement
**Authors**: Leo Henry, Thomas Neele, Mohammad Mousavi, Matteo Sammartino

**Updated**: 2025-04-23T11:30:01Z

**Summary**: Active automata learning infers automaton models of systems from behavioral observations, a technique successfully applied to a wide range of domains. Compositional approaches for concurrent systems have recently emerged. We take a significant step beyond available results, including those by the authors, and develop a general technique for compositional learning of a synchronizing parallel system with an unknown decomposition. Our approach automatically refines the global alphabet into component alphabets while learning the component models. We develop a theoretical treatment of distributions of alphabets, i.e., sets of possibly overlapping component alphabets. We characterize counter-examples that reveal inconsistencies with global observations, and show how to systematically update the distribution to restore consistency. We present a compositional learning algorithm implementing these ideas, where learning counterexamples precisely correspond to distribution counterexamples under well-defined conditions. We provide an implementation, called CoalA, using the state-of-the-art active learning library LearnLib. Our experiments show that in more than 630 subject systems, CoalA delivers orders of magnitude improvements (up to five orders) in membership queries and in systems with significant concurrency, it also achieves better scalability in the number of equivalence queries.

**Link**: [arxiv](http://arxiv.org/abs/2504.16624v1),  [pdf](http://arxiv.org/pdf/2504.16624v1)

**Tags**: cs.LG cs.FL 



### Censored lifespans in a double-truncated sample: Maximum likelihood   inference for the exponential distribution
**Authors**: Fiete Sieg, Anne-Marie Toparkus, Rafael Weissbach

**Updated**: 2025-04-23T11:29:31Z

**Summary**: The analysis of a truncated sample can be hindered by censoring. Survival information may be lost to follow-up or the birthdate may be missing. The data can still be modeled as a truncated point process and it is close to a Poisson process, in the Hellinger distance, as long as the sample is small relative to the population. We assume an exponential distribution for the lifespan, derive the likelihood and profile out the unobservable sample size. Identification of the exponential parameter is shown, together with consistency and asymptotic normality of its M-estimator. Even though the estimator sequence is indexed in the sample size, both the point estimator and the standard error are observable. Enterprise lifespans in Germany constitute our example.

**Link**: [arxiv](http://arxiv.org/abs/2504.16623v1),  [pdf](http://arxiv.org/pdf/2504.16623v1)

**Tags**: stat.ME 62N01, 62D10, 60G55 G.3; I.5.2 



### Leveraging LLMs for User Stories in AI Systems: UStAI Dataset
**Authors**: Asma Yamani, Malak Baslyman, Moataz Ahmed

**Updated**: 2025-04-23T11:26:49Z

**Summary**: AI systems are gaining widespread adoption across various sectors and domains. Creating high-quality AI system requirements is crucial for aligning the AI system with business goals and consumer values and for social responsibility. However, with the uncertain nature of AI systems and the heavy reliance on sensitive data, more research is needed to address the elicitation and analysis of AI systems requirements. With the proprietary nature of many AI systems, there is a lack of open-source requirements artifacts and technical requirements documents for AI systems, limiting broader research and investigation. With Large Language Models (LLMs) emerging as a promising alternative to human-generated text, this paper investigates the potential use of LLMs to generate user stories for AI systems based on abstracts from scholarly papers. We conducted an empirical evaluation using three LLMs and generated $1260$ user stories from $42$ abstracts from $26$ domains. We assess their quality using the Quality User Story (QUS) framework. Moreover, we identify relevant non-functional requirements (NFRs) and ethical principles. Our analysis demonstrates that the investigated LLMs can generate user stories inspired by the needs of various stakeholders, offering a promising approach for generating user stories for research purposes and for aiding in the early requirements elicitation phase of AI systems. We have compiled and curated a collection of stories generated by various LLMs into a dataset (UStAI), which is now publicly available for use.

**Link**: [arxiv](http://arxiv.org/abs/2504.00513v2),  [pdf](http://arxiv.org/pdf/2504.00513v2)

**Tags**: cs.SE cs.AI 



### Cognitive Silicon: An Architectural Blueprint for Post-Industrial   Computing Systems
**Authors**: Christoforus Yoga Haryanto, Emily Lomempow

**Updated**: 2025-04-23T11:24:30Z

**Summary**: Autonomous AI systems reveal foundational limitations in deterministic, human-authored computing architectures. This paper presents Cognitive Silicon: a hypothetical full-stack architectural framework projected toward 2035, exploring a possible trajectory for cognitive computing system design. The proposed architecture would integrate symbolic scaffolding, governed memory, runtime moral coherence, and alignment-aware execution across silicon-to-semantics layers. Our design grammar has emerged from dialectical co-design with LLMs under asymmetric epistemic conditions--creating structured friction to expose blind spots and trade-offs. The envisioned framework would establish mortality as a natural consequence of physical constraints, non-copyable tacit knowledge, and non-cloneable identity keys as cognitive-embodiment primitives. Core tensions (trust/agency, scaffolding/emergence, execution/governance) would function as central architectural pressures rather than edge cases. The architecture theoretically converges with the Free Energy Principle, potentially offering a formal account of how cognitive systems could maintain identity through prediction error minimization across physical and computational boundaries. The resulting framework aims to deliver a morally tractable cognitive infrastructure that could maintain human-alignment through irreversible hardware constraints and identity-bound epistemic mechanisms resistant to replication or subversion.

**Link**: [arxiv](http://arxiv.org/abs/2504.16622v1),  [pdf](http://arxiv.org/pdf/2504.16622v1)

**Tags**: cs.AI cs.CY 



### Algorithmic Mirror: Designing an Interactive Tool to Promote   Self-Reflection for YouTube Recommendations
**Authors**: Yui Kondo, Kevin Dunnell, Qing Xiao, Jun Zhao, Luc Rocher

**Updated**: 2025-04-23T11:00:17Z

**Summary**: Big Data analytics and Artificial Intelligence systems derive non-intuitive and often unverifiable inferences about individuals' behaviors, preferences, and private lives. Drawing on diverse, feature-rich datasets of unpredictable value, these systems erode the intuitive connection between our actions and how we are perceived, diminishing control over our digital identities. While Explainable Artificial Intelligence scholars have attempted to explain the inner workings of algorithms, their visualizations frequently overwhelm end-users with complexity. This research introduces 'hypothetical inference', a novel approach that uses language models to simulate how algorithms might interpret users' digital footprints and infer personal characteristics without requiring access to proprietary platform algorithms. Through empirical studies with fourteen adult participants, we identified three key design opportunities to foster critical algorithmic literacy: (1) reassembling scattered digital footprints into a unified map, (2) simulating algorithmic inference through LLM-generated interpretations, and (3) incorporating temporal dimensions to visualize evolving patterns. This research lays the groundwork for tools that can help users recognize the influence of data on platforms and develop greater autonomy in increasingly algorithm-mediated digital environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.16615v1),  [pdf](http://arxiv.org/pdf/2504.16615v1)

**Tags**: cs.HC 



### Reinforcement Learning With LLMs Interaction For Distributed Diffusion   Model Services
**Authors**: Hongyang Du, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shuguang Cui, Xuemin Shen, Dong In Kim

**Updated**: 2025-04-23T11:00:16Z

**Summary**: Distributed Artificial Intelligence-Generated Content (AIGC) has attracted significant attention, but two key challenges remain: maximizing subjective Quality of Experience (QoE) and improving energy efficiency, which are particularly pronounced in widely adopted Generative Diffusion Model (GDM)-based image generation services. In this paper, we propose a novel user-centric Interactive AI (IAI) approach for service management, with a distributed GDM-based AIGC framework that emphasizes efficient and cooperative deployment. The proposed method restructures the GDM inference process by allowing users with semantically similar prompts to share parts of the denoising chain. Furthermore, to maximize the users' subjective QoE, we propose an IAI approach, i.e., Reinforcement Learning With Large Language Models Interaction (RLLI), which utilizes Large Language Model (LLM)-empowered generative agents to replicate user interaction, providing real-time and subjective QoE feedback aligned with diverse user personalities. Lastly, we present the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm, adapted to the proposed RLLI framework, to allocate communication and computing resources effectively while accounting for subjective user traits and dynamic wireless conditions. Simulation results demonstrate that G-DDPG improves total QoE by 15% compared with the standard DDPG algorithm.

**Link**: [arxiv](http://arxiv.org/abs/2311.11094v2),  [pdf](http://arxiv.org/pdf/2311.11094v2)

**Tags**: cs.NI 



### Synergy: Towards On-Body AI via Tiny AI Accelerator Collaboration on   Wearables
**Authors**: Taesik Gong, Si Young Jang, Utku Günay Acer, Fahim Kawsar, Chulhong Min

**Updated**: 2025-04-23T10:44:56Z

**Summary**: The advent of tiny artificial intelligence (AI) accelerators enables AI to run at the extreme edge, offering reduced latency, lower power cost, and improved privacy. When integrated into wearable devices, these accelerators open exciting opportunities, allowing various AI apps to run directly on the body. We present Synergy that provides AI apps with best-effort performance via system-driven holistic collaboration over AI accelerator-equipped wearables. To achieve this, Synergy provides device-agnostic programming interfaces to AI apps, giving the system visibility and controllability over the app's resource use. Then, Synergy maximizes the inference throughput of concurrent AI models by creating various execution plans for each app considering AI accelerator availability and intelligently selecting the best set of execution plans. Synergy further improves throughput by leveraging parallelization opportunities over multiple computation units. Our evaluations with 7 baselines and 8 models demonstrate that, on average, Synergy achieves a 23.0 times improvement in throughput, while reducing latency by 73.9% and power consumption by 15.8%, compared to the baselines.

**Link**: [arxiv](http://arxiv.org/abs/2401.08637v3),  [pdf](http://arxiv.org/pdf/2401.08637v3)

**Tags**: cs.DC cs.LG 



### Bayesian Nonparametric Inference in Elliptic PDEs: Convergence Rates and   Implementation
**Authors**: Matteo Giordano

**Updated**: 2025-04-23T10:33:46Z

**Summary**: Parameter identification problems in partial differential equations (PDEs) consist in determining one or more functional coefficient in a PDE. In this article, the Bayesian nonparametric approach to such problems is considered. Focusing on the representative example of inferring the diffusivity function in an elliptic PDE from noisy observations of the PDE solution, the performance of Bayesian procedures based on Gaussian process priors is investigated. Building on recent developments in the literature, we derive novel asymptotic theoretical guarantees that establish posterior consistency and convergence rates for methodologically attractive Gaussian series priors based on the Dirichlet-Laplacian eigenbasis. An implementation of the associated posterior-based inference is provided and illustrated via a numerical simulation study, where excellent agreement with the theory is obtained.

**Link**: [arxiv](http://arxiv.org/abs/2311.18322v2),  [pdf](http://arxiv.org/pdf/2311.18322v2)

**Tags**: math.ST stat.TH 



### Debunking with Dialogue? Exploring AI-Generated Counterspeech to   Challenge Conspiracy Theories
**Authors**: Mareike Lisker, Christina Gottschalk, Helena Mihaljević

**Updated**: 2025-04-23T10:32:45Z

**Summary**: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.

**Link**: [arxiv](http://arxiv.org/abs/2504.16604v1),  [pdf](http://arxiv.org/pdf/2504.16604v1)

**Tags**: cs.CL cs.AI cs.SI I.2.7 



### Comparing Large Language Models and Traditional Machine Translation   Tools for Translating Medical Consultation Summaries: A Pilot Study
**Authors**: Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon

**Updated**: 2025-04-23T10:31:33Z

**Summary**: This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.

**Link**: [arxiv](http://arxiv.org/abs/2504.16601v1),  [pdf](http://arxiv.org/pdf/2504.16601v1)

**Tags**: cs.CL cs.AI 



### Advances in Embodied Navigation Using Large Language Models: A Survey
**Authors**: Jinzhou Lin, Han Gao, Xuxiang Feng, Rongtao Xu, Changwei Wang, Man Zhang, Li Guo, Shibiao Xu

**Updated**: 2025-04-23T10:26:16Z

**Summary**: In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN.

**Link**: [arxiv](http://arxiv.org/abs/2311.00530v5),  [pdf](http://arxiv.org/pdf/2311.00530v5)

**Tags**: cs.AI 



### Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization
**Authors**: Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma

**Updated**: 2025-04-23T10:12:52Z

**Summary**: Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model's ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2504.13460v2),  [pdf](http://arxiv.org/pdf/2504.13460v2)

**Tags**: cs.CV cs.AI 



### FREAK: Frequency-modulated High-fidelity and Real-time Audio-driven   Talking Portrait Synthesis
**Authors**: Ziqi Ni, Ao Fu, Yi Zhou

**Updated**: 2025-04-23T10:10:57Z

**Summary**: Achieving high-fidelity lip-speech synchronization in audio-driven talking portrait synthesis remains challenging. While multi-stage pipelines or diffusion models yield high-quality results, they suffer from high computational costs. Some approaches perform well on specific individuals with low resources, yet still exhibit mismatched lip movements. The aforementioned methods are modeled in the pixel domain. We observed that there are noticeable discrepancies in the frequency domain between the synthesized talking videos and natural videos. Currently, no research on talking portrait synthesis has considered this aspect. To address this, we propose a FREquency-modulated, high-fidelity, and real-time Audio-driven talKing portrait synthesis framework, named FREAK, which models talking portraits from the frequency domain perspective, enhancing the fidelity and naturalness of the synthesized portraits. FREAK introduces two novel frequency-based modules: 1) the Visual Encoding Frequency Modulator (VEFM) to couple multi-scale visual features in the frequency domain, better preserving visual frequency information and reducing the gap in the frequency spectrum between synthesized and natural frames. and 2) the Audio Visual Frequency Modulator (AVFM) to help the model learn the talking pattern in the frequency domain and improve audio-visual synchronization. Additionally, we optimize the model in both pixel domain and frequency domain jointly. Furthermore, FREAK supports seamless switching between one-shot and video dubbing settings, offering enhanced flexibility. Due to its superior performance, it can simultaneously support high-resolution video results and real-time inference. Extensive experiments demonstrate that our method synthesizes high-fidelity talking portraits with detailed facial textures and precise lip synchronization in real-time, outperforming state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2503.04067v2),  [pdf](http://arxiv.org/pdf/2503.04067v2)

**Tags**: cs.CV 



### LaMsS: When Large Language Models Meet Self-Skepticism
**Authors**: Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji

**Updated**: 2025-04-24T03:29:59Z

**Summary**: Hallucination is a major challenge for large language models (LLMs), preventing their further application in some fields. The skeptical thinking of humankind could be useful for LLMs to self-cognition, self-reflection and alleviate their hallucinations. Inspired by this consideration, we propose a novel approach called LaMsS, which combines the semantic understanding capability of LLMs with self-skepticism. By introducing a series of skepticism tokens and augmenting them into the vocabulary, we conduct both pertaining and finetuning, which allow the LLM to decode each normal token followed by a skeptical token, representing different skepticism levels. By calculating the response skepticism given a query, one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold. By examining the accuracy, AUC and AP of willingly answering questions, we demonstrate that LaMsS achieves better performance than baselines on both multi-choice questions and open-domain question-answering benchmarks, and can generalize to multi-task and out-of-domain settings. Our study sheds some lights on the self-skepticism modeling on further artificial intelligence. Project code and model checkpoints can be found in https://anonymous.4open.science/r/SM-1E76.

**Link**: [arxiv](http://arxiv.org/abs/2409.06601v3),  [pdf](http://arxiv.org/pdf/2409.06601v3)

**Tags**: cs.CL cs.LG 



### Case Study: Fine-tuning Small Language Models for Accurate and Private   CWE Detection in Python Code
**Authors**: Md. Azizul Hakim Bappy, Hossen A Mustafa, Prottoy Saha, Rajinus Salehat

**Updated**: 2025-04-23T10:05:27Z

**Summary**: Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows.

**Link**: [arxiv](http://arxiv.org/abs/2504.16584v1),  [pdf](http://arxiv.org/pdf/2504.16584v1)

**Tags**: cs.CR cs.AI 



### CAPO: Cost-Aware Prompt Optimization
**Authors**: Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer

**Updated**: 2025-04-23T09:59:31Z

**Summary**: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2504.16005v2),  [pdf](http://arxiv.org/pdf/2504.16005v2)

**Tags**: cs.CL cs.AI cs.NE stat.ML 



### PIS: Linking Importance Sampling and Attention Mechanisms for Efficient   Prompt Compression
**Authors**: Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI

**Updated**: 2025-04-23T09:53:01Z

**Summary**: Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.16574v1),  [pdf](http://arxiv.org/pdf/2504.16574v1)

**Tags**: cs.CL cs.AI 



### PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling   Assistant System
**Authors**: Xianghe Liu, Jiaqi Xu, Tao Sun

**Updated**: 2025-04-23T09:49:05Z

**Summary**: Psychological counseling is a highly personalized and dynamic process that requires therapists to continuously monitor emotional changes, document session insights, and maintain therapeutic continuity. In this paper, we introduce PsyCounAssist, a comprehensive AI-powered counseling assistant system specifically designed to augment psychological counseling practices. PsyCounAssist integrates multimodal emotion recognition combining speech and photoplethysmography (PPG) signals for accurate real-time affective analysis, automated structured session reporting using large language models (LLMs), and personalized AI-generated follow-up support. Deployed on Android-based tablet devices, the system demonstrates practical applicability and flexibility in real-world counseling scenarios. Experimental evaluation confirms the reliability of PPG-based emotional classification and highlights the system's potential for non-intrusive, privacy-aware emotional support. PsyCounAssist represents a novel approach to ethically and effectively integrating AI into psychological counseling workflows.

**Link**: [arxiv](http://arxiv.org/abs/2504.16573v1),  [pdf](http://arxiv.org/pdf/2504.16573v1)

**Tags**: cs.HC cs.AI 



### CountingDINO: A Training-free Pipeline for Class-Agnostic Counting using   Unsupervised Backbones
**Authors**: Giacomo Pacini, Lorenzo Bianchi, Luca Ciampi, Nicola Messina, Giuseppe Amato, Fabrizio Falchi

**Updated**: 2025-04-23T09:48:08Z

**Summary**: Class-agnostic counting (CAC) aims to estimate the number of objects in images without being restricted to predefined categories. However, while current exemplar-based CAC methods offer flexibility at inference time, they still rely heavily on labeled data for training, which limits scalability and generalization to many downstream use cases. In this paper, we introduce CountingDINO, the first training-free exemplar-based CAC framework that exploits a fully unsupervised feature extractor. Specifically, our approach employs self-supervised vision-only backbones to extract object-aware features, and it eliminates the need for annotated data throughout the entire proposed pipeline. At inference time, we extract latent object prototypes via ROI-Align from DINO features and use them as convolutional kernels to generate similarity maps. These are then transformed into density maps through a simple yet effective normalization scheme. We evaluate our approach on the FSC-147 benchmark, where we outperform a baseline under the same label-free setting. Our method also achieves competitive -- and in some cases superior -- results compared to training-free approaches relying on supervised backbones, as well as several fully supervised state-of-the-art methods. This demonstrates that training-free CAC can be both scalable and competitive. Website: https://lorebianchi98.github.io/CountingDINO/

**Link**: [arxiv](http://arxiv.org/abs/2504.16570v1),  [pdf](http://arxiv.org/pdf/2504.16570v1)

**Tags**: cs.CV 



### Enhancing LLM-Based Agents via Global Planning and Hierarchical   Execution
**Authors**: Junjie Chen, Haitao Li, Jingli Yang, Yiqun Liu, Qingyao Ai

**Updated**: 2025-04-23T09:43:40Z

**Summary**: Intelligent agent systems based on Large Language Models (LLMs) have shown great potential in real-world applications. However, existing agent frameworks still face critical limitations in task planning and execution, restricting their effectiveness and generalizability. Specifically, current planning methods often lack clear global goals, leading agents to get stuck in local branches, or produce non-executable plans. Meanwhile, existing execution mechanisms struggle to balance complexity and stability, and their limited action space restricts their ability to handle diverse real-world tasks. To address these limitations, we propose GoalAct, a novel agent framework that introduces a continuously updated global planning mechanism and integrates a hierarchical execution strategy. GoalAct decomposes task execution into high-level skills, including searching, coding, writing and more, thereby reducing planning complexity while enhancing the agents' adaptability across diverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark with multiple types of legal tasks that require the use of multiple types of tools. Experimental results demonstrate that GoalAct achieves state-of-the-art (SOTA) performance, with an average improvement of 12.22% in success rate. These findings highlight GoalAct's potential to drive the development of more advanced intelligent agent systems, making them more effective across complex real-world applications. Our code can be found at https://github.com/cjj826/GoalAct.

**Link**: [arxiv](http://arxiv.org/abs/2504.16563v1),  [pdf](http://arxiv.org/pdf/2504.16563v1)

**Tags**: cs.IR 



### DTVM: Revolutionizing Smart Contract Execution with Determinism and   Compatibility
**Authors**: Wei Zhou, Changzheng Wei, Ying Yan, Wei Tang, Zhihao Chen, Xiong Xu, Xuebing Huang, Wengang Chen, Jie Zhang, Yang Chen, Xiaofu Zheng, Hanghang Wu, Shenglong Chen, Ermei Wang, Xiangfei Chen, Yang Yu, Meng Wu, Tao Zhu, Liwei Yuan, Feng Yu, Alex Zhang, Wei Wang, Ji Luo, Zhengyu He, Wenbiao Zhao

**Updated**: 2025-04-23T09:28:09Z

**Summary**: We introduce the DeTerministic Virtual Machine (DTVM) Stack, a next-generation smart contract execution framework designed to address critical performance, determinism, and ecosystem compatibility challenges in blockchain networks. Building upon WebAssembly (Wasm) while maintaining full Ethereum Virtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle Intermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to balance compilation speed and execution efficiency. DTVM further accommodates diverse instruction set architectures (e.g., EVM, RISC-V) through modular adaptation layers. This enables seamless integration with DTVM's hybrid lazy-JIT compilation engine, which dynamically optimizes performance while preserving deterministic execution guarantees across heterogeneous environments. The key contributions including: 1). The framework achieves up to 2$\times$ acceleration over evmone in dominant Ethereum contract (e.g. ERC20/721/1155) execution and reduces fibonacci computation latency by 11.8$\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch mechanism enables sub-millisecond (0.95ms) post-deployment invocation times, outperforming up to about 23$\times$ in compilation and invocation efficiency. 3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and AssemblyScript) through unified bytecode conversion while maintaining EVM ABI compatibility for seamless invocation. It reduces machine code object sizes by 30.0$\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers SmartCogent, an AI-driven full-stack development experience, leveraging fine-tuned LLMs and retrieval-augmented generation to automate tasks across the smart contract lifecycle: development, debugging, security auditing, and deployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack).

**Link**: [arxiv](http://arxiv.org/abs/2504.16552v1),  [pdf](http://arxiv.org/pdf/2504.16552v1)

**Tags**: cs.DC 



### Exploring human-SAV interaction using large language models: The impact   of psychological ownership and anthropomorphism on user experience
**Authors**: Lirui Guo, Michael G. Burke, Wynita M. Griggs

**Updated**: 2025-04-23T09:25:22Z

**Summary**: There has been extensive prior work exploring how psychological factors such as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs). However, limited research has been conducted on how prompt strategies in large language model (LLM)-powered SAV User Interfaces (UIs) affect users' perceptions, experiences, and intentions to adopt such technology. In this work, we investigate how conversational UIs powered by LLMs drive these psychological factors and psychological ownership, the sense of possession a user may come to feel towards an entity or object they may not legally own. We designed four SAV UIs with varying levels of anthropomorphic characteristics and psychological ownership triggers. Quantitative measures of psychological ownership, anthropomorphism, quality of service, disclosure tendency, sentiment of SAV responses, and overall acceptance were collected after participants interacted with each SAV. Qualitative feedback was also gathered regarding the experience of psychological ownership during the interactions. The results indicate that an SAV conversational UI designed to be more anthropomorphic and to induce psychological ownership improved users' perceptions of the SAV's human-like qualities and improved the sentiment of responses compared to a control condition. These findings provide practical guidance for designing LLM-based conversational UIs that enhance user experience and adoption of SAVs.

**Link**: [arxiv](http://arxiv.org/abs/2504.16548v1),  [pdf](http://arxiv.org/pdf/2504.16548v1)

**Tags**: cs.HC cs.AI cs.ET 



### Streetscape Analysis with Generative AI (SAGAI): Vision-Language   Assessment and Mapping of Urban Scenes
**Authors**: Joan Perez, Giovanni Fusco

**Updated**: 2025-04-23T09:08:06Z

**Summary**: Streetscapes are an essential component of urban space. Their assessment is presently either limited to morphometric properties of their mass skeleton or requires labor-intensive qualitative evaluations of visually perceived qualities. This paper introduces SAGAI: Streetscape Analysis with Generative Artificial Intelligence, a modular workflow for scoring street-level urban scenes using open-access data and vision-language models. SAGAI integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight version of the LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. The pipeline includes an automated mapping module that aggregates visual scores at both the point and street levels, enabling direct cartographic interpretation. It operates without task-specific training or proprietary software dependencies, supporting scalable and interpretable analysis of urban environments. Two exploratory case studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial outputs from vision-language inference. The initial results show strong performance for binary urban-rural scene classification, moderate precision in commercial feature detection, and lower estimates, but still informative, of sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a wide range of urban research themes, such as walkability, safety, or urban design, through prompt modification alone.

**Link**: [arxiv](http://arxiv.org/abs/2504.16538v1),  [pdf](http://arxiv.org/pdf/2504.16538v1)

**Tags**: cs.CV cs.LG I.2; I.4; J.4 



### 6G EdgeAI: Performance Evaluation and Analysis
**Authors**: Chien-Sheng Yang, Yu-Jen Ku, Yuan-Yao Lou, Nathan Tenny, Alex C. -C. Hsu

**Updated**: 2025-04-23T08:54:10Z

**Summary**: Generative AI (GenAI) services powered by large language models (LLMs) increasingly deliver real-time interactions, yet existing 5G multi-access edge computing (MEC) architectures often treat communication and computing as separate domains, limiting their ability to meet stringent latency requirements. To address this challenge, we introduce an Integrated Communication and Computing (ICC) framework where computing capabilities are enabled to reside directly in radio access network (RAN) nodes and jointly manage bandwidth and computing resources. Our queueing-theoretic analysis shows that ICC outperforms 5G MEC, achieving higher service capacity (defined as the maximum arrival rate that maintains a specified fraction of jobs completed within a given delay budget) by 98%. We corroborate these gains through system-level simulations that account for transformer-based LLM workloads, realistic GPU specifications, and a priority-based scheduling scheme. The simulations show that ICC improves service capacity by 60%, demonstrating its potential to enable efficient, cost-effective real-time GenAI services in 6G.

**Link**: [arxiv](http://arxiv.org/abs/2504.16529v1),  [pdf](http://arxiv.org/pdf/2504.16529v1)

**Tags**: cs.DC 



### Using Causal Inference to Test Systems with Hidden and Interacting   Variables: An Evaluative Case Study
**Authors**: Michael Foster, Robert M. Hierons, Donghwan Shin, Neil Walkinshaw, Christopher Wild

**Updated**: 2025-04-23T08:50:24Z

**Summary**: Software systems with large parameter spaces, nondeterminism and high computational cost are challenging to test. Recently, software testing techniques based on causal inference have been successfully applied to systems that exhibit such characteristics, including scientific models and autonomous driving systems. One significant limitation is that these are restricted to test properties where all of the variables involved can be observed and where there are no interactions between variables. In practice, this is rarely guaranteed; the logging infrastructure may not be available to record all of the necessary runtime variable values, and it can often be the case that an output of the system can be affected by complex interactions between variables. To address this, we leverage two additional concepts from causal inference, namely effect modification and instrumental variable methods. We build these concepts into an existing causal testing tool and conduct an evaluative case study which uses the concepts to test three system-level requirements of CARLA, a high-fidelity driving simulator widely used in autonomous vehicle development and testing. The results show that we can obtain reliable test outcomes without requiring large amounts of highly controlled test data or instrumentation of the code, even when variables interact with each other and are not recorded in the test data.

**Link**: [arxiv](http://arxiv.org/abs/2504.16526v1),  [pdf](http://arxiv.org/pdf/2504.16526v1)

**Tags**: cs.SE 



### Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion   and Reasoning for Vision-and-Language Navigation
**Authors**: Junrong Yue, Yifan Zhang, Chuan Qin, Bo Li, Xiaomin Lie, Xinlei Yu, Wenxin Zhang, Zhendong Zhao

**Updated**: 2025-04-23T08:41:27Z

**Summary**: Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.

**Link**: [arxiv](http://arxiv.org/abs/2504.16516v1),  [pdf](http://arxiv.org/pdf/2504.16516v1)

**Tags**: cs.CV cs.AI 



### QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM   Pretraining
**Authors**: Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Xiaohuan Zhou, Taifeng Wang, Yong Cao

**Updated**: 2025-04-23T08:36:50Z

**Summary**: Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.

**Link**: [arxiv](http://arxiv.org/abs/2504.16511v1),  [pdf](http://arxiv.org/pdf/2504.16511v1)

**Tags**: cs.CL 



### A Comprehensive Survey of Synthetic Tabular Data Generation
**Authors**: Ruxue Shi, Yili Wang, Mengnan Du, Xu Shen, Xin Wang

**Updated**: 2025-04-23T08:33:34Z

**Summary**: Tabular data remains one of the most prevalent and critical data formats across diverse real-world applications. However, its effective use in machine learning (ML) is often constrained by challenges such as data scarcity, privacy concerns, and class imbalance. Synthetic data generation has emerged as a promising solution, leveraging generative models to learn the distribution of real datasets and produce high-fidelity, privacy-preserving samples. Various generative paradigms have been explored, including energy-based models (EBMs), variational autoencoders (VAEs), generative adversarial networks (GANs), large language models (LLMs), and diffusion models. While several surveys have investigated synthetic tabular data generation, most focus on narrow subdomains or specific generative methods, such as GANs, diffusion models, or privacy-preserving techniques. This limited scope often results in fragmented insights, lacking a comprehensive synthesis that bridges diverse approaches. In particular, recent advances driven by LLMs and diffusion-based models remain underexplored. This gap hinders a holistic understanding of the field`s evolution, methodological interplay, and open challenges. To address this, our survey provides a unified and systematic review of synthetic tabular data generation. Our contributions are threefold: (1) we propose a comprehensive taxonomy that organizes existing methods into traditional approaches, diffusion-based methods, and LLM-based models, and provide an in-depth comparative analysis; (2) we detail the complete pipeline for synthetic tabular data generation, including data synthesis, post-processing, and evaluation; (3) we identify major challenges, explore real-world applications, and outline open research questions and future directions to guide future work in this rapidly evolving area.

**Link**: [arxiv](http://arxiv.org/abs/2504.16506v1),  [pdf](http://arxiv.org/pdf/2504.16506v1)

**Tags**: cs.LG 



### Intelligent Depression Prevention via LLM-Based Dialogue Analysis:   Overcoming the Limitations of Scale-Dependent Diagnosis through Precise   Emotional Pattern Recognition
**Authors**: Zhenguang Zhong, Zhixuan Wang

**Updated**: 2025-04-23T08:31:51Z

**Summary**: Existing depression screening predominantly relies on standardized questionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates (18-34% in clinical studies) due to their static, symptom-counting nature and susceptibility to patient recall bias. This paper presents an AI-powered depression prevention system that leverages large language models (LLMs) to analyze real-time conversational cues--including subtle emotional expressions (e.g., micro-sentiment shifts, self-referential language patterns)--for more accurate and dynamic mental state assessment. Our system achieves three key innovations: (1) Continuous monitoring through natural dialogue, detecting depression-indicative linguistic features (anhedonia markers, hopelessness semantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk stratification that updates severity levels based on conversational context, reducing false positives by 41% compared to scale-based thresholds; and (3) Personalized intervention strategies tailored to users' emotional granularity, demonstrating 2.3x higher adherence rates than generic advice. Clinical validation with 450 participants shows the system identifies 92% of at-risk cases missed by traditional scales, while its explainable AI interface bridges the gap between automated analysis and clinician judgment. This work establishes conversational AI as a paradigm shift from episodic scale-dependent diagnosis to continuous, emotionally intelligent mental health monitoring.

**Link**: [arxiv](http://arxiv.org/abs/2504.16504v1),  [pdf](http://arxiv.org/pdf/2504.16504v1)

**Tags**: q-bio.NC cs.HC 



### Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based   Multi-Agent Debate
**Authors**: Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, Dongxiao Yu

**Updated**: 2025-04-23T08:01:50Z

**Summary**: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2504.16489v1),  [pdf](http://arxiv.org/pdf/2504.16489v1)

**Tags**: cs.CR cs.AI 



### Needle Threading: Can LLMs Follow Threads through Near-Million-Scale   Haystacks?
**Authors**: Jonathan Roberts, Kai Han, Samuel Albanie

**Updated**: 2025-04-23T07:50:49Z

**Summary**: As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.

**Link**: [arxiv](http://arxiv.org/abs/2411.05000v2),  [pdf](http://arxiv.org/pdf/2411.05000v2)

**Tags**: cs.CL 



### Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:   Open Research Challenges
**Authors**: Mark Harman, Peter O'Hearn, Shubho Sengupta

**Updated**: 2025-04-23T07:32:43Z

**Summary**: Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated `just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper\footnote{Author order is alphabetical. The corresponding author is Mark Harman.} was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025.

**Link**: [arxiv](http://arxiv.org/abs/2504.16472v1),  [pdf](http://arxiv.org/pdf/2504.16472v1)

**Tags**: cs.SE cs.AI 



### A Highly Scalable LLM Clusters with Optical Interconnect
**Authors**: Xinchi Han, Yongxi Lv, Shizhen Zhao, Zhuotao Liu, Ximeng Liu, Xinbing Wang

**Updated**: 2025-04-23T07:27:35Z

**Summary**: We propose \emph{LumosCore} to build high-bandwidth and large-scale data center networks for LLM jobs. By replacing the core-layer electrical packet switches by optical circuit switches, \emph{LumosCore} could achieves $2\times$ increase in bandwidth or $8\times$ increase in network size. We offer the detailed design of \emph{LumosCore} at both deployment stage and running stage. At deployment stage, we propose Interleaved Wiring, which is compatible with all possible logical topologies. At running stage, we design polynomial-time algorithms for GPU placement, logical topology generating and OCS reconfiguration to minimize network contention and reduce impact to scheduled jobs. We evaluate \emph{LumosCore} using both testbed experiments and large-scale simulation. Compared to traditional hybrid optical/electrical architectures, \emph{LumosCore} increases the end-to-end training throughput by up to 39.5\% on a 128-node testbed. Compared to the state-of-art Clos architectures, \emph{LumosCore} reduces the average job completion time by up to 34.1\% in a 16k simulation platform.

**Link**: [arxiv](http://arxiv.org/abs/2411.01503v4),  [pdf](http://arxiv.org/pdf/2411.01503v4)

**Tags**: cs.NI 



### Language Models to Support Multi-Label Classification of Industrial Data
**Authors**: Waleed Abdeen, Michael Unterkalmsteiner, Krzysztof Wnuk, Alessio Ferrari, Panagiota Chatzipetrou

**Updated**: 2025-04-23T07:24:40Z

**Summary**: Multi-label requirements classification is a challenging task, especially when dealing with numerous classes at varying levels of abstraction. The difficulties increases when a limited number of requirements is available to train a supervised classifier. Zero-shot learning (ZSL) does not require training data and can potentially address this problem. This paper investigates the performance of zero-shot classifiers (ZSCs) on a multi-label industrial dataset. We focuse on classifying requirements according to a taxonomy designed to support requirements tracing. We compare multiple variants of ZSCs using different embeddings, including 9 language models (LMs) with a reduced number of parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large number of parameters (up to 70B), e.g., Llama. Our ground truth includes 377 requirements and 1968 labels from 6 output spaces. For the evaluation, we adopt traditional metrics, i.e., precision, recall, F1, and $F_\beta$, as well as a novel label distance metric Dn. This aims to better capture the classification's hierarchical nature and provides a more nuanced evaluation of how far the results are from the ground truth. 1) The top-performing model on 5 out of 6 output spaces is T5-xl, with maximum $F_\beta$ = 0.78 and Dn = 0.04, while BERT base outperformed the other models in one case, with maximum $F_\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the best classification results compared to LLMs. Thus, addressing the problem in practice is feasible as limited computing power is needed. 3) The model architecture (autoencoding, autoregression, and sentence-to-sentence) significantly affects the classifier's performance. We conclude that using ZSL for multi-label requirements classification offers promising results. We also present a novel metric that can be used to select the top-performing model for this problem

**Link**: [arxiv](http://arxiv.org/abs/2504.15922v2),  [pdf](http://arxiv.org/pdf/2504.15922v2)

**Tags**: cs.SE 



### Large Language Models for Automated Literature Review: An Evaluation of   Reference Generation, Abstract Writing, and Review Composition
**Authors**: Xuemei Tang, Xufeng Duan, Zhenguang G. Cai

**Updated**: 2025-04-23T07:09:06Z

**Summary**: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.

**Link**: [arxiv](http://arxiv.org/abs/2412.13612v3),  [pdf](http://arxiv.org/pdf/2412.13612v3)

**Tags**: cs.CL cs.AI 



### EAGLE-3: Scaling up Inference Acceleration of Large Language Models via   Training-Time Test
**Authors**: Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang

**Updated**: 2025-04-23T07:08:17Z

**Summary**: The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves a 1.38x throughput improvement at a batch size of 64. The code is available at https://github.com/SafeAILab/EAGLE.

**Link**: [arxiv](http://arxiv.org/abs/2503.01840v3),  [pdf](http://arxiv.org/pdf/2503.01840v3)

**Tags**: cs.CL 



## Keyword: LLM Deployment 
 ### IberBench: LLM Evaluation on Iberian Languages
**Authors**: José Ángel González, Ian Borrego Obrador, Álvaro Romo Herrero, Areg Mikael Sarvazyan, Mara Chinea-Ríos, Angelo Basile, Marc Franco-Salvador

**Updated**: 2025-04-23T17:48:25Z

**Summary**: Large Language Models (LLMs) remain difficult to evaluate comprehensively, particularly for languages other than English, where high-quality data is often limited. Existing benchmarks and leaderboards are predominantly English-centric, with only a few addressing other languages. These benchmarks fall short in several key areas: they overlook the diversity of language varieties, prioritize fundamental Natural Language Processing (NLP) capabilities over tasks of industrial relevance, and are static. With these aspects in mind, we present IberBench, a comprehensive and extensible benchmark designed to assess LLM performance on both fundamental and industry-relevant NLP tasks, in languages spoken across the Iberian Peninsula and Ibero-America. IberBench integrates 101 datasets from evaluation campaigns and recent benchmarks, covering 22 task categories such as sentiment and emotion analysis, toxicity detection, and summarization. The benchmark addresses key limitations in current evaluation practices, such as the lack of linguistic diversity and static evaluation setups by enabling continual updates and community-driven model and dataset submissions moderated by a committee of experts. We evaluate 23 LLMs ranging from 100 million to 14 billion parameters and provide empirical insights into their strengths and limitations. Our findings indicate that (i) LLMs perform worse on industry-relevant tasks than in fundamental ones, (ii) performance is on average lower for Galician and Basque, (iii) some tasks show results close to random, and (iv) in other tasks LLMs perform above random but below shared task systems. IberBench offers open-source implementations for the entire evaluation pipeline, including dataset normalization and hosting, incremental evaluation of LLMs, and a publicly accessible leaderboard.

**Link**: [arxiv](http://arxiv.org/abs/2504.16921v1),  [pdf](http://arxiv.org/pdf/2504.16921v1)

**Tags**: cs.CL 



### Stop Overthinking: A Survey on Efficient Reasoning for Large Language   Models
**Authors**: Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Hu

**Updated**: 2025-04-23T17:46:54Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the "overthinking phenomenon". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking.

**Link**: [arxiv](http://arxiv.org/abs/2503.16419v3),  [pdf](http://arxiv.org/pdf/2503.16419v3)

**Tags**: cs.CL 



### DiffArtist: Towards Structure and Appearance Controllable Image   Stylization
**Authors**: Ruixiang Jiang, Changwen Chen

**Updated**: 2025-04-23T17:46:08Z

**Summary**: Artistic style includes both structural and appearance elements. Existing neural stylization techniques primarily focus on transferring appearance features such as color and texture, often neglecting the equally crucial aspect of structural stylization. In this paper, we present a comprehensive study on the simultaneous stylization of structure and appearance of 2D images. Specifically, we introduce DiffArtist, which, to the best of our knowledge, is the first stylization method to allow for dual controllability over structure and appearance. Our key insight is to represent structure and appearance as separate diffusion processes to achieve complete disentanglement without requiring any training, thereby endowing users with unprecedented controllability for both components. The evaluation of stylization of both appearance and structure, however, remains challenging as it necessitates semantic understanding. To this end, we further propose a Multimodal LLM-based style evaluator, which better aligns with human preferences than metrics lacking semantic understanding. With this powerful evaluator, we conduct extensive analysis, demonstrating that DiffArtist achieves superior style fidelity, editability, and structure-appearance disentanglement. These merits make DiffArtist a highly versatile solution for creative applications. Project homepage: https://github.com/songrise/Artist.

**Link**: [arxiv](http://arxiv.org/abs/2407.15842v3),  [pdf](http://arxiv.org/pdf/2407.15842v3)

**Tags**: cs.CV cs.GR 



### OptimAI: Optimization from Natural Language Using LLM-Powered AI Agents
**Authors**: Raghav Thind, Youran Sun, Ling Liang, Haizhao Yang

**Updated**: 2025-04-23T17:45:05Z

**Summary**: Optimization plays a vital role in scientific research and practical applications, but formulating a concrete optimization problem described in natural language into a mathematical form and selecting a suitable solver to solve the problem requires substantial domain expertise. We introduce \textbf{OptimAI}, a framework for solving \underline{Optim}ization problems described in natural language by leveraging LLM-powered \underline{AI} agents, achieving superior performance over current state-of-the-art methods. Our framework is built upon four key roles: (1) a \emph{formulator} that translates natural language problem descriptions into precise mathematical formulations; (2) a \emph{planner} that constructs a high-level solution strategy prior to execution; and (3) a \emph{coder} and a \emph{code critic} capable of interacting with the environment and reflecting on outcomes to refine future actions. Ablation studies confirm that all roles are essential; removing the planner or code critic results in $5.8\times$ and $3.1\times$ drops in productivity, respectively. Furthermore, we introduce UCB-based debug scheduling to dynamically switch between alternative plans, yielding an additional $3.3\times$ productivity gain. Our design emphasizes multi-agent collaboration, allowing us to conveniently explore the synergistic effect of combining diverse models within a unified system. Our approach attains 88.1\% accuracy on the NLP4LP dataset and 71.2\% on the Optibench (non-linear w/o table) subset, reducing error rates by 58\% and 50\% respectively over prior best results.

**Link**: [arxiv](http://arxiv.org/abs/2504.16918v1),  [pdf](http://arxiv.org/pdf/2504.16918v1)

**Tags**: cs.CL cs.AI 



### Tracing Thought: Using Chain-of-Thought Reasoning to Identify the LLM   Behind AI-Generated Text
**Authors**: Shifali Agrahari, Sanasam Ranbir Singh

**Updated**: 2025-04-23T17:39:49Z

**Summary**: In recent years, the detection of AI-generated text has become a critical area of research due to concerns about academic integrity, misinformation, and ethical AI deployment. This paper presents COT Fine-tuned, a novel framework for detecting AI-generated text and identifying the specific language model. responsible for generating the text. We propose a dual-task approach, where Task A involves classifying text as AI-generated or human-written, and Task B identifies the specific LLM behind the text. The key innovation of our method lies in the use of Chain-of-Thought reasoning, which enables the model to generate explanations for its predictions, enhancing transparency and interpretability. Our experiments demonstrate that COT Fine-tuned achieves high accuracy in both tasks, with strong performance in LLM identification and human-AI classification. We also show that the CoT reasoning process contributes significantly to the models effectiveness and interpretability.

**Link**: [arxiv](http://arxiv.org/abs/2504.16913v1),  [pdf](http://arxiv.org/pdf/2504.16913v1)

**Tags**: cs.CL cs.AI 



### A Measure Based Generalizable Approach to Understandability
**Authors**: Vikas Kushwaha, Sruti Srinivasa Ragavan, Subhajit Roy

**Updated**: 2025-04-23T17:39:20Z

**Summary**: Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).   In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future.

**Link**: [arxiv](http://arxiv.org/abs/2503.21615v2),  [pdf](http://arxiv.org/pdf/2503.21615v2)

**Tags**: cs.HC cs.AI cs.SE 



### Building A Secure Agentic AI Application Leveraging A2A Protocol
**Authors**: Idan Habler, Ken Huang, Vineeth Sai Narajala, Prashant Kulkarni

**Updated**: 2025-04-23T17:27:49Z

**Summary**: As Agentic AI systems evolve from basic workflows to complex multi agent collaboration, robust protocols such as Google's Agent2Agent (A2A) become essential enablers. To foster secure adoption and ensure the reliability of these complex interactions, understanding the secure implementation of A2A is essential. This paper addresses this goal by providing a comprehensive security analysis centered on the A2A protocol. We examine its fundamental elements and operational dynamics, situating it within the framework of agent communication development. Utilizing the MAESTRO framework, specifically designed for AI risks, we apply proactive threat modeling to assess potential security issues in A2A deployments, focusing on aspects such as Agent Card management, task execution integrity, and authentication methodologies.   Based on these insights, we recommend practical secure development methodologies and architectural best practices designed to build resilient and effective A2A systems. Our analysis also explores how the synergy between A2A and the Model Context Protocol (MCP) can further enhance secure interoperability. This paper equips developers and architects with the knowledge and practical guidance needed to confidently leverage the A2A protocol for building robust and secure next generation agentic applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16902v1),  [pdf](http://arxiv.org/pdf/2504.16902v1)

**Tags**: cs.CR cs.AI 



### Do Large Language Models know who did what to whom?
**Authors**: Joseph M. Denning, Xiaohan, Guo, Bryor Snefjella, Idan A. Blank

**Updated**: 2025-04-23T17:00:45Z

**Summary**: Large Language Models (LLMs) are commonly criticized for not understanding language. However, many critiques focus on cognitive abilities that, in humans, are distinct from language processing. Here, we instead study a kind of understanding tightly linked to language: inferring who did what to whom (thematic roles) in a sentence. Does the central training objective of LLMs-word prediction-result in sentence representations that capture thematic roles? In two experiments, we characterized sentence representations in four LLMs. In contrast to human similarity judgments, in LLMs the overall representational similarity of sentence pairs reflected syntactic similarity but not whether their agent and patient assignments were identical vs. reversed. Furthermore, we found little evidence that thematic role information was available in any subset of hidden units. However, some attention heads robustly captured thematic roles, independently of syntax. Therefore, LLMs can extract thematic roles but, relative to humans, this information influences their representations more weakly.

**Link**: [arxiv](http://arxiv.org/abs/2504.16884v1),  [pdf](http://arxiv.org/pdf/2504.16884v1)

**Tags**: cs.CL 



### Enhancing Critical Thinking with AI: A Tailored Warning System for RAG   Models
**Authors**: Xuyang Zhu, Sejoon Chang, Andrew Kuik

**Updated**: 2025-04-23T17:00:25Z

**Summary**: Retrieval-Augmented Generation (RAG) systems offer a powerful approach to enhancing large language model (LLM) outputs by incorporating fact-checked, contextually relevant information. However, fairness and reliability concerns persist, as hallucinations can emerge at both the retrieval and generation stages, affecting users' reasoning and decision-making. Our research explores how tailored warning messages -- whose content depends on the specific context of hallucination -- shape user reasoning and actions in an educational quiz setting. Preliminary findings suggest that while warnings improve accuracy and awareness of high-level hallucinations, they may also introduce cognitive friction, leading to confusion and diminished trust in the system. By examining these interactions, this work contributes to the broader goal of AI-augmented reasoning: developing systems that actively support human reflection, critical thinking, and informed decision-making rather than passive information consumption.

**Link**: [arxiv](http://arxiv.org/abs/2504.16883v1),  [pdf](http://arxiv.org/pdf/2504.16883v1)

**Tags**: cs.HC 



### Context-Enhanced Vulnerability Detection Based on Large Language Model
**Authors**: Yixin Yang, Bowen Xu, Xiang Gao, Hailong Sun

**Updated**: 2025-04-23T16:54:16Z

**Summary**: Vulnerability detection is a critical aspect of software security. Accurate detection is essential to prevent potential security breaches and protect software systems from malicious attacks. Recently, vulnerability detection methods leveraging deep learning and large language models (LLMs) have garnered increasing attention. However, existing approaches often focus on analyzing individual files or functions, which limits their ability to gather sufficient contextual information. Analyzing entire repositories to gather context introduces significant noise and computational overhead. To address these challenges, we propose a context-enhanced vulnerability detection approach that combines program analysis with LLMs. Specifically, we use program analysis to extract contextual information at various levels of abstraction, thereby filtering out irrelevant noise. The abstracted context along with source code are provided to LLM for vulnerability detection. We investigate how different levels of contextual granularity improve LLM-based vulnerability detection performance. Our goal is to strike a balance between providing sufficient detail to accurately capture vulnerabilities and minimizing unnecessary complexity that could hinder model performance. Based on an extensive study using GPT-4, DeepSeek, and CodeLLaMA with various prompting strategies, our key findings includes: (1) incorporating abstracted context significantly enhances vulnerability detection effectiveness; (2) different models benefit from distinct levels of abstraction depending on their code understanding capabilities; and (3) capturing program behavior through program analysis for general LLM-based code analysis tasks can be a direction that requires further attention.

**Link**: [arxiv](http://arxiv.org/abs/2504.16877v1),  [pdf](http://arxiv.org/pdf/2504.16877v1)

**Tags**: cs.SE 



### aiXamine: Simplified LLM Safety and Security
**Authors**: Fatih Deniz, Dorde Popovic, Yazan Boshmaf, Euisuh Jeong, Minhaj Ahmad, Sanjay Chawla, Issa Khalil

**Updated**: 2025-04-23T16:52:54Z

**Summary**: Evaluating Large Language Models (LLMs) for safety and security remains a complex task, often requiring users to navigate a fragmented landscape of ad hoc benchmarks, datasets, metrics, and reporting formats. To address this challenge, we present aiXamine, a comprehensive black-box evaluation platform for LLM safety and security. aiXamine integrates over 40 tests (i.e., benchmarks) organized into eight key services targeting specific dimensions of safety and security: adversarial robustness, code security, fairness and bias, hallucination, model and data privacy, out-of-distribution (OOD) robustness, over-refusal, and safety alignment. The platform aggregates the evaluation results into a single detailed report per model, providing a detailed breakdown of model performance, test examples, and rich visualizations. We used aiXamine to assess over 50 publicly available and proprietary LLMs, conducting over 2K examinations. Our findings reveal notable vulnerabilities in leading models, including susceptibility to adversarial attacks in OpenAI's GPT-4o, biased outputs in xAI's Grok-3, and privacy weaknesses in Google's Gemini 2.0. Additionally, we observe that open-source models can match or exceed proprietary models in specific services such as safety alignment, fairness and bias, and OOD robustness. Finally, we identify trade-offs between distillation strategies, model size, training methods, and architectural choices.

**Link**: [arxiv](http://arxiv.org/abs/2504.14985v2),  [pdf](http://arxiv.org/pdf/2504.14985v2)

**Tags**: cs.CR cs.AI 



### Natural Language Processing in the Patent Domain: A Survey
**Authors**: Lekang Jiang, Stephan Goetz

**Updated**: 2025-04-23T16:48:56Z

**Summary**: Patents, which encapsulate crucial technical and legal information in text form and referenced drawings, present a rich domain for natural language processing (NLP) applications. As NLP technologies evolve, large language models (LLMs) have demonstrated outstanding capabilities in general text processing and generation tasks. However, the application of LLMs in the patent domain remains under-explored and under-developed due to the complexity of patents, particularly their language and legal framework. Understanding the unique characteristics of patent documents and related research in the patent domain becomes essential for researchers to apply these tools effectively. Therefore, this paper aims to equip NLP researchers with the essential knowledge to navigate this complex domain efficiently. We introduce the relevant fundamental aspects of patents to provide solid background information. In addition, we systematically break down the structural and linguistic characteristics unique to patents and map out how NLP can be leveraged for patent analysis and generation. Moreover, we demonstrate the spectrum of text-based and multimodal patent-related tasks, including nine patent analysis and four patent generation tasks.

**Link**: [arxiv](http://arxiv.org/abs/2403.04105v3),  [pdf](http://arxiv.org/pdf/2403.04105v3)

**Tags**: cs.AI 



### Exploring How LLMs Capture and Represent Domain-Specific Knowledge
**Authors**: Mirian Hipolito Garcia, Camille Couturier, Daniel Madrigal Diaz, Ankur Mallick, Anastasios Kyrillidis, Robert Sim, Victor Ruhle, Saravan Rajmohan

**Updated**: 2025-04-23T16:46:06Z

**Summary**: We study whether Large Language Models (LLMs) inherently capture domain-specific nuances in natural language. Our experiments probe the domain sensitivity of LLMs by examining their ability to distinguish queries from different domains using hidden states generated during the prefill phase. We reveal latent domain-related trajectories that indicate the model's internal recognition of query domains. We also study the robustness of these domain representations to variations in prompt styles and sources. Our approach leverages these representations for model selection, mapping the LLM that best matches the domain trace of the input query (i.e., the model with the highest performance on similar traces). Our findings show that LLMs can differentiate queries for related domains, and that the fine-tuned model is not always the most accurate. Unlike previous work, our interpretations apply to both closed and open-ended generative tasks

**Link**: [arxiv](http://arxiv.org/abs/2504.16871v1),  [pdf](http://arxiv.org/pdf/2504.16871v1)

**Tags**: cs.LG 



### TALES: Text Adventure Learning Environment Suite
**Authors**: Christopher Zhang Cui, Xingdi Yuan, Ziang Xiao, Prithviraj Ammanabrolu, Marc-Alexandre Côté

**Updated**: 2025-04-24T02:50:28Z

**Summary**: Reasoning is an essential skill to enable Large Language Models (LLMs) to interact with the world. As tasks become more complex, they demand increasingly sophisticated and diverse reasoning capabilities for sequential decision-making, requiring structured reasoning over the context history to determine the next best action. We introduce TALES, a diverse collection of synthetic and human-written text-adventure games designed to challenge and evaluate diverse reasoning capabilities. We present results over a range of LLMs, open- and closed-weights, performing a qualitative analysis on the top performing models. Despite an impressive showing on synthetic games, even the top LLM-driven agents fail to achieve 15% on games designed for human enjoyment. Code and visualization of the experiments can be found at https://microsoft.github.io/tale-suite.

**Link**: [arxiv](http://arxiv.org/abs/2504.14128v4),  [pdf](http://arxiv.org/pdf/2504.14128v4)

**Tags**: cs.AI cs.CL 



### Planning with Diffusion Models for Target-Oriented Dialogue Systems
**Authors**: Hanwen Du, Bo Peng, Xia Ning

**Updated**: 2025-04-23T16:27:15Z

**Summary**: Target-Oriented Dialogue (TOD) remains a significant challenge in the LLM era, where strategic dialogue planning is crucial for directing conversations toward specific targets. However, existing dialogue planning methods generate dialogue plans in a step-by-step sequential manner, and may suffer from compounding errors and myopic actions. To address these limitations, we introduce a novel dialogue planning framework, DiffTOD, which leverages diffusion models to enable non-sequential dialogue planning. DiffTOD formulates dialogue planning as a trajectory generation problem with conditional guidance, and leverages a diffusion language model to estimate the likelihood of the dialogue trajectory. To optimize the dialogue action strategies, DiffTOD introduces three tailored guidance mechanisms for different target types, offering flexible guidance towards diverse TOD targets at test time. Extensive experiments across three diverse TOD settings show that DiffTOD can effectively perform non-myopic lookahead exploration and optimize action strategies over a long horizon through non-sequential dialogue planning, and demonstrates strong flexibility across complex and diverse dialogue scenarios. Our code and data are accessible through https://anonymous.4open.science/r/DiffTOD.

**Link**: [arxiv](http://arxiv.org/abs/2504.16858v1),  [pdf](http://arxiv.org/pdf/2504.16858v1)

**Tags**: cs.CL 



### Emo Pillars: Knowledge Distillation to Support Fine-Grained   Context-Aware and Context-Less Emotion Classification
**Authors**: Alexander Shvets

**Updated**: 2025-04-23T16:23:17Z

**Summary**: Most datasets for sentiment analysis lack context in which an opinion was expressed, often crucial for emotion understanding, and are mainly limited by a few emotion categories. Foundation large language models (LLMs) like GPT-4 suffer from over-predicting emotions and are too resource-intensive. We design an LLM-based data synthesis pipeline and leverage a large model, Mistral-7b, for the generation of training examples for more accessible, lightweight BERT-type encoder models. We focus on enlarging the semantic diversity of examples and propose grounding the generation into a corpus of narratives to produce non-repetitive story-character-centered utterances with unique contexts over 28 emotion classes. By running 700K inferences in 450 GPU hours, we contribute with the dataset of 100K contextual and also 300K context-less examples to cover both scenarios. We use it for fine-tuning pre-trained encoders, which results in several Emo Pillars models. We show that Emo Pillars models are highly adaptive to new domains when tuned to specific tasks such as GoEmotions, ISEAR, IEMOCAP, and EmoContext, reaching the SOTA performance on the first three. We also validate our dataset, conducting statistical analysis and human evaluation, and confirm the success of our measures in utterance diversification (although less for the neutral class) and context personalization, while pointing out the need for improved handling of out-of-taxonomy labels within the pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2504.16856v1),  [pdf](http://arxiv.org/pdf/2504.16856v1)

**Tags**: cs.CL 



### Monte Carlo Planning with Large Language Model for Text-Based Game   Agents
**Authors**: Zijing Shi, Meng Fang, Ling Chen

**Updated**: 2025-04-23T16:23:15Z

**Summary**: Text-based games provide valuable environments for language-based autonomous agents. However, planning-then-learning paradigms, such as those combining Monte Carlo Tree Search (MCTS) and reinforcement learning (RL), are notably time-consuming due to extensive iterations. Additionally, these algorithms perform uncertainty-driven exploration but lack language understanding and reasoning abilities. In this paper, we introduce the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm. MC-DML leverages the language understanding and reasoning capabilities of Large Language Models (LLMs) alongside the exploratory advantages of tree search algorithms. Specifically, we enhance LLMs with in-trial and cross-trial memory mechanisms, enabling them to learn from past experiences and dynamically adjust action evaluations during planning. We conduct experiments on a series of text-based games from the Jericho benchmark. Our results demonstrate that the MC-DML algorithm significantly enhances performance across various games at the initial planning phase, outperforming strong contemporary methods that require multiple iterations. This demonstrates the effectiveness of our algorithm, paving the way for more efficient language-grounded planning in complex environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.16855v1),  [pdf](http://arxiv.org/pdf/2504.16855v1)

**Tags**: cs.CL 



### Comparative Performance Evaluation of Large Language Models for   Extracting Molecular Interactions and Pathway Knowledge
**Authors**: Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Shinjae Yoo, Shantenu Jha

**Updated**: 2025-04-23T16:14:18Z

**Summary**: Background: Identification of the interactions and regulatory relations between biomolecules play pivotal roles in understanding complex biological systems and the mechanisms underlying diverse biological functions. However, the collection of such molecular interactions has heavily relied on expert curation in the past, making it labor-intensive and time-consuming. To mitigate these challenges, we propose leveraging the capabilities of large language models (LLMs) to automate genome-scale extraction of this crucial knowledge.   Results: In this study, we investigate the efficacy of various LLMs in addressing biological tasks, such as the recognition of protein interactions, identification of genes linked to pathways affected by low-dose radiation, and the delineation of gene regulatory relationships. Overall, the larger models exhibited superior performance, indicating their potential for specific tasks that involve the extraction of complex interactions among genes and proteins. Although these models possessed detailed information for distinct gene and protein groups, they faced challenges in identifying groups with diverse functions and in recognizing highly correlated gene regulatory relationships.   Conclusions: By conducting a comprehensive assessment of the state-of-the-art models using well-established molecular interaction and pathway databases, our study reveals that LLMs can identify genes/proteins associated with pathways of interest and predict their interactions to a certain extent. Furthermore, these models can provide important insights, marking a noteworthy stride toward advancing our understanding of biological systems through AI-assisted knowledge discovery.

**Link**: [arxiv](http://arxiv.org/abs/2307.08813v4),  [pdf](http://arxiv.org/pdf/2307.08813v4)

**Tags**: cs.CL cs.LG 



### Certified Mitigation of Worst-Case LLM Copyright Infringement
**Authors**: Jingyu Zhang, Jiacan Yu, Marc Marone, Benjamin Van Durme, Daniel Khashabi

**Updated**: 2025-04-23T16:13:08Z

**Summary**: The exposure of large language models (LLMs) to copyrighted material during pre-training raises concerns about unintentional copyright infringement post deployment. This has driven the development of "copyright takedown" methods, post-training approaches aimed at preventing models from generating content substantially similar to copyrighted ones. While current mitigation approaches are somewhat effective for average-case risks, we demonstrate that they overlook worst-case copyright risks exhibits by the existence of long, verbatim quotes from copyrighted sources. We propose BloomScrub, a remarkably simple yet highly effective inference-time approach that provides certified copyright takedown. Our method repeatedly interleaves quote detection with rewriting techniques to transform potentially infringing segments. By leveraging efficient data sketches (Bloom filters), our approach enables scalable copyright screening even for large-scale real-world corpora. When quotes beyond a length threshold cannot be removed, the system can abstain from responding, offering certified risk reduction. Experimental results show that BloomScrub reduces infringement risk, preserves utility, and accommodates different levels of enforcement stringency with adaptive abstention. Our results suggest that lightweight, inference-time methods can be surprisingly effective for copyright prevention.

**Link**: [arxiv](http://arxiv.org/abs/2504.16046v2),  [pdf](http://arxiv.org/pdf/2504.16046v2)

**Tags**: cs.CL 



### Improving QoS Prediction in Urban V2X Networks by Leveraging Data from   Leading Vehicles and Historical Trends
**Authors**: Sanket Partani, Michael Zentarra, Anthony Kiggundu, Hans D. Schotten

**Updated**: 2025-04-23T16:10:31Z

**Summary**: With the evolution of Vehicle-to-Everything (V2X) technology and increased deployment of 5G networks and edge computing, Predictive Quality of Service (PQoS) is seen as an enabler for resilient and adaptive V2X communication systems. PQoS incorporates data-driven techniques, such as Machine Learning (ML), to forecast/predict Key Performing Indicators (KPIs) such as throughput, latency, etc. In this paper, we aim to predict downlink throughput in an urban environment using the Berlin V2X cellular dataset. We select features from the ego and lead vehicles to train different ML models to help improve the predicted throughput for the ego vehicle. We identify these features based on an in-depth exploratory data analysis. Results show an improvement in model performance when adding features from the lead vehicle. Moreover, we show that the improvement in model performance is model-agnostic.

**Link**: [arxiv](http://arxiv.org/abs/2504.16848v1),  [pdf](http://arxiv.org/pdf/2504.16848v1)

**Tags**: cs.NI 



### On Benchmarking Code LLMs for Android Malware Analysis
**Authors**: Yiling He, Hongyu She, Xingzhi Qian, Xinran Zheng, Zhuo Chen, Zhan Qin, Lorenzo Cavallaro

**Updated**: 2025-04-23T16:07:20Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities in various code intelligence tasks. However, their effectiveness for Android malware analysis remains underexplored. Decompiled Android malware code presents unique challenges for analysis, due to the malicious logic being buried within a large number of functions and the frequent lack of meaningful function names. This paper presents CAMA, a benchmarking framework designed to systematically evaluate the effectiveness of Code LLMs in Android malware analysis. CAMA specifies structured model outputs to support key malware analysis tasks, including malicious function identification and malware purpose summarization. Built on these, it integrates three domain-specific evaluation metrics (consistency, fidelity, and semantic relevance), enabling rigorous stability and effectiveness assessment and cross-model comparison. We construct a benchmark dataset of 118 Android malware samples from 13 families collected in recent years, encompassing over 7.5 million distinct functions, and use CAMA to evaluate four popular open-source Code LLMs. Our experiments provide insights into how Code LLMs interpret decompiled code and quantify the sensitivity to function renaming, highlighting both their potential and current limitations in malware analysis.

**Link**: [arxiv](http://arxiv.org/abs/2504.00694v2),  [pdf](http://arxiv.org/pdf/2504.00694v2)

**Tags**: cs.CR cs.LG 



### Improving Significant Wave Height Prediction Using Chronos Models
**Authors**: Yilin Zhai, Hongyuan Shi, Chao Zhan, Qing Wang, Zaijin You, Nan Wang

**Updated**: 2025-04-23T15:56:28Z

**Summary**: Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.

**Link**: [arxiv](http://arxiv.org/abs/2504.16834v1),  [pdf](http://arxiv.org/pdf/2504.16834v1)

**Tags**: cs.LG cs.AI physics.ao-ph 



### LRASGen: LLM-based RESTful API Specification Generation
**Authors**: Sida Deng, Rubing Huang, Man Zhang, Chenhui Cui, Dave Towey, Rongcun Wang

**Updated**: 2025-04-23T15:52:50Z

**Summary**: REpresentation State Transfer (REST) is an architectural style for designing web applications that enable scalable, stateless communication between clients and servers via common HTTP techniques. Web APIs that employ the REST style are known as RESTful (or REST) APIs. When using or testing a RESTful API, developers may need to employ its specification, which is often defined by open-source standards such as the OpenAPI Specification (OAS). However, it can be very time-consuming and error-prone to write and update these specifications, which may negatively impact the use of RESTful APIs, especially when the software requirements change. Many tools and methods have been proposed to solve this problem, such as Respector and Swagger Core. OAS generation can be regarded as a common text-generation task that creates a formal description of API endpoints derived from the source code. A potential solution for this may involve using Large Language Models (LLMs), which have strong capabilities in both code understanding and text generation. Motivated by this, we propose a novel approach for generating the OASs of RESTful APIs using LLMs: LLM-based RESTful API-Specification Generation (LRASGen). To the best of our knowledge, this is the first use of LLMs and API source code to generate OASs for RESTful APIs. Compared with existing tools and methods, LRASGen can generate the OASs, even when the implementation is incomplete (with partial code, and/or missing annotations/comments, etc.). To evaluate the LRASGen performance, we conducted a series of empirical studies on 20 real-world RESTful APIs. The results show that two LLMs (GPT-4o mini and DeepSeek V3) can both support LARSGen to generate accurate specifications, and LRASGen-generated specifications cover an average of 48.85% more missed entities than the developer-provided specifications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16833v1),  [pdf](http://arxiv.org/pdf/2504.16833v1)

**Tags**: cs.SE 



### GreenMind: A Next-Generation Vietnamese Large Language Model for   Structured and Logical Reasoning
**Authors**: Luu Quy Tung, Hoang Quoc Viet, Vo Trong Thu

**Updated**: 2025-04-23T15:48:55Z

**Summary**: Chain-of-Thought (CoT) is a robust approach for tackling LLM tasks that require intermediate reasoning steps prior to generating a final answer. In this paper, we present GreenMind-Medium-14B-R1, the Vietnamese reasoning model inspired by the finetuning strategy based on Group Relative Policy Optimization. We also leverage a high-quality Vietnamese synthesized reasoning dataset and design two reward functions to tackle the main limitations of this technique: (i) language mixing, where we explicitly detect the presence of biased language characters during the process of sampling tokens, and (ii) we leverage Sentence Transformer-based models to ensure that the generated reasoning content maintains factual correctness and does not distort the final output. Experimental results on the Vietnamese dataset from the VLSP 2023 Challenge demonstrate that our model outperforms prior works and enhances linguistic consistency in its responses. Furthermore, we extend our evaluation to SeaExam-a multilingual multiple-choice dataset, showing the effectiveness of our reasoning method compared to few-shot prompting techniques.

**Link**: [arxiv](http://arxiv.org/abs/2504.16832v1),  [pdf](http://arxiv.org/pdf/2504.16832v1)

**Tags**: cs.CL 



### ChatDBG: Augmenting Debugging with Large Language Models
**Authors**: Kyla H. Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund

**Updated**: 2025-04-23T15:48:42Z

**Summary**: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.

**Link**: [arxiv](http://arxiv.org/abs/2403.16354v4),  [pdf](http://arxiv.org/pdf/2403.16354v4)

**Tags**: cs.SE cs.AI cs.LG cs.PL 



### Process Reward Models That Think
**Authors**: Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, Lu Wang

**Updated**: 2025-04-23T15:44:54Z

**Summary**: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models will be released at https://github.com/mukhal/thinkprm.

**Link**: [arxiv](http://arxiv.org/abs/2504.16828v1),  [pdf](http://arxiv.org/pdf/2504.16828v1)

**Tags**: cs.LG cs.AI cs.CL 



### LLM-assisted Graph-RAG Information Extraction from IFC Data
**Authors**: Sima Iranmanesh, Hadeel Saadany, Edlira Vakaj

**Updated**: 2025-04-23T15:31:11Z

**Summary**: IFC data has become the general building information standard for collaborative work in the construction industry. However, IFC data can be very complicated because it allows for multiple ways to represent the same product information. In this research, we utilise the capabilities of LLMs to parse the IFC data with Graph Retrieval-Augmented Generation (Graph-RAG) technique to retrieve building object properties and their relations. We will show that, despite limitations due to the complex hierarchy of the IFC data, the Graph-RAG parsing enhances generative LLMs like GPT-4o with graph-based knowledge, enabling natural language query-response retrieval without the need for a complex pipeline.

**Link**: [arxiv](http://arxiv.org/abs/2504.16813v1),  [pdf](http://arxiv.org/pdf/2504.16813v1)

**Tags**: cs.CL 



### Decoupled Global-Local Alignment for Improving Compositional   Understanding
**Authors**: Xiaoxing Hu, Kaicheng Yang, Jun Wang, Haoran Xu, Ziyong Feng, Yupei Wang

**Updated**: 2025-04-23T15:20:53Z

**Summary**: Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA

**Link**: [arxiv](http://arxiv.org/abs/2504.16801v1),  [pdf](http://arxiv.org/pdf/2504.16801v1)

**Tags**: cs.CV 



### How Effective are Generative Large Language Models in Performing   Requirements Classification?
**Authors**: Waad Alhoshan, Alessio Ferrari, Liping Zhao

**Updated**: 2025-04-23T14:41:11Z

**Summary**: In recent years, transformer-based large language models (LLMs) have revolutionised natural language processing (NLP), with generative models opening new possibilities for tasks that require context-aware text generation. Requirements engineering (RE) has also seen a surge in the experimentation of LLMs for different tasks, including trace-link detection, regulatory compliance, and others. Requirements classification is a common task in RE. While non-generative LLMs like BERT have been successfully applied to this task, there has been limited exploration of generative LLMs. This gap raises an important question: how well can generative LLMs, which produce context-aware outputs, perform in requirements classification? In this study, we explore the effectiveness of three generative LLMs-Bloom, Gemma, and Llama-in performing both binary and multi-class requirements classification. We design an extensive experimental study involving over 400 experiments across three widely used datasets (PROMISE NFR, Functional-Quality, and SecReq). Our study concludes that while factors like prompt design and LLM architecture are universally important, others-such as dataset variations-have a more situational impact, depending on the complexity of the classification task. This insight can guide future model development and deployment strategies, focusing on optimising prompt structures and aligning model architectures with task-specific needs for improved performance.

**Link**: [arxiv](http://arxiv.org/abs/2504.16768v1),  [pdf](http://arxiv.org/pdf/2504.16768v1)

**Tags**: cs.CL cs.AI cs.SE 



### Noise-Tolerant Coreset-Based Class Incremental Continual Learning
**Authors**: Edison Mucllari, Aswin Raghavan, Zachary Alan Daniels

**Updated**: 2025-04-23T14:34:20Z

**Summary**: Many applications of computer vision require the ability to adapt to novel data distributions after deployment. Adaptation requires algorithms capable of continual learning (CL). Continual learners must be plastic to adapt to novel tasks while minimizing forgetting of previous tasks.However, CL opens up avenues for noise to enter the training pipeline and disrupt the CL. This work focuses on label noise and instance noise in the context of class-incremental learning (CIL), where new classes are added to a classifier over time, and there is no access to external data from past classes. We aim to understand the sensitivity of CL methods that work by replaying items from a memory constructed using the idea of Coresets. We derive a new bound for the robustness of such a method to uncorrelated instance noise under a general additive noise threat model, revealing several insights. Putting the theory into practice, we create two continual learning algorithms to construct noise-tolerant replay buffers. We empirically compare the effectiveness of prior memory-based continual learners and the proposed algorithms under label and uncorrelated instance noise on five diverse datasets. We show that existing memory-based CL are not robust whereas the proposed methods exhibit significant improvements in maximizing classification accuracy and minimizing forgetting in the noisy CIL setting.

**Link**: [arxiv](http://arxiv.org/abs/2504.16763v1),  [pdf](http://arxiv.org/pdf/2504.16763v1)

**Tags**: cs.LG cs.AI cs.CV cs.NE 



### Lightweight Latent Verifiers for Efficient Meta-Generation Strategies
**Authors**: Bartosz Piotrowski, Witold Drzewakowski, Konrad Staniszewski, Piotr Miłoś

**Updated**: 2025-04-23T14:33:20Z

**Summary**: Verifiers are auxiliary models that assess the correctness of outputs generated by base large language models (LLMs). They play a crucial role in many strategies for solving reasoning-intensive problems with LLMs. Typically, verifiers are LLMs themselves, often as large (or larger) than the base model they support, making them computationally expensive. In this work, we introduce a novel lightweight verification approach, LiLaVe, which reliably extracts correctness signals from the hidden states of the base LLM. A key advantage of LiLaVe is its ability to operate with only a small fraction of the computational budget required by traditional LLM-based verifiers. To demonstrate its practicality, we couple LiLaVe with popular meta-generation strategies, like best-of-n or self-consistency. Moreover, we design novel LiLaVe-based approaches, like conditional self-correction or conditional majority voting, that significantly improve both accuracy and efficiency in generation tasks with smaller LLMs. Our work demonstrates the fruitfulness of extracting latent information from the hidden states of LLMs, and opens the door to scalable and resource-efficient solutions for reasoning-intensive applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16760v1),  [pdf](http://arxiv.org/pdf/2504.16760v1)

**Tags**: cs.AI 



### HEMA : A Hippocampus-Inspired Extended Memory Architecture for   Long-Context AI Conversations
**Authors**: Kwangseob Ahn

**Updated**: 2025-04-23T14:27:12Z

**Summary**: Large language models (LLMs) struggle with maintaining coherence in extended conversations spanning hundreds of turns, despite performing well within their context windows. This paper introduces HEMA (Hippocampus-Inspired Extended Memory Architecture), a dual-memory system inspired by human cognitive processes. HEMA combines Compact Memory - a continuously updated one-sentence summary preserving global narrative coherence, and Vector Memory - an episodic store of chunk embeddings queried via cosine similarity. When integrated with a 6B-parameter transformer, HEMA maintains coherent dialogues beyond 300 turns while keeping prompt length under 3,500 tokens. Experimental results show substantial improvements: factual recall accuracy increases from 41% to 87%, and human-rated coherence improves from 2.7 to 4.3 on a 5-point scale. With 10K indexed chunks, Vector Memory achieves P@5 >= 0.80 and R@50 >= 0.74, doubling the area under the precision-recall curve compared to summarization-only approaches. Ablation studies reveal two key insights: semantic forgetting through age-weighted pruning reduces retrieval latency by 34% with minimal recall loss, and a two-level summary hierarchy prevents cascade errors in ultra-long conversations exceeding 1,000 turns. HEMA demonstrates that combining verbatim recall with semantic continuity provides a practical solution for privacy-aware conversational AI capable of month-long dialogues without model retraining.

**Link**: [arxiv](http://arxiv.org/abs/2504.16754v1),  [pdf](http://arxiv.org/pdf/2504.16754v1)

**Tags**: cs.CL cs.AI 



### Right Question is Already Half the Answer: Fully Unsupervised LLM   Reasoning Incentivization
**Authors**: Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian

**Updated**: 2025-04-23T14:25:51Z

**Summary**: While large language models (LLMs) have demonstrated exceptional capabilities in challenging tasks such as mathematical reasoning, existing methods to enhance reasoning ability predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data after pre-training. However, these approaches critically depend on external supervision--such as human-labelled reasoning traces, verified golden answers, or pre-trained reward models--which limits scalability and practical applicability. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. EMPO does not require any supervised information for incentivizing reasoning capabilities (i.e., neither verifiable reasoning traces, problems with golden answers, nor additional pre-trained reward models). By continuously minimizing the predictive entropy of LLMs on unlabeled user queries in a latent semantic space, EMPO enables purely self-supervised evolution of reasoning capabilities with strong flexibility and practicality. Our experiments demonstrate competitive performance of EMPO on both mathematical reasoning and free-form natural reasoning tasks. Specifically, without any supervised signals, \ours boosts the accuracy of Qwen2.5-Math-7B Base from 30.7\% to 48.1\% on mathematical benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1\% to 50.1\% on MMLU-Pro.

**Link**: [arxiv](http://arxiv.org/abs/2504.05812v2),  [pdf](http://arxiv.org/pdf/2504.05812v2)

**Tags**: cs.LG 



### SemioLLM: Evaluating Large Language Models for Diagnostic Reasoning from   Unstructured Clinical Narratives in Epilepsy
**Authors**: Meghal Dani, Muthu Jeyanthi Prakash, Zeynep Akata, Stefanie Liebe

**Updated**: 2025-04-23T14:25:46Z

**Summary**: Large Language Models (LLMs) have been shown to encode clinical knowledge. Many evaluations, however, rely on structured question-answer benchmarks, overlooking critical challenges of interpreting and reasoning about unstructured clinical narratives in real-world settings. Using free-text clinical descriptions, we present SemioLLM, an evaluation framework that benchmarks 6 state-of-the-art models (GPT-3.5, GPT-4, Mixtral-8x7B, Qwen-72B, LlaMa2, LlaMa3) on a core diagnostic task in epilepsy. Leveraging a database of 1,269 seizure descriptions, we show that most LLMs are able to accurately and confidently generate probabilistic predictions of seizure onset zones in the brain. Most models approach clinician-level performance after prompt engineering, with expert-guided chain-of-thought reasoning leading to the most consistent improvements. Performance was further strongly modulated by clinical in-context impersonation, narrative length and language context (13.7%, 32.7% and 14.2% performance variation, respectively). However, expert analysis of reasoning outputs revealed that correct prediction can be based on hallucinated knowledge and deficient source citation accuracy, underscoring the need to improve interpretability of LLMs in clinical use. Overall, SemioLLM provides a scalable, domain-adaptable framework for evaluating LLMs in clinical disciplines where unstructured verbal descriptions encode diagnostic information. By identifying both the strengths and limitations of state-of-the-art models, our work supports the development of clinically robust and globally applicable AI systems for healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2407.03004v2),  [pdf](http://arxiv.org/pdf/2407.03004v2)

**Tags**: cs.CL cs.AI 



### A Survey of AI Agent Protocols
**Authors**: Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, Weinan Zhang

**Updated**: 2025-04-23T14:07:26Z

**Summary**: The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide a systematic overview of existing communication protocols for LLM agents. We classify them into four main categories and make an analysis to help users and developers select the most suitable protocols for specific applications. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore future challenges, such as how protocols can adapt and survive in fast-evolving environments, and what qualities future protocols might need to support the next generation of LLM agent ecosystems. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.

**Link**: [arxiv](http://arxiv.org/abs/2504.16736v1),  [pdf](http://arxiv.org/pdf/2504.16736v1)

**Tags**: cs.AI 



### Robotic World Model: A Neural Network Simulator for Robust Policy   Optimization in Robotics
**Authors**: Chenhao Li, Andreas Krause, Marco Hutter

**Updated**: 2025-04-24T05:33:20Z

**Summary**: Learning robust and generalizable world models is crucial for enabling efficient and scalable robotic control in real-world environments. In this work, we introduce a novel framework for learning world models that accurately capture complex, partially observable, and stochastic dynamics. The proposed method employs a dual-autoregressive mechanism and self-supervised training to achieve reliable long-horizon predictions without relying on domain-specific inductive biases, ensuring adaptability across diverse robotic tasks. We further propose a policy optimization framework that leverages world models for efficient training in imagined environments and seamless deployment in real-world systems. This work advances model-based reinforcement learning by addressing the challenges of long-horizon prediction, error accumulation, and sim-to-real transfer. By providing a scalable and robust framework, the introduced methods pave the way for adaptive and efficient robotic systems in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2501.10100v3),  [pdf](http://arxiv.org/pdf/2501.10100v3)

**Tags**: cs.RO cs.AI cs.LG 



### IRIS: Interactive Research Ideation System for Accelerating Scientific   Discovery
**Authors**: Aniketh Garikaparthi, Manasi Patwardhan, Lovekesh Vig, Arman Cohan

**Updated**: 2025-04-23T14:01:36Z

**Summary**: The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System

**Link**: [arxiv](http://arxiv.org/abs/2504.16728v1),  [pdf](http://arxiv.org/pdf/2504.16728v1)

**Tags**: cs.AI cs.CL 



### Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical   Question Answering with Small-Scale LLMs
**Authors**: Yingjian Chen, Feiyang Li, Xingyu Song, Tianxiao Li, Zixin Xu, Xiujie Chen, Issey Sukeda, Irene Li

**Updated**: 2025-04-23T13:54:01Z

**Summary**: Large language models (LLMs) perform well in medical QA, but their effectiveness in Japanese contexts is limited due to privacy constraints that prevent the use of commercial models like GPT-4 in clinical settings. As a result, recent efforts focus on instruction-tuning open-source LLMs, though the potential of combining them with retrieval-augmented generation (RAG) remains underexplored. To bridge this gap, we are the first to explore a knowledge graph-based (KG) RAG framework for Japanese medical QA small-scale open-source LLMs. Experimental results show that KG-based RAG has only a limited impact on Japanese medical QA using small-scale open-source LLMs. Further case studies reveal that the effectiveness of the RAG is sensitive to the quality and relevance of the external retrieved content. These findings offer valuable insights into the challenges and potential of applying RAG in Japanese medical QA, while also serving as a reference for other low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.10982v4),  [pdf](http://arxiv.org/pdf/2504.10982v4)

**Tags**: cs.CL cs.AI 



### From Human Memory to AI Memory: A Survey on Memory Mechanisms in the Era   of LLMs
**Authors**: Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, Yong Liu

**Updated**: 2025-04-23T13:47:27Z

**Summary**: Memory is the process of encoding, storing, and retrieving information, allowing humans to retain experiences, knowledge, skills, and facts over time, and serving as the foundation for growth and effective interaction with the world. It plays a crucial role in shaping our identity, making decisions, learning from past experiences, building relationships, and adapting to changes. In the era of large language models (LLMs), memory refers to the ability of an AI system to retain, recall, and use information from past interactions to improve future responses and interactions. Although previous research and reviews have provided detailed descriptions of memory mechanisms, there is still a lack of a systematic review that summarizes and analyzes the relationship between the memory of LLM-driven AI systems and human memory, as well as how we can be inspired by human memory to construct more powerful memory systems. To achieve this, in this paper, we propose a comprehensive survey on the memory of LLM-driven AI systems. In particular, we first conduct a detailed analysis of the categories of human memory and relate them to the memory of AI systems. Second, we systematically organize existing memory-related work and propose a categorization method based on three dimensions (object, form, and time) and eight quadrants. Finally, we illustrate some open problems regarding the memory of current AI systems and outline possible future directions for memory in the era of large language models.

**Link**: [arxiv](http://arxiv.org/abs/2504.15965v2),  [pdf](http://arxiv.org/pdf/2504.15965v2)

**Tags**: cs.IR H.0 



### Resource Reduction in Multiparty Quantum Secret Sharing of both   Classical and Quantum Information under Noisy Scenario
**Authors**: Nirupam Basak, Goutam Paul

**Updated**: 2025-04-23T13:40:06Z

**Summary**: Quantum secret sharing (QSS) enables secure distribution of information among multiple parties but remains vulnerable to noise. We analyze the effects of bit-flip, phase-flip, and amplitude damping noise on the multiparty QSS for classical message (QSSCM) and secret sharing of quantum information (SSQI) protocols proposed by Zhang et al. (Phys. Rev. A, 71:044301, 2005). To scale down these effects, we introduce an efficient quantum error correction (QEC) scheme based on a simplified version of Shor's code. Leveraging the specific structure of the QSS protocols, we reduce the qubit overhead from the standard 9 of Shor's code to as few as 3 while still achieving lower average error rates than existing QEC methods. Thus, our approach can also be adopted for other single-qubit-based quantum protocols. Simulations demonstrate that our approach significantly enhances the protocols' resilience, improving their practicality for real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2504.16709v1),  [pdf](http://arxiv.org/pdf/2504.16709v1)

**Tags**: quant-ph cs.CR 



### Synthetic Lyrics Detection Across Languages and Genres
**Authors**: Yanis Labrak, Markus Frohmann, Gabriel Meseguer-Brocal, Elena V. Epure

**Updated**: 2025-04-24T07:21:44Z

**Summary**: In recent years, the use of large language models (LLMs) to generate music content, particularly lyrics, has gained in popularity. These advances provide valuable tools for artists and enhance their creative processes, but they also raise concerns about copyright violations, consumer satisfaction, and content spamming. Previous research has explored content detection in various domains. However, no work has focused on the text modality, lyrics, in music. To address this gap, we curated a diverse dataset of real and synthetic lyrics from multiple languages, music genres, and artists. The generation pipeline was validated using both humans and automated methods. We performed a thorough evaluation of existing synthetic text detection approaches on lyrics, a previously unexplored data type. We also investigated methods to adapt the best-performing features to lyrics through unsupervised domain adaptation. Following both music and industrial constraints, we examined how well these approaches generalize across languages, scale with data availability, handle multilingual language content, and perform on novel genres in few-shot settings. Our findings show promising results that could inform policy decisions around AI-generated music and enhance transparency for users.

**Link**: [arxiv](http://arxiv.org/abs/2406.15231v4),  [pdf](http://arxiv.org/pdf/2406.15231v4)

**Tags**: cs.CL cs.AI cs.LG 



### A Statistical Evaluation of Indoor LoRaWAN Environment-Aware Propagation   for 6G: MLR, ANOVA, and Residual Distribution Analysis
**Authors**: Nahshon Mokua Obiri, Kristof Van Laerhoven

**Updated**: 2025-04-24T08:37:38Z

**Summary**: Modeling path loss in indoor LoRaWAN technology deployments is inherently challenging due to structural obstructions, occupant density and activities, and fluctuating environmental conditions. This study proposes a two-stage approach to capture and analyze these complexities using an extensive dataset of 1,328,334 field measurements collected over six months in a single-floor office at the University of Siegen's Hoelderlinstrasse Campus, Germany. First, we implement a multiple linear regression model that includes traditional propagation metrics (distance, structural walls) and an extension with proposed environmental variables (relative humidity, temperature, carbon dioxide, particulate matter, and barometric pressure). Using analysis of variance, we demonstrate that adding these environmental factors can reduce unexplained variance by 42.32 percent. Secondly, we examine residual distributions by fitting five candidate probability distributions: Normal, Skew-Normal, Cauchy, Student's t, and Gaussian Mixture Models with one to five components. Our results show that a four-component Gaussian Mixture Model captures the residual heterogeneity of indoor signal propagation most accurately, significantly outperforming single-distribution approaches. Given the push toward ultra-reliable, context-aware communications in 6G networks, our analysis shows that environment-aware modeling can substantially improve LoRaWAN network design in dynamic indoor IoT deployments.

**Link**: [arxiv](http://arxiv.org/abs/2504.16688v2),  [pdf](http://arxiv.org/pdf/2504.16688v2)

**Tags**: cs.NI cs.LG eess.SP 



### The MRS UAV System: Pushing the Frontiers of Reproducible Research,   Real-world Deployment, and Education with Autonomous Unmanned Aerial Vehicles
**Authors**: Tomas Baca, Matej Petrlik, Matous Vrba, Vojtech Spurny, Robert Penicka, Daniel Hert, Martin Saska

**Updated**: 2025-04-23T13:12:39Z

**Summary**: We present a multirotor Unmanned Aerial Vehicle control (UAV) and estimation system for supporting replicable research through realistic simulations and real-world experiments. We propose a unique multi-frame localization paradigm for estimating the states of a UAV in various frames of reference using multiple sensors simultaneously. The system enables complex missions in GNSS and GNSS-denied environments, including outdoor-indoor transitions and the execution of redundant estimators for backing up unreliable localization sources. Two feedback control designs are presented: one for precise and aggressive maneuvers, and the other for stable and smooth flight with a noisy state estimate. The proposed control and estimation pipeline are constructed without using the Euler/Tait-Bryan angle representation of orientation in 3D. Instead, we rely on rotation matrices and a novel heading-based convention to represent the one free rotational degree-of-freedom in 3D of a standard multirotor helicopter. We provide an actively maintained and well-documented open-source implementation, including realistic simulation of UAV, sensors, and localization systems. The proposed system is the product of years of applied research on multi-robot systems, aerial swarms, aerial manipulation, motion planning, and remote sensing. All our results have been supported by real-world system deployment that shaped the system into the form presented here. In addition, the system was utilized during the participation of our team from the CTU in Prague in the prestigious MBZIRC 2017 and 2020 robotics competitions, and also in the DARPA SubT challenge. Each time, our team was able to secure top places among the best competitors from all over the world. On each occasion, the challenges has motivated the team to improve the system and to gain a great amount of high-quality experience within tight deadlines.

**Link**: [arxiv](http://arxiv.org/abs/2008.08050v6),  [pdf](http://arxiv.org/pdf/2008.08050v6)

**Tags**: cs.RO cs.SY eess.SY 



### NovelQA: Benchmarking Question Answering on Documents Exceeding 200K   Tokens
**Authors**: Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Xiangkun Hu, Zheng Zhang, Qian Wang, Yue Zhang

**Updated**: 2025-04-23T12:52:18Z

**Summary**: Recent advancements in Large Language Models (LLMs) have pushed the boundaries of natural language processing, especially in long-context understanding. However, the evaluation of these models' long-context abilities remains a challenge due to the limitations of current benchmarks. To address this gap, we introduce NovelQA, a benchmark tailored for evaluating LLMs with complex, extended narratives. Constructed from English novels, NovelQA offers a unique blend of complexity, length, and narrative coherence, making it an ideal tool for assessing deep textual understanding in LLMs. This paper details the design and construction of NovelQA, focusing on its comprehensive manual annotation process and the variety of question types aimed at evaluating nuanced comprehension. Our evaluation of long-context LLMs on NovelQA reveals significant insights into their strengths and weaknesses. Notably, the models struggle with multi-hop reasoning, detail-oriented questions, and handling extremely long inputs, with average lengths exceeding 200,000 tokens. Results highlight the need for substantial advancements in LLMs to enhance their long-context comprehension and contribute effectively to computational literary analysis.

**Link**: [arxiv](http://arxiv.org/abs/2403.12766v3),  [pdf](http://arxiv.org/pdf/2403.12766v3)

**Tags**: cs.CL 



### LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative   Analysis
**Authors**: Joel Oksanen, Andrés Lucero, Perttu Hämäläinen

**Updated**: 2025-04-23T12:39:06Z

**Summary**: The use of large language models (LLMs) in qualitative analysis offers enhanced efficiency but raises questions about their alignment with the contextual nature of research for design (RfD). This research examines the trustworthiness of LLM-driven design insights, using qualitative coding as a case study to explore the interpretive processes central to RfD. We introduce LLMCode, an open-source tool integrating two metrics, namely Intersection over Union (IoU) and Modified Hausdorff Distance, to assess the alignment between human and LLM-generated insights. Across two studies involving 26 designers, we find that while the model performs well with deductive coding, its ability to emulate a designer's deeper interpretive lens over the data is limited, emphasising the importance of human-AI collaboration. Our results highlight a reciprocal dynamic where users refine LLM outputs and adapt their own perspectives based on the model's suggestions. These findings underscore the importance of fostering appropriate reliance on LLMs by designing tools that preserve interpretive depth while facilitating intuitive collaboration between designers and AI.

**Link**: [arxiv](http://arxiv.org/abs/2504.16671v1),  [pdf](http://arxiv.org/pdf/2504.16671v1)

**Tags**: cs.HC 



### Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation
**Authors**: Chenxi Li, Weining Lu, Qingquan Lin, Litong Meng, Haolu Li, Bin Liang

**Updated**: 2025-04-23T12:12:50Z

**Summary**: This paper proposes a lightweight systematic solution for multi-robot coordinated navigation with decentralized cooperative perception. An information flow is first created to facilitate real-time observation sharing over unreliable ad-hoc networks. Then, the environmental uncertainties of each robot are reduced by interaction fields that deliver complementary information. Finally, path optimization is achieved, enabling self-organized coordination with effective convergence, divergence, and collision avoidance. Our method is fully interpretable and ready for deployment without gaps. Comprehensive simulations and real-world experiments demonstrate reduced path redundancy, robust performance across various tasks, and minimal demands on computation and communication.

**Link**: [arxiv](http://arxiv.org/abs/2409.10049v2),  [pdf](http://arxiv.org/pdf/2409.10049v2)

**Tags**: cs.RO 



### ParetoHqD: Fast Offline Multiobjective Alignment of Large Language   Models using Pareto High-quality Data
**Authors**: Haoran Gu, Handing Wang, Yi Mei, Mengjie Zhang, Yaochu Jin

**Updated**: 2025-04-23T11:35:57Z

**Summary**: Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as ''high-quality'' data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.16628v1),  [pdf](http://arxiv.org/pdf/2504.16628v1)

**Tags**: cs.LG cs.CL 



### TIFIN India at SemEval-2025: Harnessing Translation to Overcome   Multilingual IR Challenges in Fact-Checked Claim Retrieval
**Authors**: Prasanna Devadiga, Arya Suneesh, Pawan Kumar Rajpoot, Bharatdeep Hazarika, Aditya U Baliga

**Updated**: 2025-04-23T11:34:35Z

**Summary**: We address the challenge of retrieving previously fact-checked claims in monolingual and crosslingual settings - a critical task given the global prevalence of disinformation. Our approach follows a two-stage strategy: a reliable baseline retrieval system using a fine-tuned embedding model and an LLM-based reranker. Our key contribution is demonstrating how LLM-based translation can overcome the hurdles of multilingual information retrieval. Additionally, we focus on ensuring that the bulk of the pipeline can be replicated on a consumer GPU. Our final integrated system achieved a success@10 score of 0.938 and 0.81025 on the monolingual and crosslingual test sets, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2504.16627v1),  [pdf](http://arxiv.org/pdf/2504.16627v1)

**Tags**: cs.CL 



### Leveraging LLMs for User Stories in AI Systems: UStAI Dataset
**Authors**: Asma Yamani, Malak Baslyman, Moataz Ahmed

**Updated**: 2025-04-23T11:26:49Z

**Summary**: AI systems are gaining widespread adoption across various sectors and domains. Creating high-quality AI system requirements is crucial for aligning the AI system with business goals and consumer values and for social responsibility. However, with the uncertain nature of AI systems and the heavy reliance on sensitive data, more research is needed to address the elicitation and analysis of AI systems requirements. With the proprietary nature of many AI systems, there is a lack of open-source requirements artifacts and technical requirements documents for AI systems, limiting broader research and investigation. With Large Language Models (LLMs) emerging as a promising alternative to human-generated text, this paper investigates the potential use of LLMs to generate user stories for AI systems based on abstracts from scholarly papers. We conducted an empirical evaluation using three LLMs and generated $1260$ user stories from $42$ abstracts from $26$ domains. We assess their quality using the Quality User Story (QUS) framework. Moreover, we identify relevant non-functional requirements (NFRs) and ethical principles. Our analysis demonstrates that the investigated LLMs can generate user stories inspired by the needs of various stakeholders, offering a promising approach for generating user stories for research purposes and for aiding in the early requirements elicitation phase of AI systems. We have compiled and curated a collection of stories generated by various LLMs into a dataset (UStAI), which is now publicly available for use.

**Link**: [arxiv](http://arxiv.org/abs/2504.00513v2),  [pdf](http://arxiv.org/pdf/2504.00513v2)

**Tags**: cs.SE cs.AI 



### Cognitive Silicon: An Architectural Blueprint for Post-Industrial   Computing Systems
**Authors**: Christoforus Yoga Haryanto, Emily Lomempow

**Updated**: 2025-04-23T11:24:30Z

**Summary**: Autonomous AI systems reveal foundational limitations in deterministic, human-authored computing architectures. This paper presents Cognitive Silicon: a hypothetical full-stack architectural framework projected toward 2035, exploring a possible trajectory for cognitive computing system design. The proposed architecture would integrate symbolic scaffolding, governed memory, runtime moral coherence, and alignment-aware execution across silicon-to-semantics layers. Our design grammar has emerged from dialectical co-design with LLMs under asymmetric epistemic conditions--creating structured friction to expose blind spots and trade-offs. The envisioned framework would establish mortality as a natural consequence of physical constraints, non-copyable tacit knowledge, and non-cloneable identity keys as cognitive-embodiment primitives. Core tensions (trust/agency, scaffolding/emergence, execution/governance) would function as central architectural pressures rather than edge cases. The architecture theoretically converges with the Free Energy Principle, potentially offering a formal account of how cognitive systems could maintain identity through prediction error minimization across physical and computational boundaries. The resulting framework aims to deliver a morally tractable cognitive infrastructure that could maintain human-alignment through irreversible hardware constraints and identity-bound epistemic mechanisms resistant to replication or subversion.

**Link**: [arxiv](http://arxiv.org/abs/2504.16622v1),  [pdf](http://arxiv.org/pdf/2504.16622v1)

**Tags**: cs.AI cs.CY 



### Algorithmic Mirror: Designing an Interactive Tool to Promote   Self-Reflection for YouTube Recommendations
**Authors**: Yui Kondo, Kevin Dunnell, Qing Xiao, Jun Zhao, Luc Rocher

**Updated**: 2025-04-23T11:00:17Z

**Summary**: Big Data analytics and Artificial Intelligence systems derive non-intuitive and often unverifiable inferences about individuals' behaviors, preferences, and private lives. Drawing on diverse, feature-rich datasets of unpredictable value, these systems erode the intuitive connection between our actions and how we are perceived, diminishing control over our digital identities. While Explainable Artificial Intelligence scholars have attempted to explain the inner workings of algorithms, their visualizations frequently overwhelm end-users with complexity. This research introduces 'hypothetical inference', a novel approach that uses language models to simulate how algorithms might interpret users' digital footprints and infer personal characteristics without requiring access to proprietary platform algorithms. Through empirical studies with fourteen adult participants, we identified three key design opportunities to foster critical algorithmic literacy: (1) reassembling scattered digital footprints into a unified map, (2) simulating algorithmic inference through LLM-generated interpretations, and (3) incorporating temporal dimensions to visualize evolving patterns. This research lays the groundwork for tools that can help users recognize the influence of data on platforms and develop greater autonomy in increasingly algorithm-mediated digital environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.16615v1),  [pdf](http://arxiv.org/pdf/2504.16615v1)

**Tags**: cs.HC 



### Reinforcement Learning With LLMs Interaction For Distributed Diffusion   Model Services
**Authors**: Hongyang Du, Ruichen Zhang, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shuguang Cui, Xuemin Shen, Dong In Kim

**Updated**: 2025-04-23T11:00:16Z

**Summary**: Distributed Artificial Intelligence-Generated Content (AIGC) has attracted significant attention, but two key challenges remain: maximizing subjective Quality of Experience (QoE) and improving energy efficiency, which are particularly pronounced in widely adopted Generative Diffusion Model (GDM)-based image generation services. In this paper, we propose a novel user-centric Interactive AI (IAI) approach for service management, with a distributed GDM-based AIGC framework that emphasizes efficient and cooperative deployment. The proposed method restructures the GDM inference process by allowing users with semantically similar prompts to share parts of the denoising chain. Furthermore, to maximize the users' subjective QoE, we propose an IAI approach, i.e., Reinforcement Learning With Large Language Models Interaction (RLLI), which utilizes Large Language Model (LLM)-empowered generative agents to replicate user interaction, providing real-time and subjective QoE feedback aligned with diverse user personalities. Lastly, we present the GDM-based Deep Deterministic Policy Gradient (GDDPG) algorithm, adapted to the proposed RLLI framework, to allocate communication and computing resources effectively while accounting for subjective user traits and dynamic wireless conditions. Simulation results demonstrate that G-DDPG improves total QoE by 15% compared with the standard DDPG algorithm.

**Link**: [arxiv](http://arxiv.org/abs/2311.11094v2),  [pdf](http://arxiv.org/pdf/2311.11094v2)

**Tags**: cs.NI 



### UAV-Mounted IRS (UMI) in the Presence of Hovering Fluctuations: 3D   Pattern Characterization and Performance Analysis
**Authors**: Mohammad Javad Zakavi, Mahtab Mirmohseni, Farid Ashtiani, Masoumeh Nasiri-Kenari

**Updated**: 2025-04-23T10:59:03Z

**Summary**: This paper investigates unmanned aerial vehicle (UAV)-mounted intelligent reflecting surfaces (IRS) to leverage the benefits of this technology for future communication networks, such as 6G. Key advantages include enhanced spectral and energy efficiency, expanded network coverage, and flexible deployment. One of the main challenges in employing UAV-mounted IRS (UMI) technology is the random fluctuations of hovering UAVs. Focusing on this challenge, this paper explores the capabilities of UMI with passive/active elements affected by UAV fluctuations in both horizontal and vertical angles, considering the three-dimensional (3D) radiation pattern of the IRS. The relationship between UAV fluctuations and IRS pattern is investigated by taking into account the random angular vibrations of UAVs. A tractable and closed-form distribution function for the IRS pattern is derived, using linear approximation and by dividing it into several sectors. In addition, closed-form expressions for outage probability (OP) are obtained using central limit theorem (CLT) and Gamma approximation. The theoretical expressions are validated through Monte Carlo simulations. The findings indicate that the random fluctuations of hovering UAVs have a notable impact on the performance of UMI systems. To avoid link interruptions due to UAV instability, IRS should utilize fewer elements, even though this leads to a decrease in directivity. As a result, unlike terrestrial IRS, incorporating more elements into aerial IRS systems does not necessarily improve performance due to the fluctuations in UAV. Numerical results show that the OP can be minimized by selecting the optimal number of IRS elements and using active elements.

**Link**: [arxiv](http://arxiv.org/abs/2504.16613v1),  [pdf](http://arxiv.org/pdf/2504.16613v1)

**Tags**: eess.SP 



### Routine haematological markers can predict and discriminate health   status and biological age even from noisy sources
**Authors**: Santiago Hernández-Orozco, Abicumaran Uthamacumaran, Francisco Hernández-Quiroz, Kourosh Saeb-Parsy, Hector Zenil

**Updated**: 2025-04-23T10:58:38Z

**Summary**: For more than two decades, advances in personalised medicine and precision healthcare have largely been based on genomics and other omics data. These strategies aim to tailor interventions to individual patient profiles, promising greater treatment efficacy and more efficient allocation of healthcare resources. Here, we show that widely collected common hematologic markers can reliably predict and discriminate individual chronological age and health status from even noisy sources. Our analysis includes synthetic and real retrospective patient data, including medically relevant and extreme cases, and draws on more than 100\,000 complete blood count records over 13 years from the United States Centers for Disease Control and Prevention's National Health and Nutrition Examination Survey (CDC NHANES). We combine fully explainable risk assessment scores with machine and deep learning techniques to focus on clinically significant characteristics without functioning purely as a ``black-box model allowing interpretation and control. Unlike current biological ageing indicators, this approach may offer rapid, and scalable implementations of personalised, precision and predictive approaches to healthcare and medicine without requiring specialised, uncommon or costly deployment of tests.

**Link**: [arxiv](http://arxiv.org/abs/2303.01444v8),  [pdf](http://arxiv.org/pdf/2303.01444v8)

**Tags**: q-bio.QM 



### Debunking with Dialogue? Exploring AI-Generated Counterspeech to   Challenge Conspiracy Theories
**Authors**: Mareike Lisker, Christina Gottschalk, Helena Mihaljević

**Updated**: 2025-04-23T10:32:45Z

**Summary**: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.

**Link**: [arxiv](http://arxiv.org/abs/2504.16604v1),  [pdf](http://arxiv.org/pdf/2504.16604v1)

**Tags**: cs.CL cs.AI cs.SI I.2.7 



### Comparing Large Language Models and Traditional Machine Translation   Tools for Translating Medical Consultation Summaries: A Pilot Study
**Authors**: Andy Li, Wei Zhou, Rashina Hoda, Chris Bain, Peter Poon

**Updated**: 2025-04-23T10:31:33Z

**Summary**: This study evaluates how well large language models (LLMs) and traditional machine translation (MT) tools translate medical consultation summaries from English into Arabic, Chinese, and Vietnamese. It assesses both patient, friendly and clinician, focused texts using standard automated metrics. Results showed that traditional MT tools generally performed better, especially for complex texts, while LLMs showed promise, particularly in Vietnamese and Chinese, when translating simpler summaries. Arabic translations improved with complexity due to the language's morphology. Overall, while LLMs offer contextual flexibility, they remain inconsistent, and current evaluation metrics fail to capture clinical relevance. The study highlights the need for domain-specific training, improved evaluation methods, and human oversight in medical translation.

**Link**: [arxiv](http://arxiv.org/abs/2504.16601v1),  [pdf](http://arxiv.org/pdf/2504.16601v1)

**Tags**: cs.CL cs.AI 



### Advances in Embodied Navigation Using Large Language Models: A Survey
**Authors**: Jinzhou Lin, Han Gao, Xuxiang Feng, Rongtao Xu, Changwei Wang, Man Zhang, Li Guo, Shibiao Xu

**Updated**: 2025-04-23T10:26:16Z

**Summary**: In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN.

**Link**: [arxiv](http://arxiv.org/abs/2311.00530v5),  [pdf](http://arxiv.org/pdf/2311.00530v5)

**Tags**: cs.AI 



### Chain-of-Thought Textual Reasoning for Few-shot Temporal Action   Localization
**Authors**: Hongwei Ji, Wulian Yun, Mengshi Qi, Huadong Ma

**Updated**: 2025-04-23T10:12:52Z

**Summary**: Traditional temporal action localization (TAL) methods rely on large amounts of detailed annotated data, whereas few-shot TAL reduces this dependence by using only a few training samples to identify unseen action categories. However, existing few-shot TAL methods typically focus solely on video-level information, neglecting textual information, which can provide valuable semantic support for the localization task. Therefore, we propose a new few-shot temporal action localization method by Chain-of-Thought textual reasoning to improve localization performance. Specifically, we design a novel few-shot learning framework that leverages textual semantic information to enhance the model's ability to capture action commonalities and variations, which includes a semantic-aware text-visual alignment module designed to align the query and support videos at different levels. Meanwhile, to better express the temporal dependencies and causal relationships between actions at the textual level to assist action localization, we design a Chain of Thought (CoT)-like reasoning method that progressively guides the Vision Language Model (VLM) and Large Language Model (LLM) to generate CoT-like text descriptions for videos. The generated texts can capture more variance of action than visual features. We conduct extensive experiments on the publicly available ActivityNet1.3 and THUMOS14 datasets. We introduce the first dataset named Human-related Anomaly Localization and explore the application of the TAL task in human anomaly detection. The experimental results demonstrate that our proposed method significantly outperforms existing methods in single-instance and multi-instance scenarios. We will release our code, data and benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2504.13460v2),  [pdf](http://arxiv.org/pdf/2504.13460v2)

**Tags**: cs.CV cs.AI 



### LaMsS: When Large Language Models Meet Self-Skepticism
**Authors**: Yetao Wu, Yihong Wang, Teng Chen, Ningyuan Xi, Qingqing Gu, Hongyang Lei, Luo Ji

**Updated**: 2025-04-24T03:29:59Z

**Summary**: Hallucination is a major challenge for large language models (LLMs), preventing their further application in some fields. The skeptical thinking of humankind could be useful for LLMs to self-cognition, self-reflection and alleviate their hallucinations. Inspired by this consideration, we propose a novel approach called LaMsS, which combines the semantic understanding capability of LLMs with self-skepticism. By introducing a series of skepticism tokens and augmenting them into the vocabulary, we conduct both pertaining and finetuning, which allow the LLM to decode each normal token followed by a skeptical token, representing different skepticism levels. By calculating the response skepticism given a query, one can define a new self-aware LLM which is only willing to answer with relative lower skepticism level than the threshold. By examining the accuracy, AUC and AP of willingly answering questions, we demonstrate that LaMsS achieves better performance than baselines on both multi-choice questions and open-domain question-answering benchmarks, and can generalize to multi-task and out-of-domain settings. Our study sheds some lights on the self-skepticism modeling on further artificial intelligence. Project code and model checkpoints can be found in https://anonymous.4open.science/r/SM-1E76.

**Link**: [arxiv](http://arxiv.org/abs/2409.06601v3),  [pdf](http://arxiv.org/pdf/2409.06601v3)

**Tags**: cs.CL cs.LG 



### Case Study: Fine-tuning Small Language Models for Accurate and Private   CWE Detection in Python Code
**Authors**: Md. Azizul Hakim Bappy, Hossen A Mustafa, Prottoy Saha, Rajinus Salehat

**Updated**: 2025-04-23T10:05:27Z

**Summary**: Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows.

**Link**: [arxiv](http://arxiv.org/abs/2504.16584v1),  [pdf](http://arxiv.org/pdf/2504.16584v1)

**Tags**: cs.CR cs.AI 



### CAPO: Cost-Aware Prompt Optimization
**Authors**: Tom Zehle, Moritz Schlager, Timo Heiß, Matthias Feurer

**Updated**: 2025-04-23T09:59:31Z

**Summary**: Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2504.16005v2),  [pdf](http://arxiv.org/pdf/2504.16005v2)

**Tags**: cs.CL cs.AI cs.NE stat.ML 



### PIS: Linking Importance Sampling and Attention Mechanisms for Efficient   Prompt Compression
**Authors**: Lizhe Chen, Binjia Zhou, Yuyao Ge, Jiayi Chen, Shiguang NI

**Updated**: 2025-04-23T09:53:01Z

**Summary**: Large language models (LLMs) have achieved remarkable progress, demonstrating unprecedented capabilities across various natural language processing tasks. However, the high costs associated with such exceptional performance limit the widespread adoption of LLMs, highlighting the need for prompt compression. Existing prompt compression methods primarily rely on heuristic truncation or abstractive summarization techniques, which fundamentally overlook the intrinsic mechanisms of LLMs and lack a systematic evaluation of token importance for generation. In this work, we introduce Prompt Importance Sampling (PIS), a novel compression framework that dynamically compresses prompts by sampling important tokens based on the analysis of attention scores of hidden states. PIS employs a dual-level compression mechanism: 1) at the token level, we quantify saliency using LLM-native attention scores and implement adaptive compression through a lightweight 9-layer reinforcement learning (RL) network; 2) at the semantic level, we propose a Russian roulette sampling strategy for sentence-level importance sampling. Comprehensive evaluations across multiple domain benchmarks demonstrate that our method achieves state-of-the-art compression performance. Notably, our framework serendipitously enhances reasoning efficiency through optimized context structuring. This work advances prompt engineering by offering both theoretical grounding and practical efficiency in context management for LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.16574v1),  [pdf](http://arxiv.org/pdf/2504.16574v1)

**Tags**: cs.CL cs.AI 



### PsyCounAssist: A Full-Cycle AI-Powered Psychological Counseling   Assistant System
**Authors**: Xianghe Liu, Jiaqi Xu, Tao Sun

**Updated**: 2025-04-23T09:49:05Z

**Summary**: Psychological counseling is a highly personalized and dynamic process that requires therapists to continuously monitor emotional changes, document session insights, and maintain therapeutic continuity. In this paper, we introduce PsyCounAssist, a comprehensive AI-powered counseling assistant system specifically designed to augment psychological counseling practices. PsyCounAssist integrates multimodal emotion recognition combining speech and photoplethysmography (PPG) signals for accurate real-time affective analysis, automated structured session reporting using large language models (LLMs), and personalized AI-generated follow-up support. Deployed on Android-based tablet devices, the system demonstrates practical applicability and flexibility in real-world counseling scenarios. Experimental evaluation confirms the reliability of PPG-based emotional classification and highlights the system's potential for non-intrusive, privacy-aware emotional support. PsyCounAssist represents a novel approach to ethically and effectively integrating AI into psychological counseling workflows.

**Link**: [arxiv](http://arxiv.org/abs/2504.16573v1),  [pdf](http://arxiv.org/pdf/2504.16573v1)

**Tags**: cs.HC cs.AI 



### Enhancing LLM-Based Agents via Global Planning and Hierarchical   Execution
**Authors**: Junjie Chen, Haitao Li, Jingli Yang, Yiqun Liu, Qingyao Ai

**Updated**: 2025-04-23T09:43:40Z

**Summary**: Intelligent agent systems based on Large Language Models (LLMs) have shown great potential in real-world applications. However, existing agent frameworks still face critical limitations in task planning and execution, restricting their effectiveness and generalizability. Specifically, current planning methods often lack clear global goals, leading agents to get stuck in local branches, or produce non-executable plans. Meanwhile, existing execution mechanisms struggle to balance complexity and stability, and their limited action space restricts their ability to handle diverse real-world tasks. To address these limitations, we propose GoalAct, a novel agent framework that introduces a continuously updated global planning mechanism and integrates a hierarchical execution strategy. GoalAct decomposes task execution into high-level skills, including searching, coding, writing and more, thereby reducing planning complexity while enhancing the agents' adaptability across diverse task scenarios. We evaluate GoalAct on LegalAgentBench, a benchmark with multiple types of legal tasks that require the use of multiple types of tools. Experimental results demonstrate that GoalAct achieves state-of-the-art (SOTA) performance, with an average improvement of 12.22% in success rate. These findings highlight GoalAct's potential to drive the development of more advanced intelligent agent systems, making them more effective across complex real-world applications. Our code can be found at https://github.com/cjj826/GoalAct.

**Link**: [arxiv](http://arxiv.org/abs/2504.16563v1),  [pdf](http://arxiv.org/pdf/2504.16563v1)

**Tags**: cs.IR 



### DTVM: Revolutionizing Smart Contract Execution with Determinism and   Compatibility
**Authors**: Wei Zhou, Changzheng Wei, Ying Yan, Wei Tang, Zhihao Chen, Xiong Xu, Xuebing Huang, Wengang Chen, Jie Zhang, Yang Chen, Xiaofu Zheng, Hanghang Wu, Shenglong Chen, Ermei Wang, Xiangfei Chen, Yang Yu, Meng Wu, Tao Zhu, Liwei Yuan, Feng Yu, Alex Zhang, Wei Wang, Ji Luo, Zhengyu He, Wenbiao Zhao

**Updated**: 2025-04-23T09:28:09Z

**Summary**: We introduce the DeTerministic Virtual Machine (DTVM) Stack, a next-generation smart contract execution framework designed to address critical performance, determinism, and ecosystem compatibility challenges in blockchain networks. Building upon WebAssembly (Wasm) while maintaining full Ethereum Virtual Machine (EVM) ABI compatibility, DTVM introduces a Deterministic Middle Intermediate Representation (dMIR) and a hybrid lazy-JIT compilation engine to balance compilation speed and execution efficiency. DTVM further accommodates diverse instruction set architectures (e.g., EVM, RISC-V) through modular adaptation layers. This enables seamless integration with DTVM's hybrid lazy-JIT compilation engine, which dynamically optimizes performance while preserving deterministic execution guarantees across heterogeneous environments. The key contributions including: 1). The framework achieves up to 2$\times$ acceleration over evmone in dominant Ethereum contract (e.g. ERC20/721/1155) execution and reduces fibonacci computation latency by 11.8$\sim$40.5% compared to Wasm based VMs. 2). A novel trampoline hot-switch mechanism enables sub-millisecond (0.95ms) post-deployment invocation times, outperforming up to about 23$\times$ in compilation and invocation efficiency. 3). It supports multi-language development (Solidity, C++, Rust, Java, Go, and AssemblyScript) through unified bytecode conversion while maintaining EVM ABI compatibility for seamless invocation. It reduces machine code object sizes by 30.0$\sim$72.6%, coupled with a minimized Trusted Computing Base. 4). It offers SmartCogent, an AI-driven full-stack development experience, leveraging fine-tuned LLMs and retrieval-augmented generation to automate tasks across the smart contract lifecycle: development, debugging, security auditing, and deployment. DTVM Stack has been open-sourced (https://github.com/DTVMStack).

**Link**: [arxiv](http://arxiv.org/abs/2504.16552v1),  [pdf](http://arxiv.org/pdf/2504.16552v1)

**Tags**: cs.DC 



### A Collaborative Intrusion Detection System Using Snort IDS Nodes
**Authors**: Tom Davies, Max Hashem Eiza, Nathan Shone, Rob Lyon

**Updated**: 2025-04-23T09:25:52Z

**Summary**: Intrusion Detection Systems (IDSs) are integral to safeguarding networks by detecting and responding to threats from malicious traffic or compromised devices. However, standalone IDS deployments often fall short when addressing the increasing complexity and scale of modern cyberattacks. This paper proposes a Collaborative Intrusion Detection System (CIDS) that leverages Snort, an open-source network intrusion detection system, to enhance detection accuracy and reduce false positives. The proposed architecture connects multiple Snort IDS nodes to a centralised node and integrates with a Security Information and Event Management (SIEM) platform to facilitate real-time data sharing, correlation, and analysis. The CIDS design includes a scalable configuration of Snort sensors, a centralised database for log storage, and LogScale SIEM for advanced analytics and visualisation. By aggregating and analysing intrusion data from multiple nodes, the system enables improved detection of distributed and sophisticated attack patterns that standalone IDSs may miss. Performance evaluation against simulated attacks, including Nmap port scans and ICMP flood attacks, demonstrates our CIDS's ability to efficiently process large-scale network traffic, detect threats with higher accuracy, and reduce alert fatigue. This paper highlights the potential of CIDS in modern network environments and explores future enhancements, such as integrating machine learning for advanced threat detection and creating public datasets to support collaborative research. The proposed CIDS framework provides a promising foundation for building more resilient and adaptive network security systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.16550v1),  [pdf](http://arxiv.org/pdf/2504.16550v1)

**Tags**: cs.CR 



### Exploring human-SAV interaction using large language models: The impact   of psychological ownership and anthropomorphism on user experience
**Authors**: Lirui Guo, Michael G. Burke, Wynita M. Griggs

**Updated**: 2025-04-23T09:25:22Z

**Summary**: There has been extensive prior work exploring how psychological factors such as anthropomorphism affect the adoption of shared autonomous vehicles (SAVs). However, limited research has been conducted on how prompt strategies in large language model (LLM)-powered SAV User Interfaces (UIs) affect users' perceptions, experiences, and intentions to adopt such technology. In this work, we investigate how conversational UIs powered by LLMs drive these psychological factors and psychological ownership, the sense of possession a user may come to feel towards an entity or object they may not legally own. We designed four SAV UIs with varying levels of anthropomorphic characteristics and psychological ownership triggers. Quantitative measures of psychological ownership, anthropomorphism, quality of service, disclosure tendency, sentiment of SAV responses, and overall acceptance were collected after participants interacted with each SAV. Qualitative feedback was also gathered regarding the experience of psychological ownership during the interactions. The results indicate that an SAV conversational UI designed to be more anthropomorphic and to induce psychological ownership improved users' perceptions of the SAV's human-like qualities and improved the sentiment of responses compared to a control condition. These findings provide practical guidance for designing LLM-based conversational UIs that enhance user experience and adoption of SAVs.

**Link**: [arxiv](http://arxiv.org/abs/2504.16548v1),  [pdf](http://arxiv.org/pdf/2504.16548v1)

**Tags**: cs.HC cs.AI cs.ET 



### Better Together? The Role of Explanations in Supporting Novices in   Individual and Collective Deliberations about AI
**Authors**: Timothée Schmude, Laura Koesten, Torsten Möller, Sebastian Tschiatschek

**Updated**: 2025-04-23T09:25:09Z

**Summary**: Deploying AI systems in public institutions can have far-reaching consequences for many people, making it a matter of public interest. Providing opportunities for stakeholders to come together, understand these systems, and debate their merits and harms is thus essential. Explainable AI often focuses on individuals, but deliberation benefits from group settings, which are underexplored. To address this gap, we present findings from an interview study with 8 focus groups and 12 individuals. Our findings provide insight into how explanations support AI novices in deliberating alone and in groups. Participants used modular explanations with four information categories to solve tasks and decide about an AI system's deployment. We found that the explanations supported groups in creating shared understanding and in finding arguments for and against the system's deployment. In comparison, individual participants engaged with explanations in more depth and performed better in the study tasks, but missed an exchange with others. Based on our findings, we provide suggestions on how explanations should be designed to work in group settings and describe their potential use in real-world contexts. With this, our contributions inform XAI research that aims to enable AI novices to understand and deliberate AI systems in the public sector.

**Link**: [arxiv](http://arxiv.org/abs/2411.11449v2),  [pdf](http://arxiv.org/pdf/2411.11449v2)

**Tags**: cs.HC 



### Streetscape Analysis with Generative AI (SAGAI): Vision-Language   Assessment and Mapping of Urban Scenes
**Authors**: Joan Perez, Giovanni Fusco

**Updated**: 2025-04-23T09:08:06Z

**Summary**: Streetscapes are an essential component of urban space. Their assessment is presently either limited to morphometric properties of their mass skeleton or requires labor-intensive qualitative evaluations of visually perceived qualities. This paper introduces SAGAI: Streetscape Analysis with Generative Artificial Intelligence, a modular workflow for scoring street-level urban scenes using open-access data and vision-language models. SAGAI integrates OpenStreetMap geometries, Google Street View imagery, and a lightweight version of the LLaVA model to generate structured spatial indicators from images via customizable natural language prompts. The pipeline includes an automated mapping module that aggregates visual scores at both the point and street levels, enabling direct cartographic interpretation. It operates without task-specific training or proprietary software dependencies, supporting scalable and interpretable analysis of urban environments. Two exploratory case studies in Nice and Vienna illustrate SAGAI's capacity to produce geospatial outputs from vision-language inference. The initial results show strong performance for binary urban-rural scene classification, moderate precision in commercial feature detection, and lower estimates, but still informative, of sidewalk width. Fully deployable by any user, SAGAI can be easily adapted to a wide range of urban research themes, such as walkability, safety, or urban design, through prompt modification alone.

**Link**: [arxiv](http://arxiv.org/abs/2504.16538v1),  [pdf](http://arxiv.org/pdf/2504.16538v1)

**Tags**: cs.CV cs.LG I.2; I.4; J.4 



### A Deep Learning System for Rapid and Accurate Warning of Acute Aortic   Syndrome on Non-contrast CT in China
**Authors**: Yujian Hu, Yilang Xiang, Yan-Jie Zhou, Yangyan He, Dehai Lang, Shifeng Yang, Xiaolong Du, Chunlan Den, Youyao Xu, Gaofeng Wang, Zhengyao Ding, Jingyong Huang, Wenjun Zhao, Xuejun Wu, Donglin Li, Qianqian Zhu, Zhenjiang Li, Chenyang Qiu, Ziheng Wu, Yunjun He, Chen Tian, Yihui Qiu, Zuodong Lin, Xiaolong Zhang, Yuan He, Zhenpeng Yuan, Xiaoxiang Zhou, Rong Fan, Ruihan Chen, Wenchao Guo, Jianpeng Zhang, Tony C. W. Mok, Zi Li, Mannudeep K. Kalra, Le Lu, Wenbo Xiao, Xiaoqiang Li, Yun Bian, Chengwei Shao, Guofu Wang, Wei Lu, Zhengxing Huang, Minfeng Xu, Hongkun Zhang

**Updated**: 2025-04-23T09:05:22Z

**Summary**: The accurate and timely diagnosis of acute aortic syndromes (AAS) in patients presenting with acute chest pain remains a clinical challenge. Aortic CT angiography (CTA) is the imaging protocol of choice in patients with suspected AAS. However, due to economic and workflow constraints in China, the majority of suspected patients initially undergo non-contrast CT as the initial imaging testing, and CTA is reserved for those at higher risk. In this work, we present an artificial intelligence-based warning system, iAorta, using non-contrast CT for AAS identification in China, which demonstrates remarkably high accuracy and provides clinicians with interpretable warnings. iAorta was evaluated through a comprehensive step-wise study. In the multi-center retrospective study (n = 20,750), iAorta achieved a mean area under the receiver operating curve (AUC) of 0.958 (95% CI 0.950-0.967). In the large-scale real-world study (n = 137,525), iAorta demonstrated consistently high performance across various non-contrast CT protocols, achieving a sensitivity of 0.913-0.942 and a specificity of 0.991-0.993. In the prospective comparative study (n = 13,846), iAorta demonstrated the capability to significantly shorten the time to correct diagnostic pathway. For the prospective pilot deployment that we conducted, iAorta correctly identified 21 out of 22 patients with AAS among 15,584 consecutive patients presenting with acute chest pain and under non-contrast CT protocol in the emergency department (ED) and enabled the average diagnostic time of these 21 AAS positive patients to be 102.1 (75-133) mins. Last, the iAorta can help avoid delayed or missed diagnosis of AAS in settings where non-contrast CT remains the unavoidable the initial or only imaging test in resource-constrained regions and in patients who cannot or did not receive intravenous contrast.

**Link**: [arxiv](http://arxiv.org/abs/2406.15222v4),  [pdf](http://arxiv.org/pdf/2406.15222v4)

**Tags**: eess.IV cs.AI cs.CV 



### D-LoRa: a Distributed Parameter Adaptation Scheme for LoRa Network
**Authors**: Ruiqi Wang, Tongyu Song, Jing Ren, Xiong Wang, Shizhong Xu, Sheng Wang

**Updated**: 2025-04-23T08:57:56Z

**Summary**: The deployment of LoRa networks necessitates joint performance optimization, including packet delivery rate, energy efficiency, and throughput. Additionally, multiple LoRa parameters for packet transmission must be dynamically configured to tailor the performance metrics prioritization across varying channel environments. Because of the coupling relationship between LoRa parameters and metrics, existing works have opted to focus on certain parameters or specific metrics to circumvent the intricate coupling relationship, leading to limited adaptability. Therefore, we propose D-LoRa, a distributed parameter adaptation scheme, based on reinforcement learning towards network performance. We decompose the joint performance optimization problem into multiple independent Multi-Armed Bandit (MAB) problems with different reward functions. We have also built a comprehensive analytical model for the LoRa network that considers path loss, quasi-orthogonality of spreading factor, and packet collision. Experimental results show that our scheme can increase packet delivery rate by up to 28.8% and demonstrates superior adaptability across different performance metrics.

**Link**: [arxiv](http://arxiv.org/abs/2501.12589v2),  [pdf](http://arxiv.org/pdf/2501.12589v2)

**Tags**: cs.NI cs.DC 



### 6G EdgeAI: Performance Evaluation and Analysis
**Authors**: Chien-Sheng Yang, Yu-Jen Ku, Yuan-Yao Lou, Nathan Tenny, Alex C. -C. Hsu

**Updated**: 2025-04-23T08:54:10Z

**Summary**: Generative AI (GenAI) services powered by large language models (LLMs) increasingly deliver real-time interactions, yet existing 5G multi-access edge computing (MEC) architectures often treat communication and computing as separate domains, limiting their ability to meet stringent latency requirements. To address this challenge, we introduce an Integrated Communication and Computing (ICC) framework where computing capabilities are enabled to reside directly in radio access network (RAN) nodes and jointly manage bandwidth and computing resources. Our queueing-theoretic analysis shows that ICC outperforms 5G MEC, achieving higher service capacity (defined as the maximum arrival rate that maintains a specified fraction of jobs completed within a given delay budget) by 98%. We corroborate these gains through system-level simulations that account for transformer-based LLM workloads, realistic GPU specifications, and a priority-based scheduling scheme. The simulations show that ICC improves service capacity by 60%, demonstrating its potential to enable efficient, cost-effective real-time GenAI services in 6G.

**Link**: [arxiv](http://arxiv.org/abs/2504.16529v1),  [pdf](http://arxiv.org/pdf/2504.16529v1)

**Tags**: cs.DC 



### Federated Learning of Low-Rank One-Shot Image Detection Models in Edge   Devices with Scalable Accuracy and Compute Complexity
**Authors**: Abdul Hannaan, Zubair Shah, Aiman Erbad, Amr Mohamed, Ali Safa

**Updated**: 2025-04-23T08:40:44Z

**Summary**: This paper introduces a novel federated learning framework termed LoRa-FL designed for training low-rank one-shot image detection models deployed on edge devices. By incorporating low-rank adaptation techniques into one-shot detection architectures, our method significantly reduces both computational and communication overhead while maintaining scalable accuracy. The proposed framework leverages federated learning to collaboratively train lightweight image recognition models, enabling rapid adaptation and efficient deployment across heterogeneous, resource-constrained devices. Experimental evaluations on the MNIST and CIFAR10 benchmark datasets, both in an independent-and-identically-distributed (IID) and non-IID setting, demonstrate that our approach achieves competitive detection performance while significantly reducing communication bandwidth and compute complexity. This makes it a promising solution for adaptively reducing the communication and compute power overheads, while not sacrificing model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.16515v1),  [pdf](http://arxiv.org/pdf/2504.16515v1)

**Tags**: cs.CV cs.AI 



### QuaDMix: Quality-Diversity Balanced Data Selection for Efficient LLM   Pretraining
**Authors**: Fengze Liu, Weidong Zhou, Binbin Liu, Zhimiao Yu, Yifan Zhang, Haobin Lin, Yifeng Yu, Xiaohuan Zhou, Taifeng Wang, Yong Cao

**Updated**: 2025-04-23T08:36:50Z

**Summary**: Quality and diversity are two critical metrics for the training data of large language models (LLMs), positively impacting performance. Existing studies often optimize these metrics separately, typically by first applying quality filtering and then adjusting data proportions. However, these approaches overlook the inherent trade-off between quality and diversity, necessitating their joint consideration. Given a fixed training quota, it is essential to evaluate both the quality of each data point and its complementary effect on the overall dataset. In this paper, we introduce a unified data selection framework called QuaDMix, which automatically optimizes the data distribution for LLM pretraining while balancing both quality and diversity. Specifically, we first propose multiple criteria to measure data quality and employ domain classification to distinguish data points, thereby measuring overall diversity. QuaDMix then employs a unified parameterized data sampling function that determines the sampling probability of each data point based on these quality and diversity related labels. To accelerate the search for the optimal parameters involved in the QuaDMix framework, we conduct simulated experiments on smaller models and use LightGBM for parameters searching, inspired by the RegMix method. Our experiments across diverse models and datasets demonstrate that QuaDMix achieves an average performance improvement of 7.2% across multiple benchmarks. These results outperform the independent strategies for quality and diversity, highlighting the necessity and ability to balance data quality and diversity.

**Link**: [arxiv](http://arxiv.org/abs/2504.16511v1),  [pdf](http://arxiv.org/pdf/2504.16511v1)

**Tags**: cs.CL 



### A Comprehensive Survey of Synthetic Tabular Data Generation
**Authors**: Ruxue Shi, Yili Wang, Mengnan Du, Xu Shen, Xin Wang

**Updated**: 2025-04-23T08:33:34Z

**Summary**: Tabular data remains one of the most prevalent and critical data formats across diverse real-world applications. However, its effective use in machine learning (ML) is often constrained by challenges such as data scarcity, privacy concerns, and class imbalance. Synthetic data generation has emerged as a promising solution, leveraging generative models to learn the distribution of real datasets and produce high-fidelity, privacy-preserving samples. Various generative paradigms have been explored, including energy-based models (EBMs), variational autoencoders (VAEs), generative adversarial networks (GANs), large language models (LLMs), and diffusion models. While several surveys have investigated synthetic tabular data generation, most focus on narrow subdomains or specific generative methods, such as GANs, diffusion models, or privacy-preserving techniques. This limited scope often results in fragmented insights, lacking a comprehensive synthesis that bridges diverse approaches. In particular, recent advances driven by LLMs and diffusion-based models remain underexplored. This gap hinders a holistic understanding of the field`s evolution, methodological interplay, and open challenges. To address this, our survey provides a unified and systematic review of synthetic tabular data generation. Our contributions are threefold: (1) we propose a comprehensive taxonomy that organizes existing methods into traditional approaches, diffusion-based methods, and LLM-based models, and provide an in-depth comparative analysis; (2) we detail the complete pipeline for synthetic tabular data generation, including data synthesis, post-processing, and evaluation; (3) we identify major challenges, explore real-world applications, and outline open research questions and future directions to guide future work in this rapidly evolving area.

**Link**: [arxiv](http://arxiv.org/abs/2504.16506v1),  [pdf](http://arxiv.org/pdf/2504.16506v1)

**Tags**: cs.LG 



### Intelligent Depression Prevention via LLM-Based Dialogue Analysis:   Overcoming the Limitations of Scale-Dependent Diagnosis through Precise   Emotional Pattern Recognition
**Authors**: Zhenguang Zhong, Zhixuan Wang

**Updated**: 2025-04-23T08:31:51Z

**Summary**: Existing depression screening predominantly relies on standardized questionnaires (e.g., PHQ-9, BDI), which suffer from high misdiagnosis rates (18-34% in clinical studies) due to their static, symptom-counting nature and susceptibility to patient recall bias. This paper presents an AI-powered depression prevention system that leverages large language models (LLMs) to analyze real-time conversational cues--including subtle emotional expressions (e.g., micro-sentiment shifts, self-referential language patterns)--for more accurate and dynamic mental state assessment. Our system achieves three key innovations: (1) Continuous monitoring through natural dialogue, detecting depression-indicative linguistic features (anhedonia markers, hopelessness semantics) with 89% precision (vs. 72% for PHQ-9); (2) Adaptive risk stratification that updates severity levels based on conversational context, reducing false positives by 41% compared to scale-based thresholds; and (3) Personalized intervention strategies tailored to users' emotional granularity, demonstrating 2.3x higher adherence rates than generic advice. Clinical validation with 450 participants shows the system identifies 92% of at-risk cases missed by traditional scales, while its explainable AI interface bridges the gap between automated analysis and clinician judgment. This work establishes conversational AI as a paradigm shift from episodic scale-dependent diagnosis to continuous, emotionally intelligent mental health monitoring.

**Link**: [arxiv](http://arxiv.org/abs/2504.16504v1),  [pdf](http://arxiv.org/pdf/2504.16504v1)

**Tags**: q-bio.NC cs.HC 



### Amplified Vulnerabilities: Structured Jailbreak Attacks on LLM-based   Multi-Agent Debate
**Authors**: Senmao Qi, Yifei Zou, Peng Li, Ziyi Lin, Xiuzhen Cheng, Dongxiao Yu

**Updated**: 2025-04-23T08:01:50Z

**Summary**: Multi-Agent Debate (MAD), leveraging collaborative interactions among Large Language Models (LLMs), aim to enhance reasoning capabilities in complex tasks. However, the security implications of their iterative dialogues and role-playing characteristics, particularly susceptibility to jailbreak attacks eliciting harmful content, remain critically underexplored. This paper systematically investigates the jailbreak vulnerabilities of four prominent MAD frameworks built upon leading commercial LLMs (GPT-4o, GPT-4, GPT-3.5-turbo, and DeepSeek) without compromising internal agents. We introduce a novel structured prompt-rewriting framework specifically designed to exploit MAD dynamics via narrative encapsulation, role-driven escalation, iterative refinement, and rhetorical obfuscation. Our extensive experiments demonstrate that MAD systems are inherently more vulnerable than single-agent setups. Crucially, our proposed attack methodology significantly amplifies this fragility, increasing average harmfulness from 28.14% to 80.34% and achieving attack success rates as high as 80% in certain scenarios. These findings reveal intrinsic vulnerabilities in MAD architectures and underscore the urgent need for robust, specialized defenses prior to real-world deployment.

**Link**: [arxiv](http://arxiv.org/abs/2504.16489v1),  [pdf](http://arxiv.org/pdf/2504.16489v1)

**Tags**: cs.CR cs.AI 



### Needle Threading: Can LLMs Follow Threads through Near-Million-Scale   Haystacks?
**Authors**: Jonathan Roberts, Kai Han, Samuel Albanie

**Updated**: 2025-04-23T07:50:49Z

**Summary**: As the context limits of Large Language Models (LLMs) increase, the range of possible applications and downstream functions broadens. In many real-world tasks, decisions depend on details scattered across collections of often disparate documents containing mostly irrelevant information. Long-context LLMs appear well-suited to this form of complex information retrieval and reasoning, which has traditionally proven costly and time-consuming. However, although the development of longer context models has seen rapid gains in recent years, our understanding of how effectively LLMs use their context has not kept pace. To address this, we conduct a set of retrieval experiments designed to evaluate the capabilities of 17 leading LLMs, such as their ability to follow threads of information through the context window. Strikingly, we find that many models are remarkably threadsafe: capable of simultaneously following multiple threads without significant loss in performance. Still, for many models, we find the effective context limit is significantly shorter than the supported context length, with accuracy decreasing as the context window grows. Our study also highlights the important point that token counts from different tokenizers should not be directly compared -- they often correspond to substantially different numbers of written characters. We release our code and long-context experimental data.

**Link**: [arxiv](http://arxiv.org/abs/2411.05000v2),  [pdf](http://arxiv.org/pdf/2411.05000v2)

**Tags**: cs.CL 



### Harden and Catch for Just-in-Time Assured LLM-Based Software Testing:   Open Research Challenges
**Authors**: Mark Harman, Peter O'Hearn, Shubho Sengupta

**Updated**: 2025-04-23T07:32:43Z

**Summary**: Despite decades of research and practice in automated software testing, several fundamental concepts remain ill-defined and under-explored, yet offer enormous potential real-world impact. We show that these concepts raise exciting new challenges in the context of Large Language Models for software test generation. More specifically, we formally define and investigate the properties of hardening and catching tests. A hardening test is one that seeks to protect against future regressions, while a catching test is one that catches such a regression or a fault in new functionality introduced by a code change. Hardening tests can be generated at any time and may become catching tests when a future regression is caught. We also define and motivate the Catching `Just-in-Time' (JiTTest) Challenge, in which tests are generated `just-in-time' to catch new faults before they land into production. We show that any solution to Catching JiTTest generation can also be repurposed to catch latent faults in legacy code. We enumerate possible outcomes for hardening and catching tests and JiTTests, and discuss open research problems, deployment options, and initial results from our work on automated LLM-based hardening at Meta. This paper\footnote{Author order is alphabetical. The corresponding author is Mark Harman.} was written to accompany the keynote by the authors at the ACM International Conference on the Foundations of Software Engineering (FSE) 2025.

**Link**: [arxiv](http://arxiv.org/abs/2504.16472v1),  [pdf](http://arxiv.org/pdf/2504.16472v1)

**Tags**: cs.SE cs.AI 



### Closed-form analysis of Multi-RIS Reflected Signals in RIS-Aided   Networks Using Stochastic Geometry
**Authors**: Guodong Sun, Francois Baccelli

**Updated**: 2025-04-23T07:29:32Z

**Summary**: Reconfigurable intelligent surfaces (RISs) enhance wireless communication by creating engineered signal reflection paths in addition to direct links. This work presents a stochastic geometry framework using point processes (PPs) to model multiple randomly deployed RISs conditioned on their associated base station (BS) locations. By characterizing aggregated reflections from multiple RISs using the Laplace transform, we analytically assess the performance impact of RIS-reflected signals by integrating this characterization into well-established stochastic geometry frameworks. Specifically, we derive closed-form expressions for the Laplace transform of the reflected signal power in several deployment scenarios. These analytical results facilitate performance evaluation of RIS-enabled enhancements. Numerical simulations validate that optimal RIS placement favors proximity to BSs or user equipment (UEs), and further quantify the impact of reflected interference, various fading assumptions, and diverse spatial deployment strategies. Importantly, our analytical approach shows superior computational efficiency compared to Monte Carlo simulations.

**Link**: [arxiv](http://arxiv.org/abs/2504.16469v1),  [pdf](http://arxiv.org/pdf/2504.16469v1)

**Tags**: cs.PF cs.IT math.IT 



### A Highly Scalable LLM Clusters with Optical Interconnect
**Authors**: Xinchi Han, Yongxi Lv, Shizhen Zhao, Zhuotao Liu, Ximeng Liu, Xinbing Wang

**Updated**: 2025-04-23T07:27:35Z

**Summary**: We propose \emph{LumosCore} to build high-bandwidth and large-scale data center networks for LLM jobs. By replacing the core-layer electrical packet switches by optical circuit switches, \emph{LumosCore} could achieves $2\times$ increase in bandwidth or $8\times$ increase in network size. We offer the detailed design of \emph{LumosCore} at both deployment stage and running stage. At deployment stage, we propose Interleaved Wiring, which is compatible with all possible logical topologies. At running stage, we design polynomial-time algorithms for GPU placement, logical topology generating and OCS reconfiguration to minimize network contention and reduce impact to scheduled jobs. We evaluate \emph{LumosCore} using both testbed experiments and large-scale simulation. Compared to traditional hybrid optical/electrical architectures, \emph{LumosCore} increases the end-to-end training throughput by up to 39.5\% on a 128-node testbed. Compared to the state-of-art Clos architectures, \emph{LumosCore} reduces the average job completion time by up to 34.1\% in a 16k simulation platform.

**Link**: [arxiv](http://arxiv.org/abs/2411.01503v4),  [pdf](http://arxiv.org/pdf/2411.01503v4)

**Tags**: cs.NI 



### Language Models to Support Multi-Label Classification of Industrial Data
**Authors**: Waleed Abdeen, Michael Unterkalmsteiner, Krzysztof Wnuk, Alessio Ferrari, Panagiota Chatzipetrou

**Updated**: 2025-04-23T07:24:40Z

**Summary**: Multi-label requirements classification is a challenging task, especially when dealing with numerous classes at varying levels of abstraction. The difficulties increases when a limited number of requirements is available to train a supervised classifier. Zero-shot learning (ZSL) does not require training data and can potentially address this problem. This paper investigates the performance of zero-shot classifiers (ZSCs) on a multi-label industrial dataset. We focuse on classifying requirements according to a taxonomy designed to support requirements tracing. We compare multiple variants of ZSCs using different embeddings, including 9 language models (LMs) with a reduced number of parameters (up to 3B), e.g., BERT, and 5 large LMs (LLMs) with a large number of parameters (up to 70B), e.g., Llama. Our ground truth includes 377 requirements and 1968 labels from 6 output spaces. For the evaluation, we adopt traditional metrics, i.e., precision, recall, F1, and $F_\beta$, as well as a novel label distance metric Dn. This aims to better capture the classification's hierarchical nature and provides a more nuanced evaluation of how far the results are from the ground truth. 1) The top-performing model on 5 out of 6 output spaces is T5-xl, with maximum $F_\beta$ = 0.78 and Dn = 0.04, while BERT base outperformed the other models in one case, with maximum $F_\beta$ = 0.83 and Dn = 0.04. 2) LMs with smaller parameter size produce the best classification results compared to LLMs. Thus, addressing the problem in practice is feasible as limited computing power is needed. 3) The model architecture (autoencoding, autoregression, and sentence-to-sentence) significantly affects the classifier's performance. We conclude that using ZSL for multi-label requirements classification offers promising results. We also present a novel metric that can be used to select the top-performing model for this problem

**Link**: [arxiv](http://arxiv.org/abs/2504.15922v2),  [pdf](http://arxiv.org/pdf/2504.15922v2)

**Tags**: cs.SE 



### Large Language Models for Automated Literature Review: An Evaluation of   Reference Generation, Abstract Writing, and Review Composition
**Authors**: Xuemei Tang, Xufeng Duan, Zhenguang G. Cai

**Updated**: 2025-04-23T07:09:06Z

**Summary**: Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.

**Link**: [arxiv](http://arxiv.org/abs/2412.13612v3),  [pdf](http://arxiv.org/pdf/2412.13612v3)

**Tags**: cs.CL cs.AI 



### EAGLE-3: Scaling up Inference Acceleration of Large Language Models via   Training-Time Test
**Authors**: Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang

**Updated**: 2025-04-23T07:08:17Z

**Summary**: The sequential nature of modern LLMs makes them expensive and slow, and speculative sampling has proven to be an effective solution to this problem. Methods like EAGLE perform autoregression at the feature level, reusing top-layer features from the target model to achieve better results than vanilla speculative sampling. A growing trend in the LLM community is scaling up training data to improve model intelligence without increasing inference costs. However, we observe that scaling up data provides limited improvements for EAGLE. We identify that this limitation arises from EAGLE's feature prediction constraints. In this paper, we introduce EAGLE-3, which abandons feature prediction in favor of direct token prediction and replaces reliance on top-layer features with multi-layer feature fusion via a technique named training-time test. These improvements significantly enhance performance and enable the draft model to fully benefit from scaling up training data. Our experiments include both chat models and reasoning models, evaluated on five tasks. The results show that EAGLE-3 achieves a speedup ratio up to 6.5x, with about 1.4x improvement over EAGLE-2. In the SGLang framework, EAGLE-3 achieves a 1.38x throughput improvement at a batch size of 64. The code is available at https://github.com/SafeAILab/EAGLE.

**Link**: [arxiv](http://arxiv.org/abs/2503.01840v3),  [pdf](http://arxiv.org/pdf/2503.01840v3)

**Tags**: cs.CL 



### ITERTL: An Iterative Framework for Fine-tuning LLMs for RTL Code   Generation
**Authors**: Peiyang Wu, Nan Guo, Xiao Xiao, Wenming Li, Xiaochun Ye, Dongrui Fan

**Updated**: 2025-04-23T06:56:00Z

**Summary**: Recently, large language models (LLMs) have demonstrated excellent performance, inspiring researchers to explore their use in automating register transfer level (RTL) code generation and improving hardware design efficiency. However, the existing approaches to fine-tune LLMs for RTL generation typically are conducted on fixed datasets, which do not fully stimulate the capability of LLMs and require large amounts of reference data, which are costly to acquire. To mitigate these issues, we innovatively introduce an iterative training paradigm named ITERTL. During each iteration, samples are drawn from the model trained in the previous cycle. Then these new samples are employed for training in current loop. Furthermore, we introduce a plug-and-play data filtering strategy, thereby encouraging the model to generate high-quality, self-contained code. Our model outperforms GPT4 and state-of-the-art (SOTA) open-source models, achieving remarkable 53.8% pass@1 rate on VerilogEval-human benchmark. Under similar conditions of data quantity and quality, our approach significantly outperforms the baseline. Extensive experiments validate the effectiveness of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2407.12022v3),  [pdf](http://arxiv.org/pdf/2407.12022v3)

**Tags**: cs.CL cs.AI 



### ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems
**Authors**: Ishneet Sukhvinder Singh, Ritvik Aggarwal, Ibrahim Allahverdiyev, Muhammad Taha, Aslihan Akalin, Kevin Zhu, Sean O'Brien

**Updated**: 2025-04-23T06:29:39Z

**Summary**: Retrieval-Augmented Generation (RAG) systems using large language models (LLMs) often generate inaccurate responses due to the retrieval of irrelevant or loosely related information. Existing methods, which operate at the document level, fail to effectively filter out such content. We propose LLM-driven chunk filtering, ChunkRAG, a framework that enhances RAG systems by evaluating and filtering retrieved information at the chunk level. Our approach employs semantic chunking to divide documents into coherent sections and utilizes LLM-based relevance scoring to assess each chunk's alignment with the user's query. By filtering out less pertinent chunks before the generation phase, we significantly reduce hallucinations and improve factual accuracy. Experiments show that our method outperforms existing RAG models, achieving higher accuracy on tasks requiring precise information retrieval. This advancement enhances the reliability of RAG systems, making them particularly beneficial for applications like fact-checking and multi-hop reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2410.19572v5),  [pdf](http://arxiv.org/pdf/2410.19572v5)

**Tags**: cs.CL 



### From Past to Present: A Survey of Malicious URL Detection Techniques,   Datasets and Code Repositories
**Authors**: Ye Tian, Yanqiu Yu, Jianguo Sun, Yanbin Wang

**Updated**: 2025-04-23T06:23:18Z

**Summary**: Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into divulging private data or distributing harmful payloads to infiltrate host systems. Gaining timely insights into the current state of this ongoing battle holds significant importance. However, existing reviews exhibit 4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures understanding of how detection approaches exploit specific modal information channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate benchmarking; 4) Insufficient dataset coverage.This paper presents a comprehensive review of malicious URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced deep learning approaches (e.g. Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel modality-based taxonomy that categorizes existing works according to their primary data modalities (URL, HTML, Visual, etc.). This hierarchical classification enables both rigorous technical analysis and clear understanding of multimodal information utilization. Furthermore, to establish a profile of accessible datasets and address the lack of standardized benchmarking (where current studies often lack proper baseline comparisons), we curate and analyze: 1) publicly available datasets (2016-2024), and 2) open-source implementations from published works(2013-2025). Then, we outline essential design principles and architectural frameworks for product-level implementations. The review concludes by examining emerging challenges and proposing actionable directions for future research. We maintain a GitHub repository for ongoing curating datasets and open-source implementations: https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.

**Link**: [arxiv](http://arxiv.org/abs/2504.16449v1),  [pdf](http://arxiv.org/pdf/2504.16449v1)

**Tags**: cs.CR cs.LG 



### SocioVerse: A World Model for Social Simulation Powered by LLM Agents   and A Pool of 10 Million Real-World Users
**Authors**: Xinnong Zhang, Jiayu Lin, Xinyi Mou, Shiyue Yang, Xiawei Liu, Libo Sun, Hanjia Lyu, Yihang Yang, Weihong Qi, Yue Chen, Guanying Li, Ling Yan, Yao Hu, Siming Chen, Yu Wang, Xuanjing Huang, Jiebo Luo, Shiping Tang, Libo Wu, Baohua Zhou, Zhongyu Wei

**Updated**: 2025-04-23T06:08:32Z

**Summary**: Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.

**Link**: [arxiv](http://arxiv.org/abs/2504.10157v2),  [pdf](http://arxiv.org/pdf/2504.10157v2)

**Tags**: cs.CL cs.CY 



### Private Federated Learning using Preference-Optimized Synthetic Data
**Authors**: Charlie Hou, Mei-Yu Wang, Yige Zhu, Daniel Lazar, Giulia Fanti

**Updated**: 2025-04-23T05:57:20Z

**Summary**: In practical settings, differentially private Federated learning (DP-FL) is the dominant method for training models from private, on-device client data. Recent work has suggested that DP-FL may be enhanced or outperformed by methods that use DP synthetic data (Wu et al., 2024; Hou et al., 2024). The primary algorithms for generating DP synthetic data for FL applications require careful prompt engineering based on public information and/or iterative private client feedback. Our key insight is that the private client feedback collected by prior DP synthetic data methods (Hou et al., 2024; Xie et al., 2024) can be viewed as a preference ranking. Our algorithm, Preference Optimization for Private Client Data (POPri) harnesses client feedback using preference optimization algorithms such as Direct Preference Optimization (DPO) to fine-tune LLMs to generate high-quality DP synthetic data. To evaluate POPri, we release LargeFedBench, a new federated text benchmark for uncontaminated LLM evaluations on federated client data. POPri substantially improves the utility of DP synthetic data relative to prior work on LargeFedBench datasets and an existing benchmark from Xie et al. (2024). POPri closes the gap between next-token prediction accuracy in the fully-private and non-private settings by up to 68%, compared to 52% for prior synthetic data methods, and 10% for state-of-the-art DP federated learning methods. The code and data are available at https://github.com/meiyuw/POPri.

**Link**: [arxiv](http://arxiv.org/abs/2504.16438v1),  [pdf](http://arxiv.org/pdf/2504.16438v1)

**Tags**: cs.LG cs.AI cs.CR cs.DC 



### MegaScale-Infer: Serving Mixture-of-Experts at Scale with Disaggregated   Expert Parallelism
**Authors**: Ruidong Zhu, Ziheng Jiang, Chao Jin, Peng Wu, Cesar A. Stuardo, Dongyang Wang, Xinlei Zhang, Huaping Zhou, Haoran Wei, Yang Cheng, Jianzhe Xiao, Xinyi Zhang, Lingjun Liu, Haibin Lin, Li-Wen Chang, Jianxi Ye, Xiao Yu, Xuanzhe Liu, Xin Jin, Xin Liu

**Updated**: 2025-04-23T05:47:32Z

**Summary**: Mixture-of-Experts (MoE) showcases tremendous potential to scale large language models (LLMs) with enhanced performance and reduced computational complexity. However, its sparsely activated architecture shifts feed-forward networks (FFNs) from being compute-intensive to memory-intensive during inference, leading to substantially lower GPU utilization and increased operational costs. We present MegaScale-Infer, an efficient and cost-effective system for serving large-scale MoE models. MegaScale-Infer disaggregates attention and FFN modules within each model layer, enabling independent scaling, tailored parallelism strategies, and heterogeneous deployment for both modules. To fully exploit disaggregation in the presence of MoE's sparsity, MegaScale-Infer introduces ping-pong pipeline parallelism, which partitions a request batch into micro-batches and shuttles them between attention and FFNs for inference. Combined with distinct model parallelism for each module, MegaScale-Infer effectively hides communication overhead and maximizes GPU utilization. To adapt to disaggregated attention and FFN modules and minimize data transmission overhead (e.g., token dispatch), MegaScale-Infer provides a high-performance M2N communication library that eliminates unnecessary GPU-to-CPU data copies, group initialization overhead, and GPU synchronization. Experimental results indicate that MegaScale-Infer achieves up to 1.90x higher per-GPU throughput than state-of-the-art solutions.

**Link**: [arxiv](http://arxiv.org/abs/2504.02263v3),  [pdf](http://arxiv.org/pdf/2504.02263v3)

**Tags**: cs.DC cs.LG 



### iTFKAN: Interpretable Time Series Forecasting with Kolmogorov-Arnold   Network
**Authors**: Ziran Liang, Rui An, Wenqi Fan, Yanghui Rao, Yuxuan Liang

**Updated**: 2025-04-23T05:34:49Z

**Summary**: As time evolves, data within specific domains exhibit predictability that motivates time series forecasting to predict future trends from historical data. However, current deep forecasting methods can achieve promising performance but generally lack interpretability, hindering trustworthiness and practical deployment in safety-critical applications such as auto-driving and healthcare. In this paper, we propose a novel interpretable model, iTFKAN, for credible time series forecasting. iTFKAN enables further exploration of model decision rationales and underlying data patterns due to its interpretability achieved through model symbolization. Besides, iTFKAN develops two strategies, prior knowledge injection, and time-frequency synergy learning, to effectively guide model learning under complex intertwined time series data. Extensive experimental results demonstrated that iTFKAN can achieve promising forecasting performance while simultaneously possessing high interpretive capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2504.16432v1),  [pdf](http://arxiv.org/pdf/2504.16432v1)

**Tags**: cs.LG cs.AI 



### Give LLMs a Security Course: Securing Retrieval-Augmented Code   Generation via Knowledge Injection
**Authors**: Bo Lin, Shangwen Wang, Yihao Qin, Liqian Chen, Xiaoguang Mao

**Updated**: 2025-04-23T05:27:27Z

**Summary**: Retrieval-Augmented Code Generation (RACG) leverages external knowledge to enhance Large Language Models (LLMs) in code synthesis, improving the functional correctness of the generated code. However, existing RACG systems largely overlook security, leading to substantial risks. Especially, the poisoning of malicious code into knowledge bases can mislead LLMs, resulting in the generation of insecure outputs, which poses a critical threat in modern software development. To address this, we propose a security-hardening framework for RACG systems, CodeGuarder, that shifts the paradigm from retrieving only functional code examples to incorporating both functional code and security knowledge. Our framework constructs a security knowledge base from real-world vulnerability databases, including secure code samples and root cause annotations. For each code generation query, a retriever decomposes the query into fine-grained sub-tasks and fetches relevant security knowledge. To prioritize critical security guidance, we introduce a re-ranking and filtering mechanism by leveraging the LLMs' susceptibility to different vulnerability types. This filtered security knowledge is seamlessly integrated into the generation prompt. Our evaluation shows CodeGuarder significantly improves code security rates across various LLMs, achieving average improvements of 20.12\% in standard RACG, and 31.53\% and 21.91\% under two distinct poisoning scenarios without compromising functional correctness. Furthermore, CodeGuarder demonstrates strong generalization, enhancing security even when the targeted language's security knowledge is lacking. This work presents CodeGuarder as a pivotal advancement towards building secure and trustworthy RACG systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.16429v1),  [pdf](http://arxiv.org/pdf/2504.16429v1)

**Tags**: cs.CR cs.SE 



### Can Large Language Models Help Multimodal Language Analysis? MMLA: A   Comprehensive Benchmark
**Authors**: Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Haige Zhu, Jie Zhou, Jinchao Zhang

**Updated**: 2025-04-24T07:35:03Z

**Summary**: Multimodal language analysis is a rapidly evolving field that leverages multiple modalities to enhance the understanding of high-level semantics underlying human conversational utterances. Despite its significance, little research has investigated the capability of multimodal large language models (MLLMs) to comprehend cognitive-level semantics. In this paper, we introduce MMLA, a comprehensive benchmark specifically designed to address this gap. MMLA comprises over 61K multimodal utterances drawn from both staged and real-world scenarios, covering six core dimensions of multimodal semantics: intent, emotion, dialogue act, sentiment, speaking style, and communication behavior. We evaluate eight mainstream branches of LLMs and MLLMs using three methods: zero-shot inference, supervised fine-tuning, and instruction tuning. Extensive experiments reveal that even fine-tuned models achieve only about 60%~70% accuracy, underscoring the limitations of current MLLMs in understanding complex human language. We believe that MMLA will serve as a solid foundation for exploring the potential of large language models in multimodal language analysis and provide valuable resources to advance this field. The datasets and code are open-sourced at https://github.com/thuiar/MMLA.

**Link**: [arxiv](http://arxiv.org/abs/2504.16427v2),  [pdf](http://arxiv.org/pdf/2504.16427v2)

**Tags**: cs.CL cs.AI cs.MM 



### Large Language Model Sentinel: LLM Agent for Adversarial Purification
**Authors**: Guang Lin, Toshihisa Tanaka, Qibin Zhao

**Updated**: 2025-04-23T05:12:45Z

**Summary**: Over the past two years, the use of large language models (LLMs) has advanced rapidly. While these LLMs offer considerable convenience, they also raise security concerns, as LLMs are vulnerable to adversarial attacks by some well-designed textual perturbations. In this paper, we introduce a novel defense technique named Large LAnguage MOdel Sentinel (LLAMOS), which is designed to enhance the adversarial robustness of LLMs by purifying the adversarial textual examples before feeding them into the target LLM. Our method comprises two main components: a) Agent instruction, which can simulate a new agent for adversarial defense, altering minimal characters to maintain the original meaning of the sentence while defending against attacks; b) Defense guidance, which provides strategies for modifying clean or adversarial examples to ensure effective defense and accurate outputs from the target LLMs. Remarkably, the defense agent demonstrates robust defensive capabilities even without learning from adversarial examples. Additionally, we conduct an intriguing adversarial experiment where we develop two agents, one for defense and one for attack, and engage them in mutual confrontation. During the adversarial interactions, neither agent completely beat the other. Extensive experiments on both open-source and closed-source LLMs demonstrate that our method effectively defends against adversarial attacks, thereby enhancing adversarial robustness.

**Link**: [arxiv](http://arxiv.org/abs/2405.20770v4),  [pdf](http://arxiv.org/pdf/2405.20770v4)

**Tags**: cs.CL cs.AI cs.CR 



### A LoRA-Based Approach to Fine-Tuning LLMs for Educational Guidance in   Resource-Constrained Settings
**Authors**: Md Millat Hosen

**Updated**: 2025-04-23T04:59:47Z

**Summary**: The current study describes a cost-effective method for adapting large language models (LLMs) for academic advising with study-abroad contexts in mind and for application in low-resource methods for acculturation. With the Mistral-7B-Instruct model applied with a Low-Rank Adaptation (LoRA) method and a 4-bit quantization method, the model underwent training in two distinct stages related to this study's purpose to enhance domain specificity while maintaining computational efficiency. In Phase 1, the model was conditioned with a synthetic dataset via the Gemini Pro API, and in Phase 2, it was trained with manually curated datasets from the StudyAbroadGPT project to achieve enhanced, contextualized responses. Technical innovations entailed memory-efficient quantization, parameter-efficient adaptation, and continuous training analytics via Weights & Biases. After training, this study demonstrated a reduction in training loss by 52.7%, 92% accuracy in domain-specific recommendations, achieved 95% markdown-based formatting support, and a median run-rate of 100 samples per second on off-the-shelf GPU equipment. These findings support the effective application of instruction-tuned LLMs within educational advisers, especially in low-resource institutional scenarios. Limitations included decreased generalizability and the application of a synthetically generated dataset, but this framework is scalable for adding new multilingual-augmented and real-time academic advising processes. Future directions may include plans for the integration of retrieval-augmented generation, applying dynamic quantization routines, and connecting to real-time academic databases to increase adaptability and accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.15610v2),  [pdf](http://arxiv.org/pdf/2504.15610v2)

**Tags**: cs.AI 68T05 (Learning and adaptive systems), 68T07 (Artificial
  intelligence and education) 



### Evaluating Multi-Hop Reasoning in Large Language Models: A   Chemistry-Centric Case Study
**Authors**: Mohammad Khodadad, Ali Shiraee Kasmaee, Mahdi Astaraki, Nicholas Sherck, Hamidreza Mahyar, Soheila Samiee

**Updated**: 2025-04-23T04:36:19Z

**Summary**: In this study, we introduced a new benchmark consisting of a curated dataset and a defined evaluation process to assess the compositional reasoning capabilities of large language models within the chemistry domain. We designed and validated a fully automated pipeline, verified by subject matter experts, to facilitate this task. Our approach integrates OpenAI reasoning models with named entity recognition (NER) systems to extract chemical entities from recent literature, which are then augmented with external knowledge bases to form a comprehensive knowledge graph. By generating multi-hop questions across these graphs, we assess LLM performance in both context-augmented and non-context augmented settings. Our experiments reveal that even state-of-the-art models face significant challenges in multi-hop compositional reasoning. The results reflect the importance of augmenting LLMs with document retrieval, which can have a substantial impact on improving their performance. However, even perfect retrieval accuracy with full context does not eliminate reasoning errors, underscoring the complexity of compositional reasoning. This work not only benchmarks and highlights the limitations of current LLMs but also presents a novel data generation pipeline capable of producing challenging reasoning datasets across various domains. Overall, this research advances our understanding of reasoning in computational linguistics.

**Link**: [arxiv](http://arxiv.org/abs/2504.16414v1),  [pdf](http://arxiv.org/pdf/2504.16414v1)

**Tags**: cs.CL 



### Less is More: Enhancing Structured Multi-Agent Reasoning via   Quality-Guided Distillation
**Authors**: Jiahao Yuan, Xingzhe Sun, Xing Yu, Jingwen Wang, Dehui Du, Zhiqing Cui, Zixiang Di

**Updated**: 2025-04-23T04:19:52Z

**Summary**: The XLLM@ACL2025 Shared Task-III formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the XLLM@ACL2025 Shared Task-III, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/Jiahao-Yuan/Less-is-More.

**Link**: [arxiv](http://arxiv.org/abs/2504.16408v1),  [pdf](http://arxiv.org/pdf/2504.16408v1)

**Tags**: cs.CL 



### Evaluating Text Style Transfer Evaluation: Are There Any Reliable   Metrics?
**Authors**: Sourabrata Mukherjee, Atul Kr. Ojha, John P. McCrae, Ondrej Dusek

**Updated**: 2025-04-23T04:06:56Z

**Summary**: Text style transfer (TST) is the task of transforming a text to reflect a particular style while preserving its original content. Evaluating TST outputs is a multidimensional challenge, requiring the assessment of style transfer accuracy, content preservation, and naturalness. Using human evaluation is ideal but costly, as is common in other natural language processing (NLP) tasks, however, automatic metrics for TST have not received as much attention as metrics for, e.g., machine translation or summarization. In this paper, we examine both set of existing and novel metrics from broader NLP tasks for TST evaluation, focusing on two popular subtasks, sentiment transfer and detoxification, in a multilingual context comprising English, Hindi, and Bengali. By conducting meta-evaluation through correlation with human judgments, we demonstrate the effectiveness of these metrics when used individually and in ensembles. Additionally, we investigate the potential of large language models (LLMs) as tools for TST evaluation. Our findings highlight newly applied advanced NLP metrics and LLM-based evaluations provide better insights than existing TST metrics. Our oracle ensemble approaches show even more potential.

**Link**: [arxiv](http://arxiv.org/abs/2502.04718v2),  [pdf](http://arxiv.org/pdf/2502.04718v2)

**Tags**: cs.CL 



### Circinus: Efficient Query Planner for Compound ML Serving
**Authors**: Banruo Liu, Wei-Yu Lin, Minghao Fang, Yihan Jiang, Fan Lai

**Updated**: 2025-04-23T03:57:24Z

**Summary**: The rise of compound AI serving -- integrating multiple operators in a pipeline that may span edge and cloud tiers -- enables end-user applications such as autonomous driving, generative AI-powered meeting companions, and immersive gaming. Achieving high service goodput -- i.e., meeting service level objectives (SLOs) for pipeline latency, accuracy, and costs -- requires effective planning of operator placement, configuration, and resource allocation across infrastructure tiers. However, the diverse SLO requirements, varying edge capabilities, and high query volumes create an enormous planning search space, rendering current solutions fundamentally limited for real-time serving and cost-efficient deployments.   This paper presents Circinus, an SLO-aware query planner for large-scale compound AI workloads. Circinus novelly decomposes multi-query planning and multi-dimensional SLO objectives while preserving global decision quality. By exploiting plan similarities within and across queries, it significantly reduces search steps. It further improves per-step efficiency with a precision-aware plan profiler that incrementally profiles and strategically applies early stopping based on imprecise estimates of plan performance. At scale, Circinus selects query-plan combinations to maximize global SLO goodput. Evaluations in real-world settings show that Circinus improves service goodput by 3.2-5.0$\times$, accelerates query planning by 4.2-5.8$\times$, achieving query response in seconds, while reducing deployment costs by 3.2-4.0$\times$ over state of the arts even in their intended single-tier deployments.

**Link**: [arxiv](http://arxiv.org/abs/2504.16397v1),  [pdf](http://arxiv.org/pdf/2504.16397v1)

**Tags**: cs.DB cs.LG 



### Sustainability via LLM Right-sizing
**Authors**: Jennifer Haase, Finn Klessascheck, Jan Mendling, Sebastian Pokutta

**Updated**: 2025-04-23T03:43:30Z

**Summary**: Large language models (LLMs) have become increasingly embedded in organizational workflows. This has raised concerns over their energy consumption, financial costs, and data sovereignty. While performance benchmarks often celebrate cutting-edge models, real-world deployment decisions require a broader perspective: when is a smaller, locally deployable model "good enough"? This study offers an empirical answer by evaluating eleven proprietary and open-weight LLMs across ten everyday occupational tasks, including summarizing texts, generating schedules, and drafting emails and proposals. Using a dual-LLM-based evaluation framework, we automated task execution and standardized evaluation across ten criteria related to output quality, factual accuracy, and ethical responsibility. Results show that GPT-4o delivers consistently superior performance but at a significantly higher cost and environmental footprint. Notably, smaller models like Gemma-3 and Phi-4 achieved strong and reliable results on most tasks, suggesting their viability in contexts requiring cost-efficiency, local deployment, or privacy. A cluster analysis revealed three model groups -- premium all-rounders, competent generalists, and limited but safe performers -- highlighting trade-offs between quality, control, and sustainability. Significantly, task type influenced model effectiveness: conceptual tasks challenged most models, while aggregation and transformation tasks yielded better performances. We argue for a shift from performance-maximizing benchmarks to task- and context-aware sufficiency assessments that better reflect organizational priorities. Our approach contributes a scalable method to evaluate AI models through a sustainability lens and offers actionable guidance for responsible LLM deployment in practice.

**Link**: [arxiv](http://arxiv.org/abs/2504.13217v2),  [pdf](http://arxiv.org/pdf/2504.13217v2)

**Tags**: cs.CL cs.AI 



