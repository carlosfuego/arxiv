# Arxiv Results
## Keyword: kv cache 
 ### Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and   Multiple Level Analysis
**Authors**: Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu

**Updated**: 2025-01-21T12:19:02Z

**Summary**: Modern GPUs, with their specialized hardware like tensor cores, are essential for demanding AI and deep learning applications. This study presents a comprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU architecture, delving into its performance characteristics and novel features. We benchmark Hopper's memory subsystem latency and throughput, comparing its L2 partitioned cache behavior and global memory access patterns against recent GPU generations, Ampere and Ada Lovelace. Our analysis reveals significant performance differences and architectural improvements in Hopper. A core contribution of this work is a detailed evaluation of Hopper's fourth-generation tensor cores, including their FP8 precision support and the novel asynchronous wgmma instructions, assessing their impact on matrix multiply-accumulate operations. We further investigate the performance implications of other key Hopper innovations: DPX instructions for accelerating dynamic programming algorithms, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. This multi-level approach encompasses instruction-level microbenchmarks, library-level analysis of the Transformer Engine, and application-level benchmarks of tensor core performance within large language models. Our findings provide valuable, in-depth insights for software developers seeking to optimize performance and develop accurate performance models for the Hopper architecture, ultimately contributing to a deeper understanding of its potential for accelerating AI and other computationally intensive workloads.

**Link**: [arxiv](http://arxiv.org/abs/2501.12084v1),  [pdf](http://arxiv.org/pdf/2501.12084v1)

**Tags**: cs.DC cs.AR cs.PF 



### Build Optimization: A Systematic Literature Review
**Authors**: Henri AÃ¯dasso, Mohammed Sayagh, Francis Bordeleau

**Updated**: 2025-01-21T07:32:06Z

**Summary**: Continuous Integration (CI) consists of an automated build process involving continuous compilation, testing, and packaging of the software system. While CI comes up with several advantages related to quality and time to delivery, CI also presents several challenges addressed by a large body of research. To better understand the literature so as to help practitioners find solutions for their problems and guide future research, we conduct a systematic review of 97 studies on build optimization published between 2006 and 2024, which we summarized according to their goals, methodologies, used datasets, and leveraged metrics. The identified build optimization studies focus on two main challenges: (1) long build durations, and (2) build failures. To meet the first challenge, existing studies have developed a range of techniques, including predicting build outcome and duration, selective build execution, and build acceleration using caching or repairing performance smells. The causes of build failures have been the subject of several studies, leading to the development of techniques for predicting build script maintenance and automating repair. Recent studies have also focused on predicting flaky build failures caused by environmental issues. The majority of these techniques use machine learning algorithms and leverage build metrics, which we classify into five categories. Additionally, we identify eight publicly available build datasets for build optimization research.

**Link**: [arxiv](http://arxiv.org/abs/2501.11940v1),  [pdf](http://arxiv.org/pdf/2501.11940v1)

**Tags**: cs.SE 



### A New Construction Structure on Coded Caching with Linear   Subpacketization: Non-Half-Sum Disjoint Packing
**Authors**: Minquan Cheng, Huimei Wei, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-21T03:13:21Z

**Summary**: Coded caching is a promising technique to effectively reduce peak traffic by using local caches and the multicast gains generated by these local caches. We prefer to design a coded caching scheme with the subpacketization $F$ and transmission load $R$ as small as possible since these are the key metrics for evaluating the implementation complexity and transmission efficiency of the scheme, respectively. However, most of the existing coded caching schemes have large subpacketizations which grow exponentially with the number of users $K$, and there are a few schemes with linear subpacketizations which have large transmission loads. In this paper, we focus on studying the linear subpacketization, i.e., $K=F$, coded caching scheme with low transmission load. Specifically, we first introduce a new combinatorial structure called non-half-sum disjoint packing (NHSDP) which can be used to generate a coded caching scheme with $K=F$. Then a class of new schemes is obtained by constructing NHSDP. Theoretical and numerical comparisons show that (i) compared to the existing schemes with linear subpacketization (to the number of users), the proposed scheme achieves a lower load; (ii) compared to some existing schemes with polynomial subpacketization, the proposed scheme can also achieve a lower load in some cases; (iii) compared to some existing schemes with exponential subpacketization, the proposed scheme has loads close to those of these schemes in some cases. Moreover, the new concept of NHSDP is closely related to the classical combinatorial structures such as cyclic difference packing (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash family (PHF). These connections indicate that NHSDP is an important combinatorial structure in the field of combinatorial design.

**Link**: [arxiv](http://arxiv.org/abs/2501.11855v1),  [pdf](http://arxiv.org/pdf/2501.11855v1)

**Tags**: cs.IT math.IT 



### PDA Construction via Union of Cartesian Product Cache Configurations for   Coded Caching
**Authors**: Jinyu Wang, Minquan Cheng, Kai Wan, Giuseppe Caire

**Updated**: 2025-01-21T02:35:31Z

**Summary**: Caching is an efficient technique to reduce peak traffic by storing popular content in local caches. Placement delivery array (PDA) proposed by Yan et al. is a combinatorial structure to design coded caching schemes with uncoded placement and one-shot linear delivery. By taking the $m$-fold Cartesian product of a small base PDA, Wang et al. constructed a big PDA while maintaining the memory ratio and transmission load unchanged, which achieves linear growth in both the number of users and coded caching gain. In order to achieve exponential growth in both the number of users and coded caching gain, in this paper we propose a PDA construction by taking the union operation of the cache configurations from the $m$-fold Cartesian product of a base PDA. The resulting PDA leads to a coded caching scheme with subpacketization increasing sub-exponentially with the number of users while keeping the load constant for fixed memory ratio. By applying the proposed construction to existing base PDAs, three new coded caching schemes are obtained, which cover some existing schemes as special cases and can achieve lower load with simultaneously lower subpacketization for some memory ratios.

**Link**: [arxiv](http://arxiv.org/abs/2501.11834v1),  [pdf](http://arxiv.org/pdf/2501.11834v1)

**Tags**: cs.IT math.IT 



### Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference
**Authors**: Pouya Hamadanian, Sadjad Fouladi

**Updated**: 2025-01-20T23:10:13Z

**Summary**: Large Language Models (LLM) have revolutionized natural language processing, but their inference demands substantial resources, while under-utilizing high-end accelerators like GPUs. A major bottleneck arises from the attention mechanism, which requires storing large key-value caches, limiting the maximum achievable throughput way below the available computing resources. Current approaches attempt to mitigate this issue through memory-efficient attention and paging mechanisms, but remained constrained by the assumption that all operations must be performed on high-end accelerators.   In this work, we propose Glinthawk, a two-tiered architecture that decouples the attention mechanism from the rest of the Transformer model. This approach allows the memory requirements for attention to scale independently, enabling larger batch sizes and more efficient use of the high-end accelerators. We prototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the other. Compared to a traditional single-tier setup, it improves throughput by $5.9\times$ and reduces cost of generation by $2.8\times$. For longer sequence lengths, it achieves $16.3\times$ throughput improvement at $2.4\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-oriented applications such as batch processing. We shared our prototype publicly at \url{https://github.com/microsoft/glinthawk}.

**Link**: [arxiv](http://arxiv.org/abs/2501.11779v1),  [pdf](http://arxiv.org/pdf/2501.11779v1)

**Tags**: cs.LG cs.DC cs.PF 



### Personalized Federated Learning for Cellular VR: Online Learning and   Dynamic Caching
**Authors**: Krishnendu S. Tharakan, Hayssam Dahrouj, Nour Kouzayha, Hesham ElSawy, Tareq Y. Al-Naffouri

**Updated**: 2025-01-20T21:07:44Z

**Summary**: Delivering an immersive experience to virtual reality (VR) users through wireless connectivity offers the freedom to engage from anywhere at any time. Nevertheless, it is challenging to ensure seamless wireless connectivity that delivers real-time and high-quality videos to the VR users. This paper proposes a field of view (FoV) aware caching for mobile edge computing (MEC)-enabled wireless VR network. In particular, the FoV of each VR user is cached/prefetched at the base stations (BSs) based on the caching strategies tailored to each BS. Specifically, decentralized and personalized federated learning (DP-FL) based caching strategies with guarantees are presented. Considering VR systems composed of multiple VR devices and BSs, a DP-FL caching algorithm is implemented at each BS to personalize content delivery for VR users. The utilized DP-FL algorithm guarantees a probably approximately correct (PAC) bound on the conditional average cache hit. Further, to reduce the cost of communicating gradients, one-bit quantization of the stochastic gradient descent (OBSGD) is proposed, and a convergence guarantee of $\mathcal{O}(1/\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is the number of iterations. Additionally, to better account for the wireless channel dynamics, the FoVs are grouped into multicast or unicast groups based on the number of requesting VR users. The performance of the proposed DP-FL algorithm is validated through realistic VR head-tracking dataset, and the proposed algorithm is shown to have better performance in terms of average delay and cache hit as compared to baseline algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2501.11745v1),  [pdf](http://arxiv.org/pdf/2501.11745v1)

**Tags**: cs.IT cs.LG math.IT 



### Hierarchical Coded Caching in High Memory Regime with Coded Placement
**Authors**: Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-01-20T14:19:48Z

**Summary**: We consider a two-layer hierarchical coded caching network where a server with a library of $N$ files is connected to $K_1$ mirrors, each having a cache memory of size $M_1$. Each mirror is further connected to $K_2$ users, each equipped with a dedicated cache of size $M_2$. In this paper, we propose two distinct coded caching schemes based on coded placement, corresponding to two distinct memory pairs, \( (M_1, M_2) \). We show that the proposed schemes outperform the existing schemes at these memory points given by the proposed schemes for smaller values of $K_2$. In setups where mirrors are positioned near each other, avoiding signal interference is crucial. This can be ensured by having all mirrors transmit using orthogonal carrier frequencies. To compare our schemes with existing ones, we used the composite rate metric, which accurately represents the total bandwidth utilized in such setups. The composite rate is given by $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to the mirrors, and $R_2$ is the rate from the mirrors to the users, with respect to $M_1$ and $M_2$.

**Link**: [arxiv](http://arxiv.org/abs/2501.11502v1),  [pdf](http://arxiv.org/pdf/2501.11502v1)

**Tags**: cs.IT math.IT 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-01-20T08:44:01Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v3),  [pdf](http://arxiv.org/pdf/2411.10659v3)

**Tags**: cs.PL 



### ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large   Vision-Language Models
**Authors**: Yassir Bendou, Amine Ouasfi, Vincent Gripon, Adnane Boukhayma

**Updated**: 2025-01-19T21:25:53Z

**Summary**: The growing popularity of Contrastive Language-Image Pretraining (CLIP) has led to its widespread application in various visual downstream tasks. To enhance CLIP's effectiveness and versatility, efficient few-shot adaptation techniques have been widely adopted. Among these approaches, training-free methods, particularly caching methods exemplified by Tip-Adapter, have gained attention for their lightweight adaptation without the need for additional fine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective, showing that caching methods function as local adapters and are connected to a well-established kernel literature. Drawing on this insight, we offer a theoretical understanding of how these methods operate and suggest multiple avenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the importance of incorporating global information in local adapters. Therefore, we subsequently propose a global method that learns a proximal regularizer in a reproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our method, which we call ProKeR (Proximal Kernel ridge Regression), has a closed form solution and achieves state-of-the-art performances across 11 datasets in the standard few-shot adaptation benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2501.11175v1),  [pdf](http://arxiv.org/pdf/2501.11175v1)

**Tags**: cs.CV cs.AI cs.LG 



### Cache Coherence Over Disaggregated Memory
**Authors**: Ruihong Wang, Jianguo Wang, Walid G. Aref

**Updated**: 2025-01-19T19:46:21Z

**Summary**: Disaggregating memory from compute offers the opportunity to better utilize stranded memory in cloud data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes. However, the limited computing power on disaggregated memory servers makes traditional cache coherence protocols suboptimal, particularly in the case of stranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. It aligns the state machine of the shared-exclusive latch protocol with the MSI protocol by introducing lazy latch-release and invalidation messages, thereby ensuring both atomicity of data access and cache coherence. SELCC embeds cache-ownership metadata directly into the RDMA latch word, enabling efficient cache ownership management via RDMA atomic operations. SELCC can serve as an abstraction layer over disaggregated memory with APIs that resemble main-memory accesses. A concurrent B-tree and three transaction concurrency control algorithms are realized using SELCC's abstraction layer. Experimental results show that SELCC significantly outperforms Remote-Procedure-Call-based protocols for cache coherence under limited remote computing power. Applications on SELCC achieve comparable or superior performance over disaggregated memory compared to competitors.

**Link**: [arxiv](http://arxiv.org/abs/2409.02088v3),  [pdf](http://arxiv.org/pdf/2409.02088v3)

**Tags**: cs.DB cs.DC cs.ET 



### SIC-free Multicast Scheduling for Multi-antenna Coded Caching
**Authors**: MohammadJavad Sojdeh, MohammadJavad Salehi, Antti TÃ¶lli

**Updated**: 2025-01-19T17:33:28Z

**Summary**: Multi-antenna coded caching (CC) with multicast beamforming often relies on complex successive interference cancellation (SIC) structures to decode a superposition of multiple streams received by each user. Traditional signal-level schemes require the regeneration of interfering signals from the cache, adding significant computational complexity. To address this, we propose a bit-level multicast scheduling scheme enabling linear, SIC-free decoding of parallel streams by repeatedly transmitting data terms with linearly independent coefficients. Two reference strategies for constructing the coefficients matrix are considered: a random strategy, which lacks control over matrix construction, and an equal-distant strategy, which balances users' interference and data terms equally. In contrast, the proposed sparse strategy minimizes the number of multicast streams transmitted in parallel during each interval, simplifying the system while optimizing resource usage. To further enhance the symmetric rate, a successive projection algorithm is applied to exploit channel properties and optimize user ordering. With the coefficients matrix and optimized user ordering in place, multicast beamformers are refined to aggregate desired data from relevant multicast streams. Numerical simulations validate the effectiveness of the sparse strategy, demonstrating significant gains in symmetric rate.

**Link**: [arxiv](http://arxiv.org/abs/2501.11126v1),  [pdf](http://arxiv.org/pdf/2501.11126v1)

**Tags**: cs.IT math.IT 



### Coded Caching for Hierarchical Two-Layer Networks with Coded Placement
**Authors**: Rajlaxmi Pandey, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-01-19T15:47:14Z

**Summary**: We examine a two-layered hierarchical coded caching problem, a configuration addressed in existing research. This involves a server connected to $K_1$ mirrors, each of which serves $K_2$ users. The mirrors and the users are equipped with caches of size $M_1$ and $M_2$, respectively. We propose a hierarchical coded caching scheme with coded placements that outperforms existing schemes. To ensure a fair comparison, we introduce the notion of composite rate, defined as $\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the rate from the server to mirrors and $R_2$ is the rate from mirrors to users. The composite rate has not been discussed before in the literature and is pertinent when mirrors transmit with different carrier frequencies. For the proposed scheme, we show a trade-off between the global memory $\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and compare with the existing schemes. Additionally, we conduct this comparative analysis by plotting $R_1$ + $R_2$ against global memory, which is particularly beneficial for systems wherein each mirror can utilize the same carrier frequency, given their significant spatial separation. Additionally, we propose an optimized scheme for the specific case of a single mirror, showing improved performance in this scenario.

**Link**: [arxiv](http://arxiv.org/abs/2312.15024v2),  [pdf](http://arxiv.org/pdf/2312.15024v2)

**Tags**: cs.IT math.IT 



### Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications
**Authors**: Mohammad NaseriTehrani, MohammadJavad Salehi, Antti TÃ¶lli

**Updated**: 2025-01-18T19:10:23Z

**Summary**: Integrating coded caching (CC) into multiple-input multiple-output (MIMO) communications can significantly enhance the achievable degrees of freedom (DoF) in wireless networks. This paper investigates a practical cache-aided asymmetric MIMO configuration with cache ratio $\gamma$, where a server equipped with $L$ transmit antennas communicates with $K$ users, each having $G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the \emph{min-G} scheme, which treats the system as symmetric by assuming all users have the same number of antennas, equal to the smallest among them; the \emph{Grouping} scheme, which maximizes spatial multiplexing gain separately within each user subset at the cost of some global caching gain; and the \emph{Phantom} scheme, which dynamically redistributes spatial resources using virtual or "phantom" antenna users, bridging the performance gains of the min-G and Grouping schemes. These strategies jointly optimize the number of users, $\Omega$, and the parallel streams decoded by each user, $\beta_k$, ensuring linear decodability for all target users. Analytical and numerical results confirm that the proposed schemes achieve significant DoF improvements across various system configurations, demonstrating the potential of content-aware MIMO-CC strategies for enhancing wireless network performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.10854v1),  [pdf](http://arxiv.org/pdf/2501.10854v1)

**Tags**: cs.IT eess.SP math.IT 



### D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial   Access Topology
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-01-18T13:04:23Z

**Summary**: This paper considers wireless device-to-device (D2D) coded caching in a multiaccess network, where the users communicate with each other and each user can access multiple cache nodes. Access topologies derived from two combinatorial designs known as the $t$-design and $t$-group divisible design ($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively, which subsume a few other known topologies, have been studied for the multiaccess coded caching (MACC) network by Cheng \textit{et al.} in \cite{MACC_des}. These access topologies are extended to a multiaccess D2D coded caching (MADCC) network and novel MADCC schemes are proposed. MADCC network has been studied so far only for the cyclic wrap-around topology. Apart from the proposed novel MADCC schemes, MADCC schemes are also derived from the existing MACC schemes in \cite{MACC_des}. To compare the performance of different MADCC schemes, the metrics of load per user and subpacketization level are used while keeping the number of caches and cache memory size same. The proposed MADCC scheme with $t$-design topology performs better in terms of subpacketization level while achieving the same load per user compared to the MADCC scheme derived from the MACC scheme with $t$-design topology in \cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs better in terms of load per user while achieving the same subpacketization level compared to the MADCC scheme derived from the MACC scheme with $t$-GDD topology in \cite{MACC_des} in some cases. Compared to the existing MADCC scheme with cyclic wrap-around topology, the proposed MADCC scheme with $t$-design topology performs better in terms of load per user, and the proposed MADCC scheme with $t$-GDD topology performs better in terms of subpacketization level at the expense of an increase in load per user.

**Link**: [arxiv](http://arxiv.org/abs/2501.10756v1),  [pdf](http://arxiv.org/pdf/2501.10756v1)

**Tags**: cs.IT math.IT 



### SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS   and Hardware Co-design
**Authors**: Haoyang Zhang, Yuqi Xue, Yirui Eric Zhou, Shaobo Li, Jian Huang

**Updated**: 2025-01-18T07:29:20Z

**Summary**: The CXL-based solid-state drive (CXL-SSD) provides a promising approach towards scaling the main memory capacity at low cost. However, the CXL-SSD faces performance challenges due to the long flash access latency and unpredictable events such as garbage collection in the SSD device, stalling the host processor and wasting compute cycles. Although the CXL interface enables the byte-granular data access to the SSD, accessing flash chips is still at page granularity due to physical limitations. The mismatch of access granularity causes significant unnecessary I/O traffic to flash chips, worsening the suboptimal end-to-end data access performance. In this paper, we present SkyByte, an efficient CXL-based SSD that employs a holistic approach to address the aforementioned challenges by co-designing the host operating system (OS) and SSD controller. To alleviate the long memory stall when accessing the CXL-SSD, SkyByte revisits the OS context switch mechanism and enables opportunistic context switches upon the detection of long access delays. To accommodate byte-granular data accesses, SkyByte architects the internal DRAM of the SSD controller into a cacheline-level write log and a page-level data cache, and enables data coalescing upon log cleaning to reduce the I/O traffic to flash chips. SkyByte also employs optimization techniques that include adaptive page migration for exploring the performance benefits of fast host memory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with a CXL-SSD simulator and evaluate its efficiency with various data-intensive applications. Our experiments show that SkyByte outperforms current CXL-based SSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average. SkyByte also reaches 75% of the performance of the ideal case that assumes unlimited DRAM capacity in the host, which offers an attractive cost-effective solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.10682v1),  [pdf](http://arxiv.org/pdf/2501.10682v1)

**Tags**: cs.AR 



### Geometric rigidity of simple modules for algebraic groups
**Authors**: Michael Bate, David I. Stewart

**Updated**: 2025-01-17T16:16:54Z

**Summary**: Let k be a field, let G be an affine algebraic k-group and V a finite-dimensional G-module. We say V is rigid if the socle series and radical series coincide for the action of G on each indecomposable summand of V; say V is geometrically rigid (resp. absolutely rigid) if V is rigid after base change of G and V to k (resp. any field extension of k). We show that all simple G-modules are geometrically rigid, though not in general absolutely rigid. More precisely, we show that if V is a simple G-module, then there is a finite purely inseparable extension kV /k naturally attached to V such that V is absolutely rigid as a G-module after base change to kV. The proof turns on an investigation of algebras of the form K otimes E where K and E are field extensions of k; we give an example of such an algebra which is not rigid as a module over itself. We establish the existence of the purely inseparable field extension kV /k through an analogous version for artinian algebras.   In the second half of the paper we apply recent results on the structure and representation theory of pseudo-reductive groups to give a concrete description of kV when G is smooth and connected. Namely, we combine the main structure theorem of the Conrad-Prasad classification of pseudo-reductive G together with our previous high weight theory. For V a simple G-module, we calculate the minimal field of definition of the geometric Jacobson radical of EndG(V) in terms of the high weight of V and the Conrad-Prasad classification data; this gives a concrete construction of the field kV as a subextension of the minimal field of definition of the geometric unipotent radical of G. We also observe that the Conrad-Prasad classification can be used to hone the dimension formula for V we had previously established; we also use it to give a description of EndG(V) which includes a dimension formula.

**Link**: [arxiv](http://arxiv.org/abs/2409.05221v3),  [pdf](http://arxiv.org/pdf/2409.05221v3)

**Tags**: math.RT math.GR math.RA 20G05 



### The NIC should be part of the OS
**Authors**: Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-01-17T12:01:28Z

**Summary**: The network interface adapter (NIC) is a critical component of a modern cloud server which occupies a unique position. Not only is network performance vital to the efficient operation of the machine, but unlike application-oriented compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency. Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path. However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernel-bypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.10138v1),  [pdf](http://arxiv.org/pdf/2501.10138v1)

**Tags**: cs.OS cs.AR cs.NI 



### BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix   Sharing and Throughput-oriented Token Batching
**Authors**: Zhen Zheng, Xin Ji, Taosong Fang, Fanghao Zhou, Chuanjie Liu, Gang Peng

**Updated**: 2025-01-17T09:37:36Z

**Summary**: Large language models (LLMs) increasingly play an important role in a wide range of information processing and management tasks. Many of these tasks are performed in large batches or even offline, and the performance indictor for which is throughput. These tasks usually show the characteristic of prefix sharing, where different prompt input can partially show the common prefix. However, the existing LLM inference engines tend to optimize the streaming requests and show limitations of supporting the large batched tasks with the prefix sharing characteristic. The existing solutions use the LRU-based cache to reuse the KV context of common prefix between requests. The KV context that are about to be reused may prematurely evicted with the implicit cache management. Besides, the streaming oriented systems do not leverage the request-batch information and can not mix the decoding tokens with the prefill chunks to the best for the batched scenarios, and thus fails to saturate the GPU. We propose BatchLLM to address the above problems. BatchLLM explicitly identifies the common prefixes globally. The requests sharing the same prefix will be scheduled together to reuse the KV context the best. BatchLLM reorders the requests and schedules the requests with larger ratio of decoding first to better mix the decoding tokens with the latter prefill chunks, and applies memory-centric token batching to enlarge the token-batch sizes, which helps to increase the GPU utilization. Finally, BatchLLM optimizes the prefix-shared Attention kernel with horizontal fusion to reduce tail effect and kernel launch overhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang by 1.3$\times$ to 10.8$\times$ on a set of microbenchmarks and a typical industry workload under different hardware environments.

**Link**: [arxiv](http://arxiv.org/abs/2412.03594v2),  [pdf](http://arxiv.org/pdf/2412.03594v2)

**Tags**: cs.CL cs.AI cs.DC cs.LG 



### Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing
**Authors**: Alireza Khadem, Daichi Fujiki, Hilbert Chen, Yufeng Gu, Nishil Talati, Scott Mahlke, Reetuparna Das

**Updated**: 2025-01-17T01:24:12Z

**Summary**: In-cache computing technology transforms existing caches into long-vector compute units and offers low-cost alternatives to building expensive vector engines for mobile CPUs. Unfortunately, existing long-vector Instruction Set Architecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm Scalable Vector Extension (SVE), provide only one-dimensional strided and random memory accesses. While this is sufficient for typical vector engines, it fails to effectively utilize the large Single Instruction, Multiple Data (SIMD) widths of in-cache vector engines. This is because mobile data-parallel kernels expose limited parallelism across a single dimension.   Based on our analysis of mobile vector kernels, we introduce a long-vector Multi-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE achieves high SIMD resource utilization and enables flexible programming by abstracting cache geometry and data layout. The proposed ISA features multi-dimensional strided and random memory accesses and efficient dimension-level masked execution to encode parallelism across multiple dimensions. Using a wide range of data-parallel mobile workloads, we demonstrate that MVE offers significant performance and energy reduction benefits of 2.9x and 8.8x, on average, compared to the SIMD units of a commercial mobile processor, at an area overhead of 3.6%.

**Link**: [arxiv](http://arxiv.org/abs/2501.09902v1),  [pdf](http://arxiv.org/pdf/2501.09902v1)

**Tags**: cs.AR 



### Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of   the Atomic Layer Deposition Temperature and the Lithographic Patterning   Method
**Authors**: Alessandro Paghi, Sebastiano Battisti, Simone Tortorella, Giorgio De Simoni, Francesco Giazotto

**Updated**: 2025-01-16T15:11:42Z

**Summary**: Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics, have become the standard insulators in gate architectures, enhancing the electrical performance of both room temperature and cryogenic electronics. This study delves into the cryogenic (3 K) performance of high-k dielectrics commonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via Atomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and dielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on metal-insulator-metal capacitors. Our findings reveal a strong dependence of HfO2 cryogenic performance on the ALD growth temperature, while the latter shows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of the relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K. Additionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked their properties at cryogenic temperatures. The study also investigates the impact of the patterning method, namely, UV or electron-beam lithography (acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric properties.

**Link**: [arxiv](http://arxiv.org/abs/2407.04501v2),  [pdf](http://arxiv.org/pdf/2407.04501v2)

**Tags**: cond-mat.mtrl-sci cond-mat.mes-hall cond-mat.supr-con 



### Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk   Synchronization
**Authors**: Congcong Chen, Jinhua Cui, Gang Qu, Jiliang Zhang

**Updated**: 2025-01-16T10:35:59Z

**Summary**: Memory-disk synchronization is a critical technology for ensuring data correctness, integrity, and security, especially in systems that handle sensitive information like financial transactions and medical records. We propose SYNC+SYNC, a group of attacks that exploit the memory-disk synchronization primitives. SYNC+SYNC works by subtly varying the timing of synchronization on the write buffer, offering several advantages: 1) implemented purely in software, enabling deployment on any hardware devices; 2) resilient against existing cache partitioning and randomization techniques; 3) unaffected by prefetching techniques and cache replacement strategies. We present the principles of SYNC+SYNC through the implementation of two write covert channel protocols, using either a single file or page, and introduce three enhanced strategies that utilize multiple files and pages. The feasibility of these channels is demonstrated in both cross-process and cross-sandbox scenarios across diverse operating systems (OSes). Experimental results show that, the average rate can reach 2.036 Kb/s (with a peak rate of 14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the average rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the error rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first high-speed write covert channel for software cache.

**Link**: [arxiv](http://arxiv.org/abs/2312.11501v2),  [pdf](http://arxiv.org/pdf/2312.11501v2)

**Tags**: cs.CR 



### Adaptive Contextual Caching for Mobile Edge Large Language Model Service
**Authors**: Guangyuan Liu, Yinqiu Liu, Jiacheng Wang, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong

**Updated**: 2025-01-16T08:52:38Z

**Summary**: Mobile edge Large Language Model (LLM) deployments face inherent constraints, such as limited computational resources and network bandwidth. Although Retrieval-Augmented Generation (RAG) mitigates some challenges by integrating external knowledge bases, inefficient cache management can still result in high retrieval latency and frequent cache updates. To address these issues, we propose an Adaptive Contextual Caching (ACC) framework that anticipates user needs by proactively caching semantically relevant data for mobile-edge LLMs. ACC utilizes a deep reinforcement learning (DRL) module to refine cache replacement policies, balancing user context, document similarity, and the overhead associated with cache misses. Experimental results demonstrate that ACC increases cache hit rates to over 80\% after only 11 training episodes, outperforming FIFO, LRU, and semantic-only caching while reducing retrieval latency by up to 40\%. In particular, ACC also reduces local caching overhead (i.e., the cost of updating the cache when a miss occurs) by as much as 55\%, enabling scalable, low-latency LLM services in resource-constrained edge environments.

**Link**: [arxiv](http://arxiv.org/abs/2501.09383v1),  [pdf](http://arxiv.org/pdf/2501.09383v1)

**Tags**: cs.NI 



### Interoceptive Robots for Convergent Shared Control in Collaborative   Construction Work
**Authors**: Xiaoshan Zhou, Carol C. Menassa, Vineet R. Kamat

**Updated**: 2025-01-16T04:50:15Z

**Summary**: Building autonomous mobile robots (AMRs) with optimized efficiency and adaptive capabilities-able to respond to changing task demands and dynamic environments-is a strongly desired goal for advancing construction robotics. Such robots can play a critical role in enabling automation, reducing operational carbon footprints, and supporting modular construction processes. Inspired by the adaptive autonomy of living organisms, we introduce interoception, which centers on the robot's internal state representation, as a foundation for developing self-reflection and conscious learning to enable continual learning and adaptability in robotic agents. In this paper, we factorize internal state variables and mathematical properties as "cognitive dissonance" in shared control paradigms, where human interventions occasionally occur. We offer a new perspective on how interoception can help build adaptive motion planning in AMRs by integrating the legacy of heuristic costs from grid/graph-based algorithms with recent advances in neuroscience and reinforcement learning. Declarative and procedural knowledge extracted from human semantic inputs is encoded into a hypergraph model that overlaps with the spatial configuration of onsite layout for path planning. In addition, we design a velocity-replay module using an encoder-decoder architecture with few-shot learning to enable robots to replicate velocity profiles in contextualized scenarios for multi-robot synchronization and handover collaboration. These "cached" knowledge representations are demonstrated in simulated environments for multi-robot motion planning and stacking tasks. The insights from this study pave the way toward artificial general intelligence in AMRs, fostering their progression from complexity to competence in construction automation.

**Link**: [arxiv](http://arxiv.org/abs/2501.09290v1),  [pdf](http://arxiv.org/pdf/2501.09290v1)

**Tags**: cs.RO 



### PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid   Resolution Diffusion Serving
**Authors**: Desen Sun, Zepeng Zhao, Yuke Wang

**Updated**: 2025-01-16T02:40:07Z

**Summary**: The Text-to-Image (T2I) diffusion model is one of the most popular models in the world. However, serving diffusion models at the entire image level faces several problems, especially when there are multiple candidate resolutions. First, image based serving system prevents requests with different resolutions from batching together. On the other hand, requests with hybrid resolutions also indicate diverse locality features, which makes it hard to apply the same cache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch Management Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that provides a patch-level management strategy to gather hybrid resolution requests into batches. Specifically, PATCHEDSERVE incorporates a novel patch-based processing workflow, significantly enhancing throughput for hybrid resolution inputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to fully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features an SLO-aware scheduling algorithm with lightweight online latency prediction, achieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve 30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while not hurt the image quality.

**Link**: [arxiv](http://arxiv.org/abs/2501.09253v1),  [pdf](http://arxiv.org/pdf/2501.09253v1)

**Tags**: cs.DC 



### Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of   Micro-UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-01-15T21:09:22Z

**Summary**: This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content management system for disaster scenarios where communication infrastructure is generally compromised. Utilizing a hybrid network of stationary and mobile Micro-UAVs, this system aims to provide crucial content access to isolated communities. In the developed architecture, stationary anchor UAVs, equipped with vertical and lateral links, serve users in individual disaster-affected communities. and mobile micro-ferrying UAVs, with enhanced mobility, extend coverage across multiple such communities. The primary goal is to devise a content dissemination system that dynamically learns caching policies to maximize content accessibility to users left without communication infrastructure. The core contribution is an adaptive content dissemination framework that employs a decentralized Top-k Multi-Armed Bandit learning approach for efficient UAV caching decisions. This approach accounts for geo-temporal variations in content popularity and diverse user demands. Additionally, a Selective Caching Algorithm is proposed to minimize redundant content copies by leveraging inter-UAV information sharing. Through functional verification and performance evaluation, the proposed framework demonstrates improved system performance and adaptability across varying network sizes, micro-UAV swarms, and content popularity distributions.

**Link**: [arxiv](http://arxiv.org/abs/2404.10845v2),  [pdf](http://arxiv.org/pdf/2404.10845v2)

**Tags**: cs.LG cs.NI I.2.11 



### Towards Federated Multi-Armed Bandit Learning for Content Dissemination   using Swarm of UAVs
**Authors**: Amit Kumar Bhuyan, Hrishikesh Dutta, Subir Biswas

**Updated**: 2025-01-15T20:55:13Z

**Summary**: This paper introduces an Unmanned Aerial Vehicle - enabled content management architecture that is suitable for critical content access in communities of users that are communication-isolated during diverse types of disaster scenarios. The proposed architecture leverages a hybrid network of stationary anchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The anchor UAVs are equipped with both vertical and lateral communication links, and they serve local users, while the mobile micro-ferrying UAVs extend coverage across communities with increased mobility. The focus is on developing a content dissemination system that dynamically learns optimal caching policies to maximize content availability. The core innovation is an adaptive content dissemination framework based on distributed Federated Multi-Armed Bandit learning. The goal is to optimize UAV content caching decisions based on geo-temporal content popularity and user demand variations. A Selective Caching Algorithm is also introduced to reduce redundant content replication by incorporating inter-UAV information sharing. This method strategically preserves the uniqueness in user preferences while amalgamating the intelligence across a distributed learning system. This approach improves the learning algorithm's ability to adapt to diverse user preferences. Functional verification and performance evaluation confirm the proposed architecture's utility across different network sizes, UAV swarms, and content popularity patterns.

**Link**: [arxiv](http://arxiv.org/abs/2501.09146v1),  [pdf](http://arxiv.org/pdf/2501.09146v1)

**Tags**: cs.LG cs.NI I.2.11 



### LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System
**Authors**: Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi

**Updated**: 2025-01-15T01:34:46Z

**Summary**: The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.

**Link**: [arxiv](http://arxiv.org/abs/2412.20166v2),  [pdf](http://arxiv.org/pdf/2412.20166v2)

**Tags**: cs.AR cs.AI 



### CORD: Co-design of Resource Allocation and Deadline Decomposition with   Generative Profiling
**Authors**: Robert Gifford, Abby Eisenklam, Georgiy A. Bondar, Yifan Cai, Tushar Sial, Linh Thi Xuan Phan, Abhishek Halder

**Updated**: 2025-01-14T23:13:14Z

**Summary**: As multicore hardware is becoming increasingly common in real-time systems, traditional scheduling techniques that assume a single worst-case execution time for a task are no longer adequate, since they ignore the impact of shared resources on execution time. When tasks execute concurrently on different cores, their execution times often vary substantially with their allocated budgets of shared resources, such as cache and memory bandwidth. Even under a specific resource allocation, the resource use pattern of a task also changes with time during a job execution. It is therefore important to consider the relationship between multicore resources and execution time in task modeling and scheduling algorithm design.   In this paper, we propose a much more precise execution model for DAG-based real-time tasks that captures the time-varying resource use characteristics of a task under different budgets of shared resources. We present a generative resource profiling algorithm that efficiently predicts, from limited measurement data, the resource profile of a task at any time during its execution under a given resource budget. The generative profiles can then be used to construct the execution models for tasks, using which one can make informed resource allocation decisions. We further introduce a multicore resource allocation and deadline decomposition co-design technique for DAG-based tasks that leverages the generated execution models to jointly allocate resources and deadlines to subtasks, to maximize resource efficiency and schedulability. Our evaluation results show that our generative profiling algorithm achieves high accuracy while being efficient, and that our co-allocation technique substantially improves schedulability compared to a state-of-the-art deadline decomposition method.

**Link**: [arxiv](http://arxiv.org/abs/2501.08484v1),  [pdf](http://arxiv.org/pdf/2501.08484v1)

**Tags**: cs.OS cs.PF 



### Reciprocating Locks
**Authors**: Dave Dice, Alex Kogan

**Updated**: 2025-01-14T20:04:15Z

**Summary**: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.

**Link**: [arxiv](http://arxiv.org/abs/2501.02380v3),  [pdf](http://arxiv.org/pdf/2501.02380v3)

**Tags**: cs.DC D.4.1 



### PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM   Serving
**Authors**: Ahmet Caner YÃ¼zÃ¼gÃ¼ler, Jiawei Zhuang, Lukas Cavigelli

**Updated**: 2025-01-14T15:14:10Z

**Summary**: Large language models (LLMs) are widely used across various applications, but their substantial computational requirements pose significant challenges, particularly in terms of HBM bandwidth bottlenecks and inter-device communication overhead. In this paper, we present PRESERVE, a novel prefetching framework designed to optimize LLM inference by overlapping memory reads for model weights and KV-cache with collective communication operations. Through extensive experiments conducted on commercial AI accelerators, we demonstrate up to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs. Additionally, we perform a design space exploration that identifies the optimal hardware configuration for the proposed method, showing a further 1.25x improvement in performance per cost by selecting the optimal L2 cache size. Our results show that PRESERVE has the potential to mitigate the memory bottlenecks and communication overheads, offering a solution to improve the performance and scalability of the LLM inference systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.08192v1),  [pdf](http://arxiv.org/pdf/2501.08192v1)

**Tags**: cs.AI cs.AR cs.DC 



### AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out   Context Attribution
**Authors**: Fengyuan Liu, Nikhil Kandpal, Colin Raffel

**Updated**: 2025-01-14T14:07:55Z

**Summary**: The influence of contextual input on the behavior of large language models (LLMs) has prompted the development of context attribution methods that aim to quantify each context span's effect on an LLM's generations. The leave-one-out (LOO) error, which measures the change in the likelihood of the LLM's response when a given span of the context is removed, provides a principled way to perform context attribution, but can be prohibitively expensive to compute for large models. In this work, we introduce AttriBoT, a series of novel techniques for efficiently computing an approximation of the LOO error for context attribution. Specifically, AttriBoT uses cached activations to avoid redundant operations, performs hierarchical attribution to reduce computation, and emulates the behavior of large target models with smaller proxy models. Taken together, AttriBoT can provide a >300x speedup while remaining more faithful to a target model's LOO error than prior context attribution methods. This stark increase in performance makes computing context attributions for a given response 30x faster than generating the response itself, empowering real-world applications that require computing attributions at scale. We release a user-friendly and efficient implementation of AttriBoT to enable efficient LLM interpretability as well as encourage future development of efficient context attribution methods.

**Link**: [arxiv](http://arxiv.org/abs/2411.15102v2),  [pdf](http://arxiv.org/pdf/2411.15102v2)

**Tags**: cs.LG 



### TreeKV: Smooth Key-Value Cache Compression with Tree Structures
**Authors**: Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang

**Updated**: 2025-01-14T12:06:33Z

**Summary**: Efficient key-value (KV) cache compression is critical for scaling transformer-based Large Language Models (LLMs) in long sequences and resource-limited settings. Existing methods evict tokens based on their positions or importance scores, but position-based strategies can miss crucial information outside predefined regions, while those relying on global importance scores resulting in strong regional biases, limiting the KV cache's overall context retention and potentially impairing the performance of LLMs on complex tasks. Our wavelet analysis reveals that as tokens approach the end of sequence, their contributions to generation gradually increase and tends to diverge more from neighboring tokens, indicating a smooth transition with increasing complexity and variability from distant to nearby context. Motivated by this observation, we propose TreeKV, an intuitive, training-free method that employs a tree structure for smooth cache compression. TreeKV maintains a fixed cache size, allowing LLMs to deliver high-quality output even in long text scenarios. Unlike most compression methods, TreeKV is applicable to both the generation and prefilling stages. TreeKV consistently surpasses all baseline models in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with short context window to generalize to longer window with a 16x cache reduction. On the Longbench benchmark, TreeKV achieves the best performance with only 6\% of the budget at optimal efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.04987v2),  [pdf](http://arxiv.org/pdf/2501.04987v2)

**Tags**: cs.CL 



### Cell-level modelling of homeostasis in confined epithelial monolayers
**Authors**: KVS Chaithanya, Jan Rozman, Andrej KoÅ¡mrlj, Rastko Sknepnek

**Updated**: 2025-01-14T11:41:14Z

**Summary**: Tissue homeostasis, the biological process of maintaining a steady state in tissue via control of cell proliferation, death, and metabolic function, is essential for the development, growth, maintenance, and proper function of living organisms. Disruptions to this process can lead to serious diseases and even death. In this study, we use the vertex model for the cell-level description of tissue mechanics to investigate the impact of the tissue microenvironment and local mechanical properties of cells on homeostasis in confined epithelial tissues. We find a dynamic steady state, where the balance between cell divisions and removals sustains homeostasis. By characterising homeostasis in terms of cell count, tissue area, and the cells' neighbour count distribution, we identify the factors that govern regulated and ordered tissue growth. This work, therefore, sheds light on the mechanisms underlying tissue homeostasis and highlights the importance of mechanics in the control of biological processes such as tissue development and disease pathology.

**Link**: [arxiv](http://arxiv.org/abs/2403.15896v2),  [pdf](http://arxiv.org/pdf/2403.15896v2)

**Tags**: physics.bio-ph cond-mat.soft 



### Multi-matrix Factorization Attention
**Authors**: Jingcheng Hu, Houyi Li, Yinmin Zhang, Zili Wang, Shuigeng Zhou, Xiangyu Zhang, Heung-Yeung Shum, Daxin Jiang

**Updated**: 2025-01-14T05:48:07Z

**Summary**: We propose novel attention architectures, Multi-matrix Factorization Attention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard Multi-Head Attention (MHA), including SOTA methods like MLA, fail to maintain as strong performance under stringent Key-Value cache (KV cache) constraints. MFA enhances model capacity by efficiently scaling up both the number and dimension of attention heads through low-rank matrix factorization in the Query-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory requirements by repurposing the key cache as value through value projection re-parameterization. MFA's design enables strong model capacity when working under tight KV cache budget, while MFA-KR is suitable for even harsher KV cache limits with minor performance trade-off. Notably, in our extensive and large-scale experiments, the proposed architecture outperforms MLA and performs comparably to MHA, while reducing KV cache usage by up to 56% and 93.7%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2412.19255v2),  [pdf](http://arxiv.org/pdf/2412.19255v2)

**Tags**: cs.LG cs.CL 



### Lean Attention: Hardware-Aware Scalable Attention Mechanism for the   Decode-Phase of Transformers
**Authors**: Rya Sanovar, Srikant Bharadwaj, Renee St. Amant, Victor RÃ¼hle, Saravan Rajmohan

**Updated**: 2025-01-14T05:00:34Z

**Summary**: Transformer-based models have emerged as one of the most widely used architectures for natural language processing, natural language generation, and image generation. The size of the state-of-the-art models has increased steadily reaching billions of parameters. These huge models are memory hungry and incur significant inference latency even on cutting edge AI-accelerators, such as GPUs. Specifically, the time and memory complexity of the attention operation is quadratic in terms of the total context length, i.e., prompt and output tokens. Thus, several optimizations such as key-value tensor caching and FlashAttention computation have been proposed to deliver the low latency demands of applications relying on such large models. However, these techniques do not cater to the computationally distinct nature of different phases during inference.   To that end, we propose LeanAttention, a scalable technique of computing self-attention for the token-generation phase (decode-phase) of decoder-only transformer models. LeanAttention enables scaling the attention mechanism implementation for the challenging case of long context lengths by re-designing the execution flow for the decode-phase. We identify that the associative property of online softmax can be treated as a reduction operation thus allowing us to parallelize the attention computation over these large context lengths. We extend the "stream-K" style reduction of tiled calculation to self-attention to enable parallel computation resulting in an average of 2.6x attention execution speedup over FlashAttention-2 and up to 8.33x speedup for 512k context lengths.

**Link**: [arxiv](http://arxiv.org/abs/2405.10480v2),  [pdf](http://arxiv.org/pdf/2405.10480v2)

**Tags**: cs.AR cs.LG I.2.7; C.1.4 



### QMDB: Quick Merkle Database
**Authors**: Isaac Zhang, Ryan Zarick, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong

**Updated**: 2025-01-14T02:02:01Z

**Summary**: Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain state management by integrating key-value (KV) and Merkle tree storage into a single unified architecture. QMDB delivers a significant throughput improvement over existing architectures, achieving up to 6X over the widely used RocksDB and 8X over NOMT, a leading verifiable database. Its novel append-only twig-based design enables one SSD read per state access, O(1) IOs for updates, and in-memory Merkleization on a memory footprint as small as 2.3 bytes per entry, enabling it to run on even modest consumer-grade PCs. QMDB scales seamlessly across both commodity and enterprise hardware, achieving up to 2.28 million state updates per second. This performance enables support for 1 million token transfers per second (TPS), marking QMDB as the first solution achieving such a milestone. QMDB has been benchmarked with workloads exceeding 15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to scale to 280 billion entries on a single server. Furthermore, QMDB introduces historical proofs, unlocking the ability to query its blockchain's historical state at the latest block. QMDB not only meets the demands of current blockchains but also provides a robust foundation for building scalable, efficient, and verifiable decentralized applications across diverse use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.05262v2),  [pdf](http://arxiv.org/pdf/2501.05262v2)

**Tags**: cs.NI cs.DB 



### Parallel Key-Value Cache Fusion for Position Invariant RAG
**Authors**: Philhoon Oh, Jinwoo Shin, James Thorne

**Updated**: 2025-01-13T17:50:30Z

**Summary**: Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as `Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2501.07523v1),  [pdf](http://arxiv.org/pdf/2501.07523v1)

**Tags**: cs.AI cs.CL 



### FlashRNN: Optimizing Traditional RNNs on Modern Hardware
**Authors**: Korbinian PÃ¶ppel, Maximilian Beck, Sepp Hochreiter

**Updated**: 2025-01-13T17:34:22Z

**Summary**: While Transformers and other sequence-parallelizable neural network architectures seem like the current state of the art in sequence modeling, they specifically lack state-tracking capabilities. These are important for time-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs, as well as modern variants like sLSTM do have these capabilities at the cost of strictly sequential processing. While this is often seen as a strong limitation, we show how fast these networks can get with our hardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the register level on modern GPUs. We extend traditional RNNs with a parallelization variant that processes multiple RNNs of smaller hidden state in parallel, similar to the head-wise processing in Transformers. To enable flexibility on different GPU variants, we introduce a new optimization framework for hardware-internal cache sizes, memory and compute handling. It models the hardware in a setting using polyhedral-like constraints, including the notion of divisibility. This speeds up the solution process in our ConstrINT library for general integer constraint satisfaction problems (integer CSPs). We show that our kernels can achieve 50x speed-ups over a vanilla PyTorch implementation and allow 40x larger hidden sizes compared to our Triton implementation. Our open-source kernels and the optimization library are released here to boost research in the direction of state-tracking enabled RNNs and sequence modeling: \url{https://github.com/NX-AI/flashrnn}

**Link**: [arxiv](http://arxiv.org/abs/2412.07752v2),  [pdf](http://arxiv.org/pdf/2412.07752v2)

**Tags**: cs.LG cs.AI 



### DID Link: Authentication in TLS with Decentralized Identifiers and   Verifiable Credentials
**Authors**: Sandro Rodriguez Garzon, Dennis Natusch, Artur Philipp, Axel KÃ¼pper, Hans Joachim Einsiedler, Daniela Schneider

**Updated**: 2025-01-13T09:33:25Z

**Summary**: Authentication in TLS is predominately carried out with X.509 digital certificates issued by certificate authorities (CA). The centralized nature of current public key infrastructures, however, comes along with severe risks, such as single points of failure and susceptibility to cyber-attacks, potentially undermining the security and trustworthiness of the entire system. With Decentralized Identifiers (DID) alongside distributed ledger technology, it becomes technically feasible to prove ownership of a unique identifier without requiring an attestation of the proof's public key by a centralized and therefore vulnerable CA. This article presents DID Link, a novel authentication scheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant way with self-issued X.509 certificates that are equipped with ledger-anchored DIDs instead of CA-issued identifiers. It facilitates the exchange of tamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable Credentials after the TLS handshake to complete the authentication with a full identification of the communication partner. A prototypical implementation shows comparable TLS handshake durations of DID Link if verification material is cached and reasonable prolongations if it is obtained from a ledger. The significant speed improvement of the resulting TLS channel over a widely used, DID-based alternative transport protocol on the application layer demonstrates the potential of DID Link to become a viable solution for the establishment of secure and trustful end-to-end communication links with decentrally managed digital identities.

**Link**: [arxiv](http://arxiv.org/abs/2405.07533v4),  [pdf](http://arxiv.org/pdf/2405.07533v4)

**Tags**: cs.CR cs.NI 



### Generating Data Locality to Accelerate Sparse Matrix-Matrix   Multiplication on CPUs
**Authors**: Jordi Wolfson-Pou, Jan Laukemann, Fabrizio Petrini

**Updated**: 2025-01-13T04:31:04Z

**Summary**: Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation in many applications. Current multithreaded implementations are based on Gustavson's algorithm and often perform poorly on large matrices due to limited cache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic NUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To generate locality, MAGNUS reorders the intermediate product into discrete cache-friendly chunks using a two-level hierarchical approach. The accumulator is applied to each chunk, where the chunk size is chosen such that the accumulator is cache-efficient. MAGNUS is input- and system-aware: based on the matrix characteristics and target system specifications, the optimal number of chunks is computed by minimizing the storage cost of the necessary data structures. MAGNUS allows for a hybrid accumulation strategy in which each chunk uses a different accumulator based on an input threshold. We consider two accumulators: an AVX-512 vectorized bitonic sorting algorithm and classical dense accumulation. An OpenMP implementation of MAGNUS is compared with several baselines for a variety of different matrices on three Intel x86 architectures. For matrices from the SuiteSparse collection, MAGNUS is faster than all the baselines in most cases and is orders of magnitude faster than Intel MKL for several matrices. For massive random matrices that model social network graphs, MAGNUS scales to the largest matrix sizes, while the baselines fail to do so. Furthermore, MAGNUS is close to the optimal bound for these matrices, regardless of the matrix size, structure, and density.

**Link**: [arxiv](http://arxiv.org/abs/2501.07056v1),  [pdf](http://arxiv.org/pdf/2501.07056v1)

**Tags**: cs.DC 



### A Unified Framework for Automated Code Transformation and Pragma   Insertion
**Authors**: StÃ©phane Pouget, Louis-NoÃ«l Pouchet, Jason Cong

**Updated**: 2025-01-13T03:11:28Z

**Summary**: High-level synthesis, source-to-source compilers, and various Design Space Exploration techniques for pragma insertion have significantly improved the Quality of Results of generated designs. These tools offer benefits such as reduced development time and enhanced performance. However, achieving high-quality results often requires additional manual code transformations and tiling selections, which are typically performed separately or as pre-processing steps. Although DSE techniques enable code transformation upfront, the vastness of the search space often limits the exploration of all possible code transformations, making it challenging to determine which transformations are necessary. Additionally, ensuring correctness remains challenging, especially for complex transformations and optimizations.   To tackle this obstacle, we first propose a comprehensive framework leveraging HLS compilers. Our system streamlines code transformation, pragma insertion, and tiles size selection for on-chip data caching through a unified optimization problem, aiming to enhance parallelization, particularly beneficial for computation-bound kernels. Them employing a novel Non-Linear Programming (NLP) approach, we simultaneously ascertain transformations, pragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation demonstrates that our framework adeptly identifies the appropriate transformations, including scenarios where no transformation is necessary, and inserts pragmas to achieve a favorable Quality of Results.

**Link**: [arxiv](http://arxiv.org/abs/2405.03058v5),  [pdf](http://arxiv.org/pdf/2405.03058v5)

**Tags**: cs.SE cs.PL 



### On Optimizing Locality of Graph Transposition on Modern Architectures
**Authors**: Mohsen Koohi Esfahani, Hans Vandierendonck

**Updated**: 2025-01-12T17:01:40Z

**Summary**: This paper investigates the shared-memory Graph Transposition (GT) problem, a fundamental graph algorithm that is widely used in graph analytics and scientific computing.   Previous GT algorithms have significant memory requirements that are proportional to the number of vertices and threads which obstructs their use on large graphs. Moreover, atomic memory operations have become comparably fast on recent CPU architectures, which creates new opportunities for improving the performance of concurrent atomic accesses in GT.   We design PoTra, a GT algorithm which leverages graph structure and processor and memory architecture to optimize locality and performance. PoTra limits the size of additional data structures close to CPU cache sizes and utilizes the skewed degree distribution of graph datasets to optimize locality and performance. We present the performance model of PoTra to explain the connection between cache and memory response times and graph locality.   Our evaluation of PoTra on three CPU architectures and 20 real-world and synthetic graph datasets with up to 128 billion edges demonstrates that PoTra achieves up to 8.7 times speedup compared to previous works and if there is a performance loss it remains limited to 15.7%, on average.

**Link**: [arxiv](http://arxiv.org/abs/2501.06872v1),  [pdf](http://arxiv.org/pdf/2501.06872v1)

**Tags**: cs.DC cs.AR cs.DS cs.PF 



### MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large   Language Model Inference
**Authors**: Wenxuan Zeng, Ye Dong, Jinjin Zhou, Junming Ma, Jin Tan, Runsheng Wang, Meng Li

**Updated**: 2025-01-12T13:18:04Z

**Summary**: Private large language model (LLM) inference based on secure multi-party computation (MPC) offers cryptographically-secure protection for both user prompt and proprietary model weights. However, it suffers from large latency overhead especially for long input sequences. While key-value (KV) cache eviction algorithms have been proposed to reduce the computation and memory cost for plaintext inference, they are not designed for MPC and cannot benefit private inference easily. In this paper, we propose an accurate and MPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on the observation that historical tokens in a long sequence may have different effects on the downstream decoding. Hence, MPCache combines a look-once static eviction algorithm to discard unimportant tokens and a query-aware dynamic selection algorithm to further select a small subset of tokens for attention computation. As existing dynamic selection algorithms incur too much latency, we propose a series of optimizations to drastically reduce the KV cache selection overhead, including MPC-friendly similarity approximation, hierarchical KV cache clustering, and cross-layer index sharing strategy. With extensive experiments, we demonstrate that MPCache consistently outperforms prior-art KV cache eviction baselines across different LLM generation tasks and achieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction on different sequence lengths, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.06807v1),  [pdf](http://arxiv.org/pdf/2501.06807v1)

**Tags**: cs.CR 



### Linear Attention Sequence Parallelism
**Authors**: Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong

**Updated**: 2025-01-12T12:01:47Z

**Summary**: Sequence parallelism (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single device. However, for linear sequence modeling methods like linear attention, existing SP approaches do not take advantage of their right-product-first feature, resulting in sub-optimal communication efficiency and usability. In this paper, we introduce Linear Attention Sequence Parallelism (LASP), an efficient SP approach designed for linear attention-based transformer models. Specifically, we design an efficient point-to-point ring-style communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead, comparing with existing SP methods. We enhance the computation efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPUs. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with very-long sequences. We also discuss the generalization of LASP on other linear sequence modeling methods. Extensive experiments on linear attention-based models are conducted with varying sequence lengths from 2K to 4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\times$ longer than existing SP methods. The code is available at https://github.com/OpenNLPLab/LASP.

**Link**: [arxiv](http://arxiv.org/abs/2404.02882v2),  [pdf](http://arxiv.org/pdf/2404.02882v2)

**Tags**: cs.LG cs.CL 



### Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma   Generated THz Pulses
**Authors**: Benjamin Colmey, Rodrigo T. Paulino, Gaspard Beaufort, David G. Cooke

**Updated**: 2025-01-12T11:15:41Z

**Summary**: Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.

**Link**: [arxiv](http://arxiv.org/abs/2409.07196v3),  [pdf](http://arxiv.org/pdf/2409.07196v3)

**Tags**: cond-mat.mtrl-sci physics.plasm-ph 



### Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion
**Authors**: Bohai Gu, Hao Luo, Song Guo, Peiran Dong

**Updated**: 2025-01-12T05:25:06Z

**Summary**: Recently, diffusion-based methods have achieved great improvements in the video inpainting task. However, these methods still face many challenges, such as maintaining temporal consistency and the time-consuming issue. This paper proposes an advanced video inpainting framework using optical Flow-guided Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch architecture, where a flow branch first restores corrupted flow and a multi-scale flow adapter provides motion guidance to the main inpainting branch. Additionally, a training-free latent interpolation method is proposed to accelerate the multi-step denoising process using flow warping. Further introducing a flow attention cache mechanism, FLoED efficiently reduces the computational cost brought by incorporating optical flow. Comprehensive experiments in both background restoration and object removal tasks demonstrate that FloED outperforms state-of-the-art methods from the perspective of both performance and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2412.00857v2),  [pdf](http://arxiv.org/pdf/2412.00857v2)

**Tags**: cs.CV 



### Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV   Cache Management
**Authors**: Liu Qianli, Hong Zicong, Chen Fahao, Li Peng, Guo Song

**Updated**: 2025-01-12T04:29:39Z

**Summary**: Serving large language models (LLMs) for massive users is challenged by the significant memory footprint of the transient state, known as the key-value (KV) cache, which scales with sequence length and number of requests. Instead of renting or buying more expensive GPUs, the load imbalance of the KV cache across GPUs, coupled with recent advances in inter-GPU communication, provides an opportunity to serve more requests via request migration. However, high migration overhead and unpredictable request patterns make it challenging. Therefore, this paper proposes MELL, a memory-efficient LLM serving system via multi-GPU KV cache management. It saves the number of GPUs needed in the system by considering the dynamic KV cache load and the costly request migration. Specifically, we first develop an adaptive request migration mechanism to balance the computational and communication overheads and adapt to diverse resource conditions. Then, we design an online algorithm tailored to a multi-LLM request and multi-GPU scheduling problem with migration enabled. It aims to minimise the required GPUs while limiting the number of migrations. Finally, we implement a prototype of MELL and demonstrate that it reduces the number of GPUs by 31% and increases the GPU utilization by 43% at most compared to existing LLM serving systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.06709v1),  [pdf](http://arxiv.org/pdf/2501.06709v1)

**Tags**: cs.DC 



### GraphSnapShot: Caching Local Structure for Fast Graph Learning
**Authors**: Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman

**Updated**: 2025-01-11T15:26:48Z

**Summary**: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.   The code for GraphSnapShot is publicly available at https://github.com/NoakLiu/GraphSnapShot.

**Link**: [arxiv](http://arxiv.org/abs/2406.17918v4),  [pdf](http://arxiv.org/pdf/2406.17918v4)

**Tags**: cs.LG cs.DC cs.SI 



### Optimizing digital experiences with content delivery networks:   Architectures, performance strategies, and future trends
**Authors**: Anuj Tyagi

**Updated**: 2025-01-11T03:47:04Z

**Summary**: This research investigates how CDNs (Content Delivery Networks) can improve the digital experience, as consumers increasingly expect fast, efficient, and effortless access to online resources. CDNs play a crucial role in reducing latency, enhancing scalability, and optimizing delivery mechanisms, which is evident across various platforms and regions. The study focuses on key CDN concerns, such as foundational and modern CDN architectures, edge computing, hybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing topics, including caching, load balancing, and the novel features of HTTP/3 and QUIC.   Current trends, such as integrating CDNs with 5G networks, serverless architectures, and AI-driven traffic management, are examined to demonstrate how CDN technology is likely to evolve. The study also addresses challenges related to security, cost, and global regulations. Practical examples from the e-commerce, streaming, and gaming industries highlight how enhanced CDNs are transforming these sectors.   The conclusions emphasize the need to evolve CDN strategies to meet growing user expectations and adapt to the rapidly changing digital landscape. Additionally, the research identifies future research opportunities, particularly in exploring the impact of QC, the enhancement of AI services, and the sustainability of CDN solutions. Overall, the study situates architectural design, performance strategies, and emerging trends to address gaps and create a more efficient and secure approach for improving digital experiences.

**Link**: [arxiv](http://arxiv.org/abs/2501.06428v1),  [pdf](http://arxiv.org/pdf/2501.06428v1)

**Tags**: cs.NI cs.SE 



### Tensor Product Attention Is All You Need
**Authors**: Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao

**Updated**: 2025-01-11T03:37:10Z

**Summary**: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. By factorizing these representations into contextual low-rank components (contextual factorization) and seamlessly integrating with RoPE, TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation of language modeling tasks, we demonstrate that T6 exceeds the performance of standard Transformer baselines including MHA, MQA, GQA, and MLA across various metrics, including perplexity and a range of renowned evaluation benchmarks. Notably, TPAs memory efficiency enables the processing of significantly longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. The code is available at https://github.com/tensorgi/T6.

**Link**: [arxiv](http://arxiv.org/abs/2501.06425v1),  [pdf](http://arxiv.org/pdf/2501.06425v1)

**Tags**: cs.CL cs.AI cs.LG 



### Unispeaker: A Unified Approach for Multimodality-driven Speaker   Generation
**Authors**: Zhengyan Sheng, Zhihao Du, Heng Lu, Shiliang Zhang, Zhen-Hua Ling

**Updated**: 2025-01-11T00:47:29Z

**Summary**: Recent advancements in personalized speech generation have brought synthetic speech increasingly close to the realism of target speakers' recordings, yet multimodal speaker generation remains on the rise. This paper introduces UniSpeaker, a unified approach for multimodality-driven speaker generation. Specifically, we propose a unified voice aggregator based on KV-Former, applying soft contrastive loss to map diverse voice description modalities into a shared voice space, ensuring that the generated voice aligns more closely with the input descriptions. To evaluate multimodality-driven voice control, we build the first multimodality-based voice control (MVC) benchmark, focusing on voice suitability, voice diversity, and speech quality. UniSpeaker is evaluated across five tasks using the MVC benchmark, and the experimental results demonstrate that UniSpeaker outperforms previous modality-specific models. Speech samples are available at \url{https://UniSpeaker.github.io}.

**Link**: [arxiv](http://arxiv.org/abs/2501.06394v1),  [pdf](http://arxiv.org/pdf/2501.06394v1)

**Tags**: cs.SD cs.AI eess.AS 



### Tame fields, Graded Rings and Finite Complete Sequences of Key   Polynomials
**Authors**: Caio Henrique Silva de Souza

**Updated**: 2025-01-10T10:11:45Z

**Summary**: In this paper, we present a criterion for $(K,v)$ to be henselian and defectless in terms of finite complete sequences of key polynomials. For this, we use the theory of Mac Lane-Vaqui\'e chains and abstract key polynomials. We then prove that a valued field $(K,v)$ is tame if and only if $vK$ is $p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$ admits a finite complete sequence of key polynomials. The properties $vK$ $p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on the associated graded ring. We also make considerations on simply defectless and algebraically maximal valued fields and purely inertial and purely ramified extensions.

**Link**: [arxiv](http://arxiv.org/abs/2407.01030v2),  [pdf](http://arxiv.org/pdf/2407.01030v2)

**Tags**: math.AC 13A18 



### Handover_Management_in_UAV_Networks_with_Blockages
**Authors**: Neetu R R, Gourab Ghatak, Vivek Ashok Bohara

**Updated**: 2025-01-09T15:14:05Z

**Summary**: We investigate the performance of unmanned aerial vehicle (UAV)-based networks in urban environments characterized by blockages, focusing on their capability to support the service demands of mobile users. The UAV-base stations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson point process (MPPP), where the marks represent the altitude of each UAV-BS. Leveraging stochastic geometry, we analyze the impact of blockages on network reliability by studying the meta distribution (MD) of the signal-to-interference noise ratio (SINR) for a specific reliability threshold and the association probabilities for both line-of-sight (LoS) and non line-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile users, we propose a novel cache-based handover management strategy that dynamically selects the cell search time and delays the received signal strength (RSS)-based base station (BS) associations. This strategy aims to minimize unnecessary handovers (HOs) experienced by users by leveraging caching capabilities at user equipment (UE), thus reducing latency, ensuring seamless connectivity, and maintaining the quality of service (QoS). This study provides valuable insights into optimizing UAV network deployments to support the stringent requirements in the network, ensuring reliable, low-latency, and high-throughput communication for next-generation smart cities.

**Link**: [arxiv](http://arxiv.org/abs/2409.20433v2),  [pdf](http://arxiv.org/pdf/2409.20433v2)

**Tags**: eess.SP 



### ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State   Drives
**Authors**: Shaobo Li, Yirui Eric Zhou, Hao Ren, Jian Huang

**Updated**: 2025-01-09T06:18:39Z

**Summary**: Unlike non-volatile memory that resides on the processor memory bus, memory-semantic solid-state drives (SSDs) support both byte and block access granularity via PCIe or CXL interconnects. They provide scalable memory capacity using NAND flash at a much lower cost. In addition, they have different performance characteristics for their dual byte/block interface respectively, while offering essential memory semantics for upper-level software. Such a byte-accessible storage device provides new implications on the software system design.   In this paper, we develop a new file system, named ByteFS, by rethinking the design primitives of file systems and SSD firmware to exploit the advantages of both byte and block-granular data accesses. ByteFS supports byte-granular data persistence to retain the persistence nature of SSDs. It extends the core data structure of file systems by enabling dual byte/block-granular data accesses. To facilitate the support for byte-granular writes, \pname{} manages the internal DRAM of SSD firmware in a log-structured manner and enables data coalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also enables coordinated data caching between the host page cache and SSD cache for best utilizing the precious memory resource. We implement ByteFS on both a real programmable SSD and an emulated memory-semantic SSD for sensitivity study. Compared to state-of-the-art file systems for non-volatile memory and conventional SSDs, ByteFS outperforms them by up to 2.7$\times$, while preserving the essential properties of a file system. ByteFS also reduces the write traffic to SSDs by up to 5.1$\times$ by alleviating unnecessary writes caused by both metadata and data updates in file systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.04993v1),  [pdf](http://arxiv.org/pdf/2501.04993v1)

**Tags**: cs.OS 



### Optimal Oblivious Algorithms for Multi-way Joins
**Authors**: Xiao Hu, Zhiang Wu

**Updated**: 2025-01-09T03:02:31Z

**Summary**: In cloud databases, cloud computation over sensitive data uploaded by clients inevitably causes concern about data security and privacy. Even when encryption primitives and trusted computing environments are integrated into query processing to safeguard the actual contents of the data, access patterns of algorithms can still leak private information about the data. Oblivious Random Access Memory (ORAM) and circuits are two generic approaches to address this issue, ensuring that access patterns of algorithms remain oblivious to the data. However, deploying these methods on insecure algorithms, particularly for multi-way join processing, is computationally expensive and inherently challenging.   In this paper, we propose a novel sorting-based algorithm for multi-way join processing that operates without relying on ORAM simulations or other security assumptions. Our algorithm is a non-trivial, provably oblivious composition of basic primitives, with time complexity matching the insecure worst-case optimal join algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic, with cache complexity matching the insecure lower bound, also up to a logarithmic factor. This clean and straightforward approach has the potential to be extended to other security settings and implemented in practical database systems.

**Link**: [arxiv](http://arxiv.org/abs/2501.04216v2),  [pdf](http://arxiv.org/pdf/2501.04216v2)

**Tags**: cs.DB cs.CR 



### Modern Hardware Security: A Review of Attacks and Countermeasures
**Authors**: Jyotiprakash Mishra, Sanjay K. Sahay

**Updated**: 2025-01-08T10:14:19Z

**Summary**: With the exponential rise in the use of cloud services, smart devices, and IoT devices, advanced cyber attacks have become increasingly sophisticated and ubiquitous. Furthermore, the rapid evolution of computing architectures and memory technologies has created an urgent need to understand and address hardware security vulnerabilities. In this paper, we review the current state of vulnerabilities and mitigation strategies in contemporary computing systems. We discuss cache side-channel attacks (including Spectre and Meltdown), power side-channel attacks (such as Simple Power Analysis, Differential Power Analysis, Correlation Power Analysis, and Template Attacks), and advanced techniques like Voltage Glitching and Electromagnetic Analysis to help understand and build robust cybersecurity defense systems and guide further research. We also examine memory encryption, focusing on confidentiality, granularity, key management, masking, and re-keying strategies. Additionally, we cover Cryptographic Instruction Set Architectures, Secure Boot, Root of Trust mechanisms, Physical Unclonable Functions, and hardware fault injection techniques. The paper concludes with an analysis of the RISC-V architecture's unique security challenges. The comprehensive analysis presented in this paper is essential for building resilient hardware security solutions that can protect against both current and emerging threats in an increasingly challenging security landscape.

**Link**: [arxiv](http://arxiv.org/abs/2501.04394v1),  [pdf](http://arxiv.org/pdf/2501.04394v1)

**Tags**: cs.CR cs.AR 



### Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear   Approximation
**Authors**: Samrat Mukhopadhyay, Debasmita Mukherjee

**Updated**: 2025-01-07T17:32:19Z

**Summary**: We consider the problem of \textit{online sparse linear approximation}, where one predicts the best sparse approximation of a sequence of measurements in terms of linear combination of columns of a given measurement matrix. Such online prediction problems are ubiquitous, ranging from medical trials to web caching to resource allocation. The inherent difficulty of offline recovery also makes the online problem challenging. In this letter, we propose Follow-The-Approximate-Sparse-Leader, an efficient online meta-policy to address this online problem. Through a detailed theoretical analysis, we prove that under certain assumptions on the measurement sequence, the proposed policy enjoys a data-dependent sublinear upper bound on the static regret, which can range from logarithmic to square-root. Numerical simulations are performed to corroborate the theoretical findings and demonstrate the efficacy of the proposed online policy.

**Link**: [arxiv](http://arxiv.org/abs/2501.00799v2),  [pdf](http://arxiv.org/pdf/2501.00799v2)

**Tags**: cs.LG math.OC 



### Not all tokens are created equal: Perplexity Attention Weighted Networks   for AI generated text detection
**Authors**: Pablo Miralles-GonzÃ¡lez, Javier Huertas-Tato, Alejandro MartÃ­n, David Camacho

**Updated**: 2025-01-07T17:00:49Z

**Summary**: The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.03940v1),  [pdf](http://arxiv.org/pdf/2501.03940v1)

**Tags**: cs.CL cs.AI 



### Parallel $k$d-tree with Batch Updates
**Authors**: Ziyang Men, Zheqi Shen, Yan Gu, Yihan Sun

**Updated**: 2025-01-06T23:16:22Z

**Summary**: The $k$d-tree is one of the most widely used data structures to manage multi-dimensional data. Due to the ever-growing data volume, it is imperative to consider parallelism in $k$d-trees. However, we observed challenges in existing parallel kd-tree implementations, for both constructions and updates.   The goal of this paper is to develop efficient in-memory $k$d-trees by supporting high parallelism and cache-efficiency. We propose the Pkd-tree (Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and in practice. The Pkd-tree supports parallel tree construction, batch update (insertion and deletion), and various queries including k-nearest neighbor search, range query, and range count. We proved that our algorithms have strong theoretical bounds in work (sequential time complexity), span (parallelism), and cache complexity. Our key techniques include 1) an efficient construction algorithm that optimizes work, span, and cache complexity simultaneously, and 2) reconstruction-based update algorithms that guarantee the tree to be weight-balanced. With the new algorithmic insights and careful engineering effort, we achieved a highly optimized implementation of the Pkd-tree.   We tested Pkd-tree with various synthetic and real-world datasets, including both uniform and highly skewed data. We compare the Pkd-tree with state-of-the-art parallel $k$d-tree implementations. In all tests, with better or competitive query performance, Pkd-tree is much faster in construction and updates consistently than all baselines. We released our code.

**Link**: [arxiv](http://arxiv.org/abs/2411.09275v2),  [pdf](http://arxiv.org/pdf/2411.09275v2)

**Tags**: cs.DS cs.DB cs.DC cs.PF 



### The Power of Negative Zero: Datatype Customization for Quantized Large   Language Models
**Authors**: Yuzong Chen, Xilai Dai, Chi-chih Chang, Yash Akhauri, Mohamed S. Abdelfattah

**Updated**: 2025-01-06T22:40:40Z

**Summary**: Large language models (LLMs) have demonstrated remarkable performance across various machine learning tasks, quickly becoming one of the most prevalent AI workloads. Yet the substantial memory requirement of LLMs significantly hinders their deployment for end users. Post-training quantization (PTQ) serves as one of the most hardware-efficient methods to mitigate the memory and computational demands of LLMs. Although the traditional integer (INT) datatype has received widespread adoption in PTQ methods, floating-point (FP) quantization has emerged as a viable alternative thanks to its effectiveness in fitting LLM numerical distributions. However, the FP datatype in sign-magnitude binary representation contains both positive and negative zero, which constrains its representation capability, particularly under low precision (3 and 4 bits). In this paper, we extend the basic FP datatype to perform Redundant Zero Remapping (RaZeR), which remaps the negative zero FP encoding to a set of pre-defined special values to maximally utilize FP quantization encodings and to better fit LLM numerical distributions. Through careful selection of special values, RaZeR outperforms conventional asymmetric INT quantization while achieving high computational efficiency. We demonstrate that RaZeR can be seamlessly integrated with quantization algorithms for both weights and KV-cache, including advanced methods with clipping and transformations, and consistently achieve better model accuracy. Additionally, we implement a fast GEMV kernel with fused dequantization that efficiently converts the 4-bit RaZeR value to FP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows that RaZeR improves the GEMV speed by up to 7.56$\times$ compared to the FP16 implementation, while achieving up to 2.72$\times$ speedup in the LLM decoding throughput.

**Link**: [arxiv](http://arxiv.org/abs/2501.04052v1),  [pdf](http://arxiv.org/pdf/2501.04052v1)

**Tags**: cs.LG cs.CL 



### Twinkle: A GPU-based binary-lens microlensing code with contour   integration method
**Authors**: Suwei Wang, Lile Wang, Subo Dong

**Updated**: 2025-01-06T19:00:03Z

**Summary**: With the rapidly increasing rate of microlensing planet detections, microlensing modeling software faces significant challenges in computation efficiency. Here, we develop the Twinkle code, an efficient and robust binary-lens modeling software suite optimized for heterogeneous computing devices, especially GPUs. Existing microlensing codes have the issue of catastrophic cancellation that undermines the numerical stability and precision, and Twinkle resolves them by refining the coefficients of the binary-lens equation. We also devise an improved method for robustly identifying ghost images, thereby enhancing computational reliability. We have advanced the state of the art by optimizing Twinkle specifically for heterogeneous computing devices by taking into account the unique task and cache memory dispatching patterns of GPUs, while the compatibility with the traditional computing architectures of CPUs is still maintained. Twinkle has demonstrated an acceleration of approximately 2 orders of magnitude (>~100 times) on contemporary GPUs. The enhancement in computational speed of Twinkle will translate to the delivery of accurate and highly efficient data analysis for ongoing and upcoming microlensing projects. Both GPU and CPU versions of Twinkle are open-source and publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2501.03322v1),  [pdf](http://arxiv.org/pdf/2501.03322v1)

**Tags**: astro-ph.IM astro-ph.EP astro-ph.GA astro-ph.SR 



### Direct Comparison of Magnetic Penetration Depth in Kagome   Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)
**Authors**: Austin Kaczmarek, Andrea Capa Salinas, Stephen D. Wilson, Katja C. Nowack

**Updated**: 2025-01-06T15:59:23Z

**Summary**: We report measurements of the local temperature-dependent penetration depth, $\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using scanning superconducting quantum interference device (SQUID) microscopy. Our results suggest that the superconducting order in all three compounds is fully gapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density, $\rho_s(T)$, shows deviations from the behavior expected for a single isotropic gap, but the data are well described by models incorporating either a single anisotropic gap or two isotropic gaps. Notably, the temperature dependences of $\lambda(T)$ and $\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are qualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with the superconducting phase reflecting features of the normal-state band structure. Our findings provide a direct comparison of the superconducting properties across the AV$_3$Sb$_5$ family.

**Link**: [arxiv](http://arxiv.org/abs/2412.19919v2),  [pdf](http://arxiv.org/pdf/2412.19919v2)

**Tags**: cond-mat.supr-con 



### Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism
**Authors**: Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li, Sven Koenig

**Updated**: 2025-01-06T06:44:13Z

**Summary**: Multi-Agent Path Finding (MAPF), which focuses on finding collision-free paths for multiple robots, is crucial in autonomous warehouse operations. Lifelong MAPF (L-MAPF), where agents are continuously reassigned new targets upon completing their current tasks, offers a more realistic approximation of real-world warehouse scenarios. While cache storage systems can enhance efficiency and reduce operational costs, existing approaches primarily rely on expectations and mathematical models, often without adequately addressing the challenges of multi-robot planning and execution. In this paper, we introduce a novel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which integrates high-level cache storage with low-level path planning. We have involved a new type of map grid called cache for temporary item storage. Additionally, we involved a task assigner (TA) with a locking mechanism to bridge the gap between the new cache grid and L-MAPF algorithm. The TA dynamically allocates target locations to agents based on their status in various scenarios. We evaluated L-MAPF-CM using different cache replacement policies and task distributions. L-MAPF-CM has demonstrated performance improvements particularly with high cache hit rates and smooth traffic conditions.

**Link**: [arxiv](http://arxiv.org/abs/2501.02803v1),  [pdf](http://arxiv.org/pdf/2501.02803v1)

**Tags**: cs.RO cs.AI 



### From Slow Bidirectional to Fast Autoregressive Video Diffusion Models
**Authors**: Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang

**Updated**: 2025-01-06T01:26:42Z

**Summary**: Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner. We will release the code based on an open-source model in the future.

**Link**: [arxiv](http://arxiv.org/abs/2412.07772v2),  [pdf](http://arxiv.org/pdf/2412.07772v2)

**Tags**: cs.CV 



### ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding
**Authors**: Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie

**Updated**: 2025-01-05T14:11:48Z

**Summary**: Video Large Language Models (VideoLLMs) have achieved remarkable progress in video understanding. However, existing VideoLLMs often inherit the limitations of their backbone LLMs in handling long sequences, leading to challenges for long video understanding. Common solutions either simply uniformly sample videos' frames or compress visual tokens, which focus primarily on low-level temporal visual redundancy, overlooking high-level knowledge redundancy. This limits the achievable compression rate with minimal loss. To this end. we introduce a training-free method, $\textbf{ReTaKe}$, containing two novel modules DPSelect and PivotKV, to jointly model and reduce both temporal visual redundancy and knowledge redundancy for long video understanding. Specifically, DPSelect identifies keyframes with local maximum peak distance based on their visual features, which are closely aligned with human video perception. PivotKV employs the obtained keyframes as pivots and conducts KV-Cache compression for the non-pivot tokens with low attention scores, which are derived from the learned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and LVBench, show that ReTaKe can support 4x longer video sequences with minimal performance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%, even surpassing or on par with much larger ones. Our code is available at https://github.com/SCZwangxiao/video-ReTaKe

**Link**: [arxiv](http://arxiv.org/abs/2412.20504v2),  [pdf](http://arxiv.org/pdf/2412.20504v2)

**Tags**: cs.CV cs.CL cs.MM 



### A Full-System Simulation Framework for CXL-Based SSD Memory System
**Authors**: Yaohui Wang, Zicong Wang, Fanfeng Meng, Yanjing Wang, Yang Ou, Lizhou Wu, Wentao Hong, Xuran Ge, Jijun Cao

**Updated**: 2025-01-05T12:51:08Z

**Summary**: Compute eXpress Link (CXL) is a promising technology for memory disaggregation and expansion. Especially, CXL makes it more effectively for large-capacity storage devices such as Solid State Drive (SSD) to be deployed in the memory pool. However, CXL-based SSDs are still in early stages, necessitating the development of reliable simulation tools. In this paper, we propose CXL-SSD-Sim, the first open-source full-system simulator designed to simulate CXL-based SSD memory system. Constructed on the foundation of gem5 and SimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along with the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM layer as a caching mechanism for the SSD, meticulously engineered to counteract latency issues inherent to CXL-based SSD memory access. Experiments are performed among five different memory devices with CXL-SSD-Sim in aspect of latency, bandwidth and real-world benchmark performance. These experiments serve to underscore the efficacy of our simulation tool in providing a comprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim simulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.

**Link**: [arxiv](http://arxiv.org/abs/2501.02524v1),  [pdf](http://arxiv.org/pdf/2501.02524v1)

**Tags**: cs.AR 



### LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas   and Ad-Hoc Networks
**Authors**: Atonu Ghosh, Sudip Misra

**Updated**: 2025-01-05T07:41:53Z

**Summary**: The minimal infrastructure requirements of LoRa make it suitable for deployments in remote and disaster-stricken areas. Concomitantly, the modern era is witnessing the proliferation of web applications in all aspects of human life, including IoT and other network services. Contemporary IoT and network solutions heavily rely on web applications to render services. However, despite the recent research and development pivoted around LoRa, there is still a lack of studies focusing on web application access over LoRa networks. Specifically, technical challenges like payload size limitation, low data rate, and contentions in multi-user setups limit the applicability of LoRa for web applications. Hence, we propose LoRaWeb, which enables web access over LoRa networks. The LoRaWeb hardware tethers a WiFi hotspot to which the client devices connect and access the web pages using a web browser. LoRa backbone of the network handles the web page transmission from the requester and receiver devices. LoRaWeb implements a synchronization procedure to address the aforementioned challenges for effective message exchange between requesters and responders. The system implements a caching mechanism to reduce latency and contention. Additionally, it implements a message-slicing mechanism in the application layer to overcome the hardware limitations on the message length. The actual hardware-based implementation results indicate seamless deployment, and the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and ~$6 S$ for a $10 KB$ size web page.

**Link**: [arxiv](http://arxiv.org/abs/2501.02469v1),  [pdf](http://arxiv.org/pdf/2501.02469v1)

**Tags**: cs.NI cs.CY cs.SY eess.SY 



### End-to-End Long Document Summarization using Gradient Caching
**Authors**: Rohit Saxena, Hao Tang, Frank Keller

**Updated**: 2025-01-03T13:32:57Z

**Summary**: Training transformer-based encoder-decoder models for long document summarization poses a significant challenge due to the quadratic memory consumption during training. Several approaches have been proposed to extend the input length at test time, but training with these approaches is still difficult, requiring truncation of input documents and causing a mismatch between training and test conditions. In this work, we propose CachED (Gradient $\textbf{Cach}$ing for $\textbf{E}$ncoder-$\textbf{D}$ecoder models), an approach that enables end-to-end training of existing transformer-based encoder-decoder models, using the entire document without truncation. Specifically, we apply non-overlapping sliding windows to input documents, followed by fusion in decoder. During backpropagation, the gradients are cached at the decoder and are passed through the encoder in chunks by re-computing the hidden vectors, similar to gradient checkpointing. In the experiments on long document summarization, we extend BART to CachED BART, processing more than 500K tokens during training and achieving superior performance without using any additional parameters.

**Link**: [arxiv](http://arxiv.org/abs/2501.01805v1),  [pdf](http://arxiv.org/pdf/2501.01805v1)

**Tags**: cs.CL cs.AI 



### Efficient LLM Inference with Activation Checkpointing and Hybrid Caching
**Authors**: Sanghyeon Lee, Hongbeen Kim, Soojin Hwang, Guseul Heo, Minwoo Noh, Jaehyuk Huh

**Updated**: 2025-01-03T12:51:37Z

**Summary**: Recent large language models (LLMs) with enormous model sizes use many GPUs to meet memory capacity requirements incurring substantial costs for token generation. To provide cost-effective LLM inference with relaxed latency constraints, extensive research has focused on expanding GPU memory by leveraging the host memory. However, LLM inference engines that utilize the host memory often face underutilization of GPU compute units, as a considerable portion of inference time is spent in loading the model onto the GPU via host-GPU interconnect. To tackle these challenges of the host memory offloading for LLM, we introduce HybridServe, an LLM inference system with activation checkpointing based on activation caching. The activation cache stores activation checkpoints generated during intermediate inference stages, allowing the fast recomputation of KV cache while model parameters are transferred to GPU from host memory. Unlike conventional methods that recompute the KV cache from scratch using token IDs, the activation cache allows bypassing projection and FFN operations. To balance between the activation recomputation and parameter loading overhead, this study proposes a KV-activation hybrid caching scheme which finds the best ratio of the key-value and activation caches to adjust the recomputation time. Our system achieves 2.19x throughput improvement over the state-of-the-art prior work for offloading both model weights and KV cache.

**Link**: [arxiv](http://arxiv.org/abs/2501.01792v1),  [pdf](http://arxiv.org/pdf/2501.01792v1)

**Tags**: cs.DC 



### Object-level Visual Prompts for Compositional Image Generation
**Authors**: Gaurav Parmar, Or Patashnik, Kuan-Chieh Wang, Daniil Ostashev, Srinivasa Narasimhan, Jun-Yan Zhu, Daniel Cohen-Or, Kfir Aberman

**Updated**: 2025-01-02T18:59:44Z

**Summary**: We introduce a method for composing object-level visual prompts within a text-to-image diffusion model. Our approach addresses the task of generating semantically coherent compositions across diverse scenes and styles, similar to the versatility and expressiveness offered by text prompts. A key challenge in this task is to preserve the identity of the objects depicted in the input visual prompts, while also generating diverse compositions across different images. To address this challenge, we introduce a new KV-mixed cross-attention mechanism, in which keys and values are learned from distinct visual representations. The keys are derived from an encoder with a small bottleneck for layout control, whereas the values come from a larger bottleneck encoder that captures fine-grained appearance details. By mixing keys and values from these complementary sources, our model preserves the identity of the visual prompts while supporting flexible variations in object arrangement, pose, and composition. During inference, we further propose object-level compositional guidance to improve the method's identity preservation and layout correctness. Results show that our technique produces diverse scene compositions that preserve the unique characteristics of each visual prompt, expanding the creative potential of text-to-image generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01424v1),  [pdf](http://arxiv.org/pdf/2501.01424v1)

**Tags**: cs.CV cs.AI cs.GR 



### MSWA: Refining Local Attention with Multi-ScaleWindow Attention
**Authors**: Yixing Xu, Shivank Nag, Dong Li, Lu Tian, Emad Barsoum

**Updated**: 2025-01-02T03:41:32Z

**Summary**: Transformer-based LLMs have achieved exceptional performance across a wide range of NLP tasks. However, the standard self-attention mechanism suffers from quadratic time complexity and linearly increased cache size. Sliding window attention (SWA) solves this problem by restricting the attention range to a fixed-size local context window. Nevertheless, SWA employs a uniform window size for each head in each layer, making it inefficient in capturing context of varying scales. To mitigate this limitation, we propose Multi-Scale Window Attention (MSWA) which applies diverse window sizes across heads and layers in the Transformer. It not only allows for different window sizes among heads within the same layer but also progressively increases window size allocation from shallow to deep layers, thus enabling the model to capture contextual information with different lengths and distances. Experimental results on language modeling and common-sense reasoning tasks substantiate that MSWA outperforms traditional local attention in both effectiveness and efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.01039v1),  [pdf](http://arxiv.org/pdf/2501.01039v1)

**Tags**: cs.CL cs.AI 



### A Survey on Large Language Model Acceleration based on KV Cache   Management
**Authors**: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen

**Updated**: 2025-01-02T03:40:15Z

**Summary**: Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.

**Link**: [arxiv](http://arxiv.org/abs/2412.19442v2),  [pdf](http://arxiv.org/pdf/2412.19442v2)

**Tags**: cs.AI cs.DC 



### FlashInfer: Efficient and Customizable Attention Engine for LLM   Inference Serving
**Authors**: Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze

**Updated**: 2025-01-02T02:02:20Z

**Summary**: Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01005v1),  [pdf](http://arxiv.org/pdf/2501.01005v1)

**Tags**: cs.DC cs.AI cs.LG 



### Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant   Computation Elimination in Diffusion Model
**Authors**: Omid Saghatchian, Atiyeh Gh. Moghadam, Ahmad Nickabadi

**Updated**: 2025-01-01T20:16:27Z

**Summary**: Diffusion models have emerged as a promising approach for generating high-quality, high-dimensional images. Nevertheless, these models are hindered by their high computational cost and slow inference, partly due to the quadratic computational complexity of the self-attention mechanisms with respect to input size. Various approaches have been proposed to address this drawback. One such approach focuses on reducing the number of tokens fed into the self-attention, known as token merging (ToMe). In our method, which is called cached adaptive token merging(CA-ToMe), we calculate the similarity between tokens and then merge the r proportion of the most similar tokens. However, due to the repetitive patterns observed in adjacent steps and the variation in the frequency of similarities, we aim to enhance this approach by implementing an adaptive threshold for merging tokens and adding a caching mechanism that stores similar pairs across several adjacent steps. Empirical results demonstrate that our method operates as a training-free acceleration method, achieving a speedup factor of 1.24 in the denoising process while maintaining the same FID scores compared to existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2501.00946v1),  [pdf](http://arxiv.org/pdf/2501.00946v1)

**Tags**: cs.CV 



### EdgeRAG: Online-Indexed RAG for Edge Devices
**Authors**: Korakit Seemakhupt, Sihang Liu, Samira Khan

**Updated**: 2024-12-31T20:40:43Z

**Summary**: Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge devices is challenging due to limited memory and processing power. In this work, we propose EdgeRAG which addresses the memory constraint by pruning embeddings within clusters and generating embeddings on-demand during retrieval. To avoid the latency of generating embeddings for large tail clusters, EdgeRAG pre-computes and stores embeddings for these clusters, while adaptively caching remaining embeddings to minimize redundant computations and further optimize latency. The result from BEIR suite shows that EdgeRAG offers significant latency reduction over the baseline IVF index, but with similar generation quality while allowing all of our evaluated datasets to fit into the memory.

**Link**: [arxiv](http://arxiv.org/abs/2412.21023v2),  [pdf](http://arxiv.org/pdf/2412.21023v2)

**Tags**: cs.LG 



### Token Pruning for Caching Better: 9 Times Acceleration on Stable   Diffusion for Free
**Authors**: Evelyn Zhang, Bang Xiao, Jiayi Tang, Qianli Ma, Chang Zou, Xuefei Ning, Xuming Hu, Linfeng Zhang

**Updated**: 2024-12-31T09:56:40Z

**Summary**: Stable Diffusion has achieved remarkable success in the field of text-to-image generation, with its powerful generative capabilities and diverse generation results making a lasting impact. However, its iterative denoising introduces high computational costs and slows generation speed, limiting broader adoption. The community has made numerous efforts to reduce this computational burden, with methods like feature caching attracting attention due to their effectiveness and simplicity. Nonetheless, simply reusing features computed at previous timesteps causes the features across adjacent timesteps to become similar, reducing the dynamics of features over time and ultimately compromising the quality of generated images. In this paper, we introduce a dynamics-aware token pruning (DaTo) approach that addresses the limitations of feature caching. DaTo selectively prunes tokens with lower dynamics, allowing only high-dynamic tokens to participate in self-attention layers, thereby extending feature dynamics across timesteps. DaTo combines feature caching with token pruning in a training-free manner, achieving both temporal and token-wise information reuse. Applied to Stable Diffusion on the ImageNet, our approach delivered a 9$\times$ speedup while reducing FID by 0.33, indicating enhanced image quality. On the COCO-30k, we observed a 7$\times$ acceleration coupled with a notable FID reduction of 2.17.

**Link**: [arxiv](http://arxiv.org/abs/2501.00375v1),  [pdf](http://arxiv.org/pdf/2501.00375v1)

**Tags**: cs.CV cs.LG 



### RetrievalAttention: Accelerating Long-Context LLM Inference via Vector   Retrieval
**Authors**: Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu

**Updated**: 2024-12-31T07:11:00Z

**Summary**: Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference speed and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to build approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieve the most relevant ones through vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in the attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation demonstrates that RetrievalAttention achieves near full attention accuracy while only requiring access to 1--3% of the data. This leads to a significant reduction in the inference cost of long-context LLMs, with a much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.

**Link**: [arxiv](http://arxiv.org/abs/2409.10516v3),  [pdf](http://arxiv.org/pdf/2409.10516v3)

**Tags**: cs.LG cs.CL 



### Performant Automatic BLAS Offloading on Unified Memory Architecture with   OpenMP First-Touch Style Data Movement
**Authors**: Junjie Li

**Updated**: 2024-12-31T05:24:30Z

**Summary**: BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.

**Link**: [arxiv](http://arxiv.org/abs/2501.00279v1),  [pdf](http://arxiv.org/pdf/2501.00279v1)

**Tags**: cs.DC cs.MS cs.PF cs.SE 



### Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained   Image Recognition
**Authors**: Edwin Arkel Rios, Jansen Christopher Yuanda, Vincent Leon Ghanz, Cheng-Wei Yu, Bo-Cheng Lai, Min-Chun Hu

**Updated**: 2024-12-31T03:19:38Z

**Summary**: Ultra-fine-grained image recognition (UFGIR) is a challenging task that involves classifying images within a macro-category. While traditional FGIR deals with classifying different species, UFGIR goes beyond by classifying sub-categories within a species such as cultivars of a plant. In recent times the usage of Vision Transformer-based backbones has allowed methods to obtain outstanding recognition performances in this task but this comes at a significant cost in terms of computation specially since this task significantly benefits from incorporating higher resolution images. Therefore, techniques such as token reduction have emerged to reduce the computational cost. However, dropping tokens leads to loss of essential information for fine-grained categories, specially as the token keep rate is reduced. Therefore, to counteract the loss of information brought by the usage of token reduction we propose a novel Cross-Layer Aggregation Classification Head and a Cross-Layer Cache mechanism to recover and access information from previous layers in later locations. Extensive experiments covering more than 2000 runs across diverse settings including 5 datasets, 9 backbones, 7 token reduction methods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the proposed plug-and-play modules and allow us to push the boundaries of accuracy vs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to 10\% while maintaining a competitive accuracy to state-of-the-art models. Code is available at: \url{https://github.com/arkel23/CLCA}

**Link**: [arxiv](http://arxiv.org/abs/2501.00243v1),  [pdf](http://arxiv.org/pdf/2501.00243v1)

**Tags**: cs.CV I.2; I.4 



### MapQaTor: A System for Efficient Annotation of Map Query Datasets
**Authors**: Mahir Labib Dihan, Mohammed Eunus Ali, Md Rizwan Parvez

**Updated**: 2024-12-30T15:33:19Z

**Summary**: Mapping and navigation services like Google Maps, Apple Maps, Openstreet Maps, are essential for accessing various location-based data, yet they often struggle to handle natural language geospatial queries. Recent advancements in Large Language Models (LLMs) show promise in question answering (QA), but creating reliable geospatial QA datasets from map services remains challenging. We introduce MapQaTor, a web application that streamlines the creation of reproducible, traceable map-based QA datasets. With its plug-and-play architecture, MapQaTor enables seamless integration with any maps API, allowing users to gather and visualize data from diverse sources with minimal setup. By caching API responses, the platform ensures consistent ground truth, enhancing the reliability of the data even as real-world information evolves. MapQaTor centralizes data retrieval, annotation, and visualization within a single platform, offering a unique opportunity to evaluate the current state of LLM-based geospatial reasoning while advancing their capabilities for improved geospatial understanding. Evaluation metrics show that, MapQaTor speeds up the annotation process by at least 30 times compared to manual methods, underscoring its potential for developing geospatial resources, such as complex map reasoning datasets. The website is live at: https://mapqator.github.io/ and a demo video is available at: https://youtu.be/7_aV9Wmhs6Q.

**Link**: [arxiv](http://arxiv.org/abs/2412.21015v1),  [pdf](http://arxiv.org/pdf/2412.21015v1)

**Tags**: cs.CL cs.HC 



### SepLLM: Accelerate Large Language Models by Compressing One Segment into   One Separator
**Authors**: Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, Chao Huang

**Updated**: 2024-12-30T14:54:29Z

**Summary**: Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2412.12094v3),  [pdf](http://arxiv.org/pdf/2412.12094v3)

**Tags**: cs.CL cs.AI cs.LG 



### A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field
**Authors**: Wei Li, Hanbyul Kim, Xinbo Wang, Jianlin Luo, Simone Latini, Dongbin Shin, Jun-Ming Liu, Jing-Feng Li, Angel Rubio, Ce-Wen Nan, Qian Li

**Updated**: 2024-12-30T11:54:19Z

**Summary**: Coherent manipulation of lattice vibrations using ultrafast light pulses enables access to nonequilibrium 'hidden' phases with designed functionalities in quantum materials. However, expanding the understanding of nonlinear light-phonon interaction mechanisms remains crucial for developing new strategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3 driven by intense terahertz excitation. As the terahertz field increases, the system transitions from the quantum paraelectric (QPE) ground state to an intermediate ferroelectric phase, and then unexpectedly reverts to a QPE state above ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice dynamics compared to the initial phases, highlighting activated antiferrodistortive phonon modes. Aided by first-principles dynamical calculations, we identify the mechanism for these complex behaviors as a superposition of multiple coherently excited eigenstates of the polar soft mode. Our results reveal a previously uncharted quantum facet of SrTiO3 and open pathways for harnessing high-order excitations to engineer quantum materials in the ultrafast regime.

**Link**: [arxiv](http://arxiv.org/abs/2412.20887v1),  [pdf](http://arxiv.org/pdf/2412.20887v1)

**Tags**: cond-mat.mtrl-sci physics.optics 



### SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language   Models
**Authors**: Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar

**Updated**: 2024-12-30T05:01:44Z

**Summary**: Large language models (LLMs) have seen widespread adoption due to their remarkable performance across various applications, driving the accelerated development of a large number of diverse LLMs. However, these individual LLMs show limitations in generalization and performance on complex tasks due to inherent training biases, model size constraints, and the quality or diversity of pre-training datasets. A promising direction is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. To address these limitations, we introduce a novel LLM selection algorithm called SelectLLM, which efficiently directs input queries to the most suitable subset of LLMs from a large pool, ensuring that the selected models collectively provide accurate responses. SelectLLM employs a multi-label classifier and policy based on the classifier's predictions and confidence scores in selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings indicate that the proposed model outperforms existing ensemble-based baselines and achieves competitive performance with similarly sized top-performing LLMs while maintaining efficiency. Specifically, it achieves a huge reduction in inference latency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU, compared to the top-performing baselines. Also, we establish a theoretical upper bound by an oracle with LLMs and explore in-depth linguistic analysis to understand the performance gap between Oracle and SelectLLM.

**Link**: [arxiv](http://arxiv.org/abs/2408.08545v2),  [pdf](http://arxiv.org/pdf/2408.08545v2)

**Tags**: cs.CL 



### Align Attention Heads Before Merging Them: An Effective Way for   Converting MHA to GQA
**Authors**: Qingyun Jin, Xiaohui Song, Feng Zhou, Zengchang Qin

**Updated**: 2024-12-30T03:05:45Z

**Summary**: Large language models have been shown to perform well on a variety of natural language processing problems. However, as the model size and the input sequence's length increase, the rapid increase of KV Cache significantly slows down inference speed. Therefore GQA model, as an alternative to MHA model, has been widely introduced into LLMs. In this work, we propose a low-cost method for pruning MHA models into GQA models with any compression ratio of key-value heads. Our method is based on $\mathit{L_0}$ masks to gradually remove redundant parameters. In addition, we apply orthogonal transformations to attention heads without changing the model to increase similarity between attention heads before pruning training, in order to further improve performance of the model. Our method can be compatible with rotary position embedding (RoPE), which means the model after training can be fully adapted to the mainstream standard GQA framework. Experiments demonstrate that our strategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model without too much performance degradation, just achieved through supervised fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2412.20677v1),  [pdf](http://arxiv.org/pdf/2412.20677v1)

**Tags**: cs.CL 



### Dynamic Optimization of Storage Systems Using Reinforcement Learning   Techniques
**Authors**: Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao

**Updated**: 2024-12-29T17:41:40Z

**Summary**: The exponential growth of data-intensive applications has placed unprecedented demands on modern storage systems, necessitating dynamic and efficient optimization strategies. Traditional heuristics employed for storage performance optimization often fail to adapt to the variability and complexity of contemporary workloads, leading to significant performance bottlenecks and resource inefficiencies. To address these challenges, this paper introduces RL-Storage, a novel reinforcement learning (RL)-based framework designed to dynamically optimize storage system configurations. RL-Storage leverages deep Q-learning algorithms to continuously learn from real-time I/O patterns and predict optimal storage parameters, such as cache size, queue depths, and readahead settings[1]. The proposed framework operates within the storage kernel, ensuring minimal latency and low computational overhead. Through an adaptive feedback mechanism, RL-Storage dynamically adjusts critical parameters, achieving efficient resource utilization across a wide range of workloads. Experimental evaluations conducted on a range of benchmarks, including RocksDB and PostgreSQL, demonstrate significant improvements, with throughput gains of up to 2.6x and latency reductions of 43% compared to baseline heuristics. Additionally, RL-Storage achieves these performance enhancements with a negligible CPU overhead of 0.11% and a memory footprint of only 5 KB, making it suitable for seamless deployment in production environments. This work underscores the transformative potential of reinforcement learning techniques in addressing the dynamic nature of modern storage systems. By autonomously adapting to workload variations in real time, RL-Storage provides a robust and scalable solution for optimizing storage performance, paving the way for next-generation intelligent storage infrastructures.

**Link**: [arxiv](http://arxiv.org/abs/2501.00068v1),  [pdf](http://arxiv.org/pdf/2501.00068v1)

**Tags**: cs.OS cs.DC cs.LG 



### Ns3 meets Sionna: Using Realistic Channels in Network Simulation
**Authors**: Anatolij Zubow, Yannik Pilz, Sascha RÃ¶sler, Falko Dressler

**Updated**: 2024-12-29T17:18:21Z

**Summary**: Network simulators are indispensable tools for the advancement of wireless network technologies, offering a cost-effective and controlled environment to simulate real-world network behavior. However, traditional simulators, such as the widely used ns-3, exhibit limitations in accurately modeling indoor and outdoor scenarios due to their reliance on simplified statistical and stochastic channel propagation models, which often fail to accurately capture physical phenomena like multipath signal propagation and shadowing by obstacles in the line-of-sight path. We present Ns3Sionna, which integrates a ray tracing-based channel model, implemented using the Sionna RT framework, within the ns-3 network simulator. It allows to simulate environment-specific and physically accurate channel realizations for a given 3D scene and wireless device positions. Additionally, a mobility model based on ray tracing was developed to accurately represent device movements within the simulated 3D space. Ns3Sionna provides more realistic path and delay loss estimates for both indoor and outdoor environments than existing ns-3 propagation models, particularly in terms of spatial and temporal correlation. Moreover, fine-grained channel state information is provided, which could be used for the development of sensing applications. Due to the significant computational demands of ray tracing, Ns3Sionna takes advantage of the parallel execution capabilities of modern GPUs and multi-core CPUs by incorporating intelligent pre-caching mechanisms that leverage the channel's coherence time to optimize runtime performance. This enables the efficient simulation of scenarios with a small to medium number of mobile nodes.

**Link**: [arxiv](http://arxiv.org/abs/2412.20524v1),  [pdf](http://arxiv.org/pdf/2412.20524v1)

**Tags**: cs.NI 



### Revisiting Cache Freshness for Emerging Real-Time Applications
**Authors**: Ziming Mao, Rishabh Iyer, Scott Shenker, Ion Stoica

**Updated**: 2024-12-28T17:17:03Z

**Summary**: Caching is widely used in industry to improve application performance by reducing data-access latency and taking the load off the backend infrastructure. TTLs have become the de-facto mechanism used to keep cached data reasonably fresh (i.e., not too out of date with the backend). However, the emergence of real-time applications requires tighter data freshness, which is impractical to achieve with TTLs. We discuss why this is the case, and propose a simple yet effective adaptive policy to achieve the desired freshness.

**Link**: [arxiv](http://arxiv.org/abs/2412.20221v1),  [pdf](http://arxiv.org/pdf/2412.20221v1)

**Tags**: cs.OS cs.DC cs.NI 



### ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal   Visual Token Trimming
**Authors**: Jiedong Zhuang, Lu Lu, Ming Dai, Rui Hu, Jian Chen, Qiang Liu, Haoji Hu

**Updated**: 2024-12-28T10:17:29Z

**Summary**: Multimodal large language models (MLLMs) enhance their perceptual capabilities by integrating visual and textual information. However, processing the massive number of visual tokens incurs a significant computational cost. Existing analysis of the MLLM attention mechanisms remains shallow, leading to coarse-grain token pruning strategies that fail to effectively balance speed and accuracy. In this paper, we conduct a comprehensive investigation of MLLM attention mechanisms with LLaVA. We find that numerous visual tokens and partial attention computations are redundant during the decoding process. Based on this insight, we propose Spatial-Temporal Visual Token Trimming ($\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without retraining. $\textbf{ST}^{3}$ consists of two primary components: 1) Progressive Visual Token Pruning (\textbf{PVTP}), which eliminates inattentive visual tokens across layers, and 2) Visual Token Annealing (\textbf{VTA}), which dynamically reduces the number of visual tokens in each layer as the generated tokens grow. Together, these techniques deliver around $\mathbf{2\times}$ faster inference with only about $\mathbf{30\%}$ KV cache memory compared to the original LLaVA, while maintaining consistent performance across various datasets. Crucially, $\textbf{ST}^{3}$ can be seamlessly integrated into existing pre-trained MLLMs, providing a plug-and-play solution for efficient inference.

**Link**: [arxiv](http://arxiv.org/abs/2412.20105v1),  [pdf](http://arxiv.org/pdf/2412.20105v1)

**Tags**: cs.CV 



### A Robust Federated Learning Framework for Undependable Devices at Scale
**Authors**: Shilong Wang, Jianchun Liu, Hongli Xu, Chunming Qiao, Huarong Deng, Qiuye Zheng, Jiantao Gong

**Updated**: 2024-12-28T03:28:52Z

**Summary**: In a federated learning (FL) system, many devices, such as smartphones, are often undependable (e.g., frequently disconnected from WiFi) during training. Existing FL frameworks always assume a dependable environment and exclude undependable devices from training, leading to poor model performance and resource wastage. In this paper, we propose FLUDE to effectively deal with undependable environments. First, FLUDE assesses the dependability of devices based on the probability distribution of their historical behaviors (e.g., the likelihood of successfully completing training). Based on this assessment, FLUDE adaptively selects devices with high dependability for training. To mitigate resource wastage during the training phase, FLUDE maintains a model cache on each device, aiming to preserve the latest training state for later use in case local training on an undependable device is interrupted. Moreover, FLUDE proposes a staleness-aware strategy to judiciously distribute the global model to a subset of devices, thus significantly reducing resource wastage while maintaining model performance. We have implemented FLUDE on two physical platforms with 120 smartphones and NVIDIA Jetson devices. Extensive experimental results demonstrate that FLUDE can effectively improve model performance and resource efficiency of FL training in undependable environments.

**Link**: [arxiv](http://arxiv.org/abs/2412.19991v1),  [pdf](http://arxiv.org/pdf/2412.19991v1)

**Tags**: cs.LG cs.DC 



### Performance Characterization and Optimizations of Traditional ML   Applications
**Authors**: Harsh Kumar, R. Govindarajan

**Updated**: 2024-12-26T04:13:52Z

**Summary**: Even in the era of Deep Learning based methods, traditional machine learning methods with large data sets continue to attract significant attention. However, we find an apparent lack of a detailed performance characterization of these methods in the context of large training datasets. In this work, we study the system's behavior of a number of traditional ML methods as implemented in popular free software libraries/modules to identify critical performance bottlenecks experienced by these applications. The performance characterization study reveals several interesting insights on the performance of these applications. Then we evaluate the performance benefits of applying some well-known optimizations at the levels of caches and the main memory. More specifically, we test the usefulness of optimizations such as (i) software prefetching to improve cache performance and (ii) data layout and computation reordering optimizations to improve locality in DRAM accesses. These optimizations are implemented as modifications to the well-known scikit-learn library, and hence can be easily leveraged by application programmers. We evaluate the impact of the proposed optimizations using a combination of simulation and execution on a real system. The software prefetching optimization results in performance benefits varying from 5.2%-27.1% on different ML applications while the data layout and computation reordering approaches yield 6.16%-28.0% performance improvement.

**Link**: [arxiv](http://arxiv.org/abs/2412.19051v1),  [pdf](http://arxiv.org/pdf/2412.19051v1)

**Tags**: cs.PF 



### XRFlux: Virtual Reality Benchmark for Edge Caching Systems
**Authors**: Nader Alfares, George Kesidis

**Updated**: 2024-12-25T18:36:21Z

**Summary**: We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality (VR) delivery systems using edge-cloud caching. As VR applications and systems progress, the need to meet strict latency and Quality of Experience (QoE) requirements is increasingly evident. In the context of VR, traditional cloud architectures (e.g., remote AWS S3 for content delivery) often struggle to meet these demands, especially for users of the same application in different locations. With edge computing, resources are brought closer to users in efforts to reduce latency and improve QoEs. However, VR's dynamic nature, with changing fields of view (FoVs) and user synchronization requirements, creates various challenges for edge caching. We address the lack of suitable benchmarks and propose a framework that simulates multiuser VR scenarios while logging users' interaction with objects within their actual and predicted FoVs. The benchmark's activity log can then be played back through an edge cache to assess the resulting QoEs. This tool fills a gap by supporting research in the optimization of edge caching (and other edge-cloud functions) for VR streaming.

**Link**: [arxiv](http://arxiv.org/abs/2412.18960v1),  [pdf](http://arxiv.org/pdf/2412.18960v1)

**Tags**: cs.PF cs.MM 



### Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With   Structured Memories
**Authors**: Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel

**Updated**: 2024-12-25T14:14:31Z

**Summary**: Long-range tasks require reasoning over long inputs. Existing solutions either need large compute budgets, training data, access to model weights, or use complex, task-specific approaches. We present PRISM, which alleviates these concerns by processing information as a stream of chunks, maintaining a structured in-context memory specified by a typed hierarchy schema. This approach demonstrates superior performance to baselines on diverse tasks while using at least 4x smaller contexts than long-context models. Moreover, PRISM is token-efficient. By producing short outputs and efficiently leveraging key-value (KV) caches, it achieves up to 54% cost reduction when compared to alternative short-context approaches. The method also scales down to tiny information chunks (e.g., 500 tokens) without increasing the number of tokens encoded or sacrificing quality. Furthermore, we show that it is possible to generate schemas to generalize our approach to new tasks with minimal effort.

**Link**: [arxiv](http://arxiv.org/abs/2412.18914v1),  [pdf](http://arxiv.org/pdf/2412.18914v1)

**Tags**: cs.AI 



### Accelerating Diffusion Transformers with Dual Feature Caching
**Authors**: Chang Zou, Evelyn Zhang, Runlin Guo, Haohang Xu, Conghui He, Xuming Hu, Linfeng Zhang

**Updated**: 2024-12-25T14:00:14Z

**Summary**: Diffusion Transformers (DiT) have become the dominant methods in image and video generation yet still suffer substantial computational costs. As an effective approach for DiT acceleration, feature caching methods are designed to cache the features of DiT in previous timesteps and reuse them in the next timesteps, allowing us to skip the computation in the next timesteps. However, on the one hand, aggressively reusing all the features cached in previous timesteps leads to a severe drop in generation quality. On the other hand, conservatively caching only the features in the redundant layers or tokens but still computing the important ones successfully preserves the generation quality but results in reductions in acceleration ratios. Observing such a tradeoff between generation quality and acceleration performance, this paper begins by quantitatively studying the accumulated error from cached features. Surprisingly, we find that aggressive caching does not introduce significantly more caching errors in the caching step, and the conservative feature caching can fix the error introduced by aggressive caching. Thereby, we propose a dual caching strategy that adopts aggressive and conservative caching iteratively, leading to significant acceleration and high generation quality at the same time. Besides, we further introduce a V-caching strategy for token-wise conservative caching, which is compatible with flash attention and requires no training and calibration data.   Our codes have been released in Github: \textbf{Code: \href{https://github.com/Shenyi-Z/DuCa}{\texttt{\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}

**Link**: [arxiv](http://arxiv.org/abs/2412.18911v1),  [pdf](http://arxiv.org/pdf/2412.18911v1)

**Tags**: cs.LG cs.AI cs.CV 



### Aspect-oriented Programming with Julia
**Authors**: Osamu Ishimura, Yoshihide Yoshimoto

**Updated**: 2024-12-25T11:59:17Z

**Summary**: This paper proposes integrating Aspect-oriented Programming (AOP) into Julia, a language widely used in scientific and High-Performance Computing (HPC). AOP enhances software modularity by encapsulating cross-cutting concerns, such as logging, caching, and parallelizing, into separate, reusable aspects. Leveraging Julia's powerful metaprogramming and abstract syntax tree (AST) manipulation capabilities, we introduce AspectJulia, an AOP framework designed to operate within Julia's runtime environment as a package. AspectJulia enables developers to define and apply aspects seamlessly, leading to more modular, maintainable, and adaptable code. We detail the implementation of AspectJulia and present diverse use cases, ranging from HPC and scientific computing to business applications, demonstrating its effectiveness in managing cross-cutting concerns. This integration simplifies application development and improves the adaptability of existing Julia modules and packages, paving the way for more efficient and maintainable software systems.

**Link**: [arxiv](http://arxiv.org/abs/2412.18885v1),  [pdf](http://arxiv.org/pdf/2412.18885v1)

**Tags**: cs.PL 



### Efficiently serving large multimedia models using EPD Disaggregation
**Authors**: Gursimran Singh, Xinglu Wang, Ivan Hu, Timothy Yu, Linzi Xing, Wei Jiang, Zhefeng Wang, Xiaolong Bai, Yi Li, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2024-12-25T10:11:31Z

**Summary**: Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by handling diverse inputs such as images, audio, and video, but at the cost of adding a multimodal encoding stage that increases both computational and memory overhead. This step helps convert raw inputs into tokenized representations that inflate the token sequence for the prefill phase, negatively impacting key Service Level Objectives (SLOs) like time to first token (TTFT) and end-to-end throughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates the encoding, prefill, and decode stages onto dedicated resources. Unlike current systems, which bundle encoding and prefill together, our disaggregation approach alleviates memory bottlenecks, mitigates synchronization delays, and supports flexible batching. Specifically, we employ a new caching mechanism for multimodal tokens, enabling asynchronous transfer of multimodal tokens and introduce an integrated module to find optimal config for EPD system and minimize resource usage while maximizing SLO-based performance metric. Experimental evaluations with popular LMMs show substantial gains in memory efficiency (up to 15$\times$ lesser for encoding-stage GPUs), that supports upto 22$\times$ higher batch sizes, 10$\times$ more number of images/ request, 2.2$\times$ higher kv cache size. Further, it leads to significant improvements in end-to-end throughput (up to 57\% better), and latency metrics (TTFT up to 71\% lower), compared to systems that do not disaggregate. Our findings underscore the potential of EPD disaggregation to enable resource-efficient and high-performance multimodal inference at scale.

**Link**: [arxiv](http://arxiv.org/abs/2501.05460v1),  [pdf](http://arxiv.org/pdf/2501.05460v1)

**Tags**: cs.DC cs.AI cs.CV cs.LG 



### HashEvict: A Pre-Attention KV Cache Eviction Strategy using   Locality-Sensitive Hashing
**Authors**: Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Brian Gravelle, Furong Huang, Cornelia FermÃ¼ller, Yiannis Aloimonos

**Updated**: 2024-12-24T13:04:45Z

**Summary**: Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.16187v2),  [pdf](http://arxiv.org/pdf/2412.16187v2)

**Tags**: cs.LG cs.AI cs.CL cs.DS cs.PF 



### Development and Application of a Decentralized Domain Name Service
**Authors**: Guang Yang

**Updated**: 2024-12-24T00:46:00Z

**Summary**: The current Domain Name System (DNS), as a core infrastructure of the internet, exhibits several shortcomings: its centralized architecture leads to censorship risks and single points of failure, making domain name resolution vulnerable to attacks. The lack of encryption in the resolution process exposes it to DNS hijacking and cache poisoning attacks. Additionally, the high operational costs limit participation and innovation among small to medium-sized users. To address these issues, this paper proposes a Decentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and distributed storage (IPFS). By leveraging the immutability of blockchain and the content verification of IPFS, the system achieves decentralized storage and distribution of domain name records, eliminating the centralized dependencies of traditional DNS. With a block time of 15 seconds, the system supports rapid broadcasting of domain name updates, significantly improving resolution efficiency. The DDNS aims to serve as a complement or backup to the existing DNS system, providing a pollution-resistant, censorship-resistant, high-performance, and low-cost domain name resolution solution, offering a new technical path for the security and stability of the internet.

**Link**: [arxiv](http://arxiv.org/abs/2412.01959v2),  [pdf](http://arxiv.org/pdf/2412.01959v2)

**Tags**: cs.NI 



### Deliberation in Latent Space via Differentiable Cache Augmentation
**Authors**: Luyang Liu, Jonas Pfeiffer, Jiaxing Wu, Jun Xie, Arthur Szlam

**Updated**: 2024-12-23T18:02:25Z

**Summary**: Techniques enabling large language models (LLMs) to "think more" by generating and attending to intermediate reasoning steps have shown promise in solving complex problems. However, the standard approaches generate sequences of discrete tokens immediately before responding, and so they can incur significant latency costs and be challenging to optimize. In this work, we demonstrate that a frozen LLM can be augmented with an offline coprocessor that operates on the model's key-value (kv) cache. This coprocessor augments the cache with a set of latent embeddings designed to improve the fidelity of subsequent decoding. We train this coprocessor using the language modeling loss from the decoder on standard pretraining data, while keeping the decoder itself frozen. This approach enables the model to learn, in an end-to-end differentiable fashion, how to distill additional computation into its kv-cache. Because the decoder remains unchanged, the coprocessor can operate offline and asynchronously, and the language model can function normally if the coprocessor is unavailable or if a given cache is deemed not to require extra computation. We show experimentally that when a cache is augmented, the decoder achieves lower perplexity on numerous subsequent tokens. Furthermore, even without any task-specific training, our experiments demonstrate that cache augmentation consistently reduces perplexity and improves performance across a range of reasoning-intensive tasks.

**Link**: [arxiv](http://arxiv.org/abs/2412.17747v1),  [pdf](http://arxiv.org/pdf/2412.17747v1)

**Tags**: cs.CL cs.AI cs.LG 



### A Reproducible Method for Mapping Electricity Transmission   Infrastructure for Space Weather Risk Assessment
**Authors**: Edward J. Oughton, Evan Alexander Peters, Dennies Bor, Noah Rivera, C. Trevor Gaunt, Robert Weigel

**Updated**: 2024-12-23T16:11:18Z

**Summary**: Space weather impact assessment is constrained by the lack of available asset information to undertake modeling of Geomagnetically Induced Currents (GICs) in Extra High Voltage electricity infrastructure networks. The U.S. National Space Weather Strategy and Action Plan identifies underutilized data as a central issue for improving risk assessment, motivating this research. Accurate GIC prediction is generally not possible without information on the electrical circuit, therefore we define a reproducible method based on open-source data, which enables risk analysts to collect their own substation component data. This process converts OpenStreetMap (OSM) substation locations to high-resolution, component-level mapping of electricity transmission assets by utilizing an innovative web-browser platform to facilitate component annotation. As a case study example, we convert an initial 1,313 high-voltage (>115 kV) substations to 52,273 substation components via Google Earth APIs utilizing low-altitude, satellite, and Streetview imagery. We find that a total of 41,642 substation components (79.6%) connect to the highest substation voltage levels (>345 kV) and are possibly susceptible to GIC, with a total of 7,949 transformers identified. Compared to the initial OSM baseline, we provide new detailed insights on voltage levels, line capacities, and substation configurations. Two validation workshops were undertaken to align the method and data with GIC assessment needs. The approach ensures consistency and rapid scalability, enabling users to quickly count components via a flexible web-browser application.

**Link**: [arxiv](http://arxiv.org/abs/2412.17685v1),  [pdf](http://arxiv.org/pdf/2412.17685v1)

**Tags**: physics.geo-ph cs.SY eess.SY 



### Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT   in SAGIN
**Authors**: Qian Chen, Chenyu Wu, Shuai Han, Weixiao Meng, Tony Q. S. Quek

**Updated**: 2024-12-23T14:40:26Z

**Summary**: The rapid development of the aviation Internet of Things (IoT) has positioned in-flight connectivity (IFC) as one of its critical applications. Space-air-ground integrated networks (SAGIN) are essential for ensuring the performance of IFC by enabling seamless and reliable connectivity. However, most existing research treats satellites merely as transparent forwarding nodes and overlooks their potential caching capabilities to enhance IFC data rates. In this article, we explore an IFC-oriented SAGIN where satellites and ground stations (GSs) work together to transmit content to airborne passengers, thereby facilitating airborne communication. By categorizing files into cached (instantly accessible via satellites) and non-cached files (available only through GSs), this article pioneers the integration of multiple inter-satellite links (ISLs) into the IFC framework, thus innovating the content delivery process for both types of files. To minimize the average delay of content delivery, we formulate the corresponding optimization problems: 1) For cached files, we propose an exact penalty-based method to determine the satellite association scheme. 2) For non-cached files, we present an efficient algorithm based on alternating optimization to jointly optimize satellite association and GS bandwidth allocation. Our proposed framework is low in complexity, paving the way for high-speed Internet connectivity for aviation passengers. Finally, simulation results are provided to demonstrate the effectiveness of our proposed IFC framework for SAGIN.

**Link**: [arxiv](http://arxiv.org/abs/2405.18919v2),  [pdf](http://arxiv.org/pdf/2405.18919v2)

**Tags**: cs.IT math.IT 



## Keyword: LLM Inference 
 ### Continuous 3D Perception Model with Persistent State
**Authors**: Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, Angjoo Kanazawa

**Updated**: 2025-01-21T18:59:23Z

**Summary**: We present a unified framework capable of solving a broad range of 3D tasks. Our approach features a stateful recurrent model that continuously updates its state representation with each new observation. Given a stream of images, this evolving state can be used to generate metric-scale pointmaps (per-pixel 3D points) for each new input in an online fashion. These pointmaps reside within a common coordinate system, and can be accumulated into a coherent, dense scene reconstruction that updates as new images arrive. Our model, called CUT3R (Continuous Updating Transformer for 3D Reconstruction), captures rich priors of real-world scenes: not only can it predict accurate pointmaps from image observations, but it can also infer unseen regions of the scene by probing at virtual, unobserved views. Our method is simple yet highly flexible, naturally accepting varying lengths of images that may be either video streams or unordered photo collections, containing both static and dynamic content. We evaluate our method on various 3D/4D tasks and demonstrate competitive or state-of-the-art performance in each. Project Page: https://cut3r.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2501.12387v1),  [pdf](http://arxiv.org/pdf/2501.12387v1)

**Tags**: cs.CV 



### Video Depth Anything: Consistent Depth Estimation for Super-Long Videos
**Authors**: Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, Bingyi Kang

**Updated**: 2025-01-21T18:53:30Z

**Summary**: Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (< 10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS.

**Link**: [arxiv](http://arxiv.org/abs/2501.12375v1),  [pdf](http://arxiv.org/pdf/2501.12375v1)

**Tags**: cs.CV cs.AI 



### Is Long Context All You Need? Leveraging LLM's Extended Context for   NL2SQL
**Authors**: Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan

**Updated**: 2025-01-21T18:52:15Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.   In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve a strong performance with 67.41\% on BIRD benchmark (dev) without finetuning and expensive self-consistency based techniques.

**Link**: [arxiv](http://arxiv.org/abs/2501.12372v1),  [pdf](http://arxiv.org/pdf/2501.12372v1)

**Tags**: cs.DB cs.AI 



### Diffusion-aware Censored Gaussian Processes for Demand Modelling
**Authors**: Filipe Rodrigues

**Updated**: 2025-01-21T18:33:08Z

**Summary**: Inferring the true demand for a product or a service from aggregate data is often challenging due to the limited available supply, thus resulting in observations that are censored and correspond to the realized demand, thereby not accounting for the unsatisfied demand. Censored regression models are able to account for the effect of censoring due to the limited supply, but they don't consider the effect of substitutions, which may cause the demand for similar alternative products or services to increase. This paper proposes Diffusion-aware Censored Demand Models, which combine a Tobit likelihood with a graph diffusion process in order to model the latent process of transfer of unsatisfied demand between similar products or services. We instantiate this new class of models under the framework of GPs and, based on both simulated and real-world data for modeling sales, bike-sharing demand, and EV charging demand, demonstrate its ability to better recover the true demand and produce more accurate out-of-sample predictions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12354v1),  [pdf](http://arxiv.org/pdf/2501.12354v1)

**Tags**: cs.LG stat.ME stat.ML 



### Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated   Tokens
**Authors**: Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhari, Huzefa Rangwala, George Karypis, Rasool Fakoor

**Updated**: 2025-01-21T18:14:30Z

**Summary**: Language models are often trained to maximize the likelihood of the next token given past tokens in the training dataset. However, during inference time, they are utilized differently, generating text sequentially and auto-regressively by using previously generated tokens as input to predict the next one. Marginal differences in predictions at each step can cascade over successive steps, resulting in different distributions from what the models were trained for and potentially leading to unpredictable behavior. This paper proposes two simple approaches based on model own generation to address this discrepancy between the training and inference time. Our first approach is Batch-Scheduled Sampling, where, during training, we stochastically choose between the ground-truth token from the dataset and the model's own generated token as input to predict the next token. This is done in an offline manner, modifying the context window by interleaving ground-truth tokens with those generated by the model. Our second approach is Reference-Answer-based Correction, where we explicitly incorporate a self-correction capability into the model during training. This enables the model to effectively self-correct the gaps between the generated sequences and the ground truth data without relying on an external oracle model. By incorporating our proposed strategies during training, we have observed an overall improvement in performance compared to baseline methods, as demonstrated by our extensive experiments using summarization, general question-answering, and math question-answering tasks.

**Link**: [arxiv](http://arxiv.org/abs/2410.14655v2),  [pdf](http://arxiv.org/pdf/2410.14655v2)

**Tags**: cs.LG 



### Treefix: Enabling Execution with a Tree of Prefixes
**Authors**: Beatriz Souza, Michael Pradel

**Updated**: 2025-01-21T18:13:43Z

**Summary**: The ability to execute code is a prerequisite for various dynamic program analyses. Learning-guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables. Although state-of-the-art learning-guided execution approaches, such as LExecutor, can enable the execution of a relative high amount of code, they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code. This paper presents Treefix, a novel learning-guided execution approach that leverages LLMs to iteratively create code prefixes that enable the execution of a given code snippet. The approach addresses the problem in a multi-step fashion, where each step uses feedback about the code snippet and its execution to instruct an LLM to improve a previously generated prefix. This process iteratively creates a tree of prefixes, a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet. In our experiments with two datasets of Python code snippets, Treefix achieves 25% and 7% more coverage relative to the current state of the art in learning-guided execution, covering a total of 84% and 82% of all lines in the code snippets.

**Link**: [arxiv](http://arxiv.org/abs/2501.12339v1),  [pdf](http://arxiv.org/pdf/2501.12339v1)

**Tags**: cs.SE cs.AI 



### Automatic Labelling with Open-source LLMs using Dynamic Label Schema   Integration
**Authors**: Thomas Walshe, Sae Young Moon, Chunyang Xiao, Yawwani Gunawardana, Fran Silavong

**Updated**: 2025-01-21T18:06:54Z

**Summary**: Acquiring labelled training data remains a costly task in real world machine learning projects to meet quantity and quality requirements. Recently Large Language Models (LLMs), notably GPT-4, have shown great promises in labelling data with high accuracy. However, privacy and cost concerns prevent the ubiquitous use of GPT-4. In this work, we explore effectively leveraging open-source models for automatic labelling. We identify integrating label schema as a promising technology but found that naively using the label description for classification leads to poor performance on high cardinality tasks. To address this, we propose Retrieval Augmented Classification (RAC) for which LLM performs inferences for one label at a time using corresponding label schema; we start with the most related label and iterates until a label is chosen by the LLM. We show that our method, which dynamically integrates label description, leads to performance improvements in labelling tasks. We further show that by focusing only on the most promising labels, RAC can trade off between label quality and coverage - a property we leverage to automatically label our internal datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.12332v1),  [pdf](http://arxiv.org/pdf/2501.12332v1)

**Tags**: cs.CL cs.AI cs.LG 



### Fully quantum inflation: quantum marginal problem constraints in the   service of causal inference
**Authors**: Isaac D. Smith, Elie Wolfe, Robert W. Spekkens

**Updated**: 2025-01-21T17:40:44Z

**Summary**: Consider the problem of deciding, for a particular multipartite quantum state, whether or not it is realizable in a quantum network with a particular causal structure. This is a fully quantum version of what causal inference researchers refer to as the problem of causal discovery. In this work, we introduce a fully quantum version of the inflation technique for causal inference, which leverages the quantum marginal problem. We illustrate the utility of this method using a simple example: testing compatibility of tripartite quantum states with the quantum network known as the triangle scenario. We show, in particular, how the method yields a complete classification of pure three-qubit states into those that are and those that are not compatible with the triangle scenario. We also provide some illustrative examples involving mixed states and some where one or more of the systems is higher-dimensional. Finally, we examine the question of when the incompatibility of a multipartite quantum state with a causal structure can be inferred from the incompatibility of a joint probability distribution induced by implementing measurements on each subsystem.

**Link**: [arxiv](http://arxiv.org/abs/2501.12320v1),  [pdf](http://arxiv.org/pdf/2501.12320v1)

**Tags**: quant-ph 



### Uncertainty Quantification With Noise Injection in Neural Networks: A   Bayesian Perspective
**Authors**: Xueqiong Yuan, Jipeng Li, Ercan Engin Kuruoglu

**Updated**: 2025-01-21T17:28:52Z

**Summary**: Model uncertainty quantification involves measuring and evaluating the uncertainty linked to a model's predictions, helping assess their reliability and confidence. Noise injection is a technique used to enhance the robustness of neural networks by introducing randomness. In this paper, we establish a connection between noise injection and uncertainty quantification from a Bayesian standpoint. We theoretically demonstrate that injecting noise into the weights of a neural network is equivalent to Bayesian inference on a deep Gaussian process. Consequently, we introduce a Monte Carlo Noise Injection (MCNI) method, which involves injecting noise into the parameters during training and performing multiple forward propagations during inference to estimate the uncertainty of the prediction. Through simulation and experiments on regression and classification tasks, our method demonstrates superior performance compared to the baseline model.

**Link**: [arxiv](http://arxiv.org/abs/2501.12314v1),  [pdf](http://arxiv.org/pdf/2501.12314v1)

**Tags**: stat.ML cs.LG 



### A recent evaluation on the performance of LLMs on radiation oncology   physics using questions of randomly shuffled options
**Authors**: Peilong Wang, Jason Holmes, Zhengliang Liu, Dequan Chen, Tianming Liu, Jiajian Shen, Wei Liu

**Updated**: 2025-01-21T17:20:31Z

**Summary**: Purpose: We present an updated study evaluating the performance of large language models (LLMs) in answering radiation oncology physics questions, focusing on the recently released models.   Methods: A set of 100 multiple-choice radiation oncology physics questions, previously created by a well-experienced physicist, was used for this study. The answer options of the questions were randomly shuffled to create "new" exam sets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet -- with the versions released before September 30, 2024, were queried using these new exam sets. To evaluate their deductive reasoning ability, the correct answer options in the questions were replaced with "None of the above." Then, the explain-first and step-by-step instruction prompts were used to test if this strategy improved their reasoning ability. The performance of the LLMs was compared with the answers from medical physicists.   Results: All models demonstrated expert-level performance on these questions, with o1-preview even surpassing medical physicists with a majority vote. When replacing the correct answer options with 'None of the above', all models exhibited a considerable decline in performance, suggesting room for improvement. The explain-first and step-by-step instruction prompts helped enhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet models.   Conclusion: These recently released LLMs demonstrated expert-level performance in answering radiation oncology physics questions, exhibiting great potential to assist in radiation oncology physics education and training.

**Link**: [arxiv](http://arxiv.org/abs/2412.10622v3),  [pdf](http://arxiv.org/pdf/2412.10622v3)

**Tags**: physics.med-ph cs.AI 



### LLM-Assisted Knowledge Graph Completion for Curriculum and Domain   Modelling in Personalized Higher Education Recommendations
**Authors**: Hasan Abu-Rasheed, Constance Jumbo, Rashed Al Amin, Christian Weber, Veit Wiese, Roman Obermaisser, Madjid Fathi

**Updated**: 2025-01-21T17:13:13Z

**Summary**: While learning personalization offers great potential for learners, modern practices in higher education require a deeper consideration of domain models and learning contexts, to develop effective personalization algorithms. This paper introduces an innovative approach to higher education curriculum modelling that utilizes large language models (LLMs) for knowledge graph (KG) completion, with the goal of creating personalized learning-path recommendations. Our research focuses on modelling university subjects and linking their topics to corresponding domain models, enabling the integration of learning modules from different faculties and institutions in the student's learning path. Central to our approach is a collaborative process, where LLMs assist human experts in extracting high-quality, fine-grained topics from lecture materials. We develop a domain, curriculum, and user models for university modules and stakeholders. We implement this model to create the KG from two study modules: Embedded Systems and Development of Embedded Systems Using FPGA. The resulting KG structures the curriculum and links it to the domain models. We evaluate our approach through qualitative expert feedback and quantitative graph quality metrics. Domain experts validated the relevance and accuracy of the model, while the graph quality metrics measured the structural properties of our KG. Our results show that the LLM-assisted graph completion approach enhances the ability to connect related courses across disciplines to personalize the learning experience. Expert feedback also showed high acceptance of the proposed collaborative approach for concept extraction and classification.

**Link**: [arxiv](http://arxiv.org/abs/2501.12300v1),  [pdf](http://arxiv.org/pdf/2501.12300v1)

**Tags**: cs.HC cs.AI 



### Towards spectral descriptions of cyclic functions
**Authors**: Miguel Monsalve, Daniel Seco

**Updated**: 2025-01-21T17:07:22Z

**Summary**: We build on a characterization of inner functions $f$ due to Le, in terms of the spectral properties of the operator $V=M_f^*M_f$ and study to what extent the cyclicity on weighted Hardy spaces $H^2_\omega$ of the function $z \mapsto a-z$ can be inferred from the spectral properties of analogous operators $V_a$. We describe several properties of the spectra that hold in a large class of spaces and then, we focus on the particular case of Bergman-type spaces, for which we describe completely the spectrum of such operators and find all eigenfunctions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12298v1),  [pdf](http://arxiv.org/pdf/2501.12298v1)

**Tags**: math.FA math.CA math.CV Primary: 47B32, Secondary: 30J05, 47A10, 47A16 



### Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and   Refinement
**Authors**: Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen

**Updated**: 2025-01-21T16:44:12Z

**Summary**: The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.

**Link**: [arxiv](http://arxiv.org/abs/2501.12273v1),  [pdf](http://arxiv.org/pdf/2501.12273v1)

**Tags**: cs.CL cs.AI 



### Leveraging Explicit Reasoning for Inference Integration in   Commonsense-Augmented Dialogue Models
**Authors**: Sarah E. Finch, Jinho D. Choi

**Updated**: 2025-01-21T16:40:55Z

**Summary**: Open-domain dialogue systems need to grasp social commonsense to understand and respond effectively to human users. Commonsense-augmented dialogue models have been proposed that aim to infer commonsense knowledge from dialogue contexts in order to improve response quality. However, existing approaches to commonsense-augmented dialogue rely on implicit reasoning to integrate commonsense inferences during response generation. In this study, we explore the impact of explicit reasoning against implicit reasoning over commonsense for dialogue response generation. Our findings demonstrate that separating commonsense reasoning into explicit steps for generating, selecting, and integrating commonsense into responses leads to better dialogue interactions, improving naturalness, engagement, specificity, and overall quality. Subsequent analyses of these findings unveil insights into the effectiveness of various types of commonsense in generating responses and the particular response traits enhanced through explicit reasoning for commonsense integration. Our work advances research in open-domain dialogue by achieving a new state-of-the-art in commonsense-augmented response generation.

**Link**: [arxiv](http://arxiv.org/abs/2406.09138v2),  [pdf](http://arxiv.org/pdf/2406.09138v2)

**Tags**: cs.CL 



### Hire Me or Not? Examining Language Model's Behavior with Occupation   Attributes
**Authors**: Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz

**Updated**: 2025-01-21T16:15:37Z

**Summary**: With the impressive performance in various downstream tasks, large language models (LLMs) have been widely integrated into production pipelines, like recruitment and recommendation systems. A known issue of models trained on natural language data is the presence of human biases, which can impact the fairness of the system. This paper investigates LLMs' behavior with respect to gender stereotypes, in the context of occupation decision making. Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering. Inspired by prior works, we construct a dataset by leveraging a standard occupation classification knowledge base released by authoritative agencies. We tested three LLMs (RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models exhibit gender stereotypes analogous to human biases, but with different preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may imply the current alignment methods are insufficient for debiasing and could introduce new biases contradicting the traditional gender stereotypes.

**Link**: [arxiv](http://arxiv.org/abs/2405.06687v3),  [pdf](http://arxiv.org/pdf/2405.06687v3)

**Tags**: cs.CL 



### ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via   Mechanistic Interpretability
**Authors**: Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li

**Updated**: 2025-01-21T16:05:30Z

**Summary**: Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) utilize external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual stream, while Copying Heads fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose ReDeEP, a novel method that detects hallucinations by decoupling LLM's utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.

**Link**: [arxiv](http://arxiv.org/abs/2410.11414v2),  [pdf](http://arxiv.org/pdf/2410.11414v2)

**Tags**: cs.CL 



### FOCUS: First Order Concentrated Updating Scheme
**Authors**: Yizhou Liu, Ziming Liu, Jeff Gore

**Updated**: 2025-01-21T16:03:42Z

**Summary**: Large language models (LLMs) demonstrate remarkable performance, and improving their pre-training process appears to be key to enhancing their capabilities further. Based on the documented success of Adam, learning rate decay, and weight decay, we hypothesize that the pre-training loss landscape features a narrowing valley structure. Through experiments with synthetic loss functions, we discover that when gradient query noise is high relative to the valley's sharpness, Adam's performance falls behind that of Signum because Adam reduces the effective step size too drastically. This observation led us to develop FOCUS, an optimizer that enhances Signum by incorporating attraction toward moving averaged parameters, allowing it to handle noise better while maintaining larger step sizes. In training GPT-2, FOCUS proves to be more stable than Signum and faster than Adam. These results suggest that gradient noise may be an underappreciated limiting factor in LLM training, and FOCUS offers promising solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12243v1),  [pdf](http://arxiv.org/pdf/2501.12243v1)

**Tags**: cs.LG cs.CL math.OC 



### A Dusty Dawn: Galactic Dust Buildup at $z\gtrsim5$
**Authors**: Caleb R. Choban, Samir Salim, DuÅ¡an KereÅ¡, Christopher C. Hayward, Karin M. Sandstrom

**Updated**: 2025-01-21T16:03:33Z

**Summary**: Over the last decade, the Atacama Large Millimeter Array has revealed massive, dusty star-forming galaxies at $z\gtrsim5$, and the James Webb Space Telescope is primed to uncover even more information about them. These observations need dust evolution theory to provide context and are excellent benchmarks to test this theory. Here, we investigate the evolution of galactic dust budget at cosmic dawn using a suite of cosmological zoom-in simulations of moderately massive, high-redshift ($M_{\rm star}\gtrsim10^9 M_{\odot}$; $z\gtrsim5$) galaxies from the FIRE project, the highest resolution ($m_{\rm b} \approx 7100\, M_{\odot}$) of such simulations to date. Our simulations incorporate a dust evolution model that accounts for the dominant sources of dust production, growth, and destruction and follows the evolution of specific dust species, allowing it to replicate a wide range of present-day observations. We find, similar to other theoretical works, that dust growth via gas-dust accretion is the dominant producer of dust mass for these massive, $z\gtrsim 5$ galaxies. However, our fiducial model produces $M_{\rm dust}$ that fall ${\gtrsim}1$ dex below observations at any given $M_{\rm star}$ (typical uncertainties are ${\sim}1$ dex), which we attribute to reduced accretion efficiencies caused by a combination of low galactic metallicities and extremely bursty star formation. Modest enhancements (i.e., within observational/theoretical uncertainties) to accretion and SNe II dust creation raise $M_{\rm dust}$ by ${\lesssim}1$ dex, but this still falls below observations which assume $T_{\rm dust}\sim25$ K. One possibility is that inferred dust masses for $z\gtrsim4$ galaxies are overestimated, and recent observational/analytical works that find $T_{\rm dust}\sim50$ K along with metallicity constraints tentatively support this.

**Link**: [arxiv](http://arxiv.org/abs/2408.08962v2),  [pdf](http://arxiv.org/pdf/2408.08962v2)

**Tags**: astro-ph.GA 



### InsTALL: Context-aware Instructional Task Assistance with Multi-modal   Large Language Models
**Authors**: Pha Nguyen, Sailik Sengupta, Girik Malik, Arshit Gupta, Bonan Min

**Updated**: 2025-01-21T15:55:06Z

**Summary**: The improved competence of generative models can help building multi-modal virtual assistants that leverage modalities beyond language. By observing humans performing multi-step tasks, one can build assistants that have situational awareness of actions and tasks being performed, enabling them to cater assistance based on this understanding. In this paper, we develop a Context-aware Instructional Task Assistant with Multi-modal Large Language Models (InsTALL) that leverages an online visual stream (e.g. a user's screen share or video recording) and responds in real-time to user queries related to the task at hand. To enable useful assistance, InsTALL 1) trains a multi-modal model on task videos and paired textual data, and 2) automatically extracts task graph from video data and leverages it at training and inference time. We show InsTALL achieves state-of-the-art performance across proposed sub-tasks considered for multimodal activity understanding -- task recognition (TR), action recognition (AR), next action prediction (AP), and plan prediction (PP) -- and outperforms existing baselines on two novel sub-tasks related to automatic error identification.

**Link**: [arxiv](http://arxiv.org/abs/2501.12231v1),  [pdf](http://arxiv.org/pdf/2501.12231v1)

**Tags**: cs.CV cs.AI cs.CL 



### CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning
**Authors**: Yuanheng Fang, Guoqing Chao, Wenqiang Lei, Shaobo Li, Dianhui Chu

**Updated**: 2025-01-21T15:51:07Z

**Summary**: Large Language Models (LLMs) have recently achieved impressive results in complex reasoning tasks through Chain of Thought (CoT) prompting. However, most existing CoT methods rely on using the same prompts, whether manually designed or automatically generated, to handle the entire dataset. This one-size-fits-all approach may fail to meet the specific needs arising from the diversities within a single dataset. To solve this problem, we propose the Clustered Distance-Weighted Chain of Thought (CDW-CoT) method, which dynamically constructs prompts tailored to the characteristics of each data instance by integrating clustering and prompt optimization techniques. Our method employs clustering algorithms to categorize the dataset into distinct groups, from which a candidate pool of prompts is selected to reflect the inherent diversity within the dataset. For each cluster, CDW-CoT trains the optimal prompt probability distribution tailored to their specific characteristics. Finally, it dynamically constructs a unique prompt probability distribution for each test instance, based on its proximity to cluster centers, from which prompts are selected for reasoning. CDW-CoT consistently outperforms traditional CoT methods across six datasets, including commonsense, symbolic, and mathematical reasoning tasks. Specifically, when compared to manual CoT, CDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and 15.72% on LLaMA3 (8B).

**Link**: [arxiv](http://arxiv.org/abs/2501.12226v1),  [pdf](http://arxiv.org/pdf/2501.12226v1)

**Tags**: cs.LG 



### Leveraging Large Language Models for Realizing Truly Intelligent User   Interfaces
**Authors**: Allard Oelen, SÃ¶ren Auer

**Updated**: 2025-01-21T15:47:32Z

**Summary**: The number of published scholarly articles is growing at a significant rate, making scholarly knowledge organization increasingly important. Various approaches have been proposed to organize scholarly information, including describing scholarly knowledge semantically leveraging knowledge graphs. Transforming unstructured knowledge, presented within articles, to structured and semantically represented knowledge generally requires human intelligence and labor since natural language processing methods alone typically do not render sufficient precision and recall for many applications. With the recent developments of Large Language Models (LLMs), it becomes increasingly possible to provide truly intelligent user interfaces guiding humans in the transformation process. We present an approach to integrate non-intrusive LLMs guidance into existing user interfaces. More specifically, we integrate LLM-supported user interface components into an existing scholarly knowledge infrastructure. Additionally, we provide our experiences with LLM integration, detailing best practices and obstacles. Finally, we evaluate the approach using a small-scale user evaluation with domain experts.

**Link**: [arxiv](http://arxiv.org/abs/2501.12221v1),  [pdf](http://arxiv.org/pdf/2501.12221v1)

**Tags**: cs.DL 



### NoteLLM-2: Multimodal Large Representation Models for Recommendation
**Authors**: Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen

**Updated**: 2025-01-21T15:40:43Z

**Summary**: Large Language Models (LLMs) have demonstrated exceptional proficiency in text understanding and embedding tasks. However, their potential in multimodal representation, particularly for item-to-item (I2I) recommendations, remains underexplored. While leveraging existing Multimodal Large Language Models (MLLMs) for such tasks is promising, challenges arise due to their delayed release compared to corresponding LLMs and the inefficiency in representation tasks. To address these issues, we propose an end-to-end fine-tuning method that customizes the integration of any existing LLMs and vision encoders for efficient multimodal representation. Preliminary experiments revealed that fine-tuned LLMs often neglect image content. To counteract this, we propose NoteLLM-2, a novel framework that enhances visual information. Specifically, we propose two approaches: first, a prompt-based method that segregates visual and textual content, employing a multimodal In-Context Learning strategy to balance focus across modalities; second, a late fusion technique that directly integrates visual information into the final representations. Extensive experiments, both online and offline, demonstrate the effectiveness of our approach. Code is available at https://github.com/Applied-Machine-Learning-Lab/NoteLLM.

**Link**: [arxiv](http://arxiv.org/abs/2405.16789v2),  [pdf](http://arxiv.org/pdf/2405.16789v2)

**Tags**: cs.IR 



### VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction
**Authors**: Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He

**Updated**: 2025-01-21T15:36:41Z

**Summary**: Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.

**Link**: [arxiv](http://arxiv.org/abs/2501.01957v3),  [pdf](http://arxiv.org/pdf/2501.01957v3)

**Tags**: cs.CV cs.SD eess.AS 



### RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression
**Authors**: Uri Gadot, Assaf Shocher, Shie Mannor, Gal Chechik, Assaf Hallak

**Updated**: 2025-01-21T15:36:08Z

**Summary**: Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional task agnostic encoding methods, paving the way for more efficient task-aware video compression.

**Link**: [arxiv](http://arxiv.org/abs/2501.12216v1),  [pdf](http://arxiv.org/pdf/2501.12216v1)

**Tags**: cs.LG cs.CV eess.IV 



### A Search-to-Control Reinforcement Learning Based Framework for Quadrotor   Local Planning in Dense Environments
**Authors**: Zhaohong Liu, Wenxuan Gao, Yinshuai Sun, Peng Dong

**Updated**: 2025-01-21T15:33:35Z

**Summary**: Agile flight in complex environments poses significant challenges to current motion planning methods, as they often fail to fully leverage the quadrotor's dynamic potential, leading to performance failures and reduced efficiency during aggressive maneuvers. Existing approaches frequently decouple trajectory optimization from control generation and neglect the dynamics, further limiting their ability to generate aggressive and feasible motions. To address these challenges, we introduce an enhanced Search-to-Control planning framework that integrates visibility path searching with reinforcement learning (RL) control generation, directly accounting for dynamics and bridging the gap between planning and control. Our method first extracts control points from collision-free paths using a proposed heuristic search, which are then refined by an RL policy to generate low-level control commands for the quadrotor's controller, utilizing reduced-dimensional obstacle observations for efficient inference with lightweight neural networks. We validate the framework through simulations and real-world experiments, demonstrating improved time efficiency and dynamic maneuverability compared to existing methods, while confirming its robustness and applicability. To support further research, We will release our implementation as an open-source package.

**Link**: [arxiv](http://arxiv.org/abs/2408.00275v3),  [pdf](http://arxiv.org/pdf/2408.00275v3)

**Tags**: cs.RO 



### You Can't Eat Your Cake and Have It Too: The Performance Degradation of   LLMs with Jailbreak Defense
**Authors**: Wuyuao Mai, Geng Hong, Pei Chen, Xudong Pan, Baojun Liu, Yuan Zhang, Haixin Duan, Min Yang

**Updated**: 2025-01-21T15:24:29Z

**Summary**: With the rise of generative large language models (LLMs) like LLaMA and ChatGPT, these models have significantly transformed daily life and work by providing advanced insights. However, as jailbreak attacks continue to circumvent built-in safety mechanisms, exploiting carefully crafted scenarios or tokens, the safety risks of LLMs have come into focus. While numerous defense strategies--such as prompt detection, modification, and model fine-tuning--have been proposed to counter these attacks, a critical question arises: do these defenses compromise the utility and usability of LLMs for legitimate users? Existing research predominantly focuses on the effectiveness of defense strategies without thoroughly examining their impact on performance, leaving a gap in understanding the trade-offs between LLM safety and performance. Our research addresses this gap by conducting a comprehensive study on the utility degradation, safety elevation, and exaggerated-safety escalation of LLMs with jailbreak defense strategies. We propose USEBench, a novel benchmark designed to evaluate these aspects, along with USEIndex, a comprehensive metric for assessing overall model performance. Through experiments on seven state-of-the-art LLMs, we found that mainstream jailbreak defenses fail to ensure both safety and performance simultaneously. Although model-finetuning performs the best overall, their effectiveness varies across LLMs. Furthermore, vertical comparisons reveal that developers commonly prioritize performance over safety when iterating or fine-tuning their LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.12210v1),  [pdf](http://arxiv.org/pdf/2501.12210v1)

**Tags**: cs.CR 



### TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and   Agent Generation
**Authors**: Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su

**Updated**: 2025-01-21T15:11:41Z

**Summary**: The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents' abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2402.10178v2),  [pdf](http://arxiv.org/pdf/2402.10178v2)

**Tags**: cs.CL 



### FLARE: Faithful Logic-Aided Reasoning and Exploration
**Authors**: Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein

**Updated**: 2025-01-21T14:57:22Z

**Summary**: Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.

**Link**: [arxiv](http://arxiv.org/abs/2410.11900v4),  [pdf](http://arxiv.org/pdf/2410.11900v4)

**Tags**: cs.AI cs.CL cs.LG cs.LO 



### Modelling polarized X-ray pulses from accreting millisecond pulsars with   X-PSI, using different surface patterns
**Authors**: Tuomo Salmi, Bas Dorsman, Anna L. Watts, Anna Bobrikova, Alessandro Di Marco, Vladislav Loktev, Alessandro Papitto, Maura Pilia, Juri Poutanen, John Rankin

**Updated**: 2025-01-21T14:54:28Z

**Summary**: We present an analysis of polarized X-ray pulses based on simulated data for accreting millisecond pulsars (AMPs). We used the open-source X-ray Pulse Simulation and Inference code (previously applied to NICER observations), that we upgraded to allow polarization analysis. We predicted neutron star parameter constraints for the Imaging X-ray Polarimetry Explorer (IXPE) and found that strong limits on the hot region geometries can be hard to obtain if the emitting hot region is large and the number of polarized photons relatively small. However, if the star is bright enough and the hot regions are small and located so that polarization degree is higher, the observer inclination and hotspot colatitude can be constrained to a precision of within a few degrees. We also found that the shape of the hot region, whether a circle or a ring, cannot be distinguished in our most optimistic scenario. Nevertheless, future X-ray polarization missions are expected to improve the constraints, and already the recent AMP polarization detections by IXPE should help to infer the neutron star mass and radius when combined with modelling of X-ray pulse data sets that do not contain polarization information.

**Link**: [arxiv](http://arxiv.org/abs/2501.12190v1),  [pdf](http://arxiv.org/pdf/2501.12190v1)

**Tags**: astro-ph.HE 



### Reinforcement Learning from Human Feedback without Reward Inference:   Model-Free Algorithm and Instance-Dependent Analysis
**Authors**: Qining Zhang, Honghao Wei, Lei Ying

**Updated**: 2025-01-21T14:53:36Z

**Summary**: In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\tilde{\mathcal{O}}(c_{\mathcal{M}}SA^3H^3M\log\frac{1}{\delta})$ which resembles the result in classic RL, where $c_{\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.

**Link**: [arxiv](http://arxiv.org/abs/2406.07455v2),  [pdf](http://arxiv.org/pdf/2406.07455v2)

**Tags**: cs.LG stat.ML 



### The Impact of Copyrighted Material on Large Language Models: A Norwegian   Perspective
**Authors**: Javier de la Rosa, Vladislav Mikhailov, Lemei Zhang, Freddy Wetjen, David Samuel, Peng Liu, Rolv-Arild Braaten, Petter MÃ¦hlum, Magnus Breder Birkenes, Andrey Kutuzov, Tita Enstad, Hans Christian FarsethÃ¥s, Svein Arne Brygfjeld, Jon Atle Gulla, Stephan Oepen, Erik Velldal, Wilfred Ãstgulen, Liljia Ãvrelid, Aslak Sira Myhre

**Updated**: 2025-01-21T14:38:35Z

**Summary**: The use of copyrighted materials in training language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of publisher-controlled copyrighted corpora on the performance of generative large language models (LLMs) for Norwegian. When evaluated on a diverse set of tasks, we found that adding both books and newspapers to the data mixture of LLMs tend to improve their performance, while the addition of fiction works seems to be detrimental. Our experiments could inform the creation of a compensation scheme for authors whose works contribute to AI development.

**Link**: [arxiv](http://arxiv.org/abs/2412.09460v2),  [pdf](http://arxiv.org/pdf/2412.09460v2)

**Tags**: cs.CL 



### Block Adaptive Progressive Type-II Censored Sampling for the Inverted   Exponentiated Pareto Distribution: Parameter Inference and Reliability   Assessment
**Authors**: Rajendranath Mondal, Aditi Kar Gangopadhyay, Raju Bhakta, Kousik Maiti

**Updated**: 2025-01-21T14:36:41Z

**Summary**: This article explores the estimation of unknown parameters and reliability characteristics under the assumption that the lifetimes of the testing units follow an Inverted Exponentiated Pareto (IEP) distribution. Here, both point and interval estimates are calculated by employing the classical maximum likelihood and a pivotal estimation methods. Also, existence and uniqueness of the maximum likelihood estimates are verified. Further, asymptotic confidence intervals are derived by using the asymptotic normality property of the maximum likelihood estimator. Moreover, generalized confidence intervals are obtained by utilizing the pivotal quantities. Additionally, some mathematical developments of the IEP distribution are discussed based on the concept of order statistics. Furthermore, all the estimations are performed on the basis of the block censoring procedure, where an adaptive progressive Type-II censoring is employed to every block. In this regard, the performances of two estimation methods, namely maximum likelihood estimation and pivotal estimation, is evaluated and compared through a simulation study. Finally, a real data is illustrated to demonstrate the flexibility of the proposed IEP model.

**Link**: [arxiv](http://arxiv.org/abs/2501.12179v1),  [pdf](http://arxiv.org/pdf/2501.12179v1)

**Tags**: math.ST stat.TH 



### Ladder-residual: parallelism-aware architecture for accelerating large   model inference with communication overlapping
**Authors**: Muru Zhang, Mayank Mishra, Zhongzhu Zhou, William Brandon, Jue Wang, Yoon Kim, Jonathan Ragan-Kelley, Shuaiwen Leon Song, Ben Athiwaratkun, Tri Dao

**Updated**: 2025-01-21T14:33:38Z

**Summary**: Large language model inference is both memory-intensive and time-consuming, often requiring distributed algorithms to efficiently scale. Various model parallelism strategies are used in multi-gpu training and inference to partition computation across multiple devices, reducing memory load and computation time. However, using model parallelism necessitates communication of information between GPUs, which has been a major bottleneck and limits the gains obtained by scaling up the number of devices. We introduce Ladder Residual, a simple architectural modification applicable to all residual-based models that enables straightforward overlapping that effectively hides the latency of communication. Our insight is that in addition to systems optimization, one can also redesign the model architecture to decouple communication from computation. While Ladder Residual can allow communication-computation decoupling in conventional parallelism patterns, we focus on Tensor Parallelism in this paper, which is particularly bottlenecked by its heavy communication. For a Transformer model with 70B parameters, applying Ladder Residual to all its layers can achieve 30% end-to-end wall clock speed up at inference time with TP sharding over 8 devices. We refer the resulting Transformer model as the Ladder Transformer. We train a 1B and 3B Ladder Transformer from scratch and observe comparable performance to a standard dense transformer baseline. We also show that it is possible to convert parts of the Llama-3.1 8B model to our Ladder Residual architecture with minimal accuracy degradation by only retraining for 3B tokens.

**Link**: [arxiv](http://arxiv.org/abs/2501.06589v2),  [pdf](http://arxiv.org/pdf/2501.06589v2)

**Tags**: cs.LG cs.CL cs.DC 



### BiMarker: Enhancing Text Watermark Detection for Large Language Models   with Bipolar Watermarks
**Authors**: Zhuang Li

**Updated**: 2025-01-21T14:32:50Z

**Summary**: The rapid proliferation of Large Language Models (LLMs) has raised concerns about misuse and the challenges of distinguishing AI-generated text from human-written content. Existing watermarking techniques, such as \kgw, still face limitations under low watermark strength, stringent false-positive requirements, and low-entropy scenarios. Our analysis reveals that current detection methods rely on coarse estimates of non-watermarked text, which constrains watermark detectability. We propose the Bipolar Watermark (BiMarker), a novel approach that divides generated text into positive and negative poles, leveraging the difference in green token counts for detection. This differential mechanism significantly enhances the detectability of watermarked text. Theoretical analysis and experimental results demonstrate BiMarker's effectiveness and compatibility with existing optimization techniques, offering a new optimization dimension for watermarking in LLM-generated content.

**Link**: [arxiv](http://arxiv.org/abs/2501.12174v1),  [pdf](http://arxiv.org/pdf/2501.12174v1)

**Tags**: cs.LG 



### AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative   Decoding
**Authors**: Zikun Li, Zhuofu Chen, Remi Delacourt, Gabriele Oliaro, Zeyu Wang, Qinghan Chen, Shuhuai Lin, April Yang, Zhihao Zhang, Zhuoming Chen, Sean Lai, Xupeng Miao, Zhihao Jia

**Updated**: 2025-01-21T14:15:01Z

**Summary**: This paper introduces AdaServe, the first LLM serving system to support SLO customization through fine-grained speculative decoding. AdaServe leverages the logits of a draft model to predict the speculative accuracy of tokens and employs a theoretically optimal algorithm to construct token trees for verification. To accommodate diverse SLO requirements without compromising throughput, AdaServe employs a speculation-and-selection scheme that first constructs candidate token trees for each request and then dynamically selects tokens to meet individual SLO constraints while optimizing throughput. Comprehensive evaluations demonstrate that AdaServe achieves up to 73% higher SLO attainment and 74% higher goodput compared to state-of-the-art systems. These results underscore AdaServe's potential to enhance the efficiency and adaptability of LLM deployments across varied application scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.12162v1),  [pdf](http://arxiv.org/pdf/2501.12162v1)

**Tags**: cs.CL cs.AI cs.DC cs.LG 



### Contextualizing Recommendation Explanations with LLMs: A User Study
**Authors**: Yuanjun Feng, Stefan Feuerriegel, Yash Raj Shrestha

**Updated**: 2025-01-21T14:02:39Z

**Summary**: Large language models (LLMs) are increasingly prevalent in recommender systems, where LLMs can be used to generate personalized recommendations. Here, we examine how different LLM-generated explanations for movie recommendations affect users' perceptions of cognitive, affective, and utilitarian needs and consumption intentions. In a pre-registered, between-subject online experiment (N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic explanations, and (b) LLM-generated contextualized explanations. Our findings show that contextualized explanations (i.e., explanations that incorporate users' past behaviors) effectively meet users' cognitive needs while increasing users' intentions to watch recommended movies. However, adding explanations offers limited benefits in meeting users' utilitarian and affective needs, raising concerns about the proper design and implications of LLM-generated explanations. Qualitative insights from interviews reveal that referencing users' past preferences enhances trust and understanding but can feel excessive if overused. Furthermore, users with more active and positive engagement with the recommender system and movie-watching get substantial gains from contextualized explanations. Overall, our research clarifies how LLM-generated recommendations influence users' motivations and behaviors, providing valuable insights for the future development of user-centric recommender systems, a key element in social media platforms and online ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.12152v1),  [pdf](http://arxiv.org/pdf/2501.12152v1)

**Tags**: cs.HC 



### Improving Influence-based Instruction Tuning Data Selection for Balanced   Learning of Diverse Capabilities
**Authors**: Qirun Dai, Dylan Zhang, Jiaqi W. Ma, Hao Peng

**Updated**: 2025-01-21T14:00:43Z

**Summary**: Selecting appropriate training data is crucial for effective instruction fine-tuning of large language models (LLMs), which aims to (1) elicit strong capabilities, and (2) achieve balanced performance across a diverse range of tasks. Influence-based methods show promise in achieving (1) by estimating the contribution of each training example to the model's predictions, but often struggle with (2). Our systematic investigation reveals that this underperformance can be attributed to an inherent bias where certain tasks intrinsically have greater influence than others. As a result, data selection is often biased towards these tasks, not only hurting the model's performance on others but also, counterintuitively, harms performance on these high-influence tasks themselves.   As a remedy, we propose BIDS, a Balanced and Influential Data Selection algorithm. BIDS first normalizes influence scores of the training data, and then iteratively balances data selection by choosing the training example with the highest influence on the most underrepresented task. Experiments with both Llama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities show that BIDS consistently outperforms both state-of-the-art influence-based algorithms and other non-influence-based selection frameworks. Surprisingly, training on a 15% subset selected by BIDS can even outperform full-dataset training with a much more balanced performance. Our analysis further highlights the importance of both instance-level normalization and iterative optimization of selected data for balanced learning of diverse capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.12147v1),  [pdf](http://arxiv.org/pdf/2501.12147v1)

**Tags**: cs.CL cs.AI cs.LG I.2.7 



### Do LLMs Provide Links to Code Similar to what they Generate? A Study   with Gemini and Bing CoPilot
**Authors**: Daniele Bifolco, Pietro Cassieri, Giuseppe Scanniello, Massimiliano Di Penta, Fiorella Zampetti

**Updated**: 2025-01-21T13:47:22Z

**Summary**: Large Language Models (LLMs) are currently used for various software development tasks, including generating code snippets to solve specific problems. Unlike reuse from the Web, LLMs are limited in providing provenance information about the generated code, which may have important trustworthiness and legal consequences. While LLM-based assistants may provide external links that are "related" to the generated code, we do not know how relevant such links are. This paper presents the findings of an empirical study assessing the extent to which 243 and 194 code snippets, across six programming languages, generated by Bing CoPilot and Google Gemini, likely originate from the links provided by these two LLM-based assistants. The study leverages automated code similarity assessments with thorough manual analysis. The study's findings indicate that the LLM-based assistants provide a mix of relevant and irrelevant links having a different nature. Specifically, although 66% of the links from Bing CoPilot and 28% from Google Gemini are relevant, LLMs-based assistants still suffer from serious "provenance debt".

**Link**: [arxiv](http://arxiv.org/abs/2501.12134v1),  [pdf](http://arxiv.org/pdf/2501.12134v1)

**Tags**: cs.SE 



### Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced   Human-Robot Interactions
**Authors**: Tim Schreiter, Jens V. RÃ¼ppel, Rishi Hazra, Andrey Rudenko, Martin Magnusson, Achim J. Lilienthal

**Updated**: 2025-01-21T13:42:06Z

**Summary**: To achieve natural and intuitive interaction with people, HRI frameworks combine a wide array of methods for human perception, intention communication, human-aware navigation and collaborative action. In practice, when encountering unpredictable behavior of people or unexpected states of the environment, these frameworks may lack the ability to dynamically recognize such states, adapt and recover to resume the interaction. Large Language Models (LLMs), owing to their advanced reasoning capabilities and context retention, present a promising solution for enhancing robot adaptability. This potential, however, may not directly translate to improved interaction metrics. This paper considers a representative interaction with an industrial robot involving approach, instruction, and object manipulation, implemented in two conditions: (1) fully scripted and (2) including LLM-enhanced responses. We use gaze tracking and questionnaires to measure the participants' task efficiency, engagement, and robot perception. The results indicate higher subjective ratings for the LLM condition, but objective metrics show that the scripted condition performs comparably, particularly in efficiency and focus during simple tasks. We also note that the scripted condition may have an edge over LLM-enhanced responses in terms of response latency and energy consumption, especially for trivial and repetitive interactions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12128v1),  [pdf](http://arxiv.org/pdf/2501.12128v1)

**Tags**: cs.RO cs.HC 



### Can open source large language models be used for tumor documentation in   Germany? -- An evaluation on urological doctors' notes
**Authors**: Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer

**Updated**: 2025-01-21T12:56:47Z

**Summary**: Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors' notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP.

**Link**: [arxiv](http://arxiv.org/abs/2501.12106v1),  [pdf](http://arxiv.org/pdf/2501.12106v1)

**Tags**: cs.CL cs.AI 



### Checkification: A Practical Approach for Testing Static Analysis Truths
**Authors**: Daniela Ferreiro, Ignacio Casso, Jose F. Morales, Pedro LÃ³pez-GarcÃ­a, Manuel V. Hermenegildo

**Updated**: 2025-01-21T12:38:04Z

**Summary**: Static analysis is an essential component of many modern software development tools. Unfortunately, the ever-increasing complexity of static analyzers makes their coding error-prone. Even analysis tools based on rigorous mathematical techniques, such as abstract interpretation, are not immune to bugs. Ensuring the correctness and reliability of software analyzers is critical if they are to be inserted in production compilers and development environments. While compiler validation has seen notable success, formal validation of static analysis tools remains relatively unexplored. In this paper, we propose a method for testing abstract interpretation-based static analyzers. Broadly, it consists in checking, over a suite of benchmarks, that the properties inferred statically are satisfied dynamically. The main advantage of our approach lies in its simplicity, which stems directly from framing it within the Ciao assertion-based validation framework, and its blended static/dynamic assertion checking approach. We demonstrate that in this setting, the analysis can be tested with little effort by combining the following components already present in the framework: 1) the static analyzer, which outputs its results as the original program source with assertions interspersed; 2) the assertion run-time checking mechanism, which instruments a program to ensure that no assertion is violated at run time; 3) the random test case generator, which generates random test cases satisfying the properties present in assertion preconditions; and 4) the unit-test framework, which executes those test cases. We have applied our approach to the CiaoPP static analyzer, resulting in the identification of many bugs with reasonable overhead. Most of these bugs have been either fixed or confirmed, helping us detect a range of errors not only related to analysis soundness but also within other aspects of the framework.

**Link**: [arxiv](http://arxiv.org/abs/2501.12093v1),  [pdf](http://arxiv.org/pdf/2501.12093v1)

**Tags**: cs.SE cs.PL 



### UAV-Assisted Real-Time Disaster Detection Using Optimized Transformer   Model
**Authors**: Branislava Jankovic, Sabina Jangirova, Waseem Ullah, Latif U. Khan, Mohsen Guizani

**Updated**: 2025-01-21T12:29:45Z

**Summary**: Disaster recovery and management present significant challenges, particularly in unstable environments and hard-to-reach terrains. These difficulties can be overcome by employing unmanned aerial vehicles (UAVs) equipped with onboard embedded platforms and camera sensors. In this work, we address the critical need for accurate and timely disaster detection by enabling onboard aerial imagery processing and avoiding connectivity, privacy, and latency issues despite the challenges posed by limited onboard hardware resources. We propose a UAV-assisted edge framework for real-time disaster management, leveraging our proposed model optimized for real-time aerial image classification. The optimization of the model employs post-training quantization techniques. For real-world disaster scenarios, we introduce a novel dataset, DisasterEye, featuring UAV-captured disaster scenes as well as ground-level images taken by individuals on-site. Experimental results demonstrate the effectiveness of our model, achieving high accuracy with reduced inference latency and memory usage on resource-constrained devices. The framework's scalability and adaptability make it a robust solution for real-time disaster detection on resource-limited UAV platforms.

**Link**: [arxiv](http://arxiv.org/abs/2501.12087v1),  [pdf](http://arxiv.org/pdf/2501.12087v1)

**Tags**: cs.CV 



### Exploring Learning Rate Selection in Generalised Bayesian Inference   using Posterior Predictive Checks
**Authors**: Schyan Zafar, Geoff K. Nicholls

**Updated**: 2025-01-21T12:20:18Z

**Summary**: Generalised Bayesian Inference (GBI) attempts to address model misspecification in a standard Bayesian setup by tempering the likelihood. The likelihood is raised to a fractional power, called the learning rate, which reduces its importance in the posterior and has been established as a method to address certain kinds of model misspecification. Posterior Predictive Checks (PPC) attempt to detect model misspecification by locating a diagnostic, computed on the observed data, within the posterior predictive distribution of the diagnostic. This can be used to construct a hypothesis test where a small $p$-value indicates potential misfit. The recent Embedded Diachronic Sense Change (EDiSC) model suffers from misspecification and benefits from likelihood tempering. Using EDiSC as a case study, this exploratory work examines whether PPC could be used in a novel way to set the learning rate in a GBI setup. Specifically, the learning rate selected is the lowest value for which a hypothesis test using the log likelihood diagnostic is not rejected at the 10% level. The experimental results are promising, though not definitive, and indicate the need for further research along the lines suggested here.

**Link**: [arxiv](http://arxiv.org/abs/2410.01475v2),  [pdf](http://arxiv.org/pdf/2410.01475v2)

**Tags**: stat.ME stat.AP 



### Goal-Oriented Status Updating for Real-time Remote Inference over   Networks with Two-Way~Delay
**Authors**: Cagri Ari, Md Kamran Chowdhury Shisher, Yin Sun, Elif Uysal

**Updated**: 2025-01-21T12:08:11Z

**Summary**: We study a setting where an intelligent model (e.g., a pre-trained neural network) predicts the real-time value of a target signal using data samples transmitted from a remote source according to a scheduling policy. The scheduler decides on i) the age of the samples to be sent, ii) when to send them, and iii) the length of each packet (i.e., the number of samples contained in each packet). The dependence of inference quality on the Age of Information (AoI) for a given packet length is modeled by a general relationship. Previous work assumed i.i.d. transmission delays with immediate feedback or were restricted to the case where inference performance degrades as the input data ages. Our formulation, in addition to capturing non-monotone age dependence, also covers Markovian delay on both forward and feedback links. We model this as an infinite-horizon average-cost Semi-Markov Decision Process. We obtain a closed-form solution that decides on (i) and (ii) for any constant packet length. The solution for when to send is an index-based threshold policy, where the index function is expressed in terms of the delay state and AoI at the receiver. The age of the packet selected is a function of the delay state. We separately optimize the value of the constant length. We also develop an index-based threshold policy for the variable length case, which allows a complexity reduction. In simulation results, we observe that our goal-oriented scheduler drops inference error down to one sixth with respect to age-based scheduling of unit-length packets.

**Link**: [arxiv](http://arxiv.org/abs/2410.08706v2),  [pdf](http://arxiv.org/pdf/2410.08706v2)

**Tags**: cs.NI eess.SP 



### EDoRA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular   Value Decomposition
**Authors**: Hamid Nasiri, Peter Garraghan

**Updated**: 2025-01-21T11:42:09Z

**Summary**: Parameter-efficient fine-tuning methods, such as LoRA, reduces the number of trainable parameters. However, they often suffer from scalability issues and differences between their learning pattern and full fine-tuning. To overcome these limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation (EDoRA): a novel PEFT method that decomposes pre-trained weights into magnitude and directional components. By freezing low-rank matrices, initializing them by singular value decomposition, and introducing a small trainable matrix between them, EDoRA achieves substantial reduction in trainable parameters while maintaining learning capacity. Experimental results on the GLUE benchmark demonstrate that EDoRA achieves competitive or superior performance compared to state-of-the-art methods, such as LoRA and DoRA, with up to 30x fewer trainable parameters. This makes EDoRA a highly efficient solution for adapting LLMs to diverse tasks under memory-constrained settings. Code is available at https://github.com/Hamid-Nasiri/EDoRA .

**Link**: [arxiv](http://arxiv.org/abs/2501.12067v1),  [pdf](http://arxiv.org/pdf/2501.12067v1)

**Tags**: cs.LG cs.AI cs.CL 



### Large Language Model-Brained GUI Agents: A Survey
**Authors**: Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

**Updated**: 2025-01-21T11:36:03Z

**Summary**: GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.   To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.

**Link**: [arxiv](http://arxiv.org/abs/2411.18279v7),  [pdf](http://arxiv.org/pdf/2411.18279v7)

**Tags**: cs.AI cs.CL cs.HC 



### PINNsAgent: Automated PDE Surrogation with Large Language Models
**Authors**: Qingpo Wuwu, Chonghan Gao, Tianyu Chen, Yihang Huang, Yuekai Zhang, Jianing Wang, Jianxin Li, Haoyi Zhou, Shanghang Zhang

**Updated**: 2025-01-21T11:26:02Z

**Summary**: Solving partial differential equations (PDEs) using neural methods has been a long-standing scientific and engineering research pursuit. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative to traditional numerical methods for solving PDEs. However, the gap between domain-specific knowledge and deep learning expertise often limits the practical application of PINNs. Previous works typically involve manually conducting extensive PINNs experiments and summarizing heuristic rules for hyperparameter tuning. In this work, we introduce PINNsAgent, a novel surrogation framework that leverages large language models (LLMs) and utilizes PINNs as a foundation to bridge the gap between domain-specific knowledge and deep learning. Specifically, PINNsAgent integrates (1) Physics-Guided Knowledge Replay (PGKR), which encodes the essential characteristics of PDEs and their associated best-performing PINNs configurations into a structured format, enabling efficient knowledge transfer from solved PDEs to similar problems and (2) Memory Tree Reasoning, a strategy that effectively explores the search space for optimal PINNs architectures. By leveraging LLMs and exploration strategies, PINNsAgent enhances the automation and efficiency of PINNs-based solutions. We evaluate PINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating the surrogation process and significantly improving the accuracy of PINNs-based solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12053v1),  [pdf](http://arxiv.org/pdf/2501.12053v1)

**Tags**: cs.CE 



### MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow   Thinking
**Authors**: Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, Yu Wang

**Updated**: 2025-01-21T11:24:55Z

**Summary**: Medical language models (MLMs) have become pivotal in advancing medical natural language processing. However, prior models that rely on pre-training or supervised fine-tuning often exhibit low data efficiency and limited practicality in real-world clinical applications. While OpenAIs O1 highlights test-time scaling in mathematics, attempts to replicate this approach in medicine typically distill responses from GPT-series models to open-source models, focusing primarily on multiple-choice tasks. This strategy, though straightforward, neglects critical concerns like data privacy and realistic deployment in clinical settings. In this work, we present a deployable, small-scale medical language model, \mone, designed for long-chain reasoning in clinical tasks using a self-evolution paradigm. Starting with a seed dataset of around 8,000 instances spanning five domains and 16 datasets, we prompt a base policy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable reasoning chains. Each reasoning step is assigned an evolution rollout value, allowing verified trajectories to train the policy model and the reward model. During inference, the policy model generates multiple responses, and the reward model selects the one with the highest reward score. Experiments on eleven evaluation datasets demonstrate that \mone outperforms prior open-source models by 2 points, with the addition of the reward model further boosting performance ($\sim$13 points), surpassing GPT-4o-mini. Code and data are available at \url{https://github.com/pixas/MedSSS}.

**Link**: [arxiv](http://arxiv.org/abs/2501.12051v1),  [pdf](http://arxiv.org/pdf/2501.12051v1)

**Tags**: cs.CL 



### Inferring the Ionizing Photon Contributions of High-Redshift Galaxies to   Reionization with JWST NIRCam Photometry
**Authors**: Nicholas Choustikov, Richard Stiskalek, Aayush Saxena, Harley Katz, Julien Devriendt, Adrianne Slyz

**Updated**: 2025-01-21T11:03:55Z

**Summary**: JWST is providing constraints on the history of reionization owing to its ability to detect faint galaxies at $z\gg6$. Modeling this history requires understanding both the ionizing photon production rate ($\xi_{\rm ion}$) and the fraction of those photons that escape into the intergalactic medium ($f_{\rm esc}$). Observational estimates of these quantities generally rely on spectroscopy for which large samples with well-defined selection functions are limited. To overcome this challenge, we present and release an implicit likelihood inference pipeline, PHOTONIOn, trained on mock photometry to predict the escaped ionizing luminosity of individual galaxies ($\dot{N}_{\rm ion}$) based on photometric magnitudes and redshifts. We show that PHOTONIOn is able to reliably infer $\dot{N}_{\rm ion}$ from photometry. This is in contrast to traditional SED-fitting approaches which rely on $f_{\rm esc}$ prescriptions that often over-predict $\dot{N}_{\rm ion}$ for LyC-dim galaxies, even when given access to spectroscopic data. We have deployed PHOTONIOn on a sample of 4,559 high-redshift galaxies from the JADES Deep survey, finding gentle redshift evolutions of $\log_{10}(\dot{N}_{\rm ion}) = (0.08\pm0.01)z + (51.60\pm0.06)$ and $\log_{10}(f_{\rm esc}\xi_{\rm ion}) = (0.07\pm0.01)z + (24.12\pm0.07)$. Late-time values for the ionizing photon production rate density are consistent with theoretical models and observations. We measure the evolution of the IGM ionized fraction to find that observed populations of star-forming galaxies are capable of driving reionization in GOODS-S to completion by $z\sim 5.3$ without the need for AGN or other exotic sources, consistent with other studies of the same field. The $20\%$ of UV-brightest galaxies ($M_{\rm UV}<-18.5$) reionize $35\%$ of the survey volume, demonstrating that UV faint LyC emitters are crucial for reionization.

**Link**: [arxiv](http://arxiv.org/abs/2405.09720v2),  [pdf](http://arxiv.org/pdf/2405.09720v2)

**Tags**: astro-ph.GA astro-ph.CO 



### A comparison of shrinkage estimators of the cosmological precision   matrix
**Authors**: Marnix J. Looijmans, Mike Shengbo Wang, Florian Beutler

**Updated**: 2025-01-21T11:02:11Z

**Summary**: The determination of the covariance matrix and its inverse, the precision matrix, is critical in the statistical analysis of cosmological measurements. The covariance matrix is typically estimated with a limited number of simulations at great computational cost before inversion into the precision matrix; therefore, it can be ill-conditioned and overly noisy when the sample size $n$ used for estimation is not much larger than the data vector dimension. In this work, we consider a class of methods known as shrinkage estimation for the precision matrix, which combines an empirical estimate with a target that is either analytical or stochastic. These methods include linear and non-linear shrinkage applied to the covariance matrix (the latter represented by the so-called NERCOME estimator), and the direct linear shrinkage estimation of the precision matrix which we introduce in a cosmological setting. Using Bayesian parameter inference as well as metrics like matrix loss functions and the eigenvalue spectrum, we compare their performance against the standard sample estimator with varying sample size $n$. We have found the shrinkage estimators to significantly improve the posterior distribution at low $n$, especially for the linear shrinkage estimators either inverted from the covariance matrix or applied directly to the precision matrix, with an empirical target constructed from the sample estimate. Our results should be particularly relevant to the analyses of Stage-IV spectroscopic galaxy surveys such as the Dark Energy Spectroscopic Instrument (DESI) and Euclid, whose statistical power can be limited by the computational cost of obtaining an accurate precision matrix estimate.

**Link**: [arxiv](http://arxiv.org/abs/2402.13783v2),  [pdf](http://arxiv.org/pdf/2402.13783v2)

**Tags**: astro-ph.CO astro-ph.IM 



### Bayesian variable selection in sample selection models using   spike-and-slab priors
**Authors**: Adam Iqbal, Emmanuel O. Ogundimu, F. Javier Rubio

**Updated**: 2025-01-21T11:01:24Z

**Summary**: Sample selection models represent a common methodology for correcting bias induced by data missing not at random. These models are not empirically identifiable without exclusion restrictions. In other words, some variables predictive of missingness do not affect the outcome model of interest. The drive to establish this requirement often leads to the inclusion of irrelevant variables in the model. A recent proposal uses adaptive LASSO to circumvent this problem, but its performance depends on the so-called covariance assumption, which can be violated in small to moderate samples. Additionally, there are no tools yet for post-selection inference for this model. To address these challenges, we propose two families of spike-and-slab priors to conduct Bayesian variable selection in sample selection models. These prior structures allow for constructing a Gibbs sampler with tractable conditionals, which is scalable to the dimensions of practical interest. We illustrate the performance of the proposed methodology through a simulation study and present a comparison against adaptive LASSO and stepwise selection. We also provide two applications using publicly available real data. An implementation and code to reproduce the results in this paper can be found at https://github.com/adam-iqbal/selection-spike-slab.

**Link**: [arxiv](http://arxiv.org/abs/2312.03538v3),  [pdf](http://arxiv.org/pdf/2312.03538v3)

**Tags**: stat.CO stat.ME 



### Probing Dark Star Parameters Through $f$-Mode Gravitational Wave Signals
**Authors**: Mariachiara Celato, Christian J. KrÃ¼ger, Kostas D. Kokkotas

**Updated**: 2025-01-21T10:48:55Z

**Summary**: Theoretical models of self-interacting dark matter offer a promising solution to several unresolved issues within the collisionless cold dark matter framework. For asymmetric dark matter, these self-interactions may encourage gravitational collapse, potentially leading to the creation of compact objects primarily composed of dark matter. By considering both fermionic and bosonic equations of state, we analyze the equilibrium structure of non-rotating dark stars, examining their bulk properties and comparing them with baryonic neutron stars. We show that the frequency and damping rate of $f$-mode oscillations of dark compact stars can be expressed in terms of universal functions of stellar mass, radius and moment of inertia. Finally, by employing the universality in the $f$-mode, we propose a scheme to infer accurate values of the physical parameters of dark compact stars from their $f$-mode gravitational wave signal.

**Link**: [arxiv](http://arxiv.org/abs/2501.12031v1),  [pdf](http://arxiv.org/pdf/2501.12031v1)

**Tags**: gr-qc 



### Distilling Calibration via Conformalized Credal Inference
**Authors**: Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone

**Updated**: 2025-01-21T10:48:54Z

**Summary**: Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.

**Link**: [arxiv](http://arxiv.org/abs/2501.06066v2),  [pdf](http://arxiv.org/pdf/2501.06066v2)

**Tags**: cs.LG cs.AI eess.SP 



### TabularARGN: A Flexible and Efficient Auto-Regressive Framework for   Generating High-Fidelity Synthetic Data
**Authors**: Paul Tiwald, Ivona Krchova, Andrey Sidorenko, Mariana Vargas-Vieyra, Mario Scriminaci, Michael Platzer

**Updated**: 2025-01-21T10:06:19Z

**Summary**: Synthetic data generation for tabular datasets must balance fidelity, efficiency, and versatility to meet the demands of real-world applications. We introduce the Tabular Auto-Regressive Generative Network (TabularARGN), a flexible framework designed to handle mixed-type, multivariate, and sequential datasets. By training on all possible conditional probabilities, TabularARGN supports advanced features such as fairness-aware generation, imputation, and conditional generation on any subset of columns. The framework achieves state-of-the-art synthetic data quality while significantly reducing training and inference times, making it ideal for large-scale datasets with diverse structures. Evaluated across established benchmarks, including realistic datasets with complex relationships, TabularARGN demonstrates its capability to synthesize high-quality data efficiently. By unifying flexibility and performance, this framework paves the way for practical synthetic data generation across industries.

**Link**: [arxiv](http://arxiv.org/abs/2501.12012v1),  [pdf](http://arxiv.org/pdf/2501.12012v1)

**Tags**: cs.LG 



### Deep Photometric and Astrometric Investigation of the Non-relaxed Star   Cluster Stock 3 using Gaia DR3
**Authors**: A. Ahmed, Amira R. Youssef, M. S. El-Nawawy, W. H. Elsanhoury

**Updated**: 2025-01-21T09:45:24Z

**Summary**: The study presents both photometric and kinematic analyses of the non-relaxed open cluster Stock 3 with Gaia DR3 which found to be positioned at 2.945 $\pm$ 0.700 kpc and having an age of 16.00 $\pm$ 4.00 Myr. We analyse the data to infer the membership and thus determine the total mass, IMF and the dynamical and kinematical status.

**Link**: [arxiv](http://arxiv.org/abs/2501.11997v1),  [pdf](http://arxiv.org/pdf/2501.11997v1)

**Tags**: astro-ph.SR astro-ph.GA 



### Federated Instruction Tuning of LLMs with Domain Coverage Augmentation
**Authors**: Zezhou Wang, Yaxin Du, Xingjun Ma, Yugang Jiang, Zhuzhong Qian, Siheng Chen

**Updated**: 2025-01-21T09:25:25Z

**Summary**: Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited cross-client private data together with various strategies of instruction augmentation, ultimately boosting model performance within specific domains. To date, the factors affecting FedDIT remain unclear, and existing instruction augmentation methods primarily focus on the centralized setting without considering distributed environments. Our experiments reveal that the cross-client domain coverage, rather than data heterogeneity, drives model performance in FedDIT. In response, we propose FedDCA, which optimizes domain coverage through greedy client center selection and retrieval-based augmentation. At its core, the greedy selection procedure iteratively picks client centers that maximize the diversity and coverage of the instruction space while avoiding redundancy with previously selected centers. This ensures broad yet efficient coverage of the domain distribution across clients. For client-side computational efficiency and system scalability, FedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with server-side feature alignment. Extensive experiments across code, medical, financial, and mathematical domains substantiate the effectiveness of both methods, as well as plug-and-play capability. We further analyze privacy preservation against memory extraction attacks, showing that while privacy leakage risk is independent of augmented public data ratio, it decreases or converges as training progresses.

**Link**: [arxiv](http://arxiv.org/abs/2409.20135v5),  [pdf](http://arxiv.org/pdf/2409.20135v5)

**Tags**: cs.LG cs.CL cs.DC 



### Diffeomorphic ICP Registration for Single and Multiple Point Sets
**Authors**: Adrien Wohrer

**Updated**: 2025-01-21T09:05:46Z

**Summary**: We propose a generalization of the iterative closest point (ICP) algorithm for point set registration, in which the registration functions are non-rigid and follow the large deformation diffeomorphic metric mapping (LDDMM) framework. The algorithm is formulated as a well-posed probabilistic inference, and requires to solve a novel variation of LDDMM landmark registration with an additional term involving the Jacobian of the mapping. The algorithm can easily be generalized to construct a diffeomorphic, statistical atlas of multiple point sets. The method is successfully validated on a first set of synthetic data.

**Link**: [arxiv](http://arxiv.org/abs/2501.11986v1),  [pdf](http://arxiv.org/pdf/2501.11986v1)

**Tags**: eess.SP 



### Linear Feedback Control Systems for Iterative Prompt Optimization in   Large Language Models
**Authors**: Rupesh Raj Karn

**Updated**: 2025-01-21T08:52:47Z

**Summary**: Large Language Models (LLMs) have revolutionized various applications by generating outputs based on given prompts. However, achieving the desired output requires iterative prompt refinement. This paper presents a novel approach that draws parallels between the iterative prompt optimization process in LLMs and feedback control systems. We iteratively refine the prompt by treating the deviation between the LLM output and the desired result as an error term until the output criteria are met. This process is akin to a feedback control system, where the LLM, despite being non-linear and non-deterministic, is managed using principles from linear feedback control systems. We explore the application of different types of controllers within this framework, providing a mathematical foundation for integrating linear feedback control mechanisms with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.11979v1),  [pdf](http://arxiv.org/pdf/2501.11979v1)

**Tags**: cs.LG 



### Leveraging Graph Structures and Large Language Models for End-to-End   Synthetic Task-Oriented Dialogues
**Authors**: Maya Medjad, Hugo Imbert, Bruno Yun, RaphaÃ«l Szymocha, FrÃ©dÃ©ric Armetta

**Updated**: 2025-01-21T08:51:12Z

**Summary**: Training task-oriented dialogue systems is both costly and time-consuming, due to the need for high-quality datasets encompassing diverse intents. Traditional methods depend on extensive human annotation, while recent advancements leverage large language models (LLMs) to generate synthetic data. However, these approaches often require custom prompts or code, limiting accessibility for non-technical users. We introduce GraphTOD, an end-to-end framework that simplifies the generation of task-oriented dialogues. Users can create dialogues by specifying transition graphs in JSON format. Our evaluation demonstrates that GraphTOD generates high-quality dialogues across various domains, significantly lowering the cost and complexity of dataset creation.

**Link**: [arxiv](http://arxiv.org/abs/2501.11977v1),  [pdf](http://arxiv.org/pdf/2501.11977v1)

**Tags**: cs.CL cs.AI 



### AGON: Automated Design Framework for Customizing Processors from ISA   Documents
**Authors**: Chongxiao Li, Di Huang, Pengwei Jin, Tianyun Ma, Husheng Han, Shuyao Cheng, Yifan Hao, Yongwei Zhao, Guanglin Xu, Zidong Du, Rui Zhang, Xiaqing Li, Yuanbo Wen, Xing Hu, Qi Guo

**Updated**: 2025-01-21T08:38:01Z

**Summary**: Customized processors are attractive solutions for vast domain-specific applications due to their high energy efficiency. However, designing a processor in traditional flows is time-consuming and expensive. To address this, researchers have explored methods including the use of agile development tools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming languages like C or SystemC, and more recently, leveraging large language models (LLMs) to generate hardware description language (HDL) code from natural language descriptions. However, each method has limitations in terms of expressiveness, correctness, and performance, leading to a persistent contradiction between the level of automation and the effectiveness of the design. Overall, how to automatically design highly efficient and practical processors with minimal human effort remains a challenge.   In this paper, we propose AGON, a novel framework designed to leverage LLMs for the efficient design of out-of-order (OoO) customized processors with minimal human effort. Central to AGON is the nano-operator function (nOP function) based Intermediate Representation (IR), which bridges high-level descriptions and hardware implementations while decoupling functionality from performance optimization, thereby providing an automatic design framework that is expressive and efficient, has correctness guarantees, and enables PPA (Power, Performance, and Area) optimization.   Experimental results show that superior to previous LLM-assisted automatic design flows, AGON facilitates designing a series of customized OoO processors that achieve on average 2.35 $\times$ speedup compared with BOOM, a general-purpose CPU designed by experts, with minimal design effort.

**Link**: [arxiv](http://arxiv.org/abs/2412.20954v2),  [pdf](http://arxiv.org/pdf/2412.20954v2)

**Tags**: cs.AR 



### F3D-Gaus: Feed-forward 3D-aware Generation on ImageNet with   Cycle-Consistent Gaussian Splatting
**Authors**: Yuxin Wang, Qianyi Wu, Dan Xu

**Updated**: 2025-01-21T08:33:26Z

**Summary**: This paper tackles the problem of generalizable 3D-aware generation from monocular datasets, e.g., ImageNet. The key challenge of this task is learning a robust 3D-aware representation without multi-view or dynamic data, while ensuring consistent texture and geometry across different viewpoints. Although some baseline methods are capable of 3D-aware generation, the quality of the generated images still lags behind state-of-the-art 2D generation approaches, which excel in producing high-quality, detailed images. To address this severe limitation, we propose a novel feed-forward pipeline based on pixel-aligned Gaussian Splatting, coined as F3D-Gaus, which can produce more realistic and reliable 3D renderings from monocular inputs. In addition, we introduce a self-supervised cycle-consistent constraint to enforce cross-view consistency in the learned 3D representation. This training strategy naturally allows aggregation of multiple aligned Gaussian primitives and significantly alleviates the interpolation limitations inherent in single-view pixel-aligned Gaussian Splatting. Furthermore, we incorporate video model priors to perform geometry-aware refinement, enhancing the generation of fine details in wide-viewpoint scenarios and improving the model's capability to capture intricate 3D textures. Extensive experiments demonstrate that our approach not only achieves high-quality, multi-view consistent 3D-aware generation from monocular datasets, but also significantly improves training and inference efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.06714v2),  [pdf](http://arxiv.org/pdf/2501.06714v2)

**Tags**: cs.CV 



### A Hybrid Attention Framework for Fake News Detection with Large Language   Models
**Authors**: Xiaochuan Xu, Peiyang Yu, Zeqiu Xu, Jiani Wang

**Updated**: 2025-01-21T08:26:20Z

**Summary**: With the rapid growth of online information, the spread of fake news has become a serious social challenge. In this study, we propose a novel detection framework based on Large Language Models (LLMs) to identify and classify fake news by integrating textual statistical features and deep semantic features. Our approach utilizes the contextual understanding capability of the large language model for text analysis and introduces a hybrid attention mechanism to focus on feature combinations that are particularly important for fake news identification. Extensive experiments on the WELFake news dataset show that our model significantly outperforms existing methods, with a 1.5\% improvement in F1 score. In addition, we assess the interpretability of the model through attention heat maps and SHAP values, providing actionable insights for content review strategies. Our framework provides a scalable and efficient solution to deal with the spread of fake news and helps build a more reliable online information ecosystem.

**Link**: [arxiv](http://arxiv.org/abs/2501.11967v1),  [pdf](http://arxiv.org/pdf/2501.11967v1)

**Tags**: cs.CL 



### QROA: A Black-Box Query-Response Optimization Attack on LLMs
**Authors**: Hussein Jawad, Nicolas J. -B. BRUNEL

**Updated**: 2025-01-21T08:17:27Z

**Summary**: Large Language Models (LLMs) have surged in popularity in recent months, yet they possess concerning capabilities for generating harmful content when manipulated. This study introduces the Query-Response Optimization Attack (QROA), an optimization-based strategy designed to exploit LLMs through a black-box, query-only interaction. QROA adds an optimized trigger to a malicious instruction to compel the LLM to generate harmful content. Unlike previous approaches, QROA does not require access to the model's logit information or any other internal data and operates solely through the standard query-response interface of LLMs. Inspired by deep Q-learning and Greedy coordinate descent, the method iteratively updates tokens to maximize a designed reward function. We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\%. We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed. This study demonstrates the feasibility of generating jailbreak attacks against deployed LLMs in the public domain using black-box optimization methods, enabling more comprehensive safety testing of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2406.02044v2),  [pdf](http://arxiv.org/pdf/2406.02044v2)

**Tags**: cs.CL cs.LG 



### DehazeGS: Seeing Through Fog with 3D Gaussian Splatting
**Authors**: Jinze Yu, Yiqun Wang, Zhengda Lu, Jianwei Guo, Yong Li, Hongxing Qin, Xiaopeng Zhang

**Updated**: 2025-01-21T08:09:03Z

**Summary**: Current novel view synthesis tasks primarily rely on high-quality and clear images. However, in foggy scenes, scattering and attenuation can significantly degrade the reconstruction and rendering quality. Although NeRF-based dehazing reconstruction algorithms have been developed, their use of deep fully connected neural networks and per-ray sampling strategies leads to high computational costs. Moreover, NeRF's implicit representation struggles to recover fine details from hazy scenes. In contrast, recent advancements in 3D Gaussian Splatting achieve high-quality 3D scene reconstruction by explicitly modeling point clouds into 3D Gaussians. In this paper, we propose leveraging the explicit Gaussian representation to explain the foggy image formation process through a physically accurate forward rendering process. We introduce DehazeGS, a method capable of decomposing and rendering a fog-free background from participating media using only muti-view foggy images as input. We model the transmission within each Gaussian distribution to simulate the formation of fog. During this process, we jointly learn the atmospheric light and scattering coefficient while optimizing the Gaussian representation of the hazy scene. In the inference stage, we eliminate the effects of scattering and attenuation on the Gaussians and directly project them onto a 2D plane to obtain a clear view. Experiments on both synthetic and real-world foggy datasets demonstrate that DehazeGS achieves state-of-the-art performance in terms of both rendering quality and computational efficiency. visualizations are available at https://dehazegs.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2501.03659v4),  [pdf](http://arxiv.org/pdf/2501.03659v4)

**Tags**: cs.CV 



### Modified gravitational wave propagations in linearized gravity with   Lorentz and diffeomorphism violations and their gravitational wave   constraints
**Authors**: Qiang Wang, Jian-Ming Yan, Tao Zhu, Wen Zhao

**Updated**: 2025-01-21T08:07:15Z

**Summary**: The standard model extension (SME) is an effective field theory framework that can be used to study the possible violations of Lorentz symmetry and diffeomorphism invariance in the gravitational interaction. In this paper, we explore both the Lorentz- and diffeomorphism-violating effects on the propagations of gravitational waves in the SME's linearized gravity. It is shown that the violations of Lorentz symmetry and diffeomorphism invariance modify the conventional linear dispersion relation of gravitational waves, leading to anisotropy, birefringence, and dispersion effects in the propagation of gravitational waves. With these modified dispersion relations, we then calculate the dephasing effects due to the Lorentz and diffeomorphism violations in the waveforms of gravitational waves produced by the coalescence of compact binaries. With the distorted waveforms, we perform full Bayesian inference with the help of the open source software \texttt{BILBY} on the gravitational wave events of the compact binary mergers in the LIGO-Virgo-KAGRA catalogs GWTC-3. We consider the effects from the operators with the lowest mass dimension $d=2$ and $d=3$ due to the Lorentz and diffeomorphism violations in the linearized gravity. No signature of Lorentz and diffeomorphism violations arsing from the SME's linearized gravity are found for most GW events, which allows us to give a $90\%$ confidence interval for each Lorentz- and diffeomorphism-violating coefficient.

**Link**: [arxiv](http://arxiv.org/abs/2501.11956v1),  [pdf](http://arxiv.org/pdf/2501.11956v1)

**Tags**: gr-qc hep-th 



### GPU-accelerated LISA parameter estimation with full time domain response
**Authors**: Cecilio GarcÃ­a-QuirÃ³s, Shubhanshu Tiwari, Stanislav Babak

**Updated**: 2025-01-21T08:04:19Z

**Summary**: We conduct the first full Bayesian inference analysis for LISA parameter estimation incorporating the effects of subdominant harmonics and spin-precession through a full time domain response. The substantial computational demands of using time domain waveforms for LISA are significantly mitigated by implementing a novel Python version of the IMRPhenomT family of waveform models and the LISA response with GPU acceleration. This time domain response alleviates the theoretical necessity of developing specific transfer functions to approximate the LISA response in the Fourier domain for each specific type of system and allows for the use of unequal arms configurations and realistic LISA orbits. Our analysis includes a series of zero-noise injections for a Massive Black Hole Binary with aligned and precessing spins. We investigate the impact of including subdominant harmonics, compare equal and unequal arm configurations, and analyze different Time-Delay-Interferometry (TDI) configurations. We utilize full and uniform priors, with a lower frequency cutoff of 0.1mHz, and a signal duration of approximately two months, sampled every 5 seconds. The sampler is initialized based on Fisher estimates. Our results demonstrate LISA capability to measure the two spin magnitudes and the primary spin tilt angle, alongside sky localization, with percent-level precision, while component masses are determined with sub-percent accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.08261v2),  [pdf](http://arxiv.org/pdf/2501.08261v2)

**Tags**: gr-qc astro-ph.IM 



### Proverbs Run in Pairs: Evaluating Proverb Translation Capability of   Large Language Model
**Authors**: Minghan Wang, Viet-Thanh Pham, Farhad Moghimifar, Thuy-Trang Vu

**Updated**: 2025-01-21T07:54:22Z

**Summary**: Despite achieving remarkable performance, machine translation (MT) research remains underexplored in terms of translating cultural elements in languages, such as idioms, proverbs, and colloquial expressions. This paper investigates the capability of state-of-the-art neural machine translation (NMT) and large language models (LLMs) in translating proverbs, which are deeply rooted in cultural contexts. We construct a translation dataset of standalone proverbs and proverbs in conversation for four language pairs. Our experiments show that the studied models can achieve good translation between languages with similar cultural backgrounds, and LLMs generally outperform NMT models in proverb translation. Furthermore, we find that current automatic evaluation metrics such as BLEU, CHRF++ and COMET are inadequate for reliably assessing the quality of proverb translation, highlighting the need for more culturally aware evaluation metrics.

**Link**: [arxiv](http://arxiv.org/abs/2501.11953v1),  [pdf](http://arxiv.org/pdf/2501.11953v1)

**Tags**: cs.CL 



### GLAM: Global-Local Variation Awareness in Mamba-based World Model
**Authors**: Qian He, Wenqi Liang, Chunhui Hao, Gan Sun, Jiandong Tian

**Updated**: 2025-01-21T07:47:03Z

**Summary**: Mimicking the real interaction trajectory in the inference of the world model has been shown to improve the sample efficiency of model-based reinforcement learning (MBRL) algorithms. Many methods directly use known state sequences for reasoning. However, this approach fails to enhance the quality of reasoning by capturing the subtle variation between states. Much like how humans infer trends in event development from this variation, in this work, we introduce Global-Local variation Awareness Mamba-based world model (GLAM) that improves reasoning quality by perceiving and predicting variation between states. GLAM comprises two Mambabased parallel reasoning modules, GMamba and LMamba, which focus on perceiving variation from global and local perspectives, respectively, during the reasoning process. GMamba focuses on identifying patterns of variation between states in the input sequence and leverages these patterns to enhance the prediction of future state variation. LMamba emphasizes reasoning about unknown information, such as rewards, termination signals, and visual representations, by perceiving variation in adjacent states. By integrating the strengths of the two modules, GLAM accounts for highervalue variation in environmental changes, providing the agent with more efficient imagination-based training. We demonstrate that our method outperforms existing methods in normalized human scores on the Atari 100k benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2501.11949v1),  [pdf](http://arxiv.org/pdf/2501.11949v1)

**Tags**: cs.LG 



### Regularization Using Synthetic Data in High-Dimensional Models
**Authors**: Weihao Li, Dongming Huang

**Updated**: 2025-01-21T07:41:54Z

**Summary**: To overcome challenges in fitting complex models with small samples, catalytic priors were recently proposed to stabilize the inference by supplementing observed data with synthetic data generated from simpler models. The resulting Maximum A Posteriori (MAP) estimator is a regularized method that maximizes the weighted likelihood of the combined data. While this estimator is computationally straightforward and empirically promising, its theoretical properties are unexplored. This paper provides a theoretical analysis of this MAP estimator in generalized linear models, focusing on logistic regression. We first establish the existence and stability, even in high dimensions. We then prove the consistency when the dimension of covariates diverges. Furthermore, we use the convex Gaussian min-max theorem to characterize the asymptotic behavior of the MAP estimator when the dimension grows linearly with the sample size. Our theory clarifies the role of the tuning parameters and provides practical guidance, particularly for high-dimensional inference tasks such as constructing confidence intervals and performing variable selection. We demonstrate the effectiveness of our methods on simulations and real-world data. Our work provides a theoretically justified framework for enhancing statistical inference using synthetic data.

**Link**: [arxiv](http://arxiv.org/abs/2407.04194v2),  [pdf](http://arxiv.org/pdf/2407.04194v2)

**Tags**: math.ST stat.TH 



### BlockDialect: Block-wise Fine-grained Mixed Format Quantization for   Energy-Efficient LLM Inference
**Authors**: Wonsuk Jang, Thierry Tambe

**Updated**: 2025-01-21T07:34:54Z

**Summary**: The rapidly increasing size of large language models (LLMs) presents significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with hardware-supported fine-grained scaling emerging as a promising solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. We propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from a formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. To leverage this efficiently, we propose a two-stage approach for online DialectFP4 activation quantization. Importantly, DialectFP4 ensures energy efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit usage per data, while being only 5.45% (2.69%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2501.01144v3),  [pdf](http://arxiv.org/pdf/2501.01144v3)

**Tags**: cs.CL cs.LG 



### Webvs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students
**Authors**: Aayush Kumar, Daniel Prol, Amin Alipour, Sruti Srinivasa Ragavan

**Updated**: 2025-01-21T07:16:18Z

**Summary**: LLMs such as ChatGPT have been widely adopted by students in higher education as tools for learning programming and related concepts. However, it remains unclear how effective students are and what strategies students use while learning with LLMs. Since the majority of students' experiences in online self-learning have come through using search engines such as Google, evaluating AI tools in this context can help us address these gaps. In this mixed methods research, we conducted an exploratory within-subjects study to understand how CS2 students learn programming concepts using both LLMs as well as traditional online methods such as educational websites and videos to examine how students approach learning within and across both scenarios. We discovered that students found it easier to learn a more difficult concept using traditional methods than using ChatGPT. We also found that students ask fewer follow-ups and use more keyword-based queries for search engines while their prompts to LLMs tend to explicitly ask for information.

**Link**: [arxiv](http://arxiv.org/abs/2501.11935v1),  [pdf](http://arxiv.org/pdf/2501.11935v1)

**Tags**: cs.HC cs.AI 



### RUIE: Retrieval-based Unified Information Extraction using Large   Language Model
**Authors**: Xincheng Liao, Junwen Duan, Yixi Huang, Jianxin Wang

**Updated**: 2025-01-21T07:09:35Z

**Summary**: Unified information extraction (UIE) aims to extract diverse structured information from unstructured text. While large language models (LLMs) have shown promise for UIE, they require significant computational resources and often struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based Unified Information Extraction), a framework that leverages in-context learning for efficient task generalization. RUIE introduces a novel demonstration selection mechanism combining LLM preferences with a keyword-enhanced reward model, and employs a bi-encoder retriever trained through contrastive learning and knowledge distillation. As the first trainable retrieval framework for UIE, RUIE serves as a universal plugin for various LLMs. Experimental results on eight held-out datasets demonstrate RUIE's effectiveness, with average F1-score improvements of 19.22 and 3.22 compared to instruction-tuning methods and other retrievers, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2409.11673v2),  [pdf](http://arxiv.org/pdf/2409.11673v2)

**Tags**: cs.CL 



### ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation
**Authors**: Peter Devine

**Updated**: 2025-01-21T07:07:58Z

**Summary**: Retrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains.   We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models.   By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively.   Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.

**Link**: [arxiv](http://arxiv.org/abs/2501.11929v1),  [pdf](http://arxiv.org/pdf/2501.11929v1)

**Tags**: cs.LG 



### Multi-Agent Consensus Seeking via Large Language Models
**Authors**: Huaben Chen, Wenkang Ji, Lufeng Xu, Shiyu Zhao

**Updated**: 2025-01-21T06:26:43Z

**Summary**: Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: windylab.github.io/ConsensusLLM/.

**Link**: [arxiv](http://arxiv.org/abs/2310.20151v2),  [pdf](http://arxiv.org/pdf/2310.20151v2)

**Tags**: cs.CL cs.RO cs.SY eess.SY 



### Integrate Temporal Graph Learning into LLM-based Temporal Knowledge   Graph Model
**Authors**: He Chang, Jie Wu, Zhulin Tao, Yunshan Ma, Xianglin Huang, Tat-Seng Chua

**Updated**: 2025-01-21T06:12:49Z

**Summary**: Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events based on the observed events in history. Recently, Large Language Models (LLMs) have exhibited remarkable capabilities, generating significant research interest in their application for reasoning over temporal knowledge graphs (TKGs). Existing LLM-based methods have integrated retrieved historical facts or static graph representations into LLMs. Despite the notable performance of LLM-based methods, they are limited by the insufficient modeling of temporal patterns and ineffective cross-modal alignment between graph and language, hindering the ability of LLMs to fully grasp the temporal and structural information in TKGs. To tackle these issues, we propose a novel framework TGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge graph model. Specifically, we introduce temporal graph learning to capture the temporal and relational patterns and obtain the historical graph embedding. Furthermore, we design a hybrid graph tokenization to sufficiently model the temporal patterns within LLMs. To achieve better alignment between graph and language, we employ a two-stage training paradigm to finetune LLMs on high-quality and diverse data, thereby resulting in better performance. Extensive experiments on three real-world datasets show that our approach outperforms a range of state-of-the-art (SOTA) methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.11911v1),  [pdf](http://arxiv.org/pdf/2501.11911v1)

**Tags**: cs.IR 



### LogLLM: Log-based Anomaly Detection Using Large Language Models
**Authors**: Wei Guan, Jian Cao, Shiyou Qian, Jianqi Gao, Chun Ouyang

**Updated**: 2025-01-21T06:12:26Z

**Summary**: Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.

**Link**: [arxiv](http://arxiv.org/abs/2411.08561v2),  [pdf](http://arxiv.org/pdf/2411.08561v2)

**Tags**: cs.SE cs.AI 



### Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with   Synthetic Images
**Authors**: Zhuoran Yu, Chenchen Zhu, Sean Culatana, Raghuraman Krishnamoorthi, Fanyi Xiao, Yong Jae Lee

**Updated**: 2025-01-21T06:03:07Z

**Summary**: Recent advances in generative deep learning have enabled the creation of high-quality synthetic images in text-to-image generation. Prior work shows that fine-tuning a pretrained diffusion model on ImageNet and generating synthetic training images from the finetuned model can enhance an ImageNet classifier's performance. However, performance degrades as synthetic images outnumber real ones. In this paper, we explore whether generative fine-tuning is essential for this improvement and whether it is possible to further scale up training using more synthetic data. We present a new framework leveraging off-the-shelf generative models to generate synthetic training images, addressing multiple challenges: class name ambiguity, lack of diversity in naive prompts, and domain shifts. Specifically, we leverage large language models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we propose contextualized diversification (CD) and stylized diversification (SD) methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage domain adaptation techniques with auxiliary batch normalization for synthetic images. Our framework consistently enhances recognition model performance with more synthetic data, up to 6x of original ImageNet size showcasing the potential of synthetic data for improved recognition models and strong out-of-domain generalization.

**Link**: [arxiv](http://arxiv.org/abs/2312.02253v2),  [pdf](http://arxiv.org/pdf/2312.02253v2)

**Tags**: cs.CV cs.AI cs.LG 



### Towards LifeSpan Cognitive Systems
**Authors**: Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang, Gholamreza Haffari, Heng Ji, Julian McAuley

**Updated**: 2025-01-21T05:50:19Z

**Summary**: Building a human-like system that continuously interacts with complex environments -- whether simulated digital worlds or human society -- presents several key challenges. Central to this is enabling continuous, high-frequency interactions, where the interactions are termed experiences. We refer to this envisioned system as the LifeSpan Cognitive System (LSCS). A critical feature of LSCS is its ability to engage in incremental and rapid updates while retaining and accurately recalling past experiences. In this paper we focus on the domain of Large Language Models (LLMs), where we identify two major challenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention with Accurate Recall. These properties are essential for storing new experiences, organizing past experiences, and responding to the environment in ways that leverage relevant historical data. Unlike language models with continual learning, which typically rely on large corpora for fine-tuning and focus on improving performance within specific domains or tasks, LSCS must rapidly and incrementally update with new information from its environment at a high frequency. Existing technologies with the potential of solving the above two major challenges can be classified into four classes based on a conceptual metric called Storage Complexity, which measures the relative space required to store past experiences. Each of these four classes of technologies has its own strengths and limitations while we argue none of them alone can achieve LSCS alone. To this end, we propose a potential instantiation for LSCS that can integrate all four classes of technologies. The new instantiation, serving as a conjecture, operates through two core processes: Absorbing Experiences and Generating Responses.

**Link**: [arxiv](http://arxiv.org/abs/2409.13265v2),  [pdf](http://arxiv.org/pdf/2409.13265v2)

**Tags**: cs.CL 



### Panoramic Interests: Stylistic-Content Aware Personalized Headline   Generation
**Authors**: Junhong Lian, Xiang Ao, Xinyu Liu, Yang Liu, Qing He

**Updated**: 2025-01-21T05:30:20Z

**Summary**: Personalized news headline generation aims to provide users with attention-grabbing headlines that are tailored to their preferences. Prevailing methods focus on user-oriented content preferences, but most of them overlook the fact that diverse stylistic preferences are integral to users' panoramic interests, leading to suboptimal personalization. In view of this, we propose a novel Stylistic-Content Aware Personalized Headline Generation (SCAPE) framework. SCAPE extracts both content and stylistic features from headlines with the aid of large language model (LLM) collaboration. It further adaptively integrates users' long- and short-term interests through a contrastive learning-based hierarchical fusion network. By incorporating the panoramic interests into the headline generator, SCAPE reflects users' stylistic-content preferences during the generation process. Extensive experiments on the real-world dataset PENS demonstrate the superiority of SCAPE over baselines.

**Link**: [arxiv](http://arxiv.org/abs/2501.11900v1),  [pdf](http://arxiv.org/pdf/2501.11900v1)

**Tags**: cs.CL cs.AI 



### Informed total-error-minimizing priors: Interpretable cosmological   parameter constraints despite complex nuisance effects
**Authors**: Bernardita Ried Guachalla, Dylan Britt, Daniel Gruen, Oliver Friedrich

**Updated**: 2025-01-21T05:20:21Z

**Summary**: While Bayesian inference techniques are standard in cosmological analyses, it is common to interpret resulting parameter constraints with a frequentist intuition. This intuition can fail, for example, when marginalizing high-dimensional parameter spaces onto subsets of parameters, because of what has come to be known as projection effects or prior volume effects. We present the method of informed total-error-minimizing (ITEM) priors to address this problem. An ITEM prior is a prior distribution on a set of nuisance parameters, such as those describing astrophysical or calibration systematics, intended to enforce the validity of a frequentist interpretation of the posterior constraints derived for a set of target parameters (e.g., cosmological parameters). Our method works as follows. For a set of plausible nuisance realizations, we generate target parameter posteriors using several different candidate priors for the nuisance parameters. We reject candidate priors that do not accomplish the minimum requirements of bias (of point estimates) and coverage (of confidence regions among a set of noisy realizations of the data) for the target parameters on one or more of the plausible nuisance realizations. Of the priors that survive this cut, we select the ITEM prior as the one that minimizes the total error of the marginalized posteriors of the target parameters. As a proof of concept, we applied our method to the density split statistics measured in Dark Energy Survey Year 1 data. We demonstrate that the ITEM priors substantially reduce prior volume effects that otherwise arise and that they allow for sharpened yet robust constraints on the parameters of interest.

**Link**: [arxiv](http://arxiv.org/abs/2405.00261v2),  [pdf](http://arxiv.org/pdf/2405.00261v2)

**Tags**: astro-ph.CO 



### Scalable Bayesian Physics-Informed Kolmogorov-Arnold Networks
**Authors**: Zhiwei Gao, George Em Karniadakis

**Updated**: 2025-01-21T04:51:11Z

**Summary**: Uncertainty quantification (UQ) plays a pivotal role in scientific machine learning, especially when surrogate models are used to approximate complex systems. Although multilayer perceptions (MLPs) are commonly employed as surrogates, they often suffer from overfitting due to their large number of parameters. Kolmogorov-Arnold networks (KANs) offer an alternative solution with fewer parameters. However, gradient-based inference methods, such as Hamiltonian Monte Carlo (HMC), may result in computational inefficiency when applied to KANs, especially for large-scale datasets, due to the high cost of back-propagation. To address these challenges, we propose a novel approach, combining the dropout Tikhonov ensemble Kalman inversion (DTEKI) with Chebyshev KANs. This gradient-free method effectively mitigates overfitting and enhances numerical stability. Additionally, we incorporate the active subspace method to reduce the parameter-space dimensionality, allowing us to improve the accuracy of predictions and obtain more reliable uncertainty estimates. Extensive experiments demonstrate the efficacy of our approach in various test cases, including scenarios with large datasets and high noise levels. Our results show that the new method achieves comparable or better accuracy, much higher efficiency as well as stability compared to HMC, in addition to scalability. Moreover, by leveraging the low-dimensional parameter subspace, our method preserves prediction accuracy while substantially reducing further the computational cost.

**Link**: [arxiv](http://arxiv.org/abs/2501.08501v2),  [pdf](http://arxiv.org/pdf/2501.08501v2)

**Tags**: math.NA cs.LG cs.NA 



### Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and   Reasoning of Evidence-Based Medicine
**Authors**: Keer Lu, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang

**Updated**: 2025-01-21T04:40:43Z

**Summary**: In recent years, Large Language Models (LLMs) have exhibited remarkable capabilities in clinical scenarios. However, despite their potential, existing works face challenges when applying LLMs to medical settings. Strategies relying on training with medical datasets are highly cost-intensive and may suffer from outdated training data. Leveraging external knowledge bases is a suitable alternative, yet it faces obstacles such as limited retrieval precision and poor effectiveness in answer extraction. These issues collectively prevent LLMs from demonstrating the expected level of proficiency in mastering medical expertise. To address these challenges, we introduce Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as the selection and reasoning processes of evidence, thereby enhancing the problem-solving capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2 achieves a 14.87\% improvement over vanilla RAG methods and even a 3.59\% enhancement compared to fine-tuning strategies, without incurring additional training costs.

**Link**: [arxiv](http://arxiv.org/abs/2501.11885v1),  [pdf](http://arxiv.org/pdf/2501.11885v1)

**Tags**: cs.CL 



### Investigating Recurrent Transformers with Dynamic Halt
**Authors**: Jishnu Ray Chowdhury, Cornelia Caragea

**Updated**: 2025-01-21T04:20:26Z

**Summary**: In this paper, we comprehensively study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism: (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformers and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks, such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference. The code is released in: https://github.com/JRC1995/InvestigatingRecurrentTransformers/tree/main

**Link**: [arxiv](http://arxiv.org/abs/2402.00976v4),  [pdf](http://arxiv.org/pdf/2402.00976v4)

**Tags**: cs.LG cs.AI cs.NE 



### From Drafts to Answers: Unlocking LLM Potential via Aggregation   Fine-Tuning
**Authors**: Yafu Li, Zhilin Wang, Tingchen Fu, Ganqu Cui, Sen Yang, Yu Cheng

**Updated**: 2025-01-21T04:11:59Z

**Summary**: Scaling data and model size has been proven effective for boosting the performance of large language models. In addition to training-time scaling, recent studies have revealed that increasing test-time computational resources can further improve performance. In this work, we introduce Aggregation Fine-Tuning (AFT), a supervised finetuning paradigm where the model learns to synthesize multiple draft responses, referred to as proposals, into a single, refined answer, termed aggregation. At inference time, a propose-and-aggregate strategy further boosts performance by iteratively generating proposals and aggregating them. Empirical evaluations on benchmark datasets show that AFT-trained models substantially outperform standard SFT. Notably, an AFT model, fine-tuned from Llama3.1-8B-Base with only 64k data, achieves a 41.3% LC win rate on AlpacaEval 2, surpassing significantly larger LLMs such as Llama3.1-405B-Instruct and GPT4. By combining sequential refinement and parallel sampling, the propose-and-aggregate framework scales inference-time computation in a flexible manner. Overall, These findings position AFT as a promising approach to unlocking additional capabilities of LLMs without resorting to increasing data volume or model size.

**Link**: [arxiv](http://arxiv.org/abs/2501.11877v1),  [pdf](http://arxiv.org/pdf/2501.11877v1)

**Tags**: cs.CL cs.AI 



### Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges
**Authors**: Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes

**Updated**: 2025-01-21T04:10:13Z

**Summary**: Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different 'examtaker models' - both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores.

**Link**: [arxiv](http://arxiv.org/abs/2406.12624v5),  [pdf](http://arxiv.org/pdf/2406.12624v5)

**Tags**: cs.CL cs.AI 



### FLAME: Learning to Navigate with Multimodal LLM in Urban Environments
**Authors**: Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang

**Updated**: 2025-01-21T04:06:09Z

**Summary**: Large Language Models (LLMs) have demonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for route summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAME's superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion on Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs) in complex navigation tasks, representing an advancement towards applications of MLLMs in the field of embodied intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2408.11051v2),  [pdf](http://arxiv.org/pdf/2408.11051v2)

**Tags**: cs.CV cs.AI cs.CL cs.RO 



### Demons in the Detail: On Implementing Load Balancing Loss for Training   Specialized Mixture-of-Expert Models
**Authors**: Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, Junyang Lin

**Updated**: 2025-01-21T04:04:39Z

**Summary**: This paper revisits the implementation of $\textbf{L}$oad-$\textbf{b}$alancing $\textbf{L}$oss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E \sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$ represents the frequency of expert $i$ being selected, and $p_i$ denotes the average gating score of the expert $i$. Existing MoE training frameworks usually employ the parallel training strategy so that $f_i$ and the LBL are calculated within a $\textbf{micro-batch}$ and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence ($\textit{e.g.}$, code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a $\textbf{global-batch}$ to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize $f_i$ across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to $\textbf{42.8B}$ total parameters and $\textbf{400B}$ tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.

**Link**: [arxiv](http://arxiv.org/abs/2501.11873v1),  [pdf](http://arxiv.org/pdf/2501.11873v1)

**Tags**: cs.LG cs.CL 



### Grid: Omni Visual Generation
**Authors**: Cong Wan, Xiangyang Luo, Hao Luo, Zijian Cai, Yiren Song, Yunlong Zhao, Yifan Bai, Yuhang He, Yihong Gong

**Updated**: 2025-01-21T04:00:36Z

**Summary**: Visual generation has witnessed remarkable progress in single-image tasks, yet extending these capabilities to temporal sequences remains challenging. Current approaches either build specialized video models from scratch with enormous computational costs or add separate motion modules to image generators, both requiring learning temporal dynamics anew. We observe that modern image generation models possess underutilized potential in handling structured layouts with implicit temporal understanding. Building on this insight, we introduce GRID, which reformulates temporal sequences as grid layouts, enabling holistic processing of visual sequences while leveraging existing model capabilities. Through a parallel flow-matching training strategy with coarse-to-fine scheduling, our approach achieves up to 67 faster inference speeds while using <1/1000 of the computational resources compared to specialized models. Extensive experiments demonstrate that GRID not only excels in temporal tasks from Text-to-Video to 3D Editing but also preserves strong performance in image generation, establishing itself as an efficient and versatile omni-solution for visual generation.

**Link**: [arxiv](http://arxiv.org/abs/2412.10718v4),  [pdf](http://arxiv.org/pdf/2412.10718v4)

**Tags**: cs.CV 



### Automatic Debiased Machine Learning for Smooth Functionals of   Nonparametric M-Estimands
**Authors**: Lars van der Laan, Aurelien Bibaut, Nathan Kallus, Alex Luedtke

**Updated**: 2025-01-21T03:50:51Z

**Summary**: We propose a unified framework for automatic debiased machine learning (autoDML) to perform inference on smooth functionals of infinite-dimensional M-estimands, defined as population risk minimizers over Hilbert spaces. By automating debiased estimation and inference procedures in causal inference and semiparametric statistics, our framework enables practitioners to construct valid estimators for complex parameters without requiring specialized expertise. The framework supports Neyman-orthogonal loss functions with unknown nuisance parameters requiring data-driven estimation, as well as vector-valued M-estimands involving simultaneous loss minimization across multiple Hilbert space models. We formalize the class of parameters efficiently estimable by autoDML as a novel class of nonparametric projection parameters, defined via orthogonal minimum loss objectives. We introduce three autoDML estimators based on one-step estimation, targeted minimum loss-based estimation, and the method of sieves. For data-driven model selection, we derive a novel decomposition of model approximation error for smooth functionals of M-estimands and propose adaptive debiased machine learning estimators that are superefficient and adaptive to the functional form of the M-estimand. Finally, we illustrate the flexibility of our framework by constructing autoDML estimators for the long-term survival under a beta-geometric model.

**Link**: [arxiv](http://arxiv.org/abs/2501.11868v1),  [pdf](http://arxiv.org/pdf/2501.11868v1)

**Tags**: stat.ME math.ST stat.ML stat.TH 



### A White Dwarf Binary Candidate Discovered by LAMOST Using Dynamical   Method
**Authors**: Haifan Zhu, Wei Wang, Xue Lib, Jia-jia Li, Pengfu Tian

**Updated**: 2025-01-21T03:44:40Z

**Summary**: We present the discovery of a binary system containing a white dwarf candidate using data from the LAMOST. Our analysis of the radial velocity data allowed us to determine an orbital period of approximately 0.953 days and a mass function of 0.129 $M_\odot$. Through spectral energy distribution (SED) fitting, we obtained the stellar parameters of the visible star. By combining these results with the mass function, we established a relationship between the mass of the invisible star and the system's inclination angle, along with the Roche lobe radius. We find that the mass of the invisible star is below the Chandrasekhar limit when the inclination angle exceeds $35^\circ$. Given that systems with large variations in radial velocity typically have high inclination angles, we classify the invisible star as a white dwarf candidate. The Roche lobe radius exceeds the physical radius of the visible star, indicating that no mass transfer occurs, which results in a weak ellipsoidal modulation effect. Additionally, we obtained light curves from the TESS, ASAS-SN, and CRTS surveys. The light curves also exhibit a periodicity of approximately 0.95 days, with ellipsoidal modulation only in the 2019 TESS observations. Coupled with the strong $\rm H_{\alpha}$ emission line observed in the LAMOST MRS spectrum, we infer that the surface of the visible star contains significant hot spots. This obscures the system's inherently weak ellipsoidal modulation, resulting in a manifestation of rotational variables. Furthermore, an analysis of the dynamical characteristics of this system indicates that it has a high inclination angle ($>60$ degrees) and its orbital properties are consistent with those of typical thin disk stars, supporting the hypothesis that the invisible object is a white dwarf.

**Link**: [arxiv](http://arxiv.org/abs/2501.11865v1),  [pdf](http://arxiv.org/pdf/2501.11865v1)

**Tags**: astro-ph.SR 



### LLM-Agents Driven Automated Simulation Testing and Analysis of small   Uncrewed Aerial Systems
**Authors**: Venkata Sai Aswath Duvvuru, Bohan Zhang, Michael Vierhauser, Ankit Agrawal

**Updated**: 2025-01-21T03:42:21Z

**Summary**: Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that AutoSimTest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.

**Link**: [arxiv](http://arxiv.org/abs/2501.11864v1),  [pdf](http://arxiv.org/pdf/2501.11864v1)

**Tags**: cs.SE 



### Customizing Language Models with Instance-wise LoRA for Sequential   Recommendation
**Authors**: Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He

**Updated**: 2025-01-21T03:40:14Z

**Summary**: Sequential recommendation systems predict the next interaction item based on users' past interactions, aligning recommendations with individual preferences. Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation. A common paradigm is converting user behavior sequences into instruction data, and fine-tuning the LLM with parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA). However, the uniform application of LoRA across diverse user behaviors is insufficient to capture individual variability, resulting in negative transfer between disparate sequences. To address these challenges, we propose Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation task as a form of multi-task learning, integrating LoRA with the Mixture of Experts (MoE) framework. This approach encourages different experts to capture various aspects of user behavior. Additionally, we introduce a sequence representation guided gate function that generates customized expert participation weights for each user sequence, which allows dynamic parameter adjustment for instance-wise recommendations. In sequential recommendation, iLoRA achieves an average relative improvement of 11.4\% over basic LoRA in the hit ratio metric, with less than a 1\% relative increase in trainable parameters. Extensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in mitigating negative transfer and improving recommendation accuracy. Our data and code are available at https://github.com/AkaliKong/iLoRA.

**Link**: [arxiv](http://arxiv.org/abs/2408.10159v4),  [pdf](http://arxiv.org/pdf/2408.10159v4)

**Tags**: cs.IR cs.AI 



### A Planet Candidate Orbiting near the Hot Jupiter TOI-2818 b Inferred   through Transit Timing
**Authors**: Brendan J. McKee, Benjamin T. Montet, Samuel W. Yee, Joel D. Hartman, Joshua N. Winn, Jorge H. C. Martins, AndrÃ© M. Silva, Alexander L. Wallace

**Updated**: 2025-01-21T03:40:01Z

**Summary**: TOI-2818 b is a hot Jupiter orbiting a slightly evolved G-type star on a 4.04-day orbit that shows transit timing variations (TTVs) suggestive of a decreasing orbital period. In the most recent year of TESS observations, transits were observed $\sim$8 minutes earlier than expected for a constant period. The implied orbital decay rate is $1.35 \pm 0.25$ s yr$^{-1}$, too fast to be explained by tidal dissipation even considering the evolved nature of the host star. Radial velocity monitoring rules out the possibility that the apparent change in period is due to a steady acceleration of the star by a long-period companion. Apsidal precession due to the tidal distortion of the planet is also physically implausible. The most plausible explanation for the TTVs appears to be gravitational perturbations from a hitherto undetected planet with mass $\lesssim$$10\,M_\oplus$ that is in (or near) a mean-motion resonance with the hot Jupiter. Such a planet could be responsible for the observed TTVs while avoiding detection with the available radial velocity and transit data.

**Link**: [arxiv](http://arxiv.org/abs/2411.04192v2),  [pdf](http://arxiv.org/pdf/2411.04192v2)

**Tags**: astro-ph.EP 



### Towards Explainable Vulnerability Detection with Large Language Models
**Authors**: Qiheng Mao, Zhenhao Li, Xing Hu, Kui Liu, Xin Xia, Jianling Sun

**Updated**: 2025-01-21T03:27:58Z

**Summary**: Software vulnerabilities pose significant risks to the security and integrity of software systems. Although prior studies have explored vulnerability detection using deep learning and pre-trained models, these approaches often fail to provide the detailed explanations necessary for developers to understand and remediate vulnerabilities effectively. The advent of large language models (LLMs) has introduced transformative potential due to their advanced generative capabilities and ability to comprehend complex contexts, offering new possibilities for addressing these challenges. In this paper, we propose LLMVulExp, an automated framework designed to specialize LLMs for the dual tasks of vulnerability detection and explanation. To address the challenges of acquiring high-quality annotated data and injecting domain-specific knowledge, LLMVulExp leverages prompt-based techniques for annotating vulnerability explanations and finetunes LLMs using instruction tuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect vulnerability types in code while generating detailed explanations, including the cause, location, and repair suggestions. Additionally, we employ a Chain-of-Thought (CoT) based key code extraction strategy to focus LLMs on analyzing vulnerability-prone code, further enhancing detection accuracy and explanatory depth. Our experimental results demonstrate that LLMVulExp achieves over a 90% F1 score on the SeVC dataset, effectively combining high detection accuracy with actionable and coherent explanations. This study highlights the feasibility of utilizing LLMs for real-world vulnerability detection and explanation tasks, providing critical insights into their adaptation and application in software security.

**Link**: [arxiv](http://arxiv.org/abs/2406.09701v3),  [pdf](http://arxiv.org/pdf/2406.09701v3)

**Tags**: cs.SE 



### EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents
**Authors**: Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun

**Updated**: 2025-01-21T03:22:10Z

**Summary**: Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.

**Link**: [arxiv](http://arxiv.org/abs/2501.11858v1),  [pdf](http://arxiv.org/pdf/2501.11858v1)

**Tags**: cs.CV cs.CL 



### Network-informed Prompt Engineering against Organized Astroturf   Campaigns under Extreme Class Imbalance
**Authors**: Nikos Kanakaris, Heng Ping, Xiongye Xiao, Nesreen K. Ahmed, Luca Luceri, Emilio Ferrara, Paul Bogdan

**Updated**: 2025-01-21T03:07:21Z

**Summary**: Detecting organized political campaigns is of paramount importance in fighting against disinformation on social media. Existing approaches for the identification of such organized actions employ techniques mostly from network science, graph machine learning and natural language processing. Their ultimate goal is to analyze the relationships and interactions (e.g. re-posting) among users and the textual similarities of their posts. Despite their effectiveness in recognizing astroturf campaigns, these methods face significant challenges, notably the class imbalance in available training datasets. To mitigate this issue, recent methods usually resort to data augmentation or increasing the number of positive samples, which may not always be feasible or sufficient in real-world settings. Following a different path, in this paper, we propose a novel framework for identifying astroturf campaigns based solely on large language models (LLMs), introducing a Balanced Retrieval-Augmented Generation (Balanced RAG) component. Our approach first gives both textual information concerning the posts (in our case tweets) and the user interactions of the social network as input to a language model. Then, through prompt engineering and the proposed Balanced RAG method, it effectively detects coordinated disinformation campaigns on X (Twitter). The proposed framework does not require any training or fine-tuning of the language model. Instead, by strategically harnessing the strengths of prompt engineering and Balanced RAG, it facilitates LLMs to overcome the effects of class imbalance and effectively identify coordinated political campaigns. The experimental results demonstrate that by incorporating the proposed prompt engineering and Balanced RAG methods, our framework outperforms the traditional graph-based baselines, achieving 2x-3x improvements in terms of precision, recall and F1 scores.

**Link**: [arxiv](http://arxiv.org/abs/2501.11849v1),  [pdf](http://arxiv.org/pdf/2501.11849v1)

**Tags**: cs.CL cs.AI cs.SI 



### A Survey on Memory-Efficient Large-Scale Model Training in AI for   Science
**Authors**: Kaiyuan Tian, Linbo Qiao, Baihui Liu, Gongqingjian Jiang, Dongsheng Li

**Updated**: 2025-01-21T03:06:30Z

**Summary**: Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. To address this, we review memory-efficient training techniques for LLMs based on the transformer architecture, including distributed training, mixed precision training, and gradient checkpointing. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. We also discuss the challenges of memory optimization in practice and potential future directions, hoping to provide valuable insights for researchers and engineers.

**Link**: [arxiv](http://arxiv.org/abs/2501.11847v1),  [pdf](http://arxiv.org/pdf/2501.11847v1)

**Tags**: cs.LG cs.AI 



### Survey on Monocular Metric Depth Estimation
**Authors**: Jiuling Zhang

**Updated**: 2025-01-21T02:51:10Z

**Summary**: Monocular Depth Estimation (MDE) is a fundamental computer vision task underpinning applications such as spatial understanding, 3D reconstruction, and autonomous driving. While deep learning-based MDE methods can predict relative depth from a single image, their lack of metric scale information often results in scale inconsistencies, limiting their utility in downstream tasks like visual SLAM, 3D reconstruction, and novel view synthesis. Monocular Metric Depth Estimation (MMDE) addresses these challenges by enabling precise, scene-scale depth inference. MMDE improves depth consistency, enhances sequential task stability, simplifies integration into downstream applications, and broadens practical use cases. This paper provides a comprehensive review of depth estimation technologies, highlighting the evolution from geometry-based methods to state-of-the-art deep learning approaches. It emphasizes advancements in scale-agnostic methods, which are crucial for enabling zero-shot generalization as the foundational capability for MMDE. Recent progress in zero-shot MMDE research is explored, focusing on challenges such as model generalization and the loss of detail at scene boundaries. Innovative strategies to address these issues include unlabelled data augmentation, image patching, architectural optimization, and generative techniques. These advancements, analyzed in detail, demonstrate significant contributions to overcoming existing limitations. Finally, this paper synthesizes recent developments in zero-shot MMDE, identifies unresolved challenges, and outlines future research directions. By offering a clear roadmap and cutting-edge insights, this work aims to deepen understanding of MMDE, inspire novel applications, and drive technological innovation.

**Link**: [arxiv](http://arxiv.org/abs/2501.11841v1),  [pdf](http://arxiv.org/pdf/2501.11841v1)

**Tags**: cs.CV 



### Large Language Models with Human-In-The-Loop Validation for Systematic   Review Data Extraction
**Authors**: Noah L. Schroeder, Chris Davis Jaldi, Shan Zhang

**Updated**: 2025-01-21T02:49:43Z

**Summary**: Systematic reviews are time-consuming endeavors. Historically speaking, knowledgeable humans have had to screen and extract data from studies before it can be analyzed. However, large language models (LLMs) hold promise to greatly accelerate this process. After a pilot study which showed great promise, we investigated the use of freely available LLMs for extracting data for systematic reviews. Using three different LLMs, we extracted 24 types of data, 9 explicitly stated variables and 15 derived categorical variables, from 112 studies that were included in a published scoping review. Overall we found that Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably well, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with human coding, respectively. While promising, these results highlight the dire need for a human-in-the-loop (HIL) process for AI-assisted data extraction. As a result, we present a free, open-source program we developed (AIDE) to facilitate user-friendly, HIL data extraction with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.11840v1),  [pdf](http://arxiv.org/pdf/2501.11840v1)

**Tags**: cs.HC 



### CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based   Verification
**Authors**: Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song

**Updated**: 2025-01-21T02:45:49Z

**Summary**: Large Language Models (LLMs) have made significant progress in code generation, offering developers groundbreaking automated programming support. However, LLMs often generate code that is syntactically correct and even semantically plausible, but may not execute as expected or fulfill specified requirements. This phenomenon of hallucinations in the code domain has not been systematically explored. To advance the community's understanding and research on this issue, we introduce the concept of code hallucinations and propose a classification method for code hallucination based on execution verification. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, with each category further divided into different subcategories to understand and address the unique challenges faced by LLMs in code generation with finer granularity. Additionally, we present a dynamic detection algorithm called CodeHalu designed to detect and quantify code hallucinations. We also introduce the CodeHaluEval benchmark, which includes 8,883 samples from 699 tasks, to systematically and quantitatively evaluate code hallucinations. By evaluating 17 popular LLMs using this benchmark, we reveal significant differences in their accuracy and reliability in code generation, offering detailed insights for further improving the code generation capabilities of LLMs. The CodeHalu benchmark and code are publicly available at https://github.com/yuchen814/CodeHalu.

**Link**: [arxiv](http://arxiv.org/abs/2405.00253v4),  [pdf](http://arxiv.org/pdf/2405.00253v4)

**Tags**: cs.CL cs.SE 



## Keyword: LLM Deployment 
 ### Is Long Context All You Need? Leveraging LLM's Extended Context for   NL2SQL
**Authors**: Yeounoh Chung, Gaurav T. Kakkar, Yu Gan, Brenton Milne, Fatma Ozcan

**Updated**: 2025-01-21T18:52:15Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive capabilities across a range of natural language processing tasks. In particular, improvements in reasoning abilities and the expansion of context windows have opened new avenues for leveraging these powerful models. NL2SQL is challenging in that the natural language question is inherently ambiguous, while the SQL generation requires a precise understanding of complex data schema and semantics. One approach to this semantic ambiguous problem is to provide more and sufficient contextual information.   In this work, we explore the performance and the latency trade-offs of the extended context window (a.k.a., long context) offered by Google's state-of-the-art LLM (\textit{gemini-1.5-pro}). We study the impact of various contextual information, including column example values, question and SQL query pairs, user-provided hints, SQL documentation, and schema. To the best of our knowledge, this is the first work to study how the extended context window and extra contextual information can help NL2SQL generation with respect to both accuracy and latency cost. We show that long context LLMs are robust and do not get lost in the extended contextual information. Additionally, our long-context NL2SQL pipeline based on Google's \textit{gemini-pro-1.5} achieve a strong performance with 67.41\% on BIRD benchmark (dev) without finetuning and expensive self-consistency based techniques.

**Link**: [arxiv](http://arxiv.org/abs/2501.12372v1),  [pdf](http://arxiv.org/pdf/2501.12372v1)

**Tags**: cs.DB cs.AI 



### Bridging the Training-Inference Gap in LLMs by Leveraging Self-Generated   Tokens
**Authors**: Zhepeng Cen, Yao Liu, Siliang Zeng, Pratik Chaudhari, Huzefa Rangwala, George Karypis, Rasool Fakoor

**Updated**: 2025-01-21T18:14:30Z

**Summary**: Language models are often trained to maximize the likelihood of the next token given past tokens in the training dataset. However, during inference time, they are utilized differently, generating text sequentially and auto-regressively by using previously generated tokens as input to predict the next one. Marginal differences in predictions at each step can cascade over successive steps, resulting in different distributions from what the models were trained for and potentially leading to unpredictable behavior. This paper proposes two simple approaches based on model own generation to address this discrepancy between the training and inference time. Our first approach is Batch-Scheduled Sampling, where, during training, we stochastically choose between the ground-truth token from the dataset and the model's own generated token as input to predict the next token. This is done in an offline manner, modifying the context window by interleaving ground-truth tokens with those generated by the model. Our second approach is Reference-Answer-based Correction, where we explicitly incorporate a self-correction capability into the model during training. This enables the model to effectively self-correct the gaps between the generated sequences and the ground truth data without relying on an external oracle model. By incorporating our proposed strategies during training, we have observed an overall improvement in performance compared to baseline methods, as demonstrated by our extensive experiments using summarization, general question-answering, and math question-answering tasks.

**Link**: [arxiv](http://arxiv.org/abs/2410.14655v2),  [pdf](http://arxiv.org/pdf/2410.14655v2)

**Tags**: cs.LG 



### Treefix: Enabling Execution with a Tree of Prefixes
**Authors**: Beatriz Souza, Michael Pradel

**Updated**: 2025-01-21T18:13:43Z

**Summary**: The ability to execute code is a prerequisite for various dynamic program analyses. Learning-guided execution has been proposed as an approach to enable the execution of arbitrary code snippets by letting a neural model predict likely values for any missing variables. Although state-of-the-art learning-guided execution approaches, such as LExecutor, can enable the execution of a relative high amount of code, they are limited to predicting a restricted set of possible values and do not use any feedback from previous executions to execute even more code. This paper presents Treefix, a novel learning-guided execution approach that leverages LLMs to iteratively create code prefixes that enable the execution of a given code snippet. The approach addresses the problem in a multi-step fashion, where each step uses feedback about the code snippet and its execution to instruct an LLM to improve a previously generated prefix. This process iteratively creates a tree of prefixes, a subset of which is returned to the user as prefixes that maximize the number of executed lines in the code snippet. In our experiments with two datasets of Python code snippets, Treefix achieves 25% and 7% more coverage relative to the current state of the art in learning-guided execution, covering a total of 84% and 82% of all lines in the code snippets.

**Link**: [arxiv](http://arxiv.org/abs/2501.12339v1),  [pdf](http://arxiv.org/pdf/2501.12339v1)

**Tags**: cs.SE cs.AI 



### Quantum Compressive Sensing Meets Quantum Noise: A Practical Exploration
**Authors**: Naveed Naimipour, Collin Frink, Harry Shaw, Haleh Safavi, Mojtaba Soltanalian

**Updated**: 2025-01-21T18:10:03Z

**Summary**: Compressive sensing is a signal processing technique that enables the reconstruction of sparse signals from a limited number of measurements, leveraging the signal's inherent sparsity to facilitate efficient recovery. Recent works on the Quantum Compressive Sensing (QCS) architecture, a quantum data-driven approach to compressive sensing where the state of the tensor network is represented by a quantum state over a set of entangled qubits, have shown promise in advancing quantum data-driven methods for compressive sensing. However, the QCS framework has remained largely untested on quantum computing resources or in the presence of quantum noise. In this work, we present a practical implementation of QCS on Amazon Braket, utilizing the Quantum Imaginary Time Evolution (QITE) projection technique to assess the framework's capabilities under quantum noise. We outline the necessary modifications to the QCS framework for deployment on Amazon Braket, followed by results under four types of quantum noise. Finally, we discuss potential long-term directions aimed at unlocking the full potential of quantum compressive sensing for applications such as signal recovery and image processing.

**Link**: [arxiv](http://arxiv.org/abs/2501.12335v1),  [pdf](http://arxiv.org/pdf/2501.12335v1)

**Tags**: quant-ph 



### Automatic Labelling with Open-source LLMs using Dynamic Label Schema   Integration
**Authors**: Thomas Walshe, Sae Young Moon, Chunyang Xiao, Yawwani Gunawardana, Fran Silavong

**Updated**: 2025-01-21T18:06:54Z

**Summary**: Acquiring labelled training data remains a costly task in real world machine learning projects to meet quantity and quality requirements. Recently Large Language Models (LLMs), notably GPT-4, have shown great promises in labelling data with high accuracy. However, privacy and cost concerns prevent the ubiquitous use of GPT-4. In this work, we explore effectively leveraging open-source models for automatic labelling. We identify integrating label schema as a promising technology but found that naively using the label description for classification leads to poor performance on high cardinality tasks. To address this, we propose Retrieval Augmented Classification (RAC) for which LLM performs inferences for one label at a time using corresponding label schema; we start with the most related label and iterates until a label is chosen by the LLM. We show that our method, which dynamically integrates label description, leads to performance improvements in labelling tasks. We further show that by focusing only on the most promising labels, RAC can trade off between label quality and coverage - a property we leverage to automatically label our internal datasets.

**Link**: [arxiv](http://arxiv.org/abs/2501.12332v1),  [pdf](http://arxiv.org/pdf/2501.12332v1)

**Tags**: cs.CL cs.AI cs.LG 



### A recent evaluation on the performance of LLMs on radiation oncology   physics using questions of randomly shuffled options
**Authors**: Peilong Wang, Jason Holmes, Zhengliang Liu, Dequan Chen, Tianming Liu, Jiajian Shen, Wei Liu

**Updated**: 2025-01-21T17:20:31Z

**Summary**: Purpose: We present an updated study evaluating the performance of large language models (LLMs) in answering radiation oncology physics questions, focusing on the recently released models.   Methods: A set of 100 multiple-choice radiation oncology physics questions, previously created by a well-experienced physicist, was used for this study. The answer options of the questions were randomly shuffled to create "new" exam sets. Five LLMs -- OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet -- with the versions released before September 30, 2024, were queried using these new exam sets. To evaluate their deductive reasoning ability, the correct answer options in the questions were replaced with "None of the above." Then, the explain-first and step-by-step instruction prompts were used to test if this strategy improved their reasoning ability. The performance of the LLMs was compared with the answers from medical physicists.   Results: All models demonstrated expert-level performance on these questions, with o1-preview even surpassing medical physicists with a majority vote. When replacing the correct answer options with 'None of the above', all models exhibited a considerable decline in performance, suggesting room for improvement. The explain-first and step-by-step instruction prompts helped enhance the reasoning ability of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5 Sonnet models.   Conclusion: These recently released LLMs demonstrated expert-level performance in answering radiation oncology physics questions, exhibiting great potential to assist in radiation oncology physics education and training.

**Link**: [arxiv](http://arxiv.org/abs/2412.10622v3),  [pdf](http://arxiv.org/pdf/2412.10622v3)

**Tags**: physics.med-ph cs.AI 



### LLM-Assisted Knowledge Graph Completion for Curriculum and Domain   Modelling in Personalized Higher Education Recommendations
**Authors**: Hasan Abu-Rasheed, Constance Jumbo, Rashed Al Amin, Christian Weber, Veit Wiese, Roman Obermaisser, Madjid Fathi

**Updated**: 2025-01-21T17:13:13Z

**Summary**: While learning personalization offers great potential for learners, modern practices in higher education require a deeper consideration of domain models and learning contexts, to develop effective personalization algorithms. This paper introduces an innovative approach to higher education curriculum modelling that utilizes large language models (LLMs) for knowledge graph (KG) completion, with the goal of creating personalized learning-path recommendations. Our research focuses on modelling university subjects and linking their topics to corresponding domain models, enabling the integration of learning modules from different faculties and institutions in the student's learning path. Central to our approach is a collaborative process, where LLMs assist human experts in extracting high-quality, fine-grained topics from lecture materials. We develop a domain, curriculum, and user models for university modules and stakeholders. We implement this model to create the KG from two study modules: Embedded Systems and Development of Embedded Systems Using FPGA. The resulting KG structures the curriculum and links it to the domain models. We evaluate our approach through qualitative expert feedback and quantitative graph quality metrics. Domain experts validated the relevance and accuracy of the model, while the graph quality metrics measured the structural properties of our KG. Our results show that the LLM-assisted graph completion approach enhances the ability to connect related courses across disciplines to personalize the learning experience. Expert feedback also showed high acceptance of the proposed collaborative approach for concept extraction and classification.

**Link**: [arxiv](http://arxiv.org/abs/2501.12300v1),  [pdf](http://arxiv.org/pdf/2501.12300v1)

**Tags**: cs.HC cs.AI 



### Condor: Enhance LLM Alignment with Knowledge-Driven Data Synthesis and   Refinement
**Authors**: Maosong Cao, Taolin Zhang, Mo Li, Chuyu Zhang, Yunxin Liu, Haodong Duan, Songyang Zhang, Kai Chen

**Updated**: 2025-01-21T16:44:12Z

**Summary**: The quality of Supervised Fine-Tuning (SFT) data plays a critical role in enhancing the conversational capabilities of Large Language Models (LLMs). However, as LLMs become more advanced, the availability of high-quality human-annotated SFT data has become a significant bottleneck, necessitating a greater reliance on synthetic training data. In this work, we introduce Condor, a novel two-stage synthetic data generation framework that incorporates World Knowledge Tree and Self-Reflection Refinement to produce high-quality SFT data at scale. Our experimental results demonstrate that a base model fine-tuned on only 20K Condor-generated samples achieves superior performance compared to counterparts. The additional refinement stage in Condor further enables iterative self-improvement for LLMs at various scales (up to 72B), validating the effectiveness of our approach. Furthermore, our investigation into the scaling for synthetic data in post-training reveals substantial unexplored potential for performance improvements, opening promising avenues for future research.

**Link**: [arxiv](http://arxiv.org/abs/2501.12273v1),  [pdf](http://arxiv.org/pdf/2501.12273v1)

**Tags**: cs.CL cs.AI 



### mmCooper: A Multi-agent Multi-stage Communication-efficient and   Collaboration-robust Cooperative Perception Framework
**Authors**: Bingyi Liu, Jian Teng, Hongfei Xue, Enshu Wang, Chuanhui Zhu, Pu Wang, Libing Wu

**Updated**: 2025-01-21T16:34:16Z

**Summary**: Collaborative perception significantly enhances individual vehicle perception performance through the exchange of sensory information among agents. However, real-world deployment faces challenges due to bandwidth constraints and inevitable calibration errors during information exchange. To address these issues, we propose mmCooper, a novel multi-agent, multi-stage, communication-efficient, and collaboration-robust cooperative perception framework. Our framework leverages a multi-stage collaboration strategy that dynamically and adaptively balances intermediate- and late-stage information to share among agents, enhancing perceptual performance while maintaining communication efficiency. To support robust collaboration despite potential misalignments and calibration errors, our framework captures multi-scale contextual information for robust fusion in the intermediate stage and calibrates the received detection results to improve accuracy in the late stage. We validate the effectiveness of mmCooper through extensive experiments on real-world and simulated datasets. The results demonstrate the superiority of our proposed framework and the effectiveness of each component.

**Link**: [arxiv](http://arxiv.org/abs/2501.12263v1),  [pdf](http://arxiv.org/pdf/2501.12263v1)

**Tags**: cs.CV 



### Hire Me or Not? Examining Language Model's Behavior with Occupation   Attributes
**Authors**: Damin Zhang, Yi Zhang, Geetanjali Bihani, Julia Rayz

**Updated**: 2025-01-21T16:15:37Z

**Summary**: With the impressive performance in various downstream tasks, large language models (LLMs) have been widely integrated into production pipelines, like recruitment and recommendation systems. A known issue of models trained on natural language data is the presence of human biases, which can impact the fairness of the system. This paper investigates LLMs' behavior with respect to gender stereotypes, in the context of occupation decision making. Our framework is designed to investigate and quantify the presence of gender stereotypes in LLMs' behavior via multi-round question answering. Inspired by prior works, we construct a dataset by leveraging a standard occupation classification knowledge base released by authoritative agencies. We tested three LLMs (RoBERTa-large, GPT-3.5-turbo, and Llama2-70b-chat) and found that all models exhibit gender stereotypes analogous to human biases, but with different preferences. The distinct preferences of GPT-3.5-turbo and Llama2-70b-chat may imply the current alignment methods are insufficient for debiasing and could introduce new biases contradicting the traditional gender stereotypes.

**Link**: [arxiv](http://arxiv.org/abs/2405.06687v3),  [pdf](http://arxiv.org/pdf/2405.06687v3)

**Tags**: cs.CL 



### ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via   Mechanistic Interpretability
**Authors**: Zhongxiang Sun, Xiaoxue Zang, Kai Zheng, Yang Song, Jun Xu, Xiao Zhang, Weijie Yu, Yang Song, Han Li

**Updated**: 2025-01-21T16:05:30Z

**Summary**: Retrieval-Augmented Generation (RAG) models are designed to incorporate external knowledge, reducing hallucinations caused by insufficient parametric (internal) knowledge. However, even with accurate and relevant retrieved content, RAG models can still produce hallucinations by generating outputs that conflict with the retrieved information. Detecting such hallucinations requires disentangling how Large Language Models (LLMs) utilize external and parametric knowledge. Current detection methods often focus on one of these mechanisms or without decoupling their intertwined effects, making accurate detection difficult. In this paper, we investigate the internal mechanisms behind hallucinations in RAG scenarios. We discover hallucinations occur when the Knowledge FFNs in LLMs overemphasize parametric knowledge in the residual stream, while Copying Heads fail to effectively retain or integrate external knowledge from retrieved content. Based on these findings, we propose ReDeEP, a novel method that detects hallucinations by decoupling LLM's utilization of external context and parametric knowledge. Our experiments show that ReDeEP significantly improves RAG hallucination detection accuracy. Additionally, we introduce AARF, which mitigates hallucinations by modulating the contributions of Knowledge FFNs and Copying Heads.

**Link**: [arxiv](http://arxiv.org/abs/2410.11414v2),  [pdf](http://arxiv.org/pdf/2410.11414v2)

**Tags**: cs.CL 



### FOCUS: First Order Concentrated Updating Scheme
**Authors**: Yizhou Liu, Ziming Liu, Jeff Gore

**Updated**: 2025-01-21T16:03:42Z

**Summary**: Large language models (LLMs) demonstrate remarkable performance, and improving their pre-training process appears to be key to enhancing their capabilities further. Based on the documented success of Adam, learning rate decay, and weight decay, we hypothesize that the pre-training loss landscape features a narrowing valley structure. Through experiments with synthetic loss functions, we discover that when gradient query noise is high relative to the valley's sharpness, Adam's performance falls behind that of Signum because Adam reduces the effective step size too drastically. This observation led us to develop FOCUS, an optimizer that enhances Signum by incorporating attraction toward moving averaged parameters, allowing it to handle noise better while maintaining larger step sizes. In training GPT-2, FOCUS proves to be more stable than Signum and faster than Adam. These results suggest that gradient noise may be an underappreciated limiting factor in LLM training, and FOCUS offers promising solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12243v1),  [pdf](http://arxiv.org/pdf/2501.12243v1)

**Tags**: cs.LG cs.CL math.OC 



### CDW-CoT: Clustered Distance-Weighted Chain-of-Thoughts Reasoning
**Authors**: Yuanheng Fang, Guoqing Chao, Wenqiang Lei, Shaobo Li, Dianhui Chu

**Updated**: 2025-01-21T15:51:07Z

**Summary**: Large Language Models (LLMs) have recently achieved impressive results in complex reasoning tasks through Chain of Thought (CoT) prompting. However, most existing CoT methods rely on using the same prompts, whether manually designed or automatically generated, to handle the entire dataset. This one-size-fits-all approach may fail to meet the specific needs arising from the diversities within a single dataset. To solve this problem, we propose the Clustered Distance-Weighted Chain of Thought (CDW-CoT) method, which dynamically constructs prompts tailored to the characteristics of each data instance by integrating clustering and prompt optimization techniques. Our method employs clustering algorithms to categorize the dataset into distinct groups, from which a candidate pool of prompts is selected to reflect the inherent diversity within the dataset. For each cluster, CDW-CoT trains the optimal prompt probability distribution tailored to their specific characteristics. Finally, it dynamically constructs a unique prompt probability distribution for each test instance, based on its proximity to cluster centers, from which prompts are selected for reasoning. CDW-CoT consistently outperforms traditional CoT methods across six datasets, including commonsense, symbolic, and mathematical reasoning tasks. Specifically, when compared to manual CoT, CDW-CoT achieves an average accuracy improvement of 25.34% on LLaMA2 (13B) and 15.72% on LLaMA3 (8B).

**Link**: [arxiv](http://arxiv.org/abs/2501.12226v1),  [pdf](http://arxiv.org/pdf/2501.12226v1)

**Tags**: cs.LG 



### Leveraging Large Language Models for Realizing Truly Intelligent User   Interfaces
**Authors**: Allard Oelen, SÃ¶ren Auer

**Updated**: 2025-01-21T15:47:32Z

**Summary**: The number of published scholarly articles is growing at a significant rate, making scholarly knowledge organization increasingly important. Various approaches have been proposed to organize scholarly information, including describing scholarly knowledge semantically leveraging knowledge graphs. Transforming unstructured knowledge, presented within articles, to structured and semantically represented knowledge generally requires human intelligence and labor since natural language processing methods alone typically do not render sufficient precision and recall for many applications. With the recent developments of Large Language Models (LLMs), it becomes increasingly possible to provide truly intelligent user interfaces guiding humans in the transformation process. We present an approach to integrate non-intrusive LLMs guidance into existing user interfaces. More specifically, we integrate LLM-supported user interface components into an existing scholarly knowledge infrastructure. Additionally, we provide our experiences with LLM integration, detailing best practices and obstacles. Finally, we evaluate the approach using a small-scale user evaluation with domain experts.

**Link**: [arxiv](http://arxiv.org/abs/2501.12221v1),  [pdf](http://arxiv.org/pdf/2501.12221v1)

**Tags**: cs.DL 



### NoteLLM-2: Multimodal Large Representation Models for Recommendation
**Authors**: Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, Enhong Chen

**Updated**: 2025-01-21T15:40:43Z

**Summary**: Large Language Models (LLMs) have demonstrated exceptional proficiency in text understanding and embedding tasks. However, their potential in multimodal representation, particularly for item-to-item (I2I) recommendations, remains underexplored. While leveraging existing Multimodal Large Language Models (MLLMs) for such tasks is promising, challenges arise due to their delayed release compared to corresponding LLMs and the inefficiency in representation tasks. To address these issues, we propose an end-to-end fine-tuning method that customizes the integration of any existing LLMs and vision encoders for efficient multimodal representation. Preliminary experiments revealed that fine-tuned LLMs often neglect image content. To counteract this, we propose NoteLLM-2, a novel framework that enhances visual information. Specifically, we propose two approaches: first, a prompt-based method that segregates visual and textual content, employing a multimodal In-Context Learning strategy to balance focus across modalities; second, a late fusion technique that directly integrates visual information into the final representations. Extensive experiments, both online and offline, demonstrate the effectiveness of our approach. Code is available at https://github.com/Applied-Machine-Learning-Lab/NoteLLM.

**Link**: [arxiv](http://arxiv.org/abs/2405.16789v2),  [pdf](http://arxiv.org/pdf/2405.16789v2)

**Tags**: cs.IR 



### VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction
**Authors**: Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He

**Updated**: 2025-01-21T15:36:41Z

**Summary**: Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a significant challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing our method against state-of-the-art counterparts across benchmarks for image, video, and speech tasks, we demonstrate that our model is equipped with both strong visual and speech capabilities, making near real-time vision and speech interaction.

**Link**: [arxiv](http://arxiv.org/abs/2501.01957v3),  [pdf](http://arxiv.org/pdf/2501.01957v3)

**Tags**: cs.CV cs.SD eess.AS 



### You Can't Eat Your Cake and Have It Too: The Performance Degradation of   LLMs with Jailbreak Defense
**Authors**: Wuyuao Mai, Geng Hong, Pei Chen, Xudong Pan, Baojun Liu, Yuan Zhang, Haixin Duan, Min Yang

**Updated**: 2025-01-21T15:24:29Z

**Summary**: With the rise of generative large language models (LLMs) like LLaMA and ChatGPT, these models have significantly transformed daily life and work by providing advanced insights. However, as jailbreak attacks continue to circumvent built-in safety mechanisms, exploiting carefully crafted scenarios or tokens, the safety risks of LLMs have come into focus. While numerous defense strategies--such as prompt detection, modification, and model fine-tuning--have been proposed to counter these attacks, a critical question arises: do these defenses compromise the utility and usability of LLMs for legitimate users? Existing research predominantly focuses on the effectiveness of defense strategies without thoroughly examining their impact on performance, leaving a gap in understanding the trade-offs between LLM safety and performance. Our research addresses this gap by conducting a comprehensive study on the utility degradation, safety elevation, and exaggerated-safety escalation of LLMs with jailbreak defense strategies. We propose USEBench, a novel benchmark designed to evaluate these aspects, along with USEIndex, a comprehensive metric for assessing overall model performance. Through experiments on seven state-of-the-art LLMs, we found that mainstream jailbreak defenses fail to ensure both safety and performance simultaneously. Although model-finetuning performs the best overall, their effectiveness varies across LLMs. Furthermore, vertical comparisons reveal that developers commonly prioritize performance over safety when iterating or fine-tuning their LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.12210v1),  [pdf](http://arxiv.org/pdf/2501.12210v1)

**Tags**: cs.CR 



### TDAG: A Multi-Agent Framework based on Dynamic Task Decomposition and   Agent Generation
**Authors**: Yaoxiang Wang, Zhiyong Wu, Junfeng Yao, Jinsong Su

**Updated**: 2025-01-21T15:11:41Z

**Summary**: The emergence of Large Language Models (LLMs) like ChatGPT has inspired the development of LLM-based agents capable of addressing complex, real-world tasks. However, these agents often struggle during task execution due to methodological constraints, such as error propagation and limited adaptability. To address this issue, we propose a multi-agent framework based on dynamic Task Decomposition and Agent Generation (TDAG). This framework dynamically decomposes complex tasks into smaller subtasks and assigns each to a specifically generated subagent, thereby enhancing adaptability in diverse and unpredictable real-world tasks. Simultaneously, existing benchmarks often lack the granularity needed to evaluate incremental progress in complex, multi-step tasks. In response, we introduce ItineraryBench in the context of travel planning, featuring interconnected, progressively complex tasks with a fine-grained evaluation system. ItineraryBench is designed to assess agents' abilities in memory, planning, and tool usage across tasks of varying complexity. Our experimental results reveal that TDAG significantly outperforms established baselines, showcasing its superior adaptability and context awareness in complex task scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2402.10178v2),  [pdf](http://arxiv.org/pdf/2402.10178v2)

**Tags**: cs.CL 



### FLARE: Faithful Logic-Aided Reasoning and Exploration
**Authors**: Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein

**Updated**: 2025-01-21T14:57:22Z

**Summary**: Modern Question Answering (QA) and Reasoning approaches based on Large Language Models (LLMs) commonly use prompting techniques, such as Chain-of-Thought (CoT), assuming the resulting generation will have a more granular exploration and reasoning over the question space and scope. However, such methods struggle with generating outputs that are faithful to the intermediate chain of reasoning produced by the model. On the other end of the spectrum, neuro-symbolic methods such as Faithful CoT (F-CoT) propose to combine LLMs with external symbolic solvers. While such approaches boast a high degree of faithfulness, they usually require a model trained for code generation and struggle with tasks that are ambiguous or hard to formalise strictly. We introduce $\textbf{F}$aithful $\textbf{L}$ogic-$\textbf{A}$ided $\textbf{R}$easoning and $\textbf{E}$xploration ($\textbf{FLARE}$), a novel interpretable approach for traversing the problem space using task decompositions. We use the LLM to plan a solution, soft-formalise the query into facts and predicates using a logic programming code and simulate that code execution using an exhaustive multi-hop search over the defined space. Our method allows us to compute the faithfulness of the reasoning process w.r.t. the generated code and analyse the steps of the multi-hop search without relying on external solvers. Our methods achieve SOTA results on $\mathbf{7}$ out of $\mathbf{9}$ diverse reasoning benchmarks. We also show that model faithfulness positively correlates with overall performance and further demonstrate that $\textbf{FLARE}$ allows pinpointing the decisive factors sufficient for and leading to the correct answer with optimal reasoning during the multi-hop search.

**Link**: [arxiv](http://arxiv.org/abs/2410.11900v4),  [pdf](http://arxiv.org/pdf/2410.11900v4)

**Tags**: cs.AI cs.CL cs.LG cs.LO 



### Reinforcement Learning from Human Feedback without Reward Inference:   Model-Free Algorithm and Instance-Dependent Analysis
**Authors**: Qining Zhang, Honghao Wei, Lei Ying

**Updated**: 2025-01-21T14:53:36Z

**Summary**: In this paper, we study reinforcement learning from human feedback (RLHF) under an episodic Markov decision process with a general trajectory-wise reward model. We developed a model-free RLHF best policy identification algorithm, called $\mathsf{BSAD}$, without explicit reward model inference, which is a critical intermediate step in the contemporary RLHF paradigms for training large language models (LLM). The algorithm identifies the optimal policy directly from human preference information in a backward manner, employing a dueling bandit sub-routine that constantly duels actions to identify the superior one. $\mathsf{BSAD}$ adopts a reward-free exploration and best-arm-identification-like adaptive stopping criteria to equalize the visitation among all states in the same decision step while moving to the previous step as soon as the optimal action is identifiable, leading to a provable, instance-dependent sample complexity $\tilde{\mathcal{O}}(c_{\mathcal{M}}SA^3H^3M\log\frac{1}{\delta})$ which resembles the result in classic RL, where $c_{\mathcal{M}}$ is the instance-dependent constant and $M$ is the batch size. Moreover, $\mathsf{BSAD}$ can be transformed into an explore-then-commit algorithm with logarithmic regret and generalized to discounted MDPs using a frame-based approach. Our results show: (i) sample-complexity-wise, RLHF is not significantly harder than classic RL and (ii) end-to-end RLHF may deliver improved performance by avoiding pitfalls in reward inferring such as overfit and distribution shift.

**Link**: [arxiv](http://arxiv.org/abs/2406.07455v2),  [pdf](http://arxiv.org/pdf/2406.07455v2)

**Tags**: cs.LG stat.ML 



### The Impact of Copyrighted Material on Large Language Models: A Norwegian   Perspective
**Authors**: Javier de la Rosa, Vladislav Mikhailov, Lemei Zhang, Freddy Wetjen, David Samuel, Peng Liu, Rolv-Arild Braaten, Petter MÃ¦hlum, Magnus Breder Birkenes, Andrey Kutuzov, Tita Enstad, Hans Christian FarsethÃ¥s, Svein Arne Brygfjeld, Jon Atle Gulla, Stephan Oepen, Erik Velldal, Wilfred Ãstgulen, Liljia Ãvrelid, Aslak Sira Myhre

**Updated**: 2025-01-21T14:38:35Z

**Summary**: The use of copyrighted materials in training language models raises critical legal and ethical questions. This paper presents a framework for and the results of empirically assessing the impact of publisher-controlled copyrighted corpora on the performance of generative large language models (LLMs) for Norwegian. When evaluated on a diverse set of tasks, we found that adding both books and newspapers to the data mixture of LLMs tend to improve their performance, while the addition of fiction works seems to be detrimental. Our experiments could inform the creation of a compensation scheme for authors whose works contribute to AI development.

**Link**: [arxiv](http://arxiv.org/abs/2412.09460v2),  [pdf](http://arxiv.org/pdf/2412.09460v2)

**Tags**: cs.CL 



### Power-Efficient RAN Intelligent Controllers Through Optimized KPI   Monitoring
**Authors**: JoÃ£o Paulo S. H. Lima, George N. Katsaros, Konstantinos Nikitopoulos

**Updated**: 2025-01-21T14:34:31Z

**Summary**: The Open Radio Access Network (RAN) paradigm envisions a more flexible, interoperable, and intelligent RAN ecosystem via new open interfaces and elements like the RAN Intelligent Controller (RIC). However, the impact of these elements on Open RAN's power consumption remains heavily unexplored. This work for the first time evaluates the impact of Key Performance Indicator (KPI) monitoring on RIC's power consumption using real traffic and power measurements. By analyzing various RIC-RAN communication scenarios, we identify that RIC's power consumption can become a scalability bottleneck, particularly in large-scale deployments, even when RIC is limited to its core operational functionalities and without incorporating application-specific processes. In this context, also for the first time we explore potential power savings through the elimination of redundant KPI transmissions, extending existing techniques for identical subscription removal and KPI selection, achieving significant power consumption gains exceeding 87\% of the overall RIC power consumption.

**Link**: [arxiv](http://arxiv.org/abs/2501.09509v2),  [pdf](http://arxiv.org/pdf/2501.09509v2)

**Tags**: eess.SY cs.SY 68M12, 90B18 C.2.1; C.2.3; D.4.8 



### BiMarker: Enhancing Text Watermark Detection for Large Language Models   with Bipolar Watermarks
**Authors**: Zhuang Li

**Updated**: 2025-01-21T14:32:50Z

**Summary**: The rapid proliferation of Large Language Models (LLMs) has raised concerns about misuse and the challenges of distinguishing AI-generated text from human-written content. Existing watermarking techniques, such as \kgw, still face limitations under low watermark strength, stringent false-positive requirements, and low-entropy scenarios. Our analysis reveals that current detection methods rely on coarse estimates of non-watermarked text, which constrains watermark detectability. We propose the Bipolar Watermark (BiMarker), a novel approach that divides generated text into positive and negative poles, leveraging the difference in green token counts for detection. This differential mechanism significantly enhances the detectability of watermarked text. Theoretical analysis and experimental results demonstrate BiMarker's effectiveness and compatibility with existing optimization techniques, offering a new optimization dimension for watermarking in LLM-generated content.

**Link**: [arxiv](http://arxiv.org/abs/2501.12174v1),  [pdf](http://arxiv.org/pdf/2501.12174v1)

**Tags**: cs.LG 



### AdaServe: SLO-Customized LLM Serving with Fine-Grained Speculative   Decoding
**Authors**: Zikun Li, Zhuofu Chen, Remi Delacourt, Gabriele Oliaro, Zeyu Wang, Qinghan Chen, Shuhuai Lin, April Yang, Zhihao Zhang, Zhuoming Chen, Sean Lai, Xupeng Miao, Zhihao Jia

**Updated**: 2025-01-21T14:15:01Z

**Summary**: This paper introduces AdaServe, the first LLM serving system to support SLO customization through fine-grained speculative decoding. AdaServe leverages the logits of a draft model to predict the speculative accuracy of tokens and employs a theoretically optimal algorithm to construct token trees for verification. To accommodate diverse SLO requirements without compromising throughput, AdaServe employs a speculation-and-selection scheme that first constructs candidate token trees for each request and then dynamically selects tokens to meet individual SLO constraints while optimizing throughput. Comprehensive evaluations demonstrate that AdaServe achieves up to 73% higher SLO attainment and 74% higher goodput compared to state-of-the-art systems. These results underscore AdaServe's potential to enhance the efficiency and adaptability of LLM deployments across varied application scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.12162v1),  [pdf](http://arxiv.org/pdf/2501.12162v1)

**Tags**: cs.CL cs.AI cs.DC cs.LG 



### Contextualizing Recommendation Explanations with LLMs: A User Study
**Authors**: Yuanjun Feng, Stefan Feuerriegel, Yash Raj Shrestha

**Updated**: 2025-01-21T14:02:39Z

**Summary**: Large language models (LLMs) are increasingly prevalent in recommender systems, where LLMs can be used to generate personalized recommendations. Here, we examine how different LLM-generated explanations for movie recommendations affect users' perceptions of cognitive, affective, and utilitarian needs and consumption intentions. In a pre-registered, between-subject online experiment (N=759) and follow-up interviews (N=30), we compare (a) LLM-generated generic explanations, and (b) LLM-generated contextualized explanations. Our findings show that contextualized explanations (i.e., explanations that incorporate users' past behaviors) effectively meet users' cognitive needs while increasing users' intentions to watch recommended movies. However, adding explanations offers limited benefits in meeting users' utilitarian and affective needs, raising concerns about the proper design and implications of LLM-generated explanations. Qualitative insights from interviews reveal that referencing users' past preferences enhances trust and understanding but can feel excessive if overused. Furthermore, users with more active and positive engagement with the recommender system and movie-watching get substantial gains from contextualized explanations. Overall, our research clarifies how LLM-generated recommendations influence users' motivations and behaviors, providing valuable insights for the future development of user-centric recommender systems, a key element in social media platforms and online ecosystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.12152v1),  [pdf](http://arxiv.org/pdf/2501.12152v1)

**Tags**: cs.HC 



### Improving Influence-based Instruction Tuning Data Selection for Balanced   Learning of Diverse Capabilities
**Authors**: Qirun Dai, Dylan Zhang, Jiaqi W. Ma, Hao Peng

**Updated**: 2025-01-21T14:00:43Z

**Summary**: Selecting appropriate training data is crucial for effective instruction fine-tuning of large language models (LLMs), which aims to (1) elicit strong capabilities, and (2) achieve balanced performance across a diverse range of tasks. Influence-based methods show promise in achieving (1) by estimating the contribution of each training example to the model's predictions, but often struggle with (2). Our systematic investigation reveals that this underperformance can be attributed to an inherent bias where certain tasks intrinsically have greater influence than others. As a result, data selection is often biased towards these tasks, not only hurting the model's performance on others but also, counterintuitively, harms performance on these high-influence tasks themselves.   As a remedy, we propose BIDS, a Balanced and Influential Data Selection algorithm. BIDS first normalizes influence scores of the training data, and then iteratively balances data selection by choosing the training example with the highest influence on the most underrepresented task. Experiments with both Llama-3 and Mistral-v0.3 on seven benchmarks spanning five diverse capabilities show that BIDS consistently outperforms both state-of-the-art influence-based algorithms and other non-influence-based selection frameworks. Surprisingly, training on a 15% subset selected by BIDS can even outperform full-dataset training with a much more balanced performance. Our analysis further highlights the importance of both instance-level normalization and iterative optimization of selected data for balanced learning of diverse capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2501.12147v1),  [pdf](http://arxiv.org/pdf/2501.12147v1)

**Tags**: cs.CL cs.AI cs.LG I.2.7 



### Do LLMs Provide Links to Code Similar to what they Generate? A Study   with Gemini and Bing CoPilot
**Authors**: Daniele Bifolco, Pietro Cassieri, Giuseppe Scanniello, Massimiliano Di Penta, Fiorella Zampetti

**Updated**: 2025-01-21T13:47:22Z

**Summary**: Large Language Models (LLMs) are currently used for various software development tasks, including generating code snippets to solve specific problems. Unlike reuse from the Web, LLMs are limited in providing provenance information about the generated code, which may have important trustworthiness and legal consequences. While LLM-based assistants may provide external links that are "related" to the generated code, we do not know how relevant such links are. This paper presents the findings of an empirical study assessing the extent to which 243 and 194 code snippets, across six programming languages, generated by Bing CoPilot and Google Gemini, likely originate from the links provided by these two LLM-based assistants. The study leverages automated code similarity assessments with thorough manual analysis. The study's findings indicate that the LLM-based assistants provide a mix of relevant and irrelevant links having a different nature. Specifically, although 66% of the links from Bing CoPilot and 28% from Google Gemini are relevant, LLMs-based assistants still suffer from serious "provenance debt".

**Link**: [arxiv](http://arxiv.org/abs/2501.12134v1),  [pdf](http://arxiv.org/pdf/2501.12134v1)

**Tags**: cs.SE 



### Evaluating Efficiency and Engagement in Scripted and LLM-Enhanced   Human-Robot Interactions
**Authors**: Tim Schreiter, Jens V. RÃ¼ppel, Rishi Hazra, Andrey Rudenko, Martin Magnusson, Achim J. Lilienthal

**Updated**: 2025-01-21T13:42:06Z

**Summary**: To achieve natural and intuitive interaction with people, HRI frameworks combine a wide array of methods for human perception, intention communication, human-aware navigation and collaborative action. In practice, when encountering unpredictable behavior of people or unexpected states of the environment, these frameworks may lack the ability to dynamically recognize such states, adapt and recover to resume the interaction. Large Language Models (LLMs), owing to their advanced reasoning capabilities and context retention, present a promising solution for enhancing robot adaptability. This potential, however, may not directly translate to improved interaction metrics. This paper considers a representative interaction with an industrial robot involving approach, instruction, and object manipulation, implemented in two conditions: (1) fully scripted and (2) including LLM-enhanced responses. We use gaze tracking and questionnaires to measure the participants' task efficiency, engagement, and robot perception. The results indicate higher subjective ratings for the LLM condition, but objective metrics show that the scripted condition performs comparably, particularly in efficiency and focus during simple tasks. We also note that the scripted condition may have an edge over LLM-enhanced responses in terms of response latency and energy consumption, especially for trivial and repetitive interactions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12128v1),  [pdf](http://arxiv.org/pdf/2501.12128v1)

**Tags**: cs.RO cs.HC 



### TwinRAN: Twinning the 5G RAN in Azure Cloud
**Authors**: Yash Deshpande, Eni Sulkaj, Wolfgang Kellerer

**Updated**: 2025-01-21T13:07:30Z

**Summary**: The proliferation of 5G technology necessitates advanced network management strategies to ensure optimal performance and reliability. Digital Twin (DT)s have emerged as a promising paradigm for modeling and simulating complex systems like the 5G Radio Access Network (RAN). In this paper, we present TwinRAN, a DT of the 5G RAN built leveraging the Azure DT platform. TwinRAN is built on top of the Open RAN (O-RAN) architecture and is agnostic to the vendor of the underlying equipment. We demonstrate three applications using TwinRAN and evaluate the required resources and their performance for a network with 800 users and eight gNBs. We first evaluate the performance and limitations of the Azure DT platform, measuring the latency under different conditions. The results from this evaluation allow us to optimize TwinRAN for the DT platform it uses. Then, we present the system's architectural design, emphasizing its components and interactions. We propose that two types of twin graphs be simultaneously maintained on the cloud: one for intercell operations, keeping a broad overview of all the cells in the network, and another where each cell is spawned in a separate Azure DT instance for more granular operation and monitoring of intracell tasks. We evaluate the performance and operating costs of TwinRAN for each of the three applications. The TwinRAN DT in the cloud can keep track of its physical twin within a few hundred milliseconds, extending its utility to many 5G network management tasks, some of which are shown in this paper. The novel framework for building and maintaining a DT of the 5G RAN presented in this paper offers network operators enhanced capabilities, empowering efficient deployments and management.

**Link**: [arxiv](http://arxiv.org/abs/2407.13340v2),  [pdf](http://arxiv.org/pdf/2407.13340v2)

**Tags**: cs.NI 



### Can open source large language models be used for tumor documentation in   Germany? -- An evaluation on urological doctors' notes
**Authors**: Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Torsten Panholzer

**Updated**: 2025-01-21T12:56:47Z

**Summary**: Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors' notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP.

**Link**: [arxiv](http://arxiv.org/abs/2501.12106v1),  [pdf](http://arxiv.org/pdf/2501.12106v1)

**Tags**: cs.CL cs.AI 



### EDoRA: Efficient Weight-Decomposed Low-Rank Adaptation via Singular   Value Decomposition
**Authors**: Hamid Nasiri, Peter Garraghan

**Updated**: 2025-01-21T11:42:09Z

**Summary**: Parameter-efficient fine-tuning methods, such as LoRA, reduces the number of trainable parameters. However, they often suffer from scalability issues and differences between their learning pattern and full fine-tuning. To overcome these limitations, we propose Efficient Weight-Decomposed Low-Rank Adaptation (EDoRA): a novel PEFT method that decomposes pre-trained weights into magnitude and directional components. By freezing low-rank matrices, initializing them by singular value decomposition, and introducing a small trainable matrix between them, EDoRA achieves substantial reduction in trainable parameters while maintaining learning capacity. Experimental results on the GLUE benchmark demonstrate that EDoRA achieves competitive or superior performance compared to state-of-the-art methods, such as LoRA and DoRA, with up to 30x fewer trainable parameters. This makes EDoRA a highly efficient solution for adapting LLMs to diverse tasks under memory-constrained settings. Code is available at https://github.com/Hamid-Nasiri/EDoRA .

**Link**: [arxiv](http://arxiv.org/abs/2501.12067v1),  [pdf](http://arxiv.org/pdf/2501.12067v1)

**Tags**: cs.LG cs.AI cs.CL 



### Large Language Model-Brained GUI Agents: A Survey
**Authors**: Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang

**Updated**: 2025-01-21T11:36:03Z

**Summary**: GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.   To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.

**Link**: [arxiv](http://arxiv.org/abs/2411.18279v7),  [pdf](http://arxiv.org/pdf/2411.18279v7)

**Tags**: cs.AI cs.CL cs.HC 



### PINNsAgent: Automated PDE Surrogation with Large Language Models
**Authors**: Qingpo Wuwu, Chonghan Gao, Tianyu Chen, Yihang Huang, Yuekai Zhang, Jianing Wang, Jianxin Li, Haoyi Zhou, Shanghang Zhang

**Updated**: 2025-01-21T11:26:02Z

**Summary**: Solving partial differential equations (PDEs) using neural methods has been a long-standing scientific and engineering research pursuit. Physics-Informed Neural Networks (PINNs) have emerged as a promising alternative to traditional numerical methods for solving PDEs. However, the gap between domain-specific knowledge and deep learning expertise often limits the practical application of PINNs. Previous works typically involve manually conducting extensive PINNs experiments and summarizing heuristic rules for hyperparameter tuning. In this work, we introduce PINNsAgent, a novel surrogation framework that leverages large language models (LLMs) and utilizes PINNs as a foundation to bridge the gap between domain-specific knowledge and deep learning. Specifically, PINNsAgent integrates (1) Physics-Guided Knowledge Replay (PGKR), which encodes the essential characteristics of PDEs and their associated best-performing PINNs configurations into a structured format, enabling efficient knowledge transfer from solved PDEs to similar problems and (2) Memory Tree Reasoning, a strategy that effectively explores the search space for optimal PINNs architectures. By leveraging LLMs and exploration strategies, PINNsAgent enhances the automation and efficiency of PINNs-based solutions. We evaluate PINNsAgent on 14 benchmark PDEs, demonstrating its effectiveness in automating the surrogation process and significantly improving the accuracy of PINNs-based solutions.

**Link**: [arxiv](http://arxiv.org/abs/2501.12053v1),  [pdf](http://arxiv.org/pdf/2501.12053v1)

**Tags**: cs.CE 



### MedS$^3$: Towards Medical Small Language Models with Self-Evolved Slow   Thinking
**Authors**: Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, Yu Wang

**Updated**: 2025-01-21T11:24:55Z

**Summary**: Medical language models (MLMs) have become pivotal in advancing medical natural language processing. However, prior models that rely on pre-training or supervised fine-tuning often exhibit low data efficiency and limited practicality in real-world clinical applications. While OpenAIs O1 highlights test-time scaling in mathematics, attempts to replicate this approach in medicine typically distill responses from GPT-series models to open-source models, focusing primarily on multiple-choice tasks. This strategy, though straightforward, neglects critical concerns like data privacy and realistic deployment in clinical settings. In this work, we present a deployable, small-scale medical language model, \mone, designed for long-chain reasoning in clinical tasks using a self-evolution paradigm. Starting with a seed dataset of around 8,000 instances spanning five domains and 16 datasets, we prompt a base policy model to perform Monte Carlo Tree Search (MCTS) to construct verifiable reasoning chains. Each reasoning step is assigned an evolution rollout value, allowing verified trajectories to train the policy model and the reward model. During inference, the policy model generates multiple responses, and the reward model selects the one with the highest reward score. Experiments on eleven evaluation datasets demonstrate that \mone outperforms prior open-source models by 2 points, with the addition of the reward model further boosting performance ($\sim$13 points), surpassing GPT-4o-mini. Code and data are available at \url{https://github.com/pixas/MedSSS}.

**Link**: [arxiv](http://arxiv.org/abs/2501.12051v1),  [pdf](http://arxiv.org/pdf/2501.12051v1)

**Tags**: cs.CL 



### High-Fidelity Coherent-One-Way QKD Simulation Framework for 6G Networks:   Bridging Theory and Reality
**Authors**: Aitor Brazaola-Vicario, Vasileios Kouvakis, Stylianos E. Trevlakis, Alejandra Ruiz, Alexandros-Apostolos A. Boulogeorgos, Theodoros Tsiftsis, Dusit Niyato

**Updated**: 2025-01-21T11:03:59Z

**Summary**: Quantum key distribution (QKD) has been emerged as a promising solution for guaranteeing information-theoretic security. Inspired by this, a great amount of research effort has been recently put on designing and testing QKD systems as well as articulating preliminary application scenarios. However, due to the considerable high-cost of QKD equipment, a lack of QKD communication system design tools, wide deployment of such systems and networks is challenging. Motivated by this, this paper introduces a QKD communication system design tool. First we articulate key operation elements of the QKD, and explain the feasibility and applicability of coherent-one-way (COW) QKD solutions. Next, we focus on documenting the corresponding simulation framework as well as defining the key performance metrics, i.e., quantum bit error rate (QBER), and secrecy key rate. To verify the accuracy of the simulation framework, we design and deploy a real-world QKD setup. We perform extensive experiments for three deployments of diverse transmission distance in the presence or absence of a QKD eavesdropper. The results reveal an acceptable match between simulations and experiments rendering the simulation framework a suitable tool for QKD communication system design.

**Link**: [arxiv](http://arxiv.org/abs/2501.12043v1),  [pdf](http://arxiv.org/pdf/2501.12043v1)

**Tags**: quant-ph cs.SY eess.SY 



### In-Network Preprocessing of Recommender Systems on Multi-Tenant   SmartNICs
**Authors**: Yu Zhu, Wenqi Jiang, Gustavo Alonso

**Updated**: 2025-01-21T10:53:17Z

**Summary**: Keeping ML-based recommender models up-to-date as data drifts and evolves is essential to maintain accuracy. As a result, online data preprocessing plays an increasingly important role in serving recommender systems. Existing solutions employ multiple CPU workers to saturate the input bandwidth of a single training node. Such an approach results in high deployment costs and energy consumption. For instance, a recent report from industrial deployments shows that data storage and ingestion pipelines can account for over 60\% of the power consumption in a recommender system. In this paper, we tackle the issue from a hardware perspective by introducing Piper, a flexible and network-attached accelerator that executes data loading and preprocessing pipelines in a streaming fashion. As part of the design, we define MiniPipe, the smallest pipeline unit enabling multi-pipeline implementation by executing various data preprocessing tasks across the single board, giving Piper the ability to be reconfigured at runtime. Our results, using publicly released commercial pipelines, show that Piper, prototyped on a power-efficient FPGA, achieves a 39$\sim$105$\times$ speedup over a server-grade, 128-core CPU and 3$\sim$17$\times$ speedup over GPUs like RTX 3090 and A100 in multiple pipelines. The experimental analysis demonstrates that Piper provides advantages in both latency and energy efficiency for preprocessing tasks in recommender systems, providing an alternative design point for systems that today are in very high demand.

**Link**: [arxiv](http://arxiv.org/abs/2501.12032v1),  [pdf](http://arxiv.org/pdf/2501.12032v1)

**Tags**: cs.AR cs.DC cs.LG 



### Distilling Calibration via Conformalized Credal Inference
**Authors**: Jiayi Huang, Sangwoo Park, Nicola Paoletti, Osvaldo Simeone

**Updated**: 2025-01-21T10:48:54Z

**Summary**: Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.

**Link**: [arxiv](http://arxiv.org/abs/2501.06066v2),  [pdf](http://arxiv.org/pdf/2501.06066v2)

**Tags**: cs.LG cs.AI eess.SP 



### Federated Instruction Tuning of LLMs with Domain Coverage Augmentation
**Authors**: Zezhou Wang, Yaxin Du, Xingjun Ma, Yugang Jiang, Zhuzhong Qian, Siheng Chen

**Updated**: 2025-01-21T09:25:25Z

**Summary**: Federated Domain-specific Instruction Tuning (FedDIT) utilizes limited cross-client private data together with various strategies of instruction augmentation, ultimately boosting model performance within specific domains. To date, the factors affecting FedDIT remain unclear, and existing instruction augmentation methods primarily focus on the centralized setting without considering distributed environments. Our experiments reveal that the cross-client domain coverage, rather than data heterogeneity, drives model performance in FedDIT. In response, we propose FedDCA, which optimizes domain coverage through greedy client center selection and retrieval-based augmentation. At its core, the greedy selection procedure iteratively picks client centers that maximize the diversity and coverage of the instruction space while avoiding redundancy with previously selected centers. This ensures broad yet efficient coverage of the domain distribution across clients. For client-side computational efficiency and system scalability, FedDCA$^*$, the variant of FedDCA, utilizes heterogeneous encoders with server-side feature alignment. Extensive experiments across code, medical, financial, and mathematical domains substantiate the effectiveness of both methods, as well as plug-and-play capability. We further analyze privacy preservation against memory extraction attacks, showing that while privacy leakage risk is independent of augmented public data ratio, it decreases or converges as training progresses.

**Link**: [arxiv](http://arxiv.org/abs/2409.20135v5),  [pdf](http://arxiv.org/pdf/2409.20135v5)

**Tags**: cs.LG cs.CL cs.DC 



### Linear Feedback Control Systems for Iterative Prompt Optimization in   Large Language Models
**Authors**: Rupesh Raj Karn

**Updated**: 2025-01-21T08:52:47Z

**Summary**: Large Language Models (LLMs) have revolutionized various applications by generating outputs based on given prompts. However, achieving the desired output requires iterative prompt refinement. This paper presents a novel approach that draws parallels between the iterative prompt optimization process in LLMs and feedback control systems. We iteratively refine the prompt by treating the deviation between the LLM output and the desired result as an error term until the output criteria are met. This process is akin to a feedback control system, where the LLM, despite being non-linear and non-deterministic, is managed using principles from linear feedback control systems. We explore the application of different types of controllers within this framework, providing a mathematical foundation for integrating linear feedback control mechanisms with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.11979v1),  [pdf](http://arxiv.org/pdf/2501.11979v1)

**Tags**: cs.LG 



### Leveraging Graph Structures and Large Language Models for End-to-End   Synthetic Task-Oriented Dialogues
**Authors**: Maya Medjad, Hugo Imbert, Bruno Yun, RaphaÃ«l Szymocha, FrÃ©dÃ©ric Armetta

**Updated**: 2025-01-21T08:51:12Z

**Summary**: Training task-oriented dialogue systems is both costly and time-consuming, due to the need for high-quality datasets encompassing diverse intents. Traditional methods depend on extensive human annotation, while recent advancements leverage large language models (LLMs) to generate synthetic data. However, these approaches often require custom prompts or code, limiting accessibility for non-technical users. We introduce GraphTOD, an end-to-end framework that simplifies the generation of task-oriented dialogues. Users can create dialogues by specifying transition graphs in JSON format. Our evaluation demonstrates that GraphTOD generates high-quality dialogues across various domains, significantly lowering the cost and complexity of dataset creation.

**Link**: [arxiv](http://arxiv.org/abs/2501.11977v1),  [pdf](http://arxiv.org/pdf/2501.11977v1)

**Tags**: cs.CL cs.AI 



### AGON: Automated Design Framework for Customizing Processors from ISA   Documents
**Authors**: Chongxiao Li, Di Huang, Pengwei Jin, Tianyun Ma, Husheng Han, Shuyao Cheng, Yifan Hao, Yongwei Zhao, Guanglin Xu, Zidong Du, Rui Zhang, Xiaqing Li, Yuanbo Wen, Xing Hu, Qi Guo

**Updated**: 2025-01-21T08:38:01Z

**Summary**: Customized processors are attractive solutions for vast domain-specific applications due to their high energy efficiency. However, designing a processor in traditional flows is time-consuming and expensive. To address this, researchers have explored methods including the use of agile development tools like Chisel or SpinalHDL, high-level synthesis (HLS) from programming languages like C or SystemC, and more recently, leveraging large language models (LLMs) to generate hardware description language (HDL) code from natural language descriptions. However, each method has limitations in terms of expressiveness, correctness, and performance, leading to a persistent contradiction between the level of automation and the effectiveness of the design. Overall, how to automatically design highly efficient and practical processors with minimal human effort remains a challenge.   In this paper, we propose AGON, a novel framework designed to leverage LLMs for the efficient design of out-of-order (OoO) customized processors with minimal human effort. Central to AGON is the nano-operator function (nOP function) based Intermediate Representation (IR), which bridges high-level descriptions and hardware implementations while decoupling functionality from performance optimization, thereby providing an automatic design framework that is expressive and efficient, has correctness guarantees, and enables PPA (Power, Performance, and Area) optimization.   Experimental results show that superior to previous LLM-assisted automatic design flows, AGON facilitates designing a series of customized OoO processors that achieve on average 2.35 $\times$ speedup compared with BOOM, a general-purpose CPU designed by experts, with minimal design effort.

**Link**: [arxiv](http://arxiv.org/abs/2412.20954v2),  [pdf](http://arxiv.org/pdf/2412.20954v2)

**Tags**: cs.AR 



### A Hybrid Attention Framework for Fake News Detection with Large Language   Models
**Authors**: Xiaochuan Xu, Peiyang Yu, Zeqiu Xu, Jiani Wang

**Updated**: 2025-01-21T08:26:20Z

**Summary**: With the rapid growth of online information, the spread of fake news has become a serious social challenge. In this study, we propose a novel detection framework based on Large Language Models (LLMs) to identify and classify fake news by integrating textual statistical features and deep semantic features. Our approach utilizes the contextual understanding capability of the large language model for text analysis and introduces a hybrid attention mechanism to focus on feature combinations that are particularly important for fake news identification. Extensive experiments on the WELFake news dataset show that our model significantly outperforms existing methods, with a 1.5\% improvement in F1 score. In addition, we assess the interpretability of the model through attention heat maps and SHAP values, providing actionable insights for content review strategies. Our framework provides a scalable and efficient solution to deal with the spread of fake news and helps build a more reliable online information ecosystem.

**Link**: [arxiv](http://arxiv.org/abs/2501.11967v1),  [pdf](http://arxiv.org/pdf/2501.11967v1)

**Tags**: cs.CL 



### QROA: A Black-Box Query-Response Optimization Attack on LLMs
**Authors**: Hussein Jawad, Nicolas J. -B. BRUNEL

**Updated**: 2025-01-21T08:17:27Z

**Summary**: Large Language Models (LLMs) have surged in popularity in recent months, yet they possess concerning capabilities for generating harmful content when manipulated. This study introduces the Query-Response Optimization Attack (QROA), an optimization-based strategy designed to exploit LLMs through a black-box, query-only interaction. QROA adds an optimized trigger to a malicious instruction to compel the LLM to generate harmful content. Unlike previous approaches, QROA does not require access to the model's logit information or any other internal data and operates solely through the standard query-response interface of LLMs. Inspired by deep Q-learning and Greedy coordinate descent, the method iteratively updates tokens to maximize a designed reward function. We tested our method on various LLMs such as Vicuna, Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80\%. We also tested the model against Llama2-chat, the fine-tuned version of Llama2 designed to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger seed. This study demonstrates the feasibility of generating jailbreak attacks against deployed LLMs in the public domain using black-box optimization methods, enabling more comprehensive safety testing of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2406.02044v2),  [pdf](http://arxiv.org/pdf/2406.02044v2)

**Tags**: cs.CL cs.LG 



### Simultaneously decoding the unknown stationary state and function   parameters for mean field games
**Authors**: Hongyu Liu, Catharine W. K. Lo

**Updated**: 2025-01-21T08:02:17Z

**Summary**: Mean field games (MFGs) offer a versatile framework for modeling large-scale interactive systems across multiple domains. This paper builds upon a previous work, by developing a state-of-the-art unified approach to decode or design the unknown stationary state of MFGs, in addition to the underlying parameter functions governing their behavior. This result is novel, even in the general realm of inverse problems for nonlinear PDEs. By enabling agents to distill crucial insights from observed data and unveil intricate hidden structures and unknown states within MFG systems, our approach surmounts a significant obstacle, enhancing the applicability of MFGs in real-world scenarios. This advancement not only enriches our understanding of MFG dynamics but also broadens the scope for their practical deployment in various contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.11955v1),  [pdf](http://arxiv.org/pdf/2501.11955v1)

**Tags**: math.AP math.OC Primary 35Q89, 35R30, secondary 91A16, 35R35 



### Proverbs Run in Pairs: Evaluating Proverb Translation Capability of   Large Language Model
**Authors**: Minghan Wang, Viet-Thanh Pham, Farhad Moghimifar, Thuy-Trang Vu

**Updated**: 2025-01-21T07:54:22Z

**Summary**: Despite achieving remarkable performance, machine translation (MT) research remains underexplored in terms of translating cultural elements in languages, such as idioms, proverbs, and colloquial expressions. This paper investigates the capability of state-of-the-art neural machine translation (NMT) and large language models (LLMs) in translating proverbs, which are deeply rooted in cultural contexts. We construct a translation dataset of standalone proverbs and proverbs in conversation for four language pairs. Our experiments show that the studied models can achieve good translation between languages with similar cultural backgrounds, and LLMs generally outperform NMT models in proverb translation. Furthermore, we find that current automatic evaluation metrics such as BLEU, CHRF++ and COMET are inadequate for reliably assessing the quality of proverb translation, highlighting the need for more culturally aware evaluation metrics.

**Link**: [arxiv](http://arxiv.org/abs/2501.11953v1),  [pdf](http://arxiv.org/pdf/2501.11953v1)

**Tags**: cs.CL 



### BlockDialect: Block-wise Fine-grained Mixed Format Quantization for   Energy-Efficient LLM Inference
**Authors**: Wonsuk Jang, Thierry Tambe

**Updated**: 2025-01-21T07:34:54Z

**Summary**: The rapidly increasing size of large language models (LLMs) presents significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with hardware-supported fine-grained scaling emerging as a promising solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. We propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from a formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. To leverage this efficiently, we propose a two-stage approach for online DialectFP4 activation quantization. Importantly, DialectFP4 ensures energy efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit usage per data, while being only 5.45% (2.69%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2501.01144v3),  [pdf](http://arxiv.org/pdf/2501.01144v3)

**Tags**: cs.CL cs.LG 



### Sample-Independent Federated Learning Backdoor Attack in Speaker   Recognition
**Authors**: Weida Xu, Yang Xu, Sicong Zhang

**Updated**: 2025-01-21T07:28:48Z

**Summary**: In federated learning, backdoor attacks embed triggers in the adversarial client's data to inject a backdoor into the model. In order to enhance the stealth, an attack method based on the dropout layer has been proposed, which can implant the backdoor without modifying the sample. However, these methods struggle to covertly utilize dropout in evaluation mode, thus hindering their deployment in real-world scenarios. To address these, this paper introduces GhostB, a novel approach to federated learning backdoor attacks in speaker recognition that neither alters samples nor relies on dropout. This method employs the behavior of neurons producing specific values as triggers. By mapping these neuronal values to categories specified by the adversary, the backdoor is implanted and activated when particular feature values are detected at designated neurons. Our experiments conducted on TIMIT, LibriSpeech, and VoxCeleb2 databases in both Closed Set Identification (CSI) and Open Set Identification (OSI) scenarios demonstrate that GhostB achieves a 100% success rate upon activation in speaker recognition, with this rate maintained across experiments involving 1 to 50 ghost neurons. This paper investigates how the dispersion of neurons and their depth within hidden layers affect the success rate, revealing that increased dispersion and positioning of neurons can significantly decrease effectiveness, potentially rendering the attack unsuccessful.

**Link**: [arxiv](http://arxiv.org/abs/2408.13849v2),  [pdf](http://arxiv.org/pdf/2408.13849v2)

**Tags**: cs.CR 



### Webvs. LLMs: An Empirical Study of Learning Behaviors of CS2 Students
**Authors**: Aayush Kumar, Daniel Prol, Amin Alipour, Sruti Srinivasa Ragavan

**Updated**: 2025-01-21T07:16:18Z

**Summary**: LLMs such as ChatGPT have been widely adopted by students in higher education as tools for learning programming and related concepts. However, it remains unclear how effective students are and what strategies students use while learning with LLMs. Since the majority of students' experiences in online self-learning have come through using search engines such as Google, evaluating AI tools in this context can help us address these gaps. In this mixed methods research, we conducted an exploratory within-subjects study to understand how CS2 students learn programming concepts using both LLMs as well as traditional online methods such as educational websites and videos to examine how students approach learning within and across both scenarios. We discovered that students found it easier to learn a more difficult concept using traditional methods than using ChatGPT. We also found that students ask fewer follow-ups and use more keyword-based queries for search engines while their prompts to LLMs tend to explicitly ask for information.

**Link**: [arxiv](http://arxiv.org/abs/2501.11935v1),  [pdf](http://arxiv.org/pdf/2501.11935v1)

**Tags**: cs.HC cs.AI 



### RUIE: Retrieval-based Unified Information Extraction using Large   Language Model
**Authors**: Xincheng Liao, Junwen Duan, Yixi Huang, Jianxin Wang

**Updated**: 2025-01-21T07:09:35Z

**Summary**: Unified information extraction (UIE) aims to extract diverse structured information from unstructured text. While large language models (LLMs) have shown promise for UIE, they require significant computational resources and often struggle to generalize to unseen tasks. We propose RUIE (Retrieval-based Unified Information Extraction), a framework that leverages in-context learning for efficient task generalization. RUIE introduces a novel demonstration selection mechanism combining LLM preferences with a keyword-enhanced reward model, and employs a bi-encoder retriever trained through contrastive learning and knowledge distillation. As the first trainable retrieval framework for UIE, RUIE serves as a universal plugin for various LLMs. Experimental results on eight held-out datasets demonstrate RUIE's effectiveness, with average F1-score improvements of 19.22 and 3.22 compared to instruction-tuning methods and other retrievers, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2409.11673v2),  [pdf](http://arxiv.org/pdf/2409.11673v2)

**Tags**: cs.CL 



### ALoFTRAG: Automatic Local Fine Tuning for Retrieval Augmented Generation
**Authors**: Peter Devine

**Updated**: 2025-01-21T07:07:58Z

**Summary**: Retrieval Augmented Generation (RAG) systems have been shown to improve the accuracy of Large Language Model (LLM) outputs. However, these models can often achieve low accuracy when applied to new data domains.   We introduce the Automatic Local Fine Tuning of Retrieval Augmented Generation models (ALoFTRAG) framework, designed to improve the accuracy of RAG systems on a given domain by training LLMs without manually labeled data or using larger teacher models.   By generating and filtering synthetic training data and performing LoRA fine-tuning, ALoFTRAG improves citation and answer accuracy across 20 datasets in 26 languages by, on average, 8.3% and 3.0% respectively.   Our results demonstrate that ALoFTRAG offers a practical, cost-effective, and data-secure solution for improving RAG accuracy, making it particularly applicable to sensitive domains such as healthcare and finance.

**Link**: [arxiv](http://arxiv.org/abs/2501.11929v1),  [pdf](http://arxiv.org/pdf/2501.11929v1)

**Tags**: cs.LG 



### Multi-Agent Consensus Seeking via Large Language Models
**Authors**: Huaben Chen, Wenkang Ji, Lufeng Xu, Shiyu Zhao

**Updated**: 2025-01-21T06:26:43Z

**Summary**: Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: windylab.github.io/ConsensusLLM/.

**Link**: [arxiv](http://arxiv.org/abs/2310.20151v2),  [pdf](http://arxiv.org/pdf/2310.20151v2)

**Tags**: cs.CL cs.RO cs.SY eess.SY 



### Integrate Temporal Graph Learning into LLM-based Temporal Knowledge   Graph Model
**Authors**: He Chang, Jie Wu, Zhulin Tao, Yunshan Ma, Xianglin Huang, Tat-Seng Chua

**Updated**: 2025-01-21T06:12:49Z

**Summary**: Temporal Knowledge Graph Forecasting (TKGF) aims to predict future events based on the observed events in history. Recently, Large Language Models (LLMs) have exhibited remarkable capabilities, generating significant research interest in their application for reasoning over temporal knowledge graphs (TKGs). Existing LLM-based methods have integrated retrieved historical facts or static graph representations into LLMs. Despite the notable performance of LLM-based methods, they are limited by the insufficient modeling of temporal patterns and ineffective cross-modal alignment between graph and language, hindering the ability of LLMs to fully grasp the temporal and structural information in TKGs. To tackle these issues, we propose a novel framework TGL-LLM to integrate temporal graph learning into LLM-based temporal knowledge graph model. Specifically, we introduce temporal graph learning to capture the temporal and relational patterns and obtain the historical graph embedding. Furthermore, we design a hybrid graph tokenization to sufficiently model the temporal patterns within LLMs. To achieve better alignment between graph and language, we employ a two-stage training paradigm to finetune LLMs on high-quality and diverse data, thereby resulting in better performance. Extensive experiments on three real-world datasets show that our approach outperforms a range of state-of-the-art (SOTA) methods.

**Link**: [arxiv](http://arxiv.org/abs/2501.11911v1),  [pdf](http://arxiv.org/pdf/2501.11911v1)

**Tags**: cs.IR 



### LogLLM: Log-based Anomaly Detection Using Large Language Models
**Authors**: Wei Guan, Jian Cao, Shiyou Qian, Jianqi Gao, Chun Ouyang

**Updated**: 2025-01-21T06:12:26Z

**Summary**: Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.

**Link**: [arxiv](http://arxiv.org/abs/2411.08561v2),  [pdf](http://arxiv.org/pdf/2411.08561v2)

**Tags**: cs.SE cs.AI 



### Diversify, Don't Fine-Tune: Scaling Up Visual Recognition Training with   Synthetic Images
**Authors**: Zhuoran Yu, Chenchen Zhu, Sean Culatana, Raghuraman Krishnamoorthi, Fanyi Xiao, Yong Jae Lee

**Updated**: 2025-01-21T06:03:07Z

**Summary**: Recent advances in generative deep learning have enabled the creation of high-quality synthetic images in text-to-image generation. Prior work shows that fine-tuning a pretrained diffusion model on ImageNet and generating synthetic training images from the finetuned model can enhance an ImageNet classifier's performance. However, performance degrades as synthetic images outnumber real ones. In this paper, we explore whether generative fine-tuning is essential for this improvement and whether it is possible to further scale up training using more synthetic data. We present a new framework leveraging off-the-shelf generative models to generate synthetic training images, addressing multiple challenges: class name ambiguity, lack of diversity in naive prompts, and domain shifts. Specifically, we leverage large language models (LLMs) and CLIP to resolve class name ambiguity. To diversify images, we propose contextualized diversification (CD) and stylized diversification (SD) methods, also prompted by LLMs. Finally, to mitigate domain shifts, we leverage domain adaptation techniques with auxiliary batch normalization for synthetic images. Our framework consistently enhances recognition model performance with more synthetic data, up to 6x of original ImageNet size showcasing the potential of synthetic data for improved recognition models and strong out-of-domain generalization.

**Link**: [arxiv](http://arxiv.org/abs/2312.02253v2),  [pdf](http://arxiv.org/pdf/2312.02253v2)

**Tags**: cs.CV cs.AI cs.LG 



### Towards LifeSpan Cognitive Systems
**Authors**: Yu Wang, Chi Han, Tongtong Wu, Xiaoxin He, Wangchunshu Zhou, Nafis Sadeq, Xiusi Chen, Zexue He, Wei Wang, Gholamreza Haffari, Heng Ji, Julian McAuley

**Updated**: 2025-01-21T05:50:19Z

**Summary**: Building a human-like system that continuously interacts with complex environments -- whether simulated digital worlds or human society -- presents several key challenges. Central to this is enabling continuous, high-frequency interactions, where the interactions are termed experiences. We refer to this envisioned system as the LifeSpan Cognitive System (LSCS). A critical feature of LSCS is its ability to engage in incremental and rapid updates while retaining and accurately recalling past experiences. In this paper we focus on the domain of Large Language Models (LLMs), where we identify two major challenges: (1) Abstraction and Experience Merging, and (2) Long-term Retention with Accurate Recall. These properties are essential for storing new experiences, organizing past experiences, and responding to the environment in ways that leverage relevant historical data. Unlike language models with continual learning, which typically rely on large corpora for fine-tuning and focus on improving performance within specific domains or tasks, LSCS must rapidly and incrementally update with new information from its environment at a high frequency. Existing technologies with the potential of solving the above two major challenges can be classified into four classes based on a conceptual metric called Storage Complexity, which measures the relative space required to store past experiences. Each of these four classes of technologies has its own strengths and limitations while we argue none of them alone can achieve LSCS alone. To this end, we propose a potential instantiation for LSCS that can integrate all four classes of technologies. The new instantiation, serving as a conjecture, operates through two core processes: Absorbing Experiences and Generating Responses.

**Link**: [arxiv](http://arxiv.org/abs/2409.13265v2),  [pdf](http://arxiv.org/pdf/2409.13265v2)

**Tags**: cs.CL 



### Panoramic Interests: Stylistic-Content Aware Personalized Headline   Generation
**Authors**: Junhong Lian, Xiang Ao, Xinyu Liu, Yang Liu, Qing He

**Updated**: 2025-01-21T05:30:20Z

**Summary**: Personalized news headline generation aims to provide users with attention-grabbing headlines that are tailored to their preferences. Prevailing methods focus on user-oriented content preferences, but most of them overlook the fact that diverse stylistic preferences are integral to users' panoramic interests, leading to suboptimal personalization. In view of this, we propose a novel Stylistic-Content Aware Personalized Headline Generation (SCAPE) framework. SCAPE extracts both content and stylistic features from headlines with the aid of large language model (LLM) collaboration. It further adaptively integrates users' long- and short-term interests through a contrastive learning-based hierarchical fusion network. By incorporating the panoramic interests into the headline generator, SCAPE reflects users' stylistic-content preferences during the generation process. Extensive experiments on the real-world dataset PENS demonstrate the superiority of SCAPE over baselines.

**Link**: [arxiv](http://arxiv.org/abs/2501.11900v1),  [pdf](http://arxiv.org/pdf/2501.11900v1)

**Tags**: cs.CL cs.AI 



### Med-R$^2$: Crafting Trustworthy LLM Physicians through Retrieval and   Reasoning of Evidence-Based Medicine
**Authors**: Keer Lu, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, Wentao Zhang

**Updated**: 2025-01-21T04:40:43Z

**Summary**: In recent years, Large Language Models (LLMs) have exhibited remarkable capabilities in clinical scenarios. However, despite their potential, existing works face challenges when applying LLMs to medical settings. Strategies relying on training with medical datasets are highly cost-intensive and may suffer from outdated training data. Leveraging external knowledge bases is a suitable alternative, yet it faces obstacles such as limited retrieval precision and poor effectiveness in answer extraction. These issues collectively prevent LLMs from demonstrating the expected level of proficiency in mastering medical expertise. To address these challenges, we introduce Med-R^2, a novel LLM physician framework that adheres to the Evidence-Based Medicine (EBM) process, efficiently integrating retrieval mechanisms as well as the selection and reasoning processes of evidence, thereby enhancing the problem-solving capabilities of LLMs in healthcare scenarios and fostering a trustworthy LLM physician. Our comprehensive experiments indicate that Med-R^2 achieves a 14.87\% improvement over vanilla RAG methods and even a 3.59\% enhancement compared to fine-tuning strategies, without incurring additional training costs.

**Link**: [arxiv](http://arxiv.org/abs/2501.11885v1),  [pdf](http://arxiv.org/pdf/2501.11885v1)

**Tags**: cs.CL 



### From Drafts to Answers: Unlocking LLM Potential via Aggregation   Fine-Tuning
**Authors**: Yafu Li, Zhilin Wang, Tingchen Fu, Ganqu Cui, Sen Yang, Yu Cheng

**Updated**: 2025-01-21T04:11:59Z

**Summary**: Scaling data and model size has been proven effective for boosting the performance of large language models. In addition to training-time scaling, recent studies have revealed that increasing test-time computational resources can further improve performance. In this work, we introduce Aggregation Fine-Tuning (AFT), a supervised finetuning paradigm where the model learns to synthesize multiple draft responses, referred to as proposals, into a single, refined answer, termed aggregation. At inference time, a propose-and-aggregate strategy further boosts performance by iteratively generating proposals and aggregating them. Empirical evaluations on benchmark datasets show that AFT-trained models substantially outperform standard SFT. Notably, an AFT model, fine-tuned from Llama3.1-8B-Base with only 64k data, achieves a 41.3% LC win rate on AlpacaEval 2, surpassing significantly larger LLMs such as Llama3.1-405B-Instruct and GPT4. By combining sequential refinement and parallel sampling, the propose-and-aggregate framework scales inference-time computation in a flexible manner. Overall, These findings position AFT as a promising approach to unlocking additional capabilities of LLMs without resorting to increasing data volume or model size.

**Link**: [arxiv](http://arxiv.org/abs/2501.11877v1),  [pdf](http://arxiv.org/pdf/2501.11877v1)

**Tags**: cs.CL cs.AI 



### Judging the Judges: Evaluating Alignment and Vulnerabilities in   LLMs-as-Judges
**Authors**: Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes

**Updated**: 2025-01-21T04:10:13Z

**Summary**: Offering a promising solution to the scalability challenges associated with human evaluation, the LLM-as-a-judge paradigm is rapidly gaining traction as an approach to evaluating large language models (LLMs). However, there are still many open questions about the strengths and weaknesses of this paradigm, and what potential biases it may hold. In this paper, we present a comprehensive study of the performance of various LLMs acting as judges, focusing on a clean scenario in which inter-human agreement is high. Investigating thirteen judge models of different model sizes and families, judging answers of nine different 'examtaker models' - both base and instruction-tuned - we find that only the best (and largest) models achieve reasonable alignment with humans. However, they are still quite far behind inter-human agreement and their assigned scores may still differ with up to 5 points from human-assigned scores. In terms of their ranking of the nine exam-taker models, instead, also smaller models and even the lexical metric contains may provide a reasonable signal. Through error analysis and other studies, we identify vulnerabilities in judge models, such as their sensitivity to prompt complexity and length, and a tendency toward leniency. The fact that even the best judges differ from humans in this comparatively simple setup suggest that caution may be wise when using judges in more complex setups. Lastly, our research rediscovers the importance of using alignment metrics beyond simple percent alignment, showing that judges with high percent agreement can still assign vastly different scores.

**Link**: [arxiv](http://arxiv.org/abs/2406.12624v5),  [pdf](http://arxiv.org/pdf/2406.12624v5)

**Tags**: cs.CL cs.AI 



### FLAME: Learning to Navigate with Multimodal LLM in Urban Environments
**Authors**: Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang

**Updated**: 2025-01-21T04:06:09Z

**Summary**: Large Language Models (LLMs) have demonstrated potential in Vision-and-Language Navigation (VLN) tasks, yet current applications face challenges. While LLMs excel in general conversation scenarios, they struggle with specialized navigation tasks, yielding suboptimal performance compared to specialized VLN models. We introduce FLAME (FLAMingo-Architected Embodied Agent), a novel Multimodal LLM-based agent and architecture designed for urban VLN tasks that efficiently handles multiple observations. Our approach implements a three-phase tuning technique for effective adaptation to navigation tasks, including single perception tuning for street view description, multiple perception tuning for route summarization, and end-to-end training on VLN datasets. The augmented datasets are synthesized automatically. Experimental results demonstrate FLAME's superiority over existing methods, surpassing state-of-the-art methods by a 7.3% increase in task completion on Touchdown dataset. This work showcases the potential of Multimodal LLMs (MLLMs) in complex navigation tasks, representing an advancement towards applications of MLLMs in the field of embodied intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2408.11051v2),  [pdf](http://arxiv.org/pdf/2408.11051v2)

**Tags**: cs.CV cs.AI cs.CL cs.RO 



### Demons in the Detail: On Implementing Load Balancing Loss for Training   Specialized Mixture-of-Expert Models
**Authors**: Zihan Qiu, Zeyu Huang, Bo Zheng, Kaiyue Wen, Zekun Wang, Rui Men, Ivan Titov, Dayiheng Liu, Jingren Zhou, Junyang Lin

**Updated**: 2025-01-21T04:04:39Z

**Summary**: This paper revisits the implementation of $\textbf{L}$oad-$\textbf{b}$alancing $\textbf{L}$oss (LBL) when training Mixture-of-Experts (MoEs) models. Specifically, LBL for MoEs is defined as $N_E \sum_{i=1}^{N_E} f_i p_i$, where $N_E$ is the total number of experts, $f_i$ represents the frequency of expert $i$ being selected, and $p_i$ denotes the average gating score of the expert $i$. Existing MoE training frameworks usually employ the parallel training strategy so that $f_i$ and the LBL are calculated within a $\textbf{micro-batch}$ and then averaged across parallel groups. In essence, a micro-batch for training billion-scale LLMs normally contains very few sequences. So, the micro-batch LBL is almost at the sequence level, and the router is pushed to distribute the token evenly within each sequence. Under this strict constraint, even tokens from a domain-specific sequence ($\textit{e.g.}$, code) are uniformly routed to all experts, thereby inhibiting expert specialization. In this work, we propose calculating LBL using a $\textbf{global-batch}$ to loose this constraint. Because a global-batch contains much more diverse sequences than a micro-batch, which will encourage load balance at the corpus level. Specifically, we introduce an extra communication step to synchronize $f_i$ across micro-batches and then use it to calculate the LBL. Through experiments on training MoEs-based LLMs (up to $\textbf{42.8B}$ total parameters and $\textbf{400B}$ tokens), we surprisingly find that the global-batch LBL strategy yields excellent performance gains in both pre-training perplexity and downstream tasks. Our analysis reveals that the global-batch LBL also greatly improves the domain specialization of MoE experts.

**Link**: [arxiv](http://arxiv.org/abs/2501.11873v1),  [pdf](http://arxiv.org/pdf/2501.11873v1)

**Tags**: cs.LG cs.CL 



### LLM-Agents Driven Automated Simulation Testing and Analysis of small   Uncrewed Aerial Systems
**Authors**: Venkata Sai Aswath Duvvuru, Bohan Zhang, Michael Vierhauser, Ankit Agrawal

**Updated**: 2025-01-21T03:42:21Z

**Summary**: Thorough simulation testing is crucial for validating the correct behavior of small Uncrewed Aerial Systems (sUAS) across multiple scenarios, including adverse weather conditions (such as wind, and fog), diverse settings (hilly terrain, or urban areas), and varying mission profiles (surveillance, tracking). While various sUAS simulation tools exist to support developers, the entire process of creating, executing, and analyzing simulation tests remains a largely manual and cumbersome task. Developers must identify test scenarios, set up the simulation environment, integrate the System under Test (SuT) with simulation tools, formulate mission plans, and collect and analyze results. These labor-intensive tasks limit the ability of developers to conduct exhaustive testing across a wide range of scenarios. To alleviate this problem, in this paper, we propose AutoSimTest, a Large Language Model (LLM)-driven framework, where multiple LLM agents collaborate to support the sUAS simulation testing process. This includes: (1) creating test scenarios that subject the SuT to unique environmental contexts; (2) preparing the simulation environment as per the test scenario; (3) generating diverse sUAS missions for the SuT to execute; and (4) analyzing simulation results and providing an interactive analytics interface. Further, the design of the framework is flexible for creating and testing scenarios for a variety of sUAS use cases, simulation tools, and SuT input requirements. We evaluated our approach by (a) conducting simulation testing of PX4 and ArduPilot flight-controller-based SuTs, (b) analyzing the performance of each agent, and (c) gathering feedback from sUAS developers. Our findings indicate that AutoSimTest significantly improves the efficiency and scope of the sUAS testing process, allowing for more comprehensive and varied scenario evaluations while reducing the manual effort.

**Link**: [arxiv](http://arxiv.org/abs/2501.11864v1),  [pdf](http://arxiv.org/pdf/2501.11864v1)

**Tags**: cs.SE 



### Customizing Language Models with Instance-wise LoRA for Sequential   Recommendation
**Authors**: Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, Xiangnan He

**Updated**: 2025-01-21T03:40:14Z

**Summary**: Sequential recommendation systems predict the next interaction item based on users' past interactions, aligning recommendations with individual preferences. Leveraging the strengths of Large Language Models (LLMs) in knowledge comprehension and reasoning, recent approaches are eager to apply LLMs to sequential recommendation. A common paradigm is converting user behavior sequences into instruction data, and fine-tuning the LLM with parameter-efficient fine-tuning (PEFT) methods like Low-Rank Adaption (LoRA). However, the uniform application of LoRA across diverse user behaviors is insufficient to capture individual variability, resulting in negative transfer between disparate sequences. To address these challenges, we propose Instance-wise LoRA (iLoRA). We innovatively treat the sequential recommendation task as a form of multi-task learning, integrating LoRA with the Mixture of Experts (MoE) framework. This approach encourages different experts to capture various aspects of user behavior. Additionally, we introduce a sequence representation guided gate function that generates customized expert participation weights for each user sequence, which allows dynamic parameter adjustment for instance-wise recommendations. In sequential recommendation, iLoRA achieves an average relative improvement of 11.4\% over basic LoRA in the hit ratio metric, with less than a 1\% relative increase in trainable parameters. Extensive experiments on three benchmark datasets demonstrate the effectiveness of iLoRA, highlighting its superior performance compared to existing methods in mitigating negative transfer and improving recommendation accuracy. Our data and code are available at https://github.com/AkaliKong/iLoRA.

**Link**: [arxiv](http://arxiv.org/abs/2408.10159v4),  [pdf](http://arxiv.org/pdf/2408.10159v4)

**Tags**: cs.IR cs.AI 



### Towards Explainable Vulnerability Detection with Large Language Models
**Authors**: Qiheng Mao, Zhenhao Li, Xing Hu, Kui Liu, Xin Xia, Jianling Sun

**Updated**: 2025-01-21T03:27:58Z

**Summary**: Software vulnerabilities pose significant risks to the security and integrity of software systems. Although prior studies have explored vulnerability detection using deep learning and pre-trained models, these approaches often fail to provide the detailed explanations necessary for developers to understand and remediate vulnerabilities effectively. The advent of large language models (LLMs) has introduced transformative potential due to their advanced generative capabilities and ability to comprehend complex contexts, offering new possibilities for addressing these challenges. In this paper, we propose LLMVulExp, an automated framework designed to specialize LLMs for the dual tasks of vulnerability detection and explanation. To address the challenges of acquiring high-quality annotated data and injecting domain-specific knowledge, LLMVulExp leverages prompt-based techniques for annotating vulnerability explanations and finetunes LLMs using instruction tuning with Low-Rank Adaptation (LoRA), enabling LLMVulExp to detect vulnerability types in code while generating detailed explanations, including the cause, location, and repair suggestions. Additionally, we employ a Chain-of-Thought (CoT) based key code extraction strategy to focus LLMs on analyzing vulnerability-prone code, further enhancing detection accuracy and explanatory depth. Our experimental results demonstrate that LLMVulExp achieves over a 90% F1 score on the SeVC dataset, effectively combining high detection accuracy with actionable and coherent explanations. This study highlights the feasibility of utilizing LLMs for real-world vulnerability detection and explanation tasks, providing critical insights into their adaptation and application in software security.

**Link**: [arxiv](http://arxiv.org/abs/2406.09701v3),  [pdf](http://arxiv.org/pdf/2406.09701v3)

**Tags**: cs.SE 



### EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents
**Authors**: Zhili Cheng, Yuge Tu, Ran Li, Shiqi Dai, Jinyi Hu, Shengding Hu, Jiahao Li, Yang Shi, Tianyu Yu, Weize Chen, Lei Shi, Maosong Sun

**Updated**: 2025-01-21T03:22:10Z

**Summary**: Multimodal Large Language Models (MLLMs) have shown significant advancements, providing a promising future for embodied agents. Existing benchmarks for evaluating MLLMs primarily utilize static images or videos, limiting assessments to non-interactive scenarios. Meanwhile, existing embodied AI benchmarks are task-specific and not diverse enough, which do not adequately evaluate the embodied capabilities of MLLMs. To address this, we propose EmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs with embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied 3D scenes, each of which is rigorously selected and annotated. It covers a broad spectrum of existing embodied AI tasks with significantly enhanced diversity, all within a unified simulation and evaluation framework tailored for MLLMs. The tasks are organized into five categories: navigation, object interaction, social interaction, attribute question answering, and spatial question answering to assess different capabilities of the agents. We evaluated the state-of-the-art MLLMs on EmbodiedEval and found that they have a significant shortfall compared to human level on embodied tasks. Our analysis demonstrates the limitations of existing MLLMs in embodied capabilities, providing insights for their future development. We open-source all evaluation data and simulation framework at https://github.com/thunlp/EmbodiedEval.

**Link**: [arxiv](http://arxiv.org/abs/2501.11858v1),  [pdf](http://arxiv.org/pdf/2501.11858v1)

**Tags**: cs.CV cs.CL 



### Network-informed Prompt Engineering against Organized Astroturf   Campaigns under Extreme Class Imbalance
**Authors**: Nikos Kanakaris, Heng Ping, Xiongye Xiao, Nesreen K. Ahmed, Luca Luceri, Emilio Ferrara, Paul Bogdan

**Updated**: 2025-01-21T03:07:21Z

**Summary**: Detecting organized political campaigns is of paramount importance in fighting against disinformation on social media. Existing approaches for the identification of such organized actions employ techniques mostly from network science, graph machine learning and natural language processing. Their ultimate goal is to analyze the relationships and interactions (e.g. re-posting) among users and the textual similarities of their posts. Despite their effectiveness in recognizing astroturf campaigns, these methods face significant challenges, notably the class imbalance in available training datasets. To mitigate this issue, recent methods usually resort to data augmentation or increasing the number of positive samples, which may not always be feasible or sufficient in real-world settings. Following a different path, in this paper, we propose a novel framework for identifying astroturf campaigns based solely on large language models (LLMs), introducing a Balanced Retrieval-Augmented Generation (Balanced RAG) component. Our approach first gives both textual information concerning the posts (in our case tweets) and the user interactions of the social network as input to a language model. Then, through prompt engineering and the proposed Balanced RAG method, it effectively detects coordinated disinformation campaigns on X (Twitter). The proposed framework does not require any training or fine-tuning of the language model. Instead, by strategically harnessing the strengths of prompt engineering and Balanced RAG, it facilitates LLMs to overcome the effects of class imbalance and effectively identify coordinated political campaigns. The experimental results demonstrate that by incorporating the proposed prompt engineering and Balanced RAG methods, our framework outperforms the traditional graph-based baselines, achieving 2x-3x improvements in terms of precision, recall and F1 scores.

**Link**: [arxiv](http://arxiv.org/abs/2501.11849v1),  [pdf](http://arxiv.org/pdf/2501.11849v1)

**Tags**: cs.CL cs.AI cs.SI 



### A Survey on Memory-Efficient Large-Scale Model Training in AI for   Science
**Authors**: Kaiyuan Tian, Linbo Qiao, Baihui Liu, Gongqingjian Jiang, Dongsheng Li

**Updated**: 2025-01-21T03:06:30Z

**Summary**: Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. To address this, we review memory-efficient training techniques for LLMs based on the transformer architecture, including distributed training, mixed precision training, and gradient checkpointing. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. We also discuss the challenges of memory optimization in practice and potential future directions, hoping to provide valuable insights for researchers and engineers.

**Link**: [arxiv](http://arxiv.org/abs/2501.11847v1),  [pdf](http://arxiv.org/pdf/2501.11847v1)

**Tags**: cs.LG cs.AI 



### Large Language Models with Human-In-The-Loop Validation for Systematic   Review Data Extraction
**Authors**: Noah L. Schroeder, Chris Davis Jaldi, Shan Zhang

**Updated**: 2025-01-21T02:49:43Z

**Summary**: Systematic reviews are time-consuming endeavors. Historically speaking, knowledgeable humans have had to screen and extract data from studies before it can be analyzed. However, large language models (LLMs) hold promise to greatly accelerate this process. After a pilot study which showed great promise, we investigated the use of freely available LLMs for extracting data for systematic reviews. Using three different LLMs, we extracted 24 types of data, 9 explicitly stated variables and 15 derived categorical variables, from 112 studies that were included in a published scoping review. Overall we found that Gemini 1.5 Flash, Gemini 1.5 Pro, and Mistral Large 2 performed reasonably well, with 71.17%, 72.14%, and 62.43% of data extracted being consistent with human coding, respectively. While promising, these results highlight the dire need for a human-in-the-loop (HIL) process for AI-assisted data extraction. As a result, we present a free, open-source program we developed (AIDE) to facilitate user-friendly, HIL data extraction with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.11840v1),  [pdf](http://arxiv.org/pdf/2501.11840v1)

**Tags**: cs.HC 



### CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based   Verification
**Authors**: Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song

**Updated**: 2025-01-21T02:45:49Z

**Summary**: Large Language Models (LLMs) have made significant progress in code generation, offering developers groundbreaking automated programming support. However, LLMs often generate code that is syntactically correct and even semantically plausible, but may not execute as expected or fulfill specified requirements. This phenomenon of hallucinations in the code domain has not been systematically explored. To advance the community's understanding and research on this issue, we introduce the concept of code hallucinations and propose a classification method for code hallucination based on execution verification. We categorize code hallucinations into four main types: mapping, naming, resource, and logic hallucinations, with each category further divided into different subcategories to understand and address the unique challenges faced by LLMs in code generation with finer granularity. Additionally, we present a dynamic detection algorithm called CodeHalu designed to detect and quantify code hallucinations. We also introduce the CodeHaluEval benchmark, which includes 8,883 samples from 699 tasks, to systematically and quantitatively evaluate code hallucinations. By evaluating 17 popular LLMs using this benchmark, we reveal significant differences in their accuracy and reliability in code generation, offering detailed insights for further improving the code generation capabilities of LLMs. The CodeHalu benchmark and code are publicly available at https://github.com/yuchen814/CodeHalu.

**Link**: [arxiv](http://arxiv.org/abs/2405.00253v4),  [pdf](http://arxiv.org/pdf/2405.00253v4)

**Tags**: cs.CL cs.SE 



### Reward-Augmented Data Enhances Direct Preference Alignment of LLMs
**Authors**: Shenao Zhang, Zhihan Liu, Boyi Liu, Yufeng Zhang, Yingxiang Yang, Yongfei Liu, Liyu Chen, Tao Sun, Zhaoran Wang

**Updated**: 2025-01-21T02:40:27Z

**Summary**: Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to responses with the highest rewards, which are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. This dataset is easily integrated with existing direct alignment algorithms and is applicable to any preference dataset. The experimental results across instruction-following benchmarks including AlpacaEval, MT-Bench, and Arena-Hard-Auto demonstrate that our approach consistently boosts the performance of DPO by a considerable margin across diverse models. Additionally, our method improves the average accuracy on various academic benchmarks. When applying our method to on-policy data, the resulting DPO model achieves SOTA results on AlpacaEval. Through ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere dataset expansion. Our code is available at https://github.com/shenao-zhang/reward-augmented-preference.

**Link**: [arxiv](http://arxiv.org/abs/2410.08067v3),  [pdf](http://arxiv.org/pdf/2410.08067v3)

**Tags**: cs.LG cs.AI 



### Hybrid Adaptive Modeling using Neural Networks Trained with Nonlinear   Dynamics Based Features
**Authors**: Zihan Liu, Prashant N. Kambali, C. Nataraj

**Updated**: 2025-01-21T02:38:28Z

**Summary**: Accurate models are essential for design, performance prediction, control, and diagnostics in complex engineering systems. Physics-based models excel during the design phase but often become outdated during system deployment due to changing operational conditions, unknown interactions, excitations, and parametric drift. While data-based models can capture the current state of complex systems, they face significant challenges, including excessive data dependence, limited generalizability to changing conditions, and inability to predict parametric dependence. This has led to combining physics and data in modeling, termed physics-infused machine learning, often using numerical simulations from physics-based models. This paper introduces a novel approach that departs from standard techniques by uncovering information from nonlinear dynamical modeling and embedding it in data-based models. The goal is to create a hybrid adaptive modeling framework that integrates data-based modeling with newly measured data and analytical nonlinear dynamical models for enhanced accuracy, parametric dependence, and improved generalizability. By explicitly incorporating nonlinear dynamic phenomena through perturbation methods, the predictive capabilities are more realistic and insightful compared to knowledge obtained from brute-force numerical simulations. In particular, perturbation methods are utilized to derive asymptotic solutions which are parameterized to generate frequency responses. Frequency responses provide comprehensive insights into dynamics and nonlinearity which are quantified and extracted as high-quality features. A machine-learning model, trained by these features, tracks parameter variations and updates the mismatched model. The results demonstrate that this adaptive modeling method outperforms numerical gray box models in prediction accuracy and computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2501.11835v1),  [pdf](http://arxiv.org/pdf/2501.11835v1)

**Tags**: cs.LG nlin.AO 



### Is your LLM trapped in a Mental Set? Investigative study on how mental   sets affect the reasoning capabilities of LLMs
**Authors**: Saiful Haq, Niyati Chhaya, Piyush Pandey, Pushpak Bhattacharya

**Updated**: 2025-01-21T02:29:15Z

**Summary**: In this paper, we present an investigative study on how Mental Sets influence the reasoning capabilities of LLMs. LLMs have excelled in diverse natural language processing (NLP) tasks, driven by advancements in parameter-efficient fine-tuning (PEFT) and emergent capabilities like in-context learning (ICL). For complex reasoning tasks, selecting the right model for PEFT or ICL is critical, often relying on scores on benchmarks such as MMLU, MATH, and GSM8K. However, current evaluation methods, based on metrics like F1 Score or reasoning chain assessments by larger models, overlook a key dimension: adaptability to unfamiliar situations and overcoming entrenched thinking patterns. In cognitive psychology, Mental Set refers to the tendency to persist with previously successful strategies, even when they become inefficient - a challenge for problem solving and reasoning. We compare the performance of LLM models like Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and GPT-4o in the presence of mental sets. To the best of our knowledge, this is the first study to integrate cognitive psychology concepts into the evaluation of LLMs for complex reasoning tasks, providing deeper insights into their adaptability and problem-solving efficacy.

**Link**: [arxiv](http://arxiv.org/abs/2501.11833v1),  [pdf](http://arxiv.org/pdf/2501.11833v1)

**Tags**: cs.CL cs.AI 



### What Lies Beneath? Exploring the Impact of Underlying AI Model Updates   in AI-Infused Systems
**Authors**: Vikram Mohanty, Jude Lim, Kurt Luther

**Updated**: 2025-01-21T02:21:04Z

**Summary**: AI models are constantly evolving, with new versions released frequently. Human-AI interaction guidelines encourage notifying users about changes in model capabilities, ideally supported by thorough benchmarking. However, as AI systems integrate into domain-specific workflows, exhaustive benchmarking can become impractical, often resulting in silent or minimally communicated updates. This raises critical questions: Can users notice these updates? What cues do they rely on to distinguish between models? How do such changes affect their behavior and task performance? We address these questions through two studies in the context of facial recognition for historical photo identification: an online experiment examining users' ability to detect model updates, followed by a diary study exploring perceptions in a real-world deployment. Our findings highlight challenges in noticing AI model updates, their impact on downstream user behavior and performance, and how they lead users to develop divergent folk theories. Drawing on these insights, we discuss strategies for effectively communicating model updates in AI-infused systems.

**Link**: [arxiv](http://arxiv.org/abs/2311.10652v3),  [pdf](http://arxiv.org/pdf/2311.10652v3)

**Tags**: cs.HC 



### PXGen: A Post-hoc Explainable Method for Generative Models
**Authors**: Yen-Lung Huang, Ming-Hsi Weng, Hao-Tsung Yang

**Updated**: 2025-01-21T02:10:50Z

**Summary**: With the rapid growth of generative AI in numerous applications, explainable AI (XAI) plays a crucial role in ensuring the responsible development and deployment of generative AI technologies. XAI has undergone notable advancements and widespread adoption in recent years, reflecting a concerted push to enhance the transparency, interpretability, and credibility of AI systems. Recent research emphasizes that a proficient XAI method should adhere to a set of criteria, primarily focusing on two key areas. Firstly, it should ensure the quality and fluidity of explanations, encompassing aspects like faithfulness, plausibility, completeness, and tailoring to individual needs. Secondly, the design principle of the XAI system or mechanism should cover the following factors such as reliability, resilience, the verifiability of its outputs, and the transparency of its algorithm. However, research in XAI for generative models remains relatively scarce, with little exploration into how such methods can effectively meet these criteria in that domain. In this work, we propose PXGen, a post-hoc explainable method for generative models. Given a model that needs to be explained, PXGen prepares two materials for the explanation, the Anchor set and intrinsic & extrinsic criteria. Those materials are customizable by users according to their purpose and requirements. Via the calculation of each criterion, each anchor has a set of feature values and PXGen provides examplebased explanation methods according to the feature values among all the anchors and illustrated and visualized to the users via tractable algorithms such as k-dispersion or k-center.

**Link**: [arxiv](http://arxiv.org/abs/2501.11827v1),  [pdf](http://arxiv.org/pdf/2501.11827v1)

**Tags**: cs.LG cs.AI 



### Assessing the Alignment of FOL Closeness Metrics with Human Judgement
**Authors**: Ramya Keerthy Thatikonda, Wray Buntine, Ehsan Shareghi

**Updated**: 2025-01-21T02:02:39Z

**Summary**: The recent successful paradigm of solving logical reasoning problems with tool-augmented large language models (LLMs) leverages translation of natural language statements into First-Order Logic~(FOL) and external theorem provers. However, the correctness of FOL statements, comprising operators and text predicates, often goes unverified due to the lack of a reliable evaluation metric for comparing generated and ground-truth FOLs. In this paper, we present a comprehensive study of sensitivity of existing metrics and their alignment with human judgement on FOL evaluation. Using ground-truth FOLs, we carefully designed various perturbations on the ground-truth to assess metric sensitivity. We sample FOL translation candidates for natural language statements and measure the ranking alignment between automatic metrics and human annotators. Our empirical findings highlight oversensitivity in the n-gram metric BLEU for text perturbations, the semantic graph metric Smatch++ for structural perturbations, and FOL metric for operator perturbation. We also observe a closer alignment between BertScore and human judgement. Additionally, we show that combining metrics enhances both alignment and sensitivity compared to using individual metrics.

**Link**: [arxiv](http://arxiv.org/abs/2501.08613v2),  [pdf](http://arxiv.org/pdf/2501.08613v2)

**Tags**: cs.CL 



### MedCT: A Clinical Terminology Graph for Generative AI Applications in   Healthcare
**Authors**: Ye Chen, Dongdong Huang, Haoyun Xu, Cong Fu, Lin Sheng, Qingli Zhou, Yuqiang Shen, Kai Wang

**Updated**: 2025-01-21T01:56:11Z

**Summary**: We introduce the world's first clinical terminology for the Chinese healthcare community, namely MedCT, accompanied by a clinical foundation model MedBERT and an entity linking model MedLink. The MedCT system enables standardized and programmable representation of Chinese clinical data, successively stimulating the development of new medicines, treatment pathways, and better patient outcomes for the populous Chinese community. Moreover, the MedCT knowledge graph provides a principled mechanism to minimize the hallucination problem of large language models (LLMs), therefore achieving significant levels of accuracy and safety in LLM-based clinical applications. By leveraging the LLMs' emergent capabilities of generativeness and expressiveness, we were able to rapidly built a production-quality terminology system and deployed to real-world clinical field within three months, while classical terminologies like SNOMED CT have gone through more than twenty years development. Our experiments show that the MedCT system achieves state-of-the-art (SOTA) performance in semantic matching and entity linking tasks, not only for Chinese but also for English. We also conducted a longitudinal field experiment by applying MedCT and LLMs in a representative spectrum of clinical tasks, including electronic health record (EHR) auto-generation and medical document search for diagnostic decision making. Our study shows a multitude of values of MedCT for clinical workflows and patient outcomes, especially in the new genre of clinical LLM applications. We present our approach in sufficient engineering detail, such that implementing a clinical terminology for other non-English societies should be readily reproducible. We openly release our terminology, models and algorithms, along with real-world clinical datasets for the development.

**Link**: [arxiv](http://arxiv.org/abs/2501.06465v2),  [pdf](http://arxiv.org/pdf/2501.06465v2)

**Tags**: cs.CL cs.AI 



### Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP   Evaluation Benchmark
**Authors**: Alexis Roger, Prateek Humane, Daniel Z. Kaplan, Kshitij Gupta, Qi Sun, George Adamopoulos, Jonathan Siu Chi Lim, Quentin Anthony, Edwin Fennell, Irina Rish

**Updated**: 2025-01-21T01:04:52Z

**Summary**: The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks. We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.

**Link**: [arxiv](http://arxiv.org/abs/2501.09672v2),  [pdf](http://arxiv.org/pdf/2501.09672v2)

**Tags**: cs.CV cs.AI 



### Quantifying the Importance of Data Alignment in Downstream Model   Performance
**Authors**: Krrish Chawla, Aryan Sahai, Mario DePavia, Sudharsan Sundar, Brando Miranda

**Updated**: 2025-01-21T01:01:37Z

**Summary**: Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.

**Link**: [arxiv](http://arxiv.org/abs/2501.08496v2),  [pdf](http://arxiv.org/pdf/2501.08496v2)

**Tags**: cs.CL cs.AI cs.LG cs.PL 



### Benchmarking Large Language Models via Random Variables
**Authors**: Zijin Hong, Hao Wu, Su Dong, Junnan Dong, Yilin Xiao, Yujing Zhang, Zhu Wang, Feiran Huang, Linyi Li, Hongxia Yang, Xiao Huang

**Updated**: 2025-01-20T23:41:22Z

**Summary**: With the continuous advancement of large language models (LLMs) in mathematical reasoning, evaluating their performance in this domain has become a prominent research focus. Recent studies have raised concerns about the reliability of current mathematical benchmarks, highlighting issues such as simplistic design and potential data leakage. Therefore, creating a reliable benchmark that effectively evaluates the genuine capabilities of LLMs in mathematical reasoning remains a significant challenge. To address this, we propose RV-Bench, a framework for Benchmarking LLMs via Random Variables in mathematical reasoning. Specifically, the background content of a random variable question (RV question) mirrors the original problem in existing standard benchmarks, but the variable combinations are randomized into different values. LLMs must fully understand the problem-solving process for the original problem to correctly answer RV questions with various combinations of variable values. As a result, the LLM's genuine capability in mathematical reasoning is reflected by its accuracy on RV-Bench. Extensive experiments are conducted with 29 representative LLMs across 900+ RV questions. A leaderboard for RV-Bench ranks the genuine capability of these LLMs. Further analysis of accuracy dropping indicates that current LLMs still struggle with complex mathematical reasoning problems.

**Link**: [arxiv](http://arxiv.org/abs/2501.11790v1),  [pdf](http://arxiv.org/pdf/2501.11790v1)

**Tags**: cs.CL cs.AI 



### VLM Agents Generate Their Own Memories: Distilling Experience into   Embodied Programs of Thought
**Authors**: Gabriel Sarch, Lawrence Jang, Michael J. Tarr, William W. Cohen, Kenneth Marino, Katerina Fragkiadaki

**Updated**: 2025-01-20T23:33:33Z

**Summary**: Large-scale LLMs and VLMs excel at few-shot learning but require high-quality examples. We introduce In-Context Abstraction Learning (ICAL), which iteratively refines suboptimal trajectories into high-quality data with optimized actions and detailed reasoning. Given an inefficient demonstration, a VLM corrects actions and annotates causal relationships, object states, subgoals, and task-relevant visuals, forming "programs of thought." With human feedback, these programs are improved as the agent executes them in a similar environment. The resulting examples, used as prompt context or fine-tuning data, significantly boost decision-making while reducing human feedback needs. ICAL surpasses state-of-the-art in TEACh (dialogue-based instruction following), VisualWebArena (multimodal web agents), and Ego4D (egocentric video action anticipation). In TEACh, combining fine-tuning and retrieval on ICAL examples outperforms raw human demonstrations and expert examples, achieving a 17.5% increase in goal-condition success. In VisualWebArena, retrieval-augmented GPT-4V with ICAL improves task success rate 1.6x over GPT-4V, while fine-tuning Qwen2-VL achieves a 2.8x improvement. In Ego4D, ICAL outperforms few-shot GPT-4V and remains competitive with supervised models. Overall, ICAL scales 2x better than raw human demonstrations and reduces manual prompt engineering.

**Link**: [arxiv](http://arxiv.org/abs/2406.14596v5),  [pdf](http://arxiv.org/pdf/2406.14596v5)

**Tags**: cs.CV cs.AI cs.LG 



### Synthetic Data Can Mislead Evaluations: Membership Inference as Machine   Text Detection
**Authors**: Ali Naseh, Niloofar Mireshghallah

**Updated**: 2025-01-20T23:19:15Z

**Summary**: Recent work shows membership inference attacks (MIAs) on large language models (LLMs) produce inconclusive results, partly due to difficulties in creating non-member datasets without temporal shifts. While researchers have turned to synthetic data as an alternative, we show this approach can be fundamentally misleading. Our experiments indicate that MIAs function as machine-generated text detectors, incorrectly identifying synthetic data as training samples regardless of the data source. This behavior persists across different model architectures and sizes, from open-source models to commercial ones such as GPT-3.5. Even synthetic text generated by different, potentially larger models is classified as training data by the target model. Our findings highlight a serious concern: using synthetic data in membership evaluations may lead to false conclusions about model memorization and data leakage. We caution that this issue could affect other evaluations using model signals such as loss where synthetic or machine-generated translated data substitutes for real-world samples.

**Link**: [arxiv](http://arxiv.org/abs/2501.11786v1),  [pdf](http://arxiv.org/pdf/2501.11786v1)

**Tags**: cs.CL cs.CR cs.LG 



### Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference
**Authors**: Pouya Hamadanian, Sadjad Fouladi

**Updated**: 2025-01-20T23:10:13Z

**Summary**: Large Language Models (LLM) have revolutionized natural language processing, but their inference demands substantial resources, while under-utilizing high-end accelerators like GPUs. A major bottleneck arises from the attention mechanism, which requires storing large key-value caches, limiting the maximum achievable throughput way below the available computing resources. Current approaches attempt to mitigate this issue through memory-efficient attention and paging mechanisms, but remained constrained by the assumption that all operations must be performed on high-end accelerators.   In this work, we propose Glinthawk, a two-tiered architecture that decouples the attention mechanism from the rest of the Transformer model. This approach allows the memory requirements for attention to scale independently, enabling larger batch sizes and more efficient use of the high-end accelerators. We prototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the other. Compared to a traditional single-tier setup, it improves throughput by $5.9\times$ and reduces cost of generation by $2.8\times$. For longer sequence lengths, it achieves $16.3\times$ throughput improvement at $2.4\times$ less cost. Our evaluation shows that this architecture can tolerate moderate network latency with minimal performance degradation, making it highly effective for latency-tolerant, throughput-oriented applications such as batch processing. We shared our prototype publicly at \url{https://github.com/microsoft/glinthawk}.

**Link**: [arxiv](http://arxiv.org/abs/2501.11779v1),  [pdf](http://arxiv.org/pdf/2501.11779v1)

**Tags**: cs.LG cs.DC cs.PF 



### EfficientVITON: An Efficient Virtual Try-On Model using Optimized   Diffusion Process
**Authors**: Mostafa Atef, Mariam Ayman, Ahmed Rashed, Ashrakat Saeed, Abdelrahman Saeed, Ahmed Fares

**Updated**: 2025-01-20T22:44:53Z

**Summary**: Would not it be much more convenient for everybody to try on clothes by only looking into a mirror ? The answer to that problem is virtual try-on, enabling users to digitally experiment with outfits. The core challenge lies in realistic image-to-image translation, where clothing must fit diverse human forms, poses, and figures. Early methods, which used 2D transformations, offered speed, but image quality was often disappointing and lacked the nuance of deep learning. Though GAN-based techniques enhanced realism, their dependence on paired data proved limiting. More adaptable methods offered great visuals but demanded significant computing power and time. Recent advances in diffusion models have shown promise for high-fidelity translation, yet the current crop of virtual try-on tools still struggle with detail loss and warping issues. To tackle these challenges, this paper proposes EfficientVITON, a new virtual try-on system leveraging the impressive pre-trained Stable Diffusion model for better images and deployment feasibility. The system includes a spatial encoder to maintain clothings finer details and zero cross-attention blocks to capture the subtleties of how clothes fit a human body. Input images are carefully prepared, and the diffusion process has been tweaked to significantly cut generation time without image quality loss. The training process involves two distinct stages of fine-tuning, carefully incorporating a balance of loss functions to ensure both accurate try-on results and high-quality visuals. Rigorous testing on the VITON-HD dataset, supplemented with real-world examples, has demonstrated that EfficientVITON achieves state-of-the-art results.

**Link**: [arxiv](http://arxiv.org/abs/2501.11776v1),  [pdf](http://arxiv.org/pdf/2501.11776v1)

**Tags**: cs.CV 



### BEAVER: An Enterprise Benchmark for Text-to-SQL
**Authors**: Peter Baile Chen, Fabian Wenz, Yi Zhang, Devin Yang, Justin Choi, Nesime Tatbul, Michael Cafarella, ÃaÄatay Demiralp, Michael Stonebraker

**Updated**: 2025-01-20T22:24:48Z

**Summary**: Existing text-to-SQL benchmarks have largely been constructed from web tables with human-generated question-SQL pairs. LLMs typically show strong results on these benchmarks, leading to a belief that LLMs are effective at text-to-SQL tasks. However, how these results transfer to enterprise settings is unclear because tables in enterprise databases might differ substantially from web tables in structure and content. To contend with this problem, we introduce a new dataset BEAVER, the first enterprise text-to-SQL benchmark sourced from real private enterprise data warehouses. This dataset includes natural language queries and their correct SQL statements, which we collected from actual query logs. We then benchmark off-the-shelf LLMs on this dataset. LLMs perform poorly, even when augmented with standard prompt engineering and RAG techniques. We identify three main reasons for the poor performance: (1) schemas of enterprise tables are more complex than the schemas in public data, resulting in SQL-generation tasks intrinsically harder; (2) business-oriented questions are often more complex, requiring joins over multiple tables, aggregations, and nested queries; (3) public LLMs cannot train on private enterprise data warehouses that are not publicly accessible, and therefore it is difficult for the model to learn to solve (1) and (2). We believe BEAVER will facilitate future research in building text-to-SQL systems that perform better in enterprise settings.

**Link**: [arxiv](http://arxiv.org/abs/2409.02038v2),  [pdf](http://arxiv.org/pdf/2409.02038v2)

**Tags**: cs.CL cs.AI cs.DB 



### PDF-WuKong: A Large Multimodal Model for Efficient Long PDF Reading with   End-to-End Sparse Sampling
**Authors**: Xudong Xie, Hao Yan, Liang Yin, Yang Liu, Jing Ding, Minghui Liao, Yuliang Liu, Wei Chen, Xiang Bai

**Updated**: 2025-01-20T21:45:19Z

**Summary**: Multimodal document understanding is a challenging task to process and comprehend large amounts of textual and visual information. Recent advances in Large Language Models (LLMs) have significantly improved the performance of this task. However, existing methods typically focus on either plain text or a limited number of document images, struggling to handle long PDF documents with interleaved text and images, especially for academic papers. In this paper, we introduce PDF-WuKong, a multimodal large language model (MLLM) which is designed to enhance multimodal question-answering (QA) for long PDF documents. PDF-WuKong incorporates a sparse sampler that operates on both text and image representations, significantly improving the efficiency and capability of the MLLM. The sparse sampler is integrated with the MLLM's image encoder and selects the paragraphs or diagrams most pertinent to user queries for processing by the language model. To effectively train and evaluate our model, we construct PaperPDF, a dataset consisting of a broad collection of English and Chinese academic papers. Multiple strategies are proposed to automatically generate 1.1 million QA pairs along with their corresponding evidence sources. Experimental results demonstrate the superiority and high efficiency of our approach over other models on the task of long multimodal document understanding, surpassing proprietary products by an average of 8.6% on F1. Our code and dataset will be released at https://github.com/yh-hust/PDF-Wukong.

**Link**: [arxiv](http://arxiv.org/abs/2410.05970v2),  [pdf](http://arxiv.org/pdf/2410.05970v2)

**Tags**: cs.CV cs.AI cs.CL 



### Poison-RAG: Adversarial Data Poisoning Attacks on Retrieval-Augmented   Generation in Recommender Systems
**Authors**: Fatemeh Nazary, Yashar Deldjoo, Tommaso di Noia

**Updated**: 2025-01-20T21:38:36Z

**Summary**: This study presents Poison-RAG, a framework for adversarial data poisoning attacks targeting retrieval-augmented generation (RAG)-based recommender systems. Poison-RAG manipulates item metadata, such as tags and descriptions, to influence recommendation outcomes. Using item metadata generated through a large language model (LLM) and embeddings derived via the OpenAI API, we explore the impact of adversarial poisoning attacks on provider-side, where attacks are designed to promote long-tail items and demote popular ones. Two attack strategies are proposed: local modifications, which personalize tags for each item using BERT embeddings, and global modifications, applying uniform tags across the dataset. Experiments conducted on the MovieLens dataset in a black-box setting reveal that local strategies improve manipulation effectiveness by up to 50\%, while global strategies risk boosting already popular items. Results indicate that popular items are more susceptible to attacks, whereas long-tail items are harder to manipulate. Approximately 70\% of items lack tags, presenting a cold-start challenge; data augmentation and synthesis are proposed as potential defense mechanisms to enhance RAG-based systems' resilience. The findings emphasize the need for robust metadata management to safeguard recommendation frameworks. Code and data are available at https://github.com/atenanaz/Poison-RAG.

**Link**: [arxiv](http://arxiv.org/abs/2501.11759v1),  [pdf](http://arxiv.org/pdf/2501.11759v1)

**Tags**: cs.IR 



### Optimizing Pretraining Data Mixtures with LLM-Estimated Utility
**Authors**: William Held, Bhargavi Paranjape, Punit Singh Koura, Mike Lewis, Frank Zhang, Todor Mihaylov

**Updated**: 2025-01-20T21:10:22Z

**Summary**: Large Language Models improve with increasing amounts of high-quality training data. However, leveraging larger datasets requires balancing quality, quantity, and diversity across sources. After evaluating nine baseline methods under both compute- and data-constrained scenarios, we find token-count heuristics outperform manual and learned mixes, indicating that simple approaches accounting for dataset size and diversity are surprisingly effective. Building on this insight, we propose two complementary approaches: UtiliMax, which extends token-based heuristics by incorporating utility estimates from reduced-scale ablations, achieving up to a 10.6x speedup over manual baselines; and Model Estimated Data Utility (MEDU), which leverages LLMs to estimate data utility from small samples, matching ablation-based performance while reducing computational requirements by $\sim$200x. Together, these approaches establish a new framework for automated, compute-efficient data mixing that is robust across training regimes.

**Link**: [arxiv](http://arxiv.org/abs/2501.11747v1),  [pdf](http://arxiv.org/pdf/2501.11747v1)

**Tags**: cs.CL cs.AI 



### Explain-Query-Test: Self-Evaluating LLMs Via Explanation and   Comprehension Discrepancy
**Authors**: Saeid Asgari Taghanaki, Joao Monteiro

**Updated**: 2025-01-20T20:07:18Z

**Summary**: Large language models (LLMs) have demonstrated remarkable proficiency in generating detailed and coherent explanations of complex concepts. However, the extent to which these models truly comprehend the concepts they articulate remains unclear. To assess the level of comprehension of a model relative to the content it generates, we implemented a self-evaluation pipeline where models: (i) given a topic generate an excerpt with information about the topic, (ii) given an excerpt generate question-answer pairs, and finally (iii) given a question generate an answer. We refer to this self-evaluation approach as Explain-Query-Test (EQT). Interestingly, the accuracy on generated questions resulting from running the EQT pipeline correlates strongly with the model performance as verified by typical benchmarks such as MMLU-Pro. In other words, EQT's performance is predictive of MMLU-Pro's, and EQT can be used to rank models without the need for any external source of evaluation data other than lists of topics of interest. Moreover, our results reveal a disparity between the models' ability to produce detailed explanations and their performance on questions related to those explanations. This gap highlights fundamental limitations in the internal knowledge representation and reasoning abilities of current LLMs. We release the code at https://github.com/asgsaeid/EQT.

**Link**: [arxiv](http://arxiv.org/abs/2501.11721v1),  [pdf](http://arxiv.org/pdf/2501.11721v1)

**Tags**: cs.CL cs.LG 



### Smells-sus: Sustainability Smells in IaC
**Authors**: Seif Kosbar, Mohammad Hamdaqa

**Updated**: 2025-01-20T20:05:35Z

**Summary**: Practitioners use Infrastructure as Code (IaC) scripts to efficiently configure IT infrastructures through machine-readable definition files. However, during the development of these scripts, some code patterns or deployment choices may lead to sustainability issues like inefficient resource utilization or redundant provisioning for example. We call this type of patterns sustainability smells. These inefficiencies pose significant environmental and financial challenges, given the growing scale of cloud computing. This research focuses on Terraform, a widely adopted IaC tool. Our study involves defining seven sustainability smells and validating them through a survey with 19 IaC practitioners. We utilized a dataset of 28,327 Terraform scripts from 395 open-source repositories. We performed a detailed qualitative analysis of a randomly sampled 1,860 Terraform scripts from the original dataset to identify code patterns that correspond to the sustainability smells and used the other 26,467 Terraform scripts to study the prevalence of the defined sustainability smells. Our results indicate varying prevalence rates of these smells across the dataset. The most prevalent smell is Monolithic Infrastructure, which appears in 9.67\% of the scripts. Additionally, our findings highlight the complexity of conducting root cause analysis for sustainability issues, as these smells often arise from a confluence of script structures, configuration choices, and deployment contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.07676v2),  [pdf](http://arxiv.org/pdf/2501.07676v2)

**Tags**: cs.SE cs.DC cs.NI 



### Towards Detecting Prompt Knowledge Gaps for Improved LLM-guided Issue   Resolution
**Authors**: Ramtin Ehsani, Sakshi Pathak, Preetha Chatterjee

**Updated**: 2025-01-20T19:41:42Z

**Summary**: Large language models (LLMs) have become essential in software development, especially for issue resolution. However, despite their widespread use, significant challenges persist in the quality of LLM responses to issue resolution queries. LLM interactions often yield incorrect, incomplete, or ambiguous information, largely due to knowledge gaps in prompt design, which can lead to unproductive exchanges and reduced developer productivity. In this paper, we analyze 433 developer-ChatGPT conversations within GitHub issue threads to examine the impact of prompt knowledge gaps and conversation styles on issue resolution. We identify four main knowledge gaps in developer prompts: Missing Context, Missing Specifications, Multiple Context, and Unclear Instructions. Assuming that conversations within closed issues contributed to successful resolutions while those in open issues did not, we find that ineffective conversations contain knowledge gaps in 54.7% of prompts, compared to only 13.2% in effective ones. Additionally, we observe seven distinct conversational styles, with Directive Prompting, Chain of Thought, and Responsive Feedback being the most prevalent. We find that knowledge gaps are present in all styles of conversations, with Missing Context being the most repeated challenge developers face in issue-resolution conversations. Based on our analysis, we identify key textual and code related heuristics-Specificity, Contextual Richness, and Clarity-that are associated with successful issue closure and help assess prompt quality. These heuristics lay the foundation for an automated tool that can dynamically flag unclear prompts and suggest structured improvements. To test feasibility, we developed a lightweight browser extension prototype for detecting prompt gaps, that can be easily adapted to other tools within developer workflows.

**Link**: [arxiv](http://arxiv.org/abs/2501.11709v1),  [pdf](http://arxiv.org/pdf/2501.11709v1)

**Tags**: cs.SE 



### Estimating Rural Path Loss with ITU-R P.1812-7 : Impact of Geospatial   Inputs
**Authors**: Mathieu Chateauvert, Jonathan Ethier, Adrian Florea

**Updated**: 2025-01-20T19:40:45Z

**Summary**: Accurate radio wave propagation modeling is essential for effective spectrum management by regulators and network deployment by operators. This paper investigates the ITU-R P.1812-7 (P.1812) propagation model's reliance on geospatial inputs, particularly clutter information, to improve path loss estimation, with an emphasis on rural geographic regions. The research evaluates the impact of geospatial elevation and land cover datasets, including Global Forest Canopy Height (GFCH), European Space Agency WorldCover, and Natural Resources Canada LandCover, on P.1812 propagation model prediction accuracy. Results highlight the trade-offs between dataset resolution, geospatial data availability, and representative clutter height assignments. Simulations reveal that high-resolution data do not always yield better results and that global datasets such as the GFCH provide a robust alternative when high-resolution data are unavailable or out-of-date. This study provides a set of guidelines for geospatial dataset integration to enhance P.1812's rural path loss predictions.

**Link**: [arxiv](http://arxiv.org/abs/2501.11708v1),  [pdf](http://arxiv.org/pdf/2501.11708v1)

**Tags**: eess.SP 



### Trustformer: A Trusted Federated Transformer
**Authors**: Ali Abbasi Tadi, Dima Alhadidi, Luis Rueda

**Updated**: 2025-01-20T19:38:50Z

**Summary**: Transformers, a cornerstone of deep-learning architectures for sequential data, have achieved state-of-the-art results in tasks like Natural Language Processing (NLP). Models such as BERT and GPT-3 exemplify their success and have driven the rise of large language models (LLMs). However, a critical challenge persists: safeguarding the privacy of data used in LLM training. Privacy-preserving techniques like Federated Learning (FL) offer potential solutions, but practical limitations hinder their effectiveness for Transformer training. Two primary issues are (I) the risk of sensitive information leakage due to aggregation methods like FedAvg or FedSGD, and (II) the high communication overhead caused by the large size of Transformer models.   This paper introduces a novel FL method that reduces communication overhead while maintaining competitive utility. Our approach avoids sharing full model weights by simulating a global model locally. We apply k-means clustering to each Transformer layer, compute centroids locally, and transmit only these centroids to the server instead of full weights or gradients. To enhance security, we leverage Intel SGX for secure transmission of centroids. Evaluated on a translation task, our method achieves utility comparable to state-of-the-art baselines while significantly reducing communication costs. This provides a more efficient and privacy-preserving FL solution for Transformer models.

**Link**: [arxiv](http://arxiv.org/abs/2501.11706v1),  [pdf](http://arxiv.org/pdf/2501.11706v1)

**Tags**: cs.LG cs.CR 



### Human services organizations and the responsible integration of AI:   Considering ethics and contextualizing risk(s)
**Authors**: Brian E. Perron, Lauri Goldkind, Zia Qi, Bryan G. Victor

**Updated**: 2025-01-20T19:38:21Z

**Summary**: This paper examines the responsible integration of artificial intelligence (AI) in human services organizations (HSOs), proposing a nuanced framework for evaluating AI applications across multiple dimensions of risk. The authors argue that ethical concerns about AI deployment -- including professional judgment displacement, environmental impact, model bias, and data laborer exploitation -- vary significantly based on implementation context and specific use cases. They challenge the binary view of AI adoption, demonstrating how different applications present varying levels of risk that can often be effectively managed through careful implementation strategies. The paper highlights promising solutions, such as local large language models, that can facilitate responsible AI integration while addressing common ethical concerns. The authors propose a dimensional risk assessment approach that considers factors like data sensitivity, professional oversight requirements, and potential impact on client wellbeing. They conclude by outlining a path forward that emphasizes empirical evaluation, starting with lower-risk applications and building evidence-based understanding through careful experimentation. This approach enables organizations to maintain high ethical standards while thoughtfully exploring how AI might enhance their capacity to serve clients and communities effectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.11705v1),  [pdf](http://arxiv.org/pdf/2501.11705v1)

**Tags**: cs.CY cs.AI 



### Advancing Language Model Reasoning through Reinforcement Learning and   Inference Scaling
**Authors**: Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong

**Updated**: 2025-01-20T18:33:33Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks. However, existing approaches mainly rely on imitation learning and struggle to achieve effective test-time scaling. While reinforcement learning (RL) holds promise for enabling self-exploration and learning from feedback, recent attempts yield only modest improvements in complex reasoning. In this paper, we present T1 to scale RL by encouraging exploration and understand inference scaling. We first initialize the LLM using synthesized chain-of-thought data that integrates trial-and-error and self-verification. To scale RL training, we promote increased sampling diversity through oversampling. We further employ an entropy bonus as an auxiliary loss, alongside a dynamic anchor for regularization to facilitate reward optimization. We demonstrate that T1 with open LLMs as its base exhibits inference scaling behavior and achieves superior performance on challenging math reasoning benchmarks. For example, T1 with Qwen2.5-32B as the base model outperforms the recent Qwen QwQ-32B-Preview model on MATH500, AIME2024, and Omni-math-500. More importantly, we present a simple strategy to examine inference scaling, where increased inference budgets directly lead to T1's better performance without any additional verification. We will open-source the T1 models and the data used to train them at \url{https://github.com/THUDM/T1}.

**Link**: [arxiv](http://arxiv.org/abs/2501.11651v1),  [pdf](http://arxiv.org/pdf/2501.11651v1)

**Tags**: cs.LG cs.CL 



### Machine Against the RAG: Jamming Retrieval-Augmented Generation with   Blocker Documents
**Authors**: Avital Shafran, Roei Schuster, Vitaly Shmatikov

**Updated**: 2025-01-20T18:01:06Z

**Summary**: Retrieval-augmented generation (RAG) systems respond to queries by retrieving relevant documents from a knowledge database and applying an LLM to the retrieved documents. We demonstrate that RAG systems that operate on databases with untrusted content are vulnerable to denial-of-service attacks we call jamming. An adversary can add a single ``blocker'' document to the database that will be retrieved in response to a specific query and result in the RAG system not answering this query - ostensibly because it lacks the relevant information or because the answer is unsafe.   We describe and measure the efficacy of several methods for generating blocker documents, including a new method based on black-box optimization. This method (1) does not rely on instruction injection, (2) does not require the adversary to know the embedding or LLM used by the target RAG system, and (3) does not rely on an auxiliary LLM.   We evaluate jamming attacks on several LLMs and embeddings and demonstrate that the existing safety metrics for LLMs do not capture their vulnerability to jamming. We then discuss defenses against blocker documents.

**Link**: [arxiv](http://arxiv.org/abs/2406.05870v3),  [pdf](http://arxiv.org/pdf/2406.05870v3)

**Tags**: cs.CR cs.CL cs.LG 



### Early evidence of how LLMs outperform traditional systems on OCR/HTR   tasks for historical records
**Authors**: Seorin Kim, Julien Baudru, Wouter Ryckbosch, Hugues Bersini, Vincent Ginis

**Updated**: 2025-01-20T17:46:12Z

**Summary**: We explore the ability of two LLMs -- GPT-4o and Claude Sonnet 3.5 -- to transcribe historical handwritten documents in a tabular format and compare their performance to traditional OCR/HTR systems: EasyOCR, Keras, Pytesseract, and TrOCR. Considering the tabular form of the data, two types of experiments are executed: one where the images are split line by line and the other where the entire scan is used as input. Based on CER and BLEU, we demonstrate that LLMs outperform the conventional OCR/HTR methods. Moreover, we also compare the evaluated CER and BLEU scores to human evaluations to better judge the outputs of whole-scan experiments and understand influential factors for CER and BLEU. Combining judgments from all the evaluation metrics, we conclude that two-shot GPT-4o for line-by-line images and two-shot Claude Sonnet 3.5 for whole-scan images yield the transcriptions of the historical records most similar to the ground truth.

**Link**: [arxiv](http://arxiv.org/abs/2501.11623v1),  [pdf](http://arxiv.org/pdf/2501.11623v1)

**Tags**: cs.CV cs.AI cs.LG 



### Navigating the Designs of Privacy-Preserving Fine-tuning for Large   Language Models
**Authors**: Haonan Shi, Tu Ouyang, An Wang

**Updated**: 2025-01-20T17:41:59Z

**Summary**: Instruction tuning has proven effective in enhancing Large Language Models' (LLMs) performance on downstream tasks. However, real-world fine-tuning faces inherent conflicts between model providers' intellectual property protection, clients' data privacy requirements, and tuning costs. While recent approaches like split learning and offsite tuning demonstrate promising architectures for privacy-preserving fine-tuning, there is a gap in systematically addressing the multidimensional trade-offs required for diverse real-world deployments. We propose several indicative evaluation metrics to guide design trade-offs for privacy-preserving fine-tuning and a series of example designs, collectively named GuardedTuning; they result from novel combinations of system architectures with adapted privacy-enhancement methods and emerging computation techniques. Each design represents distinct trade-offs across model utility, privacy guarantees, and costs. Experimental results demonstrate that these designs protect against data reconstruction attacks while maintaining competitive fine-tuning performance.

**Link**: [arxiv](http://arxiv.org/abs/2501.04323v3),  [pdf](http://arxiv.org/pdf/2501.04323v3)

**Tags**: cs.LG cs.CR 



### Potential and Perils of Large Language Models as Judges of Unstructured   Textual Data
**Authors**: Rewina Bedemariam, Natalie Perez, Sreyoshi Bhaduri, Satya Kapoor, Alex Gil, Elizabeth Conjar, Ikkei Itoku, David Theil, Aman Chadha, Naumaan Nayyar

**Updated**: 2025-01-20T17:34:20Z

**Summary**: Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLM-as-judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as judges. This LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLM-as-judge offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. Our research contributes to the growing body of knowledge on AI assisted text analysis. Further, we provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM-as-judge models across various contexts and use cases.

**Link**: [arxiv](http://arxiv.org/abs/2501.08167v2),  [pdf](http://arxiv.org/pdf/2501.08167v2)

**Tags**: cs.CL cs.AI cs.CY 



### Conversation Routines: A Prompt Engineering Framework for Task-Oriented   Dialog Systems
**Authors**: Giorgio Robino

**Updated**: 2025-01-20T17:19:02Z

**Summary**: This study introduces Conversation Routines (CR), a structured prompt engineering framework for developing task-oriented dialog systems using Large Language Models (LLMs). While LLMs demonstrate remarkable natural language understanding capabilities, engineering them to reliably execute complex business workflows remains challenging. The proposed CR framework enables the development of Conversation Agentic Systems (CAS) through natural language specifications, embedding task-oriented logic within LLM prompts. This approach provides a systematic methodology for designing and implementing complex conversational workflows while maintaining behavioral consistency. We demonstrate the framework's effectiveness through two proof of concept implementations: a Train Ticket Booking System and an Interactive Troubleshooting Copilot. These case studies validate CR's capability to encode sophisticated behavioral patterns and decision logic while preserving natural conversational flexibility. Results show that CR enables domain experts to design conversational workflows in natural language while leveraging custom enterprise functionalities (tools) developed by software engineers, creating an efficient division of responsibilities where developers focus on core API implementation and domain experts handle conversation design. While the framework shows promise in accessibility and adaptability, we identify key challenges including computational overhead, non-deterministic behavior, and domain-specific logic optimization. Future research directions include enhancing system robustness, improving scalability for complex multi-agent interactions, and addressing the identified limitations across diverse business applications.

**Link**: [arxiv](http://arxiv.org/abs/2501.11613v1),  [pdf](http://arxiv.org/pdf/2501.11613v1)

**Tags**: cs.CL cs.AI cs.ET cs.HC cs.PL 



### SR-FoT: A Syllogistic-Reasoning Framework of Thought for Large Language   Models Tackling Knowledge-based Reasoning Tasks
**Authors**: Wentao Wan, Zhuojie Yang, Yongcan Chen, Chenglin Luo, Ruilin Wang, Kehao Cai, Nan Kang, Liang Lin, Keze Wang

**Updated**: 2025-01-20T17:00:41Z

**Summary**: Deductive reasoning is a crucial logical capability that assists us in solving complex problems based on existing knowledge. Although augmented by Chain-of-Thought prompts, Large Language Models (LLMs) might not follow the correct reasoning paths. Enhancing the deductive reasoning abilities of LLMs, and leveraging their extensive built-in knowledge for various reasoning tasks, remains an open question. Attempting to mimic the human deductive reasoning paradigm, we propose a multi-stage Syllogistic-Reasoning Framework of Thought (SR-FoT) that enables LLMs to perform syllogistic deductive reasoning to handle complex knowledge-based reasoning tasks. Our SR-FoT begins by interpreting the question and then uses the interpretation and the original question to propose a suitable major premise. It proceeds by generating and answering minor premise questions in two stages to match the minor premises. Finally, it guides LLMs to use the previously generated major and minor premises to perform syllogistic deductive reasoning to derive the answer to the original question. Extensive and thorough experiments on knowledge-based reasoning tasks have demonstrated the effectiveness and advantages of our SR-FoT.

**Link**: [arxiv](http://arxiv.org/abs/2501.11599v1),  [pdf](http://arxiv.org/pdf/2501.11599v1)

**Tags**: cs.AI cs.CL 



