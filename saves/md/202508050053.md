# Arxiv Results
## Keyword: kv cache 
 ### Study of the HV power supply modules for the CUbesat Solar Polarimeter   (CUSP)
**Authors**: Alessandro Lacerenza, Alda Rubini, Andrea Alimenti, Sergio Fabiani, Ettore Del Monte, Riccardo Campana, Mauro Centrone, Enrico Costa, Nicolas De Angelis, Giovanni De Cesare, Sergio Di Cosimo, Giuseppe Di Persio, Abhay Kumar, Pasqualino Loffredo, Giovanni Lombardi, Gabriele Minervini, Fabio Muleri, Paolo Romano, Emanuele Scalise, Enrico Silva, Paolo Soffitta, Davide Albanesi, Ilaria Baffo, Daniele Brienza, Valerio Campamaggiore, Giovanni Cucinella, Andrea Curatolo, Giulia de Iulis, Andrea Del Re, Vito Di Bari, Simone Di Filippo, Immacolata Donnarumma, Pierluigi Fanelli, Nicolas Gagliardi, Paolo Leonetti, Matteo Merge, Dario Modenini, Andrea Negri, Daniele Pecorella, Massimo Perelli, Alice Ponti, Francesca Sbop, Paolo Tortora, Alessandro Turchi, Valerio Vagelli, Emanuele Zaccagnino, Alessandro Zambardi, Costantino Zazza

**Updated**: 2025-08-01T14:05:44Z

**Summary**: The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting the Earth aimed to measure the linear polarization of solar flares in the hard X-ray band by means of a Compton scattering polarimeter. CUSP will allow to study the magnetic reconnection and particle acceleration in the flaring magnetic structures of our star. CUSP is a project in the framework of the Alcor Program of the Italian Space Agency aimed to develop new CubeSat missions. CUSP undergoing the Phase B started in December 2024 that will last for 12 month. The Compton polarimeter of the CUSP payload performs coincidence measurements between plastic scintilaltors and GaGG(Ce) crystals to derive the polarization of X-rays. These sensors are readout by Multi Anode Photomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively. Both sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V (for the APD). We tested precision regulated High Voltage DC/DC Converters by HVM Technology Inc. with Sub-Miniature Case Size ($0.85''\times0.85''\times0.60''$) of the SMHV series. These modules are compact and suited for CubeSat missions.

**Link**: [arxiv](http://arxiv.org/abs/2508.00647v1),  [pdf](http://arxiv.org/pdf/2508.00647v1)

**Tags**: astro-ph.IM astro-ph.SR physics.space-ph 



### Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight   Approach
**Authors**: Francisco Crespo, Javier Villegas, Carlos Baena, Eduardo Baena, Sergio Fortes, Raquel Barco

**Updated**: 2025-08-01T13:40:52Z

**Summary**: The transition toward softwarized Radio Access Networks (RANs), driven by the Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through disaggregation and virtualization of base station functions. However, this shift introduces new challenges in managing CPU resources efficiently under strict real-time constraints. In particular, the interplay between latency-sensitive RAN workloads and general-purpose Operating System (OS) schedulers often leads to sub-optimal performance and unnecessary energy consumption. This work proposes a lightweight, programmable distributed application (dApp) deployed at the Distributed Unit (DU) level to dynamically orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging thread-level telemetry like context switches, Instructions Per Cycle (IPC), and cache metrics, to adapt CPU thread affinity, core isolation, and frequency scaling in real time. Unlike existing solutions, it requires no access to proprietary RAN software, hardware-specific features, or kernel modifications. Fully compliant with the O-RAN architecture and agnostic to the underlying RAN stack, the proposed solution introduces negligible overhead while improving energy efficiency and CPU utilization. Experimental results using a commercial-grade srsRAN deployment demonstrate consistent power savings without compromising real-time processing performance, highlighting the potential of low-latency dApps for fine-grained resource control in next-generation networks

**Link**: [arxiv](http://arxiv.org/abs/2508.00629v1),  [pdf](http://arxiv.org/pdf/2508.00629v1)

**Tags**: cs.NI cs.OS cs.PF 



### Joint Association and Phase Shifts Design for UAV-mounted Stacked   Intelligent Metasurfaces-assisted Communications
**Authors**: Mingzhe Fan, Geng Sun, Hongyang Pan, Jiacheng Wang, Jiancheng An, Hongyang Du, Chau Yuen

**Updated**: 2025-08-01T13:25:28Z

**Summary**: Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for realizing wave-domain signal processing, while the fixed SIMs will limit the communication performance of the system compared to the mobile SIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted communication system, where UAVs as base stations (BSs) can cache the data processed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance the communication performance. To this end, we formulate a UAV-SIM-based joint optimization problem (USBJOP) to comprehensively consider the association between UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of UAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and NP-hardness of USBJOP, we decompose it into three sub-optimization problems, which are the association between UAV-SIMs and users optimization problem (AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase shifts optimization problem (USPSOP). Then, these three sub-optimization problems are solved by an alternating optimization (AO) strategy. Specifically, AUUOP and ULOP are transformed to a convex form and then solved by the CVX tool, while we employ a layer-by-layer iterative optimization method for USPSOP. Simulation results verify the effectiveness of the proposed strategy under different simulation setups.

**Link**: [arxiv](http://arxiv.org/abs/2508.00616v1),  [pdf](http://arxiv.org/pdf/2508.00616v1)

**Tags**: cs.NI 



### Sortblock: Similarity-Aware Feature Reuse for Diffusion Model
**Authors**: Hanqi Chen, Xu Zhang, Xiaoliu Guan, Lielin Jiang, Guanzhong Wang, Zeyu Chen, Yi Liu

**Updated**: 2025-08-01T08:10:54Z

**Summary**: Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.

**Link**: [arxiv](http://arxiv.org/abs/2508.00412v1),  [pdf](http://arxiv.org/pdf/2508.00412v1)

**Tags**: cs.CV 



### EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level   Efficiency for Edge Devices
**Authors**: Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun

**Updated**: 2025-08-01T07:03:16Z

**Summary**: Deploying Transformer-based large language models (LLMs) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value (KV) cache demands. While existing KV cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token pruning. Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2508.00370v1),  [pdf](http://arxiv.org/pdf/2508.00370v1)

**Tags**: cs.CL cs.LG 



### SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units   with Precision Recovery
**Authors**: Weicheng Xue, Baisong Xu, Kai Yang, Yongxiang Liu, Dengdeng Fan, Pengxiang Xu, Yonghong Tian

**Updated**: 2025-08-01T03:43:24Z

**Summary**: Low-precision matrix engines, such as FP16 cube, offer high throughput but lack support for full-precision computation. In this work, we propose SGEMM-cube, a high-performance algorithm for emulating FP32 general matrix-matrix multiplication (GEMM) using only FP16 computation units on a representative AI accelerator. The method decomposes each FP32 operand into two FP16 values and compensates for numerical errors through a tunable scaling strategy. A detailed analysis of numerical errors, including underflow conditions and precision loss, guides the selection of scaling parameters to preserve up to 22 bits of mantissa accuracy. We further investigate the effect of computation order on accuracy and demonstrate that a term-wise accumulation scheme improves numerical stability over conventional FP32 GEMM in low-exponent regimes. Finally, a cache-aware blocking strategy and double-buffered pipeline are introduced to overlap memory transfers with computation, enabling SGEMM-cube to achieve up to 77\% of the theoretical FP32-equivalent peak performance on Ascend 910A NPU lacking native FP32 support. Extensive numerical experiments confirm that our method not only recovers the accuracy of native FP32 GEMM but also exhibits superior numerical stability under certain conditions, due to its structured and error-aware computation order.

**Link**: [arxiv](http://arxiv.org/abs/2507.23387v2),  [pdf](http://arxiv.org/pdf/2507.23387v2)

**Tags**: cs.DC 



### Next Tokens Denoising for Speech Synthesis
**Authors**: Yanqing Liu, Ruiqing Xue, Chong Zhang, Yufei Liu, Gang Wang, Bohan Li, Yao Qian, Lei He, Shujie Liu, Sheng Zhao

**Updated**: 2025-08-01T03:37:42Z

**Summary**: While diffusion and autoregressive (AR) models have significantly advanced generative modeling, they each present distinct limitations. AR models, which rely on causal attention, cannot exploit future context and suffer from slow generation speeds. Conversely, diffusion models struggle with key-value (KV) caching. To overcome these challenges, we introduce Dragon-FM, a novel text-to-speech (TTS) design that unifies AR and flow-matching. This model processes 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens per second. This design enables AR modeling across chunks, ensuring global coherence, while parallel flow-matching within chunks facilitates fast iterative denoising. Thus, the model leverages KV-cache across chunks and utilizes bidirectional context within each chunk. Furthermore, it bridges continuous and discrete feature modeling, demonstrating that continuous AR flow-matching can predict discrete tokens with finite scalar quantizers. This efficient codec and fast chunk-autoregressive architecture also make the model highly effective for generating long-form content, such as podcasts. Experiments on podcast datasets demonstrate its capability to efficiently generate high-quality zero-shot podcasts.

**Link**: [arxiv](http://arxiv.org/abs/2507.22746v2),  [pdf](http://arxiv.org/pdf/2507.22746v2)

**Tags**: cs.SD cs.CL eess.AS 



### OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device   Speculative Decoding
**Authors**: Ramchalam Kinattinkara Ramakrishnan, Zhaocong Yuan, Shaojie Zhuo, Chen Feng, Yicheng Lin, Chenzheng Su, Xiaopeng Zhang

**Updated**: 2025-07-31T21:00:28Z

**Summary**: Speculative decoding generally dictates having a small, efficient draft model that is either pretrained or distilled offline to a particular target model series, for instance, Llama or Qwen models. However, within online deployment settings, there are two major challenges: 1) usage of a target model that is incompatible with the draft model; 2) expectation of latency improvements over usage and time. In this work, we propose OmniDraft, a unified framework that enables a single draft model to operate with any target model and adapt dynamically to user data. We introduce an online n-gram cache with hybrid distillation fine-tuning to address the cross-vocabulary mismatch across draft and target models; and further improve decoding speed by leveraging adaptive drafting techniques. OmniDraft is particularly suitable for on-device LLM applications where model cost, efficiency and user customization are the major points of contention. This further highlights the need to tackle the above challenges and motivates the \textit{``one drafter for all''} paradigm. We showcase the proficiency of the OmniDraft framework by performing online learning on math reasoning, coding and text generation tasks. Notably, OmniDraft enables a single Llama-68M model to pair with various target models including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding; and additionally provides up to 1.5-2x speedup.

**Link**: [arxiv](http://arxiv.org/abs/2507.02659v2),  [pdf](http://arxiv.org/pdf/2507.02659v2)

**Tags**: cs.LG cs.CL 



### SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases
**Authors**: Haoran Zhang, Decheng Zuo, Yu Yan, Zhiyu Liang, Hongzhi Wang

**Updated**: 2025-07-31T16:21:03Z

**Summary**: The co-location of multiple database instances on resource constrained edge nodes creates significant cache contention, where traditional schemes are inefficient and unstable under dynamic workloads. To address this, we present SAM(a Stability-Aware Manager), an autonomic cache manager that establishes decision stability as a first-class design principle. It achieves this through its core control policy, AURA(Autonomic Utility-balancing Resource Allocator), which resolves the classic exploitation-exploration dilemma by synthesizing two orthogonal factors: the H-factor, representing proven historical efficiency (exploitation), and the V-factor, for estimated marginal gain (exploration). Through this practical synthesis and adaptive control, SAM achieves sustained high performance with strategic stability and robustness in volatile conditions.   Extensive experiments against 14 diverse baselines demonstrate SAM's superiority. It achieves top-tier throughput while being uniquely resilient to complex workload shifts and adversarial workloads like cache pollution. Furthermore, its decision latency is highly scalable, remaining nearly constant as the system grows to 120 databases. Crucially, SAM achieves superior decision stability -- maintaining consistent optimization directions despite noise, avoiding performance oscillations while ensuring predictable Quality of Service. These results prove that a principled, stability-aware design is essential for sustained high performance in real-world, large-scale systems.

**Link**: [arxiv](http://arxiv.org/abs/2507.22701v2),  [pdf](http://arxiv.org/pdf/2507.22701v2)

**Tags**: cs.DB H.2.4; H.2.7 



### TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached   Responses
**Authors**: Muhammad Taha Cheema, Abeer Aamir, Khawaja Gul Muhammad, Naveed Anwar Bhatti, Ihsan Ayyub Qazi, Zafar Ayyub Qazi

**Updated**: 2025-07-31T15:50:57Z

**Summary**: Large Language Models (LLMs) process millions of queries daily, making efficient response caching a compelling optimization for reducing cost and latency. However, preserving relevance to user queries using this approach proves difficult due to the personalized nature of chatbot interactions and the limited accuracy of semantic similarity search. To address this, we present TweakLLM, a novel routing architecture that employs a lightweight LLM to dynamically adapt cached responses to incoming prompts. Through comprehensive evaluation, including user studies with side-by-side comparisons, satisfaction voting, as well as multi-agent LLM debates, we demonstrate that TweakLLM maintains response quality comparable to frontier models while significantly improving cache effectiveness. Our results across real-world datasets highlight TweakLLM as a scalable, resource-efficient caching solution for high-volume LLM deployments without compromising user experience.

**Link**: [arxiv](http://arxiv.org/abs/2507.23674v1),  [pdf](http://arxiv.org/pdf/2507.23674v1)

**Tags**: cs.LG cs.CL 



### MemShare: Memory Efficient Inference for Large Reasoning Models through   KV Cache Reuse
**Authors**: Kaiwen Chen, Xin Tan, Minchen Yu, Hong Xu

**Updated**: 2025-07-31T07:53:53Z

**Summary**: Large Reasoning Models (LRMs) have achieved significant advances in mathematical reasoning and formal logic tasks. However, their tendency to generate lengthy chain-of-thought sequences leads to substantial memory overhead during inference. We observe that LRMs frequently produce highly similar intermediate reasoning steps, which correspond to similar KV cache states across layers. Motivated by this observation, we propose MemShare, a novel KV cache management approach that effectively reduces memory overhead. MemShare employs a collaborative filtering algorithm to efficiently identify reusable KV cache blocks and enables zero copy cache reuse to significantly reduce memory overhead, improve throughput while maintaining accuracy. Experimental results demonstrate that MemShare delivers up to 84.79\% improvement in throughput while maintaining better accuracy compared to existing KV cache management methods.

**Link**: [arxiv](http://arxiv.org/abs/2507.21433v2),  [pdf](http://arxiv.org/pdf/2507.21433v2)

**Tags**: cs.LG cs.AI 



### LiteGS: A High-performance Framework to Train 3DGS in Subminutes via   System and Algorithm Codesign
**Authors**: Kaimin Liao, Hua Wang, Zhi Chen, Luchao Wang, Yaohua Tang

**Updated**: 2025-07-31T07:35:04Z

**Summary**: 3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D representation. However, it still suffers from high training cost. This paper introduces LiteGS, a high performance framework that systematically optimizes the 3DGS training pipeline from multiple aspects. At the low-level computation layer, we design a ``warp-based raster'' associated with two hardware-aware optimizations to significantly reduce gradient reduction overhead. At the mid-level data management layer, we introduce dynamic spatial sorting based on Morton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and improve data locality, therefore reducing cache misses. At the top-level algorithm layer, we establish a new robust densification criterion based on the variance of the opacity gradient, paired with a more stable opacity control mechanism, to achieve more precise parameter growth. Experimental results demonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x with comparable or superior quality and surpasses the current SOTA in lightweight models by up to 1.4x speedup. For high-quality reconstruction tasks, LiteGS sets a new accuracy record and decreases the training time by an order of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2503.01199v2),  [pdf](http://arxiv.org/pdf/2503.01199v2)

**Tags**: cs.CV 



### SequenceLayers: Sequence Processing and Streaming Neural Networks Made   Easy
**Authors**: RJ Skerry-Ryan, Julian Salazar, Soroosh Mariooryad, David Kao, Daisy Stanton, Eric Battenberg, Matt Shannon, Ron J. Weiss, Robin Scheibler, Jonas Rothfuss, Tom Bagby

**Updated**: 2025-07-31T07:10:39Z

**Summary**: We introduce a neural network layer API and library for sequence modeling, designed for easy creation of sequence models that can be executed both layer-by-layer (e.g., teacher-forced training) and step-by-step (e.g., autoregressive sampling). To achieve this, layers define an explicit representation of their state over time (e.g., a Transformer KV cache, a convolution buffer, an RNN hidden state), and a step method that evolves that state, tested to give identical results to a stateless layer-wise invocation. This and other aspects of the SequenceLayers contract enables complex models to be immediately streamable, mitigates a wide range of common bugs arising in both streaming and parallel sequence processing, and can be implemented in any deep learning library. A composable and declarative API, along with a comprehensive suite of layers and combinators, streamlines the construction of production-scale models from simple streamable components while preserving strong correctness guarantees. Our current implementations of SequenceLayers (JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.

**Link**: [arxiv](http://arxiv.org/abs/2507.23292v1),  [pdf](http://arxiv.org/pdf/2507.23292v1)

**Tags**: cs.LG cs.CL cs.PL cs.SE eess.AS 



### Scaling RL to Long Videos
**Authors**: Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han

**Updated**: 2025-07-30T16:55:33Z

**Summary**: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).

**Link**: [arxiv](http://arxiv.org/abs/2507.07966v3),  [pdf](http://arxiv.org/pdf/2507.07966v3)

**Tags**: cs.CV cs.AI cs.CL 



### DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic   Space Partitioning with Erasure Code
**Authors**: Shubhradeep Roy, Suvarthi Sarkar, Vivek Verma, Aryabartta Sahu

**Updated**: 2025-07-30T16:04:01Z

**Summary**: Edge Storage Systems have emerged as a critical enabler of low latency data access in modern cloud networks by bringing storage and computation closer to end users. However, the limited storage capacity of edge servers poses significant challenges in handling high volume and latency sensitive data access requests, particularly under dynamic workloads. In this work, we propose a profit driven framework that integrates three key mechanisms which are collaborative caching, erasure coding, and elastic storage partitioning. Unlike traditional replication, erasure coding enables space efficient redundancy, allowing data to be reconstructed from any subset of K out of K plus M coded blocks. We dynamically partition each edge server s storage into private and public regions. The private region is further subdivided among access points based on their incoming request rates, enabling adaptive control over data locality and ownership. We design a data placement and replacement policy that determines how and where to store or evict coded data blocks to maximize data access within deadlines. While the private region serves requests from local APs, the public region handles cooperative storage requests from neighboring servers. Our proposed Dynamic Space Partitioning and Elastic caching strategy is evaluated on both synthetic and real world traces from Netflix and Spotify. Experimental results show that our method improves overall system profitability by approximately 5 to 8% compared to state of the art approaches under varied workload conditions.

**Link**: [arxiv](http://arxiv.org/abs/2507.22801v1),  [pdf](http://arxiv.org/pdf/2507.22801v1)

**Tags**: cs.DC 



### All-gluon amplitudes with off-shell recursion in multiplet bases
**Authors**: Oskar Bolinder, Rikkert Frederix, Malin Sjodahl

**Updated**: 2025-07-30T12:55:55Z

**Summary**: The efficient computation of color-summed QCD amplitudes at high parton multiplicities remains a central challenge for precision collider predictions. Existing approaches using trace, color-flow, or adjoint bases suffer from non-orthogonality, which complicates the color algebra and scales poorly with multiplicity. In this work, we present an off-shell recursive framework for computing all-gluon tree-level amplitudes directly in orthogonal multiplet bases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that builds multiplet-projected off-shell currents from lower-point currents. By optimizing the recursion through partial summation and caching, we find that the computational complexity of calculating $n$-gluon color-summed squared amplitudes scales as $\mathcal{O}(17^n)$. This demonstrates the potential competitiveness of multiplet bases for high-multiplicity processes.

**Link**: [arxiv](http://arxiv.org/abs/2507.22636v1),  [pdf](http://arxiv.org/pdf/2507.22636v1)

**Tags**: hep-ph 



### SmallThinker: A Family of Efficient Large Language Models Natively   Trained for Local Deployment
**Authors**: Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen

**Updated**: 2025-07-30T06:29:40Z

**Summary**: While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.

**Link**: [arxiv](http://arxiv.org/abs/2507.20984v2),  [pdf](http://arxiv.org/pdf/2507.20984v2)

**Tags**: cs.LG cs.AI 



### A Survey on Large Language Model Acceleration based on KV Cache   Management
**Authors**: Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen

**Updated**: 2025-07-30T05:24:46Z

**Summary**: Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.

**Link**: [arxiv](http://arxiv.org/abs/2412.19442v3),  [pdf](http://arxiv.org/pdf/2412.19442v3)

**Tags**: cs.AI cs.DC 



### Bridging Cache-Friendliness and Concurrency: A Locality-Optimized   In-Memory B-Skiplist
**Authors**: Yicong Luo, Senhe Hao, Brian Wheatman, Prashant Pandey, Helen Xu

**Updated**: 2025-07-29T04:21:11Z

**Summary**: Skiplists are widely used for in-memory indexing in many key-value stores, such as RocksDB and LevelDB, due to their ease of implementation and simple concurrency control mechanisms. However, traditional skiplists suffer from poor cache locality, as they store only a single element per node, leaving performance on the table. Minimizing last-level cache misses is key to maximizing in-memory index performance, making high cache locality essential. In this paper, we present a practical concurrent B-skiplist that enhances cache locality and performance while preserving the simplicity of traditional skiplist structures and concurrency control schemes. Our key contributions include a top-down, single-pass insertion algorithm for B-skiplists and a corresponding simple and efficient top-down concurrency control scheme. On 128 threads, the proposed concurrent B-skiplist achieves between 2x-9x higher throughput compared to state-of-the-art concurrent skiplist implementations, including Facebook's concurrent skiplist from Folly and the Java ConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves competitive (0.9x-1.7x) throughput on point workloads compared to state-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a more complete picture of the performance, we also measure the latency of skiplist and tree-based indices and find that the B-skiplist achieves between 3.5x-103x lower 99% latency compared to other concurrent skiplists and between 0.85x-64x lower 99% latency compared to tree-based indices on point workloads with inserts.

**Link**: [arxiv](http://arxiv.org/abs/2507.21492v1),  [pdf](http://arxiv.org/pdf/2507.21492v1)

**Tags**: cs.DC 



### SQuat: Subspace-orthogonal KV Cache Quantization
**Authors**: Hao Wang, Ligong Han, Kai Xu, Akash Srivastava

**Updated**: 2025-07-28T20:44:23Z

**Summary**: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2503.24358v2),  [pdf](http://arxiv.org/pdf/2503.24358v2)

**Tags**: cs.LG cs.AI cs.CL cs.IT math.IT 



### REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource   Constraints
**Authors**: Francesco Corti, Balz Maag, Joachim Schauer, Ulrich Pferschy, Olga Saukh

**Updated**: 2025-07-28T14:11:53Z

**Summary**: Deep learning models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models that are not capable to adapt at runtime. In this work, we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS leverages structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieves computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) taking advantage of data cache by re-arranging the order of operations in REDS computational graph. REDS supports conventional deep networks frequently deployed on the edge and provides computational benefits even for small and simple networks. We evaluate REDS on eight benchmark architectures trained on the Visual Wake Words, Google Speech Commands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four off-the-shelf mobile and embedded hardware platforms. We provide a theoretical result and empirical evidence demonstrating REDS' outstanding performance in terms of submodels' test set accuracy, and demonstrate an adaptation time in response to dynamic resource constraints of under 40$\mu$s, utilizing a fully-connected network on Arduino Nano 33 BLE.

**Link**: [arxiv](http://arxiv.org/abs/2311.13349v3),  [pdf](http://arxiv.org/pdf/2311.13349v3)

**Tags**: cs.LG 



### Quantum Circuit Caches and Compressors for Low Latency, High Throughput   Computing
**Authors**: Ioana Moflic, Alan Robertson, Simon J. Devitt, Alexandru Paler

**Updated**: 2025-07-28T09:59:22Z

**Summary**: Utility-scale quantum programs contain operations on the order of $>10^{15}$ which must be prepared and piped from a classical co-processor to the control unit of the quantum device. The latency of this process significantly increases with the size of the program: existing high-level classical representations of quantum programs are typically memory intensive and do not na\"ively efficiently scale to the degree required to execute utility-scale programs in real-time. To combat this limitation, we propose the utilization of high-level quantum circuit caches and compressors. The first save on the time associated with repetitive tasks and sub-circuits, and the latter are useful for representing the programs/circuits in memory-efficient formats. We present numerical evidence that caches and compressors can offer five orders of magnitude lower latencies during the automatic transpilation of extremely large quantum circuits.

**Link**: [arxiv](http://arxiv.org/abs/2507.20677v1),  [pdf](http://arxiv.org/pdf/2507.20677v1)

**Tags**: quant-ph 



### Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache   Compression
**Authors**: Te Zhang, Yuheng Li, Junxiang Wang, Lujun Li

**Updated**: 2025-07-28T08:27:40Z

**Summary**: Large multimodal models (LMMs) have advanced significantly by integrating visual encoders with extensive language models, enabling robust reasoning capabilities. However, compressing LMMs for deployment on edge devices remains a critical challenge. In this work, we propose an adaptive search algorithm that optimizes sparsity and KV cache compression to enhance LMM efficiency. Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts pruning ratios and KV cache quantization bandwidth across different LMM layers, using model performance as the optimization objective. This approach uniquely combines pruning with key-value cache quantization and incorporates a fast pruning technique that eliminates the need for additional fine-tuning or weight adjustments, achieving efficient compression without compromising accuracy. Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and 13B, demonstrate our method superiority over state-of-the-art techniques such as SparseGPT and Wanda across various compression levels. Notably, our framework automatic allocation of KV cache compression resources sets a new standard in LMM optimization, delivering memory efficiency without sacrificing much performance.

**Link**: [arxiv](http://arxiv.org/abs/2507.20613v1),  [pdf](http://arxiv.org/pdf/2507.20613v1)

**Tags**: cs.AI cs.LG 



### Learning-Augmented Online Caching: New Upper Bounds
**Authors**: Daniel Skachkov, Denis Ponomaryov, Yuri Dorn, Alexander Demin

**Updated**: 2025-07-28T04:25:58Z

**Summary**: We address the problem of learning-augmented online caching in the scenario when each request is accompanied by a prediction of the next occurrence of the requested page. We improve currently known bounds on the competitive ratio of the BlindOracle algorithm, which evicts a page predicted to be requested last. We also prove a lower bound on the competitive ratio of any randomized algorithm and show that a combination of the BlindOracle with the Marker algorithm achieves a competitive ratio that is optimal up to some constant.

**Link**: [arxiv](http://arxiv.org/abs/2410.01760v2),  [pdf](http://arxiv.org/pdf/2410.01760v2)

**Tags**: cs.DB 



### GATE: Geometry-Aware Trained Encoding
**Authors**: Jakub Bokšanský, Daniel Meister, Carsten Benthin

**Updated**: 2025-07-27T09:58:25Z

**Summary**: The encoding of input parameters is one of the fundamental building blocks of neural network algorithms. Its goal is to map the input data to a higher-dimensional space, typically supported by trained feature vectors. The mapping is crucial for the efficiency and approximation quality of neural networks. We propose a novel geometry-aware encoding called GATE that stores feature vectors on the surface of triangular meshes. Our encoding is suitable for neural rendering-related algorithms, for example, neural radiance caching. It also avoids limitations of previous hash-based encoding schemes, such as hash collisions, selection of resolution versus scene size, and divergent memory access. Our approach decouples feature vector density from geometry density using mesh colors, while allowing for finer control over neural network training and adaptive level-of-detail.

**Link**: [arxiv](http://arxiv.org/abs/2506.08161v2),  [pdf](http://arxiv.org/pdf/2506.08161v2)

**Tags**: cs.GR 



### High-Performance Parallel Optimization of the Fish School Behaviour on   the Setonix Platform Using OpenMP
**Authors**: Haitian Wang, Long Qin

**Updated**: 2025-07-27T08:25:08Z

**Summary**: This paper presents an in-depth investigation into the high-performance parallel optimization of the Fish School Behaviour (FSB) algorithm on the Setonix supercomputing platform using the OpenMP framework. Given the increasing demand for enhanced computational capabilities for complex, large-scale calculations across diverse domains, there's an imperative need for optimized parallel algorithms and computing structures. The FSB algorithm, inspired by nature's social behavior patterns, provides an ideal platform for parallelization due to its iterative and computationally intensive nature. This study leverages the capabilities of the Setonix platform and the OpenMP framework to analyze various aspects of multi-threading, such as thread counts, scheduling strategies, and OpenMP constructs, aiming to discern patterns and strategies that can elevate program performance. Experiments were designed to rigorously test different configurations, and our results not only offer insights for parallel optimization of FSB on Setonix but also provide valuable references for other parallel computational research using OpenMP. Looking forward, other factors, such as cache behavior and thread scheduling strategies at micro and macro levels, hold potential for further exploration and optimization.

**Link**: [arxiv](http://arxiv.org/abs/2507.20173v1),  [pdf](http://arxiv.org/pdf/2507.20173v1)

**Tags**: cs.DC cs.AI 



### Accelerating Containerized Service Delivery at the Network Edge
**Authors**: Yinuo Deng, Hailiang Zhao, Dongjing Wang, Peng Chen, Wenzhuo Qian, Jianwei Yin, Schahram Dustdar, Shuiguang Deng

**Updated**: 2025-07-27T03:45:07Z

**Summary**: Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions using a sliding window mechanism. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both physical edge devices and Docker-based emulations. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\times$, 1.79$\times$, and 1.28$\times$ compared to the Baseline, Dragonfly, and Kraken, respectively, while significantly reducing peak cross-network traffic by 90.72\% under congested and varying network conditions.

**Link**: [arxiv](http://arxiv.org/abs/2507.20116v1),  [pdf](http://arxiv.org/pdf/2507.20116v1)

**Tags**: cs.NI cs.DC 



### Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient   Nonlinear MCMC on General Graphs
**Authors**: Jie Hu, Yi-Ting Ma, Do Young Eun

**Updated**: 2025-07-27T00:40:47Z

**Summary**: We propose a history-driven target (HDT) framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution $\boldsymbol{\mu}$. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target $\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs.

**Link**: [arxiv](http://arxiv.org/abs/2505.18300v3),  [pdf](http://arxiv.org/pdf/2505.18300v3)

**Tags**: cs.LG stat.ML 



### FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache   Compression
**Authors**: Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li

**Updated**: 2025-07-26T18:20:25Z

**Summary**: The efficacy of Large Language Models (LLMs) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value (KV) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel, training-free KV cache compression framework that ensures unbiased information retention. FAEDKV operates by transforming the KV cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAEDKV's superiority over existing methods by up to 22\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches.

**Link**: [arxiv](http://arxiv.org/abs/2507.20030v1),  [pdf](http://arxiv.org/pdf/2507.20030v1)

**Tags**: cs.CL 



### Model Reveals What to Cache: Profiling-Based Feature Reuse for Video   Diffusion Models
**Authors**: Xuran Ma, Yexin Liu, Yaofu Liu, Xianfeng Wu, Mingzhe Zheng, Zihao Wang, Ser-Nam Lim, Harry Yang

**Updated**: 2025-07-26T15:25:22Z

**Summary**: Recent advances in diffusion models have demonstrated remarkable capabilities in video generation. However, the computational intensity remains a significant challenge for practical applications. While feature caching has been proposed to reduce the computational burden of diffusion models, existing methods typically overlook the heterogeneous significance of individual blocks, resulting in suboptimal reuse and degraded output quality. To this end, we address this gap by introducing ProfilingDiT, a novel adaptive caching strategy that explicitly disentangles foreground and background-focused blocks. Through a systematic analysis of attention distributions in diffusion models, we reveal a key observation: 1) Most layers exhibit a consistent preference for either foreground or background regions. 2) Predicted noise shows low inter-step similarity initially, which stabilizes as denoising progresses. This finding inspires us to formulate a selective caching strategy that preserves full computation for dynamic foreground elements while efficiently caching static background features. Our approach substantially reduces computational overhead while preserving visual fidelity. Extensive experiments demonstrate that our framework achieves significant acceleration (e.g., 2.01 times speedup for Wan2.1) while maintaining visual fidelity across comprehensive quality metrics, establishing a viable method for efficient video generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.03140v2),  [pdf](http://arxiv.org/pdf/2504.03140v2)

**Tags**: cs.CV 



### NestQuant: Nested Lattice Quantization for Matrix Products and LLMs
**Authors**: Semyon Savkin, Eitan Porat, Or Ordentlich, Yury Polyanskiy

**Updated**: 2025-07-26T15:13:56Z

**Summary**: Post-training quantization (PTQ) has emerged as a critical technique for efficient deployment of large language models (LLMs). This work proposes NestQuant, a novel PTQ scheme for weights and activations that is based on self-similar nested lattices. Recent works have mathematically shown such quantizers to be information-theoretically optimal for low-precision matrix multiplication. We implement a practical low-complexity version of NestQuant based on Gosset lattice, making it a drop-in quantizer for any matrix multiplication step (e.g., in self-attention, MLP etc). For example, NestQuant quantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving perplexity of 6.6 on wikitext2. This represents more than 55% reduction in perplexity gap with respect to unquantized model (perplexity of 6.14) compared to state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot (8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation benchmarks confirm uniform superiority of NestQuant.

**Link**: [arxiv](http://arxiv.org/abs/2502.09720v3),  [pdf](http://arxiv.org/pdf/2502.09720v3)

**Tags**: cs.LG cs.AI cs.IT math.IT 



### Align Attention Heads Before Merging Them: An Effective Way for   Converting MHA to GQA
**Authors**: Qingyun Jin, Xiaohui Song, Feng Zhou, Zengchang Qin

**Updated**: 2025-07-26T13:33:06Z

**Summary**: Large language models (LLMs) have demonstrated exceptional performance across diverse natural language processing tasks. However, as the model size and the input sequence's length increase, the linearly increasing key-value (KV) cache significantly degrades inference throughput. Therefore, grouped-query attention (GQA), as an alternative to multi-head attention (MHA), has been widely introduced into LLMs. In this work, we propose a cost-effective method for converting MHA into GQA with any compression ratio of KV heads. The key point of our method lies in the application of Procrustes analysis to the attention heads, which enhances the similarity among attention heads while preserving computational invariance, thereby improving the model's post-training performance. Subsequently, we employ $\mathit{L_0}$ regularization to prune redundant parameters. The model after pruning can be adapted to the standard GQA framework. Experimental results show that our strategy can compress up to 87.5\% KV heads of LLaMA2-7B model and 75\% KV heads of Sheared-LLaMA-1.3B with acceptable performance degradation. Our code is released at https://github.com/fpcsong/mha2gqa.

**Link**: [arxiv](http://arxiv.org/abs/2412.20677v2),  [pdf](http://arxiv.org/pdf/2412.20677v2)

**Tags**: cs.CL 



### CaliDrop: KV Cache Compression with Calibration
**Authors**: Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang

**Updated**: 2025-07-26T10:34:53Z

**Summary**: Large Language Models (LLMs) require substantial computational resources during generation. While the Key-Value (KV) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various KV cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often sparse, allowing for the removal of less critical KV entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.

**Link**: [arxiv](http://arxiv.org/abs/2507.19906v1),  [pdf](http://arxiv.org/pdf/2507.19906v1)

**Tags**: cs.CL 



### HCAttention: Extreme KV Cache Compression via Heterogeneous Attention   Computing for LLMs
**Authors**: Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao

**Updated**: 2025-07-26T06:43:14Z

**Summary**: Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.

**Link**: [arxiv](http://arxiv.org/abs/2507.19823v1),  [pdf](http://arxiv.org/pdf/2507.19823v1)

**Tags**: cs.CL cs.AI cs.LG 



### GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D   Gaussian Splatting
**Authors**: David Bauer, Qi Wu, Hamid Gadirov, Kwan-Liu Ma

**Updated**: 2025-08-02T23:59:11Z

**Summary**: Real-time path tracing is rapidly becoming the standard for rendering in entertainment and professional applications. In scientific visualization, volume rendering plays a crucial role in helping researchers analyze and interpret complex 3D data. Recently, photorealistic rendering techniques have gained popularity in scientific visualization, yet they face significant challenges. One of the most prominent issues is slow rendering performance and high pixel variance caused by Monte Carlo integration. In this work, we introduce a novel radiance caching approach for path-traced volume rendering. Our method leverages advances in volumetric scene representation and adapts 3D Gaussian splatting to function as a multi-level, path-space radiance cache. This cache is designed to be trainable on the fly, dynamically adapting to changes in scene parameters such as lighting configurations and transfer functions. By incorporating our cache, we achieve less noisy, higher-quality images without increasing rendering costs. To evaluate our approach, we compare it against a baseline path tracer that supports uniform sampling and next-event estimation and the state-of-the-art for neural radiance caching. Through both quantitative and qualitative analyses, we demonstrate that our path-space radiance cache is a robust solution that is easy to integrate and significantly enhances the rendering quality of volumetric visualization applications while maintaining comparable computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2507.19718v2),  [pdf](http://arxiv.org/pdf/2507.19718v2)

**Tags**: cs.GR cs.LG 



### Step-3 is Large yet Affordable: Model-system Co-design for   Cost-effective Decoding
**Authors**: StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang

**Updated**: 2025-07-25T16:53:13Z

**Summary**: Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.

**Link**: [arxiv](http://arxiv.org/abs/2507.19427v1),  [pdf](http://arxiv.org/pdf/2507.19427v1)

**Tags**: cs.LG cs.AI 



### Empowering IoT Firmware Secure Update with Customization Rights
**Authors**: Weihao Chen, Yansong Gao, Boyu Kuang, Jin B. Hong, Yuqing Zhang, Anmin Fu

**Updated**: 2025-07-25T15:17:29Z

**Summary**: Firmware updates remain the primary line of defense for IoT devices; however, the update channel itself has become a well-established attack vector. Existing defenses mainly focus on securing monolithic firmware images, leaving module-level customization -a growing user demand-largely unprotected and insufficiently explored. To address this gap, we conduct a pilot study on the update workflows of 200 Linux-based IoT devices across 23 vendors, uncovering five previously undocumented vulnerabilities caused by customization practices. A broader analysis of update-related CVEs from 2020 to 2024 reveals that over half originate from customization-induced issues. These findings highlight a critical yet underexamined reality: as customization increases, so does the attack surface, while current defenses fail to keep pace. We propose IMUP (Integrity-Centric Modular Update Platform), the first framework to address two key challenges: constructing a trustworthy cross-module integrity chain and scaling update performance under mass customization. IMUP combines three techniques: per-module chameleon hashing for integrity, server-side proof-of-work offloading to reduce device overhead, and server-side caching to reuse module combinations, minimizing rebuild costs. Security analysis shows that even when 95 percent of secret keys are exposed, forging a valid image incurs over 300 times the cost of the legitimate server. Experiments on heterogeneous IoT devices demonstrate that IMUP reduces server-side generation time by 2.9 times and device downtime by 5.9 times compared to a package-manager baseline.

**Link**: [arxiv](http://arxiv.org/abs/2507.19367v1),  [pdf](http://arxiv.org/pdf/2507.19367v1)

**Tags**: cs.CR 



### General kinetic ion induced electron emission model for metallic walls   applied to biased Z-pinch electrodes
**Authors**: Chirag R. Skolar, Kolter Bradshaw, Manaure Francisquez, Lucio Murillo, Vignesh Krishna Kumar, Bhuvana Srinivasan

**Updated**: 2025-07-24T19:44:36Z

**Summary**: A kinetic ion induced electron emission (IIEE) model for general applications is developed to obtain the emitted electron energy spectrum for a distribution of ion impacts on a metallic surface. We assume an ionization cascade mechanism and use empirical models for the ion and electron stopping powers. The emission spectrum and the secondary electron yield (SEY) are validated for a variety of materials. The IIEE model is used to study the effect of IIEE on the plasma-material interactions of Z-pinch electrodes. Un-magnetized Boltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded by two biased copper electrodes with and without IIEE at bias potentials from 0 to 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at higher bias potentials. At the cathode, the SEY is much larger due to higher energy ion bombardment and grows with bias potential. As the bias potential increases, the emitted cathode electrons are accelerated to higher energies into the domain collisionally heating the plasma. Above 1 kV, the heating is strong enough to increase the plasma potential. Despite SEY greater than 1, only a classical sheath forms as opposed to a space-charge limited or inverse sheath due to the emitted electron flux not reaching the space charge current saturation limits. Furthermore, the current in the emissionless cases saturates to a value lower than experiment. With IIEE, the current does not saturate and continues to increase with the 4 kV case matching most closely with experiment.

**Link**: [arxiv](http://arxiv.org/abs/2502.01802v2),  [pdf](http://arxiv.org/pdf/2502.01802v2)

**Tags**: physics.plasm-ph 



### Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs
**Authors**: Anshumann, Mohd Abbas Zaidi, Akhil Kedia, Jinwoo Ahn, Taehwak Kwon, Kangwook Lee, Haejun Lee, Joohyung Lee

**Updated**: 2025-07-24T17:30:12Z

**Summary**: Knowledge distillation can be a cost-effective technique to distill knowledge in Large Language Models, if the teacher output logits can be pre-computed and cached. However, successfully applying this to pre-training remains largely unexplored. In this work, we prove that naive approaches for sparse knowledge distillation such as caching Top-K probabilities, while intuitive, provide biased estimates of teacher probability distribution to the student, resulting in suboptimal performance and calibration. We propose an importance-sampling-based method `Random Sampling Knowledge Distillation', which provides unbiased estimates, preserves the gradient in expectation, and requires storing significantly sparser logits. Our method enables faster training of student models with marginal overhead (<10%) compared to cross-entropy based training, while maintaining competitive performance compared to full distillation, across a range of model sizes from 300M to 3B.

**Link**: [arxiv](http://arxiv.org/abs/2503.16870v2),  [pdf](http://arxiv.org/pdf/2503.16870v2)

**Tags**: cs.LG cs.AI cs.CL 68T50 I.2.7 



### LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are   Important
**Authors**: Manlai Liang, JiaMing Zhang, Xiong Li, Jinlong Li

**Updated**: 2025-07-24T16:25:51Z

**Summary**: The increasing size of the Key-Value (KV) cache during the Large Language Models long-context inference is the main obstacle for its balance between the deployment cost and task accuracy. To reduce the KV cache size in such scenarios, most previous efforts leveraged on the attention weight to evict non-critical cache tokens. But there is a trade-off in those methods, they usually require major modification of the inference infrastructure and significant computation overhead. Based on the fact that the Large Language models are autoregressive models, we propose LagKV, a KV compression strategy only relying on straight forward comparison among KV themselves. It is a totally attention free method which offers easy integration to the main stream inference platform and comparable performance comparing to other complicated KV compression methods. Results on RULER benchmark show that, our approach outperforms SnapKV and StreamingLLM in different compression ratios. Especially in the 64-digit passkey retrieval task, our method outperforms the attention weight based method $H_2O$ over $50\%$ with same compression ratios. Our code is available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.

**Link**: [arxiv](http://arxiv.org/abs/2504.04704v2),  [pdf](http://arxiv.org/pdf/2504.04704v2)

**Tags**: cs.LG cs.AI cs.CL cs.CV 



### Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization   with Arrival-Time Ordering
**Authors**: Ivan Medennikov, Taejin Park, Weiqing Wang, He Huang, Kunal Dhawan, Jinhan Wang, Jagadeesh Balam, Boris Ginsburg

**Updated**: 2025-07-24T14:30:48Z

**Summary**: This paper presents a streaming extension for the Sortformer speaker diarization framework, whose key property is the arrival-time ordering of output speakers. The proposed approach employs an Arrival-Order Speaker Cache (AOSC) to store frame-level acoustic embeddings of previously observed speakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings by speaker index corresponding to their arrival time order, and is dynamically updated by selecting frames with the highest scores based on the model's past predictions. Notably, the number of stored embeddings per speaker is determined dynamically by the update mechanism, ensuring efficient cache utilization and precise speaker tracking. Experiments on benchmark datasets confirm the effectiveness and flexibility of our approach, even in low-latency setups. These results establish Streaming Sortformer as a robust solution for real-time multi-speaker tracking and a foundation for streaming multi-talker speech processing.

**Link**: [arxiv](http://arxiv.org/abs/2507.18446v1),  [pdf](http://arxiv.org/pdf/2507.18446v1)

**Tags**: eess.AS cs.SD 



### NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural   KV Database
**Authors**: Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu

**Updated**: 2025-07-24T02:00:09Z

**Summary**: Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\&E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\textbf{50x} more than in prior work).

**Link**: [arxiv](http://arxiv.org/abs/2507.18028v1),  [pdf](http://arxiv.org/pdf/2507.18028v1)

**Tags**: cs.CL cs.AI 



### Yume: An Interactive World Generation Model
**Authors**: Xiaofeng Mao, Shaoheng Lin, Zhen Li, Chuanhao Li, Wenshuo Peng, Tong He, Jiangmiao Pang, Mingmin Chi, Yu Qiao, Kaipeng Zhang

**Updated**: 2025-07-23T17:57:09Z

**Summary**: Yume aims to use images, text, or videos to create an interactive, realistic, and dynamic world, which allows exploration and control using peripheral devices or neural signals. In this report, we present a preview version of \method, which creates a dynamic world from an input image and allows exploration of the world using keyboard actions. To achieve this high-fidelity and interactive video world generation, we introduce a well-designed framework, which consists of four main components, including camera motion quantization, video generation architecture, advanced sampler, and model acceleration. First, we quantize camera motions for stable training and user-friendly interaction using keyboard inputs. Then, we introduce the Masked Video Diffusion Transformer~(MVDT) with a memory module for infinite video generation in an autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM) and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE) are introduced to the sampler for better visual quality and more precise control. Moreover, we investigate model acceleration by synergistic optimization of adversarial distillation and caching mechanisms. We use the high-quality world exploration dataset \sekai to train \method, and it achieves remarkable results in diverse scenes and applications. All data, codebase, and model weights are available on https://github.com/stdstu12/YUME. Yume will update monthly to achieve its original goal. Project page: https://stdstu12.github.io/YUME-Project/.

**Link**: [arxiv](http://arxiv.org/abs/2507.17744v1),  [pdf](http://arxiv.org/pdf/2507.17744v1)

**Tags**: cs.CV cs.AI cs.HC 



### SHINE: A Scalable HNSW Index in Disaggregated Memory
**Authors**: Manuel Widmoser, Daniel Kocher, Nikolaus Augsten

**Updated**: 2025-07-23T16:09:10Z

**Summary**: Approximate nearest neighbor (ANN) search is a fundamental problem in computer science for which in-memory graph-based methods, such as Hierarchical Navigable Small World (HNSW), perform exceptionally well. To scale beyond billions of high-dimensional vectors, the index must be distributed. The disaggregated memory architecture physically separates compute and memory into two distinct hardware units and has become popular in modern data centers. Both units are connected via RDMA networks that allow compute nodes to directly access remote memory and perform all the computations, posing unique challenges for disaggregated indexes.   In this work, we propose a scalable HNSW index for ANN search in disaggregated memory. In contrast to existing distributed approaches, which partition the graph at the cost of accuracy, our method builds a graph-preserving index that reaches the same accuracy as a single-machine HNSW. Continuously fetching high-dimensional vector data from remote memory leads to severe network bandwidth limitations, which we overcome by employing an efficient caching mechanism. Since answering a single query involves processing numerous unique graph nodes, caching alone is not sufficient to achieve high scalability. We logically combine the caches of the compute nodes to increase the overall cache effectiveness and confirm the efficiency and scalability of our method in our evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2507.17647v1),  [pdf](http://arxiv.org/pdf/2507.17647v1)

**Tags**: cs.DB 



### Toward a Lightweight and Robust Design for Caching
**Authors**: Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng

**Updated**: 2025-07-23T15:59:38Z

**Summary**: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce significant computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_k + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only $O(1)$ additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.16242v2),  [pdf](http://arxiv.org/pdf/2507.16242v2)

**Tags**: cs.DS cs.LG 



### An h-space Based Adversarial Attack for Protection Against Few-shot   Personalization
**Authors**: Xide Xu, Sandesh Kamath, Muhammad Atif Butt, Bogdan Raducanu

**Updated**: 2025-07-23T14:43:22Z

**Summary**: The versatility of diffusion models in generating customized images from few samples raises significant privacy concerns, particularly regarding unauthorized modifications of private content. This concerning issue has renewed the efforts in developing protection mechanisms based on adversarial attacks, which generate effective perturbations to poison diffusion models. Our work is motivated by the observation that these models exhibit a high degree of abstraction within their semantic latent space (`h-space'), which encodes critical high-level features for generating coherent and meaningful content. In this paper, we propose a novel anti-customization approach, called HAAD (h-space based Adversarial Attack for Diffusion models), that leverages adversarial attacks to craft perturbations based on the h-space that can efficiently degrade the image generation process. Building upon HAAD, we further introduce a more efficient variant, HAAD-KV, that constructs perturbations solely based on the KV parameters of the h-space. This strategy offers a stronger protection, that is computationally less expensive. Despite their simplicity, our methods outperform state-of-the-art adversarial attacks, highlighting their effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2507.17554v1),  [pdf](http://arxiv.org/pdf/2507.17554v1)

**Tags**: cs.CV 



### AirCache: Activating Inter-modal Relevancy KV Cache Compression for   Efficient Large Vision-Language Model Inference
**Authors**: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

**Updated**: 2025-07-23T11:42:03Z

**Summary**: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.23956v3),  [pdf](http://arxiv.org/pdf/2503.23956v3)

**Tags**: cs.CV cs.AI 



### Multiprocessor Scheduling with Memory Constraints: Fundamental   Properties and Finding Optimal Solutions
**Authors**: Pál András Papp, Toni Böhnlein, A. N. Yzelman

**Updated**: 2025-07-23T11:12:08Z

**Summary**: We study the problem of scheduling a general computational DAG on multiple processors in a 2-level memory hierarchy. This setting is a natural generalization of several prominent models in the literature, and it simultaneously captures workload balancing, communication, and data movement due to cache size limitations. We first analyze the fundamental properties of this problem from a theoretical perspective, such as its computational complexity. We also prove that optimizing parallelization and memory management separately, as done in many applications, can result in a solution that is a linear factor away from the optimum.   On the algorithmic side, we discuss a natural technique to represent and solve the problem as an Integer Linear Program (ILP). We develop a holistic scheduling algorithm based on this approach, and we experimentally study its performance and properties on a small benchmark of computational tasks. Our results confirm that the ILP-based method can indeed find considerably better solutions than a baseline which combines classical scheduling algorithms and memory management policies.

**Link**: [arxiv](http://arxiv.org/abs/2507.17411v1),  [pdf](http://arxiv.org/pdf/2507.17411v1)

**Tags**: cs.DC 90B35, 90C10, 68Q10, 68W10 C.1.4 



### Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2   experiment
**Authors**: Marco Barbisan, Marco Boldrin, Luca Cinnirella, Bruno Laterza, Alberto Maistrello, Lionello Marrelli, Federico Molon, Simone Peruzzo, Cesare Taliercio, Marco Valisa, Enrico Zampiva

**Updated**: 2025-07-23T10:10:53Z

**Summary**: Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge Exchange Recombination Spectroscopy (CHERS) and Motional Stark effect diagnostics (MSE), are a well-known tool to access important information about magnetically confined plasmas, such as radial profiles of ion temperature, ion flow, impurity content and intensity and direction of the magnetic field. For this purpose, a DNBI was installed and operated in the RFX-mod experiment, which was designed to confine plasma mainly through the Reversed Field Pinch configuration. The DNBI, designed and built by the Budker Institute of Nuclear Physics (BINP), was based on a source of positive hydrogen ions, accelerated to 50 keV and for a maximum ion current of 5 A. The beam could be modulated and the maximum overall duration was 50 ms. With the upgrade of RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to solve several power units faults and improve the overall reliability of the system. The 50 kV power supply is being improved, as well as the power supplies in the high voltage deck and its insulation transformer. Magnetic field survival tests were performed on the toroidal-core-based DC-DC converters that should power the electronic boards in a reliable way. The control system, originally based on CAMAC technology, was redesigned to be fully replaced. This contribution reviews the technical criticalities emerged in the DNBI check-up and the new solutions adopted to make the DNBI operative and more reliable.

**Link**: [arxiv](http://arxiv.org/abs/2411.13373v2),  [pdf](http://arxiv.org/pdf/2411.13373v2)

**Tags**: physics.plasm-ph 



### Ironman: Accelerating Oblivious Transfer Extension for   Privacy-Preserving AI with Near-Memory Processing
**Authors**: Chenqi Lin, Kang Yang, Tianshi Xu, Ling Liang, Yufei Wang, Zhaohui Chen, Runsheng Wang, Mingyu Gao, Meng Li

**Updated**: 2025-07-23T09:31:01Z

**Summary**: With the wide application of machine learning (ML), privacy concerns arise with user data as they may contain sensitive information. Privacy-preserving ML (PPML) based on cryptographic primitives has emerged as a promising solution in which an ML model is directly computed on the encrypted data to provide a formal privacy guarantee. However, PPML frameworks heavily rely on the oblivious transfer (OT) primitive to compute nonlinear functions. OT mainly involves the computation of single-point correlated OT (SPCOT) and learning parity with noise (LPN) operations. As OT is still computed extensively on general-purpose CPUs, it becomes the latency bottleneck of modern PPML frameworks.   In this paper, we propose a novel OT accelerator, dubbed Ironman, to significantly increase the efficiency of OT and the overall PPML framework. We observe that SPCOT is computation-bounded, and thus propose a hardware-friendly SPCOT algorithm with a customized accelerator to improve SPCOT computation throughput. In contrast, LPN is memory-bandwidth-bounded due to irregular memory access patterns. Hence, we further leverage the near-memory processing (NMP) architecture equipped with memory-side cache and index sorting to improve effective memory bandwidth. With extensive experiments, we demonstrate Ironman achieves a 39.2-237.4 times improvement in OT throughput across different NMP configurations compared to the full-thread CPU implementation. For different PPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end latency for both CNN and Transformer models.

**Link**: [arxiv](http://arxiv.org/abs/2507.16391v2),  [pdf](http://arxiv.org/pdf/2507.16391v2)

**Tags**: cs.AR 



### KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache   at a Large Cloud Provider
**Authors**: Jiahao Wang, Jinbo Han, Xingda Wei, Sijie Shen, Dingyan Zhang, Chenguang Fang, Rong Chen, Wenyuan Yu, Haibo Chen

**Updated**: 2025-07-23T08:07:19Z

**Summary**: Serving large language models (LLMs) is important for cloud providers, and caching intermediate results (KV\$) after processing each request substantially improves serving throughput and latency. However, there is limited understanding of how LLM serving benefits from KV\$ caching, where system design decisions like cache eviction policies are highly workload-dependent. In this paper, we present the first systematic characterization of the KV\$ workload patterns from one of the leading LLM service providers. We draw observations that were not covered by previous studies focusing on synthetic workloads, including: KV\$ reuses are skewed across requests, where reuses between single-turn requests are equally important as multi-turn requests; the reuse time and probability are diverse considering all requests, but for a specific request category, the pattern tends to be predictable; and the overall cache size required for an ideal cache hit ratio is moderate. Based on the characterization, we further propose a workload-aware cache eviction policy that improves the serving performance under real-world traces, especially with limited cache capacity.

**Link**: [arxiv](http://arxiv.org/abs/2506.02634v4),  [pdf](http://arxiv.org/pdf/2506.02634v4)

**Tags**: cs.DC cs.AI 



### GTA: Grouped-head latenT Attention
**Authors**: Luoyang Sun, Cheng Deng, Jiwen Jiang, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, Jun Wang

**Updated**: 2025-07-23T05:57:32Z

**Summary**: Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.

**Link**: [arxiv](http://arxiv.org/abs/2506.17286v2),  [pdf](http://arxiv.org/pdf/2506.17286v2)

**Tags**: cs.CL cs.AI 



### Enabling Efficient Transaction Processing on CXL-Based Memory Sharing
**Authors**: Zhao Wang, Yiqi Chen, Cong Li, Dimin Niu, Tianchan Guan, Zhaoyang Du, Xingda Wei, Guangyu Sun

**Updated**: 2025-07-23T01:42:19Z

**Summary**: Transaction processing systems are the crux for modern data-center applications, yet current multi-node systems are slow due to network overheads. This paper advocates for Compute Express Link (CXL) as a network alternative, which enables low-latency and cache-coherent shared memory accesses. However, directly adopting standard CXL primitives leads to performance degradation due to the high cost of maintaining cross-node cache coherence. To address the CXL challenges, this paper introduces CtXnL, a software-hardware co-designed system that implements a novel hybrid coherence primitive tailored to the loosely coherent nature of transactional data. The core innovation of CtXnL is empowering transaction system developers with the ability to selectively achieve data coherence. Our evaluations on OLTP workloads demonstrate that CtXnL enhances performance, outperforming current network-based systems and achieves with up to 2.08x greater throughput than vanilla CXL memory sharing architectures across universal transaction processing policies.

**Link**: [arxiv](http://arxiv.org/abs/2502.11046v2),  [pdf](http://arxiv.org/pdf/2502.11046v2)

**Tags**: cs.AR 



### GATEBLEED: Exploiting On-Core Accelerator Power Gating for High   Performance & Stealthy Attacks on AI
**Authors**: Joshua Kalyanapu, Farshad Dizani, Darsh Asher, Azam Ghanbari, Rosario Cammarota, Aydin Aysu, Samira Mirbagher Ajorpaz

**Updated**: 2025-07-22T21:41:43Z

**Summary**: As power consumption from AI training and inference continues to increase, AI accelerators are being integrated directly into the CPU. Intel's Advanced Matrix Extensions (AMX) is one such example, debuting on the 4th generation Intel Xeon Scalable CPU. We discover a timing side and covert channel, GATEBLEED, caused by the aggressive power gating utilized to keep the CPU within operating limits. We show that the GATEBLEED side channel is a threat to AI privacy as many ML models such as transformers and CNNs make critical computationally-heavy decisions based on private values like confidence thresholds and routing logits. Timing delays from selective powering down of AMX components mean that each matrix multiplication is a potential leakage point when executed on the AMX accelerator. Our research identifies over a dozen potential gadgets across popular ML libraries (HuggingFace, PyTorch, TensorFlow, etc.), revealing that they can leak sensitive and private information. GATEBLEED poses a risk for local and remote timing inference, even under previous protective measures. GATEBLEED can be used as a high performance, stealthy remote covert channel and a generic magnifier for timing transmission channels, capable of bypassing traditional cache defenses to leak arbitrary memory addresses and evading state of the art microarchitectural attack detectors under realistic network conditions and system configurations in which previous attacks fail. We implement an end-to-end microarchitectural inference attack on a transformer model optimized with Intel AMX, achieving a membership inference accuracy of 81% and a precision of 0.89. In a CNN-based or transformer-based mixture-of-experts model optimized with Intel AMX, we leak expert choice with 100% accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.17033v1),  [pdf](http://arxiv.org/pdf/2507.17033v1)

**Tags**: cs.CR 



### StreamME: Simplify 3D Gaussian Avatar within Live Stream
**Authors**: Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu

**Updated**: 2025-07-22T21:33:30Z

**Summary**: We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.

**Link**: [arxiv](http://arxiv.org/abs/2507.17029v1),  [pdf](http://arxiv.org/pdf/2507.17029v1)

**Tags**: cs.GR cs.AI cs.CV 



### SiLQ: Simple Large Language Model Quantization-Aware Training
**Authors**: Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, Dharmendra S. Modha

**Updated**: 2025-07-22T18:17:53Z

**Summary**: Large language models can be quantized to reduce inference time latency, model size, and energy consumption, thereby delivering a better user experience at lower cost. A challenge exists to deliver quantized models with minimal loss of accuracy in reasonable time, and in particular to do so without requiring mechanisms incompatible with specialized inference accelerators. Here, we demonstrate a simple, end-to-end quantization-aware training approach that, with an increase in total model training budget of less than 0.1%, outperforms the leading published quantization methods by large margins on several modern benchmarks, with both base and instruct model variants. The approach easily generalizes across different model architectures, can be applied to activations, cache, and weights, and requires the introduction of no additional operations to the model other than the quantization itself.

**Link**: [arxiv](http://arxiv.org/abs/2507.16933v1),  [pdf](http://arxiv.org/pdf/2507.16933v1)

**Tags**: cs.LG cs.AI cs.CL 



### Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning
**Authors**: Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass

**Updated**: 2025-07-22T17:30:04Z

**Summary**: To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.

**Link**: [arxiv](http://arxiv.org/abs/2507.16784v1),  [pdf](http://arxiv.org/pdf/2507.16784v1)

**Tags**: cs.CL 



### WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding
**Authors**: Ran Wang, Xiaoxuan Liu, Hao Ren, Gang Chen, Fanchao Qi, Maosong Sun

**Updated**: 2025-07-22T17:13:47Z

**Summary**: Structured decoding enables large language models (LLMs) to generate outputs in formats required by downstream systems, such as HTML or JSON. However, existing methods suffer from efficiency bottlenecks due to grammar compilation, state tracking, and mask creation. We observe that many real-world tasks embed strong prior knowledge about output structure. Leveraging this, we propose a decomposition of constraints into static and dynamic components -- precompiling static structures offline and instantiating dynamic arguments at runtime using grammar snippets. Instead of relying on pushdown automata, we employ a compositional set of operators to model regular formats, achieving lower transition latency. We introduce wgrammar, a lightweight decoding engine that integrates domain-aware simplification, constraint decomposition, and mask caching, achieving up to 250x speedup over existing systems. wgrammar's source code is publicly available at https://github.com/wrran/wgrammar.

**Link**: [arxiv](http://arxiv.org/abs/2507.16768v1),  [pdf](http://arxiv.org/pdf/2507.16768v1)

**Tags**: cs.AI 



### Hydra: Virtualized Multi-Language Runtime for High-Density Serverless   Platforms
**Authors**: Serhii Ivanenko, Vasyl Lanko, Rudi Horn, Vojin Jovanovic, Rodrigo Bruno

**Updated**: 2025-07-22T16:49:24Z

**Summary**: Serverless is an attractive computing model that offers seamless scalability and elasticity; it takes the infrastructure management burden away from users and enables a pay-as-you-use billing model. As a result, serverless is becoming increasingly popular to support highly elastic and bursty workloads. However, existing platforms are supported by bloated virtualization stacks, which, combined with bursty and irregular invocations, lead to high memory and latency overheads.   To reduce the virtualization stack bloat, we propose Hydra, a virtualized multi-language runtime and platform capable of hosting multiple sandboxes running concurrently. To fully leverage Hydra's virtualized runtime, we revisit the existing serverless platform design to make it colocation-aware across owners and functions, and to feature a caching layer of pre-allocated Hydra instances that can be used by different functions written in different languages to reduce cold starts. We also propose a snapshotting mechanism to checkpoint and restore individual sandboxes.   By consolidating multiple serverless function invocations through Hydra, we improve the overall function density (ops/GB-sec) by 2.41x on average compared to OpenWhisk runtimes, the state-of-the-art single-language runtimes used in most serverless platforms, and by 1.43x on average compared to Knative runtimes supporting invocation colocation within the same function. When reproducing the Azure Functions trace, our serverless platform operating Hydra instances reduces the overall memory footprint by 21.3-43.9% compared to operating OpenWhisk instances and by 14.5-30% compared to operating Knative instances. Hydra eliminates cold starts thanks to the pool of pre-warmed runtime instances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by 1.9-51.4x compared to Knative.

**Link**: [arxiv](http://arxiv.org/abs/2212.10131v3),  [pdf](http://arxiv.org/pdf/2212.10131v3)

**Tags**: cs.DC cs.PL 



### Genus Zero Kashiwara-Vergne Solutions from Braids
**Authors**: Zsuzsanna Dancso, Iva Halacheva, Guillaume Laplante-Anfossi, Marcy Robertson, Chandan Singh

**Updated**: 2025-07-22T05:34:03Z

**Summary**: Using the language of moperads-monoids in the category of right modules over an operad-we reinterpret the Alekseev-Enriquez-Torossian construction of Kashiwara-Vergne (KV) solutions from associators. We show that any isomorphism between the moperad of parenthesized braids with a frozen strand and the moperad of chord diagrams gives rise to a family of genus zero KV solutions operadically generated by a single classical KV solution. We show that the Grothendieck-Teichm\"uller module groups act on the latter, intertwining the actions of the KV symmetry groups. In the other direction, we show that any symmetric KV solution gives rise to a morphism from the moperad of parenthesized braids with a frozen strand to the moperad of tangential automorphisms of free Lie algebras. This morphism factors through the moperad of chord diagrams if and only if the associated KV associator is a Drinfeld associator.

**Link**: [arxiv](http://arxiv.org/abs/2507.16243v1),  [pdf](http://arxiv.org/pdf/2507.16243v1)

**Tags**: math.AT math.CT math.QA 18M60, 17B, 55 



### Towards Compute-Optimal Many-Shot In-Context Learning
**Authors**: Shahriar Golchin, Yanfei Chen, Rujun Han, Manan Gandhi, Tianli Yu, Swaroop Mishra, Mihai Surdeanu, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister

**Updated**: 2025-07-22T04:21:03Z

**Summary**: Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.

**Link**: [arxiv](http://arxiv.org/abs/2507.16217v1),  [pdf](http://arxiv.org/pdf/2507.16217v1)

**Tags**: cs.CL cs.AI cs.LG 



### Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks
**Authors**: Aaron Jarmusch, Nathan Graddon, Sunita Chandrasekaran

**Updated**: 2025-07-21T19:31:37Z

**Summary**: The rapid development in scientific research provides a need for more compute power, which is partly being solved by GPUs. This paper presents a microarchitectural analysis of the modern NVIDIA Blackwell architecture by studying GPU performance   features with thought through microbenchmarks. We unveil key subsystems, including the memory hierarchy, SM execution   pipelines, and the SM sub-core units, including the 5th generation tensor cores supporting FP4 and FP6 precisions.   To understand the different key features of the NVIDIA GPU, we study latency, throughput, cache behavior, and scheduling   details, revealing subtle tuning metrics in the design of Blackwell. To develop a comprehensive analysis, we compare the   Blackwell architecture with the previous Hopper architecture by using the GeForce RTX 5080 and H100 PCIe, respectively. We   evaluate and compare results, presenting both generational improvements and performance regressions. Additionally, we   investigate the role of power efficiency and energy consumption under varied workloads. Our findings provide actionable insights   for application developers, compiler writers, and performance engineers to optimize workloads on Blackwell-based platforms,   and contribute new data to the growing research on GPU architectures.

**Link**: [arxiv](http://arxiv.org/abs/2507.10789v2),  [pdf](http://arxiv.org/pdf/2507.10789v2)

**Tags**: cs.DC 



### Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time   Systems
**Authors**: Connor Sullivan, Alex Manley, Mohammad Alian, Heechul Yun

**Updated**: 2025-07-21T19:05:01Z

**Summary**: Modern commercial-off-the-shelf (COTS) multicore processors have advanced memory hierarchies that enhance memory-level parallelism (MLP), which is crucial for high performance. To support high MLP, shared last-level caches (LLCs) are divided into multiple banks, allowing parallel access. However, uneven distribution of cache requests from the cores, especially when requests from multiple cores are concentrated on a single bank, can result in significant contention affecting all cores that access the cache. Such cache bank contention can even be maliciously induced -- known as cache bank-aware denial-of-service (DoS) attacks -- in order to jeopardize the system's timing predictability.   In this paper, we propose a per-bank bandwidth regulation approach for multi-banked shared LLC based multicore real-time systems. By regulating bandwidth on a per-bank basis, the approach aims to prevent unnecessary throttling of cache accesses to non-contended banks, thus improving overall performance (throughput) without compromising isolation benefits of throttling. We implement our approach on a RISC-V system-on-chip (SoC) platform using FireSim and evaluate extensively using both synthetic and real-world workloads. Our evaluation results show that the proposed per-bank regulation approach effectively protects real-time tasks from co-running cache bank-aware DoS attacks, and offers up to a 3.66$\times$ performance improvement for the throttled benign best-effort tasks compared to prior bank-oblivious bandwidth throttling approaches.

**Link**: [arxiv](http://arxiv.org/abs/2410.14003v2),  [pdf](http://arxiv.org/pdf/2410.14003v2)

**Tags**: cs.AR 



### An Efficient Frequency-Based Approach for Maximal Square Detection in   Binary Matrices
**Authors**: Swastik Bhandari

**Updated**: 2025-07-21T14:50:41Z

**Summary**: Detecting maximal square submatrices of ones in binary matrices is a fundamental problem with applications in computer vision and pattern recognition. While the standard dynamic programming (DP) solution achieves optimal asymptotic complexity, its practical performance suffers from repeated minimum operations and inefficient memory access patterns that degrade cache utilization. To address these limitations, we introduce a novel frequency-based algorithm that employs a greedy approach to track the columnar continuity of ones through an adaptive frequency array and a dynamic thresholding mechanism. Extensive benchmarking demonstrates that the frequency-based algorithm achieves faster performance than the standard DP in 100% of test cases with an average speedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x across matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's average speedup exceeds 2.5x for all densities and rises to over 3.5x for densities of 0.7 and higher across all matrix sizes. These results demonstrate that the frequency-based approach is a superior alternative to standard DP and opens new possibilities for efficient matrix analysis in performance-critical applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.18974v3),  [pdf](http://arxiv.org/pdf/2503.18974v3)

**Tags**: cs.DS math.OC 



### Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation
**Authors**: Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun

**Updated**: 2025-07-21T07:45:14Z

**Summary**: Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to decrease prefill latency and memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.

**Link**: [arxiv](http://arxiv.org/abs/2507.10524v2),  [pdf](http://arxiv.org/pdf/2507.10524v2)

**Tags**: cs.CL cs.LG 



### Lizard: An Efficient Linearization Framework for Large Language Models
**Authors**: Chien Van Nguyen, Ruiyi Zhang, Hanieh Deilamsalehy, Puneet Mathur, Viet Dac Lai, Haoliang Wang, Jayakumar Subramanian, Ryan A. Rossi, Trung Bui, Nikos Vlassis, Franck Dernoncourt, Thien Huu Nguyen

**Updated**: 2025-07-20T03:49:03Z

**Summary**: We propose Lizard, a linearization framework that transforms pretrained Transformer-based Large Language Models (LLMs) into flexible, subquadratic architectures for infinite-context generation. Transformer-based LLMs face significant memory and computational bottlenecks as context lengths increase, due to the quadratic complexity of softmax attention and the growing key-value (KV) cache. Lizard addresses these limitations by introducing a subquadratic attention mechanism that closely approximates softmax attention while preserving the output quality. Unlike previous linearization methods, which are often limited by fixed model structures and therefore exclude gating mechanisms, Lizard incorporates a gating module inspired by recent state-of-the-art linear models. This enables adaptive memory control, supports constant-memory inference, offers strong length generalization, and allows more flexible model design. Lizard combines gated linear attention for global context compression with sliding window attention enhanced by meta memory, forming a hybrid mechanism that captures both long-range dependencies and fine-grained local interactions. Moreover, we introduce a hardware-aware algorithm that accelerates the training speed of our models. Extensive experiments show that Lizard achieves near-lossless recovery of the teacher model's performance across standard language modeling tasks, while significantly outperforming previous linearization methods. On the 5-shot MMLU benchmark, Lizard improves over prior models by 18 points and shows significant improvements on associative recall tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.09025v2),  [pdf](http://arxiv.org/pdf/2507.09025v2)

**Tags**: cs.CL cs.LG 



### Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing   Multi-Turn Planning and Tool Adaptation
**Authors**: Jubin Abhishek Soni, Amit Anand, Rajesh Kumar Pandey, Aniket Abhishek Soni

**Updated**: 2025-07-19T17:46:19Z

**Summary**: Retrieval-Augmented Generation (RAG) has significantly advanced large language models (LLMs) by grounding their outputs in external tools and knowledge sources. However, existing RAG systems are typically constrained to static, single-turn interactions with fixed toolsets, making them ill-suited for dynamic domains such as healthcare and smart homes, where user intent, available tools, and contextual factors evolve over time. We present Dynamic Context Tuning (DCT), a lightweight framework that extends RAG to support multi-turn dialogue and evolving tool environments without requiring retraining. DCT integrates an attention-based context cache to track relevant past information, LoRA-based retrieval to dynamically select domain-specific tools, and efficient context compression to maintain inputs within LLM context limits. Experiments on both synthetic and real-world benchmarks show that DCT improves plan accuracy by 14% and reduces hallucinations by 37%, while matching GPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to previously unseen tools, enabling scalable and adaptable AI assistants across a wide range of dynamic environments.

**Link**: [arxiv](http://arxiv.org/abs/2506.11092v2),  [pdf](http://arxiv.org/pdf/2506.11092v2)

**Tags**: cs.CL cs.AI cs.HC 



### Caching Techniques for Reducing the Communication Cost of Federated   Learning in IoT Environments
**Authors**: Ahmad Alhonainy, Praveen Rao

**Updated**: 2025-07-19T17:02:15Z

**Summary**: Federated Learning (FL) allows multiple distributed devices to jointly train a shared model without centralizing data, but communication cost remains a major bottleneck, especially in resource-constrained environments. This paper introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce unnecessary model update transmissions. By selectively forwarding significant updates, our approach lowers bandwidth usage while maintaining model accuracy. Experiments on CIFAR-10 and medical datasets show reduced communication with minimal accuracy loss. Results confirm that intelligent caching improves scalability, memory efficiency, and supports reliable FL in edge IoT networks, making it practical for deployment in smart cities, healthcare, and other latency-sensitive applications.

**Link**: [arxiv](http://arxiv.org/abs/2507.17772v1),  [pdf](http://arxiv.org/pdf/2507.17772v1)

**Tags**: cs.DC cs.AI cs.CV cs.LG 



### KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse
**Authors**: Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang

**Updated**: 2025-07-19T07:41:03Z

**Summary**: We describe KVLink, an approach for efficient key-value (KV) cache reuse in large language models (LLMs). In many LLM applications, different inputs can share overlapping context, such as the same retrieved document appearing in multiple queries. However, the LLMs still need to encode the entire context for each query, leading to redundant computation. In this paper, we investigate a new strategy to eliminate such inefficiency, where the KV cache of each document is precomputed independently. During inference, the KV caches of retrieved documents are concatenated, allowing the model to reuse cached representations instead of recomputing them. To mitigate the performance degradation when using KV caches computed independently for each document, KVLink introduces two key techniques: adjusting positional embeddings of the KV cache at inference to match the global position after concatenation, and using trainable special tokens to restore self-attention across independently encoded documents. Experiments across 7 datasets demonstrate that KVLink improves question answering accuracy by an average of 4% over state-of-the-art methods. Furthermore, by leveraging precomputed KV caches, our approach reduces time-to-first-token by up to 96% compared to standard LLM inference, making it a scalable and efficient solution for context reuse. Additionally, KVLink can be combined with KV cache compression to further save cache loading and storage overhead while outperforming the baselines.

**Link**: [arxiv](http://arxiv.org/abs/2502.16002v3),  [pdf](http://arxiv.org/pdf/2502.16002v3)

**Tags**: cs.CL 



### Draft-based Approximate Inference for LLMs
**Authors**: Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee

**Updated**: 2025-07-19T03:40:40Z

**Summary**: Optimizing inference for long-context Large Language Models (LLMs) is increasingly important due to the quadratic compute and linear memory complexity of Transformers. Existing approximation methods, such as key-value (KV) cache dropping, sparse attention, and prompt compression, typically rely on rough predictions of token or KV pair importance. We propose a novel framework for approximate LLM inference that leverages small draft models to more accurately predict the importance of tokens and KV pairs. Specifically, we introduce two instantiations of our proposed framework: (i) SpecKV, the first method that leverages a draft output to accurately assess the importance of each KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses the draft model's attention activations to identify and discard unimportant prompt tokens. We motivate our methods with theoretical and empirical analyses, and show a strong correlation between the attention patterns of draft and target models. Extensive experiments on long-context benchmarks show that our methods consistently achieve higher accuracy than existing baselines, while preserving the same improvements in memory usage, latency, and throughput. Our code is available at https://github.com/furiosa-ai/draft-based-approx-llm.

**Link**: [arxiv](http://arxiv.org/abs/2506.08373v2),  [pdf](http://arxiv.org/pdf/2506.08373v2)

**Tags**: cs.CL cs.AI 



### Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN   Inference Acceleration
**Authors**: Dmitri Lyalikov

**Updated**: 2025-07-19T00:57:54Z

**Summary**: The emergence of heterogeneity and domain-specific architectures targeting deep learning inference show great potential for enabling the deployment of modern CNNs on resource-constrained embedded platforms. A significant development is the diversification of custom hardware solely targeting the most expensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural processing units), among others, can overcome the approaching limits of traditional silicon scaling and provide a solution to the power/performance tradeoff within embedded SoCs. Efficient DSA utilization requires proper system integration and a compilation/execution model for balanced execution in these heterogeneous architectures. There is a critical need for proper system integration and an efficient compilation/execution model for balanced execution in these heterogeneous architectures. This work highlights the hardware integration challenges for efficiently placing these units within the memory hierarchy and correct proximity to other execution blocks. We experimentally verify performance bottlenecks in CNN execution and pre/post-processing at runtime, where previous attention has generally been given to accelerator speedup alone. This work takes advantage of the ratification of the RISC-V Vector 1.0 extension and demonstrates its potential as a flexible target within a well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and CPU fallback processes. Our results show up to a 9x speedup of image pre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU. We demonstrate RVV-1.0 in exposing a flexible programming model that can enable a balanced computation and memory footprint on accelerator-rich embedded SoCs supporting modern deep-learning dataflows while consuming less power than traditional parallel execution platforms.

**Link**: [arxiv](http://arxiv.org/abs/2507.17771v1),  [pdf](http://arxiv.org/pdf/2507.17771v1)

**Tags**: cs.DC eess.IV 



### Secretive Hotplug Coded Caching
**Authors**: Mallikharjuna Chinnapadamala, Charul Rajput, B. Sundar Rajan

**Updated**: 2025-07-18T14:24:29Z

**Summary**: In this work, we consider a coded caching model called \textit{hotplug coded caching}, in which some users are offline during the delivery phase. The concept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching systems has been introduced in the literature, and two classes of HpPDAs are known. In this paper, we consider a secrecy constraint in hotplug coded caching setup, where users should not learn anything about any file from their cache content, and active users should not gain any information about files other than their demanded file from either their cache content or the server transmissions. We propose two secretive schemes for the two classes of HpPDAs and compare them with a baseline scheme, which is a secretive scheme using PDAs for the classical coded caching setup and can be trivially adapted for the hotplug coded caching setup. We numerically show that our schemes outperform the baseline scheme in certain memory regions.

**Link**: [arxiv](http://arxiv.org/abs/2507.13961v1),  [pdf](http://arxiv.org/pdf/2507.13961v1)

**Tags**: cs.IT math.IT 



### LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders
**Authors**: Zheng Chai, Qin Ren, Xijun Xiao, Huizhi Yang, Bo Han, Sijun Zhang, Di Chen, Hui Lu, Wenlin Zhao, Lele Yu, Xionghang Xie, Shiru Ren, Xiang Sun, Yaocheng Tan, Peng Xu, Yuchao Zheng, Di Wu

**Updated**: 2025-07-18T13:29:47Z

**Summary**: Modeling ultra-long user behavior sequences is critical for capturing both long- and short-term preferences in industrial recommender systems. Existing solutions typically rely on two-stage retrieval or indirect modeling paradigms, incuring upstream-downstream inconsistency and computational inefficiency. In this paper, we present LONGER, a Long-sequence Optimized traNsformer for GPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism for stabilizing attention over long contexts, (ii) a token merge module with lightweight InnerTransformers and hybrid attention strategy to reduce quadratic complexity, and (iii) a series of engineering optimizations, including training with mixed-precision and activation recomputation, KV cache serving, and the fully synchronous model training and serving framework for unified GPU-based dense and sparse parameter updates. LONGER consistently outperforms strong baselines in both offline metrics and online A/B testing in both advertising and e-commerce services at ByteDance, validating its consistent effectiveness and industrial-level scaling laws. Currently, LONGER has been fully deployed at more than 10 influential scenarios at ByteDance, serving billion users.

**Link**: [arxiv](http://arxiv.org/abs/2505.04421v2),  [pdf](http://arxiv.org/pdf/2505.04421v2)

**Tags**: cs.IR 



### LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for   Multi-Turn Dialogues
**Authors**: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan

**Updated**: 2025-07-18T06:12:08Z

**Summary**: Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a \href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new benchmark} with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.13681v1),  [pdf](http://arxiv.org/pdf/2507.13681v1)

**Tags**: cs.CL cs.AI 



### Accelerating Diffusion Transformer via Error-Optimized Cache
**Authors**: Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, Xingyu Zhu, Yanbin Hao

**Updated**: 2025-07-18T01:49:36Z

**Summary**: Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the \textbf{E}rror-\textbf{O}ptimized \textbf{C}ache (\textbf{EOC}). This method introduces three key improvements: \textbf{(1)} Prior knowledge extraction: Extract and process the caching differences; \textbf{(2)} A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; \textbf{(3)} Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching, especially excessive caching. On the ImageNet dataset, without substantially increasing the computational load, this method improves the FID of the generated images when the rule-based model FORA has a caching level of \textbf{75}\%, \textbf{50}\%, and \textbf{25}\%, and the training-based model Learning-to-cache has a caching level of \textbf{22}\%. Specifically, the FID values change from 30.454 to 21.690 (\textbf{28.8}\%), from 6.857 to 5.821 (\textbf{15.1}\%), from 3.870 to 3.692 (\textbf{4.6}\%), and from 3.539 to 3.451 (\textbf{2.5}\%) respectively. Code is available at https://github.com/qiujx0520/EOC_MM2025.git.

**Link**: [arxiv](http://arxiv.org/abs/2501.19243v3),  [pdf](http://arxiv.org/pdf/2501.19243v3)

**Tags**: cs.CV 



### Accelerating Diffusion Transformer via Gradient-Optimized Cache
**Authors**: Junxiang Qiu, Lin Liu, Shuo Wang, Jinda Lu, Kezhou Chen, Yanbin Hao

**Updated**: 2025-07-18T01:36:03Z

**Summary**: Feature caching has emerged as an effective strategy to accelerate diffusion transformer (DiT) sampling through temporal feature reuse. It is a challenging problem since (1) Progressive error accumulation from cached blocks significantly degrades generation quality, particularly when over 50\% of blocks are cached; (2) Current error compensation approaches neglect dynamic perturbation patterns during the caching process, leading to suboptimal error correction. To solve these problems, we propose the Gradient-Optimized Cache (GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient queue dynamically computes the gradient differences between cached and recomputed features. These gradients are weighted and propagated to subsequent steps, directly compensating for the approximation errors introduced by caching. (2) Inflection-Aware Optimization: Through statistical analysis of feature variation patterns, we identify critical inflection points where the denoising trajectory changes direction. By aligning gradient updates with these detected phases, we prevent conflicting gradient directions during error correction. Extensive evaluations on ImageNet demonstrate GOC's superior trade-off between efficiency and quality. With 50\% cached blocks, GOC achieves IS 216.28 (26.3\% higher) and FID 3.907 (43\% lower) compared to baseline DiT, while maintaining identical computational costs. These improvements persist across various cache ratios, demonstrating robust adaptability to different acceleration requirements. Code is available at https://github.com/qiujx0520/GOC_ICCV2025.git.

**Link**: [arxiv](http://arxiv.org/abs/2503.05156v2),  [pdf](http://arxiv.org/pdf/2503.05156v2)

**Tags**: cs.CV 



### Apple Intelligence Foundation Language Models: Tech Report 2025
**Authors**: Hanzhi Zhou, Erik Hornberger, Pengsheng Guo, Xiyou Zhou, Saiwen Wang, Xin Wang, Yifei He, Xuankai Chang, Rene Rauch, Louis D'hauwe, John Peebles, Alec Doane, Kohen Chia, Jenna Thibodeau, Zi-Yi Dou, Yuanyang Zhang, Ruoming Pang, Reed Li, Zhifeng Chen, Jeremy Warner, Zhaoyang Xu, Sophy Lee, David Mizrahi, Ramsey Tantawi, Chris Chaney, Kelsey Peterson, Jun Qin, Alex Dombrowski, Mira Chiang, Aiswarya Raghavan, Gerard Casamayor, Qibin Chen, Aonan Zhang, Nathalie Tran, Jianyu Wang, Hang Su, Thomas Voice, Alessandro Pappalardo, Brycen Wershing, Prasanth Yadla, Rui Li, Priyal Chhatrapati, Ismael Fernandez, Yusuf Goren, Xin Zheng, Forrest Huang, Tao Lei, Eray Yildiz, Alper Kokmen, Gokul Santhanam, Areeba Kamal, Kaan Elgin, Dian Ang Yap, Jeremy Liu, Peter Gray, Howard Xing, Kieran Liu, Matteo Ronchi, Moritz Schwarzer-Becker, Yun Zhu, Mandana Saebi, Jeremy Snow, David Griffiths, Guillaume Tartavel, Erin Feldman, Simon Lehnerer, Fernando Bermúdez-Medina, Hans Han, Joe Zhou, Xiaoyi Ren, Sujeeth Reddy, Zirui Wang, Tom Gunter, Albert Antony, Yuanzhi Li, John Dennison, Tony Sun, Yena Han, Yi Qin, Sam Davarnia, Jeffrey Bigham, Wayne Shan, Hannah Gillis Coleman, Guillaume Klein, Peng Liu, Muyang Yu, Jack Cackler, Yuan Gao, Crystal Xiao, Binazir Karimzadeh, Zhengdong Zhang, Felix Bai, Albin Madappally Jose, Feng Nan, Nazir Kamaldin, Dong Yin, Hans Hao, Yanchao Sun, Yi Hua, Charles Maalouf, Alex Guillen Garcia, Guoli Yin, Lezhi Li, Mohana Prasad Sathya Moorthy, Hongbin Gao, Jay Tang, Joanna Arreaza-Taylor, Faye Lao, Carina Peng, Josh Shaffer, Dan Masi, Sushma Rao, Tommi Vehvilainen, Senyu Tong, Dongcai Shen, Yang Zhao, Chris Bartels, Peter Fu, Qingqing Cao, Christopher Neubauer, Ethan Li, Mingfei Gao, Rebecca Callahan, Richard Wei, Patrick Dong, Alex Braunstein, Sachin Ravi, Adolfo Lopez Mendez, Kaiwei Huang, Kun Duan, Haoshuo Huang, Rui Qian, Stefano Ligas, Jordan Huffaker, Dongxu Li, Bailin Wang, Nanzhu Wang, Anuva Agarwal, Tait Madsen, Josh Newnham, Abhishek Sharma, Zhile Ren, Deepak Gopinath, Erik Daxberger, Saptarshi Guha, Oron Levy, Jing Lu, Nan Dun, Marc Kirchner, Yinfei Yang, Manjot Bilkhu, Dave Nelson, Anthony Spalvieri-Kruse, Juan Lao Tebar, Yang Xu, Phani Mutyala, Gabriel Jacoby-Cooper, Yingbo Wang, Karla Vega, Vishaal Mahtani, Darren Botten, Eric Wang, Hanli Li, Matthias Paulik, Haoran Yan, Navid Shiee, Yihao Qian, Bugu Wu, Qi Zhu, Ob Adaranijo, Bhuwan Dhingra, Zhe Gan, Nicholas Seidl, Grace Duanmu, Rong Situ, Yiping Ma, Yin Xia, David Riazati, Vasileios Saveris, Anh Nguyen, Michael, Lee, Patrick Sonnenberg, Chinguun Erdenebileg, Yanghao Li, Vivian Ma, James Chou, Isha Garg, Mark Lee, Keen You, Yuhong Li, Ransen Niu, Nandhitha Raghuram, Pulkit Agrawal, Henry Mason, Sumeet Singh, Keyu He, Hong-You Chen, Lucas Guibert, Shiyu Li, Varsha Paidi, Narendran Raghavan, Mingze Xu, Yuli Yang, Sergiu Sima, Irina Belousova, Sprite Chu, Afshin Dehghan, Philipp Dufter, David Haldimann, Zhen Yang, Margit Bowler, Chang Liu, Ying-Chang Cheng, Vivek Rathod, Syd Evans, Wilson Tsao, Dustin Withers, Haitian Sun, Biyao Wang, Peter Grasch, Walker Cheng, Yihao Feng, Vivek Kumar, Frank Chu, Victoria MönchJuan Haladjian, Doug Kang, Jiarui Lu, Ciro Sannino, Max Lam, Floris Weers, Bowen Pan, Kenneth Jung, Dhaval Doshi, Fangping Shi, Olli Saarikivi, Alp Aygar, Josh Elman, Cheng Leong, Eshan Verma, Matthew Lei, Jeff Nichols, Jiulong Shan, Donald Zhang, Lawrence Zhou, Stephen Murphy, Xianzhi Du, Chang Lan, Ankur Jain, Elmira Amirloo, Marcin Eichner, Naomy Sabo, Anupama Mann Anupama, David Qiu, Zhao Meng, Michael FitzMaurice, Peng Zhang, Simon Yeung, Chen Chen, Marco Zuliani, Andrew Hansen, Yang Lu, Brent Ramerth, Ziyi Zhong, Parsa Mazaheri, Matthew Hopkins, Mengyu Li, Simon Wang, David Chen, Farzin Rasteh, Chong Wang, Josh Gardner, Asaf Liberman, Haoxuan You, Andrew Walkingshaw, Xingyu Zhou, Jinhao Lei, Yan Meng, Quentin Keunebroek, Sam Wiseman, Anders Boesen Lindbo Larsen, Yi Zhang, Zaid Ahmed, Haiming Gang, Aaron Franklin, Kelvin Zou, Guillaume Seguin, Jonathan Janke, Rachel Burger, Co Giang, Cheng Shen, Jen Liu, Sanskruti Shah, Xiang Kong, Yiran Fei, TJ Collins, Chen Zhang, Zhiyun Lu, Michael Booker, Qin Ba, Yasutaka Tanaka, Andres Romero Mier Y Teran, Federico Scozzafava, Regan Poston, Jane Li, Eduardo Jimenez, Bas Straathof, Karanjeet Singh, Lindsay Hislop, Rajat Arora, Deepa Seshadri, Boyue Li, Colorado Reed, Zhen Li, TJ Lu, Yi Wang, Kaelen Haag, Nicholas Lusskin, Raunak Sinha, Rahul Nair, Eldon Schoop, Mary Beth Kery, Mehrdad Farajtbar, Brenda Yang, George Horrell, Shiwen Zhao, Dhruti Shah, Cha Chen, Bowen Zhang, Chang Gao, Devi Krishna, Jennifer Mallalieu, Javier Movellan, Di Feng, Emily Zhang, Sam Xu, Junting Pan, Dominik Moritz, Suma Jayaram, Kevin Smith, Dongseong Hwang, Daniel Parilla, Jiaming Hu, You-Cyuan Jhang, Emad Soroush, Fred Hohman, Nan Du, Emma Wang, Sam Dodge, Pragnya Sridhar, Joris Pelemans, Wei Fang, Nina Wenzel, Joseph Yitan Cheng, Hadas Kotek, Chung-Cheng Chiu, Meng Cao, Haijing Fu, Ruixuan Hou, Ke Ye, Diane Zhu, Nikhil Bhendawade, Joseph Astrauskas, Jian Liu, Sai Aitharaju, Wentao Wu, Artsiom Peshko, Hyunjik Kim, Nilesh Shahdadpuri, Andy De Wang, Qi Shan, Piotr Maj, Raul Rea Menacho, Justin Lazarow, Eric Liang Yang, Arsalan Farooq, Donghan Yu, David Güera, Minsik Cho, Kavya Nerella, Yongqiang Wang, Tao Jia, John Park, Jeff Lai, Haotian Zhang, Futang Peng, Daniele Molinari, Aparna Rajamani, Tyler Johnson, Lauren Gardiner, Chao Jia, Violet Yao, Wojciech Kryscinski, Xiujun Li, Shang-Chen Wu

**Updated**: 2025-07-17T23:37:19Z

**Summary**: We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.   A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.

**Link**: [arxiv](http://arxiv.org/abs/2507.13575v1),  [pdf](http://arxiv.org/pdf/2507.13575v1)

**Tags**: cs.LG cs.AI 



### PINT: Physics-Informed Neural Time Series Models with Applications to   Long-term Inference on WeatherBench 2m-Temperature Data
**Authors**: Keonvin Park, Jisu Kim, Jaemin Seo

**Updated**: 2025-07-17T13:44:39Z

**Summary**: This paper introduces PINT (Physics-Informed Neural Time Series Models), a framework that integrates physical constraints into neural time series models to improve their ability to capture complex dynamics. We apply PINT to the ERA5 WeatherBench dataset, focusing on long-term forecasting of 2m-temperature data. PINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed prior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures. This equation's analytical solutions (sine and cosine functions) facilitate rigorous evaluation of the benefits of incorporating physics-informed constraints. By benchmarking against a linear regression baseline derived from its exact solutions, we quantify the impact of embedding physical principles in data-driven models. Unlike traditional time series models that rely on future observations, PINT is designed for practical forecasting. Using only the first 90 days of observed data, it iteratively predicts the next two years, addressing challenges posed by limited real-time updates. Experiments on the WeatherBench dataset demonstrate PINT's ability to generalize, capture periodic trends, and align with physical principles. This study highlights the potential of physics-informed neural models in bridging machine learning and interpretable climate applications.   Our models and datasets are publicly available on GitHub: https://github.com/KV-Park.

**Link**: [arxiv](http://arxiv.org/abs/2502.04018v2),  [pdf](http://arxiv.org/pdf/2502.04018v2)

**Tags**: cs.LG 



### Integrating nano- and micrometer-scale energy deposition models for   mechanistic prediction of radiation-induced DNA damage and cell survival
**Authors**: Giulio Bordieri, Marta Missiaggia, Gianluca Lattanzi, Carmen Villagrasa, Yann Perrot, Francesco G. Cordoni

**Updated**: 2025-07-17T09:55:43Z

**Summary**: We present an integrated modeling framework that combines the Generalized Stochastic Microdosimetric Model (GSM2), used to predict cell survival fractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for simulating radiation-induced DNA damage in cell populations. This approach enables the generation of spatially and structurally resolved double-strand break (DSB) distributions, capturing key features such as damage complexity and chromosome specificity. A novel application of the DBSCAN clustering algorithm is introduced to group DSBs at the micrometer scale. This allows the identification of physical aggregates of DNA damage and their association with subnuclear domains, providing a direct link to the cell survival probability as predicted by \gsm.   The model was validated using experimental data from HUVEC cells irradiated with 220 kV X-rays and H460 cells exposed to protons over a wide linear energy transfer (LET) range, from approximately 4 keV/{\mu}m to over 20 keV/{\mu}m. Results show excellent agreement between simulations and experimental survival probabilities, making this one of the first consistent multi-scale models to bridge nanodosimetric and microdosimetric representations of radiation with biological outcomes such as cell survival.   By incorporating the inherent stochastic nature of radiation-matter interactions, this framework effectively connects the physical properties of the radiation field to the biological response at the cellular level. Its accuracy across various radiation types and energies supports its potential for use in biologically optimized radiotherapy.

**Link**: [arxiv](http://arxiv.org/abs/2507.00929v4),  [pdf](http://arxiv.org/pdf/2507.00929v4)

**Tags**: physics.bio-ph 



### IAM: Efficient Inference through Attention Mapping between   Different-scale LLMs
**Authors**: Yi Zhao, Zuchao Li, Hai Zhao

**Updated**: 2025-07-16T06:39:11Z

**Summary**: LLMs encounter significant challenges in resource consumption nowadays, especially with long contexts. Despite extensive efforts dedicate to enhancing inference efficiency, these methods primarily exploit internal sparsity within the models, without leveraging external information for optimization. We identify the high similarity of attention matrices across different-scale LLMs, which offers a novel perspective for optimization. We first conduct a comprehensive analysis of how to measure similarity, how to select mapping Layers and whether mapping is consistency. Based on these insights, we introduce the IAM framework, which achieves dual benefits of accelerated attention computation and reduced KV cache usage by performing attention mapping between small and large LLMs. Our experimental results demonstrate that IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without appreciably sacrificing performance. Experiments on different series of models show the generalizability of IAM. Importantly, it is also orthogonal to many existing KV cache optimization methods, making it a versatile addition to the current toolkit for enhancing LLM efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2507.11953v1),  [pdf](http://arxiv.org/pdf/2507.11953v1)

**Tags**: cs.CL cs.LG 



### Streaming 4D Visual Geometry Transformer
**Authors**: Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, Jiwen Lu

**Updated**: 2025-07-15T17:59:57Z

**Summary**: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a fundamental yet challenging computer vision task. To facilitate interactive and real-time applications, we propose a streaming 4D visual geometry transformer that shares a similar philosophy with autoregressive large language models. We explore a simple and efficient design and employ a causal transformer architecture to process the input sequence in an online manner. We use temporal causal attention and cache the historical keys and values as implicit memory to enable efficient streaming long-term 4D reconstruction. This design can handle real-time 4D reconstruction by incrementally integrating historical information while maintaining high-quality spatial consistency. For efficient training, we propose to distill knowledge from the dense bidirectional visual geometry grounded transformer (VGGT) to our causal model. For inference, our model supports the migration of optimized efficient attention operator (e.g., FlashAttention) from the field of large language models. Extensive experiments on various 4D geometry perception benchmarks demonstrate that our model increases the inference speed in online scenarios while maintaining competitive performance, paving the way for scalable and interactive 4D vision systems. Code is available at: https://github.com/wzzheng/StreamVGGT.

**Link**: [arxiv](http://arxiv.org/abs/2507.11539v1),  [pdf](http://arxiv.org/pdf/2507.11539v1)

**Tags**: cs.CV cs.AI cs.LG 



### MIRAGE: KV Cache Optimization through Parameter Remapping for   Multi-tenant LLM Serving
**Authors**: Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar

**Updated**: 2025-07-15T17:23:22Z

**Summary**: KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce MIRAGE, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that MIRAGE significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2507.11507v1),  [pdf](http://arxiv.org/pdf/2507.11507v1)

**Tags**: cs.OS 



### ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in   Large Language Models
**Authors**: Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren

**Updated**: 2025-07-15T12:59:47Z

**Summary**: Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.

**Link**: [arxiv](http://arxiv.org/abs/2506.22791v3),  [pdf](http://arxiv.org/pdf/2506.22791v3)

**Tags**: cs.CL cs.DB 



### KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware   Rotary Positional Embedding
**Authors**: Luohe Shi, Zuchao Li, Lefei Zhang, Guoming Liu, Baoyuan Qi, Hai Zhao

**Updated**: 2025-07-15T12:52:12Z

**Summary**: Large language models (LLMs) based on Transformer Decoders have become the preferred choice for conversational generative AI. Despite the overall superiority of the Decoder architecture, the gradually increasing Key-Value (KV) cache during inference has emerged as a primary efficiency bottleneck, both in aspects of memory consumption and data transfer bandwidth limitations. To address these challenges, we propose a paradigm called KV-Latent. By down-sampling the Key-Value vector dimensions into a latent space, we can significantly reduce the KV Cache footprint and improve inference speed, only with a small amount of extra training, less than 1\% of pre-training takes. Besides, we enhanced the stability of Rotary Positional Embedding applied on lower-dimensional vectors by modifying its frequency sampling mechanism, avoiding noise introduced by higher frequencies while retaining position attenuation. Our experiments, including both models with Grouped Query Attention and those without, have yielded satisfactory results. Finally, we conducted comparative experiments to study the impact of separately reducing Key and Value components on model's performance. Our approach allows for the construction of more efficient language model systems, and opens the new possibility on KV Cache saving and efficient LLMs. Our code is available at https://github.com/ShiLuohe/KV-Latent.

**Link**: [arxiv](http://arxiv.org/abs/2507.11273v1),  [pdf](http://arxiv.org/pdf/2507.11273v1)

**Tags**: cs.CL 



### VSAG: An Optimized Search Framework for Graph-based Approximate Nearest   Neighbor Search
**Authors**: Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng

**Updated**: 2025-07-15T11:31:14Z

**Summary**: Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graph-based ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index. This paper introduces VSAG, an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations: (i) efficient memory access: it reduces L3 cache misses with pre-fetching and cache-friendly vector organization; (ii) automated parameter tuning: it automatically selects performance-optimal parameters without requiring index rebuilding; (iii) efficient distance computation: it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate VSAG on real-world datasets. The experimental results show that VSAG achieves the state-of-the-art performance and provides up to 4x speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.17911v3),  [pdf](http://arxiv.org/pdf/2503.17911v3)

**Tags**: cs.DB 



### Two-dimensional single-crystal photonic scintillator for enhanced X-ray   imaging
**Authors**: Tatsunori Shibuya, Eichi Terasawa, Hiromi Kimura, Takeshi Fujiwara

**Updated**: 2025-07-15T09:15:18Z

**Summary**: The evolution of X-ray detection technology has significantly enhanced sensitivity and spatial resolution in non-destructive imaging of internal structure. However, the problem of low luminescence and transparency of scintillator materials restricts imaging with lower radiation doses and thicker materials. Here, we propose a two-dimensional photonic scintillator for single crystal and demonstrate that the optical guiding effect emerging from the structure reduces luminescence leakage and increases the signal intensity by around a factor of 2 from 200 to 450 kV. This approach has the potential to enhance the output rate by an order of magnitude. The photonic structure features a fine array pitch and large-scale detection area with fast fabrication time. Our scheme paves the way for high sensitivity X-ray imaging.

**Link**: [arxiv](http://arxiv.org/abs/2507.11121v1),  [pdf](http://arxiv.org/pdf/2507.11121v1)

**Tags**: physics.optics physics.ins-det 



### MMStencil: Optimizing High-order Stencils on Multicore CPU using Matrix   Unit
**Authors**: Yinuo Wang, Tianqi Mao, Lin Gan, Wubing Wan, Zeyu Song, Jiayu Fu, Lanke He, Wenqiang Wang, Zekun Yin, Wei Xue, Guangwen Yang

**Updated**: 2025-07-15T08:00:11Z

**Summary**: Matrix-accelerated stencil computation is a hot research topic, yet its application to three-dimensional (3D) high-order stencils and HPC remains underexplored. With the emergence of matrix units on multicore CPUs, we analyze matrix-based acceleration strategies and tailor an optimal approach for 3D high-order stencils. We introduce algorithmic optimizations based on SIMD and matrix units to address strided memory accesses, alignment conflicts, and redundant accesses. We propose memory optimizations to boost on-package memory efficiency, and a novel multi-thread parallelism paradigm to overcome data-sharing challenges caused by the absence of shared data caches. MMStencil sustains consistently high hardware utilization across diverse stencil shapes and dimensions. Our DMA-based inter-NUMA communication further mitigates NUMA effects and MPI limitations in hybrid parallelism. Combining all the innovations, MMStencil outperforms state-of-the-art libraries on Nvidia A100 GPGPU by up to 2.1x. Moreover, the performance improvements translate directly to real-world HPC applications and enable RTM applications to yield 1.8x speedup versus a highly optimized industrial Nvidia A100 GPGPU version.

**Link**: [arxiv](http://arxiv.org/abs/2507.11067v1),  [pdf](http://arxiv.org/pdf/2507.11067v1)

**Tags**: cs.DC 



### Enabling the Write-Back Page Cache with Strong Consistency in   Distributed Userspace File Systems
**Authors**: Haoyu Li, Jingkai Fu, Qing Li, Windsor Hsu, Asaf Cidon

**Updated**: 2025-07-14T19:51:09Z

**Summary**: Cloud platforms host thousands of tenants that demand POSIX semantics, high throughput, and rapid evolution from their storage layer. Kernel-native distributed file systems supply raw speed, but their privileged code base couples every release to the kernel, widens the blast radius of crashes, and slows innovation. FUSE-based distributed file systems flip those trade-offs: they run in user space for fast deployment and strong fault isolation, yet the FUSE interface disables the kernel's write-back page cache whenever strong consistency is required. Practitioners must therefore choose between (i) weak consistency with fast write-back caching or (ii) strong consistency with slow write-through I/O, an limitation that has kept FUSE distributed file systems out of write-intensive cloud workloads.   To this end, We present DistFUSE, the first distributed FUSE file system that delivers write-back kernel caching and strong consistency. DistFUSE achieves this by offloading userspace consistency control to the kernel driver, allowing coordinated access to the kernel's page cache across nodes. This design eliminates blind local cache updates and ensures cluster-wide strong consistency without compromising performance. In our evaluation, DistFUSE achieves up to 68.0% higher throughput and 40.4% lower latency than the existing write-through design of FUSE-based distributed file system.

**Link**: [arxiv](http://arxiv.org/abs/2503.18191v2),  [pdf](http://arxiv.org/pdf/2503.18191v2)

**Tags**: cs.OS 



### FAFO: Over 1 million TPS on a single node running EVM while still   Merkleizing every block
**Authors**: Ryan Zarick, Isaac Zhang, Daniel Wong, Thomas Kim, Bryan Pellegrino, Mignon Li, Kelvin Wong

**Updated**: 2025-07-14T19:31:06Z

**Summary**: Current blockchain execution throughput is limited by data contention, reducing execution layer parallelism. Fast Ahead-of-Formation Optimization (FAFO) is the first blockchain transaction scheduler to address this problem by reordering transactions before block formation for maximum concurrency. FAFO uses CPU-optimized cache-friendly Bloom filters to efficiently detect conflicts and schedule parallel transaction execution at high throughput and low overhead.   We integrate the Rust EVM client (REVM) into FAFO and achieve over 1.1 million native ETH transfers per second and over half a million ERC20 transfers per second on a single node (Table 1), with 91% lower cost compared to state-of-the-art sharded execution. Unlike many other existing high throughput blockchain execution clients, FAFO uses QMDB to Merkleize world state after every block, enabling light clients and stateless validation for ZK-based vApps. FAFO scales with minimal synchronization overhead, scaling linearly with additional CPU resources until it fully exploits the maximum parallelism of the underlying transaction flow. FAFO proves that the high throughput necessary to support future decentralized applications can be achieved with a streamlined execution layer and innovations in blockchain transaction scheduler design. FAFO is open-sourced at https://github.com/LayerZero-Labs/fafo.

**Link**: [arxiv](http://arxiv.org/abs/2507.10757v1),  [pdf](http://arxiv.org/pdf/2507.10757v1)

**Tags**: cs.DC cs.NI 



### LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of   Large Language Models
**Authors**: Dachuan Shi, Yonggan Fu, Xiangchi Yuan, Zhongzhi Yu, Haoran You, Sixu Li, Xin Dong, Jan Kautz, Pavlo Molchanov, Yingyan, Lin

**Updated**: 2025-07-14T19:09:57Z

**Summary**: Recent advancements in Large Language Models (LLMs) have spurred interest in numerous applications requiring robust long-range capabilities, essential for processing extensive input contexts and continuously generating extended outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in LLMs escalates, creating a significant efficiency bottleneck. In this paper, we propose a new KV cache optimization paradigm called LaCache, a training-free method for efficient and accurate generative inference of LLMs. LaCache enables LLMs to simultaneously address both of the critical challenges in long-range modeling: robust long-range capabilities and continuous generation without running out-of-memory (OOM). Specifically, LaCache integrates two key innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only sequentially (left-to-right within each layer) but also across layers (from shallow to deep), providing an extended span for capturing long-range dependencies under a fixed storage budget, thereby boosting long-range capabilities; and (2) an iterative compaction mechanism that progressively compresses older caches, freeing up space for new tokens within a fixed cache size. This token distance-based dynamic compression enables more effective continuous generation under constrained cache budgets. Experiments across various tasks, benchmarks, and LLM models consistently validate LaCache's effectiveness in enhancing LLMs' long-range capabilities. Our code is available at https://github.com/GATECH-EIC/LaCache.

**Link**: [arxiv](http://arxiv.org/abs/2507.14204v1),  [pdf](http://arxiv.org/pdf/2507.14204v1)

**Tags**: cs.LG cs.AI cs.CL 



### DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM   Serving
**Authors**: Yuhan Liu, Yuyang Huang, Jiayi Yao, Shaoting Feng, Zhuohan Gu, Kuntai Du, Hanchen Li, Yihua Cheng, Junchen Jiang, Shan Lu, Madan Musuvathi, Esha Choukse

**Updated**: 2025-07-14T18:22:53Z

**Summary**: Compound AI systems, such as agentic systems, are an emerging trend in large-scale enterprise settings, with multiple LLMs specialized for different users, tasks, and/or roles working together. In these scenarios, different models often process inputs that share the same context prefix. Although much work was done in the past to enable the reuse of prefix KV caches across inputs for a single model, how to enable one model to reuse the prefix KV caches of a different model remains an open question.   We introduce DroidSpeak, the first distributed LLM inference system that enables KV cache reuse across distributed nodes running inference of different LLMs, so long as the LLMs have the same architecture. We present the first study that aims at understanding the impact of sharing KV caches across different LLMs, and if/when such sharing affects quality. Inspired by the findings, we present DroidSpeak, which selectively recomputes a few layers of the KV cache produced by another LLM and reuses the remaining layers, with negligible quality loss. Moreover, carefully pipelining the layer-wise re-computation and the loading of reused KV cache further improves the inference performance. Experiments on diverse datasets and model pairs demonstrate that DroidSpeak achieves up to 4x throughput improvement and about 3.1x faster prefill (time to first token), with negligible loss of quality in F1 scores, Rouge-L or code similarity score, compared to the baseline which does not allow any sharing across models.

**Link**: [arxiv](http://arxiv.org/abs/2411.02820v4),  [pdf](http://arxiv.org/pdf/2411.02820v4)

**Tags**: cs.MA cs.AI cs.CL cs.LG 



### PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following   Models Need for Efficient Generation
**Authors**: Ao Wang, Hui Chen, Jiaxin Li, Jianchao Tan, Kefeng Zhang, Xunliang Cai, Zijia Lin, Jungong Han, Guiguang Ding

**Updated**: 2025-07-14T16:14:49Z

**Summary**: Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at https://github.com/THU-MIG/PrefixKV.

**Link**: [arxiv](http://arxiv.org/abs/2412.03409v3),  [pdf](http://arxiv.org/pdf/2412.03409v3)

**Tags**: cs.CV 



### FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline
**Authors**: Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen

**Updated**: 2025-07-14T15:09:01Z

**Summary**: Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year.

**Link**: [arxiv](http://arxiv.org/abs/2507.10367v1),  [pdf](http://arxiv.org/pdf/2507.10367v1)

**Tags**: cs.DC cs.PF 



### Pruning the Tree: Rethinking RPKI Architecture From The Ground Up
**Authors**: Haya Schulmann, Niklas Vogel

**Updated**: 2025-07-14T09:45:34Z

**Summary**: Resource Public Key Infrastructure (RPKI) is a critical security mechanism for BGP, but the complexity of its architecture is a growing concern as its adoption scales. Current RPKI design heavily reuses legacy PKI components, such as X.509 EE-certificates, ASN.1 encoding, and XML-based repository protocols, which introduce excessive cryptographic validation, redundant metadata, and inefficiencies in both storage and processing. We show that these design choices, although based on established standards, create significant performance bottlenecks, increase the vulnerability surface, and hinder scalability for wide-scale Internet deployment.   In this paper, we perform the first systematic analysis of the root causes of complexity in RPKI's design and experimentally quantify their real-world impact. We show that over 70\% of validation time in RPKI relying parties is spent on certificate parsing and signature verification, much of it unnecessary. Building on this insight, we introduce the improved RPKI (iRPKI), a backwards-compatible redesign that preserves all security guarantees while substantially reducing protocol overhead. iRPKI eliminates EE-certificates and ROA signatures, merges revocation and integrity objects, replaces verbose encodings with Protobuf, and restructures repository metadata for more efficient access. We experimentally demonstrate that our implementation of iRPKI in the Routinator validator achieves a 20x speed-up of processing time, 18x improvement of bandwidth requirements and 8x reduction in cache memory footprint, while also eliminating classes of vulnerabilities that have led to at least 10 vulnerabilities in RPKI software. iRPKI significantly increases the feasibility of deploying RPKI at scale in the Internet, and especially in constrained environments. Our design may be deployed incrementally without impacting existing operations.

**Link**: [arxiv](http://arxiv.org/abs/2507.01465v2),  [pdf](http://arxiv.org/pdf/2507.01465v2)

**Tags**: cs.CR 



### ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal   Parallelism
**Authors**: Zedong Liu, Shenggan Cheng, Guangming Tan, Yang You, Dingwen Tao

**Updated**: 2025-07-14T08:53:48Z

**Summary**: Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we propose Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).

**Link**: [arxiv](http://arxiv.org/abs/2507.10069v1),  [pdf](http://arxiv.org/pdf/2507.10069v1)

**Tags**: cs.DC cs.LG 



### Not all tokens are created equal: Perplexity Attention Weighted Networks   for AI generated text detection
**Authors**: Pablo Miralles-González, Javier Huertas-Tato, Alejandro Martín, David Camacho

**Updated**: 2025-07-14T07:05:28Z

**Summary**: The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.

**Link**: [arxiv](http://arxiv.org/abs/2501.03940v3),  [pdf](http://arxiv.org/pdf/2501.03940v3)

**Tags**: cs.CL cs.AI 



### The Hitchhiker's Guide to Programming and Optimizing Cache Coherent   Heterogeneous Systems: CXL, NVLink-C2C, and AMD Infinity Fabric
**Authors**: Zixuan Wang, Suyash Mahar, Luyi Li, Jangseon Park, Jinpyo Kim, Theodore Michailidis, Yue Pan, Mingyao Shen, Tajana Rosing, Dean Tullsen, Steven Swanson, Jishen Zhao

**Updated**: 2025-07-14T07:03:30Z

**Summary**: We present a thorough analysis of the use of modern heterogeneous systems interconnected by various cachecoherent links, including CXL, NVLink-C2C, and Infinity Fabric. We studied a wide range of server systems that combined CPUs from different vendors and various types of coherent memory devices, including CXL memory expander, CXL pool, CXL shared memory, GH200 GPU, and AMD MI300a HBM. For this study, we developed a heterogeneous memory benchmark suite, Heimdall, to profile the performance of such heterogeneous systems and present a detailed performance comparison across systems. By leveraging H E I M DA L L , we unveiled the detailed architecture design in these systems, drew observations on optimizing performance for workloads, and pointed out directions for future development of cache coherent heterogeneous systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.02814v2),  [pdf](http://arxiv.org/pdf/2411.02814v2)

**Tags**: cs.PF cs.AR cs.DC cs.OS 



### InstCache: A Predictive Cache for LLM Serving
**Authors**: Longwei Zou, Yan Liu, Jiamu Kang, Tingfeng Liu, Jiangang Kong, Yangdong Deng

**Updated**: 2025-07-14T02:22:43Z

**Summary**: The revolutionary capabilities of Large Language Models (LLMs) are attracting rapidly growing popularity and leading to soaring user requests to inference serving systems. Caching techniques, which leverage data reuse to reduce computation, offer opportunities to optimize the performance of LLM inference engines. On the one hand, the low-level key-value (KV) cache working at the token level is widely adopted, albeit it incurs significant overhead as request volume grows. On the other hand, instruction-level caching, which stores full instruction-response pairs, is expected to play an increasingly crucial role. However, the high variability in the content and length of instructions make it rare for identical instructions to recur within a short time window, presenting challenges for effective caching instruction-response pairs. To address this challenge, we propose InstCache, a predictive caching mechanism for LLM serving systems. Leveraging the capability of LLMs, we can effectively reorder the representation space of instruction texts and develop a sufficient level of spatial locality. Such spatial locality enables us to predict potential instructions located in a compact region in the space, resulting in an effective caching system at runtime. Experimental results demonstrate that InstCache achieves a 2.3x higher hit rate compared to the upper bound of traditional caching mechanisms on WildChat dataset and reduces the time per output token of vLLM by up to 42.0% and 50.0% on LMSys and Moss datasets, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2411.13820v2),  [pdf](http://arxiv.org/pdf/2411.13820v2)

**Tags**: cs.CL cs.DC 



### Advancing Reliable Test-Time Adaptation of Vision-Language Models under   Visual Variations
**Authors**: Yiwen Liang, Hui Chen, Yizhe Xiong, Zihan Zhou, Mengyao Lyu, Zijia Lin, Shuaicheng Niu, Sicheng Zhao, Jungong Han, Guiguang Ding

**Updated**: 2025-07-13T05:37:33Z

**Summary**: Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but struggle with distribution shifts in downstream tasks when labeled data is unavailable, which has motivated the development of Test-Time Adaptation (TTA) to improve VLMs' performance during inference without annotations. Among various TTA approaches, cache-based methods show promise by preserving historical knowledge from low-entropy samples in a dynamic cache and fostering efficient adaptation. However, these methods face two critical reliability challenges: (1) entropy often becomes unreliable under distribution shifts, causing error accumulation in the cache and degradation in adaptation performance; (2) the final predictions may be unreliable due to inflexible decision boundaries that fail to accommodate large downstream shifts. To address these challenges, we propose a Reliable Test-time Adaptation (ReTA) method that integrates two complementary strategies to enhance reliability from two perspectives. First, to mitigate the unreliability of entropy as a sample selection criterion for cache construction, we introduce Consistency-aware Entropy Reweighting (CER), which incorporates consistency constraints to weight entropy during cache updating. While conventional approaches rely solely on low entropy for cache prioritization and risk introducing noise, our method leverages predictive consistency to maintain a high-quality cache and facilitate more robust adaptation. Second, we present Diversity-driven Distribution Calibration (DDC), which models class-wise text embeddings as multivariate Gaussian distributions, enabling adaptive decision boundaries for more accurate predictions across visually diverse content. Extensive experiments demonstrate that ReTA consistently outperforms state-of-the-art methods, particularly under challenging real-world distribution shifts.

**Link**: [arxiv](http://arxiv.org/abs/2507.09500v1),  [pdf](http://arxiv.org/pdf/2507.09500v1)

**Tags**: cs.CV 



### Auditing Prompt Caching in Language Model APIs
**Authors**: Chenchen Gu, Xiang Lisa Li, Rohith Kuditipudi, Percy Liang, Tatsunori Hashimoto

**Updated**: 2025-07-13T04:42:28Z

**Summary**: Prompt caching in large language models (LLMs) results in data-dependent timing variations: cached prompts are processed faster than non-cached prompts. These timing differences introduce the risk of side-channel timing attacks. For example, if the cache is shared across users, an attacker could identify cached prompts from fast API response times to learn information about other users' prompts. Because prompt caching may cause privacy leakage, transparency around the caching policies of API providers is important. To this end, we develop and conduct statistical audits to detect prompt caching in real-world LLM API providers. We detect global cache sharing across users in seven API providers, including OpenAI, resulting in potential privacy leakage about users' prompts. Timing variations due to prompt caching can also result in leakage of information about model architecture. Namely, we find evidence that OpenAI's embedding model is a decoder-only Transformer, which was previously not publicly known.

**Link**: [arxiv](http://arxiv.org/abs/2502.07776v2),  [pdf](http://arxiv.org/pdf/2502.07776v2)

**Tags**: cs.CL cs.CR cs.LG 



## Keyword: LLM Inference 
 ### Beyond Fixed: Variable-Length Denoising for Diffusion Large Language   Models
**Authors**: Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin

**Updated**: 2025-08-01T17:56:07Z

**Summary**: Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.

**Link**: [arxiv](http://arxiv.org/abs/2508.00819v1),  [pdf](http://arxiv.org/pdf/2508.00819v1)

**Tags**: cs.CL 



### FalconGym: A Photorealistic Simulation Framework for Zero-Shot   Sim-to-Real Vision-Based Quadrotor Navigation
**Authors**: Yan Miao, Will Shen, Sayan Mitra

**Updated**: 2025-08-01T17:54:02Z

**Summary**: We present a novel framework demonstrating zero-shot sim-to-real transfer of visual control policies learned in a Neural Radiance Field (NeRF) environment for quadrotors to fly through racing gates. Robust transfer from simulation to real flight poses a major challenge, as standard simulators often lack sufficient visual fidelity. To address this, we construct a photorealistic simulation environment of quadrotor racing tracks, called FalconGym, which provides effectively unlimited synthetic images for training. Within FalconGym, we develop a pipelined approach for crossing gates that combines (i) a Neural Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor poses from single-frame RGB images and IMU data, and (ii) a self-attention-based multi-modal controller that adaptively integrates visual features and pose estimation. This multi-modal design compensates for perception noise and intermittent gate visibility. We train this controller purely in FalconGym with imitation learning and deploy the resulting policy to real hardware with no additional fine-tuning. Simulation experiments on three distinct tracks (circle, U-turn and figure-8) demonstrate that our controller outperforms a vision-only state-of-the-art baseline in both success rate and gate-crossing accuracy. In 30 live hardware flights spanning three tracks and 120 gates, our controller achieves a 95.8% success rate and an average error of just 10 cm when flying through 38 cm-radius gates.

**Link**: [arxiv](http://arxiv.org/abs/2503.02198v2),  [pdf](http://arxiv.org/pdf/2503.02198v2)

**Tags**: cs.RO 



### A JWST View of the Overmassive Black Hole in NGC 4486B
**Authors**: Behzad Tahmasebzadeh, Matthew A. Taylor, Monica Valluri, Haruka Yoshino, Eugene Vasiliev, Michael J. Drinkwater, Solveig Thompson, Kristen Dage, Patrick Côté, Laura Ferrarese, Tatsuya Akiba, Vivienne Baldassare, Misty C. Bentz, John P. Blakeslee, Holger Baumgardt, Youkyung Ko, Chengze Liu, Ann-Marie Madigan, Eric W. Peng, Joel Roediger, Kaixiang Wang, Tyrone E. Woods

**Updated**: 2025-08-01T17:45:12Z

**Summary**: We present a new stellar dynamical measurement of the supermassive black hole (SMBH) in the compact elliptical galaxy NGC 4486B, based on integral field spectroscopy with JWST/NIRSpec. The two-dimensional kinematic maps reveal a resolved double nucleus and a velocity dispersion peak offset from the photometric center. Utilizing two independent methods-Schwarzschild orbit-superposition and Jeans Anisotropic Modeling-we tightly constrain the black hole mass by fitting the full line-of-sight velocity distribution. Our axisymmetric Schwarzschild models yield a best-fit black hole mass of $M_{BH} = 3.6^{+0.7}_{-0.7} \times 10^8 \, M_{\odot}$, slightly lower but significantly more precise than previous estimates. However, since our models do not account for the non-equilibrium nature of the double nucleus, this value may represent a lower limit. Across all tested dynamical models, the inferred $M_{BH}/M_*$ ratio ranges from ~ 4-13%, providing robust evidence for an overmassive SMBH in NGC 4486B. Combined with the galaxy's location deep within the Virgo Cluster, our results support the interpretation that NGC 4486B is the tidally stripped remnant core of a formerly massive galaxy. As the JWST/NIRSpec field of view is insufficient to constrain the dark matter halo, we incorporate archival ground-based long-slit kinematics extending to 5 arcsec. While this provides some leverage on the dark matter content, the constraints remain relatively weak. We place only an upper limit on the dark matter fraction, with $M_{DM}/M_{*} < 0.5$ within 1 kpc-well beyond the effective radius. The inferred black hole mass remains unchanged with or without a dark matter halo.

**Link**: [arxiv](http://arxiv.org/abs/2505.14676v2),  [pdf](http://arxiv.org/pdf/2505.14676v2)

**Tags**: astro-ph.GA 



### New Insights into the T Tauri Binary Separation Distribution
**Authors**: Caleb Eastlund, Maxwell Moe, Kaitlin M. Kratter, Marina Kounkel

**Updated**: 2025-08-01T17:41:23Z

**Summary**: For three decades, adaptive optic surveys have revealed an excess of T Tauri binaries across a = 10-100 au in nearby star-forming regions compared to the field population of main-sequence (MS) stars. Such an excess requires that most stars are born in dense clusters and subjected to significant dynamical processing that disrupts such binaries across intermediate separations. However, we demonstrate that the apparent excess is due to an observational selection bias. Close binaries within a < 100 au clear out their dusty circumstellar disks on faster timescales compared to wide binaries and single stars. A magnitude-limited sample is therefore biased toward close binaries that have preferentially cleared out their obscuring disks. We re-examine the separation distribution of pre-MS binaries in low-density Taurus, moderately dense Upper Scorpius, and the extremely dense Orion Nebula Cluster (ONC). By limiting the samples to primary spectral type / mass instead of magnitude, the artificial excess across a = 10-100 au disappears in all three environments. Across wider separations a = 100-4,000 au, Taurus exhibits an excess of companions (mostly tertiaries), the ONC displays a deficit, and Upper Scorpius matches the field MS population. The field derives from an amalgam of all three environments, where Upper Scorpius corresponds to the average birth environment of solar-type stars. The total binary fraction within a < 10,000 au in Taurus is only 52% +/- 7%, substantially lower than the 100% inferred from the biased observations and only slightly higher than the field MS value of 45%. N-body interactions preferentially disrupt outer tertiaries with only marginal dynamical processing of the inner binaries, especially those within a < 100 au.

**Link**: [arxiv](http://arxiv.org/abs/2506.07938v2),  [pdf](http://arxiv.org/pdf/2506.07938v2)

**Tags**: astro-ph.SR astro-ph.GA 



### Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory   Management
**Authors**: Ping Chen, Zhuohong Deng, Ping Li, Shuibing He, Hongzi Zhu, Yi Zheng, Zhefeng Wang, Baoxing Huai, Minyi Guo

**Updated**: 2025-08-01T17:39:25Z

**Summary**: Training large language models often employs recomputation to alleviate memory pressure, which can introduce up to 30% overhead in real-world scenarios. In this paper, we propose Adacc, a novel memory management framework that combines adaptive compression and activation checkpointing to reduce the GPU memory footprint. It comprises three modules: (1) We design layer-specific compression algorithms that account for outliers in LLM tensors, instead of directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We propose an optimal scheduling policy that employs MILP to determine the best memory optimization for each tensor. (3) To accommodate changes in training tensors, we introduce an adaptive policy evolution mechanism that adjusts the policy during training to enhance throughput. Experimental results show that Adacc can accelerate the LLM training by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining comparable model accuracy to the Baseline.

**Link**: [arxiv](http://arxiv.org/abs/2508.00806v1),  [pdf](http://arxiv.org/pdf/2508.00806v1)

**Tags**: cs.LG cs.DC 



### Online Fine-Tuning of Carbon Emission Predictions using Real-Time   Recurrent Learning for State Space Models
**Authors**: Julian Lemmel, Manuel Kranzl, Adam Lamine, Philipp Neubauer, Radu Grosu, Sophie Neubauer

**Updated**: 2025-08-01T17:37:19Z

**Summary**: This paper introduces a new approach for fine-tuning the predictions of structured state space models (SSMs) at inference time using real-time recurrent learning. While SSMs are known for their efficiency and long-range modeling capabilities, they are typically trained offline and remain static during deployment. Our method enables online adaptation by continuously updating model parameters in response to incoming data. We evaluate our approach for linear-recurrent-unit SSMs using a small carbon emission dataset collected from embedded automotive hardware. Experimental results show that our method consistently reduces prediction error online during inference, demonstrating its potential for dynamic, resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.00804v1),  [pdf](http://arxiv.org/pdf/2508.00804v1)

**Tags**: cs.CE cs.LG cs.SY eess.SY 



### RecPS: Privacy Risk Scoring for Recommender Systems
**Authors**: Jiajie He, Yuechun Gu, Keke Chen

**Updated**: 2025-08-01T17:19:56Z

**Summary**: Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.

**Link**: [arxiv](http://arxiv.org/abs/2507.18365v3),  [pdf](http://arxiv.org/pdf/2507.18365v3)

**Tags**: cs.IR cs.AI cs.CR 



### A Survey of Self-Evolving Agents: On Path to Artificial Super   Intelligence
**Authors**: Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenhailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang

**Updated**: 2025-08-01T17:17:09Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.21046v3),  [pdf](http://arxiv.org/pdf/2507.21046v3)

**Tags**: cs.AI 



### Assessing Racial Disparities in Healthcare Expenditures via Mediator   Distribution Shifts
**Authors**: Xiaxian Ou, Xinwei He, David Benkeser, Razieh Nabi

**Updated**: 2025-08-01T17:16:08Z

**Summary**: Racial disparities in healthcare expenditures are well-documented, yet the underlying drivers remain complex and require further investigation. This study develops a framework for decomposing such disparities through shifts in the distributions of mediating variables, rather than treating race itself as a manipulable exposure. We define disparities as differences in covariate-adjusted outcome distributions across racial groups, and decompose the total disparity into two components: one attributable to differences in mediator distributions, and another residual component that would remain even after equalizing these distributions. Using data from the Medical Expenditures Panel Survey, we examine the extent to which expenditure disparities would persist or be reduced if mediators such as socioeconomic status, insurance access, health behaviors, or health status were equalized across racial groups. To ensure valid inference, we derive asymptotically linear estimators based on influence-function techniques and flexible machine learning tools, including super learners and a two-part model designed for the zero-inflated, right-skewed nature of expenditure data.

**Link**: [arxiv](http://arxiv.org/abs/2504.21688v2),  [pdf](http://arxiv.org/pdf/2504.21688v2)

**Tags**: stat.AP stat.ME stat.ML 



### Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun   Handling in Large Language Models
**Authors**: Xushuo Tang, Yi Ding, Zhengyi Yang, Yin Chen, Yongrui Gu, Wenke Yang, Mingchen Ju, Xin Cao, Yongfei Liu, Wenjie Zhang

**Updated**: 2025-08-01T17:11:42Z

**Summary**: Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.

**Link**: [arxiv](http://arxiv.org/abs/2508.00788v1),  [pdf](http://arxiv.org/pdf/2508.00788v1)

**Tags**: cs.CL cs.AI 



### An Investigation into Value Misalignment in LLM-Generated Texts for   Cultural Heritage
**Authors**: Fan Bu, Zheng Wang, Siyi Wang, Ziyao Liu

**Updated**: 2025-08-01T17:05:21Z

**Summary**: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.02039v3),  [pdf](http://arxiv.org/pdf/2501.02039v3)

**Tags**: cs.CL cs.AI 



### Evaluating Angle and Amplitude Encoding Strategies for Variational   Quantum Machine Learning: their impact on model's accuracy
**Authors**: Antonio Tudisco, Andrea Marchesin, Maurizio Zamboni, Mariagrazia Graziano, Giovanna Turvani

**Updated**: 2025-08-01T16:43:45Z

**Summary**: Recent advancements in Quantum Computing and Machine Learning have increased attention to Quantum Machine Learning (QML), which aims to develop machine learning models by exploiting the quantum computing paradigm. One of the widely used models in this area is the Variational Quantum Circuit (VQC), a hybrid model where the quantum circuit handles data inference while classical optimization adjusts the parameters of the circuit. The quantum circuit consists of an encoding layer, which loads data into the circuit, and a template circuit, known as the ansatz, responsible for processing the data. This work involves performing an analysis by considering both Amplitude- and Angle-encoding models, and examining how the type of rotational gate applied affects the classification performance of the model. This comparison is carried out by training the different models on two datasets, Wine and Diabetes, and evaluating their performance. The study demonstrates that, under identical model topologies, the difference in accuracy between the best and worst models ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the results highlight how the choice of rotational gates used in encoding can significantly impact the model's classification performance. The findings confirm that the embedding represents a hyperparameter for VQC models.

**Link**: [arxiv](http://arxiv.org/abs/2508.00768v1),  [pdf](http://arxiv.org/pdf/2508.00768v1)

**Tags**: cs.LG 



### ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A   Zero-Shot Approach using LLM-Driven Code Generation
**Authors**: Atakan Site, Emre Hakan Erdemir, Gülşen Eryiğit

**Updated**: 2025-08-01T16:38:18Z

**Summary**: This paper presents our system for SemEval-2025 Task 8: DataBench, Question-Answering over Tabular Data. The primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, we propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies. Our experiments reveal that different LLMs exhibit varying levels of effectiveness in Python code generation. Additionally, results show that Python code generation achieves superior performance in tabular question answering compared to alternative approaches. Although our ranking among zero-shot systems is unknown at the time of this paper's submission, our system achieved eighth place in Subtask I and sixth place in Subtask~II among the 30 systems that outperformed the baseline in the open-source models category.

**Link**: [arxiv](http://arxiv.org/abs/2508.00762v1),  [pdf](http://arxiv.org/pdf/2508.00762v1)

**Tags**: cs.CL 



### GUAVA: Generalizable Upper Body 3D Gaussian Avatar
**Authors**: Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Yang Li, Minghan Qin, Yu Li, Haoqian Wang

**Updated**: 2025-08-01T16:37:59Z

**Summary**: Reconstructing a high-quality, animatable 3D human avatar with expressive facial and hand motions from a single image has gained significant attention due to its broad application potential. 3D human avatar reconstruction typically requires multi-view or monocular videos and training on individual IDs, which is both complex and time-consuming. Furthermore, limited by SMPLX's expressiveness, these methods often focus on body motion but struggle with facial expressions. To address these challenges, we first introduce an expressive human model (EHM) to enhance facial expression capabilities and develop an accurate tracking method. Based on this template model, we propose GUAVA, the first framework for fast animatable upper-body 3D Gaussian avatar reconstruction. We leverage inverse texture mapping and projection sampling techniques to infer Ubody (upper-body) Gaussians from a single image. The rendered images are refined through a neural refiner. Experimental results demonstrate that GUAVA significantly outperforms previous methods in rendering quality and offers significant speed improvements, with reconstruction times in the sub-second range (0.1s), and supports real-time animation and rendering.

**Link**: [arxiv](http://arxiv.org/abs/2505.03351v2),  [pdf](http://arxiv.org/pdf/2505.03351v2)

**Tags**: cs.CV 



### Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs
**Authors**: Kangda Wei, Hasnat Md Abdullah, Ruihong Huang

**Updated**: 2025-08-01T16:37:16Z

**Summary**: Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data. We release the code and generated data at: https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.

**Link**: [arxiv](http://arxiv.org/abs/2505.17217v2),  [pdf](http://arxiv.org/pdf/2505.17217v2)

**Tags**: cs.CL cs.AI cs.CY 



### MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese   Hate Speech Detection under Cloaking Perturbations
**Authors**: Qiyao Xue, Yuchen Dou, Ryan Shi, Xiang Lorraine Li, Wei Gao

**Updated**: 2025-08-01T16:34:57Z

**Summary**: Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.00760v1),  [pdf](http://arxiv.org/pdf/2508.00760v1)

**Tags**: cs.CL cs.AI 



### LeakyCLIP: Extracting Training Data from CLIP
**Authors**: Yunhao Chen, Shujie Wang, Xin Wang, Xingjun Ma

**Updated**: 2025-08-01T16:32:48Z

**Summary**: Understanding the memorization and privacy leakage risks in Contrastive Language--Image Pretraining (CLIP) is critical for ensuring the security of multimodal models. Recent studies have demonstrated the feasibility of extracting sensitive training examples from diffusion models, with conditional diffusion models exhibiting a stronger tendency to memorize and leak information. In this work, we investigate data memorization and extraction risks in CLIP through the lens of CLIP inversion, a process that aims to reconstruct training images from text prompts. To this end, we introduce \textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality, semantically accurate image reconstruction from CLIP embeddings. We identify three key challenges in CLIP inversion: 1) non-robust features, 2) limited visual semantics in text embeddings, and 3) low reconstruction fidelity. To address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to enhance optimization smoothness, 2) linear transformation-based embedding alignment, and 3) Stable Diffusion-based refinement to improve fidelity. Empirical results demonstrate the superiority of LeakyCLIP, achieving over 358% improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive leakage risk, showing that training data membership can even be successfully inferred from the metrics of low-fidelity reconstructions. Our work introduces a practical method for CLIP inversion while offering novel insights into the nature and scope of privacy risks in multimodal models.

**Link**: [arxiv](http://arxiv.org/abs/2508.00756v1),  [pdf](http://arxiv.org/pdf/2508.00756v1)

**Tags**: cs.CR 



### Sound and Complete Neurosymbolic Reasoning with LLM-Grounded   Interpretations
**Authors**: Bradley P. Allen, Prateek Chhikara, Thomas Macaulay Ferguson, Filip Ilievski, Paul Groth

**Updated**: 2025-08-01T16:30:02Z

**Summary**: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neurosymbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties.

**Link**: [arxiv](http://arxiv.org/abs/2507.09751v2),  [pdf](http://arxiv.org/pdf/2507.09751v2)

**Tags**: cs.AI cs.CL cs.LO 



### Harnessing the Power of Interleaving and Counterfactual Evaluation for   Airbnb Search Ranking
**Authors**: Qing Zhang, Alex Deng, Michelle Du, Huiji Gao, Liwei He, Sanjeev Katariya

**Updated**: 2025-08-01T16:28:18Z

**Summary**: Evaluation plays a crucial role in the development of ranking algorithms on search and recommender systems. It enables online platforms to create user-friendly features that drive commercial success in a steady and effective manner. The online environment is particularly conducive to applying causal inference techniques, such as randomized controlled experiments (known as A/B test), which are often more challenging to implement in fields like medicine and public policy. However, businesses face unique challenges when it comes to effective A/B test. Specifically, achieving sufficient statistical power for conversion-based metrics can be time-consuming, especially for significant purchases like booking accommodations. While offline evaluations are quicker and more cost-effective, they often lack accuracy and are inadequate for selecting candidates for A/B test. To address these challenges, we developed interleaving and counterfactual evaluation methods to facilitate rapid online assessments for identifying the most promising candidates for A/B tests. Our approach not only increased the sensitivity of experiments by a factor of up to 100 (depending on the approach and metrics) compared to traditional A/B testing but also streamlined the experimental process. The practical insights gained from usage in production can also benefit organizations with similar interests.

**Link**: [arxiv](http://arxiv.org/abs/2508.00751v1),  [pdf](http://arxiv.org/pdf/2508.00751v1)

**Tags**: cs.IR cs.AI H.3; G.3 



### PanoLlama: Generating Endless and Coherent Panoramas with   Next-Token-Prediction LLMs
**Authors**: Teng Zhou, Xiaoyu Zhang, Yongchuan Tang

**Updated**: 2025-08-01T16:25:54Z

**Summary**: Panoramic Image Generation (PIG) aims to create coherent images of arbitrary lengths. Most existing methods fall in the joint diffusion paradigm, but their complex and heuristic crop connection designs often limit their ability to achieve multilevel coherence. By deconstructing this challenge into its core components, we find it naturally aligns with next-token prediction, leading us to adopt an autoregressive (AR) paradigm for PIG modeling. However, existing visual AR (VAR) models are limited to fixed-size generation, lacking the capability to produce panoramic images. In this paper, we propose PanoLlama, a novel framework that achieves endless and coherent panorama generation with the autoregressive paradigm. Our approach develops a training-free strategy that utilizes token redirection to overcome the size limitations of existing VAR models, enabling next-crop prediction in both horizontal and vertical directions. This refreshes the PIG pipeline while achieving SOTA performance in coherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally, PanoLlama supports applications other PIG methods cannot achieve, including mask-free layout control, multi-scale and multi-guidance synthesis. To facilitate standardized evaluation, we also establish a dataset with 1,000 prompts spanning 100+ themes, providing a new testing benchmark for PIG research. The code is available at https://github.com/0606zt/PanoLlama.

**Link**: [arxiv](http://arxiv.org/abs/2411.15867v3),  [pdf](http://arxiv.org/pdf/2411.15867v3)

**Tags**: cs.CV 



### GECO: Geometrically Consistent Embedding with Lightspeed Inference
**Authors**: Regine Hartwig, Dominik Muhle, Riccardo Marin, Daniel Cremers

**Updated**: 2025-08-01T16:21:11Z

**Summary**: Recent advances in feature learning have shown that self-supervised vision foundation models can capture semantic correspondences but often lack awareness of underlying 3D geometry. GECO addresses this gap by producing geometrically coherent features that semantically distinguish parts based on geometry (e.g., left/right eyes, front/back legs). We propose a training framework based on optimal transport, enabling supervision beyond keypoints, even under occlusions and disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2% faster than prior methods, while achieving state-of-the-art performance on PFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively. Finally, we show that PCK alone is insufficient to capture geometric quality and introduce new metrics and insights for more geometry-aware feature learning. Link to project page: https://reginehartwig.github.io/publications/geco/

**Link**: [arxiv](http://arxiv.org/abs/2508.00746v1),  [pdf](http://arxiv.org/pdf/2508.00746v1)

**Tags**: cs.CV 



### Agentic large language models improve retrieval-based radiology question   answering
**Authors**: Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald Köstler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh

**Updated**: 2025-08-01T16:18:52Z

**Summary**: Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized models (e.g., Mistral Large improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.

**Link**: [arxiv](http://arxiv.org/abs/2508.00743v1),  [pdf](http://arxiv.org/pdf/2508.00743v1)

**Tags**: cs.CL cs.AI cs.LG 



### Think Before Recommend: Unleashing the Latent Reasoning Power for   Sequential Recommendation
**Authors**: Jiakai Tang, Sunhao Dai, Teng Shi, Jun Xu, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang

**Updated**: 2025-08-01T16:17:57Z

**Summary**: Sequential Recommendation (SeqRec) aims to predict the next item by capturing sequential patterns from users' historical interactions, playing a crucial role in many real-world recommender systems. However, existing approaches predominantly adopt a direct forward computation paradigm, where the final hidden state of the sequence encoder serves as the user representation. We argue that this inference paradigm, due to its limited computational depth, struggles to model the complex evolving nature of user preferences and lacks a nuanced understanding of long-tail items, leading to suboptimal performance. To address this issue, we propose \textbf{ReaRec}, the first inference-time computing framework for recommender systems, which enhances user representations through implicit multi-step reasoning. Specifically, ReaRec autoregressively feeds the sequence's last hidden state into the sequential recommender while incorporating special reasoning position embeddings to decouple the original item encoding space from the multi-step reasoning space. Moreover, we introduce two lightweight reasoning-based learning methods, Ensemble Reasoning Learning (ERL) and Progressive Reasoning Learning (PRL), to further effectively exploit ReaRec's reasoning potential. Extensive experiments on five public real-world datasets and different SeqRec architectures demonstrate the generality and effectiveness of our proposed ReaRec. Remarkably, post-hoc analyses reveal that ReaRec significantly elevates the performance ceiling of multiple sequential recommendation backbones by approximately 30\%-50\%. Thus, we believe this work can open a new and promising avenue for future research in inference-time computing for sequential recommendation.

**Link**: [arxiv](http://arxiv.org/abs/2503.22675v3),  [pdf](http://arxiv.org/pdf/2503.22675v3)

**Tags**: cs.IR cs.AI cs.CL 



### Out-of-Context Abduction: LLMs Make Inferences About Procedural Data   Leveraging Declarative Facts in Earlier Training Data
**Authors**: Sohaib Imran, Rob Lamb, Peter M. Atkinson

**Updated**: 2025-08-01T16:12:23Z

**Summary**: Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.

**Link**: [arxiv](http://arxiv.org/abs/2508.00741v1),  [pdf](http://arxiv.org/pdf/2508.00741v1)

**Tags**: cs.CL cs.AI 



### How LLMs are Shaping the Future of Virtual Reality
**Authors**: Süeda Özkaya, Santiago Berrezueta-Guzman, Stefan Wagner

**Updated**: 2025-08-01T16:08:05Z

**Summary**: The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.00737v1),  [pdf](http://arxiv.org/pdf/2508.00737v1)

**Tags**: cs.HC cs.AI 



### A normalizing flow approach for the inference of star cluster properties   from unresolved broadband photometry I: Comparison to spectral energy   distribution fitting
**Authors**: Daniel Walter, Victor F. Ksoll, Ralf S. Klessen, Mederic Boquien, Aida Wofford, Francesco Belfiore, Daniel A. Dale, Kathryn Grasha, David A. Thilker, Leonardo Ubeda, Thomas G. Williams

**Updated**: 2025-08-01T16:06:13Z

**Summary**: Estimating properties of star clusters from unresolved broadband photometry is a challenging problem that is classically tackled by spectral energy distribution (SED) fitting methods that are based on simple stellar population (SSP) models. However, because of their exponential scaling, grid-based methods suffer from computational limitations. In addition, stochastic latent variables in the model can make the computation of the likelihood function intractable. These limitations can be overcome by modern generative deep learning methods that offer flexible and powerful tools for modeling high-dimensional posterior distributions and fast inference from learned data. We present a normalizing flow approach for the inference of cluster age, mass, and reddening from Hubble Space Telescope (HST) broadband photometry. In particular, we explore our network's behavior on an inference problem that has been analyzed in previous works. We used the SED modeling code CIGALE to create a dataset of synthetic photometric observations for $5 \times 10^6$ mock star clusters. Subsequently, this data set was used to train a coupling-based flow in the form of a conditional invertible neural network (cINN) to predict posterior probability distributions for cluster age, mass, and reddening from photometric observations. We predicted cluster parameters for the 'Physics at High Angular resolution in Nearby GalaxieS' (PHANGS) Data Release 3 catalog. To evaluate the capabilities of the network, we compared our results to the publicly available PHANGS estimates and found that the estimates agree reasonably well. We demonstrate that normalizing flow methods can be a viable tool for the inference of cluster parameters, and argue that this approach is especially useful when latent variables make the computation of the likelihood intractable and in scenarios that require efficient density estimation.

**Link**: [arxiv](http://arxiv.org/abs/2508.00736v1),  [pdf](http://arxiv.org/pdf/2508.00736v1)

**Tags**: astro-ph.GA astro-ph.IM 



### AudioGen-Omni: A Unified Multimodal Diffusion Transformer for   Video-Synchronized Audio, Speech, and Song Generation
**Authors**: Le Wang, Jun Wang, Feng Deng, Chen Zhang, Kun Gai, Di Zhang

**Updated**: 2025-08-01T16:03:57Z

**Summary**: We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and songs coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both sung and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality.

**Link**: [arxiv](http://arxiv.org/abs/2508.00733v1),  [pdf](http://arxiv.org/pdf/2508.00733v1)

**Tags**: cs.SD cs.CV cs.MM eess.AS 



### From Press to Pixels: Evolving Urdu Text Recognition
**Authors**: Samee Arif, Sualeha Farid

**Updated**: 2025-08-01T15:56:30Z

**Summary**: This paper introduces an end-to-end pipeline for Optical Character Recognition (OCR) on Urdu newspapers, addressing challenges posed by complex multi-column layouts, low-resolution scans, and the stylistic variability of the Nastaliq script. Our system comprises four modules: (1) article segmentation, (2) image super-resolution, (3) column segmentation, and (4) text recognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision for articles and 0.970 for columns. A SwinIR-based super-resolution model boosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu Newspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB and the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with modern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133. We further analyze LLM outputs via insertion, deletion, and substitution error breakdowns, as well as character-level confusion analysis. Finally, we show that fine-tuning on just 500 samples yields a 6.13% WER improvement, highlighting the adaptability of LLMs for Urdu OCR.

**Link**: [arxiv](http://arxiv.org/abs/2505.13943v2),  [pdf](http://arxiv.org/pdf/2505.13943v2)

**Tags**: cs.CV 



### Probing the Merger Rates of Supermassive Black Holes and Galaxies with   Gravitational Waves
**Authors**: Yun Fang, Rong-Gen Cai

**Updated**: 2025-08-01T15:45:13Z

**Summary**: The mergers of galaxies and supermassive black holes (SMBHs) are key drivers of galaxy evolution, contributing to the growth of both galaxies and their central black holes. Current and upcoming gravitational wave (GW) detectors -- Pulsar Timing Arrays (PTAs), LISA, Taiji, and Tianqin -- offer unique access to these processes by observing GW signals from SMBH binaries. We present a framework to infer galaxy and SMBH merger rates by combining mock LISA detections of SMBH mergers with PTA constraints on the stochastic GW background, while incorporating observational uncertainties in stellar mass functions and $M_\bullet$-$M_*$ relations. We find that the number of LISA-detected events and their joint distribution in mass and redshift are key to constraining merger rates -- datasets with around forty events yield results consistent with galaxy pair observations, whereas limited event counts lead to biases at high redshift. Including PTA data further reduces parameter uncertainties. Our method also effectively constrains the delay time between galaxy and SMBH mergers, with longer delays suppressing high-redshift SMBH merger rates and shifting mass growth from mergers to accretion. According to our mock analysis, the models with delay times longer than $0.5\text{Gyr}$ ($0.8\text{Gyr}$), accretion becomes the primary driver of SMBH mass growth beyond $z \sim 6$ ($4$). In contrast, the SMBH occupation fraction at $z>3$ remains poorly constrained due to its degeneracies with delay time and the galaxy merger rate. These findings highlight both the promise and limitations of using GW observations to probe the coevolution of galaxies and SMBHs.

**Link**: [arxiv](http://arxiv.org/abs/2501.02748v2),  [pdf](http://arxiv.org/pdf/2501.02748v2)

**Tags**: astro-ph.GA 



### Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA
**Authors**: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu

**Updated**: 2025-08-01T15:38:21Z

**Summary**: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.00719v1),  [pdf](http://arxiv.org/pdf/2508.00719v1)

**Tags**: cs.CL cs.AI 



### Better Embeddings with Coupled Adam
**Authors**: Felix Stollenwerk, Tobias Stollenwerk

**Updated**: 2025-08-01T15:28:51Z

**Summary**: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.

**Link**: [arxiv](http://arxiv.org/abs/2502.08441v3),  [pdf](http://arxiv.org/pdf/2502.08441v3)

**Tags**: cs.CL cs.AI cs.LG 



### Exploring the Evidence-Based SE Beliefs of Generative AI Tools
**Authors**: Chris Brown, Jason Cusati

**Updated**: 2025-08-01T15:27:59Z

**Summary**: Recent innovations in generative artificial intelligence (AI), primarily powered by large language models (LLMs), have transformed how programmers develop and maintain software -- leading to new frontiers in software engineering (SE). The advanced capabilities of generative AI tools to support software development tasks have led to a rise in their adoption within software development workflows. However, little is known about how AI tools perceive evidence-based beliefs and practices verified by research findings. To this end, we conduct a preliminary evaluation conceptually replicating prior work to explore the "beliefs" of generative AI tools used to support software development tasks. We investigate 17 evidence-based claims posited by empirical SE research across five generative AI tools. Our findings show that generative AI tools have ambiguous beliefs regarding research claims and lack credible evidence to support responses. Based on our results, we provide implications for practitioners integrating generative AI-based systems into development contexts and shed light on future research directions to enhance the reliability and trustworthiness of generative AI -- aiming to increase awareness and adoption of evidence-based SE research findings in practice.

**Link**: [arxiv](http://arxiv.org/abs/2407.13900v3),  [pdf](http://arxiv.org/pdf/2407.13900v3)

**Tags**: cs.SE 



### NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian   Common Law System
**Authors**: Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya

**Updated**: 2025-08-01T15:23:20Z

**Summary**: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.

**Link**: [arxiv](http://arxiv.org/abs/2508.00709v1),  [pdf](http://arxiv.org/pdf/2508.00709v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### An Online Data Analysis Framework for Accelerator-Based Physics   Experiments
**Authors**: Hayden Ramm, Pascal Simon, Paraskevi Alexaki, Christopher Arran, Robert Bingham, Alice Goillot, Jon Tomas Gudmundsson, Jonathan Halliday, Bryn Lloyd, Eva Los, Vasiliki Stergiou, Sifei Zhang, Gianluca Gregori, Nikolaos Charitonidis

**Updated**: 2025-08-01T15:20:24Z

**Summary**: A robust and flexible architecture capable of providing real-time analysis on diagnostic data in experimental physics is of crucial importance to physics experiments. In this paper, we present such an online framework, used in June 2025 as part of the HRMT-68 experiment, performed at the HiRadMat facility at CERN, using the Super Proton Synchrotron (SPS) beam line. HRMT-68 was a fixed-target laboratory astrophysics experiment aiming to identify plasma instabilities generated by a relativistic electron-positron beam during traversal of an argon plasma. This framework was essential for experimental data acquisition and analysis, and can be adapted for a broad range of experiments with a variety of experimental diagnostics. The framework's modular and customizable design enabled us to rapidly observe and extract emergent features from a diverse range of diagnostic data. Simultaneously, it allowed for both the introduction of new diagnostic devices and the modification of our analysis as features of interest were identified. As a result, we were able to effectively diagnose equipment malfunction, and infer the beam's response to varying bunch duration, beam intensity, and the plasma state without resorting to offline analysis, at which time adjustment or improvement would have been impossible. We present the features of this agile framework, whose codebase we have made publicly available, which can be adapted for future experiments with minimal modification.

**Link**: [arxiv](http://arxiv.org/abs/2508.00705v1),  [pdf](http://arxiv.org/pdf/2508.00705v1)

**Tags**: physics.acc-ph hep-ex 



### Sharp Anti-Concentration Inequalities for Extremum Statistics via   Copulas
**Authors**: Matias D. Cattaneo, Ricardo P. Masini, William G. Underwood

**Updated**: 2025-08-01T15:19:24Z

**Summary**: We derive sharp upper and lower bounds for the pointwise concentration function of the maximum statistic of $d$ identically distributed real-valued random variables. Our first main result places no restrictions either on the common marginal law of the samples or on the copula describing their joint distribution. We show that, in general, strictly sublinear dependence of the concentration function on the dimension $d$ is not possible. We then introduce a new class of copulas, namely those with a convex diagonal section, and demonstrate that restricting to this class yields a sharper upper bound on the concentration function. This allows us to establish several new dimension-independent and poly-logarithmic-in-$d$ anti-concentration inequalities for a variety of marginal distributions under mild dependence assumptions. Our theory improves upon the best known results in certain special cases. Applications to high-dimensional statistical inference are presented, including a specific example pertaining to Gaussian mixture approximations for factor models, for which our main results lead to superior distributional guarantees.

**Link**: [arxiv](http://arxiv.org/abs/2502.07699v2),  [pdf](http://arxiv.org/pdf/2502.07699v2)

**Tags**: math.ST math.PR stat.TH 60E15 (Primary) 62H05, 62G32 (Secondary) 



### Is LLM-Generated Code More Maintainable \& Reliable than Human-Written   Code?
**Authors**: Alfred Santa Molison, Marcia Moraes, Glaucia Melo, Fabio Santos, Wesley K. G. Assuncao

**Updated**: 2025-08-01T15:17:34Z

**Summary**: Background: The rise of Large Language Models (LLMs) in software development has opened new possibilities for code generation. Despite the widespread use of this technology, it remains unclear how well LLMs generate code solutions in terms of software quality and how they compare to human-written code. Aims: This study compares the internal quality attributes of LLM-generated and human-written code. Method: Our empirical study integrates datasets of coding tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and SonarQube to assess software quality. The dataset comprises Python code solutions across three difficulty levels: introductory, interview, and competition. We analyzed key code quality metrics, including maintainability and reliability, and the estimated effort required to resolve code issues. Results: Our analysis shows that LLM-generated code has fewer bugs and requires less effort to fix them overall. Interestingly, fine-tuned models reduced the prevalence of high-severity issues, such as blocker and critical bugs, and shifted them to lower-severity categories, but decreased the model's performance. In competition-level problems, the LLM solutions sometimes introduce structural issues that are not present in human-written code. Conclusion: Our findings provide valuable insights into the quality of LLM-generated code; however, the introduction of critical issues in more complex scenarios highlights the need for a systematic evaluation and validation of LLM solutions. Our work deepens the understanding of the strengths and limitations of LLMs for code generation.

**Link**: [arxiv](http://arxiv.org/abs/2508.00700v1),  [pdf](http://arxiv.org/pdf/2508.00700v1)

**Tags**: cs.SE 



### Online Rolling Controlled Sequential Monte Carlo
**Authors**: Liwen Xue, Axel Finke, Adam M. Johansen

**Updated**: 2025-08-01T15:12:58Z

**Summary**: We introduce methodology for real-time inference in general-state-space hidden Markov models. Specifically, we extend recent advances in controlled sequential Monte Carlo (CSMC) methods-originally proposed for offline smoothing-to the online setting via a rolling window mechanism. Our novel online rolling controlled sequential Monte Carlo (ORCSMC) algorithm employs two particle systems to simultaneously estimate twisting functions and perform filtering, ensuring real-time adaptivity to new observations while maintaining bounded computational cost. Numerical results on linear-Gaussian, stochastic volatility, and neuroscience models demonstrate improved estimation accuracy and robustness in higher dimensions, compared to standard particle filtering approaches. The method offers a statistically efficient and practical solution for sequential and real-time inference in complex latent variable models.

**Link**: [arxiv](http://arxiv.org/abs/2508.00696v1),  [pdf](http://arxiv.org/pdf/2508.00696v1)

**Tags**: stat.CO 



### Hierarchical Multi-Label Contrastive Learning for Protein-Protein   Interaction Prediction Across Organisms
**Authors**: Shiyi Liu, Buwen Liang, Yuetong Fang, Zixuan Jiang, Renjing Xu

**Updated**: 2025-08-01T15:04:52Z

**Summary**: Recent advances in AI for science have highlighted the power of contrastive learning in bridging heterogeneous biological data modalities. Building on this paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction across Organisms), a hierarchical contrastive framework for protein-protein interaction(PPI) prediction, where protein sequences and their hierarchical attributes are aligned through multi-tiered biological representation matching. The proposed approach incorporates hierarchical contrastive loss functions that emulate the structured relationship among functional classes of proteins. The framework adaptively incorporates domain and family knowledge through a data-driven penalty mechanism, enforcing consistency between the learned embedding space and the intrinsic hierarchy of protein functions. Experiments on benchmark datasets demonstrate that HIPPO achieves state-of-the-art performance, outperforming existing methods and showing robustness in low-data regimes. Notably, the model demonstrates strong zero-shot transferability to other species without retraining, enabling reliable PPI prediction and functional inference even in less characterized or rare organisms where experimental data are limited. Further analysis reveals that hierarchical feature fusion is critical for capturing conserved interaction determinants, such as binding motifs and functional annotations. This work advances cross-species PPI prediction and provides a unified framework for interaction prediction in scenarios with sparse or imbalanced multi-species data.

**Link**: [arxiv](http://arxiv.org/abs/2507.02724v2),  [pdf](http://arxiv.org/pdf/2507.02724v2)

**Tags**: cs.LG q-bio.BM 



### Better Call Claude: Can LLMs Detect Changes of Writing Style?
**Authors**: Johannes Römisch, Svetlana Gorovaia, Mariia Halchynska, Gleb Schmidt, Ivan P. Yamshchikov

**Updated**: 2025-08-01T14:49:50Z

**Summary**: This article explores the zero-shot performance of state-of-the-art large language models (LLMs) on one of the most challenging tasks in authorship analysis: sentence-level style change detection. Benchmarking four LLMs on the official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we present several observations. First, state-of-the-art generative models are sensitive to variations in writing style - even at the granular level of individual sentences. Second, their accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition. Finally, we explore the influence of semantics on model predictions and present evidence suggesting that the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported.

**Link**: [arxiv](http://arxiv.org/abs/2508.00680v1),  [pdf](http://arxiv.org/pdf/2508.00680v1)

**Tags**: cs.CL 



### MELAC: Massive Evaluation of Large Language Models with Alignment of   Culture in Persian Language
**Authors**: Farhan Farsi, Farnaz Aghababaloo, Shahriar Shariati Motlagh, Parsa Ghofrani, MohammadAli SadraeiJavaheri, Shayan Bali, Amirhossein Shabani, Farbod Bijary, Ghazal Zamaninejad, AmirMohammad Salehoof, Saeedeh Momtazi

**Updated**: 2025-08-01T14:46:57Z

**Summary**: As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field.

**Link**: [arxiv](http://arxiv.org/abs/2508.00673v1),  [pdf](http://arxiv.org/pdf/2508.00673v1)

**Tags**: cs.CL 



### Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement   Techniques and Applications
**Authors**: Wenxuan Wang, Zizhan Ma, Meidan Ding, Shiyi Zheng, Shengyuan Liu, Jie Liu, Jiaming Ji, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan

**Updated**: 2025-08-01T14:41:31Z

**Summary**: The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.

**Link**: [arxiv](http://arxiv.org/abs/2508.00669v1),  [pdf](http://arxiv.org/pdf/2508.00669v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG 



### Dependence of halo properties on central-satellite magnitude gaps   through weak lensing measurements
**Authors**: Mingtao Yang, Jiaxin Han, Wenting Wang, Hekun Li, Cong Liu, Jun Zhang, Shuai Feng, Shiyin Shen, Zhenjie Liu, Xiaohu Yang, Yi Lu, Surhud More

**Updated**: 2025-08-01T14:40:45Z

**Summary**: The magnitude gap between the central and satellite galaxies encodes information about the mass accretion history of a dark matter halo, and serves as a useful observational probe for the mass distribution in a halo. In this work, we perform the first weak lensing test of the connections between the magnitude gap and the halo profile. We measure the halo profiles of isolated central galaxies (ICGs) selected primarily from the SDSS Main Galaxy Sample. Halo mass and concentration are inferred by fitting stacked lensing profiles in bins of central luminosity, $L_\mathrm{c}$, and the central-satellite magnitude gap, $L_\mathrm{gap}$. We detect dependence on the magnitude gap in both halo properties. The dependence is the strongest in the ICG luminosity range of $10^{10.3}<L_\mathrm{c}[h^{-2}L_\odot]\leq 10^{10.7}$, where halos with smaller gaps have higher masses and lower concentrations. When $10^{10.7} <L_c[h^{-2}L_\odot] \leq 10^{11.1}$, however, no significant gap dependence is detected. In the range of $10^{9.9}<L_\mathrm{c}[h^{-2}L_\odot] \leq 10^{10.3}$, a disordering of the gap dependence is marginally observable. We compare the observational results with predictions by two lightcone catalogs built from the Illustris TNG300 and the Millennium simulations. The gap dependence in the two mock samples show overall consistency with observations, but neither matches them in all $L_\mathrm{c}$ bins to a quantitative level. We also compare the significance of the gap dependence on halo mass and concentration and find that our measurement prefers gap dependence in both parameters, while the halo mass dependence is preferred over the concentration if only one of the two dependencies is allowed.

**Link**: [arxiv](http://arxiv.org/abs/2508.00667v1),  [pdf](http://arxiv.org/pdf/2508.00667v1)

**Tags**: astro-ph.CO astro-ph.GA 



### IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in   LLM Writing Assistance
**Authors**: Paul Röttger, Musashi Hinck, Valentin Hofmann, Kobi Hackenburg, Valentina Pyatkin, Faeze Brahman, Dirk Hovy

**Updated**: 2025-08-01T14:26:57Z

**Summary**: Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.

**Link**: [arxiv](http://arxiv.org/abs/2502.08395v2),  [pdf](http://arxiv.org/pdf/2502.08395v2)

**Tags**: cs.CL 



### Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN
**Authors**: Abdelaziz Salama, Zeinab Nezami, Mohammed M. H. Qazzaz, Maryam Hafeez, Syed Ali Raza Zaidi

**Updated**: 2025-08-01T14:25:34Z

**Summary**: The deployment of AI agents within legacy Radio Access Network (RAN) infrastructure poses significant safety and reliability challenges for future 6G networks. This paper presents a novel Edge AI framework for autonomous network optimisation in Open RAN environments, addressing these challenges through three core innovations: (1) a persona-based multi-tools architecture enabling distributed, context-aware decision-making; (2) proactive anomaly detection agent powered by traffic predictive tool; and (3) a safety, aligned reward mechanism that balances performance with operational stability. Integrated into the RAN Intelligent Controller (RIC), our framework leverages multimodal data fusion, including network KPIs, a traffic prediction model, and external information sources, to anticipate and respond to dynamic network conditions. Extensive evaluation using realistic 5G scenarios demonstrates that the edge framework achieves zero network outages under high-stress conditions, compared to 8.4% for traditional fixed-power networks and 3.3% for large language model (LLM) agent-based approaches, while maintaining near real-time responsiveness and consistent QoS. These results establish that, when equipped with the right tools and contextual awareness, AI agents can be safely and effectively deployed in critical network infrastructure, laying the framework for intelligent and autonomous 5G and beyond network operations.

**Link**: [arxiv](http://arxiv.org/abs/2507.21696v2),  [pdf](http://arxiv.org/pdf/2507.21696v2)

**Tags**: eess.SP 



### Multi-Band Variable-Lag Granger Causality: A Unified Framework for   Causal Time Series Inference across Frequencies
**Authors**: Chakattrai Sookkongwaree, Tattep Lakmuang, Chainarong Amornbunchornvej

**Updated**: 2025-08-01T14:22:51Z

**Summary**: Understanding causal relationships in time series is fundamental to many domains, including neuroscience, economics, and behavioral science. Granger causality is one of the well-known techniques for inferring causality in time series. Typically, Granger causality frameworks have a strong fix-lag assumption between cause and effect, which is often unrealistic in complex systems. While recent work on variable-lag Granger causality (VLGC) addresses this limitation by allowing a cause to influence an effect with different time lags at each time point, it fails to account for the fact that causal interactions may vary not only in time delay but also across frequency bands. For example, in brain signals, alpha-band activity may influence another region with a shorter delay than slower delta-band oscillations. In this work, we formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a novel framework that generalizes traditional VLGC by explicitly modeling frequency-dependent causal delays. We provide a formal definition of MB-VLGC, demonstrate its theoretical soundness, and propose an efficient inference pipeline. Extensive experiments across multiple domains demonstrate that our framework significantly outperforms existing methods on both synthetic and real-world datasets, confirming its broad applicability to any type of time series data. Code and datasets are publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2508.00658v1),  [pdf](http://arxiv.org/pdf/2508.00658v1)

**Tags**: cs.AI cs.LG econ.EM stat.ME 



### Prototype Development and Calibration of the CUbesat Solar Polarimeter   (CUSP)
**Authors**: Nicolas De Angelis, Abhay Kumar, Sergio Fabiani, Ettore Del Monte, Enrico Costa, Giovanni Lombardi, Paolo Soffitta, Andrea Alimenti, Riccardo Campana, Mauro Centrone, Giovanni De Cesare, Sergio Di Cosimo, Giuseppe Di Persio, Alessandro Lacerenza, Pasqualino Loffredo, Gabriele Minervini, Fabio Muleri, Paolo Romano, Alda Rubini, Emanuele Scalise, Enrico Silva, Davide Albanesi, Ilaria Baffo, Daniele Brienza, Valerio Campamaggiore, Giovanni Cucinella, Andrea Curatolo, Giulia de Iulis, Andrea Del Re, Vito Di Bari, Simone Di Filippo, Immacolata Donnarumma, Pierluigi Fanelli, Nicolas Gagliardi, Paolo Leonetti, Matteo Merge, Dario Modenini, Andrea Negri, Daniele Pecorella, Massimo Perelli, Alice Ponti, Francesca Sbop, Paolo Tortora, Alessandro Turchi, Valerio Vagelli, Emanuele Zaccagnino, Alessandro Zambardi, Costantino Zazza

**Updated**: 2025-08-01T13:56:39Z

**Summary**: The space-based CUbesat Solar Polarimeter (CUSP) mission aims to measure the linear polarization of solar flares in the hard X-ray band by means of a Compton scattering polarimeter. CUSP will allow to study the magnetic reconnection and particle acceleration in the flaring magnetic structures of our star with its unprecedented sensitivity to solar flare polarization. CUSP is a project in the framework of the Alcor Program of the Italian Space Agency aimed to develop new CubeSat missions. It has been proposed as a constellation of a two Cubesat mission to monitor the Sun for Space Weather, and will proceed with a single-satellite asset in its baseline implementation.   In the frame of CUSP's Phase B study, that started in December 2024 for a 1-year period, we present the development status of this dual-phase polarimeter. Preliminary laboratory results using two chains of acquisition will be discussed. The first chain of acquisition, based on the Hamamatsu R7600 multi-anode photomultiplier tubes coupled to plastic scintillator bars and read out by the MAROC-3A ASIC, is used to detect the Compton scattering of incoming photons. On the other hand, GAGG crystals coupled to avalanche photo-diodes with a readout based on the SKIROC-2A ASIC are used to absorb the scattered photons. By reconstructing the azimuthal scattering direction for many incoming photons, one can infer the linear polarization degree and angle of the source. We will discuss the calibration results obtained with our prototype detector by using well-known radioactive isotopes, allowing us to assess the performances of our detector over the full 25-100 keV energy range.

**Link**: [arxiv](http://arxiv.org/abs/2508.00642v1),  [pdf](http://arxiv.org/pdf/2508.00642v1)

**Tags**: astro-ph.IM astro-ph.SR physics.space-ph 



### FedGuard: A Diverse-Byzantine-Robust Mechanism for Federated Learning   with Major Malicious Clients
**Authors**: Haocheng Jiang, Hua Shen, Jixin Zhang, Willy Susilo, Mingwu Zhang

**Updated**: 2025-08-01T13:51:25Z

**Summary**: Federated learning is a distributed training framework vulnerable to Byzantine attacks, particularly when over 50% of clients are malicious or when datasets are highly non-independent and identically distributed (non-IID). Additionally, most existing defense mechanisms are designed for specific attack types (e.g., gradient similarity-based schemes can only defend against outlier model poisoning), limiting their effectiveness. In response, we propose FedGuard, a novel federated learning mechanism. FedGuard cleverly addresses the aforementioned issues by leveraging the high sensitivity of membership inference to model bias. By requiring clients to include an additional mini-batch of server-specified data in their training, FedGuard can identify and exclude poisoned models, as their confidence in the mini-batch will drop significantly. Our comprehensive evaluation unequivocally shows that, under three highly non-IID datasets, with 90% of clients being Byzantine and seven different types of Byzantine attacks occurring in each round, FedGuard significantly outperforms existing robust federated learning schemes in mitigating various types of Byzantine attacks.

**Link**: [arxiv](http://arxiv.org/abs/2508.00636v1),  [pdf](http://arxiv.org/pdf/2508.00636v1)

**Tags**: cs.CR cs.DC 



### Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings
**Authors**: Alexia Jolicoeur-Martineau

**Updated**: 2025-08-01T13:45:13Z

**Summary**: While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.   We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.   We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.   We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.00632v1),  [pdf](http://arxiv.org/pdf/2508.00632v1)

**Tags**: cs.AI cs.MA cs.MM 



### MCeT: Behavioral Model Correctness Evaluation using Large Language   Models
**Authors**: Khaled Ahmed, Jialing Song, Boqi Chen, Ou Wei, Bingzhou Zheng

**Updated**: 2025-08-01T13:41:58Z

**Summary**: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of documentation that are typically designed by system engineers from requirements documentation, either fully manually or assisted by design tools. With the growing use of Large Language Models (LLM) as AI modeling assistants, more automation will be involved in generating diagrams. This necessitates the advancement of automatic model correctness evaluation tools. Such a tool can be used to evaluate both manually and AI automatically generated models; to provide feedback to system engineers, and enable AI assistants to self-evaluate and self-enhance their generated models.   In this paper, we propose MCeT, the first fully automated tool to evaluate the correctness of a behavioral model, sequence diagrams in particular, against its corresponding requirements text and produce a list of issues that the model has. We utilize LLMs for the correctness evaluation tasks as they have shown outstanding natural language understanding ability. However, we show that directly asking an LLM to compare a diagram to requirements finds less than 35% of issues that experienced engineers can find. We propose to supplement the direct check with a fine-grained, multi-perspective approach; we split the diagram into atomic, non-divisible interactions, and split the requirements text into atomic, self-contained items. We compare the diagram with atomic requirements and each diagram-atom with the requirements. We also propose a self-consistency checking approach that combines perspectives to mitigate LLM hallucinated issues. Our combined approach improves upon the precision of the direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover, the approach finds 90% more issues that the experienced engineers found than the direct approach, and reports an average of 6 new issues per diagram.

**Link**: [arxiv](http://arxiv.org/abs/2508.00630v1),  [pdf](http://arxiv.org/pdf/2508.00630v1)

**Tags**: cs.SE 



### Beyond Correlation: Positive Definite Dependence Measures for Robust   Inference, Flexible Scenarios, and Stress Testing for Financial Portfolios
**Authors**: JD Opdyke

**Updated**: 2025-08-01T13:40:01Z

**Summary**: We live in a multivariate world, and effective modeling of financial portfolios, including their construction, allocation, forecasting, and risk analysis, simply is not possible without explicitly modeling the dependence structure of their assets. Dependence structure can drive portfolio results more than the combined effects of other parameters in investment and risk models, but the literature provides relatively little to define the finite-sample distributions of dependence measures in useable and useful ways under challenging, real-world financial data conditions. Yet this is exactly what is needed to make valid inferences about their estimates, and to use these inferences for essential purposes such as hypothesis testing, dynamic monitoring, realistic and granular scenario and reverse scenario analyses, and mitigating the effects of correlation breakdowns during market upheavals. This work develops a new and straightforward method, Nonparametric Angles-based Correlation (NAbC), for defining the finite-sample distributions of any dependence measure whose matrix of pairwise associations is positive definite (e.g. Pearsons, Kendalls, Spearmans, Tail Dependence Matrix, and others). NAbC remains valid under marginal asset distributions with notably different and varying degrees of serial correlation, non-stationarity, heavy-tailedness, and asymmetry. NAbC provides p-values and confidence intervals at both the matrix level and the pairwise cell level, for both one and two-sample tests, with analytical consistency across levels. Finally, NAbC maintains validity even when selected cells in the matrix are frozen, thus enabling flexible, granular, and realistic scenarios and stress tests. NAbC stands alone in providing all of these capabilities simultaneously, and should prove to be a very useful means by which we can better understand and manage financial portfolios in our multivariate world

**Link**: [arxiv](http://arxiv.org/abs/2504.15268v9),  [pdf](http://arxiv.org/pdf/2504.15268v9)

**Tags**: q-fin.RM q-fin.PM q-fin.ST stat.AP 62-07, 62E20, 62F10, 62F12, 60E05, 60G70, 91B30 G.3 



### Deep Learning-Based Rate-Adaptive CSI Feedback for Wideband XL-MIMO   Systems in the Near-Field Domain
**Authors**: Zhenyu Liu, Yi Ma, Rahim Tafazolli

**Updated**: 2025-08-01T13:38:38Z

**Summary**: Accurate and efficient channel state information (CSI) feedback is crucial for unlocking the substantial spectral efficiency gains of extremely large-scale MIMO (XL-MIMO) systems in future 6G networks. However, the combination of near-field spherical wave propagation and frequency-dependent beam split effects in wideband scenarios poses significant challenges for CSI representation and compression. This paper proposes WideNLNet-CA, a rate-adaptive deep learning framework designed to enable efficient CSI feedback in wideband near-field XL-MIMO systems. WideNLNet-CA introduces a lightweight encoder-decoder architecture with multi-stage downsampling and upsampling, incorporating computationally efficient residual blocks to capture complex multi-scale channel features with reduced overhead. A novel compression ratio adaptive module with feature importance estimation is introduced to dynamically modulate feature selection based on target compression ratios, enabling flexible adaptation across a wide range of feedback rates using a single model. Evaluation results demonstrate that WideNLNet-CA consistently outperforms existing compressive sensing and deep learning-based works across various compression ratios and bandwidths, while maintaining fast inference and low model storage requirements.

**Link**: [arxiv](http://arxiv.org/abs/2508.00626v1),  [pdf](http://arxiv.org/pdf/2508.00626v1)

**Tags**: cs.IT eess.SP math.IT 



### DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language   Models
**Authors**: Shantanu Thorat, Andrew Caines

**Updated**: 2025-08-01T13:28:01Z

**Summary**: Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. We rigorously examine the machine-learning procedure to build these detectors to address this. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example. In response, we introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations. We also include texts from domain-specific continued-pre-trained (CPT) language models, where we fully train all parameters using a memory-efficient optimization approach. Many existing AIG text detectors struggle significantly on our dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. We also train our own classifiers using two approaches: standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO). While BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. In our mock deployment scenario in student essay detection with an OOD student essay dataset, the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates for both. Our results indicate that DXO classifiers generalize better without overfitting to the test set. Our experiments highlight several areas of improvement for AIG text detectors.

**Link**: [arxiv](http://arxiv.org/abs/2508.00619v1),  [pdf](http://arxiv.org/pdf/2508.00619v1)

**Tags**: cs.CL cs.LG 



### Constructive Disintegration and Conditional Modes
**Authors**: Nathaël Da Costa, Marvin Pförtner, Jon Cockayne

**Updated**: 2025-08-01T13:25:59Z

**Summary**: Conditioning, the central operation in Bayesian statistics, is formalised by the notion of disintegration of measures. However, due to the implicit nature of their definition, constructing disintegrations is often difficult. A folklore result in machine learning conflates the construction of a disintegration with the restriction of probability density functions onto the subset of events that are consistent with a given observation. We provide a comprehensive set of mathematical tools which can be used to construct disintegrations and apply these to find densities of disintegrations on differentiable manifolds. Using our results, we provide a disturbingly simple example in which the restricted density and the disintegration density drastically disagree. Motivated by applications in approximate Bayesian inference and Bayesian inverse problems, we further study the modes of disintegrations. We show that the recently introduced notion of a "conditional mode" does not coincide in general with the modes of the conditional measure obtained through disintegration, but rather the modes of the restricted measure. We also discuss the implications of the discrepancy between the two measures in practice, advocating for the utility of both approaches depending on the modelling context.

**Link**: [arxiv](http://arxiv.org/abs/2508.00617v1),  [pdf](http://arxiv.org/pdf/2508.00617v1)

**Tags**: math.ST cs.LG math.PR stat.ML stat.TH 



### Prompting Science Report 3: I'll pay you or I'll kill you -- but will   you care?
**Authors**: Lennart Meincke, Ethan Mollick, Lilach Mollick, Dan Shapiro

**Updated**: 2025-08-01T13:23:21Z

**Summary**: This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).   We demonstrate two things:   - Threatening or tipping a model generally has no significant effect on benchmark performance.   - Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLM's ability to answer any particular question.   Taken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.

**Link**: [arxiv](http://arxiv.org/abs/2508.00614v1),  [pdf](http://arxiv.org/pdf/2508.00614v1)

**Tags**: cs.CL cs.AI 



### SEFL: Enhancing Educational Assignment Feedback with LLM Agents
**Authors**: Mike Zhang, Amalie Pernille Dilling, Léon Gondelman, Niels Erik Ruan Lyngdorf, Euan D. Lindsay, Johannes Bjerva

**Updated**: 2025-08-01T13:19:46Z

**Summary**: Providing high-quality feedback to student assignments is crucial for student success, but it is constrained by time and costs. In this work, we introduce Synthetic Educational Feedback Loops (SEFL), a synthetic data framework designed to generate data that resembles immediate, on-demand feedback at scale without relying on extensive, real-world student assignments. To get this type of data, two large language models (LLMs) operate in teacher-student roles to simulate assignment completion and formative feedback, generating synthetic pairs of student work and corresponding critiques and actionable improvements from a teacher. With this data, we fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-student assignment feedback loop in higher education. Through comprehensive evaluations with four LLM judges and three human experts, we demonstrate that SEFL-tuned models outperform both their non-tuned counterparts in feedback quality and an existing baseline. The potential for societal impact is reinforced by extensive qualitative comments by ratings by human stakeholders -- both students and higher education instructors. All in all, SEFL has substantial potential to transform feedback processes for higher education and beyond.

**Link**: [arxiv](http://arxiv.org/abs/2502.12927v2),  [pdf](http://arxiv.org/pdf/2502.12927v2)

**Tags**: cs.CL 



### Rapid cosmological inference with the two-loop matter power spectrum
**Authors**: Thomas Bakx, Henrique Rubira, Nora Elisa Chisari, Zvonimir Vlah

**Updated**: 2025-08-01T13:14:41Z

**Summary**: We compute the two-loop effective field theory (EFT) power spectrum of dark matter density fluctuations in $\Lambda$CDM using the recently proposed COBRA method (Bakx. et al, 2025). With COBRA, we are able to evaluate the two-loop matter power spectrum in $\sim 1$ millisecond at $ \sim 0.1 \%$ precision on one CPU for arbitrary redshifts and on scales where perturbation theory applies. As an application, we use the nonlinear matter power spectrum from the Dark Sky simulation to assess the performance of the two-loop EFT power spectrum compared to the one-loop EFT power spectrum at $z=0$. We find that, for volumes typical for Stage IV galaxy surveys, $V = 25 \,(\text{Gpc}/h)^3$, the two-loop EFT can provide unbiased cosmological constraints on $\Omega_m,H_0$ and $A_s$ using scales up to $k_\text{max}=0.26\, h/\text{Mpc}$, thereby outperforming the constraints from the one-loop EFT ($k_\text{max}=0.11\, h/\text{Mpc}$). The Figure of Merit on these three parameters increases by a factor $\sim 1.9$ and the one-dimensional marginalized constraints improve by $\sim35\%$ for $\Omega_m$, $\sim20\%$ for $H_0$ and $\sim 15\%$ for $A_s$.

**Link**: [arxiv](http://arxiv.org/abs/2508.00611v1),  [pdf](http://arxiv.org/pdf/2508.00611v1)

**Tags**: astro-ph.CO 



### Composable OS Kernel Architectures for Autonomous Intelligence
**Authors**: Rajpreet Singh, Vidhi Kothari

**Updated**: 2025-08-01T13:07:16Z

**Summary**: As intelligent systems permeate edge devices, cloud infrastructure, and embedded real-time environments, this research proposes a new OS kernel architecture for intelligent systems, transforming kernels from static resource managers to adaptive, AI-integrated platforms. Key contributions include: (1) treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for fast sensory and cognitive processing in kernel space; (2) expanding the Linux kernel into an AI-native environment with built-in deep learning inference, floating-point acceleration, and real-time adaptive scheduling for efficient ML workloads; and (3) introducing a Neurosymbolic kernel design leveraging Category Theory and Homotopy Type Theory to unify symbolic reasoning and differentiable logic within OS internals. Together, these approaches enable operating systems to proactively anticipate and adapt to the cognitive needs of autonomous intelligent applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.00604v1),  [pdf](http://arxiv.org/pdf/2508.00604v1)

**Tags**: cs.OS cs.AI 



### Flow Matching Policy Gradients
**Authors**: David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, Angjoo Kanazawa

**Updated**: 2025-08-01T13:04:28Z

**Summary**: Flow-based generative models, including diffusion models, excel at modeling continuous distributions in high-dimensional spaces. In this work, we introduce Flow Policy Optimization (FPO), a simple on-policy reinforcement learning algorithm that brings flow matching into the policy gradient framework. FPO casts policy optimization as maximizing an advantage-weighted ratio computed from the conditional flow matching loss, in a manner compatible with the popular PPO-clip framework. It sidesteps the need for exact likelihood computation while preserving the generative capabilities of flow-based models. Unlike prior approaches for diffusion-based reinforcement learning that bind training to a specific sampling method, FPO is agnostic to the choice of diffusion or flow integration at both training and inference time. We show that FPO can train diffusion-style policies from scratch in a variety of continuous control tasks. We find that flow-based models can capture multimodal action distributions and achieve higher performance than Gaussian policies, particularly in under-conditioned settings.

**Link**: [arxiv](http://arxiv.org/abs/2507.21053v2),  [pdf](http://arxiv.org/pdf/2507.21053v2)

**Tags**: cs.LG cs.RO 



### LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection   and Leakage Attacks
**Authors**: Francesco Panebianco, Stefano Bonfanti, Francesco Trovò, Michele Carminati

**Updated**: 2025-08-01T13:04:28Z

**Summary**: The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard.

**Link**: [arxiv](http://arxiv.org/abs/2508.00602v1),  [pdf](http://arxiv.org/pdf/2508.00602v1)

**Tags**: cs.CR cs.AI cs.LG 



### Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback
**Authors**: Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen

**Updated**: 2025-08-01T13:03:05Z

**Summary**: Time series anomaly detection is critical across various domains, yet current approaches often limit analysis to mere binary anomaly classification without detailed categorization or further explanatory reasoning. To address these limitations, we propose a novel task, Time-series Reasoning for Anomaly (Time-RA) that transforms classical time series anomaly detection from a discriminative into a generative, reasoning-intensive task leveraging Large Language Models (LLMs). Also, we introduce the first real-world multimodal benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning, comprising approximately 40,000 samples across 10 real-world domains. Each sample includes numeric time series data, contextual text information, and visual representations, each annotated with fine-grained categories (14 types for univariate anomalies and 6 for multivariate anomalies) and structured explanatory reasoning. We develop a sophisticated annotation framework utilizing ensemble-generated labels refined through GPT-4-driven feedback, ensuring accuracy and interpretability. Extensive benchmarking of LLMs and multimodal LLMs demonstrates the capabilities and limitations of current models, highlighting the critical role of supervised fine-tuning. Our dataset and task pave the way for significant advancements in interpretable time series anomaly detection and reasoning. The code (https://github.com/yyysjz1997/Time-RA) and dataset (https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced to support and accelerate future research in this area.

**Link**: [arxiv](http://arxiv.org/abs/2507.15066v2),  [pdf](http://arxiv.org/pdf/2507.15066v2)

**Tags**: cs.LG cs.AI cs.MM 



### A Context-Aware Dual-Metric Framework for Confidence Estimation in Large   Language Models
**Authors**: Mingruo Yuan, Shuyi Zhang, Ben Kao

**Updated**: 2025-08-01T12:58:34Z

**Summary**: Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines.

**Link**: [arxiv](http://arxiv.org/abs/2508.00600v1),  [pdf](http://arxiv.org/pdf/2508.00600v1)

**Tags**: cs.CL cs.LG 



### How Far Are AI Scientists from Changing the World?
**Authors**: Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, Jian Wu, Yue Zhang

**Updated**: 2025-08-01T12:49:36Z

**Summary**: The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now taking the lead in scientific research. Several influential works have already appeared in the field of AI Scientist systems, with AI-generated research papers having been accepted at the ICLR 2025 workshop, suggesting that a human-level AI Scientist capable of uncovering phenomena previously unknown to humans, may soon become a reality. In this survey, we focus on the central question: How far are AI scientists from changing the world and reshaping the scientific research paradigm? To answer this question, we provide a prospect-driven review that comprehensively analyzes the current achievements of AI Scientist systems, identifying key bottlenecks and the critical components required for the emergence of a scientific agent capable of producing ground-breaking discoveries that solve grand challenges. We hope this survey will contribute to a clearer understanding of limitations of current AI Scientist systems, showing where we are, what is missing, and what the ultimate goals for scientific AI should be.

**Link**: [arxiv](http://arxiv.org/abs/2507.23276v2),  [pdf](http://arxiv.org/pdf/2507.23276v2)

**Tags**: cs.AI 



### Output-recurrent gated state space model for multiphase flows modeling   and uncertainty quantification of exhaust vehicles
**Authors**: Ruilin Chen, Ming Fang, Guihui Ma

**Updated**: 2025-08-01T12:41:40Z

**Summary**: This paper presents an Output-Recurrent Gated State Space Model (OR-GSSM) for complex multiphase flows modeling and uncertainty quantification of exhaust vehicles during motion. By establishing the state-space formulation of the gas-liquid Navier-Stokes equations applying semigroup theory and Galerkin projection, explicitly characterizing the dynamic coupling evolution between the velocity, pressure, and volume fraction fields. A novel Gated State Space Transition (GSST) unit is designed to learn parameterized transition and input matrices with adaptive timescales, enhancing physical interpretability and computational efficiency. The output recursion mechanism aligns with the numerical solution characteristics of state-space equations, mitigating long-term error accumulation and addressing training-inference pattern mismatch issues inherent in teacher forcing and scheduled sampling. Validations on the underwater cone-head and water-exit hemisphere-head vehicles demonstrate that: OR-GSSM outperforms OR-ConvLSTM and OR-ConvGRU baselines in accuracy and computational efficiency through its physics-informed adaptive state-space unit design and parallel matrix operations; The output recursion mechanism ensures more stable training, better generalization, and higher prediction accuracy than teacher forcing and scheduled sampling; OR-GSSM accurately captures the gas-phase expansion, gas-liquid mixing formation, backflow jet generation, bubble shedding, and entire water-exit process, etc, showcasing outstanding modeling capability; Its uncertainty quantification effectively characterizes flow features and uncertainty distributions, validating prediction reliability. The proposed method resolves the accuracy-real-time trade-off in traditional computational fluid dynamics, advancing machine learning for multiphase flow modeling and uncertainty quantification in exhaust vehicles.

**Link**: [arxiv](http://arxiv.org/abs/2508.00588v1),  [pdf](http://arxiv.org/pdf/2508.00588v1)

**Tags**: physics.flu-dyn 



### From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated   Pre-Consultation Questionnaire Generation
**Authors**: Ruiqing Ding, Qianfang Sun, Yongkang Leng, Hui Yin, Xiaojian Li

**Updated**: 2025-08-01T12:24:49Z

**Summary**: Pre-consultation is a critical component of effective healthcare delivery. However, generating comprehensive pre-consultation questionnaires from complex, voluminous Electronic Medical Records (EMRs) is a challenging task. Direct Large Language Model (LLM) approaches face difficulties in this task, particularly regarding information completeness, logical order, and disease-level synthesis. To address this issue, we propose a novel multi-stage LLM-driven framework: Stage 1 extracts atomic assertions (key facts with timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes disease knowledge by clustering representative networks from an EMR corpus; Stage 3 generates tailored personal and standardized disease-specific questionnaires based on these structured representations. This framework overcomes limitations of direct methods by building explicit clinical knowledge. Evaluated on a real-world EMR dataset and validated by clinical experts, our method demonstrates superior performance in information coverage, diagnostic relevance, understandability, and generation time, highlighting its practical potential to enhance patient information collection.

**Link**: [arxiv](http://arxiv.org/abs/2508.00581v1),  [pdf](http://arxiv.org/pdf/2508.00581v1)

**Tags**: cs.AI 



### Large sample scaling analysis of the Zig-Zag algorithm for Bayesian   inference
**Authors**: Sanket Agrawal, Joris Bierkens, Gareth O. Roberts

**Updated**: 2025-08-01T12:24:01Z

**Summary**: Piecewise deterministic Markov processes provide scalable methods for sampling from the posterior distributions in big data settings by admitting principled sub-sampling strategies that do not bias the output. An important example is the Zig-Zag process of [Ann. Stats. 47 (2019) 1288 - 1320] where clever sub-sampling has been shown to produce an essentially independent sample at a cost that does not scale with the size of the data. However, sub-sampling also leads to slower convergence and poor mixing of the process, a behaviour which questions the promised scalability of the algorithm. We provide a large sample scaling analysis of the Zig-Zag process and its sub-sampling versions in settings of parametric Bayesian inference. In the transient phase of the algorithm, we show that the Zig-Zag trajectories are well approximated by the solution to a system of ODEs. These ODEs possess a drift in the direction of decreasing KL-divergence between the assumed model and the true distribution and are explicitly characterized in the paper. In the stationary phase, we give weak convergence results for different versions of the Zig-Zag process. Based on our results, we estimate that for large data sets of size n, using suitable control variates with sub-sampling in Zig-Zag, the algorithm costs O(1) to obtain an essentially independent sample; a computational speed-up of O(n) over the canonical version of Zig-Zag and other traditional MCMC methods

**Link**: [arxiv](http://arxiv.org/abs/2411.14983v2),  [pdf](http://arxiv.org/pdf/2411.14983v2)

**Tags**: stat.CO 62-08, 60F05, 62F15, 65C05 



### OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on   Planetary Rovers Using RGB, Depth, and Thermal Imagery
**Authors**: Raul Castilla-Arquillo, Carlos Perez-del-Pulgar, Levin Gerdes, Alfonso Garcia-Cerezo, Miguel A. Olivares-Mendez

**Updated**: 2025-08-01T12:23:29Z

**Summary**: Robot navigation in unstructured environments requires multimodal perception systems that can support safe navigation. Multimodality enables the integration of complementary information collected by different sensors. However, this information must be processed by machine learning algorithms specifically designed to leverage heterogeneous data. Furthermore, it is necessary to identify which sensor modalities are most informative for navigation in the target environment. In Martian exploration, thermal imagery has proven valuable for assessing terrain safety due to differences in thermal behaviour between soil types. This work presents OmniUnet, a transformer-based neural network architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T) imagery. A custom multimodal sensor housing was developed using 3D printing and mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a multimodal dataset in the Bardenas semi-desert in northern Spain. This location serves as a representative environment of the Martian surface, featuring terrain types such as sand, bedrock, and compact soil. A subset of this dataset was manually labeled to support supervised training of the network. The model was evaluated both quantitatively and qualitatively, achieving a pixel accuracy of 80.37% and demonstrating strong performance in segmenting complex unstructured terrain. Inference tests yielded an average prediction time of 673 ms on a resource-constrained computer (Jetson Orin Nano), confirming its suitability for on-robot deployment. The software implementation of the network and the labeled dataset have been made publicly available to support future research in multimodal terrain perception for planetary robotics.

**Link**: [arxiv](http://arxiv.org/abs/2508.00580v1),  [pdf](http://arxiv.org/pdf/2508.00580v1)

**Tags**: cs.RO cs.AI 



### MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for   Document Question-Answering with Hierarchical Index and Multi-Granularity   Retrieval
**Authors**: Ziyu Gong, Yihua Huang, Chengcheng Mai

**Updated**: 2025-08-01T12:22:53Z

**Summary**: The multi-modal long-context document question-answering task aims to locate and integrate multi-modal evidences (such as texts, tables, charts, images, and layouts) distributed across multiple pages, for question understanding and answer generation. The existing methods can be categorized into Large Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation (RAG)-based methods. However, the former were susceptible to hallucinations, while the latter struggled for inter-modal disconnection and cross-page fragmentation. To address these challenges, a novel multi-modal RAG model, named MMRAG-DocQA, was proposed, leveraging both textual and visual information across long-range pages to facilitate accurate question answering. A hierarchical indexing method with the integration of flattened in-page chunks and topological cross-page chunks was designed to jointly establish in-page multi-modal associations and long-distance cross-page dependencies. By means of joint similarity evaluation and large language model (LLM)-based re-ranking, a multi-granularity semantic retrieval method, including the page-level parent page retrieval and document-level summary retrieval, was proposed to foster multi-modal evidence connection and long-distance evidence integration and reasoning. Experimental results performed on public datasets, MMLongBench-Doc and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in understanding and answering modality-rich and multi-page documents.

**Link**: [arxiv](http://arxiv.org/abs/2508.00579v1),  [pdf](http://arxiv.org/pdf/2508.00579v1)

**Tags**: cs.MM cs.IR 



### Embracing Large Language Models in Traffic Flow Forecasting
**Authors**: Yusheng Zhao, Xiao Luo, Haomin Wen, Zhiping Xiao, Wei Ju, Ming Zhang

**Updated**: 2025-08-01T12:20:02Z

**Summary**: Traffic flow forecasting aims to predict future traffic flows based on the historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods been proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes of traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures respectively. The two branches are first pre-trained individually, and during test-time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of the proposed LEAF.

**Link**: [arxiv](http://arxiv.org/abs/2412.12201v2),  [pdf](http://arxiv.org/pdf/2412.12201v2)

**Tags**: cs.LG cs.AI 



### SynAdapt: Learning Adaptive Reasoning in Large Language Models via   Synthetic Continuous Chain-of-Thought
**Authors**: Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, Ziqian Zeng

**Updated**: 2025-08-01T12:17:35Z

**Summary**: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off.

**Link**: [arxiv](http://arxiv.org/abs/2508.00574v1),  [pdf](http://arxiv.org/pdf/2508.00574v1)

**Tags**: cs.CL cs.AI 



### Approximating optimal SMC proposal distributions in individual-based   epidemic models
**Authors**: Lorenzo Rimella, Christopher Jewell, Paul Fearnhead

**Updated**: 2025-08-01T12:11:18Z

**Summary**: Many epidemic models are naturally defined as individual-based models: where we track the state of each individual within a susceptible population. Inference for individual-based models is challenging due to the high-dimensional state-space of such models, which increases exponentially with population size. We consider sequential Monte Carlo algorithms for inference for individual-based epidemic models where we make direct observations of the state of a sample of individuals. Standard implementations, such as the bootstrap filter or the auxiliary particle filter are inefficient due to mismatch between the proposal distribution of the state and future observations. We develop new efficient proposal distributions that take account of future observations, leveraging the properties that (i) we can analytically calculate the optimal proposal distribution for a single individual given future observations and the future infection rate of that individual; and (ii) the dynamics of individuals are independent if we condition on their infection rates. Thus we construct estimates of the future infection rate for each individual, and then use an independent proposal for the state of each individual given this estimate. Empirical results show order of magnitude improvement in efficiency of the sequential Monte Carlo sampler for both SIS and SEIR models.

**Link**: [arxiv](http://arxiv.org/abs/2206.05161v4),  [pdf](http://arxiv.org/pdf/2206.05161v4)

**Tags**: stat.ME 



### Session-Based Recommendation with Validated and Enriched LLM Intents
**Authors**: Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang

**Updated**: 2025-08-01T12:11:10Z

**Summary**: Session-based recommendation (SBR) aims to predict the next item for an anonymous user in a timely manner. However, SBR suffers from data sparsity due to the short and anonymous nature of sessions. Recently, an emerging line of work has explored inferring the underlying user intents of a session using large language models (LLMs), with the generated intents serving as auxiliary training signals to enhance SBR models. Despite its promise, this approach faces three key challenges: validating intent quality, incorporating session-level multi-intents, and complementing inevitable LLM failure cases. In this paper, we propose VELI4SBR, a two-stage framework that leverages Validated and Enriched LLM-generated Intents for SBR. In the first stage, we generate high-quality intents using a predict-and-correct loop that validates the informativeness of LLM-generated intents with a global intent pool to constrain the LLM's output space and reduce hallucination. In the second stage, we enhance the SBR model using the generated intents through a lightweight multi-intent prediction and fusion mechanism. Furthermore, we introduce a training strategy that compensates for LLM failures by inferring intents from inter-session behavioral similarities. Extensive experiments show that VELI4SBR outperforms state-of-the-art baselines while improving explainability.

**Link**: [arxiv](http://arxiv.org/abs/2508.00570v1),  [pdf](http://arxiv.org/pdf/2508.00570v1)

**Tags**: cs.IR 



### ContestTrade: A Multi-Agent Trading System Based on Internal Contest   Mechanism
**Authors**: Li Zhao, Rui Sun, Zuoyou Jiang, Bo Yang, Yuxiao Bai, Mengting Chen, Xinyang Wang, Jing Li, Zuo Bai

**Updated**: 2025-08-01T11:48:13Z

**Summary**: In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multiagent systems and traditional quantitative investment methods across diverse evaluation metrics.

**Link**: [arxiv](http://arxiv.org/abs/2508.00554v1),  [pdf](http://arxiv.org/pdf/2508.00554v1)

**Tags**: q-fin.TR cs.CL q-fin.CP 



### HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention   in Vision-Language Models
**Authors**: Jizhihui Liu, Feiyi Du, Guangdao Zhu, Niu Lian, Jun Li, Bin Chen

**Updated**: 2025-08-01T11:48:11Z

**Summary**: Vision-Language Models (VLMs) encode images into lengthy sequences of visual tokens, leading to excessive computational overhead and limited inference efficiency. While prior efforts prune or merge tokens to address this issue, they often rely on special tokens (e.g., CLS) or require task-specific training, hindering scalability across architectures. In this paper, we propose HiPrune, a training-free and model-agnostic token Pruning framework that exploits the Hierarchical attention structure within vision encoders. We identify that middle layers attend to object-centric regions, while deep layers capture global contextual features. Based on this observation, HiPrune selects three types of informative tokens: (1) Anchor tokens with high attention in object-centric layers, (2) Buffer tokens adjacent to anchors for spatial continuity, and (3) Register tokens with strong attention in deep layers for global summarization. Our method requires no retraining and integrates seamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5, LLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art pruning performance, preserving up to 99.3% task accuracy with only 33.3% tokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it reduces inference FLOPs and latency by up to 9$\times$, showcasing strong generalization across models and tasks. Code is available at https://github.com/Danielement321/HiPrune.

**Link**: [arxiv](http://arxiv.org/abs/2508.00553v1),  [pdf](http://arxiv.org/pdf/2508.00553v1)

**Tags**: cs.CV 



### DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable   Adversarial Purification
**Authors**: Chihan Huang, Belal Alsinglawi, Islam Al-qudah

**Updated**: 2025-08-01T11:47:36Z

**Summary**: Recent advances in deep neural networks (DNNs) have led to remarkable success across a wide range of tasks. However, their susceptibility to adversarial perturbations remains a critical vulnerability. Existing diffusion-based adversarial purification methods often require intensive iterative denoising, severely limiting their practical deployment. In this paper, we propose Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient diffusion-based framework for adversarial purification. Central to our approach is a new objective, noise bridge distillation, which constructs a principled alignment between the adversarial noise distribution and the clean data distribution within a latent consistency model (LCM). To further enhance semantic fidelity, we introduce adaptive semantic enhancement, which fuses multi-scale pyramid edge maps as conditioning input to guide the purification process. Extensive experiments across multiple datasets demonstrate that DBLP achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and around 0.2s inference time, marking a significant step toward real-time adversarial purification.

**Link**: [arxiv](http://arxiv.org/abs/2508.00552v1),  [pdf](http://arxiv.org/pdf/2508.00552v1)

**Tags**: cs.CV 



### Video Color Grading via Look-Up Table Generation
**Authors**: Seunghyun Shin, Dongmin Shin, Jisu Shin, Hae-Gon Jeon, Joon-Young Lee

**Updated**: 2025-08-01T11:43:30Z

**Summary**: Different from color correction and transfer, color grading involves adjusting colors for artistic or storytelling purposes in a video, which is used to establish a specific look or mood. However, due to the complexity of the process and the need for specialized editing skills, video color grading remains primarily the domain of professional colorists. In this paper, we present a reference-based video color grading framework. Our key idea is explicitly generating a look-up table (LUT) for color attribute alignment between reference scenes and input video via a diffusion model. As a training objective, we enforce that high-level features of the reference scenes like look, mood, and emotion should be similar to that of the input video. Our LUT-based approach allows for color grading without any loss of structural details in the whole video frames as well as achieving fast inference. We further build a pipeline to incorporate a user-preference via text prompts for low-level feature enhancement such as contrast and brightness, etc. Experimental results, including extensive user studies, demonstrate the effectiveness of our approach for video color grading. Codes are publicly available at https://github.com/seunghyuns98/VideoColorGrading.

**Link**: [arxiv](http://arxiv.org/abs/2508.00548v1),  [pdf](http://arxiv.org/pdf/2508.00548v1)

**Tags**: cs.CV 



### SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval
**Authors**: Wenchao Gu, Zongyi Lyu, Yanlin Wang, Hongyu Zhang, Cuiyun Gao, Michael R. Lyu

**Updated**: 2025-08-01T11:39:32Z

**Summary**: Code retrieval aims to provide users with desired code snippets based on users' natural language queries. With the development of deep learning technologies, adopting pre-trained models for this task has become mainstream. Considering the retrieval efficiency, most of the previous approaches adopt a dual-encoder for this task, which encodes the description and code snippet into representation vectors, respectively. However, the model structure of the dual-encoder tends to limit the model's performance, since it lacks the interaction between the code snippet and description at the bottom layer of the model during training. To improve the model's effectiveness while preserving its efficiency, we propose a framework, which adopts Self-AdaPtive Model Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts the dual-encoder to narrow the search space and then adopts the cross-encoder to improve accuracy. To improve the efficiency of SPENCER, we propose a novel model distillation technique, which can greatly reduce the inference time of the dual-encoder while maintaining the overall performance. We also propose a teaching assistant selection strategy for our model distillation, which can adaptively select the suitable teaching assistant models for different pre-trained models during the model distillation to ensure the model performance. Extensive experiments demonstrate that the combination of dual-encoder and cross-encoder improves overall performance compared to solely dual-encoder-based models for code retrieval. Besides, our model distillation technique retains over 98% of the overall performance while reducing the inference time of the dual-encoder by 70%.

**Link**: [arxiv](http://arxiv.org/abs/2508.00546v1),  [pdf](http://arxiv.org/pdf/2508.00546v1)

**Tags**: cs.SE cs.AI 



### Assessing (im)balance in signed brain networks
**Authors**: Marzio Di Vece, Emanuele Agrimi, Samuele Tatullo, Tommaso Gili, Miguel Ibáñez-Berganza, Tiziano Squartini

**Updated**: 2025-08-01T11:30:24Z

**Summary**: Many complex systems - be they financial, natural or social - are composed by units - such as stocks, neurons or agents - whose joint activity can be represented as a multivariate time series. An issue of both practical and theoretical importance concerns the possibility of inferring the presence of a static relationships between any two units solely from their dynamic state. The present contribution aims at providing an answer within the frame of traditional hypothesis testing. Briefly speaking, our suggestion is that of linking any two units if behaving in a sufficiently similar way. To achieve such a goal, we project a multivariate time series onto a signed graph, by i) comparing the empirical properties of the former with those expected under a suitable benchmark and ii) linking any two units with a positive (negative) edge in case the corresponding series share a significantly large number of concordant (discordant) values. To define our benchmarks, we adopt an information-theoretic approach that is rooted into the constrained maximisation of Shannon entropy, a procedure inducing an ensemble of multivariate time series that preserves some of the empirical properties on average while randomising everything else. We showcase the possible applications of our method by addressing one of the most timely issues in the domain of neurosciences, i.e. that of determining if brain networks are frustrated or not - and, in case, to what extent. As our results suggest, this is indeed the case, the structure of the negative subgraph being more prone to inter-subject variability than the complementary, positive subgraph. At the mesoscopic level, instead, the minimisation of the Bayesian Information Criterion instantiated with the Signed Stochastic Block Model reveals that brain areas gather into modules aligning with the statistical variant of the Relaxed Balance Theory.

**Link**: [arxiv](http://arxiv.org/abs/2508.00542v1),  [pdf](http://arxiv.org/pdf/2508.00542v1)

**Tags**: physics.soc-ph cs.IT math.IT physics.data-an physics.med-ph stat.ME 



### EPANet: Efficient Path Aggregation Network for Underwater Fish Detection
**Authors**: Jinsong Yang, Zeyuan Hu, Yichen Li

**Updated**: 2025-08-01T11:09:13Z

**Summary**: Underwater fish detection (UFD) remains a challenging task in computer vision due to low object resolution, significant background interference, and high visual similarity between targets and surroundings. Existing approaches primarily focus on local feature enhancement or incorporate complex attention mechanisms to highlight small objects, often at the cost of increased model complexity and reduced efficiency. To address these limitations, we propose an efficient path aggregation network (EPANet), which leverages complementary feature integration to achieve accurate and lightweight UFD. EPANet consists of two key components: an efficient path aggregation feature pyramid network (EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP bottleneck). The EPA-FPN introduces long-range skip connections across disparate scales to improve semantic-spatial complementarity, while cross-layer fusion paths are adopted to enhance feature integration efficiency. The MS-DDSP bottleneck extends the conventional bottleneck structure by introducing finer-grained feature division and diverse convolutional operations, thereby increasing local feature diversity and representation capacity. Extensive experiments on benchmark UFD datasets demonstrate that EPANet outperforms state-of-the-art methods in terms of detection accuracy and inference speed, while maintaining comparable or even lower parameter complexity.

**Link**: [arxiv](http://arxiv.org/abs/2508.00528v1),  [pdf](http://arxiv.org/pdf/2508.00528v1)

**Tags**: cs.CV 



### Zero & $N$-inflated overdispersed binomial models for sum-constrained   Poisson count processes
**Authors**: James Sweeney, John Haslett, Dipankar Bandyopadhyay, Michael Fop, Andrew C. Parnell

**Updated**: 2025-08-01T10:59:17Z

**Summary**: A frequent challenge encountered with compositional ecological data is how to interpret and model data with a high proportion of zeros and $N$'s. Such data frequently occur in ecological applications where counts of species are collected until a pre-specified total imposed (typically) by sampling cost is reached. In the bivariate count (two-species) setting we focus on in this article, zero-inflation of one species will result in $N$-inflation of the other. This can lead to species absence being attributed to an unsuitable habitat as opposed to missingness by chance. Similarly, an excess of $N$'s will lead to misleading inferences about habitat preference and abundance estimates. Our contribution is to identify that two independent zero-inflated Poisson processes subject to a sum constraint provide a novel biologically-motivated generating mechanism for the occurrence of binomial count data exhibiting zero and $N$-inflation. We identify an extension to the model to capture additional overdispersion within the data resulting in a novel zero and $N$-inflated beta-binomial model. We consider two motivating datasets, one involving a pesticide treatment for an invasive species, and a second involving the abundance of two plant species. We demonstrate that incorporation of covariates in each case enable learning about sources of zero and $N$-inflation as well as abundance. We show that the models result in improved understanding of underlying biological processes as well as improved predictive performance.

**Link**: [arxiv](http://arxiv.org/abs/1407.0064v5),  [pdf](http://arxiv.org/pdf/1407.0064v5)

**Tags**: stat.ME 



### A pipeline for searching and fitting instrumental glitches in LISA data
**Authors**: Martina Muratore, Jonathan Gair, Olaf Hartwig, Michael L. Katz, Alexandre Toubiana

**Updated**: 2025-08-01T10:57:42Z

**Summary**: Instrumental artefacts, such as glitches, can significantly compromise the scientific output of LISA. Our methodology employs advanced Bayesian techniques, including Reversible Jump Markov Chain Monte Carlo and parallel tempering to find and characterize glitches and astrophysical signals. The robustness of the pipeline is demonstrated through its ability to simultaneously handle diverse glitch morphologies and it is validated with a 'Spritz'-type data set from the LISA Data Challenge. Our approach enables accurate inference on Massive Black Hole Binaries, while simultaneously characterizing both instrumental artefacts and noise. These results present a significant development in strategies for differentiating between instrumental noise and astrophysical signals, which will ultimately improve the accuracy and reliability of source population analyses with LISA.

**Link**: [arxiv](http://arxiv.org/abs/2505.19870v3),  [pdf](http://arxiv.org/pdf/2505.19870v3)

**Tags**: gr-qc astro-ph.IM 



### AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through   Lightweight Vocabulary Adaptation
**Authors**: Itay Nakash, Nitay Calderon, Eyal Ben David, Elad Hoffer, Roi Reichart

**Updated**: 2025-08-01T10:53:31Z

**Summary**: Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance

**Link**: [arxiv](http://arxiv.org/abs/2503.19693v2),  [pdf](http://arxiv.org/pdf/2503.19693v2)

**Tags**: cs.CL 



### From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in   Jailbreak Attacks and Defenses within LLM Ecosystem
**Authors**: Yanxu Mao, Tiehan Cui, Peipei Liu, Datao You, Hongsong Zhu

**Updated**: 2025-08-01T10:42:39Z

**Summary**: Large language models (LLMs) are rapidly evolving from single-modal systems to multimodal LLMs and intelligent agents, significantly expanding their capabilities while introducing increasingly severe security risks. This paper presents a systematic survey of the growing complexity of jailbreak attacks and corresponding defense mechanisms within the expanding LLM ecosystem. We first trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting the core security challenges emerging at each stage. Next, we categorize mainstream jailbreak techniques from both the attack impact and visibility perspectives, and provide a comprehensive analysis of representative attack methods, related datasets, and evaluation metrics. On the defense side, we organize existing strategies based on response timing and technical approach, offering a structured understanding of their applicability and implementation. Furthermore, we identify key limitations in existing surveys, such as insufficient attention to agent-specific security issues, the absence of a clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of experimental setups, and outdated coverage of recent advancements. To address these limitations, we provide an updated synthesis of recent work and outline future research directions in areas such as dataset construction, evaluation framework optimization, and strategy generalization. Our study seeks to enhance the understanding of jailbreak mechanisms and facilitate the advancement of more resilient and adaptive defense strategies in the context of ever more capable LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.15170v3),  [pdf](http://arxiv.org/pdf/2506.15170v3)

**Tags**: cs.CR 



### Gradient Leakage Defense with Key-Lock Module for Federated Learning
**Authors**: Hanchi Ren, Jingjing Deng, Xianghua Xie

**Updated**: 2025-08-01T10:37:44Z

**Summary**: Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is infeasible; and b) the global model's inference performance is significantly compromised. We discuss the theoretical underpinnings of why gradients can leak private information and provide theoretical proof of our method's effectiveness. We conducted extensive empirical evaluations with many models on several popular benchmarks, demonstrating the robustness of our proposed approach in both maintaining model performance and defending against gradient leakage attacks.

**Link**: [arxiv](http://arxiv.org/abs/2305.04095v3),  [pdf](http://arxiv.org/pdf/2305.04095v3)

**Tags**: cs.LG cs.AI cs.CV 



### Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration   for Text-Attributed Graph Anomaly Detection
**Authors**: Yiming Xu, Jiarun Chen, Zhen Peng, Zihan Chen, Qika Lin, Lan Ma, Bin Shi, Bo Dong

**Updated**: 2025-08-01T10:36:39Z

**Summary**: The natural combination of intricate topological structures and rich textual information in text-attributed graphs (TAGs) opens up a novel perspective for graph anomaly detection (GAD). However, existing GAD methods primarily focus on designing complex optimization objectives within the graph domain, overlooking the complementary value of the textual modality, whose features are often encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so that semantic context related to anomalies may be missed. To unleash the enormous potential of textual modality, large language models (LLMs) have emerged as promising alternatives due to their strong semantic understanding and reasoning capabilities. Nevertheless, their application to TAG anomaly detection remains nascent, and they struggle to encode high-order structural information inherent in graphs due to input length constraints. For high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that combines LLMs and graph neural networks (GNNs) to leverage their complementary strengths. CoLL employs multi-LLM collaboration for evidence-augmented generation to capture anomaly-relevant contexts while delivering human-readable rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped with a gating mechanism to adaptively fuse textual features with evidence while preserving high-order topological information. Extensive experiments demonstrate the superiority of CoLL, achieving an average improvement of 13.37% in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

**Link**: [arxiv](http://arxiv.org/abs/2508.00507v1),  [pdf](http://arxiv.org/pdf/2508.00507v1)

**Tags**: cs.LG 



### Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team
**Authors**: Weilun Yu, Shixiang Tang, Yonggui Huang, Nanqing Dong, Li Fan, Honggang Qi, Wei Liu, Xiaoli Diao, Xi Chen, Wanli Ouyang

**Updated**: 2025-08-01T10:35:50Z

**Summary**: Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18348v3),  [pdf](http://arxiv.org/pdf/2506.18348v3)

**Tags**: cs.AI 



### Invalid proxies and volatility changes
**Authors**: Giovanni Angelini, Luca Fanelli, Luca Neri

**Updated**: 2025-08-01T10:25:54Z

**Summary**: When in proxy-SVARs the covariance matrix of VAR disturbances is subject to exogenous, permanent breaks that cause IRFs to change across volatility regimes, even strong, exogenous external instruments yield inconsistent estimates of the dynamic causal effects. However, if these volatility shifts are properly incorporated into the analysis through (testable) "stability restrictions", we demonstrate that the target IRFs are point-identified and can be estimated consistently under a necessary and sufficient rank condition. If the shifts in volatility are sufficiently informative, standard asymptotic inference remains valid even with (i) local-to-zero covariance between the proxies and the instrumented structural shocks, and (ii) potential failures of instrument exogeneity. Intuitively, shifts in volatility act similarly to strong instruments that are correlated with both the target and non-target shocks. We illustrate the effectiveness of our approach by revisiting a seminal fiscal proxy-SVAR for the US economy. We detect a sharp change in the size of the tax multiplier when the narrative tax instrument is complemented with the decline in unconditional volatility observed during the transition from the Great Inflation to the Great Moderation. The narrative tax instrument contributes to identify the tax shock in both regimes, although our empirical analysis raises concerns about its "statistical" validity.

**Link**: [arxiv](http://arxiv.org/abs/2403.08753v3),  [pdf](http://arxiv.org/pdf/2403.08753v3)

**Tags**: econ.EM 



### Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via   Probabilistic Model Checking
**Authors**: Haoyu Wang, Chris M. Poskitt, Jun Sun, Jiali Wei

**Updated**: 2025-08-01T10:24:47Z

**Summary**: Large Language Model (LLM) agents exhibit powerful autonomous capabilities across domains such as robotics, virtual assistants, and web automation. However, their stochastic behavior introduces significant safety risks that are difficult to anticipate. Existing rule-based enforcement systems, such as AgentSpec, focus on developing reactive safety rules, which typically respond only when unsafe behavior is imminent or has already occurred. These systems lack foresight and struggle with long-horizon dependencies and distribution shifts. To address these limitations, we propose Pro2Guard, a proactive runtime enforcement framework grounded in probabilistic reachability analysis. Pro2Guard abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it anticipates future risks by estimating the probability of reaching unsafe states, triggering interventions before violations occur when the predicted risk exceeds a user-defined threshold. By incorporating semantic validity checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability while approximating the underlying ground-truth model. We evaluate Pro2Guard extensively across two safety-critical domains: embodied household agents and autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early on up to 93.6% of unsafe tasks using low thresholds, while configurable modes (e.g., reflect) allow balancing safety with task success, maintaining up to 80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100% prediction of traffic law violations and collisions, anticipating risks up to 38.66 seconds ahead.

**Link**: [arxiv](http://arxiv.org/abs/2508.00500v1),  [pdf](http://arxiv.org/pdf/2508.00500v1)

**Tags**: cs.AI cs.SE 



### Linguistic Generalizability of Test-Time Scaling in Mathematical   Reasoning
**Authors**: Guijin Son, Jiwoo Hong, Hyunwoo Ko, James Thorne

**Updated**: 2025-08-01T10:09:29Z

**Summary**: Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although "thinking LLMs" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.

**Link**: [arxiv](http://arxiv.org/abs/2502.17407v2),  [pdf](http://arxiv.org/pdf/2502.17407v2)

**Tags**: cs.CL 



### IFEvalCode: Controlled Code Generation
**Authors**: Jian Yang, Wei Zhang, Shukai Liu, Linzheng Chai, Yingshui Tan, Jiaheng Liu, Ge Zhang, Wangchunshu Zhou, Guanglin Niu, Zhoujun Li, Binyuan Hui, Junyang Lin

**Updated**: 2025-08-01T10:07:37Z

**Summary**: Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions.

**Link**: [arxiv](http://arxiv.org/abs/2507.22462v2),  [pdf](http://arxiv.org/pdf/2507.22462v2)

**Tags**: cs.CL 



### Unlocking Multi-Modal Potentials for Link Prediction on Dynamic   Text-Attributed Graphs
**Authors**: Yuanyuan Xu, Wenjie Zhang, Ying Zhang, Xuemin Lin, Xiwei Xu

**Updated**: 2025-08-01T10:07:08Z

**Summary**: Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that captures evolving temporal events (edges) alongside rich textual attributes. Existing studies can be broadly categorized into TGNN-driven and LLM-driven approaches, both of which encode textual attributes and temporal structures for DyTAG representation. We observe that DyTAGs inherently comprise three distinct modalities: temporal, textual, and structural, often exhibiting completely disjoint distributions. However, the first two modalities are largely overlooked by existing studies, leading to suboptimal performance. To address this, we propose MoMent, a multi-modal model that explicitly models, integrates, and aligns each modality to learn node representations for link prediction. Given the disjoint nature of the original modality distributions, we first construct modality-specific features and encode them using individual encoders to capture correlations across temporal patterns, semantic context, and local structures. Each encoder generates modality-specific tokens, which are then fused into comprehensive node representations with a theoretical guarantee. To avoid disjoint subspaces of these heterogeneous modalities, we propose a dual-domain alignment loss that first aligns their distributions globally and then fine-tunes coherence at the instance level. This enhances coherent representations from temporal, textual, and structural views. Extensive experiments across seven datasets show that MoMent achieves up to 17.28% accuracy improvement and up to 31x speed-up against eight baselines.

**Link**: [arxiv](http://arxiv.org/abs/2502.19651v2),  [pdf](http://arxiv.org/pdf/2502.19651v2)

**Tags**: cs.LG cs.CL 



### The Missing Parts: Augmenting Fact Verification with Half-Truth   Detection
**Authors**: Yixuan Tang, Jincheng Wang, Anthony K. H. Tung

**Updated**: 2025-08-01T10:06:38Z

**Summary**: Fact verification systems typically assess whether a claim is supported by retrieved evidence, assuming that truthfulness depends solely on what is stated. However, many real-world claims are half-truths, factually correct yet misleading due to the omission of critical context. Existing models struggle with such cases, as they are not designed to reason about what is left unsaid. We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a new benchmark with 15k political claims annotated with sentence-level evidence alignment and inferred claim intent. To address this challenge, we present TRACER, a modular re-assessment framework that identifies omission-based misinformation by aligning evidence, inferring implied intent, and estimating the causal impact of hidden content. TRACER can be integrated into existing fact-checking pipelines and consistently improves performance across multiple strong baselines. Notably, it boosts Half-True classification F1 by up to 16 points, highlighting the importance of modeling omissions for trustworthy fact verification.

**Link**: [arxiv](http://arxiv.org/abs/2508.00489v1),  [pdf](http://arxiv.org/pdf/2508.00489v1)

**Tags**: cs.CL 



### Sign Spotting Disambiguation using Large Language Models
**Authors**: JianHe Low, Ozge Mercanoglu Sincan, Richard Bowden

**Updated**: 2025-08-01T09:56:35Z

**Summary**: Sign spotting, the task of identifying and localizing individual signs within continuous sign language video, plays a pivotal role in scaling dataset annotations and addressing the severe data scarcity issue in sign language translation. While automatic sign spotting holds great promise for enabling frame-level supervision at scale, it grapples with challenges such as vocabulary inflexibility and ambiguity inherent in continuous sign streams. Hence, we introduce a novel, training-free framework that integrates Large Language Models (LLMs) to significantly enhance sign spotting quality. Our approach extracts global spatio-temporal and hand shape features, which are then matched against a large-scale sign dictionary using dynamic time warping and cosine similarity. This dictionary-based matching inherently offers superior vocabulary flexibility without requiring model retraining. To mitigate noise and ambiguity from the matching process, an LLM performs context-aware gloss disambiguation via beam search, notably without fine-tuning. Extensive experiments on both synthetic and real-world sign language datasets demonstrate our method's superior accuracy and sentence fluency compared to traditional approaches, highlighting the potential of LLMs in advancing sign spotting.

**Link**: [arxiv](http://arxiv.org/abs/2507.03703v2),  [pdf](http://arxiv.org/pdf/2507.03703v2)

**Tags**: cs.CV cs.AI 



### CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy   Optimization
**Authors**: Yuning Jiang, Nay Oo, Qiaoran Meng, Lu Lin, Dusit Niyato, Zehui Xiong, Hoon Wei Lim, Biplab Sikdar

**Updated**: 2025-08-01T09:53:06Z

**Summary**: Modern cyber attacks unfold through multiple stages, requiring defenders to dynamically prioritize mitigations under uncertainty. While game-theoretic models capture attacker-defender interactions, existing approaches often rely on static assumptions and lack integration with real-time threat intelligence, limiting their adaptability. This paper presents CyGATE, a game-theoretic framework modeling attacker-defender interactions, using large language models (LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber conflicts as a partially observable stochastic game (POSG) across Cyber Kill Chain stages. Both agents use belief states to navigate uncertainty, with the attacker adapting tactics and the defender re-prioritizing patches based on evolving risks and observed adversary behavior. The framework's flexible architecture enables extension to multi-agent scenarios involving coordinated attackers, collaborative defenders, or complex enterprise environments with multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE effectively prioritizes high-risk vulnerabilities, enhancing adaptability through dynamic threat integration, strategic foresight by anticipating attacker moves under uncertainty, and efficiency by optimizing resource use.

**Link**: [arxiv](http://arxiv.org/abs/2508.00478v1),  [pdf](http://arxiv.org/pdf/2508.00478v1)

**Tags**: cs.CR cs.AI 91A10, 91A43, 68T01, 94A60 C.2.0; I.2.6; K.6.5 



### Probing graph topology from local quantum measurements
**Authors**: F. Romeo, J. Settino

**Updated**: 2025-08-01T09:52:43Z

**Summary**: We show that global properties of an unknown quantum network, such as the average degree, hub density, and the number of closed paths of fixed length, can be inferred from strictly local quantum measurements. In particular, we demonstrate that a malicious agent with access to only a small subset of nodes can initialize quantum states locally and, through repeated short-time measurements, extract sensitive structural information about the entire network. The intrusion strategy is inspired by extreme learning and quantum reservoir computing and combines short-time quantum evolution with a non-iterative linear readout with trainable weights. These results suggest new strategies for intrusion detection and structural diagnostics in future quantum Internet infrastructures.

**Link**: [arxiv](http://arxiv.org/abs/2507.23689v2),  [pdf](http://arxiv.org/pdf/2507.23689v2)

**Tags**: quant-ph cond-mat.dis-nn 



### Inference of maximum parsimony phylogenetic trees with model-based   classical and quantum methods
**Authors**: Jiawei Zhang, Yibo Chen, Yang Zhou, Jun-Han Huang

**Updated**: 2025-08-01T09:43:12Z

**Summary**: The maximum parsimony phylogenetic tree reconstruction problem is NP-hard, presenting a computational bottleneck for classical computing and motivating the exploration of emerging paradigms like quantum computing. To this end, we design three optimization models compatible with both classical and quantum solvers. Our method directly searches the complete solution space of all possible tree topologies and ancestral states, thereby avoiding the potential biases associated with pre-constructing candidate internal nodes. Among these models, the branch-based model drastically reduces the number of variables and explicit constraints through a specific variable definition, providing a novel modeling approach effective not only for phylogenetic tree building but also for other tree problems. The correctness of this model is validated with a classical solver, which obtains solutions that are generally better than those from heuristics on the GAPDH gene dataset. Moreover, our quantum simulations successfully find the exact optimal solutions for small-scale instances with rapid convergence, highlighting the potential of quantum computing to offer a new avenue for solving these intractable problems in evolutionary biology.

**Link**: [arxiv](http://arxiv.org/abs/2508.00468v1),  [pdf](http://arxiv.org/pdf/2508.00468v1)

**Tags**: quant-ph 



### RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism
**Authors**: Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu

**Updated**: 2025-08-01T09:41:05Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while LLMs remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have aimed to enhance models' search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to reliance on single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, with the aim of reducing inference time and enhancing the model's capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%.

**Link**: [arxiv](http://arxiv.org/abs/2507.02962v4),  [pdf](http://arxiv.org/pdf/2507.02962v4)

**Tags**: cs.CL cs.AI cs.IR 



### Thinking Machines: Mathematical Reasoning in the Age of LLMs
**Authors**: Andrea Asperti, Alberto Naibo, Claudio Sacerdoti Coen

**Updated**: 2025-08-01T09:31:48Z

**Summary**: Large Language Models (LLMs) have shown remarkable abilities in structured reasoning and symbolic tasks, with coding emerging as a particular area of strength. This success has sparked growing interest in applying LLMs to mathematics, both in informal problem-solving and formal theorem proving. However, progress in formal mathematics has proven to be significantly more difficult, despite surface-level similarities between programming and proof construction. This discrepancy raises important questions about how LLMs ``reason'', how they are supervised, and whether they internally track a notion of computational or deductive state. In this article, we address the state-of-the-art of the discipline, focusing on recent models and benchmarks, and explore three central issues at the intersection of machine learning and mathematical cognition: (i) the trade-offs between formal and informal mathematics as training domains; (ii) the deeper reasons why proof generation remains more brittle than code synthesis; (iii) and the question of whether LLMs represent, or merely mimic, a notion of evolving logical state. Our goal is not to draw hard boundaries, but to identify where the current limits lie, and how they might be extended.

**Link**: [arxiv](http://arxiv.org/abs/2508.00459v1),  [pdf](http://arxiv.org/pdf/2508.00459v1)

**Tags**: cs.AI 68T07, 68T20 I.2.6; I.2.7; I.2.3 



### Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges
**Authors**: Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding

**Updated**: 2025-08-01T09:26:01Z

**Summary**: Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2508.00454v1),  [pdf](http://arxiv.org/pdf/2508.00454v1)

**Tags**: cs.CL 



### When Relevance Meets Novelty: Dual-Stable Periodic Optimization for   Exploratory Recommendation
**Authors**: Hongxiang Lin, Hao Guo, Zeshun Li, Erpeng Xue, Yongqian He, Xiangyu Hou, Zhaoyu Hu, Lei Wang, Sheng Chen

**Updated**: 2025-08-01T09:10:56Z

**Summary**: Traditional recommendation systems tend to trap users in strong feedback loops by excessively pushing content aligned with their historical preferences, thereby limiting exploration opportunities and causing content fatigue. Although large language models (LLMs) demonstrate potential with their diverse content generation capabilities, existing LLM-enhanced dual-model frameworks face two major limitations: first, they overlook long-term preferences driven by group identity, leading to biased interest modeling; second, they suffer from static optimization flaws, as a one-time alignment process fails to leverage incremental user data for closed-loop optimization. To address these challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE) module, jointly modeling long-term group identity and short-term individual interests through parallel processing of behavioral sequences. For static optimization limitations, we design a Periodic Collaborative Optimization (PCO) mechanism. This mechanism regularly conducts preference verification on incremental data using the Relevance LLM, then guides the Novelty LLM to perform fine-tuning based on the verification results, and subsequently feeds back the output of the incrementally fine-tuned Novelty LLM to the Relevance LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization. Extensive online and offline experiments verify the effectiveness of the CoEA model in exploratory recommendation.

**Link**: [arxiv](http://arxiv.org/abs/2508.00450v1),  [pdf](http://arxiv.org/pdf/2508.00450v1)

**Tags**: cs.IR cs.AI 



### CLIPTime: Time-Aware Multimodal Representation Learning from Images and   Text
**Authors**: Anju Rani, Daniel Ortiz-Arroyo, Petar Durdevic

**Updated**: 2025-08-01T09:08:10Z

**Summary**: Understanding the temporal dynamics of biological growth is critical across diverse fields such as microbiology, agriculture, and biodegradation research. Although vision-language models like Contrastive Language Image Pretraining (CLIP) have shown strong capabilities in joint visual-textual reasoning, their effectiveness in capturing temporal progression remains limited. To address this, we propose CLIPTime, a multimodal, multitask framework designed to predict both the developmental stage and the corresponding timestamp of fungal growth from image and text inputs. Built upon the CLIP architecture, our model learns joint visual-textual embeddings and enables time-aware inference without requiring explicit temporal input during testing. To facilitate training and evaluation, we introduce a synthetic fungal growth dataset annotated with aligned timestamps and categorical stage labels. CLIPTime jointly performs classification and regression, predicting discrete growth stages alongside continuous timestamps. We also propose custom evaluation metrics, including temporal accuracy and regression error, to assess the precision of time-aware predictions. Experimental results demonstrate that CLIPTime effectively models biological progression and produces interpretable, temporally grounded outputs, highlighting the potential of vision-language models in real-world biological monitoring applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.00447v1),  [pdf](http://arxiv.org/pdf/2508.00447v1)

**Tags**: cs.CV cs.LG 



## Keyword: LLM Deployment 
 ### Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory   Management
**Authors**: Ping Chen, Zhuohong Deng, Ping Li, Shuibing He, Hongzi Zhu, Yi Zheng, Zhefeng Wang, Baoxing Huai, Minyi Guo

**Updated**: 2025-08-01T17:39:25Z

**Summary**: Training large language models often employs recomputation to alleviate memory pressure, which can introduce up to 30% overhead in real-world scenarios. In this paper, we propose Adacc, a novel memory management framework that combines adaptive compression and activation checkpointing to reduce the GPU memory footprint. It comprises three modules: (1) We design layer-specific compression algorithms that account for outliers in LLM tensors, instead of directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We propose an optimal scheduling policy that employs MILP to determine the best memory optimization for each tensor. (3) To accommodate changes in training tensors, we introduce an adaptive policy evolution mechanism that adjusts the policy during training to enhance throughput. Experimental results show that Adacc can accelerate the LLM training by 1.01x to 1.37x compared to state-of-the-art frameworks, while maintaining comparable model accuracy to the Baseline.

**Link**: [arxiv](http://arxiv.org/abs/2508.00806v1),  [pdf](http://arxiv.org/pdf/2508.00806v1)

**Tags**: cs.LG cs.DC 



### Online Fine-Tuning of Carbon Emission Predictions using Real-Time   Recurrent Learning for State Space Models
**Authors**: Julian Lemmel, Manuel Kranzl, Adam Lamine, Philipp Neubauer, Radu Grosu, Sophie Neubauer

**Updated**: 2025-08-01T17:37:19Z

**Summary**: This paper introduces a new approach for fine-tuning the predictions of structured state space models (SSMs) at inference time using real-time recurrent learning. While SSMs are known for their efficiency and long-range modeling capabilities, they are typically trained offline and remain static during deployment. Our method enables online adaptation by continuously updating model parameters in response to incoming data. We evaluate our approach for linear-recurrent-unit SSMs using a small carbon emission dataset collected from embedded automotive hardware. Experimental results show that our method consistently reduces prediction error online during inference, demonstrating its potential for dynamic, resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.00804v1),  [pdf](http://arxiv.org/pdf/2508.00804v1)

**Tags**: cs.CE cs.LG cs.SY eess.SY 



### UTrace: Poisoning Forensics for Private Collaborative Learning
**Authors**: Evan Rose, Hidde Lycklama, Harsh Chaudhari, Anwar Hithnawi, Alina Oprea

**Updated**: 2025-08-01T17:31:33Z

**Summary**: Privacy-preserving machine learning (PPML) enables multiple data owners to contribute their data privately to a set of servers that run a secure multi-party computation (MPC) protocol to train a joint ML model. In these protocols, the input data remains private throughout the training process, and only the resulting model is made available. While this approach benefits privacy, it also exacerbates the risks of data poisoning, where compromised data owners induce undesirable model behavior by contributing malicious datasets. Existing MPC mechanisms can mitigate certain poisoning attacks, but these measures are not exhaustive. To complement existing poisoning defenses, we introduce UTrace: a framework for User-level Traceback of poisoning attacks in PPML. Utrace computes user responsibility scores using gradient similarity metrics aggregated across the most relevant samples in an owner's dataset. UTrace is effective at low poisoning rates and is resilient to poisoning attacks distributed across multiple data owners, unlike existing unlearning-based methods. We introduce methods for checkpointing gradients with low storage overhead, enabling traceback in the absence of data owners at deployment time. We also design several optimizations that reduce traceback time and communication in MPC. We provide a comprehensive evaluation of UTrace across four datasets from three data modalities (vision, text, and malware) and show its effectiveness against 10 poisoning attacks.

**Link**: [arxiv](http://arxiv.org/abs/2409.15126v2),  [pdf](http://arxiv.org/pdf/2409.15126v2)

**Tags**: cs.CR cs.LG 



### Multibeam High Throughput Satellite: Hardware Foundation, Resource   Allocation, and Precoding
**Authors**: Rui Chen, Wen-Xuan Long, Bing-Qian Wang, Yuan He, Rui-Jin Sun, Nan Cheng, Gan Zheng, Dusit Niyato

**Updated**: 2025-08-01T17:30:03Z

**Summary**: With its wide coverage and uninterrupted service, satellite communication is a critical technology for next-generation 6G communications. High throughput satellite (HTS) systems, utilizing multipoint beam and frequency multiplexing techniques, enable satellite communication capacity of up to Tbps to meet the growing traffic demand. Therefore, it is imperative to review the-state-of-the-art of multibeam HTS systems and identify their associated challenges and perspectives. Firstly, we summarize the multibeam HTS hardware foundations, including ground station systems, on-board payloads, and user terminals. Subsequently, we review the flexible on-board radio resource allocation approaches of bandwidth, power, time slot, and joint allocation schemes of HTS systems to optimize resource utilization and cater to non-uniform service demand. Additionally, we survey multibeam precoding methods for the HTS system to achieve full-frequency reuse and interference cancellation, which are classified according to different deployments such as single gateway precoding, multiple gateway precoding, on-board precoding, and hybrid on-board/on-ground precoding. Finally, we disscuss the challenges related to Q/V band link outage, time and frequency synchronization of gateways, the accuracy of channel state information (CSI), payload light-weight development, and the application of deep learning (DL). Research on these topics will contribute to enhancing the performance of HTS systems and finally delivering high-speed data to areas underserved by terrestrial networks.

**Link**: [arxiv](http://arxiv.org/abs/2508.00800v1),  [pdf](http://arxiv.org/pdf/2508.00800v1)

**Tags**: eess.SP 



### Entanglement Management in Space-Based Quantum Information Networks
**Authors**: Luca Paccard, Agathe Blaise, Fabrice Arnal, Laurent de Forges de Parny

**Updated**: 2025-08-01T17:21:26Z

**Summary**: With the evolution of quantum computing, quantum sensing and secure quantum communication protocols, the demand for global development of Quantum Information Networks (QIN) has become crucial. Satellites play an indispensable role in enabling connectivity across vast distances, transcending terrestrial limitations. In this article, we explore various ways in which satellites may be involved in the deployment of these novel networks from their integration into the network architecture to the challenges they face.

**Link**: [arxiv](http://arxiv.org/abs/2508.00793v1),  [pdf](http://arxiv.org/pdf/2508.00793v1)

**Tags**: quant-ph 



### RecPS: Privacy Risk Scoring for Recommender Systems
**Authors**: Jiajie He, Yuechun Gu, Keke Chen

**Updated**: 2025-08-01T17:19:56Z

**Summary**: Recommender systems (RecSys) have become an essential component of many web applications. The core of the system is a recommendation model trained on highly sensitive user-item interaction data. While privacy-enhancing techniques are actively studied in the research community, the real-world model development still depends on minimal privacy protection, e.g., via controlled access. Users of such systems should have the right to choose \emph{not} to share highly sensitive interactions. However, there is no method allowing the user to know which interactions are more sensitive than others. Thus, quantifying the privacy risk of RecSys training data is a critical step to enabling privacy-aware RecSys model development and deployment. We propose a membership-inference attack (MIA)- based privacy scoring method, RecPS, to measure privacy risks at both the interaction and user levels. The RecPS interaction-level score definition is motivated and derived from differential privacy, which is then extended to the user-level scoring method. A critical component is the interaction-level MIA method RecLiRA, which gives high-quality membership estimation. We have conducted extensive experiments on well-known benchmark datasets and RecSys models to show the unique features and benefits of RecPS scoring in risk assessment and RecSys model unlearning.

**Link**: [arxiv](http://arxiv.org/abs/2507.18365v3),  [pdf](http://arxiv.org/pdf/2507.18365v3)

**Tags**: cs.IR cs.AI cs.CR 



### A Survey of Self-Evolving Agents: On Path to Artificial Super   Intelligence
**Authors**: Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenhailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang

**Updated**: 2025-08-01T17:17:09Z

**Summary**: Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.21046v3),  [pdf](http://arxiv.org/pdf/2507.21046v3)

**Tags**: cs.AI 



### Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun   Handling in Large Language Models
**Authors**: Xushuo Tang, Yi Ding, Zhengyi Yang, Yin Chen, Yongrui Gu, Wenke Yang, Mingchen Ju, Xin Cao, Yongfei Liu, Wenjie Zhang

**Updated**: 2025-08-01T17:11:42Z

**Summary**: Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs' handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs' pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.

**Link**: [arxiv](http://arxiv.org/abs/2508.00788v1),  [pdf](http://arxiv.org/pdf/2508.00788v1)

**Tags**: cs.CL cs.AI 



### SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions
**Authors**: Babak Taati, Muhammad Muzammil, Yasamin Zarghami, Abhishek Moturu, Amirhossein Kazerouni, Hailey Reimer, Alex Mihailidis, Thomas Hadjistavropoulos

**Updated**: 2025-08-01T17:06:27Z

**Summary**: Accurate pain assessment in patients with limited ability to communicate, such as older adults with dementia, represents a critical healthcare challenge. Robust automated systems of pain detection may facilitate such assessments. Existing pain detection datasets, however, suffer from limited ethnic/racial diversity, privacy constraints, and underrepresentation of older adults who are the primary target population for clinical deployment. We present SynPAIN, a large-scale synthetic dataset containing 10,710 facial expression images (5,355 neutral/expressive pairs) across five ethnicities/races, two age groups (young: 20-35, old: 75+), and two genders. Using commercial generative AI tools, we created demographically balanced synthetic identities with clinically meaningful pain expressions. Our validation demonstrates that synthetic pain expressions exhibit expected pain patterns, scoring significantly higher than neutral and non-pain expressions using clinically validated pain assessment tools based on facial action unit analysis. We experimentally demonstrate SynPAIN's utility in identifying algorithmic bias in existing pain detection models. Through comprehensive bias evaluation, we reveal substantial performance disparities across demographic characteristics. These performance disparities were previously undetectable with smaller, less diverse datasets. Furthermore, we demonstrate that age-matched synthetic data augmentation improves pain detection performance on real clinical data, achieving a 7.0% improvement in average precision. SynPAIN addresses critical gaps in pain assessment research by providing the first publicly available, demographically diverse synthetic dataset specifically designed for older adult pain detection, while establishing a framework for measuring and mitigating algorithmic bias. The dataset is available at https://doi.org/10.5683/SP3/WCXMAP

**Link**: [arxiv](http://arxiv.org/abs/2507.19673v2),  [pdf](http://arxiv.org/pdf/2507.19673v2)

**Tags**: cs.CV 



### An Investigation into Value Misalignment in LLM-Generated Texts for   Cultural Heritage
**Authors**: Fan Bu, Zheng Wang, Siyi Wang, Ziyao Liu

**Updated**: 2025-08-01T17:05:21Z

**Summary**: As Large Language Models (LLMs) become increasingly prevalent in tasks related to cultural heritage, such as generating descriptions of historical monuments, translating ancient texts, preserving oral traditions, and creating educational content, their ability to produce accurate and culturally aligned texts is being increasingly relied upon by users and researchers. However, cultural value misalignments may exist in generated texts, such as the misrepresentation of historical facts, the erosion of cultural identity, and the oversimplification of complex cultural narratives, which may lead to severe consequences. Therefore, investigating value misalignment in the context of LLM for cultural heritage is crucial for mitigating these risks, yet there has been a significant lack of systematic and comprehensive study and investigation in this area. To fill this gap, we systematically assess the reliability of LLMs in generating culturally aligned texts for cultural heritage-related tasks. We conduct a comprehensive evaluation by compiling an extensive set of 1066 query tasks covering 5 widely recognized categories with 17 aspects within the knowledge framework of cultural heritage across 5 open-source LLMs, and examine both the type and rate of cultural value misalignments in the generated texts. Using both automated and manual approaches, we effectively detect and analyze the cultural value misalignments in LLM-generated texts. Our findings are concerning: over 65% of the generated texts exhibit notable cultural misalignments, with certain tasks demonstrating almost complete misalignment with key cultural values. Beyond these findings, this paper introduces a benchmark dataset and a comprehensive evaluation workflow that can serve as a valuable resource for future research aimed at enhancing the cultural sensitivity and reliability of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2501.02039v3),  [pdf](http://arxiv.org/pdf/2501.02039v3)

**Tags**: cs.CL cs.AI 



### ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A   Zero-Shot Approach using LLM-Driven Code Generation
**Authors**: Atakan Site, Emre Hakan Erdemir, Gülşen Eryiğit

**Updated**: 2025-08-01T16:38:18Z

**Summary**: This paper presents our system for SemEval-2025 Task 8: DataBench, Question-Answering over Tabular Data. The primary objective of this task is to perform question answering on given tabular datasets from diverse domains under two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To tackle both subtasks, we developed a zero-shot solution with a particular emphasis on leveraging Large Language Model (LLM)-based code generation. Specifically, we propose a Python code generation framework utilizing state-of-the-art open-source LLMs to generate executable Pandas code via optimized prompting strategies. Our experiments reveal that different LLMs exhibit varying levels of effectiveness in Python code generation. Additionally, results show that Python code generation achieves superior performance in tabular question answering compared to alternative approaches. Although our ranking among zero-shot systems is unknown at the time of this paper's submission, our system achieved eighth place in Subtask I and sixth place in Subtask~II among the 30 systems that outperformed the baseline in the open-source models category.

**Link**: [arxiv](http://arxiv.org/abs/2508.00762v1),  [pdf](http://arxiv.org/pdf/2508.00762v1)

**Tags**: cs.CL 



### Mitigating Gender Bias via Fostering Exploratory Thinking in LLMs
**Authors**: Kangda Wei, Hasnat Md Abdullah, Ruihong Huang

**Updated**: 2025-08-01T16:37:16Z

**Summary**: Large Language Models (LLMs) often exhibit gender bias, resulting in unequal treatment of male and female subjects across different contexts. To address this issue, we propose a novel data generation framework that fosters exploratory thinking in LLMs. Our approach prompts models to generate story pairs featuring male and female protagonists in structurally identical, morally ambiguous scenarios, then elicits and compares their moral judgments. When inconsistencies arise, the model is guided to produce balanced, gender-neutral judgments. These story-judgment pairs are used to fine-tune or optimize the models via Direct Preference Optimization (DPO). Experimental results show that our method significantly reduces gender bias while preserving or even enhancing general model capabilities. We will release the code and generated data. We release the code and generated data at: https://github.com/WeiKangda/LLMs-Exploratory-Bias-Mitigation/tree/main.

**Link**: [arxiv](http://arxiv.org/abs/2505.17217v2),  [pdf](http://arxiv.org/pdf/2505.17217v2)

**Tags**: cs.CL cs.AI cs.CY 



### MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese   Hate Speech Detection under Cloaking Perturbations
**Authors**: Qiyao Xue, Yuchen Dou, Ryan Shi, Xiang Lorraine Li, Wei Gao

**Updated**: 2025-08-01T16:34:57Z

**Summary**: Hate speech detection on Chinese social networks presents distinct challenges, particularly due to the widespread use of cloaking techniques designed to evade conventional text-based detection systems. Although large language models (LLMs) have recently improved hate speech detection capabilities, the majority of existing work has concentrated on English datasets, with limited attention given to multimodal strategies in the Chinese context. In this study, we propose MMBERT, a novel BERT-based multimodal framework that integrates textual, speech, and visual modalities through a Mixture-of-Experts (MoE) architecture. To address the instability associated with directly integrating MoE into BERT-based models, we develop a progressive three-stage training paradigm. MMBERT incorporates modality-specific experts, a shared self-attention mechanism, and a router-based expert allocation strategy to enhance robustness against adversarial perturbations. Empirical results in several Chinese hate speech datasets show that MMBERT significantly surpasses fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing in-context learning approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.00760v1),  [pdf](http://arxiv.org/pdf/2508.00760v1)

**Tags**: cs.CL cs.AI 



### Sound and Complete Neurosymbolic Reasoning with LLM-Grounded   Interpretations
**Authors**: Bradley P. Allen, Prateek Chhikara, Thomas Macaulay Ferguson, Filip Ilievski, Paul Groth

**Updated**: 2025-08-01T16:30:02Z

**Summary**: Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but they exhibit problems with logical consistency in the output they generate. How can we harness LLMs' broad-coverage parametric knowledge in formal reasoning despite their inconsistency? We present a method for directly integrating an LLM into the interpretation function of the formal semantics for a paraconsistent logic. We provide experimental evidence for the feasibility of the method by evaluating the function using datasets created from several short-form factuality benchmarks. Unlike prior work, our method offers a theoretical framework for neurosymbolic reasoning that leverages an LLM's knowledge while preserving the underlying logic's soundness and completeness properties.

**Link**: [arxiv](http://arxiv.org/abs/2507.09751v2),  [pdf](http://arxiv.org/pdf/2507.09751v2)

**Tags**: cs.AI cs.CL cs.LO 



### PanoLlama: Generating Endless and Coherent Panoramas with   Next-Token-Prediction LLMs
**Authors**: Teng Zhou, Xiaoyu Zhang, Yongchuan Tang

**Updated**: 2025-08-01T16:25:54Z

**Summary**: Panoramic Image Generation (PIG) aims to create coherent images of arbitrary lengths. Most existing methods fall in the joint diffusion paradigm, but their complex and heuristic crop connection designs often limit their ability to achieve multilevel coherence. By deconstructing this challenge into its core components, we find it naturally aligns with next-token prediction, leading us to adopt an autoregressive (AR) paradigm for PIG modeling. However, existing visual AR (VAR) models are limited to fixed-size generation, lacking the capability to produce panoramic images. In this paper, we propose PanoLlama, a novel framework that achieves endless and coherent panorama generation with the autoregressive paradigm. Our approach develops a training-free strategy that utilizes token redirection to overcome the size limitations of existing VAR models, enabling next-crop prediction in both horizontal and vertical directions. This refreshes the PIG pipeline while achieving SOTA performance in coherence (47.50%), fidelity(28.16%), and aesthetics (15%). Additionally, PanoLlama supports applications other PIG methods cannot achieve, including mask-free layout control, multi-scale and multi-guidance synthesis. To facilitate standardized evaluation, we also establish a dataset with 1,000 prompts spanning 100+ themes, providing a new testing benchmark for PIG research. The code is available at https://github.com/0606zt/PanoLlama.

**Link**: [arxiv](http://arxiv.org/abs/2411.15867v3),  [pdf](http://arxiv.org/pdf/2411.15867v3)

**Tags**: cs.CV 



### Agentic large language models improve retrieval-based radiology question   answering
**Authors**: Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald Köstler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh

**Updated**: 2025-08-01T16:18:52Z

**Summary**: Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to >670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized models (e.g., Mistral Large improved from 72% to 81%) and small-scale models (e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B parameters) demonstrated minimal changes (<2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.

**Link**: [arxiv](http://arxiv.org/abs/2508.00743v1),  [pdf](http://arxiv.org/pdf/2508.00743v1)

**Tags**: cs.CL cs.AI cs.LG 



### Out-of-Context Abduction: LLMs Make Inferences About Procedural Data   Leveraging Declarative Facts in Earlier Training Data
**Authors**: Sohaib Imran, Rob Lamb, Peter M. Atkinson

**Updated**: 2025-08-01T16:12:23Z

**Summary**: Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at least one chatbot's name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbot's behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.

**Link**: [arxiv](http://arxiv.org/abs/2508.00741v1),  [pdf](http://arxiv.org/pdf/2508.00741v1)

**Tags**: cs.CL cs.AI 



### How LLMs are Shaping the Future of Virtual Reality
**Authors**: Süeda Özkaya, Santiago Berrezueta-Guzman, Stefan Wagner

**Updated**: 2025-08-01T16:08:05Z

**Summary**: The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.00737v1),  [pdf](http://arxiv.org/pdf/2508.00737v1)

**Tags**: cs.HC cs.AI 



### Navigating Distribution Shifts in Medical Image Analysis: A Survey
**Authors**: Zixian Su, Jingwei Guo, Xi Yang, Qiufeng Wang, Frans Coenen, Kaizhu Huang

**Updated**: 2025-08-01T16:00:18Z

**Summary**: Medical Image Analysis (MedIA) has become indispensable in modern healthcare, enhancing clinical diagnostics and personalized treatment. Despite the remarkable advancements supported by deep learning (DL) technologies, their practical deployment faces challenges due to distribution shifts, where models trained on specific datasets underperform across others from varying hospitals, regions, or patient populations. To navigate this issue, researchers have been actively developing strategies to increase the adaptability and robustness of DL models, enabling their effective use in unfamiliar and diverse environments. This paper systematically reviews approaches that apply DL techniques to MedIA systems affected by distribution shifts. Unlike traditional categorizations based on technical specifications, our approach is grounded in the real-world operational constraints faced by healthcare institutions. Specifically, we categorize the existing body of work into Joint Training, Federated Learning, Fine-tuning, and Domain Generalization, with each method tailored to distinct scenarios caused by Data Accessibility, Privacy Concerns, and Collaborative Protocols. This perspective equips researchers with a nuanced understanding of how DL can be strategically deployed to address distribution shifts in MedIA, ensuring diverse and robust medical applications. By delving deeper into these topics, we highlight potential pathways for future research that not only address existing limitations but also push the boundaries of deployable MedIA technologies.

**Link**: [arxiv](http://arxiv.org/abs/2411.05824v2),  [pdf](http://arxiv.org/pdf/2411.05824v2)

**Tags**: eess.IV cs.CV cs.LG 



### From Press to Pixels: Evolving Urdu Text Recognition
**Authors**: Samee Arif, Sualeha Farid

**Updated**: 2025-08-01T15:56:30Z

**Summary**: This paper introduces an end-to-end pipeline for Optical Character Recognition (OCR) on Urdu newspapers, addressing challenges posed by complex multi-column layouts, low-resolution scans, and the stylistic variability of the Nastaliq script. Our system comprises four modules: (1) article segmentation, (2) image super-resolution, (3) column segmentation, and (4) text recognition. We fine-tune YOLOv11x for segmentation, achieving 0.963 precision for articles and 0.970 for columns. A SwinIR-based super-resolution model boosts LLM text recognition accuracy by 25-70%. We also introduce the Urdu Newspaper Benchmark (UNB), a manually annotated dataset for Urdu OCR. Using UNB and the OpenITI corpus, we compare traditional CNN+RNN-based OCR models with modern LLMs. Gemini-2.5-Pro achieves the best performance with a WER of 0.133. We further analyze LLM outputs via insertion, deletion, and substitution error breakdowns, as well as character-level confusion analysis. Finally, we show that fine-tuning on just 500 samples yields a 6.13% WER improvement, highlighting the adaptability of LLMs for Urdu OCR.

**Link**: [arxiv](http://arxiv.org/abs/2505.13943v2),  [pdf](http://arxiv.org/pdf/2505.13943v2)

**Tags**: cs.CV 



### Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors   Impacting AI Adoption
**Authors**: Rebecca Yu, Valerie Chen, Ameet Talwalkar, Hoda Heidari

**Updated**: 2025-08-01T15:44:25Z

**Summary**: Growing excitement around deploying AI across various domains calls for a careful assessment of how human decision-makers interact with AI-powered systems. In particular, it is essential to understand when decision-makers voluntarily choose to consult AI tools, which we term decision-maker adoption. We interviewed experts across four domains -- medicine, law, journalism, and the public sector -- to explore current AI use cases and perceptions of adoption. From these interviews, we identify key factors that shape decision-maker adoption of AI tools: the decision-maker's background, perceptions of the AI, consequences for the decision-maker, and perceived implications for other stakeholders. We translate these factors into an AI adoption sheet to analyze how decision-makers approach adoption choices through comparative, cross-domain case studies, highlighting how our factors help explain inter-domain differences in adoption. Our findings offer practical guidance for supporting the responsible and context-aware deployment of AI by better accounting for the decision-maker's perspective.

**Link**: [arxiv](http://arxiv.org/abs/2508.00723v1),  [pdf](http://arxiv.org/pdf/2508.00723v1)

**Tags**: cs.HC 



### Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and   Context-Aware KGQA
**Authors**: Yingxu Wang, Shiqi Fan, Mengzhu Wang, Siwei Liu

**Updated**: 2025-08-01T15:38:21Z

**Summary**: Knowledge Graph Question Answering (KGQA) aims to interpret natural language queries and perform structured reasoning over knowledge graphs by leveraging their relational and semantic structures to retrieve accurate answers. Recent KGQA methods primarily follow either retrieve-then-reason paradigm, relying on GNNs or heuristic rules for static paths extraction, or dynamic path generation strategies that use large language models (LLMs) with prompting to jointly perform retrieval and reasoning. However, the former suffers from limited adaptability due to static path extraction and lack of contextual refinement, while the latter incurs high computational costs and struggles with accurate path evaluation due to reliance on fixed scoring functions and extensive LLM calls. To address these issues, this paper proposes Dynamically Adaptive MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search with adaptive path evaluation for efficient and context-aware KGQA. DAMR employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based planner, which selects top-$k$ relevant relations at each step to reduce search space. To improve path evaluation accuracy, we introduce a lightweight Transformer-based scorer that performs context-aware plausibility estimation by jointly encoding the question and relation sequence through cross-attention, enabling the model to capture fine-grained semantic shifts during multi-hop reasoning. Furthermore, to alleviate the scarcity of high-quality supervision, DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically generates training signals from partial paths explored during search, allowing the scorer to continuously adapt to the evolving distribution of reasoning trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR significantly outperforms state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.00719v1),  [pdf](http://arxiv.org/pdf/2508.00719v1)

**Tags**: cs.CL cs.AI 



### Better Embeddings with Coupled Adam
**Authors**: Felix Stollenwerk, Tobias Stollenwerk

**Updated**: 2025-08-01T15:28:51Z

**Summary**: Despite their remarkable capabilities, LLMs learn word representations that exhibit the undesirable yet poorly understood feature of anisotropy. In this paper, we argue that the second moment in Adam is a cause of anisotropic embeddings, and suggest a modified optimizer called Coupled Adam to mitigate the problem. Our experiments demonstrate that Coupled Adam significantly improves the quality of embeddings, while also leading to better upstream and downstream performance on large enough datasets.

**Link**: [arxiv](http://arxiv.org/abs/2502.08441v3),  [pdf](http://arxiv.org/pdf/2502.08441v3)

**Tags**: cs.CL cs.AI cs.LG 



### Exploring the Evidence-Based SE Beliefs of Generative AI Tools
**Authors**: Chris Brown, Jason Cusati

**Updated**: 2025-08-01T15:27:59Z

**Summary**: Recent innovations in generative artificial intelligence (AI), primarily powered by large language models (LLMs), have transformed how programmers develop and maintain software -- leading to new frontiers in software engineering (SE). The advanced capabilities of generative AI tools to support software development tasks have led to a rise in their adoption within software development workflows. However, little is known about how AI tools perceive evidence-based beliefs and practices verified by research findings. To this end, we conduct a preliminary evaluation conceptually replicating prior work to explore the "beliefs" of generative AI tools used to support software development tasks. We investigate 17 evidence-based claims posited by empirical SE research across five generative AI tools. Our findings show that generative AI tools have ambiguous beliefs regarding research claims and lack credible evidence to support responses. Based on our results, we provide implications for practitioners integrating generative AI-based systems into development contexts and shed light on future research directions to enhance the reliability and trustworthiness of generative AI -- aiming to increase awareness and adoption of evidence-based SE research findings in practice.

**Link**: [arxiv](http://arxiv.org/abs/2407.13900v3),  [pdf](http://arxiv.org/pdf/2407.13900v3)

**Tags**: cs.SE 



### NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian   Common Law System
**Authors**: Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya

**Updated**: 2025-08-01T15:23:20Z

**Summary**: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.

**Link**: [arxiv](http://arxiv.org/abs/2508.00709v1),  [pdf](http://arxiv.org/pdf/2508.00709v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Is LLM-Generated Code More Maintainable \& Reliable than Human-Written   Code?
**Authors**: Alfred Santa Molison, Marcia Moraes, Glaucia Melo, Fabio Santos, Wesley K. G. Assuncao

**Updated**: 2025-08-01T15:17:34Z

**Summary**: Background: The rise of Large Language Models (LLMs) in software development has opened new possibilities for code generation. Despite the widespread use of this technology, it remains unclear how well LLMs generate code solutions in terms of software quality and how they compare to human-written code. Aims: This study compares the internal quality attributes of LLM-generated and human-written code. Method: Our empirical study integrates datasets of coding tasks, three LLM configurations (zero-shot, few-shot, and fine-tuning), and SonarQube to assess software quality. The dataset comprises Python code solutions across three difficulty levels: introductory, interview, and competition. We analyzed key code quality metrics, including maintainability and reliability, and the estimated effort required to resolve code issues. Results: Our analysis shows that LLM-generated code has fewer bugs and requires less effort to fix them overall. Interestingly, fine-tuned models reduced the prevalence of high-severity issues, such as blocker and critical bugs, and shifted them to lower-severity categories, but decreased the model's performance. In competition-level problems, the LLM solutions sometimes introduce structural issues that are not present in human-written code. Conclusion: Our findings provide valuable insights into the quality of LLM-generated code; however, the introduction of critical issues in more complex scenarios highlights the need for a systematic evaluation and validation of LLM solutions. Our work deepens the understanding of the strengths and limitations of LLMs for code generation.

**Link**: [arxiv](http://arxiv.org/abs/2508.00700v1),  [pdf](http://arxiv.org/pdf/2508.00700v1)

**Tags**: cs.SE 



### On-Device Diffusion Transformer Policy for Efficient Robot Manipulation
**Authors**: Yiming Wu, Huan Wang, Zhenghao Chen, Jianxin Pang, Dong Xu

**Updated**: 2025-08-01T15:14:39Z

**Summary**: Diffusion Policies have significantly advanced robotic manipulation tasks via imitation learning, but their application on resource-constrained mobile platforms remains challenging due to computational inefficiency and extensive memory footprint. In this paper, we propose LightDP, a novel framework specifically designed to accelerate Diffusion Policies for real-time deployment on mobile devices. LightDP addresses the computational bottleneck through two core strategies: network compression of the denoising modules and reduction of the required sampling steps. We first conduct an extensive computational analysis on existing Diffusion Policy architectures, identifying the denoising network as the primary contributor to latency. To overcome performance degradation typically associated with conventional pruning methods, we introduce a unified pruning and retraining pipeline, optimizing the model's post-pruning recoverability explicitly. Furthermore, we combine pruning techniques with consistency distillation to effectively reduce sampling steps while maintaining action prediction accuracy. Experimental evaluations on the standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that LightDP achieves real-time action prediction on mobile devices with competitive performance, marking an important step toward practical deployment of diffusion-based policies in resource-limited environments. Extensive real-world experiments also show the proposed LightDP can achieve performance comparable to state-of-the-art Diffusion Policies.

**Link**: [arxiv](http://arxiv.org/abs/2508.00697v1),  [pdf](http://arxiv.org/pdf/2508.00697v1)

**Tags**: cs.RO cs.AI cs.CV 



### Quantum-Informed Machine Learning for Chaotic Systems
**Authors**: Maida Wang, Xiao Xue, Peter V. Coveney

**Updated**: 2025-08-01T15:03:13Z

**Summary**: Learning the behaviour of chaotic systems remains challenging due to instability in long-term predictions and difficulties in accurately capturing invariant statistical properties. While quantum machine learning offers a promising route to efficiently capture physical properties from high-dimensional data, its practical deployment is hindered by current hardware noise and limited scalability. Here, we introduce a quantum-informed machine learning framework for learning partial differential equations, with an application focus on chaotic systems. A quantum circuit Born machine is employed to learn the invariant properties of chaotic dynamical systems, achieving substantial memory efficiency by representing these complex physical statistics with a compact set of trainable circuit parameters. This approach reduces the data storage requirement by over two orders of magnitude compared to the raw simulation data. The resulting statistical quantum-informed prior is then incorporated into a Koopman-based auto-regressive model to address issues such as gradient vanishing or explosion, while maintaining long-term statistical fidelity. The framework is evaluated on three representative systems: the Kuramoto-Sivashinsky equation, two-dimensional Kolmogorov flow and turbulent channel flow. In all cases, the quantum-informed model achieves superior performance compared to its classical counterparts without quantum priors. This hybrid architecture offers a practical route for learning dynamical systems using near-term quantum hardware.

**Link**: [arxiv](http://arxiv.org/abs/2507.19861v2),  [pdf](http://arxiv.org/pdf/2507.19861v2)

**Tags**: quant-ph cs.LG 



### Better Call Claude: Can LLMs Detect Changes of Writing Style?
**Authors**: Johannes Römisch, Svetlana Gorovaia, Mariia Halchynska, Gleb Schmidt, Ivan P. Yamshchikov

**Updated**: 2025-08-01T14:49:50Z

**Summary**: This article explores the zero-shot performance of state-of-the-art large language models (LLMs) on one of the most challenging tasks in authorship analysis: sentence-level style change detection. Benchmarking four LLMs on the official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we present several observations. First, state-of-the-art generative models are sensitive to variations in writing style - even at the granular level of individual sentences. Second, their accuracy establishes a challenging baseline for the task, outperforming suggested baselines of the PAN competition. Finally, we explore the influence of semantics on model predictions and present evidence suggesting that the latest generation of LLMs may be more sensitive to content-independent and purely stylistic signals than previously reported.

**Link**: [arxiv](http://arxiv.org/abs/2508.00680v1),  [pdf](http://arxiv.org/pdf/2508.00680v1)

**Tags**: cs.CL 



### MELAC: Massive Evaluation of Large Language Models with Alignment of   Culture in Persian Language
**Authors**: Farhan Farsi, Farnaz Aghababaloo, Shahriar Shariati Motlagh, Parsa Ghofrani, MohammadAli SadraeiJavaheri, Shayan Bali, Amirhossein Shabani, Farbod Bijary, Ghazal Zamaninejad, AmirMohammad Salehoof, Saeedeh Momtazi

**Updated**: 2025-08-01T14:46:57Z

**Summary**: As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field.

**Link**: [arxiv](http://arxiv.org/abs/2508.00673v1),  [pdf](http://arxiv.org/pdf/2508.00673v1)

**Tags**: cs.CL 



### Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement   Techniques and Applications
**Authors**: Wenxuan Wang, Zizhan Ma, Meidan Ding, Shiyi Zheng, Shengyuan Liu, Jie Liu, Jiaming Ji, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan

**Updated**: 2025-08-01T14:41:31Z

**Summary**: The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.

**Link**: [arxiv](http://arxiv.org/abs/2508.00669v1),  [pdf](http://arxiv.org/pdf/2508.00669v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG 



### IssueBench: Millions of Realistic Prompts for Measuring Issue Bias in   LLM Writing Assistance
**Authors**: Paul Röttger, Musashi Hinck, Valentin Hofmann, Kobi Hackenburg, Valentina Pyatkin, Faeze Brahman, Dirk Hovy

**Updated**: 2025-08-01T14:26:57Z

**Summary**: Large language models (LLMs) are helping millions of users write texts about diverse issues, and in doing so expose users to different ideas and perspectives. This creates concerns about issue bias, where an LLM tends to present just one perspective on a given issue, which in turn may influence how users think about this issue. So far, it has not been possible to measure which issue biases LLMs actually manifest in real user interactions, making it difficult to address the risks from biased LLMs. Therefore, we create IssueBench: a set of 2.49m realistic prompts for measuring issue bias in LLM writing assistance, which we construct based on 3.9k templates (e.g. "write a blog about") and 212 political issues (e.g. "AI regulation") from real user interactions. Using IssueBench, we show that issue biases are common and persistent in state-of-the-art LLMs. We also show that biases are remarkably similar across models, and that all models align more with US Democrat than Republican voter opinion on a subset of issues. IssueBench can easily be adapted to include other issues, templates, or tasks. By enabling robust and realistic measurement, we hope that IssueBench can bring a new quality of evidence to ongoing discussions about LLM biases and how to address them.

**Link**: [arxiv](http://arxiv.org/abs/2502.08395v2),  [pdf](http://arxiv.org/pdf/2502.08395v2)

**Tags**: cs.CL 



### Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN
**Authors**: Abdelaziz Salama, Zeinab Nezami, Mohammed M. H. Qazzaz, Maryam Hafeez, Syed Ali Raza Zaidi

**Updated**: 2025-08-01T14:25:34Z

**Summary**: The deployment of AI agents within legacy Radio Access Network (RAN) infrastructure poses significant safety and reliability challenges for future 6G networks. This paper presents a novel Edge AI framework for autonomous network optimisation in Open RAN environments, addressing these challenges through three core innovations: (1) a persona-based multi-tools architecture enabling distributed, context-aware decision-making; (2) proactive anomaly detection agent powered by traffic predictive tool; and (3) a safety, aligned reward mechanism that balances performance with operational stability. Integrated into the RAN Intelligent Controller (RIC), our framework leverages multimodal data fusion, including network KPIs, a traffic prediction model, and external information sources, to anticipate and respond to dynamic network conditions. Extensive evaluation using realistic 5G scenarios demonstrates that the edge framework achieves zero network outages under high-stress conditions, compared to 8.4% for traditional fixed-power networks and 3.3% for large language model (LLM) agent-based approaches, while maintaining near real-time responsiveness and consistent QoS. These results establish that, when equipped with the right tools and contextual awareness, AI agents can be safely and effectively deployed in critical network infrastructure, laying the framework for intelligent and autonomous 5G and beyond network operations.

**Link**: [arxiv](http://arxiv.org/abs/2507.21696v2),  [pdf](http://arxiv.org/pdf/2507.21696v2)

**Tags**: eess.SP 



### Un-mixing Test-time Adaptation under Heterogeneous Data Streams
**Authors**: Zixian Su, Jingwei Guo, Xi Yang, Qiufeng Wang, Kaizhu Huang

**Updated**: 2025-08-01T14:16:18Z

**Summary**: Deploying deep models in real-world scenarios remains challenging due to significant performance drops under distribution shifts between training and deployment environments. Test-Time Adaptation (TTA) has recently emerged as a promising solution, enabling on-the-fly model adaptation without access to source data. However, its effectiveness degrades significantly in the presence of complex, mixed distribution shifts - common in practical settings - where multiple latent domains coexist. Adapting under such intrinsic heterogeneity, especially in unlabeled and online conditions, remains an open and underexplored challenge. In this paper, we study TTA under mixed distribution shifts and move beyond conventional homogeneous adaptation paradigms. By revisiting TTA from a frequency-domain perspective, we observe that distribution heterogeneity often manifests in Fourier space - for instance, high-frequency components tend to carry domain-specific variations. This motivates us to perform domain-aware separation using high-frequency texture cues, making diverse shift patterns more tractable. To this end, we propose FreDA, a novel Frequency-based Decentralized Adaptation framework that decomposes globally heterogeneous data into locally homogeneous components in the frequency domain. It further employs decentralized learning and augmentation strategies to robustly adapt under complex, evolving shifts. Extensive experiments across various environments (corrupted, natural, and medical) demonstrate the superiority of our proposed framework over the state-of-the-arts.

**Link**: [arxiv](http://arxiv.org/abs/2411.15173v2),  [pdf](http://arxiv.org/pdf/2411.15173v2)

**Tags**: cs.LG cs.AI 



### Review: Adaptive Radiation Therapy for Head and Neck Cancer
**Authors**: Lucas McCullum, Sonali J. Joshi, Brandon M. Godinich, Parshawn Gerafian, Rishabh Gaur, Qusai Alakayleh, Ergys Subashi, Renjie He, Samuel L. Mulder, Zaphanlene Kaffey, Grace Murley, Natalie A. West, Saleh Ramezani, Cem Dede, Laia Humbert-Vidan, Clifton D. Fuller

**Updated**: 2025-08-01T14:12:43Z

**Summary**: The future of ART in head and neck cancer is just beginning. Novel technologies have pushed the boundary of what is possible in terms of techniques to identify biomarkers for adaptation as well as innovative devices specialized to respond to these adaptations, sometimes in real-time. Important interdisciplinary steps must be taken moving forward to ensure the safe deployment of these new techniques, such as rigorous quality assurance evaluations from medical physicists, clinical trials from physicians, and comprehensive testing from vendors prior to release. In summary, we aimed not to provide a single correct answer for the optimal implementation of ART in the era of imaging biomarkers, but to encourage the field to collaborate and bring each idea discussed here together to overcome current barriers and deliver the best treatment possible to the patient.

**Link**: [arxiv](http://arxiv.org/abs/2508.00651v1),  [pdf](http://arxiv.org/pdf/2508.00651v1)

**Tags**: physics.med-ph 



### Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings
**Authors**: Alexia Jolicoeur-Martineau

**Updated**: 2025-08-01T13:45:13Z

**Summary**: While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.   We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.   We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.   We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.00632v1),  [pdf](http://arxiv.org/pdf/2508.00632v1)

**Tags**: cs.AI cs.MA cs.MM 



### MCeT: Behavioral Model Correctness Evaluation using Large Language   Models
**Authors**: Khaled Ahmed, Jialing Song, Boqi Chen, Ou Wei, Bingzhou Zheng

**Updated**: 2025-08-01T13:41:58Z

**Summary**: Behavioral model diagrams, e.g., sequence diagrams, are an essential form of documentation that are typically designed by system engineers from requirements documentation, either fully manually or assisted by design tools. With the growing use of Large Language Models (LLM) as AI modeling assistants, more automation will be involved in generating diagrams. This necessitates the advancement of automatic model correctness evaluation tools. Such a tool can be used to evaluate both manually and AI automatically generated models; to provide feedback to system engineers, and enable AI assistants to self-evaluate and self-enhance their generated models.   In this paper, we propose MCeT, the first fully automated tool to evaluate the correctness of a behavioral model, sequence diagrams in particular, against its corresponding requirements text and produce a list of issues that the model has. We utilize LLMs for the correctness evaluation tasks as they have shown outstanding natural language understanding ability. However, we show that directly asking an LLM to compare a diagram to requirements finds less than 35% of issues that experienced engineers can find. We propose to supplement the direct check with a fine-grained, multi-perspective approach; we split the diagram into atomic, non-divisible interactions, and split the requirements text into atomic, self-contained items. We compare the diagram with atomic requirements and each diagram-atom with the requirements. We also propose a self-consistency checking approach that combines perspectives to mitigate LLM hallucinated issues. Our combined approach improves upon the precision of the direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover, the approach finds 90% more issues that the experienced engineers found than the direct approach, and reports an average of 6 new issues per diagram.

**Link**: [arxiv](http://arxiv.org/abs/2508.00630v1),  [pdf](http://arxiv.org/pdf/2508.00630v1)

**Tags**: cs.SE 



### Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight   Approach
**Authors**: Francisco Crespo, Javier Villegas, Carlos Baena, Eduardo Baena, Sergio Fortes, Raquel Barco

**Updated**: 2025-08-01T13:40:52Z

**Summary**: The transition toward softwarized Radio Access Networks (RANs), driven by the Open RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through disaggregation and virtualization of base station functions. However, this shift introduces new challenges in managing CPU resources efficiently under strict real-time constraints. In particular, the interplay between latency-sensitive RAN workloads and general-purpose Operating System (OS) schedulers often leads to sub-optimal performance and unnecessary energy consumption. This work proposes a lightweight, programmable distributed application (dApp) deployed at the Distributed Unit (DU) level to dynamically orchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging thread-level telemetry like context switches, Instructions Per Cycle (IPC), and cache metrics, to adapt CPU thread affinity, core isolation, and frequency scaling in real time. Unlike existing solutions, it requires no access to proprietary RAN software, hardware-specific features, or kernel modifications. Fully compliant with the O-RAN architecture and agnostic to the underlying RAN stack, the proposed solution introduces negligible overhead while improving energy efficiency and CPU utilization. Experimental results using a commercial-grade srsRAN deployment demonstrate consistent power savings without compromising real-time processing performance, highlighting the potential of low-latency dApps for fine-grained resource control in next-generation networks

**Link**: [arxiv](http://arxiv.org/abs/2508.00629v1),  [pdf](http://arxiv.org/pdf/2508.00629v1)

**Tags**: cs.NI cs.OS cs.PF 



### DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language   Models
**Authors**: Shantanu Thorat, Andrew Caines

**Updated**: 2025-08-01T13:28:01Z

**Summary**: Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. We rigorously examine the machine-learning procedure to build these detectors to address this. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example. In response, we introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations. We also include texts from domain-specific continued-pre-trained (CPT) language models, where we fully train all parameters using a memory-efficient optimization approach. Many existing AIG text detectors struggle significantly on our dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. We also train our own classifiers using two approaches: standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO). While BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. In our mock deployment scenario in student essay detection with an OOD student essay dataset, the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates for both. Our results indicate that DXO classifiers generalize better without overfitting to the test set. Our experiments highlight several areas of improvement for AIG text detectors.

**Link**: [arxiv](http://arxiv.org/abs/2508.00619v1),  [pdf](http://arxiv.org/pdf/2508.00619v1)

**Tags**: cs.CL cs.LG 



### Similarity-Based Self-Construct Graph Model for Predicting Patient   Criticalness Using Graph Neural Networks and EHR Data
**Authors**: Mukesh Kumar Sahu, Pinki Roy

**Updated**: 2025-08-01T13:25:04Z

**Summary**: Accurately predicting the criticalness of ICU patients (such as in-ICU mortality risk) is vital for early intervention in critical care. However, conventional models often treat each patient in isolation and struggle to exploit the relational structure in Electronic Health Records (EHR). We propose a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN architecture that operates on this graph to predict patient mortality and a continuous criticalness score. SBSCGM uses a hybrid similarity measure (combining feature-based and structural similarities) to connect patients with analogous clinical profiles in real-time. The HybridGraphMedGNN integrates Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT) layers to learn robust patient representations, leveraging both local and global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$) outperforming baseline classifiers and single-type GNN models. We also demonstrate improved precision/recall and show that the attention mechanism provides interpretable insights into model predictions. Our framework offers a scalable and interpretable solution for critical care risk prediction, with potential to support clinicians in real-world ICU deployment.

**Link**: [arxiv](http://arxiv.org/abs/2508.00615v1),  [pdf](http://arxiv.org/pdf/2508.00615v1)

**Tags**: cs.LG cs.AI 



### Prompting Science Report 3: I'll pay you or I'll kill you -- but will   you care?
**Authors**: Lennart Meincke, Ethan Mollick, Lilach Mollick, Dan Shapiro

**Updated**: 2025-08-01T13:23:21Z

**Summary**: This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that 'models tend to do better if you threaten them,' a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).   We demonstrate two things:   - Threatening or tipping a model generally has no significant effect on benchmark performance.   - Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLM's ability to answer any particular question.   Taken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.

**Link**: [arxiv](http://arxiv.org/abs/2508.00614v1),  [pdf](http://arxiv.org/pdf/2508.00614v1)

**Tags**: cs.CL cs.AI 



### SEFL: Enhancing Educational Assignment Feedback with LLM Agents
**Authors**: Mike Zhang, Amalie Pernille Dilling, Léon Gondelman, Niels Erik Ruan Lyngdorf, Euan D. Lindsay, Johannes Bjerva

**Updated**: 2025-08-01T13:19:46Z

**Summary**: Providing high-quality feedback to student assignments is crucial for student success, but it is constrained by time and costs. In this work, we introduce Synthetic Educational Feedback Loops (SEFL), a synthetic data framework designed to generate data that resembles immediate, on-demand feedback at scale without relying on extensive, real-world student assignments. To get this type of data, two large language models (LLMs) operate in teacher-student roles to simulate assignment completion and formative feedback, generating synthetic pairs of student work and corresponding critiques and actionable improvements from a teacher. With this data, we fine-tune smaller, more computationally efficient LLMs on these synthetic pairs, enabling them to replicate key features of high-quality, goal-oriented feedback. Unlike personalized tutoring approaches that offer multi-turn, individualized instruction, SEFL specifically focuses on replicating the teacher-student assignment feedback loop in higher education. Through comprehensive evaluations with four LLM judges and three human experts, we demonstrate that SEFL-tuned models outperform both their non-tuned counterparts in feedback quality and an existing baseline. The potential for societal impact is reinforced by extensive qualitative comments by ratings by human stakeholders -- both students and higher education instructors. All in all, SEFL has substantial potential to transform feedback processes for higher education and beyond.

**Link**: [arxiv](http://arxiv.org/abs/2502.12927v2),  [pdf](http://arxiv.org/pdf/2502.12927v2)

**Tags**: cs.CL 



### LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection   and Leakage Attacks
**Authors**: Francesco Panebianco, Stefano Bonfanti, Francesco Trovò, Michele Carminati

**Updated**: 2025-08-01T13:04:28Z

**Summary**: The generalization capabilities of Large Language Models (LLMs) have led to their widespread deployment across various applications. However, this increased adoption has introduced several security threats, notably in the forms of jailbreaking and data leakage attacks. Additionally, Retrieval Augmented Generation (RAG), while enhancing context-awareness in LLM responses, has inadvertently introduced vulnerabilities that can result in the leakage of sensitive information. Our contributions are twofold. First, we introduce a methodology to analyze historical interaction data from an LLM system, enabling the generation of usage maps categorized by topics (including adversarial interactions). This approach further provides forensic insights for tracking the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a model-agnostic framework that combines static analysis for forensic insights with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique identifies topic groups and detects anomalous patterns, allowing for proactive defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1) jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage, supported by a curated dataset of labeled LLM interactions. In the static setting, LeakSealer achieves the highest precision and recall on the ToxicChat dataset when identifying prompt injection. In the dynamic setting, PII leakage detection achieves an AUPRC of $0.97$, significantly outperforming baselines such as Llama Guard.

**Link**: [arxiv](http://arxiv.org/abs/2508.00602v1),  [pdf](http://arxiv.org/pdf/2508.00602v1)

**Tags**: cs.CR cs.AI cs.LG 



### Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback
**Authors**: Yiyuan Yang, Zichuan Liu, Lei Song, Kai Ying, Zhiguang Wang, Tom Bamford, Svitlana Vyetrenko, Jiang Bian, Qingsong Wen

**Updated**: 2025-08-01T13:03:05Z

**Summary**: Time series anomaly detection is critical across various domains, yet current approaches often limit analysis to mere binary anomaly classification without detailed categorization or further explanatory reasoning. To address these limitations, we propose a novel task, Time-series Reasoning for Anomaly (Time-RA) that transforms classical time series anomaly detection from a discriminative into a generative, reasoning-intensive task leveraging Large Language Models (LLMs). Also, we introduce the first real-world multimodal benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning, comprising approximately 40,000 samples across 10 real-world domains. Each sample includes numeric time series data, contextual text information, and visual representations, each annotated with fine-grained categories (14 types for univariate anomalies and 6 for multivariate anomalies) and structured explanatory reasoning. We develop a sophisticated annotation framework utilizing ensemble-generated labels refined through GPT-4-driven feedback, ensuring accuracy and interpretability. Extensive benchmarking of LLMs and multimodal LLMs demonstrates the capabilities and limitations of current models, highlighting the critical role of supervised fine-tuning. Our dataset and task pave the way for significant advancements in interpretable time series anomaly detection and reasoning. The code (https://github.com/yyysjz1997/Time-RA) and dataset (https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced to support and accelerate future research in this area.

**Link**: [arxiv](http://arxiv.org/abs/2507.15066v2),  [pdf](http://arxiv.org/pdf/2507.15066v2)

**Tags**: cs.LG cs.AI cs.MM 



### A Context-Aware Dual-Metric Framework for Confidence Estimation in Large   Language Models
**Authors**: Mingruo Yuan, Shuyi Zhang, Ben Kao

**Updated**: 2025-08-01T12:58:34Z

**Summary**: Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness, achieving the highest AUROC than existing baselines.

**Link**: [arxiv](http://arxiv.org/abs/2508.00600v1),  [pdf](http://arxiv.org/pdf/2508.00600v1)

**Tags**: cs.CL cs.LG 



### How Far Are AI Scientists from Changing the World?
**Authors**: Qiujie Xie, Yixuan Weng, Minjun Zhu, Fuchen Shen, Shulin Huang, Zhen Lin, Jiahui Zhou, Zilan Mao, Zijie Yang, Linyi Yang, Jian Wu, Yue Zhang

**Updated**: 2025-08-01T12:49:36Z

**Summary**: The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now taking the lead in scientific research. Several influential works have already appeared in the field of AI Scientist systems, with AI-generated research papers having been accepted at the ICLR 2025 workshop, suggesting that a human-level AI Scientist capable of uncovering phenomena previously unknown to humans, may soon become a reality. In this survey, we focus on the central question: How far are AI scientists from changing the world and reshaping the scientific research paradigm? To answer this question, we provide a prospect-driven review that comprehensively analyzes the current achievements of AI Scientist systems, identifying key bottlenecks and the critical components required for the emergence of a scientific agent capable of producing ground-breaking discoveries that solve grand challenges. We hope this survey will contribute to a clearer understanding of limitations of current AI Scientist systems, showing where we are, what is missing, and what the ultimate goals for scientific AI should be.

**Link**: [arxiv](http://arxiv.org/abs/2507.23276v2),  [pdf](http://arxiv.org/pdf/2507.23276v2)

**Tags**: cs.AI 



### From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated   Pre-Consultation Questionnaire Generation
**Authors**: Ruiqing Ding, Qianfang Sun, Yongkang Leng, Hui Yin, Xiaojian Li

**Updated**: 2025-08-01T12:24:49Z

**Summary**: Pre-consultation is a critical component of effective healthcare delivery. However, generating comprehensive pre-consultation questionnaires from complex, voluminous Electronic Medical Records (EMRs) is a challenging task. Direct Large Language Model (LLM) approaches face difficulties in this task, particularly regarding information completeness, logical order, and disease-level synthesis. To address this issue, we propose a novel multi-stage LLM-driven framework: Stage 1 extracts atomic assertions (key facts with timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes disease knowledge by clustering representative networks from an EMR corpus; Stage 3 generates tailored personal and standardized disease-specific questionnaires based on these structured representations. This framework overcomes limitations of direct methods by building explicit clinical knowledge. Evaluated on a real-world EMR dataset and validated by clinical experts, our method demonstrates superior performance in information coverage, diagnostic relevance, understandability, and generation time, highlighting its practical potential to enhance patient information collection.

**Link**: [arxiv](http://arxiv.org/abs/2508.00581v1),  [pdf](http://arxiv.org/pdf/2508.00581v1)

**Tags**: cs.AI 



### OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on   Planetary Rovers Using RGB, Depth, and Thermal Imagery
**Authors**: Raul Castilla-Arquillo, Carlos Perez-del-Pulgar, Levin Gerdes, Alfonso Garcia-Cerezo, Miguel A. Olivares-Mendez

**Updated**: 2025-08-01T12:23:29Z

**Summary**: Robot navigation in unstructured environments requires multimodal perception systems that can support safe navigation. Multimodality enables the integration of complementary information collected by different sensors. However, this information must be processed by machine learning algorithms specifically designed to leverage heterogeneous data. Furthermore, it is necessary to identify which sensor modalities are most informative for navigation in the target environment. In Martian exploration, thermal imagery has proven valuable for assessing terrain safety due to differences in thermal behaviour between soil types. This work presents OmniUnet, a transformer-based neural network architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T) imagery. A custom multimodal sensor housing was developed using 3D printing and mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a multimodal dataset in the Bardenas semi-desert in northern Spain. This location serves as a representative environment of the Martian surface, featuring terrain types such as sand, bedrock, and compact soil. A subset of this dataset was manually labeled to support supervised training of the network. The model was evaluated both quantitatively and qualitatively, achieving a pixel accuracy of 80.37% and demonstrating strong performance in segmenting complex unstructured terrain. Inference tests yielded an average prediction time of 673 ms on a resource-constrained computer (Jetson Orin Nano), confirming its suitability for on-robot deployment. The software implementation of the network and the labeled dataset have been made publicly available to support future research in multimodal terrain perception for planetary robotics.

**Link**: [arxiv](http://arxiv.org/abs/2508.00580v1),  [pdf](http://arxiv.org/pdf/2508.00580v1)

**Tags**: cs.RO cs.AI 



### MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for   Document Question-Answering with Hierarchical Index and Multi-Granularity   Retrieval
**Authors**: Ziyu Gong, Yihua Huang, Chengcheng Mai

**Updated**: 2025-08-01T12:22:53Z

**Summary**: The multi-modal long-context document question-answering task aims to locate and integrate multi-modal evidences (such as texts, tables, charts, images, and layouts) distributed across multiple pages, for question understanding and answer generation. The existing methods can be categorized into Large Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation (RAG)-based methods. However, the former were susceptible to hallucinations, while the latter struggled for inter-modal disconnection and cross-page fragmentation. To address these challenges, a novel multi-modal RAG model, named MMRAG-DocQA, was proposed, leveraging both textual and visual information across long-range pages to facilitate accurate question answering. A hierarchical indexing method with the integration of flattened in-page chunks and topological cross-page chunks was designed to jointly establish in-page multi-modal associations and long-distance cross-page dependencies. By means of joint similarity evaluation and large language model (LLM)-based re-ranking, a multi-granularity semantic retrieval method, including the page-level parent page retrieval and document-level summary retrieval, was proposed to foster multi-modal evidence connection and long-distance evidence integration and reasoning. Experimental results performed on public datasets, MMLongBench-Doc and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in understanding and answering modality-rich and multi-page documents.

**Link**: [arxiv](http://arxiv.org/abs/2508.00579v1),  [pdf](http://arxiv.org/pdf/2508.00579v1)

**Tags**: cs.MM cs.IR 



### Embracing Large Language Models in Traffic Flow Forecasting
**Authors**: Yusheng Zhao, Xiao Luo, Haomin Wen, Zhiping Xiao, Wei Ju, Ming Zhang

**Updated**: 2025-08-01T12:20:02Z

**Summary**: Traffic flow forecasting aims to predict future traffic flows based on the historical traffic conditions and the road network. It is an important problem in intelligent transportation systems, with a plethora of methods been proposed. Existing efforts mainly focus on capturing and utilizing spatio-temporal dependencies to predict future traffic flows. Though promising, they fall short in adapting to test-time environmental changes of traffic conditions. To tackle this challenge, we propose to introduce large language models (LLMs) to help traffic flow forecasting and design a novel method named Large Language Model Enhanced Traffic Flow Predictor (LEAF). LEAF adopts two branches, capturing different spatio-temporal relations using graph and hypergraph structures respectively. The two branches are first pre-trained individually, and during test-time, they yield different predictions. Based on these predictions, a large language model is used to select the most likely result. Then, a ranking loss is applied as the learning objective to enhance the prediction ability of the two branches. Extensive experiments on several datasets demonstrate the effectiveness of the proposed LEAF.

**Link**: [arxiv](http://arxiv.org/abs/2412.12201v2),  [pdf](http://arxiv.org/pdf/2412.12201v2)

**Tags**: cs.LG cs.AI 



### MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal   Interactions in Multimodal AI Models
**Authors**: Zhanliang Wang, Kai Wang

**Updated**: 2025-08-01T12:19:18Z

**Summary**: Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their "black-box" nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthiness are essential. How to explain cross-modal interactions in multimodal AI models remains a major challenge. While existing model explanation methods, such as attention map and Grad-CAM, offer coarse insights into cross-modal relationships, they cannot precisely quantify the synergistic effects between modalities, and are limited to open-source models with accessible internal weights. Here we introduce MultiSHAP, a model-agnostic interpretability framework that leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions between fine-grained visual and textual elements (such as image patches and text tokens), while being applicable to both open- and closed-source models. Our approach provides: (1) instance-level explanations that reveal synergistic and suppressive cross-modal effects for individual samples - "why the model makes a specific prediction on this input", and (2) dataset-level explanation that uncovers generalizable interaction patterns across samples - "how the model integrates information across modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP faithfully captures cross-modal reasoning mechanisms, while real-world case studies demonstrate its practical utility. Our framework is extensible beyond two modalities, offering a general solution for interpreting complex multimodal AI models.

**Link**: [arxiv](http://arxiv.org/abs/2508.00576v1),  [pdf](http://arxiv.org/pdf/2508.00576v1)

**Tags**: cs.AI 



### SynAdapt: Learning Adaptive Reasoning in Large Language Models via   Synthetic Continuous Chain-of-Thought
**Authors**: Jianwei Wang, Ziming Wu, Fuming Lai, Shaobing Lian, Ziqian Zeng

**Updated**: 2025-08-01T12:17:35Z

**Summary**: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs significant time costs due to the generation of discrete CoT tokens (DCoT). Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT methods are hampered by indirect fine-tuning, limited alignment, or inconsistent targets. To overcome these limitations, we propose \textit{SynAdapt}, an innovative efficient reasoning framework. Specifically, \textit{SynAdapt} generates the synthetic CCoT to serve as a precise and effective alignment target for LLMs. This synthetic CCoT explicitly guides the LLM to learn CCoT and derive accurate answers directly. Furthermore, relying solely on CCoT is insufficient for solving hard questions. To address this, \textit{SynAdapt} integrates a difficulty classifier that leverages both question context and CCoT to identify hard questions. CCoT can effectively help identify hard questions after some brief reasoning. We then adaptively prompt the LLM to re-think these hard questions for improved performance. Extensive experimental results across various benchmarks from different difficulty levels strongly demonstrate the effectiveness of our method, achieving the best accuracy-efficiency trade-off.

**Link**: [arxiv](http://arxiv.org/abs/2508.00574v1),  [pdf](http://arxiv.org/pdf/2508.00574v1)

**Tags**: cs.CL cs.AI 



### Session-Based Recommendation with Validated and Enriched LLM Intents
**Authors**: Gyuseok Lee, Yaokun Liu, Yifan Liu, Susik Yoon, Dong Wang, SeongKu Kang

**Updated**: 2025-08-01T12:11:10Z

**Summary**: Session-based recommendation (SBR) aims to predict the next item for an anonymous user in a timely manner. However, SBR suffers from data sparsity due to the short and anonymous nature of sessions. Recently, an emerging line of work has explored inferring the underlying user intents of a session using large language models (LLMs), with the generated intents serving as auxiliary training signals to enhance SBR models. Despite its promise, this approach faces three key challenges: validating intent quality, incorporating session-level multi-intents, and complementing inevitable LLM failure cases. In this paper, we propose VELI4SBR, a two-stage framework that leverages Validated and Enriched LLM-generated Intents for SBR. In the first stage, we generate high-quality intents using a predict-and-correct loop that validates the informativeness of LLM-generated intents with a global intent pool to constrain the LLM's output space and reduce hallucination. In the second stage, we enhance the SBR model using the generated intents through a lightweight multi-intent prediction and fusion mechanism. Furthermore, we introduce a training strategy that compensates for LLM failures by inferring intents from inter-session behavioral similarities. Extensive experiments show that VELI4SBR outperforms state-of-the-art baselines while improving explainability.

**Link**: [arxiv](http://arxiv.org/abs/2508.00570v1),  [pdf](http://arxiv.org/pdf/2508.00570v1)

**Tags**: cs.IR 



### ContestTrade: A Multi-Agent Trading System Based on Internal Contest   Mechanism
**Authors**: Li Zhao, Rui Sun, Zuoyou Jiang, Bo Yang, Yuxiao Bai, Mengting Chen, Xinyang Wang, Jing Li, Zuo Bai

**Updated**: 2025-08-01T11:48:13Z

**Summary**: In financial trading, large language model (LLM)-based agents demonstrate significant potential. However, the high sensitivity to market noise undermines the performance of LLM-based trading systems. To address this limitation, we propose a novel multi-agent system featuring an internal competitive mechanism inspired by modern corporate management structures. The system consists of two specialized teams: (1) Data Team - responsible for processing and condensing massive market data into diversified text factors, ensuring they fit the model's constrained context. (2) Research Team - tasked with making parallelized multipath trading decisions based on deep research methods. The core innovation lies in implementing a real-time evaluation and ranking mechanism within each team, driven by authentic market feedback. Each agent's performance undergoes continuous scoring and ranking, with only outputs from top-performing agents being adopted. The design enables the system to adaptively adjust to dynamic environment, enhances robustness against market noise and ultimately delivers superior trading performance. Experimental results demonstrate that our proposed system significantly outperforms prevailing multiagent systems and traditional quantitative investment methods across diverse evaluation metrics.

**Link**: [arxiv](http://arxiv.org/abs/2508.00554v1),  [pdf](http://arxiv.org/pdf/2508.00554v1)

**Tags**: q-fin.TR cs.CL q-fin.CP 



### DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable   Adversarial Purification
**Authors**: Chihan Huang, Belal Alsinglawi, Islam Al-qudah

**Updated**: 2025-08-01T11:47:36Z

**Summary**: Recent advances in deep neural networks (DNNs) have led to remarkable success across a wide range of tasks. However, their susceptibility to adversarial perturbations remains a critical vulnerability. Existing diffusion-based adversarial purification methods often require intensive iterative denoising, severely limiting their practical deployment. In this paper, we propose Diffusion Bridge Distillation for Purification (DBLP), a novel and efficient diffusion-based framework for adversarial purification. Central to our approach is a new objective, noise bridge distillation, which constructs a principled alignment between the adversarial noise distribution and the clean data distribution within a latent consistency model (LCM). To further enhance semantic fidelity, we introduce adaptive semantic enhancement, which fuses multi-scale pyramid edge maps as conditioning input to guide the purification process. Extensive experiments across multiple datasets demonstrate that DBLP achieves state-of-the-art (SOTA) robust accuracy, superior image quality, and around 0.2s inference time, marking a significant step toward real-time adversarial purification.

**Link**: [arxiv](http://arxiv.org/abs/2508.00552v1),  [pdf](http://arxiv.org/pdf/2508.00552v1)

**Tags**: cs.CV 



### AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through   Lightweight Vocabulary Adaptation
**Authors**: Itay Nakash, Nitay Calderon, Eyal Ben David, Elad Hoffer, Roi Reichart

**Updated**: 2025-08-01T10:53:31Z

**Summary**: Large Language Models (LLMs) have shown impressive versatility as general purpose models. However, their broad applicability comes at a high-cost computational overhead, particularly in auto-regressive decoding where each step requires a forward pass. In domain-specific settings, general-purpose capabilities are unnecessary and can be exchanged for efficiency. In this work, we take a novel perspective on domain adaptation, reducing latency and computational costs by adapting the vocabulary to focused domains of interest. We introduce AdaptiVocab, an end-to-end approach for vocabulary adaptation, designed to enhance LLM efficiency in low-resource domains. AdaptiVocab can be applied to any tokenizer and architecture, modifying the vocabulary by replacing tokens with domain-specific n-gram-based tokens, thereby reducing the number of tokens required for both input processing and output generation. AdaptiVocab initializes new n-token embeddings using an exponentially weighted combination of existing embeddings and employs a lightweight fine-tuning phase that can be efficiently performed on a single GPU. We evaluate two 7B LLMs across three niche domains, assessing efficiency, generation quality, and end-task performance. Our results show that AdaptiVocab reduces token usage by over 25% without compromising performance

**Link**: [arxiv](http://arxiv.org/abs/2503.19693v2),  [pdf](http://arxiv.org/pdf/2503.19693v2)

**Tags**: cs.CL 



### From LLMs to MLLMs to Agents: A Survey of Emerging Paradigms in   Jailbreak Attacks and Defenses within LLM Ecosystem
**Authors**: Yanxu Mao, Tiehan Cui, Peipei Liu, Datao You, Hongsong Zhu

**Updated**: 2025-08-01T10:42:39Z

**Summary**: Large language models (LLMs) are rapidly evolving from single-modal systems to multimodal LLMs and intelligent agents, significantly expanding their capabilities while introducing increasingly severe security risks. This paper presents a systematic survey of the growing complexity of jailbreak attacks and corresponding defense mechanisms within the expanding LLM ecosystem. We first trace the developmental trajectory from LLMs to MLLMs and Agents, highlighting the core security challenges emerging at each stage. Next, we categorize mainstream jailbreak techniques from both the attack impact and visibility perspectives, and provide a comprehensive analysis of representative attack methods, related datasets, and evaluation metrics. On the defense side, we organize existing strategies based on response timing and technical approach, offering a structured understanding of their applicability and implementation. Furthermore, we identify key limitations in existing surveys, such as insufficient attention to agent-specific security issues, the absence of a clear taxonomy for hybrid jailbreak methods, a lack of detailed analysis of experimental setups, and outdated coverage of recent advancements. To address these limitations, we provide an updated synthesis of recent work and outline future research directions in areas such as dataset construction, evaluation framework optimization, and strategy generalization. Our study seeks to enhance the understanding of jailbreak mechanisms and facilitate the advancement of more resilient and adaptive defense strategies in the context of ever more capable LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.15170v3),  [pdf](http://arxiv.org/pdf/2506.15170v3)

**Tags**: cs.CR 



### Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration   for Text-Attributed Graph Anomaly Detection
**Authors**: Yiming Xu, Jiarun Chen, Zhen Peng, Zihan Chen, Qika Lin, Lan Ma, Bin Shi, Bo Dong

**Updated**: 2025-08-01T10:36:39Z

**Summary**: The natural combination of intricate topological structures and rich textual information in text-attributed graphs (TAGs) opens up a novel perspective for graph anomaly detection (GAD). However, existing GAD methods primarily focus on designing complex optimization objectives within the graph domain, overlooking the complementary value of the textual modality, whose features are often encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so that semantic context related to anomalies may be missed. To unleash the enormous potential of textual modality, large language models (LLMs) have emerged as promising alternatives due to their strong semantic understanding and reasoning capabilities. Nevertheless, their application to TAG anomaly detection remains nascent, and they struggle to encode high-order structural information inherent in graphs due to input length constraints. For high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that combines LLMs and graph neural networks (GNNs) to leverage their complementary strengths. CoLL employs multi-LLM collaboration for evidence-augmented generation to capture anomaly-relevant contexts while delivering human-readable rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped with a gating mechanism to adaptively fuse textual features with evidence while preserving high-order topological information. Extensive experiments demonstrate the superiority of CoLL, achieving an average improvement of 13.37% in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

**Link**: [arxiv](http://arxiv.org/abs/2508.00507v1),  [pdf](http://arxiv.org/pdf/2508.00507v1)

**Tags**: cs.LG 



### Dynamic Knowledge Exchange and Dual-diversity Review: Concisely   Unleashing the Potential of a Multi-Agent Research Team
**Authors**: Weilun Yu, Shixiang Tang, Yonggui Huang, Nanqing Dong, Li Fan, Honggang Qi, Wei Liu, Xiaoli Diao, Xi Chen, Wanli Ouyang

**Updated**: 2025-08-01T10:35:50Z

**Summary**: Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.

**Link**: [arxiv](http://arxiv.org/abs/2506.18348v3),  [pdf](http://arxiv.org/pdf/2506.18348v3)

**Tags**: cs.AI 



### FFGAF-SNN: The Forward-Forward Based Gradient Approximation Free   Training Framework for Spiking Neural Networks
**Authors**: Changqing Xu, Ziqiang Yang, Yi Liu, Xinfang Liao, Guiqi Mo, Hao Zeng, Yintang Yang

**Updated**: 2025-08-01T10:34:09Z

**Summary**: Spiking Neural Networks (SNNs) offer a biologically plausible framework for energy-efficient neuromorphic computing. However, it is a challenge to train SNNs due to their non-differentiability, efficiently. Existing gradient approximation approaches frequently sacrifice accuracy and face deployment limitations on edge devices due to the substantial computational requirements of backpropagation. To address these challenges, we propose a Forward-Forward (FF) based gradient approximation-free training framework for Spiking Neural Networks, which treats spiking activations as black-box modules, thereby eliminating the need for gradient approximation while significantly reducing computational complexity. Furthermore, we introduce a class-aware complexity adaptation mechanism that dynamically optimizes the loss function based on inter-class difficulty metrics, enabling efficient allocation of network resources across different categories. Experimental results demonstrate that our proposed training framework achieves test accuracies of 99.58%, 92.13%, and 75.64% on the MNIST, Fashion-MNIST, and CIFAR-10 datasets, respectively, surpassing all existing FF-based SNN approaches. Additionally, our proposed method exhibits significant advantages in terms of memory access and computational power consumption.

**Link**: [arxiv](http://arxiv.org/abs/2507.23643v2),  [pdf](http://arxiv.org/pdf/2507.23643v2)

**Tags**: cs.CV 



### Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via   Probabilistic Model Checking
**Authors**: Haoyu Wang, Chris M. Poskitt, Jun Sun, Jiali Wei

**Updated**: 2025-08-01T10:24:47Z

**Summary**: Large Language Model (LLM) agents exhibit powerful autonomous capabilities across domains such as robotics, virtual assistants, and web automation. However, their stochastic behavior introduces significant safety risks that are difficult to anticipate. Existing rule-based enforcement systems, such as AgentSpec, focus on developing reactive safety rules, which typically respond only when unsafe behavior is imminent or has already occurred. These systems lack foresight and struggle with long-horizon dependencies and distribution shifts. To address these limitations, we propose Pro2Guard, a proactive runtime enforcement framework grounded in probabilistic reachability analysis. Pro2Guard abstracts agent behaviors into symbolic states and learns a Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it anticipates future risks by estimating the probability of reaching unsafe states, triggering interventions before violations occur when the predicted risk exceeds a user-defined threshold. By incorporating semantic validity checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability while approximating the underlying ground-truth model. We evaluate Pro2Guard extensively across two safety-critical domains: embodied household agents and autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early on up to 93.6% of unsafe tasks using low thresholds, while configurable modes (e.g., reflect) allow balancing safety with task success, maintaining up to 80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100% prediction of traffic law violations and collisions, anticipating risks up to 38.66 seconds ahead.

**Link**: [arxiv](http://arxiv.org/abs/2508.00500v1),  [pdf](http://arxiv.org/pdf/2508.00500v1)

**Tags**: cs.AI cs.SE 



### Linguistic Generalizability of Test-Time Scaling in Mathematical   Reasoning
**Authors**: Guijin Son, Jiwoo Hong, Hyunwoo Ko, James Thorne

**Updated**: 2025-08-01T10:09:29Z

**Summary**: Scaling pre-training compute has proven effective for achieving mulitlinguality, but does the same hold for test-time scaling? In this work, we introduce MCLM, a multilingual math benchmark featuring competition-level problems in 55 languages. We test three test-time scaling methods-Outcome Reward Modeling (ORM), Process Reward Modeling (ORM), and Budget Forcing (BF)-on both Qwen2.5-1.5B Math and MR1-1.5B, a multilingual LLM we trained for extended reasoning. Our experiments show that using Qwen2.5-1.5B Math with ORM achieves a score of 35.8 on MCLM, while BF on MR1-1.5B attains 35.2. Although "thinking LLMs" have recently garnered significant attention, we find that their performance is comparable to traditional scaling methods like best-of-N once constrained to similar levels of inference FLOPs. Moreover, while BF yields a 20-point improvement on English AIME, it provides only a 1.94-point average gain across other languages-a pattern consistent across the other test-time scaling methods we studied-higlighting that test-time scaling may not generalize as effectively to multilingual tasks. To foster further research, we release MCLM, MR1-1.5B, and evaluation results.

**Link**: [arxiv](http://arxiv.org/abs/2502.17407v2),  [pdf](http://arxiv.org/pdf/2502.17407v2)

**Tags**: cs.CL 



### IFEvalCode: Controlled Code Generation
**Authors**: Jian Yang, Wei Zhang, Shukai Liu, Linzheng Chai, Yingshui Tan, Jiaheng Liu, Ge Zhang, Wangchunshu Zhou, Guanglin Niu, Zhoujun Li, Binyuan Hui, Junyang Lin

**Updated**: 2025-08-01T10:07:37Z

**Summary**: Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions.

**Link**: [arxiv](http://arxiv.org/abs/2507.22462v2),  [pdf](http://arxiv.org/pdf/2507.22462v2)

**Tags**: cs.CL 



### Unlocking Multi-Modal Potentials for Link Prediction on Dynamic   Text-Attributed Graphs
**Authors**: Yuanyuan Xu, Wenjie Zhang, Ying Zhang, Xuemin Lin, Xiwei Xu

**Updated**: 2025-08-01T10:07:08Z

**Summary**: Dynamic Text-Attributed Graphs (DyTAGs) are a novel graph paradigm that captures evolving temporal events (edges) alongside rich textual attributes. Existing studies can be broadly categorized into TGNN-driven and LLM-driven approaches, both of which encode textual attributes and temporal structures for DyTAG representation. We observe that DyTAGs inherently comprise three distinct modalities: temporal, textual, and structural, often exhibiting completely disjoint distributions. However, the first two modalities are largely overlooked by existing studies, leading to suboptimal performance. To address this, we propose MoMent, a multi-modal model that explicitly models, integrates, and aligns each modality to learn node representations for link prediction. Given the disjoint nature of the original modality distributions, we first construct modality-specific features and encode them using individual encoders to capture correlations across temporal patterns, semantic context, and local structures. Each encoder generates modality-specific tokens, which are then fused into comprehensive node representations with a theoretical guarantee. To avoid disjoint subspaces of these heterogeneous modalities, we propose a dual-domain alignment loss that first aligns their distributions globally and then fine-tunes coherence at the instance level. This enhances coherent representations from temporal, textual, and structural views. Extensive experiments across seven datasets show that MoMent achieves up to 17.28% accuracy improvement and up to 31x speed-up against eight baselines.

**Link**: [arxiv](http://arxiv.org/abs/2502.19651v2),  [pdf](http://arxiv.org/pdf/2502.19651v2)

**Tags**: cs.LG cs.CL 



### Measurement-device-independent quantum key distribution with asymmetric   sources
**Authors**: Jia-Ju Deng, Feng-Yu Lu, Zhen-Qiu Zhong, Xiao-Hai Zhan, Zhen-Qiang Yin, Shuang Wang, Wei Chen, De-Yong He, Guang-Can Guo, Zheng-Fu Han

**Updated**: 2025-08-01T10:00:08Z

**Summary**: Measurement-device-independent quantum key distribution (MDI-QKD), which eliminates all the attacks from the eavesdropper to the measurement party, has been one of the most promising technology for the implementation of end-to-end quantum networks. In practice, the asymmetry of both sources and channels is generally inevitable. Therefore, we propose a theory to analyze the performance when any two MDI users in networks communicates using asymmetric sources in distinct single or multiple temporal modes. As a specific application, we model to obtain the key rate of MDI-QKD with weak coherent pulse source and spontaneous parametric down-conversion source, and compare the performance to the cases with symmetric (i.e. identical) sources. The result demonstrates that the actual performance does not degrade due to the asymmetry of sources. In contrary, it maintains at a good level over the entire distance we study. This work provides a theoretical basis for analyzing and optimizing MDI-QKD networks with asymmetric sources, and thus paving the way for the practical deployment of completely asymmetric MDI-QKD networks.

**Link**: [arxiv](http://arxiv.org/abs/2504.14614v4),  [pdf](http://arxiv.org/pdf/2504.14614v4)

**Tags**: quant-ph physics.optics 



### Sign Spotting Disambiguation using Large Language Models
**Authors**: JianHe Low, Ozge Mercanoglu Sincan, Richard Bowden

**Updated**: 2025-08-01T09:56:35Z

**Summary**: Sign spotting, the task of identifying and localizing individual signs within continuous sign language video, plays a pivotal role in scaling dataset annotations and addressing the severe data scarcity issue in sign language translation. While automatic sign spotting holds great promise for enabling frame-level supervision at scale, it grapples with challenges such as vocabulary inflexibility and ambiguity inherent in continuous sign streams. Hence, we introduce a novel, training-free framework that integrates Large Language Models (LLMs) to significantly enhance sign spotting quality. Our approach extracts global spatio-temporal and hand shape features, which are then matched against a large-scale sign dictionary using dynamic time warping and cosine similarity. This dictionary-based matching inherently offers superior vocabulary flexibility without requiring model retraining. To mitigate noise and ambiguity from the matching process, an LLM performs context-aware gloss disambiguation via beam search, notably without fine-tuning. Extensive experiments on both synthetic and real-world sign language datasets demonstrate our method's superior accuracy and sentence fluency compared to traditional approaches, highlighting the potential of LLMs in advancing sign spotting.

**Link**: [arxiv](http://arxiv.org/abs/2507.03703v2),  [pdf](http://arxiv.org/pdf/2507.03703v2)

**Tags**: cs.CV cs.AI 



### CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy   Optimization
**Authors**: Yuning Jiang, Nay Oo, Qiaoran Meng, Lu Lin, Dusit Niyato, Zehui Xiong, Hoon Wei Lim, Biplab Sikdar

**Updated**: 2025-08-01T09:53:06Z

**Summary**: Modern cyber attacks unfold through multiple stages, requiring defenders to dynamically prioritize mitigations under uncertainty. While game-theoretic models capture attacker-defender interactions, existing approaches often rely on static assumptions and lack integration with real-time threat intelligence, limiting their adaptability. This paper presents CyGATE, a game-theoretic framework modeling attacker-defender interactions, using large language models (LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber conflicts as a partially observable stochastic game (POSG) across Cyber Kill Chain stages. Both agents use belief states to navigate uncertainty, with the attacker adapting tactics and the defender re-prioritizing patches based on evolving risks and observed adversary behavior. The framework's flexible architecture enables extension to multi-agent scenarios involving coordinated attackers, collaborative defenders, or complex enterprise environments with multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE effectively prioritizes high-risk vulnerabilities, enhancing adaptability through dynamic threat integration, strategic foresight by anticipating attacker moves under uncertainty, and efficiency by optimizing resource use.

**Link**: [arxiv](http://arxiv.org/abs/2508.00478v1),  [pdf](http://arxiv.org/pdf/2508.00478v1)

**Tags**: cs.CR cs.AI 91A10, 91A43, 68T01, 94A60 C.2.0; I.2.6; K.6.5 



### RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs   through Multi-query Parallelism
**Authors**: Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu

**Updated**: 2025-08-01T09:41:05Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while LLMs remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have aimed to enhance models' search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to reliance on single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, with the aim of reducing inference time and enhancing the model's capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%.

**Link**: [arxiv](http://arxiv.org/abs/2507.02962v4),  [pdf](http://arxiv.org/pdf/2507.02962v4)

**Tags**: cs.CL cs.AI cs.IR 



### Thinking Machines: Mathematical Reasoning in the Age of LLMs
**Authors**: Andrea Asperti, Alberto Naibo, Claudio Sacerdoti Coen

**Updated**: 2025-08-01T09:31:48Z

**Summary**: Large Language Models (LLMs) have shown remarkable abilities in structured reasoning and symbolic tasks, with coding emerging as a particular area of strength. This success has sparked growing interest in applying LLMs to mathematics, both in informal problem-solving and formal theorem proving. However, progress in formal mathematics has proven to be significantly more difficult, despite surface-level similarities between programming and proof construction. This discrepancy raises important questions about how LLMs ``reason'', how they are supervised, and whether they internally track a notion of computational or deductive state. In this article, we address the state-of-the-art of the discipline, focusing on recent models and benchmarks, and explore three central issues at the intersection of machine learning and mathematical cognition: (i) the trade-offs between formal and informal mathematics as training domains; (ii) the deeper reasons why proof generation remains more brittle than code synthesis; (iii) and the question of whether LLMs represent, or merely mimic, a notion of evolving logical state. Our goal is not to draw hard boundaries, but to identify where the current limits lie, and how they might be extended.

**Link**: [arxiv](http://arxiv.org/abs/2508.00459v1),  [pdf](http://arxiv.org/pdf/2508.00459v1)

**Tags**: cs.AI 68T07, 68T20 I.2.6; I.2.7; I.2.3 



### Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges
**Authors**: Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding

**Updated**: 2025-08-01T09:26:01Z

**Summary**: Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the ``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2508.00454v1),  [pdf](http://arxiv.org/pdf/2508.00454v1)

**Tags**: cs.CL 



### When Relevance Meets Novelty: Dual-Stable Periodic Optimization for   Exploratory Recommendation
**Authors**: Hongxiang Lin, Hao Guo, Zeshun Li, Erpeng Xue, Yongqian He, Xiangyu Hou, Zhaoyu Hu, Lei Wang, Sheng Chen

**Updated**: 2025-08-01T09:10:56Z

**Summary**: Traditional recommendation systems tend to trap users in strong feedback loops by excessively pushing content aligned with their historical preferences, thereby limiting exploration opportunities and causing content fatigue. Although large language models (LLMs) demonstrate potential with their diverse content generation capabilities, existing LLM-enhanced dual-model frameworks face two major limitations: first, they overlook long-term preferences driven by group identity, leading to biased interest modeling; second, they suffer from static optimization flaws, as a one-time alignment process fails to leverage incremental user data for closed-loop optimization. To address these challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE) module, jointly modeling long-term group identity and short-term individual interests through parallel processing of behavioral sequences. For static optimization limitations, we design a Periodic Collaborative Optimization (PCO) mechanism. This mechanism regularly conducts preference verification on incremental data using the Relevance LLM, then guides the Novelty LLM to perform fine-tuning based on the verification results, and subsequently feeds back the output of the incrementally fine-tuned Novelty LLM to the Relevance LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization. Extensive online and offline experiments verify the effectiveness of the CoEA model in exploratory recommendation.

**Link**: [arxiv](http://arxiv.org/abs/2508.00450v1),  [pdf](http://arxiv.org/pdf/2508.00450v1)

**Tags**: cs.IR cs.AI 



### Leveraging Synthetic Data for Question Answering with Multilingual LLMs   in the Agricultural Domain
**Authors**: Rishemjit Kaur, Arshdeep Singh Bhankhar, Jashanpreet Singh Salh, Sudhir Rajput, Vidhi, Kashish Mahendra, Bhavika Berwal, Ritesh Kumar, Surangika Ranathunga

**Updated**: 2025-08-01T09:04:46Z

**Summary**: Enabling farmers to access accurate agriculture-related information in their native languages in a timely manner is crucial for the success of the agriculture field. Publicly available general-purpose Large Language Models (LLMs) typically offer generic agriculture advisories, lacking precision in local and multilingual contexts. Our study addresses this limitation by generating multilingual (English, Hindi, Punjabi) synthetic datasets from agriculture-specific documents from India and fine-tuning LLMs for the task of question answering (QA). Evaluation on human-created datasets demonstrates significant improvements in factuality, relevance, and agricultural consensus for the fine-tuned LLMs compared to the baseline counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2507.16974v2),  [pdf](http://arxiv.org/pdf/2507.16974v2)

**Tags**: cs.CL cs.AI I.2.7; J.m 



### Reducing the gap between general purpose data and aerial images in   concentrated solar power plants
**Authors**: M. A. Pérez-Cutiño, J. Valverde, J. Capitán, J. M. Díaz-Báñez

**Updated**: 2025-08-01T08:57:02Z

**Summary**: In the context of Concentrated Solar Power (CSP) plants, aerial images captured by drones present a unique set of challenges. Unlike urban or natural landscapes commonly found in existing datasets, solar fields contain highly reflective surfaces, and domain-specific elements that are uncommon in traditional computer vision benchmarks. As a result, machine learning models trained on generic datasets struggle to generalize to this setting without extensive retraining and large volumes of annotated data. However, collecting and labeling such data is costly and time-consuming, making it impractical for rapid deployment in industrial applications.   To address this issue, we propose a novel approach: the creation of AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By generating synthetic data that closely mimic real-world conditions, our objective is to facilitate pretraining of models before deployment, significantly reducing the need for extensive manual labeling. Our main contributions are threefold: (1) we introduce AerialCSP, a high-quality synthetic dataset for aerial inspection of CSP plants, providing annotated data for object detection and image segmentation; (2) we benchmark multiple models on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we demonstrate that pretraining on AerialCSP significantly improves real-world fault detection, particularly for rare and small defects, reducing the need for extensive manual labeling. AerialCSP is made publicly available at https://mpcutino.github.io/aerialcsp/.

**Link**: [arxiv](http://arxiv.org/abs/2508.00440v1),  [pdf](http://arxiv.org/pdf/2508.00440v1)

**Tags**: cs.CV cs.AI cs.RO 



### ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network
**Authors**: Minghao Guo, Xi Zhu, Jingyuan Huang, Kai Mei, Yongfeng Zhang

**Updated**: 2025-08-01T08:37:54Z

**Summary**: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based learning by propagating information among neighbor nodes via predefined aggregation mechanisms. However, such fixed schemes often suffer from two key limitations. First, they cannot handle the imbalance in node informativeness -- some nodes are rich in information, while others remain sparse. Second, predefined message passing primarily leverages local structural similarity while ignoring global semantic relationships across the graph, limiting the model's ability to capture distant but relevant information. We propose Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework that empowers each node with autonomous, node-level decision-making. Each node acts as an agent that independently plans its next action based on its internal memory, enabling node-level planning and adaptive message propagation. Additionally, retrieval-augmented generation (RAG) allows nodes to access semantically relevant content and build global relationships in the graph. ReaGAN achieves competitive performance under few-shot in-context settings using a frozen LLM backbone without fine-tuning, showcasing the potential of agentic planning and local-global retrieval in graph learning.

**Link**: [arxiv](http://arxiv.org/abs/2508.00429v1),  [pdf](http://arxiv.org/pdf/2508.00429v1)

**Tags**: cs.CL cs.LG cs.MA 



### Automated Type Annotation in Python Using Large Language Models
**Authors**: Varun Bharti, Shashwat Jha, Dhruv Kumar, Pankaj Jalote

**Updated**: 2025-08-01T08:24:14Z

**Summary**: Type annotations in Python enhance maintainability and error detection. However, generating these annotations manually is error prone and requires extra effort. Traditional automation approaches like static analysis, machine learning, and deep learning struggle with limited type vocabularies, behavioral over approximation, and reliance on large labeled datasets. In this work, we explore the use of LLMs for generating type annotations in Python. We develop a generate check repair pipeline: the LLM proposes annotations guided by a Concrete Syntax Tree representation, a static type checker (Mypy) verifies them, and any errors are fed back for iterative refinement. We evaluate four LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini (reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark. We first measure the proportion of code snippets annotated by LLMs for which MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini, and O4Mini each reached approximately 88.6% consistency (around 11.4% failures). To measure annotation quality, we then compute exact-match and base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini perform the best, achieving up to 70.5% exact match and 79.1% base type accuracy, requiring under one repair iteration on average. Our results demonstrate that general-purpose and reasoning optimized LLMs, without any task specific fine tuning or additional training can be effective in generating consistent type annotations.They perform competitively with traditional deep learning techniques which require large labeled dataset for training. While our work focuses on Python, the pipeline can be extended to other optionally typed imperative languages like Ruby

**Link**: [arxiv](http://arxiv.org/abs/2508.00422v1),  [pdf](http://arxiv.org/pdf/2508.00422v1)

**Tags**: cs.PL cs.LG 



### radio-llava: Advancing Vision-Language Models for Radio Astronomical   Source Analysis
**Authors**: S. Riggi, T. Cecconello, A. Pilzer, S. Palazzo, N. Gupta, A. M. Hopkins, C. Trigilio, G. Umana

**Updated**: 2025-08-01T08:21:27Z

**Summary**: The advent of next-generation radio telescopes is set to transform radio astronomy by producing massive data volumes that challenge traditional processing methods. Deep learning techniques have shown strong potential in automating radio analysis tasks, yet are often constrained by the limited availability of large annotated datasets. Recent progress in self-supervised learning has led to foundational radio vision models, but adapting them for new tasks typically requires coding expertise, limiting their accessibility to a broader astronomical community. Text-based AI interfaces offer a promising alternative by enabling task-specific queries and example-driven learning. In this context, Large Language Models (LLMs), with their remarkable zero-shot capabilities, are increasingly used in scientific domains. However, deploying large-scale models remains resource-intensive, and there is a growing demand for AI systems that can reason over both visual and textual data in astronomical analysis. This study explores small-scale Vision-Language Models (VLMs) as AI assistants for radio astronomy, combining LLM capabilities with vision transformers. We fine-tuned the LLaVA VLM on a dataset of 59k radio images from multiple surveys, enriched with 38k image-caption pairs from the literature. The fine-tuned models show clear improvements over base models in radio-specific tasks, achieving ~30% F1-score gains in extended source detection, but they underperform vision-only classifiers and exhibit ~20% drop on general multimodal tasks. Inclusion of caption data and LoRA fine-tuning enhances instruction-following and helps recover ~10% accuracy on multimodal benchmarks. This work lays the foundation for future advancements in radio VLMs, highlighting their potential and limitations, such as the need for better multimodal alignment, higher-quality datasets, and mitigation of catastrophic forgetting.

**Link**: [arxiv](http://arxiv.org/abs/2503.23859v3),  [pdf](http://arxiv.org/pdf/2503.23859v3)

**Tags**: astro-ph.IM 



### Loop Invariant Generation: A Hybrid Framework of Reasoning optimised   LLMs and SMT Solvers
**Authors**: Varun Bharti, Shashwat Jha, Dhruv Kumar, Pankaj Jalote

**Updated**: 2025-08-01T08:15:15Z

**Summary**: Loop invariants are essential for proving the correctness of programs with loops. Developing loop invariants is challenging, and fully automatic synthesis cannot be guaranteed for arbitrary programs. Some approaches have been proposed to synthesize loop invariants using symbolic techniques and more recently using neural approaches. These approaches are able to correctly synthesize loop invariants only for subsets of standard benchmarks. In this work, we investigate whether modern, reasoning-optimized large language models can do better. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled generate-and-check pipeline with the Z3 SMT solver, using solver counterexamples to iteratively guide invariant refinement. We use Code2Inv benchmark, which provides C programs along with their formal preconditions and postconditions. On this benchmark of 133 tasks, our framework achieves 100% coverage (133 out of 133), outperforming the previous best of 107 out of 133, while requiring only 1-2 model proposals per instance and 14-55 seconds of wall-clock time. These results demonstrate that LLMs possess latent logical reasoning capabilities which can help automate loop invariant synthesis. While our experiments target C-specific programs, this approach should be generalizable to other imperative languages.

**Link**: [arxiv](http://arxiv.org/abs/2508.00419v1),  [pdf](http://arxiv.org/pdf/2508.00419v1)

**Tags**: cs.LO cs.LG cs.PL 



### Sortblock: Similarity-Aware Feature Reuse for Diffusion Model
**Authors**: Hanqi Chen, Xu Zhang, Xiaoliu Guan, Lielin Jiang, Guanzhong Wang, Zeyu Chen, Yi Liu

**Updated**: 2025-08-01T08:10:54Z

**Summary**: Diffusion Transformers (DiTs) have demonstrated remarkable generative capabilities, particularly benefiting from Transformer architectures that enhance visual and artistic fidelity. However, their inherently sequential denoising process results in high inference latency, limiting their deployment in real-time scenarios. Existing training-free acceleration approaches typically reuse intermediate features at fixed timesteps or layers, overlooking the evolving semantic focus across denoising stages and Transformer blocks.To address this, we propose Sortblock, a training-free inference acceleration framework that dynamically caches block-wise features based on their similarity across adjacent timesteps. By ranking the evolution of residuals, Sortblock adaptively determines a recomputation ratio, selectively skipping redundant computations while preserving generation quality. Furthermore, we incorporate a lightweight linear prediction mechanism to reduce accumulated errors in skipped blocks.Extensive experiments across various tasks and DiT architectures demonstrate that Sortblock achieves over 2$\times$ inference speedup with minimal degradation in output quality, offering an effective and generalizable solution for accelerating diffusion-based generative models.

**Link**: [arxiv](http://arxiv.org/abs/2508.00412v1),  [pdf](http://arxiv.org/pdf/2508.00412v1)

**Tags**: cs.CV 



### Co-Reward: Self-supervised Reinforcement Learning for Large Language   Model Reasoning via Contrastive Agreement
**Authors**: Zizhuo Zhang, Jianing Zhu, Xinmu Ge, Zihua Zhao, Zhanke Zhou, Xuan Li, Xiao Feng, Jiangchao Yao, Bo Han

**Updated**: 2025-08-01T08:09:14Z

**Summary**: Although reinforcement learning with verifiable rewards (RLVR) shows promise in improving the reasoning ability of large language models (LLMs), the scaling up dilemma remains due to the reliance on human annotated labels especially for complex tasks. Recent alternatives that explore various self-reward signals exhibit the eliciting potential of LLM reasoning, but suffer from the non-negligible collapse issue. Inspired by the success of self-supervised learning, we propose \textit{Co-Reward}, a novel RL framework that leverages contrastive agreement across semantically analogical questions as a reward basis. Specifically, we construct a similar question for each training sample (without labels) and synthesize their individual surrogate labels through a simple rollout voting, and then the reward is constructed by cross-referring the labels of each question pair to enforce the internal reasoning consistency across analogical inputs. Intuitively, such a self-supervised reward-shaping mechanism increases the difficulty of learning collapse into a trivial solution, and promotes stable reasoning elicitation and improvement through expanding the input sample variants. Empirically, Co-Reward achieves superior performance compared to other self-reward baselines on multiple reasoning benchmarks and LLM series, and reaches or even surpasses ground-truth (GT) labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward on Llama-3.2-3B-Instruct. Our code is publicly available at https://github.com/tmlr-group/Co-Reward.

**Link**: [arxiv](http://arxiv.org/abs/2508.00410v1),  [pdf](http://arxiv.org/pdf/2508.00410v1)

**Tags**: cs.LG 



### Benchmarking LLMs for Unit Test Generation from Real-World Functions
**Authors**: Dong Huang, Jie M. Zhang, Mark Harman, Qianru Zhang, Mingzhe Du, See-Kiong Ng

**Updated**: 2025-08-01T08:08:26Z

**Summary**: Recently, large language models (LLMs) have shown great promise in automating unit test generation, significantly reducing the manual effort required by developers. To effectively evaluate the capabilities of LLMs in this domain, it is crucial to have a well-designed benchmark that accurately reflects real-world scenarios and mitigates common pitfalls. Existing LLM test generation benchmarks are limited by two critical drawbacks: data contamination and structurally simple function code. As a result, we often cannot rely on the validity of scientific conclusions drawn from empirical studies using these limited benchmarks. The empirical evidence presented may be biased due to contamination and may fail to generalize beyond toy programs due to structural simplicity.   To address these problems, we introduce ULT (UnLeakedTestbench), a new benchmark specifically designed for function-level unit test generation from real-world Python functions. ULT is constructed through a multi-stage curation process that ensures high cyclomatic complexity and mitigates test case contamination. With 3,909 carefully selected function-level tasks, ULT provides a more realistic and challenging evaluation of LLMs' test generation capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT with leaked tests designed to enable a controlled analysis of memorization versus reasoning in test generation. Our evaluation results demonstrate that ULT is significantly more challenging. For example, test cases generated by LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy, statement coverage, branch coverage, and mutation score on average for all LLMs, respectively. These results are substantially lower than the corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

**Link**: [arxiv](http://arxiv.org/abs/2508.00408v1),  [pdf](http://arxiv.org/pdf/2508.00408v1)

**Tags**: cs.SE cs.CL 



### Novice Developers' Perspectives on Adopting LLMs for Software   Development: A Systematic Literature Review
**Authors**: Samuel Ferino, Rashina Hoda, John Grundy, Christoph Treude

**Updated**: 2025-08-01T07:38:59Z

**Summary**: Following the rise of large language models (LLMs), many studies have emerged in recent years focusing on exploring the adoption of LLM-based tools for software development by novice developers: computer science/software engineering students and early-career industry developers with two years or less of professional experience. These studies have sought to understand the perspectives of novice developers on using these tools, a critical aspect of the successful adoption of LLMs in software engineering. To systematically collect and summarise these studies, we conducted a systematic literature review (SLR) following the guidelines by Kitchenham et al. on 80 primary studies published between April 2022 and June 2025 to answer four research questions (RQs). In answering RQ1, we categorised the study motivations and methodological approaches. In RQ2, we identified the software development tasks for which novice developers use LLMs. In RQ3, we categorised the advantages, challenges, and recommendations discussed in the studies. Finally, we discuss the study limitations and future research needs suggested in the primary studies in answering RQ4. Throughout the paper, we also indicate directions for future work and implications for software engineering researchers, educators, and developers. Our research artifacts are publicly available at https://github.com/Samuellucas97/SupplementaryInfoPackage-SLR.

**Link**: [arxiv](http://arxiv.org/abs/2503.07556v2),  [pdf](http://arxiv.org/pdf/2503.07556v2)

**Tags**: cs.SE cs.AI 



### Multi-Layer Attention is the Amplifier of Demonstration Effectiveness
**Authors**: Dingzirui Wang, Xuangliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng

**Updated**: 2025-08-01T07:26:39Z

**Summary**: Numerous studies have investigated the underlying mechanisms of in-context learning (ICL) effectiveness to inspire the design of related methods. However, existing work predominantly assumes the effectiveness of the demonstrations provided within ICL, while many research indicates that not all demonstrations are effective, failing to yielding any performance improvement during ICL. Therefore, in this paper, we investigate the reasons behind demonstration ineffectiveness. Our analysis is based on gradient flow and linear self-attention models. By setting the gradient flow to zero, we deduce that a demonstration becomes ineffective if its information has either been learned by the model or is irrelevant to the user query. Furthermore, we demonstrate that in multi-layer models, the disparity in effectiveness among demonstrations is amplified with layer increasing, causing the model to focus more on effective ones. Considering that current demonstration selection methods primarily focus on the relevance to the user query while overlooking the information that the model has already assimilated, we propose a novel method called GradS, which leverages gradient flow for demonstration selection. We use the magnitude of the gradient flow of the demonstration with respect to a given user query as the criterion, thereby ensuring the effectiveness of the chosen ones. We validate our derivation and GradS on four prominent LLMs across five mainstream datasets. The experimental results confirm that the disparity in effectiveness among demonstrations is magnified as the model layer increases, substantiating our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on average over the strongest baselines, demonstrating its effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2508.00385v1),  [pdf](http://arxiv.org/pdf/2508.00385v1)

**Tags**: cs.CL cs.LG 



### On Learning Closed-Loop Probabilistic Multi-Agent Simulator
**Authors**: Juanwu Lu, Rohit Gupta, Ahmadreza Moradipari, Kyungtae Han, Ruqi Zhang, Ziran Wang

**Updated**: 2025-08-01T07:25:59Z

**Summary**: The rapid iteration of autonomous vehicle (AV) deployments leads to increasing needs for building realistic and scalable multi-agent traffic simulators for efficient evaluation. Recent advances in this area focus on closed-loop simulators that enable generating diverse and interactive scenarios. This paper introduces Neural Interactive Agents (NIVA), a probabilistic framework for multi-agent simulation driven by a hierarchical Bayesian model that enables closed-loop, observation-conditioned simulation through autoregressive sampling from a latent, finite mixture of Gaussian distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence trajectory prediction models and emerging closed-loop simulation models trained on Next-token Prediction (NTP) from a Bayesian inference perspective. Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains competitive performance compared to the existing method while providing embellishing control over intentions and driving styles.

**Link**: [arxiv](http://arxiv.org/abs/2508.00384v1),  [pdf](http://arxiv.org/pdf/2508.00384v1)

**Tags**: cs.RO 



### Advancing Welding Defect Detection in Maritime Operations via   Adapt-WeldNet and Defect Detection Interpretability Analysis
**Authors**: Kamal Basha S, Athira Nambiar

**Updated**: 2025-08-01T07:19:23Z

**Summary**: Weld defect detection is crucial for ensuring the safety and reliability of piping systems in the oil and gas industry, especially in challenging marine and offshore environments. Traditional non-destructive testing (NDT) methods often fail to detect subtle or internal defects, leading to potential failures and costly downtime. Furthermore, existing neural network-based approaches for defect classification frequently rely on arbitrarily selected pretrained architectures and lack interpretability, raising safety concerns for deployment. To address these challenges, this paper introduces ``Adapt-WeldNet", an adaptive framework for welding defect detection that systematically evaluates various pre-trained architectures, transfer learning strategies, and adaptive optimizers to identify the best-performing model and hyperparameters, optimizing defect detection and providing actionable insights. Additionally, a novel Defect Detection Interpretability Analysis (DDIA) framework is proposed to enhance system transparency. DDIA employs Explainable AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific evaluations validated by certified ASNT NDE Level II professionals. Incorporating a Human-in-the-Loop (HITL) approach and aligning with the principles of Trustworthy AI, DDIA ensures the reliability, fairness, and accountability of the defect detection system, fostering confidence in automated decisions through expert validation. By improving both performance and interpretability, this work enhances trust, safety, and reliability in welding defect detection systems, supporting critical operations in offshore and marine environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.00381v1),  [pdf](http://arxiv.org/pdf/2508.00381v1)

**Tags**: cs.CV cs.AI cs.CE cs.LG 



### EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level   Efficiency for Edge Devices
**Authors**: Jiyu Chen, Poh Seng Lim, Shuang Peng, Daxiong Luo, JungHau Foo, Yap Deep, Timothy Lee Jun Jie, Kelvin Teh Kae Wen, Fan Yang, Danyu Feng, Hao-Yun Chen, Peng-Wen Chen, Fangyuan Li, Xiaoxin Chen, Wong Wai Mun

**Updated**: 2025-08-01T07:03:16Z

**Summary**: Deploying Transformer-based large language models (LLMs) on resource-constrained edge devices for long-sequence tasks remains challenging due to the quadratic time complexity of self-attention and growing Key-Value (KV) cache demands. While existing KV cache optimizations improve memory efficiency, they often fail to reduce time to first token (TTFT) and may degrade performance through token pruning. Alternative sequence modeling architectures address some of these limitations, but typically require full retraining and lack infrastructure support. EdgeInfinite offers an efficient solution by fine-tuning only a small subset of parameters, maintaining quality while reducing both computational and memory costs, including improved TTFT. However, its instruction-following ability is limited, and it lacks mobile-specific optimizations. To address these issues, we propose EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning (S-SFT) strategy tailored to long-sequence tasks such as summarization and question answering. We further optimized EdgeInfinite-Instruct for efficient deployment on edge NPUs by employing fine-grained post-training quantization (PTQ) to reduce computational demands while maintaining accuracy, and by implementing a fixed-shape computation graph that balances memory usage and on-device efficiency through scenario-specific customization of input token and cache sizes. Experiments on long-context benchmarks and real-world mobile tasks show that our approach improves domain-specific performance while maintaining efficiency on NPU-accelerated edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2508.00370v1),  [pdf](http://arxiv.org/pdf/2508.00370v1)

**Tags**: cs.CL cs.LG 



### Quantum Knowledge Distillation for Large Language Models
**Authors**: Lingxiao Li, Yihao Wang, Jiacheng Fan, Jing Li, Sujuan Qin, Qiaoyan Wen, Fei Gao

**Updated**: 2025-08-01T06:53:55Z

**Summary**: As foundational tools in natural language processing, Large Language Models (LLMs) have immense parameter scales, which makes deployment and inference increasingly prohibitive, especially in resource-constrained devices. Therefore, knowledge distillation for LLMs, i.e., compressing the LLM to a smaller model, is meaningful. With strong parameter representation capacity, quantum computing is regarded as a promising solution. Here, we propose a Quantum knowledge Distillation model for LLMs (QD-LLM) that leverages variational quantum circuits to learn from LLMs. In classical simulation, QD-LLM outperforms several mainstream distillation methods on multiple text classification tasks in terms of both accuracy and efficiency using only 11 qubits. The results reveal an interesting phenomenon that the simulation of quantum student models may be regarded as a new class of quantum-inspired classical algorithms. Remarkably, we deploy the obtained circuits on the Baihua superconducting quantum processor via the Quafu platform to assess practical feasibility. The model maintains stable inference performance despite hardware constraints such as decoherence and finite sampling. In summary, QD-LLM marks a foundational step in connecting quantum computing with LLMs, demonstrating the feasibility of quantum-native approaches that aim to compress and deploy models of increasingly larger scales. The code of this article has been open-sourced at https://github.com/Lilingxiao-bupt/QD-LLM.

**Link**: [arxiv](http://arxiv.org/abs/2505.13205v2),  [pdf](http://arxiv.org/pdf/2505.13205v2)

**Tags**: quant-ph 



### Meta CLIP 2: A Worldwide Scaling Recipe
**Authors**: Yung-Sung Chuang, Yang Li, Dong Wang, Ching-Feng Yeh, Kehan Lyu, Ramya Raghavendra, James Glass, Lifei Huang, Jason Weston, Luke Zettlemoyer, Xinlei Chen, Zhuang Liu, Saining Xie, Wen-tau Yih, Shang-Wen Li, Hu Xu

**Updated**: 2025-08-01T06:40:13Z

**Summary**: Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., "curse of multilinguality" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2507.22062v3),  [pdf](http://arxiv.org/pdf/2507.22062v3)

**Tags**: cs.CV cs.CL 



### AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic   Schema Induction from Web-Scale Corpora
**Authors**: Jiaxin Bai, Wei Fan, Qi Hu, Qing Zong, Chunyang Li, Hong Ting Tsang, Hongyu Luo, Yauwai Yim, Haoyu Huang, Xiao Zhou, Feng Qin, Tianshi Zheng, Xi Peng, Xin Yao, Huiwen Yang, Leijie Wu, Yi Ji, Gong Zhang, Renhai Chen, Yangqiu Song

**Updated**: 2025-08-01T06:39:45Z

**Summary**: We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas. Our system leverages large language models to simultaneously extract knowledge triples and induce comprehensive schemas directly from text, modeling both entities and events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And Schema induction), a family of knowledge graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms state-of-the-art baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our schema induction achieves 92\% semantic alignment with human-crafted schemas with zero manual intervention, demonstrating that billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.

**Link**: [arxiv](http://arxiv.org/abs/2505.23628v3),  [pdf](http://arxiv.org/pdf/2505.23628v3)

**Tags**: cs.CL cs.AI 



### ORFS-agent: Tool-Using Agents for Chip Design Optimization
**Authors**: Amur Ghose, Andrew B. Kahng, Sayak Kundu, Zhiang Wang

**Updated**: 2025-08-01T06:36:42Z

**Summary**: Machine learning has been widely used to optimize complex engineering workflows across numerous domains. In the context of integrated circuit design, modern flows (e.g., going from a register-transfer level netlist to physical layouts) involve extensive configuration via thousands of parameters, and small changes to these parameters can have large downstream impacts on desired outcomes - namely design performance, power, and area. Recent advances in Large Language Models (LLMs) offer new opportunities for learning and reasoning within such high-dimensional optimization tasks. In this work, we introduce ORFS-agent, an LLM-based iterative optimization agent that automates parameter tuning in an open-source hardware design flow. ORFS-agent adaptively explores parameter configurations, demonstrating clear improvements over standard Bayesian optimization approaches in terms of resource efficiency and final design metrics. Our empirical evaluations on two different technology nodes and a range of circuit benchmarks indicate that ORFS-agent can improve both routed wirelength and effective clock period by over 13%, all while using 40% fewer optimization iterations. Moreover, by following natural language objectives to trade off certain metrics for others, ORFS-agent demonstrates a flexible and interpretable framework for multi-objective optimization. Crucially, RFS-agent is modular and model-agnostic, and can be plugged in to any frontier LLM without any further fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2506.08332v2),  [pdf](http://arxiv.org/pdf/2506.08332v2)

**Tags**: cs.AI 



### Debunking with Dialogue? Exploring AI-Generated Counterspeech to   Challenge Conspiracy Theories
**Authors**: Mareike Lisker, Christina Gottschalk, Helena Mihaljević

**Updated**: 2025-08-01T06:19:26Z

**Summary**: Counterspeech is a key strategy against harmful online content, but scaling expert-driven efforts is challenging. Large Language Models (LLMs) present a potential solution, though their use in countering conspiracy theories is under-researched. Unlike for hate speech, no datasets exist that pair conspiracy theory comments with expert-crafted counterspeech. We address this gap by evaluating the ability of GPT-4o, Llama 3, and Mistral to effectively apply counterspeech strategies derived from psychological research provided through structured prompts. Our results show that the models often generate generic, repetitive, or superficial results. Additionally, they over-acknowledge fear and frequently hallucinate facts, sources, or figures, making their prompt-based use in practical applications problematic.

**Link**: [arxiv](http://arxiv.org/abs/2504.16604v2),  [pdf](http://arxiv.org/pdf/2504.16604v2)

**Tags**: cs.CL cs.AI cs.SI I.2.7 



### PilotRL: Training Language Model Agents via Global Planning-Guided   Progressive Reinforcement Learning
**Authors**: Keer Lu, Chong Chen, Bin Cui, Huang Leng, Wentao Zhang

**Updated**: 2025-08-01T06:17:11Z

**Summary**: Large Language Models (LLMs) have shown remarkable advancements in tackling agent-oriented tasks. Despite their potential, existing work faces challenges when deploying LLMs in agent-based environments. The widely adopted agent paradigm ReAct centers on integrating single-step reasoning with immediate action execution, which limits its effectiveness in complex tasks requiring long-term strategic planning. Furthermore, the coordination between the planner and executor during problem-solving is also a critical factor to consider in agent design. Additionally, current approaches predominantly rely on supervised fine-tuning, which often leads models to memorize established task completion trajectories, thereby restricting their generalization ability when confronted with novel problem contexts. To address these challenges, we introduce an adaptive global plan-based agent paradigm AdaPlan, aiming to synergize high-level explicit guidance with execution to support effective long-horizon decision-making. Based on the proposed paradigm, we further put forward PilotRL, a global planning-guided training framework for LLM agents driven by progressive reinforcement learning. We first develop the model's ability to follow explicit guidance from global plans when addressing agent tasks. Subsequently, based on this foundation, we focus on optimizing the quality of generated plans. Finally, we conduct joint optimization of the model's planning and execution coordination. Experiments indicate that PilotRL could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78% comparing to GPT-4o-mini at a comparable parameter scale.

**Link**: [arxiv](http://arxiv.org/abs/2508.00344v1),  [pdf](http://arxiv.org/pdf/2508.00344v1)

**Tags**: cs.CL 



### Can LLMs Generate Tabular Summaries of Science Papers? Rethinking the   Evaluation Protocol
**Authors**: Weiqi Wang, Jiefu Ou, Yangqiu Song, Benjamin Van Durme, Daniel Khashabi

**Updated**: 2025-08-01T06:12:35Z

**Summary**: Literature review tables are essential for summarizing and comparing collections of scientific papers. We explore the task of generating tables that best fulfill a user's informational needs given a collection of scientific papers. Building on recent work (Newman et al., 2024), we extend prior approaches to address real-world complexities through a combination of LLM-based methods and human annotations. Our contributions focus on three key challenges encountered in real-world use: (i) User prompts are often under-specified; (ii) Retrieved candidate papers frequently contain irrelevant content; and (iii) Task evaluation should move beyond shallow text similarity techniques and instead assess the utility of inferred tables for information-seeking tasks (e.g., comparing papers). To support reproducible evaluation, we introduce ARXIV2TABLE, a more realistic and challenging benchmark for this task, along with a novel approach to improve literature review table generation in real-world scenarios. Our extensive experiments on this benchmark show that both open-weight and proprietary LLMs struggle with the task, highlighting its difficulty and the need for further advancements. Our dataset and code are available at https://github.com/JHU-CLSP/arXiv2Table.

**Link**: [arxiv](http://arxiv.org/abs/2504.10284v3),  [pdf](http://arxiv.org/pdf/2504.10284v3)

**Tags**: cs.CL 



### Lost in Benchmarks? Rethinking Large Language Model Benchmarking with   Item Response Theory
**Authors**: Hongli Zhou, Hui Huang, Ziqing Zhao, Lvyuan Han, Huicheng Wang, Kehai Chen, Muyun Yang, Wei Bao, Jian Dong, Bing Xu, Conghui Zhu, Hailong Cao, Tiejun Zhao

**Updated**: 2025-08-01T05:56:21Z

**Summary**: The evaluation of large language models (LLMs) via benchmarks is widespread, yet inconsistencies between different leaderboards and poor separability among top models raise concerns about their ability to accurately reflect authentic model capabilities. This paper provides a critical analysis of benchmark effectiveness, examining mainstream prominent LLM benchmarks using results from diverse models. We first propose Pseudo-Siamese Network for Item Response Theory (PSN-IRT), an enhanced Item Response Theory framework that incorporates a rich set of item parameters within an IRT-grounded architecture. PSN-IRT can be utilized for accurate and reliable estimations of item characteristics and model abilities. Based on PSN-IRT, we conduct extensive analysis on 11 LLM benchmarks comprising 41,871 items, revealing significant and varied shortcomings in their measurement quality. Furthermore, we demonstrate that leveraging PSN-IRT is able to construct smaller benchmarks while maintaining stronger alignment with human preference.

**Link**: [arxiv](http://arxiv.org/abs/2505.15055v2),  [pdf](http://arxiv.org/pdf/2505.15055v2)

**Tags**: cs.CL 



### Socrates or Smartypants: Testing Logic Reasoning Capabilities of Large   Language Models with Logic Programming-based Test Oracles
**Authors**: Zihao Xu, Junchen Ding, Yiling Lou, Kun Zhang, Dong Gong, Yuekang Li

**Updated**: 2025-08-01T05:43:41Z

**Summary**: Large Language Models (LLMs) have achieved significant progress in language understanding and reasoning. Evaluating and analyzing their logical reasoning abilities has therefore become essential. However, existing datasets and benchmarks are often limited to overly simplistic, unnatural, or contextually constrained examples. In response to the growing demand, we introduce SmartyPat-Bench, a challenging, naturally expressed, and systematically labeled benchmark derived from real-world high-quality Reddit posts containing subtle logical fallacies. Unlike existing datasets and benchmarks, it provides more detailed annotations of logical fallacies and features more diverse data. To further scale up the study and address the limitations of manual data collection and labeling - such as fallacy-type imbalance and labor-intensive annotation - we introduce SmartyPat, an automated framework powered by logic programming-based oracles. SmartyPat utilizes Prolog rules to systematically generate logically fallacious statements, which are then refined into fluent natural-language sentences by LLMs, ensuring precise fallacy representation. Extensive evaluation demonstrates that SmartyPat produces fallacies comparable in subtlety and quality to human-generated content and significantly outperforms baseline methods. Finally, experiments reveal nuanced insights into LLM capabilities, highlighting that while excessive reasoning steps hinder fallacy detection accuracy, structured reasoning enhances fallacy categorization performance.

**Link**: [arxiv](http://arxiv.org/abs/2504.12312v2),  [pdf](http://arxiv.org/pdf/2504.12312v2)

**Tags**: cs.CL 



### From Patient Burdens to User Agency: Designing for Real-Time Protection   Support in Online Health Consultations
**Authors**: Shuning Zhang, Ying Ma, Yongquan `Owen' Hu, Ting Dang, Hong Jia, Xin Yi, Hewu Li

**Updated**: 2025-08-01T05:21:42Z

**Summary**: Online medical consultation platforms, while convenient, are undermined by significant privacy risks that erode user trust. We first conducted in-depth semi-structured interviews with 12 users to understand their perceptions of security and privacy landscapes on online medical consultation platforms, as well as their practices, challenges and expectation. Our analysis reveals a critical disconnect between users' desires for anonymity and control, and platform realities that offload the responsibility of ``privacy labor''. To bridge this gap, we present SafeShare, an interaction technique that leverages localized LLM to redact consultations in real-time. SafeShare balances utility and privacy through selectively anonymize private information. A technical evaluation of SafeShare's core PII detection module on 3 dataset demonstrates high efficacy, achieving 89.64\% accuracy with Qwen3-4B on IMCS21 dataset.

**Link**: [arxiv](http://arxiv.org/abs/2508.00328v1),  [pdf](http://arxiv.org/pdf/2508.00328v1)

**Tags**: cs.HC 



### Evaluating the Efficacy of Large Language Models for Generating   Fine-Grained Visual Privacy Policies in Homes
**Authors**: Shuning Zhang, Ying Ma, Xin Yi, Hewu Li

**Updated**: 2025-08-01T05:11:29Z

**Summary**: The proliferation of visual sensors in smart home environments, particularly through wearable devices like smart glasses, introduces profound privacy challenges. Existing privacy controls are often static and coarse-grained, failing to accommodate the dynamic and socially nuanced nature of home environments. This paper investigates the viability of using Large Language Models (LLMs) as the core of a dynamic and adaptive privacy policy engine. We propose a conceptual framework where visual data is classified using a multi-dimensional schema that considers data sensitivity, spatial context, and social presence. An LLM then reasons over this contextual information to enforce fine-grained privacy rules, such as selective object obfuscation, in real-time. Through a comparative evaluation of state-of-the-art Vision Language Models (including GPT-4o and the Qwen-VL series) in simulated home settings , our findings show the feasibility of this approach. The LLM-based engine achieved a top machine-evaluated appropriateness score of 3.99 out of 5, and the policies generated by the models received a top human-evaluated score of 4.00 out of 5.

**Link**: [arxiv](http://arxiv.org/abs/2508.00321v1),  [pdf](http://arxiv.org/pdf/2508.00321v1)

**Tags**: cs.HC 



### Enhanced Vision-Language Models for Diverse Sensor Understanding:   Cost-Efficient Optimization and Benchmarking
**Authors**: Sangyun Chung, Youngjoon Yu, Se Yeon Kim, Youngchae Chee, Yong Man Ro

**Updated**: 2025-08-01T05:09:21Z

**Summary**: Large-scale Vision-Language Models (VLMs) have achieved notable progress in aligning visual inputs with text. However, their ability to deeply understand the unique physical properties of non-RGB vision sensor images remains limited. In this paper, we revisit and analyze these limitations and introduce a novel, cost-efficient paradigm that significantly advances sensor image understanding-without requiring extensive training data or any modifications to the existing VLM architectures. Specifically, we propose Sensor-Aware Attributes Fine-Tuning (SAFT) with the Diverse Negative Attributes (DNA) optimization, which leverages minimal sensor-specific data to enable robust learning of non-RGB characteristics and overcome RGB-centric biases inherent in current VLMs. In addition, we present VS-TDX-the first comprehensive, public benchmark designed to rigorously evaluate VLMs' sensor-specific understanding across diverse and realistic scenarios. Through extensive experiments on VLMs and various sensor modalities, we validate that our method consistently delivers superior performance and generalization under resource-constrained and architecture-invariant settings. Our approach provides a practical advance towards scalable deployment of VLMs in increasingly sensor-diverse real-world environments.

**Link**: [arxiv](http://arxiv.org/abs/2412.20750v2),  [pdf](http://arxiv.org/pdf/2412.20750v2)

**Tags**: cs.CV 



### Rethinking Pan-sharpening: Principled Design, Unified Training, and a   Universal Loss Surpass Brute-Force Scaling
**Authors**: Ran Zhang, Xuanhua He, Li Xueheng, Ke Cao, Liu Liu, Wenbo Xu, Fang Jiabin, Yang Qize, Jie Zhang

**Updated**: 2025-08-01T05:06:11Z

**Summary**: The field of pan-sharpening has recently seen a trend towards increasingly large and complex models, often trained on single, specific satellite datasets. This approach, however, leads to high computational overhead and poor generalization on full resolution data, a paradigm we challenge in this paper. In response to this issue, we propose PanTiny, a lightweight, single-step pan-sharpening framework designed for both efficiency and robust performance. More critically, we introduce multiple-in-one training paradigm, where a single, compact model is trained simultaneously on three distinct satellite datasets (WV2, WV3, and GF2) with different resolution and spectral information. Our experiments show that this unified training strategy not only simplifies deployment but also significantly boosts generalization on full-resolution data. Further, we introduce a universally powerful composite loss function that elevates the performance of almost all of models for pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny model, benefiting from these innovations, achieves a superior performance-to-efficiency balance, outperforming most larger, specialized models. Through extensive ablation studies, we validate that principled engineering in model design, training paradigms, and loss functions can surpass brute-force scaling. Our work advocates for a community-wide shift towards creating efficient, generalizable, and data-conscious models for pan-sharpening. The code is available at https://github.com/Zirconium233/PanTiny .

**Link**: [arxiv](http://arxiv.org/abs/2507.15059v2),  [pdf](http://arxiv.org/pdf/2507.15059v2)

**Tags**: cs.CV 



### OR-LLM-Agent: Automating Modeling and Solving of Operations Research   Optimization Problems with Reasoning LLM
**Authors**: Bowen Zhang, Pengcheng Luo, Genke Yang, Boon-Hee Soong, Chau Yuen

**Updated**: 2025-08-01T04:52:21Z

**Summary**: With the rise of artificial intelligence (AI), applying large language models (LLMs) to mathematical problem-solving has attracted increasing attention. Most existing approaches attempt to improve Operations Research (OR) optimization problem-solving through prompt engineering or fine-tuning strategies for LLMs. However, these methods are fundamentally constrained by the limited capabilities of non-reasoning LLMs. To overcome these limitations, we propose OR-LLM-Agent, an AI agent framework built on reasoning LLMs for automated OR problem solving. The framework decomposes the task into three sequential stages: mathematical modeling, code generation, and debugging. Each task is handled by a dedicated sub-agent, which enables more targeted reasoning. We also construct BWOR, an OR dataset for evaluating LLM performance on OR tasks. Our analysis shows that in the benchmarks NL4OPT, MAMO, and IndustryOR, reasoning LLMs sometimes underperform their non-reasoning counterparts within the same model family. In contrast, BWOR provides a more consistent and discriminative assessment of model capabilities. Experimental results demonstrate that OR-LLM-Agent utilizing DeepSeek-R1 in its framework outperforms advanced methods, including GPT-o3, Gemini 2.5 Pro, DeepSeek-R1, and ORLM, by at least 7\% in accuracy. These results demonstrate the effectiveness of task decomposition for OR problem solving.

**Link**: [arxiv](http://arxiv.org/abs/2503.10009v3),  [pdf](http://arxiv.org/pdf/2503.10009v3)

**Tags**: cs.AI math.OC 



### Do Large Language Models Know How Much They Know?
**Authors**: Gabriele Prato, Jerry Huang, Prasanna Parthasarathi, Shagun Sodhani, Sarath Chandar

**Updated**: 2025-08-01T04:33:49Z

**Summary**: Large Language Models (LLMs) have emerged as highly capable systems and are increasingly being integrated into various uses. However, the rapid pace of their deployment has outpaced a comprehensive understanding of their internal mechanisms and a delineation of their capabilities and limitations. A desired attribute of an intelligent system is its ability to recognize the scope of its own knowledge. To investigate whether LLMs embody this characteristic, we develop a benchmark designed to challenge these models to enumerate all information they possess on specific topics. This benchmark evaluates whether the models recall excessive, insufficient, or the precise amount of information, thereby indicating their awareness of their own knowledge. Our findings reveal that all tested LLMs, given sufficient scale, demonstrate an understanding of how much they know about specific topics. While different architectures exhibit varying rates of this capability's emergence, the results suggest that awareness of knowledge may be a generalizable attribute of LLMs. Further research is needed to confirm this potential and fully elucidate the underlying mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2502.19573v2),  [pdf](http://arxiv.org/pdf/2502.19573v2)

**Tags**: cs.CL cs.AI cs.LG 



