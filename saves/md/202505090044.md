# Arxiv Results
## Keyword: kv cache 
 ### Comparing CPU and GPU compute of PERMANOVA on MI300A
**Authors**: Igor Sfiligoi

**Updated**: 2025-05-07T16:44:21Z

**Summary**: Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is often challenging, due to the drastically different memory subsystems on host CPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both CPU and GPU cores in a single package, all backed by the same type of HBM memory. In this paper we analyze the performance of Permutational Multivariate Analysis of Variance (PERMANOVA), a non-parametric method that tests whether two or more groups of objects are significantly different based on a categorical factor. This method is memory-bound and has been recently optimized for CPU cache locality. Our tests show that GPU cores on the MI300A prefer the brute force approach instead, significantly outperforming the CPU-based implementation. The significant benefit of Simultaneous Multithreading (SMT) was also a pleasant surprise.

**Link**: [arxiv](http://arxiv.org/abs/2505.04556v1),  [pdf](http://arxiv.org/pdf/2505.04556v1)

**Tags**: cs.DC cs.PF q-bio.QM 



### Securing Immersive 360 Video Streams through Attribute-Based Selective   Encryption
**Authors**: Mohammad Waquas Usmani, Susmit Shannigrahi, Michael Zink

**Updated**: 2025-05-07T14:37:13Z

**Summary**: Delivering high-quality, secure 360{\deg} video content introduces unique challenges, primarily due to the high bitrates and interactive demands of immersive media. Traditional HTTPS-based methods, although widely used, face limitations in computational efficiency and scalability when securing these high-resolution streams. To address these issues, this paper proposes a novel framework integrating Attribute-Based Encryption (ABE) with selective encryption techniques tailored specifically for tiled 360{\deg} video streaming. Our approach employs selective encryption of frames at varying levels to reduce computational overhead while ensuring robust protection against unauthorized access.   Moreover, we explore viewport-adaptive encryption, dynamically encrypting more frames within tiles occupying larger portions of the viewer's field of view. This targeted method significantly enhances security in critical viewing areas without unnecessary overhead in peripheral regions. We deploy and evaluate our proposed approach using the CloudLab testbed, comparing its performance against traditional HTTPS streaming. Experimental results demonstrate that our ABE-based model achieves reduced computational load on intermediate caches, improves cache hit rates, and maintains comparable visual quality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).

**Link**: [arxiv](http://arxiv.org/abs/2505.04466v1),  [pdf](http://arxiv.org/pdf/2505.04466v1)

**Tags**: cs.MM cs.CR eess.IV 



### LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders
**Authors**: Zheng Chai, Qin Ren, Xijun Xiao, Huizhi Yang, Bo Han, Sijun Zhang, Di Chen, Hui Lu, Wenlin Zhao, Lele Yu, Xionghang Xie, Shiru Ren, Xiang Sun, Yaocheng Tan, Peng Xu, Yuchao Zheng, Di Wu

**Updated**: 2025-05-07T13:54:26Z

**Summary**: Modeling ultra-long user behavior sequences is critical for capturing both long- and short-term preferences in industrial recommender systems. Existing solutions typically rely on two-stage retrieval or indirect modeling paradigms, incuring upstream-downstream inconsistency and computational inefficiency. In this paper, we present LONGER, a Long-sequence Optimized traNsformer for GPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism for stabilizing attention over long contexts, (ii) a token merge module with lightweight InnerTransformers and hybrid attention strategy to reduce quadratic complexity, and (iii) a series of engineering optimizations, including training with mixed-precision and activation recomputation, KV cache serving, and the fully synchronous model training and serving framework for unified GPU-based dense and sparse parameter updates. LONGER consistently outperforms strong baselines in both offline metrics and online A/B testing in both advertising and e-commerce services at ByteDance, validating its consistent effectiveness and industrial-level scaling laws. Currently, LONGER has been fully deployed at more than 10 influential scenarios at ByteDance, serving billion users.

**Link**: [arxiv](http://arxiv.org/abs/2505.04421v1),  [pdf](http://arxiv.org/pdf/2505.04421v1)

**Tags**: cs.IR 



### Rehearsal-Free Continual Federated Learning with Synergistic Synaptic   Intelligence
**Authors**: Yichen Li, Yuying Wang, Haozhao Wang, Yining Qi, Tianzhe Xiao, Ruixuan Li

**Updated**: 2025-05-07T13:07:25Z

**Summary**: Continual Federated Learning (CFL) allows distributed devices to collaboratively learn novel concepts from continuously shifting training data while avoiding knowledge forgetting of previously seen tasks. To tackle this challenge, most current CFL approaches rely on extensive rehearsal of previous data. Despite effectiveness, rehearsal comes at a cost to memory, and it may also violate data privacy. Considering these, we seek to apply regularization techniques to CFL by considering their cost-efficient properties that do not require sample caching or rehearsal. Specifically, we first apply traditional regularization techniques to CFL and observe that existing regularization techniques, especially synaptic intelligence, can achieve promising results under homogeneous data distribution but fail when the data is heterogeneous. Based on this observation, we propose a simple yet effective regularization algorithm for CFL named FedSSI, which tailors the synaptic intelligence for the CFL with heterogeneous data settings. FedSSI can not only reduce computational overhead without rehearsal but also address the data heterogeneity issue. Extensive experiments show that FedSSI achieves superior performance compared to state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2412.13779v2),  [pdf](http://arxiv.org/pdf/2412.13779v2)

**Tags**: cs.LG cs.DC 



### Design and Evaluation of an NDN-Based Network for Distributed Digital   Twins
**Authors**: Chen Chen, Zihan Jia, Ze Wang, Lin Cui, Fung Po Tso

**Updated**: 2025-05-07T11:21:12Z

**Summary**: Digital twins (DT) have received significant attention due to their numerous benefits, such as real-time data analytics and cost reduction in production. DT serves as a fundamental component of many applications, encompassing smart manufacturing, intelligent vehicles, and smart cities. By using Machine Learning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently facilitate decision-making and productivity by simulating the status and changes of a physical entity. To handle the massive amount of data brought by DTs, it is challenging to achieve low response latency for data fetching over existing IP-based networks. IP-based networks use host addresses for end-to-end communication, making data distribution between DTs inefficient. Thus, we propose to use DTs in a distributed manner over Named Data Networking (NDN) networks. NDN is data-centric where data is routed based on content names, dynamically adjusting paths to optimize latency. Popular data is cached in network nodes, reducing data transmission and network congestion. Since data is fetched by content names, users and mobile devices can move freely without IP address reassignment. By using in-network caching and adaptive routing, we reckon NDN is an ideal fit for Future G Networks in the context of Digital Twins. We compared DTs in edge scenarios with cloud scenarios over NDN and IP-based networks to validate our insights. Extensive simulation results show that using DT in the edge reduces response latency by 10.2x. This position paper represents an initial investigation into the gap in distributed DTs over NDN, serving as an early-stage study.

**Link**: [arxiv](http://arxiv.org/abs/2505.04326v1),  [pdf](http://arxiv.org/pdf/2505.04326v1)

**Tags**: cs.NI 



### Computational Model for Photoionization in Pure SF6 Streamer
**Authors**: Zihao Feng

**Updated**: 2025-05-07T08:10:39Z

**Summary**: Photoionization plays a crucial role in achieving spatial numerical convergence and accurate quantitative predictions in SF6 streamer simulations, but accurate models for SF6 photoionization remains limited, motivating this paper. First, we develop a computational model for SF6 photoionization and provide the detailed modeling process. Then, we perform comparative studies against simplified approaches. The results demonstrate that the proposed model effectively captures the non-local effects of SF6 photoionization, enhancing both the spatial numerical convergence and the accuracy of the streamer structure. Finally, we perform comparative studies by artificially increasing the photoionization intensity through multiplying the photoionization source term Sph by a factor of 10 (10*Sph) relative to the baseline intensity. Regarding breakdown voltage prediction, 10*Sph leads to a significant underestimation of the breakdown voltage for positive streamers, introducing errors greater than 0.5 kV, while exerting a relatively small impact on negative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the contraction at the positive streamer head and significantly lowers the local field by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph has little impact on the morphology of the negative streamers and slightly enhances the local field by less than 200 Td, thereby consistently accelerating its propagation.

**Link**: [arxiv](http://arxiv.org/abs/2505.04216v1),  [pdf](http://arxiv.org/pdf/2505.04216v1)

**Tags**: physics.plasm-ph 



### Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer   Gate
**Authors**: Zhiyuan Fang, Zicong Hong, Yuegui Huang, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, Zibin Zheng

**Updated**: 2025-05-07T07:57:21Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.

**Link**: [arxiv](http://arxiv.org/abs/2502.12224v2),  [pdf](http://arxiv.org/pdf/2502.12224v2)

**Tags**: cs.AI cs.LG 



### Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes   in the Agave Validator
**Authors**: Turan Vural, Yuki Yuminaga, Alex Petrosyan, Ben Livshits

**Updated**: 2025-05-07T05:00:10Z

**Summary**: In this paper we analyze some of the bottlenecks in the execution pipeline of Solana's Agave validator client, focusing on RAM and program cache usage under mainnet conditions. Through a series of controlled experiments, we measure the validator's throughput and resource efficiency as RAM availability ranges between 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance degrades significantly below 256 GB, with transaction processing falling behind real-time block production. Additionally, we study the program cache behavior, identifying inefficiencies in program eviction and load latency. Our results provide practical guidance for hardware provisioning and suggest improvements to the Solana execution and caching strategy, reducing latency due to the program cache by 90%.

**Link**: [arxiv](http://arxiv.org/abs/2505.04129v1),  [pdf](http://arxiv.org/pdf/2505.04129v1)

**Tags**: cs.DC 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-05-07T01:29:10Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty, and only those elements are processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a cache-friendlier priority queue algorithm that avoids accessing auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, and animation. Moreover, thanks to numerous low-level optimizations, Spineless Traversal is competitive across the whole spectrum of incremental layout workloads. Spineless Traversal is faster than the standard approach on 83.0% of 2216 benchmarks, with a mean speedup of 1.80x concentrated in the most latency-critical interactions.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v6),  [pdf](http://arxiv.org/pdf/2411.10659v6)

**Tags**: cs.PL 



### Reciprocating Locks
**Authors**: Dave Dice, Alex Kogan

**Updated**: 2025-05-06T19:28:56Z

**Summary**: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.

**Link**: [arxiv](http://arxiv.org/abs/2501.02380v6),  [pdf](http://arxiv.org/pdf/2501.02380v6)

**Tags**: cs.DC D.4.1 



### Cobra: Efficient Line Art COlorization with BRoAder References
**Authors**: Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan

**Updated**: 2025-05-06T15:23:12Z

**Summary**: The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.

**Link**: [arxiv](http://arxiv.org/abs/2504.12240v3),  [pdf](http://arxiv.org/pdf/2504.12240v3)

**Tags**: cs.CV 



### RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM   Inference
**Authors**: Yaoqi Chen, Jinkai Zhang, Baotong Lu, Qianxi Zhang, Chengruidong Zhang, Jingjia Luo, Di Liu, Huiqiang Jiang, Qi Chen, Jing Liu, Bailu Ding, Xiao Yan, Jiawei Jiang, Chen Chen, Mingxing Zhang, Yuqing Yang, Fan Yang, Mao Yang

**Updated**: 2025-05-05T18:01:17Z

**Summary**: The growing context lengths of large language models (LLMs) pose significant challenges for efficient inference, primarily due to GPU memory and bandwidth constraints. We present RetroInfer, a novel system that reconceptualizes the key-value (KV) cache as a vector storage system which exploits the inherent attention sparsity to accelerate long-context LLM inference. At its core is the wave index, an Attention-aWare VEctor index that enables efficient and accurate retrieval of critical tokens through techniques such as tripartite attention approximation, accuracy-bounded attention estimation, and segmented clustering. Complementing this is the wave buffer, which coordinates KV cache placement and overlaps computation and data transfer across GPU and CPU to sustain high throughput. Unlike prior sparsity-based methods that struggle with token selection and hardware coordination, RetroInfer delivers robust performance without compromising model accuracy. Experiments on long-context benchmarks show up to 4.5X speedup over full attention within GPU memory limits and up to 10.5X over sparse attention baselines when KV cache is extended to CPU memory, all while preserving full-attention-level accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2505.02922v1),  [pdf](http://arxiv.org/pdf/2505.02922v1)

**Tags**: cs.LG 



### Large Language Model Partitioning for Low-Latency Inference at the Edge
**Authors**: Dimitrios Kafetzis, Ramin Khalili, Iordanis Koutsopoulos

**Updated**: 2025-05-05T10:16:16Z

**Summary**: Large Language Models (LLMs) based on autoregressive, decoder-only Transformers generate text one token at a time, where a token represents a discrete unit of text. As each newly produced token is appended to the partial output sequence, the length grows and so does the memory and compute load, due to the expanding key-value caches, which store intermediate representations of all previously generated tokens in the multi-head attention (MHA) layer. As this iterative process steadily increases memory and compute demands, layer-based partitioning in resource-constrained edge environments often results in memory overload or high inference latency. To address this and reduce inference latency, we propose a resource-aware Transformer architecture partitioning algorithm, where the partitioning decision is updated at regular intervals during token generation. The approach is myopic in that it is based on instantaneous information about device resource availability and network link bandwidths. When first executed, the algorithm places blocks on devices, and in later executions, it migrates these blocks among devices so that the sum of migration delay and inference delay remains low. Our approach partitions the decoder at the attention head level, co-locating each attention head with its key-value cache and allowing dynamic migrations whenever resources become tight. By allocating different attention heads to different devices, we exploit parallel execution of attention heads and thus achieve substantial reductions in inference delays. Our experiments show that in small-scale settings (3-5 devices), the proposed method achieves within 15 to 20 percent of an exact optimal solver's latency, while in larger-scale tests it achieves notable improvements in inference speed and memory usage compared to state-of-the-art layer-based partitioning approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.02533v1),  [pdf](http://arxiv.org/pdf/2505.02533v1)

**Tags**: cs.DC cs.AI 



### An Empirical Study on the Performance and Energy Usage of Compiled   Python Code
**Authors**: Vincenzo Stoico, Andrei Calin Dragomir, Patricia Lago

**Updated**: 2025-05-05T04:01:56Z

**Summary**: Python is a popular programming language known for its ease of learning and extensive libraries. However, concerns about performance and energy consumption have led to the development of compilers to enhance Python code efficiency. Despite the proven benefits of existing compilers on the efficiency of Python code, there is limited analysis comparing their performance and energy efficiency, particularly considering code characteristics and factors like CPU frequency and core count. Our study investigates how compilation impacts the performance and energy consumption of Python code, using seven benchmarks compiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython, Pyston-lite, and the experimental Python 3.13 version, compared to CPython. The benchmarks are single-threaded and executed on an NUC and a server, measuring energy usage, execution time, memory usage, and Last-Level Cache (LLC) miss rates at a fixed frequency and on a single core. The results show that compilation can significantly enhance execution time, energy and memory usage, with Codon, PyPy, and Numba achieving over 90\% speed and energy improvements. Nuitka optimizes memory usage consistently on both testbeds. The impact of compilation on LLC miss rate is not clear since it varies considerably across benchmarks for each compiler. Our study is important for researchers and practitioners focused on improving Python code performance and energy efficiency. We outline future research directions, such as exploring caching effects on energy usage. Our findings help practitioners choose the best compiler based on their efficiency benefits and accessibility.

**Link**: [arxiv](http://arxiv.org/abs/2505.02346v1),  [pdf](http://arxiv.org/pdf/2505.02346v1)

**Tags**: cs.PL cs.PF cs.SE 



### Performance Characterization of Containers in Edge Computing
**Authors**: Ragini Gupta, Klara Nahrstedt

**Updated**: 2025-05-08T07:37:11Z

**Summary**: Edge computing addresses critical limitations of cloud computing such as high latency and network congestion by decentralizing processing from cloud to the edge. However, the need for software replication across heterogeneous edge devices introduces dependency and portability challenges, driving the adoption of containerization technologies like Docker. While containers offer lightweight isolation and deployment advantages, they introduce new bottlenecks in edge environments, including cold-start delays, memory constraints, network throughput variability, and inefficient IO handling when interfacing with embedded peripherals. This paper presents an empirical evaluation of Docker containers on resource-constrained edge devices, using Raspberry Pi as a representative platform. We benchmark performance across diverse workloads, including microbenchmarks (CPU, memory, network profiling) and macrobenchmarks (AI inference, sensor IO operations), to quantify the overheads of containerization in real-world edge scenarios. Our testbed comprises physical Raspberry Pi nodes integrated with environmental sensors and camera modules, enabling measurements of latency, memory faults, IO throughput, and cold start delays under varying loads. Key findings reveal trade-offs between container isolation and edge-specific resource limitations, with performance degradation observed in IO heavy and latency sensitive tasks. We identify configuration optimizations to mitigate these issues, providing actionable insights for deploying containers in edge environments while meeting real time and reliability requirements. This work advances the understanding of containerized edge computing by systematically evaluating its feasibility and pitfalls on low-power embedded systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.02082v2),  [pdf](http://arxiv.org/pdf/2505.02082v2)

**Tags**: cs.PF 



### DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient   MoE Inference
**Authors**: Yujie Zhang, Shivam Aggarwal, Tulika Mitra

**Updated**: 2025-05-04T09:49:42Z

**Summary**: Mixture-of-Experts (MoE) models, though highly effective for various machine learning tasks, face significant deployment challenges on memory-constrained devices. While GPUs offer fast inference, their limited memory compared to CPUs means not all experts can be stored on the GPU simultaneously, necessitating frequent, costly data transfers from CPU memory, often negating GPU speed advantages. To address this, we present DAOP, an on-device MoE inference engine to optimize parallel GPU-CPU execution. DAOP dynamically allocates experts between CPU and GPU based on per-sequence activation patterns, and selectively pre-calculates predicted experts on CPUs to minimize transfer latency. This approach enables efficient resource utilization across various expert cache ratios while maintaining model accuracy through a novel graceful degradation mechanism. Comprehensive evaluations across various datasets show that DAOP outperforms traditional expert caching and prefetching methods by up to 8.20x and offloading techniques by 1.35x while maintaining accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2501.10375v2),  [pdf](http://arxiv.org/pdf/2501.10375v2)

**Tags**: cs.DC cs.LG 



### GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph   In-Context Learning
**Authors**: Rui Lv, Zaixi Zhang, Kai Zhang, Qi Liu, Weibo Gao, Jiawei Liu, Jiaxia Yan, Linan Yue, Fangzhou Yao

**Updated**: 2025-05-04T08:30:00Z

**Summary**: Graph In-Context Learning, with the ability to adapt pre-trained graph models to novel and diverse downstream graphs without updating any parameters, has gained much attention in the community. The key to graph in-context learning is to perform downstream graphs conditioned on chosen prompt examples. Existing methods randomly select subgraphs or edges as prompts, leading to noisy graph prompts and inferior model performance. Additionally, due to the gap between pre-training and testing graphs, when the number of classes in the testing graphs is much greater than that in the training, the in-context learning ability will also significantly deteriorate. To tackle the aforementioned challenges, we develop a multi-stage adaptive prompt optimization method GraphPrompter, which optimizes the entire process of generating, selecting, and using graph prompts for better in-context learning capabilities. Firstly, Prompt Generator introduces a reconstruction layer to highlight the most informative edges and reduce irrelevant noise for graph prompt construction. Furthermore, in the selection stage, Prompt Selector employs the $k$-nearest neighbors algorithm and pre-trained selection layers to dynamically choose appropriate samples and minimize the influence of irrelevant prompts. Finally, we leverage a Prompt Augmenter with a cache replacement strategy to enhance the generalization capability of the pre-trained model on new datasets. Extensive experiments show that GraphPrompter effectively enhances the in-context learning ability of graph models. On average across all the settings, our approach surpasses the state-of-the-art baselines by over 8%. Our code is released at https://github.com/karin0018/GraphPrompter.

**Link**: [arxiv](http://arxiv.org/abs/2505.02027v1),  [pdf](http://arxiv.org/pdf/2505.02027v1)

**Tags**: cs.LG cs.AI cs.SI 



### PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language   Model Inference
**Authors**: Yufeng Gu, Alireza Khadem, Sumanth Umesh, Ning Liang, Xavier Servot, Onur Mutlu, Ravi Iyer, Reetuparna Das

**Updated**: 2025-05-03T04:07:07Z

**Summary**: Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced LLMs and makes users pay for expensive compute resources that are poorly utilized for the memory-bound LLM inference tasks.   We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which harnesses CXL memory expansion capabilities to accommodate substantial LLM sizes, and utilizes near-bank processing units to deliver high memory bandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable CXL network to support peer-to-peer and collective communication primitives across CXL devices. We implement various parallelism strategies to distribute LLMs across these devices. Compared to GPU baselines with maximum supported batch sizes and similar average power, CENT achieves 2.3$\times$ higher throughput and consumes 2.9$\times$ less energy. CENT enhances the Total Cost of Ownership (TCO), generating 5.2$\times$ more tokens per dollar than GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2502.07578v3),  [pdf](http://arxiv.org/pdf/2502.07578v3)

**Tags**: cs.AR 



### A Survey on Inference Engines for Large Language Models: Perspectives on   Optimization and Efficiency
**Authors**: Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee

**Updated**: 2025-05-08T07:08:40Z

**Summary**: Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine

**Link**: [arxiv](http://arxiv.org/abs/2505.01658v2),  [pdf](http://arxiv.org/pdf/2505.01658v2)

**Tags**: cs.CL 



### VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with   Delayed Hits
**Authors**: Bowen Jiang, Chaofan Ma, Duo Wang

**Updated**: 2025-05-03T01:10:30Z

**Summary**: Caches are fundamental to latency-sensitive systems like Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit phenomenon where multiple requests for an object occur during its fetch from the remote server after a miss significantly inflates user-perceived latency. While recent algorithms acknowledge delayed hits by estimating the resulting aggregate delay, they predominantly focus on its mean value. We identify and demonstrate that such approaches are insufficient, as the real aggregate delay frequently exhibits substantial variance in the true production system, leading to suboptimal latency performance when ignored. Thus, we propose VA-CDH, a variance-aware method to optimize latency for caching with delayed hits. It employs a novel ranking function that explicitly incorporates both the empirically estimated mean and standard deviation of aggregate delay, allowing caching decisions to account for its variation. We derive the analytical distribution of aggregate delay under Poisson arrivals as a theoretical contribution, offering more statistical insight beyond the mean value. Through the simulations conducted on synthetic and real-world datasets, we show that VA-CDH reduces the total latency by 1%-6% approximately compared to state-of-the-art algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2504.20335v2),  [pdf](http://arxiv.org/pdf/2504.20335v2)

**Tags**: cs.NI 



### Collaborative Coded Caching for Partially Connected Networks
**Authors**: Kagan Akcay, Eleftherios Lampiris, MohammadJavad Salehi, Giuseppe Caire

**Updated**: 2025-05-02T13:55:21Z

**Summary**: Coded caching leverages the differences in user cache memories to achieve gains that scale with the total cache size, alleviating network congestion due to high-quality content requests. Additionally, distributing transmitters over a wide area can mitigate the adverse effects of path loss. In this work, we consider a partially connected network where the channel between distributed transmitters (helpers) and users is modeled as a distributed multiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a novel delivery scheme consisting of two phases: partitioning and transmission. In the partitioning phase, users with identical cache profiles are partitioned into the minimum number of sets, such that users within each set can successfully decode their desired message from a joint transmission enabled by MIMO precoding. To optimally partition the users, we employ the branch and bound method. In the transmission phase, each partition is treated as a single entity, and codewords are multicast to partitions with distinct cache profiles. The proposed delivery scheme is applicable to any partially connected network, and while the partitioning is optimal, the overall delivery scheme, including transmission, is heuristic. Interestingly, simulation results show that its performance closely approximates that of the fully connected optimal solution.

**Link**: [arxiv](http://arxiv.org/abs/2501.13298v3),  [pdf](http://arxiv.org/pdf/2501.13298v3)

**Tags**: cs.IT math.IT 



### HarmoniCa: Harmonizing Training and Inference for Better Feature Caching   in Diffusion Transformer Acceleration
**Authors**: Yushi Huang, Zining Wang, Ruihao Gong, Jing Liu, Xinjie Zhang, Jinyang Guo, Xianglong Liu, Jun Zhang

**Updated**: 2025-05-02T11:29:31Z

**Summary**: Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\%$ latency reduction (i.e., $2.07\times$ theoretical speedup) and improved performance on PixArt-$\alpha$. Remarkably, our image-free approach reduces training time by $25\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.

**Link**: [arxiv](http://arxiv.org/abs/2410.01723v4),  [pdf](http://arxiv.org/pdf/2410.01723v4)

**Tags**: cs.CV 



### CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in   RAG Systems
**Authors**: Yeonwoo Jeong, Kyuli Park, Hyunji Cho, Sungyong Park

**Updated**: 2025-05-02T10:13:12Z

**Summary**: Modern embedding models capture both semantic and syntactic structures of queries, often mapping different queries to similar regions in vector space. This results in non-uniform cluster access patterns in disk-based vector search systems, particularly in Retrieval Augmented Generation (RAG) framework. While existing approaches optimize individual queries, they overlook the impact of cluster access patterns, failing to account for the locality effects of queries that access similar clusters. This oversight reduces cache efficiency and increases search latency due to excessive disk I/O. To address this, we introduce CaGR-RAG, a context-aware query grouping mechanism that organizes queries based on shared cluster access patterns. Additionally, it incorporates opportunistic cluster prefetching to minimize cache misses during transitions between query groups, further optimizing retrieval performance. Experimental results show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55% while consistently maintaining a higher cache hit ratio than the baseline.

**Link**: [arxiv](http://arxiv.org/abs/2505.01164v1),  [pdf](http://arxiv.org/pdf/2505.01164v1)

**Tags**: cs.DC 



### High Voltage Delivery and Distribution for the NEXT-100 Time Projection   Chamber
**Authors**: NEXT Collaboration, C. Adams, H. Almazán, V. Álvarez, K. Bailey, R. Guenette, B. J. P. Jones, S. Johnston, K. Mistry, F. Monrabal, D. R. Nygren, B. Palmeiro, L. Rogers, J. Waldschmidt, B. Aparicio, A. I. Aranburu, L. Arazi, I. J. Arnquist, F. Auria-Luna, S. Ayet, C. D. R. Azevedo, F. Ballester, M. del Barrio-Torregrosa, A. Bayo, J. M. Benlloch-Rodríguez, F. I. G. M. Borges, A. Brodolin, S. Cárcel, A. Castillo, L. Cid, C. A. N. Conde, T. Contreras, F. P. Cossío, R. Coupe, E. Dey, G. Díaz, C. Echevarria, M. Elorza, J. Escada, R. Esteve, R. Felkai, L. M. P. Fernandes, P. Ferrario, A. L. Ferreira, F. W. Foss, Z. Freixa, J. García-Barrena, J. J. Gómez-Cadenas, J. W. R. Grocott, R. Guenette, J. Hauptman, C. A. O. Henriques, J. A. Hernando Morata, P. Herrero-Gómez, V. Herrero, C. Hervés Carrete, Y. Ifergan, F. Kellerer, L. Larizgoitia, A. Larumbe, P. Lebrun, F. Lopez, N. López-March, R. Madigan, R. D. P. Mano, A. P. Marques, J. Martín-Albo, G. Martínez-Lema, M. Martínez-Vara, R. L. Miller, J. Molina-Canteras, F. Monrabal, C. M. B. Monteiro, F. J. Mora, P. Novella, A. Nuñez, E. Oblak, J. Palacio, B. Palmeiro, A. Para, A. Pazos, J. Pelegrin, M. Pérez Maneiro, M. Querol, J. Renner, I. Rivilla, C. Rogero, B. Romeo, C. Romo-Luque, V. San Nacienciano, F. P. Santos, J. M. F. dos Santos, M. Seemann, I. Shomroni, P. A. O. C. Silva, A. Simón, S. R. Soleti, M. Sorel, J. Soto-Oton, J. M. R. Teixeira, S. Teruel-Pardo, J. F. Toledo, C. Tonnelé, S. Torelli, J. Torrent, A. Trettin, A. Usón, P. R. G. Valle, J. F. C. A. Veloso, J. Waiton, A. Yubero-Navarro

**Updated**: 2025-05-02T04:57:06Z

**Summary**: A critical element in the realization of large liquid and gas time projection chambers (TPCs) is the delivery and distribution of high voltages into and around the detector. Such experiments require of order tens of kilovolts to enable electron drift over meter-scale distances. This paper describes the design and operation of the cathode feedthrough and high voltage distribution through the field cage of the NEXT-100 experiment, an underground TPC that will search for neutrinoless double beta decay $0\nu\beta\beta$. The feedthrough has been demonstrated to hold pressures up to 20~bar and sustain voltages as high as -65~kV, and the TPC is operating stably at its design high voltages. The system has been realized within the constraints of a stringent radiopurity budget and is now being used to execute a suite of sensitive double beta decay analyses.

**Link**: [arxiv](http://arxiv.org/abs/2505.01002v1),  [pdf](http://arxiv.org/pdf/2505.01002v1)

**Tags**: physics.ins-det hep-ex 



### The Open-Source BlackParrot-BedRock Cache Coherence System
**Authors**: Mark Unruh Wyse

**Updated**: 2025-05-02T02:36:23Z

**Summary**: This dissertation revisits the topic of programmable cache coherence engines in the context of modern shared-memory multicore processors. First, the open-source BedRock cache coherence protocol is described. BedRock employs the canonical MOESIF coherence states and reduces implementation burden by eliminating transient coherence states from the protocol. The protocol's design complexity, concurrency, and verification effort are analyzed and compared to a canonical directory-based invalidate coherence protocol. Second, the architecture and microarchitecture of three separate cache coherence directories implementing the BedRock protocol within the BlackParrot 64-bit RISC-V multicore processor, collectively called BlackParrot-BedRock (BP-BedRock), are described. A fixed-function coherence directory engine implementation provides a baseline design for performance and area comparisons. A microcode-programmable coherence directory implementation demonstrates the feasibility of implementing a programmable coherence engine capable of maintaining sufficient protocol processing performance. A hybrid fixed-function and programmable coherence directory blends the protocol processing performance of the fixed-function design with the programmable flexibility of the microcode-programmable design. Collectively, the BedRock coherence protocol and its three BP-BedRock implementations demonstrate the feasibility and challenges of including programmable logic within the coherence system of modern shared-memory multicore processors, paving the way for future research into the application- and system-level benefits of programmable coherence engines.

**Link**: [arxiv](http://arxiv.org/abs/2505.00962v1),  [pdf](http://arxiv.org/pdf/2505.00962v1)

**Tags**: cs.AR 



### Heterogeneous Memory Benchmarking Toolkit
**Authors**: Golsana Ghaemi, Kazem Taram, Renato Mancuso

**Updated**: 2025-05-01T22:32:29Z

**Summary**: This paper presents an open-source kernel-level heterogeneous memory characterization framework (MemScope) for embedded systems that enables users to understand and precisely characterize the temporal behavior of all available memory modules under configurable contention stress scenarios. Since kernel-level provides a high degree of control over allocation, cache maintenance, $CPUs$, interrupts, and I/O device activity, seeking the most accurate way to benchmark heterogeneous memory subsystems, would be achieved by implementing it in the kernel. This gives us the privilege to directly map pieces of contiguous physical memory and instantiate allocators, allowing us to finely control cores to create and eliminate interference. Additionally, we can minimize noise and interruptions, guaranteeing more consistent and precise results compared to equivalent user-space solutions. Running our Framework on a Xilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability to precisely benchmark bandwidth and latency across various memory types, including PL-side DRAM and BRAM, in a multi-core system.

**Link**: [arxiv](http://arxiv.org/abs/2505.00901v1),  [pdf](http://arxiv.org/pdf/2505.00901v1)

**Tags**: cs.AR cs.PF 



### Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from   Large Language Models
**Authors**: Andrew Adiletta, Berk Sunar

**Updated**: 2025-05-01T19:18:56Z

**Summary**: Side-channel attacks on shared hardware resources increasingly threaten confidentiality, especially with the rise of Large Language Models (LLMs). In this work, we introduce Spill The Beans, a novel application of cache side-channels to leak tokens generated by an LLM. By co-locating an attack process on the same hardware as the victim model, we flush and reload embedding vectors from the embedding layer, where each token corresponds to a unique embedding vector. When accessed during token generation, it results in a cache hit detectable by our attack on shared lower-level caches.   A significant challenge is the massive size of LLMs, which, by nature of their compute intensive operation, quickly evicts embedding vectors from the cache. We address this by balancing the number of tokens monitored against the amount of information leaked. Monitoring more tokens increases potential vocabulary leakage but raises the chance of missing cache hits due to eviction; monitoring fewer tokens improves detection reliability but limits vocabulary coverage.   Through extensive experimentation, we demonstrate the feasibility of leaking tokens from LLMs via cache side-channels. Our findings reveal a new vulnerability in LLM deployments, highlighting that even sophisticated models are susceptible to traditional side-channel attacks. We discuss the implications for privacy and security in LLM-serving infrastructures and suggest considerations for mitigating such threats. For proof of concept we consider two concrete attack scenarios: Our experiments show that an attacker can recover as much as 80%-90% of a high entropy API key with single shot monitoring. As for English text we can reach a 40% recovery rate with a single shot. We should note that the rate highly depends on the monitored token set and these rates can be improved by targeting more specialized output domains.

**Link**: [arxiv](http://arxiv.org/abs/2505.00817v1),  [pdf](http://arxiv.org/pdf/2505.00817v1)

**Tags**: cs.CR cs.AI K.6.5 



### Optomechanical resource for fault-tolerant quantum computing
**Authors**: Margaret Pavlovich, Peter Rakich, Shruti Puri

**Updated**: 2025-05-01T18:00:40Z

**Summary**: Fusion-based quantum computing with dual-rail qubits is a leading candidate for scalable quantum computing using linear optics. This paradigm requires single photons which are entangled into small resource states before being fed into a fusion network. The most common sources for single optical photons and for small entangled states are probabilistic and heralded. The realization of a single reliable deterministic source requires many redundant probabilistic sources and a complex optical network for rerouting and retiming probabilistic outputs. In this work, we show how optomechanics enables reliable production of resources for photonic quantum computing without the redundancy of the all-optical approach. This is achieved by using acoustic modes as caches of quantum resources, ranging from single-particle states to small entangled states, with on-demand read-out. The advantages of acoustic modes as optical quantum memories, compared to other technologies, include their intrinsically long lifetimes and that they are solid state, highly tailorable, and insensitive to electromagnetic noise. We show how the resource states can be prepared directly in the acoustic modes using optical controls. This is still probabilistic and heralded, as in the all-optical approach, but the acoustic modes act as a quantum memory which is integrated into the production of the states. The quantum states may be deterministically transferred from acoustic modes to optical modes, on demand, with another optical drive.

**Link**: [arxiv](http://arxiv.org/abs/2505.00768v1),  [pdf](http://arxiv.org/pdf/2505.00768v1)

**Tags**: quant-ph 



### FreqKV: Frequency Domain Key-Value Compression for Efficient Context   Window Extension
**Authors**: Jushi Kai, Boyi Zeng, Yixuan Wang, Haoli Bai, Bo Jiang, Zhouhan Lin

**Updated**: 2025-05-01T14:53:12Z

**Summary**: Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2505.00570v1),  [pdf](http://arxiv.org/pdf/2505.00570v1)

**Tags**: cs.CL cs.AI 



### Mixture of Sparse Attention: Content-Based Learnable Sparse Attention   via Expert-Choice Routing
**Authors**: Piotr Piękos, Róbert Csordás, Jürgen Schmidhuber

**Updated**: 2025-05-01T05:22:11Z

**Summary**: Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.00315v1),  [pdf](http://arxiv.org/pdf/2505.00315v1)

**Tags**: cs.LG cs.CL 



### QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM   Serving
**Authors**: Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han

**Updated**: 2025-05-01T02:14:05Z

**Summary**: Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.

**Link**: [arxiv](http://arxiv.org/abs/2405.04532v3),  [pdf](http://arxiv.org/pdf/2405.04532v3)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### Soft-Label Caching and Sharpening for Communication-Efficient Federated   Distillation
**Authors**: Kitsuya Azuma, Takayuki Nishio, Yuichi Kitagawa, Wakako Nakano, Takahito Tanimura

**Updated**: 2025-05-01T00:13:06Z

**Summary**: Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining accuracy. Enhanced ERA can be tuned to adapt to non-IID data variations, ensuring robust aggregation and performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.

**Link**: [arxiv](http://arxiv.org/abs/2504.19602v2),  [pdf](http://arxiv.org/pdf/2504.19602v2)

**Tags**: cs.LG 



### Accelerating Diffusion Transformer via Error-Optimized Cache
**Authors**: Junxiang Qiu, Shuo Wang, Jinda Lu, Lin Liu, Houcheng Jiang, Xingyu Zhu, Yanbin Hao

**Updated**: 2025-04-30T19:48:41Z

**Summary**: Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the Error-Optimized Cache (EOC). This method introduces three key improvements: (1) Prior knowledge extraction: Extract and process the caching differences; (2) A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; (3) Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching, especially excessive caching. On the ImageNet dataset, without substantially increasing the computational load, this method improves the FID of the generated images when the rule-based model FORA has a caching level of 75%, 50%, and 25%, and the training-based model Learning-to-cache has a caching level of 22%. Specifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857 to 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%) respectively.

**Link**: [arxiv](http://arxiv.org/abs/2501.19243v2),  [pdf](http://arxiv.org/pdf/2501.19243v2)

**Tags**: cs.CV 



### SDW driven "magnetic breakdown" in a d-wave altermagnet KV$_2$Se$_2$O
**Authors**: Xu Yan, Ziyin Song, Juntao Song, Zhong Fang, Hongming Weng, Quansheng Wu

**Updated**: 2025-04-30T18:00:02Z

**Summary**: Altermagnets, combining zero net magnetization with intrinsic spin splitting, demonstrate unique quantum phenomena crucial for spintronic applications. KV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a checkerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave (SDW) state as the temperature decreases. After phase transition, the apparent paradox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals negligible Fermi surface modifications, while physical property measurement system (PPMS) measurements uncover substantial changes in transport properties. Our study explores the microscopic mechanisms governing phase-dependent transport properties of KV$_2$Se$_2$O base on first-principles calculations. The spin canting driven by periodic spin modulation in the SDW phase reduces the magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting and Fermi surface reconstruction induce the ``magnetic breakdown" phenomenon, which alters carrier trajectories, modifies carrier concentration, strengthens electron-hole compensation, and ultimately accounts for the contrasting magnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our work proposes an innovative method for identifying the electronic structure evolution across phase transitions from transport signatures, providing a novel paradigm for altermagnets research.

**Link**: [arxiv](http://arxiv.org/abs/2505.00074v1),  [pdf](http://arxiv.org/pdf/2505.00074v1)

**Tags**: cond-mat.mtrl-sci 



### Switching Transients in Constrained Transformer-Line/Cable   Configurations
**Authors**: Y. Xiang, L. Wu, K. Velitsikakis, A. L. J. Janssen

**Updated**: 2025-04-30T12:51:59Z

**Summary**: This paper investigates the transient phenomena that occur in two special cases in the Netherlands: (A) during the energization of a power transformer via a cable feeder and (B) the energization of a power transformer together with an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV transformer are connected and energized at the same time. In Case B a 150/50 kV transformer and a short 50 kV OHL are connected and energized simultaneously. The reason behind this kind of situations is related to space restrictions and cost efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2504.21594v1),  [pdf](http://arxiv.org/pdf/2504.21594v1)

**Tags**: eess.SY cs.SY 



### Responsive DNN Adaptation for Video Analytics against Environment Shift   via Hierarchical Mobile-Cloud Collaborations
**Authors**: Maozhe Zhao, Shengzhong Liu, Fan Wu, Guihai Chen

**Updated**: 2025-04-30T08:08:15Z

**Summary**: Mobile video analysis systems often encounter various deploying environments, where environment shifts present greater demands for responsiveness in adaptations of deployed "expert DNN models". Existing model adaptation frameworks primarily operate in a cloud-centric way, exhibiting degraded performance during adaptation and delayed reactions to environment shifts. Instead, this paper proposes MOCHA, a novel framework optimizing the responsiveness of continuous model adaptation through hierarchical collaborations between mobile and cloud resources. Specifically, MOCHA (1) reduces adaptation response delays by performing on-device model reuse and fast fine-tuning before requesting cloud model retrieval and end-to-end retraining; (2) accelerates history expert model retrieval by organizing them into a structured taxonomy utilizing domain semantics analyzed by a cloud foundation model as indices; (3) enables efficient local model reuse by maintaining onboard expert model caches for frequent scenes, which proactively prefetch model weights from the cloud model database. Extensive evaluations with real-world videos on three DNN tasks show MOCHA improves the model accuracy during adaptation by up to 6.8% while saving the response delay and retraining time by up to 35.5x and 3.0x respectively.

**Link**: [arxiv](http://arxiv.org/abs/2505.00745v1),  [pdf](http://arxiv.org/pdf/2505.00745v1)

**Tags**: cs.CV cs.LG 



### Kimina Lean Server: Technical Report
**Authors**: Marco Dos Santos, Haiming Wang, Hugues de Saxcé, Ran Wang, Mantas Baksys, Mert Unsal, Junqi Liu, Zhengying Liu, Jia Li

**Updated**: 2025-04-29T23:43:59Z

**Summary**: We introduce the Kimina Lean Server, an open-source project that enables fast and scalable interaction with Lean 4 via a unified REST API, designed as a simple verifier for reinforcement learning pipelines. Built on top of the Lean FRO's LeanREPL, it combines server-side parallelization by managing multiple Lean REPL processes in parallel, with an LRU caching strategy that reuses Lean imports across multiple requests. These features help reduce initialization overhead and allow large-scale batch processing of Lean code. The client-side interface allows users to submit batches of proofs and receive Lean feedback, including extracted tactics and tactic states via infotree processing. These features enable a high-performance, scalable workflow for both interaction and extraction of proofs, tactics, and tactic states. We open source our implementation on GitHub.

**Link**: [arxiv](http://arxiv.org/abs/2504.21230v1),  [pdf](http://arxiv.org/pdf/2504.21230v1)

**Tags**: cs.LO 



### CachePrune: Neural-Based Attribution Defense Against Indirect Prompt   Injection Attacks
**Authors**: Rui Wang, Junda Wu, Yu Xia, Tong Yu, Ruiyi Zhang, Ryan Rossi, Lina Yao, Julian McAuley

**Updated**: 2025-04-29T23:42:21Z

**Summary**: Large Language Models (LLMs) are identified as being susceptible to indirect prompt injection attack, where the model undesirably deviates from user-provided instructions by executing tasks injected in the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune that defends against this attack by identifying and pruning task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to treat the text spans of input prompt context as only pure data, instead of any indicator of instruction following. These neurons are identified via feature attribution with a loss function induced from an upperbound of the Direct Preference Optimization (DPO) objective. We show that such a loss function enables effective feature attribution with only a few samples. We further improve on the quality of feature attribution, by exploiting an observed triggering effect in instruction following. Our approach does not impose any formatting on the original prompt or introduce extra test-time LLM calls. Experiments show that CachePrune significantly reduces attack success rates without compromising the response quality. Note: This paper aims to defend against indirect prompt injection attacks, with the goal of developing more secure and robust AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.21228v1),  [pdf](http://arxiv.org/pdf/2504.21228v1)

**Tags**: cs.CR cs.AI 



### An Achievable Scheme for the K-user Linear Computation Broadcast Channel
**Authors**: Yinbin Ma, Daniela Tuninetti

**Updated**: 2025-04-29T17:54:42Z

**Summary**: This paper presents a new achievable scheme for the K-user Linear Computation Broadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K users, each aiming to retrieve a desired linear function of the data by leveraging their prior locally available side information in the form of another linear function of the data. The proposed scheme is based on a subspace decomposition derived from representable polymatroid spaces. This decomposition enables the server to effectively design multicast messages that simultaneously benefit multiple users and allow users to eliminate interference using their available side information. This work extends existing results for the 3-LCBC by introducing a linear programming framework to optimize multicast opportunities across an arbitrary number of users. The proposed approach can be used to derive achievable scheme for the K-user coded caching problem with linear coded placement and scalar linear function retrieval, which was our original motivation to investigate the K-LCBC.

**Link**: [arxiv](http://arxiv.org/abs/2501.12322v2),  [pdf](http://arxiv.org/pdf/2501.12322v2)

**Tags**: cs.IT math.IT 



### Activated LoRA: Fine-tuned LLMs for Intrinsics
**Authors**: Kristjan Greenewald, Luis Lastras, Thomas Parnell, Vraj Shah, Lucian Popa, Giulio Zizzo, Chulaka Gunasekara, Ambrish Rawat, David Cox

**Updated**: 2025-04-29T14:25:08Z

**Summary**: Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is highly inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), which modifies the LoRA framework to only adapt weights for the tokens in the sequence \emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \emph{intrinsics}, i.e. highly specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We use aLoRA to train a set of intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits.

**Link**: [arxiv](http://arxiv.org/abs/2504.12397v2),  [pdf](http://arxiv.org/pdf/2504.12397v2)

**Tags**: cs.LG cs.AI 



### Tree embedding based mapping system for low-latency mobile applications   in multi-access networks
**Authors**: Yu Mi, Randeep Bhatia, Fang Hao, An Wang, Steve Benno, Tv Lakshman

**Updated**: 2025-04-28T20:30:59Z

**Summary**: Low-latency applications like AR/VR and online gaming need fast, stable connections. New technologies such as V2X, LEO satellites, and 6G bring unique challenges in mobility management. Traditional solutions based on centralized or distributed anchors often fall short in supporting rapid mobility due to inefficient routing, low versatility, and insufficient multi-access support. In this paper, we design a new end-to-end system for tracking multi-connected mobile devices at scale and optimizing performance for latency-sensitive, highly dynamic applications. Our system, based on the locator/ID separation principle, extends to multi-access networks without requiring specialized routers or caching. Using a novel tree embedding-based overlay, we enable fast session setup while allowing endpoints to directly handle mobility between them. Evaluation with real network data shows our solution cuts connection latency to 7.42% inflation over the shortest path, compared to LISP's 359\% due to cache misses. It also significantly reduces location update overhead and disruption time during mobility.

**Link**: [arxiv](http://arxiv.org/abs/2504.20246v1),  [pdf](http://arxiv.org/pdf/2504.20246v1)

**Tags**: cs.NI 



### Hierarchical Coded Caching with Low Subpacketization and Coding Delay   using Combinatorial t-Designs
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2025-04-28T17:17:53Z

**Summary**: Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.12747v3),  [pdf](http://arxiv.org/pdf/2405.12747v3)

**Tags**: cs.IT math.IT 



### 3D MPSoC with On-Chip Cache Support -- Design and Exploitation
**Authors**: Rodrigo Cataldo, Cesar Marcon, Debora Matos

**Updated**: 2025-04-28T16:59:13Z

**Summary**: The increasing density of transistors in Integrated Circuits (ICs) has enabled the development of highly integrated Systems-on-Chip (SoCs) and, more recently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability challenges in communication and memory performance, three-dimensional (3D) Network-on-Chip (NoC) architectures have emerged, offering improvements in communication latency and throughput. However, memory system efficiency remains a critical bottleneck in NoC-based designs. This work proposes the design and experimental exploration of 3D MPSoCs with on-chip cache support by employing distinct communication infrastructures for inter-processor and memory interactions. Specifically, packet-based NoCs are adopted for inter-processor communication, while a crossbar-based infrastructure supports a cache coherence hierarchy for memory access. A two-layer system architecture is introduced, combining a Uniform Memory Access (UMA) model within clusters and a No Remote Memory Access (NORMA) model between clusters, aiming to balance scalability and coherence requirements. Emerging memory technologies such as PCRAM and MRAM are explored to optimize performance, energy consumption, and area usage. Experimental evaluations are conducted using the Gem5 simulator, targeting a model based on the ARM Versatile Express platform. The outcomes of this study aim to enhance MPSoC scalability while meeting the stringent demands of memory-centric applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.19984v1),  [pdf](http://arxiv.org/pdf/2504.19984v1)

**Tags**: cs.AR 



### TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate
**Authors**: Amir Zandieh, Majid Daliri, Majid Hadian, Vahab Mirrokni

**Updated**: 2025-04-28T15:05:35Z

**Summary**: Vector quantization, a problem rooted in Shannon's source coding theory, aims to quantize high-dimensional Euclidean vectors while minimizing distortion in their geometric structure. We propose TurboQuant to address both mean-squared error (MSE) and inner product distortion, overcoming limitations of existing methods that fail to achieve optimal distortion rates. Our data-oblivious algorithms, suitable for online applications, achieve near-optimal distortion rates (within a small constant factor) across all bit-widths and dimensions. TurboQuant achieves this by randomly rotating input vectors, inducing a concentrated Beta distribution on coordinates, and leveraging the near-independence property of distinct coordinates in high dimensions to simply apply optimal scalar quantizers per each coordinate. Recognizing that MSE-optimal quantizers introduce bias in inner product estimation, we propose a two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL (QJL) transform on the residual, resulting in an unbiased inner product quantizer. We also provide a formal proof of the information-theoretic lower bounds on best achievable distortion rate by any vector quantizer, demonstrating that TurboQuant closely matches these bounds, differing only by a small constant ($\approx 2.7$) factor. Experimental results validate our theoretical findings, showing that for KV cache quantization, we achieve absolute quality neutrality with 3.5 bits per channel and marginal quality degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search tasks, our method outperforms existing product quantization techniques in recall while reducing indexing time to virtually zero.

**Link**: [arxiv](http://arxiv.org/abs/2504.19874v1),  [pdf](http://arxiv.org/pdf/2504.19874v1)

**Tags**: cs.LG cs.AI cs.DB cs.DS 



### semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated   Computation and Unified Storage
**Authors**: Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, Yun Liang, Yu Wang

**Updated**: 2025-04-28T15:00:03Z

**Summary**: Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.   In this paper, we identify that the advantage of the disaggregated system lies in the disaggregated computation, i.e., partitioning the computational resource to enable the asynchronous computation of two phases. Thus, we propose a novel LLM serving system, semi-PD, characterized by disaggregated computation and unified storage. In semi-PD, we introduce a computation resource controller to achieve disaggregated computation at the streaming multi-processor (SM) level, and a unified memory manager to manage the asynchronous memory access from both phases. semi-PD has a low-overhead resource adjustment mechanism between the two phases, and a service-level objective (SLO) aware dynamic partitioning algorithm to optimize the SLO attainment. Compared to state-of-the-art systems, semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models.

**Link**: [arxiv](http://arxiv.org/abs/2504.19867v1),  [pdf](http://arxiv.org/pdf/2504.19867v1)

**Tags**: cs.CL cs.DC cs.LG 



### Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching   for Small Buffer or Small Rate
**Authors**: Han Fang, Nan Liu, Wei Kang

**Updated**: 2025-04-28T09:03:45Z

**Summary**: We consider the secure coded caching problem proposed by Ravindrakumar et. al where no user can obtain information about files other than the one requested. We first propose three new schemes for the three cases of cache size $M=1$, $N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files and $K$ users, and the general case for arbitrary $N$ files and $K$ users, respectively. Then we derive converse results by characterizing new properties of secure coded caching schemes. As a result, we characterize the two end-points of the optimal memory-rate tradeoff curve for arbitrary number of users and files. Furthermore, for the case of $N=2$ files and arbitrary number of users, we also characterize a segment of the optimal memory-rate tradeoff curve, where the cache size is relatively small.

**Link**: [arxiv](http://arxiv.org/abs/2504.19601v1),  [pdf](http://arxiv.org/pdf/2504.19601v1)

**Tags**: cs.IT math.IT 



### Quantifying Memory Utilization with Effective State-Size
**Authors**: Rom N. Parnichkun, Neehal Tumma, Armin W. Thomas, Alessandro Moro, Qi An, Taiji Suzuki, Atsushi Yamashita, Michael Poli, Stefano Massaroli

**Updated**: 2025-04-28T08:12:30Z

**Summary**: The need to develop a general framework for architecture analysis is becoming increasingly important, given the expanding design space of sequence models. To this end, we draw insights from classical signal processing and control theory, to develop a quantitative measure of \textit{memory utilization}: the internal mechanisms through which a model stores past information to produce future outputs. This metric, which we call \textbf{\textit{effective state-size}} (ESS), is tailored to the fundamental class of systems with \textit{input-invariant} and \textit{input-varying linear operators}, encompassing a variety of computational units such as variants of attention, convolutions, and recurrences. Unlike prior work on memory utilization, which either relies on raw operator visualizations (e.g. attention maps), or simply the total \textit{memory capacity} (i.e. cache size) of a model, our metrics provide highly interpretable and actionable measurements. In particular, we show how ESS can be leveraged to improve initialization strategies, inform novel regularizers and advance the performance-efficiency frontier through model distillation. Furthermore, we demonstrate that the effect of context delimiters (such as end-of-speech tokens) on ESS highlights cross-architectural differences in how large language models utilize their available memory to recall information. Overall, we find that ESS provides valuable insights into the dynamics that dictate memory utilization, enabling the design of more efficient and effective sequence models.

**Link**: [arxiv](http://arxiv.org/abs/2504.19561v1),  [pdf](http://arxiv.org/pdf/2504.19561v1)

**Tags**: cs.LG 



### Prisma: An Open Source Toolkit for Mechanistic Interpretability in   Vision and Video
**Authors**: Sonia Joseph, Praneet Suresh, Lorenz Hufe, Edward Stevinson, Robert Graham, Yash Vadi, Danilo Bzdok, Sebastian Lapuschkin, Lee Sharkey, Blake Aaron Richards

**Updated**: 2025-04-28T04:31:24Z

**Summary**: Robust tooling and publicly available pre-trained models have helped drive recent advances in mechanistic interpretability for language models. However, similar progress in vision mechanistic interpretability has been hindered by the lack of accessible frameworks and pre-trained weights. We present Prisma (Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an open-source framework designed to accelerate vision mechanistic interpretability research, providing a unified toolkit for accessing 75+ vision and video transformers; support for sparse autoencoder (SAE), transcoder, and crosscoder training; a suite of 80+ pre-trained SAE weights; activation caching, circuit analysis tools, and visualization tools; and educational resources. Our analysis reveals surprising findings, including that effective vision SAEs can exhibit substantially lower sparsity patterns than language SAEs, and that in some instances, SAE reconstructions can decrease model loss. Prisma enables new research directions for understanding vision model internals while lowering barriers to entry in this emerging field.

**Link**: [arxiv](http://arxiv.org/abs/2504.19475v1),  [pdf](http://arxiv.org/pdf/2504.19475v1)

**Tags**: cs.CV cs.AI cs.LG 



### From Cluster to Desktop: A Cache-Accelerated INR framework for   Interactive Visualization of Tera-Scale Data
**Authors**: Daniel Zavorotny, Qi Wu, David Bauer, Kwan-Liu Ma

**Updated**: 2025-04-28T04:02:30Z

**Summary**: Machine learning has enabled the use of implicit neural representations (INRs) to efficiently compress and reconstruct massive scientific datasets. However, despite advances in fast INR rendering algorithms, INR-based rendering remains computationally expensive, as computing data values from an INR is significantly slower than reading them from GPU memory. This bottleneck currently restricts interactive INR visualization to professional workstations. To address this challenge, we introduce an INR rendering framework accelerated by a scalable, multi-resolution GPU cache capable of efficiently representing tera-scale datasets. By minimizing redundant data queries and prioritizing novel volume regions, our method reduces the number of INR computations per frame, achieving an average 5x speedup over the state-of-the-art INR rendering method while still maintaining high visualization quality. Coupled with existing hardware-accelerated INR compressors, our framework enables scientists to generate and compress massive datasets in situ on high-performance computing platforms and then interactively explore them on consumer-grade hardware post hoc.

**Link**: [arxiv](http://arxiv.org/abs/2504.18001v2),  [pdf](http://arxiv.org/pdf/2504.18001v2)

**Tags**: cs.GR 



### Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and   Generalizable Point Cloud Analysis
**Authors**: Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai

**Updated**: 2025-04-28T02:58:27Z

**Summary**: This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data (often inaccessible during online inference) and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop \textbf{Point-Cache}, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2503.12150v3),  [pdf](http://arxiv.org/pdf/2503.12150v3)

**Tags**: cs.CV 



### AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration
**Authors**: Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou

**Updated**: 2025-04-27T22:05:14Z

**Summary**: Graphics Processing Units (GPUs) have become essential for computationally intensive applications. However, emerging workloads such as recommender systems, graph analytics, and data analytics often involve processing data exceeding GPU on-chip memory capacity. To mitigate this issue, existing solutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them, the GPU-centric approach lets GPU threads directly initiate NVMe requests, eliminating CPU intervention overhead over traditional methods. However, the SOTA GPU-centric approach adopts a synchronous IO model, and threads must tolerate the long latency in communication before starting any tasks.   In this work, we propose AGILE, a lightweight and efficient asynchronous library allowing GPU threads to access SSDs asynchronously while eliminating deadlock risks. AGILE also integrates a flexible software cache using GPU High-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric IO achieves up to 1.88$\times$ improvement in workloads with different computation-to-communication (CTC) ratios. We also compare AGILE with the SOTA work BaM on Deep Learning Recommendation Models (DLRM) with various settings, and the results show that AGILE achieves 1.75$\times$ performance improvement due to its efficient design and the overlapping strategy enabled by an asynchronous IO model. We further evaluate AGILE's API overhead on graph applications, and the results demonstrate AGILE reduces software cache overhead by up to 3.12$\times$ and overhead in NVMe IO requests by up to 2.85$\times$. Compared with BaM, AGILE consumes fewer registers and exhibits up to 1.32$\times$ reduction in the usage of registers.

**Link**: [arxiv](http://arxiv.org/abs/2504.19365v1),  [pdf](http://arxiv.org/pdf/2504.19365v1)

**Tags**: cs.DC 



### OpenFusion++: An Open-vocabulary Real-time Scene Understanding System
**Authors**: Xiaofeng Jin, Matteo Frosi, Matteo Matteucci

**Updated**: 2025-04-27T14:46:43Z

**Summary**: Real-time open-vocabulary scene understanding is essential for efficient 3D perception in applications such as vision-language navigation, embodied intelligence, and augmented reality. However, existing methods suffer from imprecise instance segmentation, static semantic updates, and limited handling of complex queries. To address these issues, we present OpenFusion++, a TSDF-based real-time 3D semantic-geometric reconstruction system. Our approach refines 3D point clouds by fusing confidence maps from foundational models, dynamically updates global semantic labels via an adaptive cache based on instance area, and employs a dual-path encoding framework that integrates object attributes with environmental context for precise query responses. Experiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate that OpenFusion++ significantly outperforms the baseline in both semantic accuracy and query responsiveness.

**Link**: [arxiv](http://arxiv.org/abs/2504.19266v1),  [pdf](http://arxiv.org/pdf/2504.19266v1)

**Tags**: cs.CV 68T45, 68U05 I.2.10; I.4.8 



### WuNeng: Hybrid State with Attention
**Authors**: Liu Xiao, Li Zhiyuan, Lin Yueyu

**Updated**: 2025-04-27T10:48:56Z

**Summary**: The WuNeng architecture introduces a novel approach to enhancing the expressivity and power of large language models by integrating recurrent neural network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing heightened contextual coherence over reducing KV cache size. Building upon the hybrid-head concept from Hymba, WuNeng augments standard multi-head attention with additional RWKV-7 state-driven heads, rather than replacing existing heads, to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among standard, state-driven, and newly introduced middle heads, leveraging concatenation, additive modulation, and gated fusion for robust information integration. Furthermore, a multi-token state processing mechanism harnesses the continuous RWKV-7 state to capture intricate, sequence-wide dependencies, significantly boosting expressivity. Remarkably, these enhancements are achieved with minimal additional parameters, ensuring efficiency while empowering the model to excel in complex reasoning and sequence generation tasks. WuNeng sets a new standard for balancing expressivity and computational efficiency in modern neural architectures.

**Link**: [arxiv](http://arxiv.org/abs/2504.19191v1),  [pdf](http://arxiv.org/pdf/2504.19191v1)

**Tags**: cs.CL 



### I Know What You Sync: Covert and Side Channel Attacks on File Systems   via syncfs
**Authors**: Cheng Gu, Yicheng Zhang, Nael Abu-Ghazaleh

**Updated**: 2025-04-26T12:07:35Z

**Summary**: Operating Systems enforce logical isolation using abstractions such as processes, containers, and isolation technologies to protect a system from malicious or buggy code. In this paper, we show new types of side channels through the file system that break this logical isolation. The file system plays a critical role in the operating system, managing all I/O activities between the application layer and the physical storage device. We observe that the file system implementation is shared, leading to timing leakage when using common I/O system calls. Specifically, we found that modern operating systems take advantage of any flush operation (which saves cached blocks in memory to the SSD or disk) to flush all of the I/O buffers, even those used by other isolation domains. Thus, by measuring the delay of syncfs, the attacker can infer the I/O behavior of victim programs. We then demonstrate a syncfs covert channel attack on multiple file systems, including both Linux native file systems and the Windows file system, achieving a maximum bandwidth of 5 Kbps with an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on Windows. In addition, we construct three side-channel attacks targeting both Linux and Android devices. On Linux devices, we implement a website fingerprinting attack and a video fingerprinting attack by tracking the write patterns of temporary buffering files. On Android devices, we design an application fingerprinting attack that leaks application write patterns during boot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally, we demonstrate that these attacks can be exploited across containers implementing a container detection technique and a cross-container covert channel attack.

**Link**: [arxiv](http://arxiv.org/abs/2411.10883v2),  [pdf](http://arxiv.org/pdf/2411.10883v2)

**Tags**: cs.CR 



### ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM   Inference
**Authors**: Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen

**Updated**: 2025-04-25T19:40:54Z

**Summary**: With the widespread deployment of long-context large language models (LLMs), there has been a growing demand for efficient support of high-throughput inference. However, as the key-value (KV) cache expands with the sequence length, the increasing memory footprint and the need to access it for each token generation both result in low throughput when serving long-context LLMs. While various dynamic sparse attention methods have been proposed to speed up inference while maintaining generation quality, they either fail to sufficiently reduce GPU memory consumption or introduce significant decoding latency by offloading the KV cache to the CPU. We present ShadowKV, a high-throughput long-context LLM inference system that stores the low-rank key cache and offloads the value cache to reduce the memory footprint for larger batch sizes and longer sequences. To minimize decoding latency, ShadowKV employs an accurate KV selection strategy that reconstructs minimal sparse KV pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks, including RULER, LongBench, and Needle In A Haystack, and models like Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without sacrificing accuracy, even surpassing the performance achievable with infinite batch size under the assumption of infinite GPU memory. The code is available at https://github.com/bytedance/ShadowKV.

**Link**: [arxiv](http://arxiv.org/abs/2410.21465v3),  [pdf](http://arxiv.org/pdf/2410.21465v3)

**Tags**: cs.LG cs.CL 



### Constructing Hamiltonian Decompositions of Complete $k$-Uniform   Hypergraphs
**Authors**: Javad Maheri, Petros Elia

**Updated**: 2025-04-25T15:45:36Z

**Summary**: Motivated by the wide-ranging applications of Hamiltonian decompositions in distributed computing, coded caching, routing, resource allocation, load balancing, and fault tolerance, our work presents a comprehensive design for Hamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$. Building upon the resolution of the long-standing conjecture of the existence of Hamiltonian decompositions of complete hypergraphs, a problem that was resolved using existence-based methods, our contribution goes beyond the previous explicit designs, which were confined to the specific cases of $k=2$ and $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing for a broad applicability of Hamiltonian decompositions in various settings.

**Link**: [arxiv](http://arxiv.org/abs/2504.18434v1),  [pdf](http://arxiv.org/pdf/2504.18434v1)

**Tags**: cs.IT math.IT 



### FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack
**Authors**: Xuzheng Chen, Jie Zhang, Baolin Zhu, Xueying Zhu, Zhongqing Chen, Shu Ma, Lingjun Zhu, Chao Shi, Yin Zhang, Zeke Wang

**Updated**: 2025-04-25T15:44:38Z

**Summary**: As the gap between network and CPU speeds rapidly increases, the CPU-centric network stack proves inadequate due to excessive CPU and memory overhead. While hardware-offloaded network stacks alleviate these issues, they suffer from limited flexibility in both control and data planes. Offloading network stack to off-path SmartNIC seems promising to provide high flexibility; however, throughput remains constrained by inherent SmartNIC architectural limitations.   To this end, we design FlexiNS, a SmartNIC-centric network stack with software transport programmability and line-rate packet processing capabilities. To grapple with the limitation of SmartNIC-induced challenges, FlexiNS introduces: (a) a header-only offloading TX path; (b) an unlimited-working-set in-cache processing RX path; (c) a high-performance DMA-only notification pipe; and (d) a programmable offloading engine. We prototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box RDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\times$ higher throughput than the microkernel-based baseline in block storage disaggregation and 1.3$\times$ higher throughput than the hardware-offloaded baseline in KVCache transfer.

**Link**: [arxiv](http://arxiv.org/abs/2504.18432v1),  [pdf](http://arxiv.org/pdf/2504.18432v1)

**Tags**: cs.NI 



### Demand Private Coded Caching: Small Cache Size
**Authors**: Qinyi Lu, Nan Liu, Wei Kang, Chunguo Li

**Updated**: 2025-04-25T10:43:23Z

**Summary**: We investigate the demand private coded caching problem, which is an $(N,K)$ coded caching problem with $N$ files, $K$ users, each equipped with a cache of size $M$, and an additional privacy constraint on user demands, i.e., each user can not gain any information about the demands of other users. We focus on scenarios where the size of users' caches is small, aiming to further characterize the fundamental limits of this problem. We first present a new virtual-user-based achievable scheme for arbitrary number of users and files, and two MDS-code-based achievable schemes for the case $N \le K$. With a newly derived converse bound for the case $N \le K$, these proposed schemes lead to the optimal memory-rate tradeoff of the demand private coded caching problem for $M \in \big[0, \frac{N}{(K+1)(N-1)} \big] $ where $N \le K \le 2N-2$, and the optimal memory-rate tradeoff for $M \in \big[0, \frac{1}{K+1} \big] $ where $ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users, by deriving another new converse bound, the optimal memory-rate tradeoff is characterized for $M\in \big[0,\frac{2}{K}\big] \cup \big[\frac{2(K-1)}{K+1},2\big]$. Finally, we provide the optimal memory-rate tradeoff of the demand private coded caching problem for 2 files and 3 users.

**Link**: [arxiv](http://arxiv.org/abs/2504.18242v1),  [pdf](http://arxiv.org/pdf/2504.18242v1)

**Tags**: cs.IT math.IT 



### Efficient GNN Training Through Structure-Aware Randomized Mini-Batching
**Authors**: Vignesh Balaji, Christos Kozyrakis, Gal Chechik, Haggai Maron

**Updated**: 2025-04-25T05:16:53Z

**Summary**: Graph Neural Networks (GNNs) enable learning on realworld graphs and mini-batch training has emerged as the de facto standard for training GNNs because it can scale to very large graphs and improve convergence. Current mini-batch construction policies largely ignore efficiency considerations of GNN training. Specifically, existing mini-batching techniques employ randomization schemes to improve accuracy and convergence. However, these randomization schemes are often agnostic to the structural properties of the graph (for eg. community structure), resulting in highly irregular memory access patterns during GNN training that make suboptimal use of on-chip GPU caches. On the other hand, while deterministic mini-batching based solely on graph structure delivers fast runtime performance, the lack of randomness compromises both the final model accuracy and training convergence speed. In this paper, we present Community-structure-aware Randomized Mini-batching (COMM-RAND), a novel methodology that bridges the gap between the above extremes. COMM-RAND allows practitioners to explore the space between pure randomness and pure graph structural awareness during mini-batch construction, leading to significantly more efficient GNN training with similar accuracy. We evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an accuracy that is within 1.79% points (0.42% on average) compared to popular random mini-batching approaches.

**Link**: [arxiv](http://arxiv.org/abs/2504.18082v1),  [pdf](http://arxiv.org/pdf/2504.18082v1)

**Tags**: cs.LG cs.AI 



### Optimizing ML Concurrent Computation and Communication with GPU DMA   Engines
**Authors**: Anirudha Agrawal, Shaizeen Aga, Suchita Pati, Mahzabeen Islam

**Updated**: 2025-04-25T05:08:45Z

**Summary**: Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). That is, C3 on average achieves only 21% of ideal speedup. This is so, due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).   To attain better performance for C3, first, we evaluate dual strategies of schedule prioritization and careful resource partitioning of compute units on GPUs to push performance attained with C3 (on average 42% of ideal speedup). We also provide heuristics that can guide a runtime while employing these strategies. To further enhance C3 performance, we propose to mitigate C3 interference by offloading communication tasks to the GPU's DMA engines. To this end, we build concurrent communication collectives (ConCCL) proof-of-concepts that harness DMA engines for communication. We show how ConCCL considerably closes the gap between realized and ideal speedup for C3 (on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall, our work makes a strong case for GPU DMA engine advancements to better support C3 on GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2412.14335v2),  [pdf](http://arxiv.org/pdf/2412.14335v2)

**Tags**: cs.AR cs.DC 



### Fluctuated lattice-driven charge density wave far above the condensation   temperature in kagome superconductor KV$_3$Sb$_5$
**Authors**: Haoran Liu, Shaofeng Duan, Xiangqi Liu, Zhihua Liu, Shichong Wang, Lingxiao Gu, Jiongyu Huang, Wenxuan Yang, Jianzhe Liu, Dong Qian, Yanfeng Guo, Wentao Zhang

**Updated**: 2025-04-25T05:05:49Z

**Summary**: The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including an unconventional charge density wave (CDW). Elucidating the underlying mechanism behind the CDW transition is crucial for unraveling the complex interactions among these phases. However, the driving force of the CDW remains a topic of debate due to the intertwined interactions among the system's various excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by isolating the ultrafast electronic phase transition using time- and angleresolved photoemission spectroscopy. An ultrafast electronic phase transition was observed at a critical photoexcitation fluence, F$_c$, without reduction in CDW lattice-distortion-induced band folding. This folded band persisted up to 150 K under equilibrium heating, well above the CDW condensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts at F$_c$ were comparable to those caused by thermal effects at T$_c$. These findings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane CDW emerges above 150 K, with out-of-plane electronic correlations leading to the $2\times2 \times 2$ CDW near T$_c$, offering key insights into the interplay between the electronic and structural dynamics in AV$_3$Sb$_5$.

**Link**: [arxiv](http://arxiv.org/abs/2504.16620v2),  [pdf](http://arxiv.org/pdf/2504.16620v2)

**Tags**: cond-mat.str-el cond-mat.mtrl-sci cond-mat.supr-con 



### Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A   first-principles DFT+$U$+$V$ study
**Authors**: Indukuru Ramesh Reddy, Sayandeep Ghosh, Bongjae Kim, Chang-Jong Kang

**Updated**: 2025-04-25T00:41:43Z

**Summary**: Nonlocal Coulomb interactions play a crucial role in stabilizing distinct electronic phases in kagome materials. In this work, we systematically investigate the effects of on-site ($U$) and inter-site ($V$) Coulomb interactions on the electronic structure and stability of charge-density-wave (CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory (DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and stability of CDW phases, whereas $U$ suppresses these phases, highlighting a fundamental competition between local and nonlocal electronic correlations. By directly comparing our theoretical results with angle-resolved photoemission spectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that accurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings establish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable insights into the correlated electronic states in kagome metals and serving as a foundation for future explorations of correlation-driven phenomena in related materials.

**Link**: [arxiv](http://arxiv.org/abs/2504.17995v1),  [pdf](http://arxiv.org/pdf/2504.17995v1)

**Tags**: cond-mat.str-el 



### Updated parameters of the LArQL model
**Authors**: L. Paulucci, F. Cavanna, V. Vale, F. Marinho

**Updated**: 2025-04-24T18:09:25Z

**Summary**: The need for a microscopic description of scintillation light generation in liquid argon becomes increasingly desirable with the upcoming operation of large scale LArTPCs in the next decade. While a detailed mathematical account of the process is still to be achieved, a phenomenological model for simultaneously treating ionization and scintillation, LArQL, has been successfully employed to describe the range of electric fields from 0 to 0.75 kV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the free ionization charge and scintillation light. A reanalysis of the original model parameter values has been performed within a global fit procedure and is presented.

**Link**: [arxiv](http://arxiv.org/abs/2504.17866v1),  [pdf](http://arxiv.org/pdf/2504.17866v1)

**Tags**: hep-ex 



### L3: DIMM-PIM Integrated Architecture and Coordination for Scalable   Long-Context LLM Inference
**Authors**: Qingyuan Liu, Liyan Chen, Yanning Yang, Haocheng Wang, Dong Du, Zhigang Mao, Naifeng Jing, Yubin Xia, Haibo Chen

**Updated**: 2025-04-24T14:14:07Z

**Summary**: Large Language Models (LLMs) increasingly require processing long text sequences, but GPU memory limitations force difficult trade-offs between memory capacity and bandwidth. While HBM-based acceleration offers high bandwidth, its capacity remains constrained. Offloading data to host-side DIMMs improves capacity but introduces costly data swapping overhead. We identify that the critical memory bottleneck lies in the decoding phase of multi-head attention (MHA) exclusively, which demands substantial capacity for storing KV caches and high bandwidth for attention computation. Our key insight reveals this operation uniquely aligns with modern DIMM-based processing-in-memory (PIM) architectures, which offers scalability of both capacity and bandwidth.   Based on this observation and insight, we propose L3, a hardware-software co-designed system integrating DIMM-PIM and GPU devices. L3 introduces three innovations: First, hardware redesigns resolve data layout mismatches and computational element mismatches in DIMM-PIM, enhancing LLM inference utilization. Second, communication optimization enables hiding the data transfer overhead with the computation. Third, an adaptive scheduler coordinates GPU-DIMM-PIM operations to maximize parallelism between devices. Evaluations using real-world traces show L3 achieves up to 6.1$\times$ speedup over state-of-the-art HBM-PIM solutions while significantly improving batch sizes.

**Link**: [arxiv](http://arxiv.org/abs/2504.17584v1),  [pdf](http://arxiv.org/pdf/2504.17584v1)

**Tags**: cs.AR cs.LG 



### Rethinking PM Crash Consistency in the CXL Era
**Authors**: João Oliveira, João Gonçalves, Miguel Matos

**Updated**: 2025-04-24T13:47:35Z

**Summary**: Persistent Memory (PM) introduces new opportunities for designing crash-consistent applications without the traditional storage overheads. However, ensuring crash consistency in PM demands intricate knowledge of CPU, cache, and memory interactions. Hardware and software mechanisms have been proposed to ease this burden, but neither proved sufficient, prompting a variety of bug detection tools.   With the sunset of Intel Optane comes the rise of Compute Express Link (CXL) for PM. In this position paper, we discuss the impact of CXL's disaggregated and heterogeneous nature in the development of crash-consistent PM applications, and outline three research directions: hardware primitives, persistency frameworks, and bug detection tools.

**Link**: [arxiv](http://arxiv.org/abs/2504.17554v1),  [pdf](http://arxiv.org/pdf/2504.17554v1)

**Tags**: cs.ET 



### Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent   Interconnects
**Authors**: Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe

**Updated**: 2025-04-24T08:39:13Z

**Summary**: Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA) to offload data transfer, descriptor rings for buffering and queuing, and interrupts for asynchrony between cores and device.   In this paper we question this wisdom in the light of two trends: modern and emerging cache-coherent interconnects like CXL3.0, and workloads, particularly microservices and serverless computing. Like some others before us, we argue that the assumptions of the DMA-based model are obsolete, and in many use-cases programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, delivers a more efficient system.   However, we push this idea much further. We show, in a real hardware implementation, the gains in latency for fine-grained communication achievable using an open cache-coherence protocol which exposes cache transitions to a smart device, and that throughput is competitive with DMA over modern interconnects. We also demonstrate three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting serverless functions, comparing our use of coherence with both traditional DMA-style interaction and a highly-optimized implementation using memory-mapped programmed I/O over PCIe.

**Link**: [arxiv](http://arxiv.org/abs/2409.08141v3),  [pdf](http://arxiv.org/pdf/2409.08141v3)

**Tags**: cs.AR cs.OS 



### SPAARC: Spatial Proximity and Association based prefetching for   Augmented Reality in edge Cache
**Authors**: Nikhil Sreekumar, Abhishek Chandra, Jon Weissman

**Updated**: 2025-04-24T04:36:20Z

**Summary**: Mobile Augmented Reality (MAR) applications face performance challenges due to their high computational demands and need for low-latency responses. Traditional approaches like on-device storage or reactive data fetching from the cloud often result in limited AR experiences or unacceptable lag. Edge caching, which caches AR objects closer to the user, provides a promising solution. However, existing edge caching approaches do not consider AR-specific features such as AR object sizes, user interactions, and physical location. This paper investigates how to further optimize edge caching by employing AR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and Association-based Prefetching policy specifically designed for MAR Caches. SPAARC intelligently prioritizes the caching of virtual objects based on their association with other similar objects and the user's proximity to them. It also considers the recency of associations and uses a lazy fetching strategy to efficiently manage edge resources and maximize Quality of Experience (QoE).   Through extensive evaluation using both synthetic and real-world workloads, we demonstrate that SPAARC significantly improves cache hit rates compared to standard caching algorithms, achieving gains ranging from 3% to 40% while reducing the need for on-demand data retrieval from the cloud. Further, we present an adaptive tuning algorithm that automatically tunes SPAARC parameters to achieve optimal performance. Our findings demonstrate the potential of SPAARC to substantially enhance the user experience in MAR applications by ensuring the timely availability of virtual objects.

**Link**: [arxiv](http://arxiv.org/abs/2502.15192v2),  [pdf](http://arxiv.org/pdf/2502.15192v2)

**Tags**: cs.ET cs.DC 



### Efficient Pretraining Length Scaling
**Authors**: Bohong Wu, Shen Yan, Sijun Zhang, Jianqiao Lu, Yutao Zeng, Ya Wang, Xun Zhou

**Updated**: 2025-04-24T04:13:49Z

**Summary**: Recent advances in large language models have demonstrated the effectiveness of length scaling during post-training, yet its potential in pre-training remains underexplored. We present the Parallel Hidden Decoding Transformer (\textit{PHD}-Transformer), a novel framework that enables efficient length scaling during pre-training while maintaining inference efficiency. \textit{PHD}-Transformer achieves this through an innovative KV cache management strategy that distinguishes between original tokens and hidden decoding tokens. By retaining only the KV cache of original tokens for long-range dependencies while immediately discarding hidden decoding tokens after use, our approach maintains the same KV cache size as the vanilla transformer while enabling effective length scaling. To further enhance performance, we introduce two optimized variants: \textit{PHD-SWA} employs sliding window attention to preserve local dependencies, while \textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate linear growth in pre-filling time. Extensive experiments demonstrate consistent improvements across multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.14992v2),  [pdf](http://arxiv.org/pdf/2504.14992v2)

**Tags**: cs.CL 



### Cognitive Memory in Large Language Models
**Authors**: Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu

**Updated**: 2025-04-24T01:47:25Z

**Summary**: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.

**Link**: [arxiv](http://arxiv.org/abs/2504.02441v2),  [pdf](http://arxiv.org/pdf/2504.02441v2)

**Tags**: cs.CL cs.AI 



### KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM   Inference in Resource-Constrained Environments
**Authors**: Junyoung Park, Dalton Jones, Matt J Morse, Raghavv Goel, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T18:02:55Z

**Summary**: In this work, we demonstrate that distinctive keys during LLM inference tend to have high attention scores. We explore this phenomenon and propose KeyDiff, a training-free KV cache eviction method based on key similarity. This method facilitates the deployment of LLM-based application requiring long input prompts in resource-constrained environments with limited memory and compute budgets. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses. We demonstrate that KeyDiff computes the optimal solution to a KV cache selection problem that maximizes key diversity, providing a theoretical understanding of KeyDiff. Notably,KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. We demonstrate the effectiveness of KeyDiff across diverse tasks and models, illustrating a performance gap of less than 0.04\% with 8K cache budget ($\sim$ 23\% KV cache reduction) from the non-evicting baseline on the LongBench benchmark for Llama 3.1-8B and Llama 3.2-3B.

**Link**: [arxiv](http://arxiv.org/abs/2504.15364v2),  [pdf](http://arxiv.org/pdf/2504.15364v2)

**Tags**: cs.AI 



### Iris: A Next Generation Digital Pathology Rendering Engine
**Authors**: Ryan Erik Landvater, Ulysses Balis

**Updated**: 2025-04-23T15:02:16Z

**Summary**: Digital pathology is a tool of rapidly evolving importance within the discipline of pathology. Whole slide imaging promises numerous advantages; however, adoption is limited by challenges in ease of use and speed of high-quality image rendering relative to the simplicity and visual quality of glass slides. We introduce Iris, a new high-performance digital pathology rendering system. Specifically, we outline and detail the performance metrics of Iris Core, the core rendering engine technology. Iris Core comprises machine code modules written from the ground up in C++ and using Vulkan, a low-level and low-overhead cross-platform graphical processing unit application program interface, and our novel rapid tile buffering algorithms. We provide a detailed explanation of Iris Core's system architecture, including the stateless isolation of core processes, interprocess communication paradigms, and explicit synchronization paradigms that provide powerful control over the graphical processing unit. Iris Core achieves slide rendering at the sustained maximum frame rate on all tested platforms and buffers an entire new slide field of, view without overlapping pixels, in 10 ms with enhanced detail in 30 ms. It is able to buffer and compute high-fidelity reduction-enhancements for viewing low-power cytology with increased visual quality at a rate of 100-160 us per slide tile, and with a cumulative median buffering rate of 1.36 GB of decompressed image data per second. This buffering rate allows for an entirely new field of view to be fully buffered and rendered in less than a single monitor refresh on a standard display, and high detail features within 2-3 monitor refresh frames. These metrics far exceed previously published specifications, beyond an order of magnitude in some contexts. The system shows no slowing with high use loads, but rather increases performance due to cache mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2504.15437v2),  [pdf](http://arxiv.org/pdf/2504.15437v2)

**Tags**: cs.GR 



### The NIC should be part of the OS
**Authors**: Pengcheng Xu, Timothy Roscoe

**Updated**: 2025-04-23T10:48:52Z

**Summary**: The network interface adapter (NIC) is a critical component of a cloud server occupying a unique position. Not only is network performance vital to efficient operation of the machine, but unlike compute accelerators like GPUs, the network subsystem must react to unpredictable events like the arrival of a network packet and communicate with the appropriate application end point with minimal latency.   Current approaches to server stacks navigate a trade-off between flexibility, efficiency, and performance: the fastest kernel-bypass approaches dedicate cores to applications, busy-wait on receive queues, etc. while more flexible approaches appropriate to more dynamic workload mixes incur much greater software overhead on the data path.   However, we reject this trade-off, which we ascribe to an arbitrary (and sub-optimal) split in system state between the OS and the NIC. Instead, by exploiting the properties of cache-coherent interconnects and integrating the NIC closely with the OS kernel, we can achieve something surprising: performance for RPC workloads better than the fastest kernelbypass approaches without sacrificing the robustness and dynamic adaptation of kernel-based network subsystems.

**Link**: [arxiv](http://arxiv.org/abs/2501.10138v2),  [pdf](http://arxiv.org/pdf/2501.10138v2)

**Tags**: cs.OS cs.AR cs.NI 



### CAOTE: KV Caching through Attention Output Error based Token Eviction
**Authors**: Raghavv Goel, Junyoung Park, Mukul Gagrani, Dalton Jones, Matthew Morse, Harper Langston, Mingu Lee, Chris Lott

**Updated**: 2025-04-23T05:04:58Z

**Summary**: While long context support of large language models has extended their abilities, it also incurs challenges in memory and compute which becomes crucial bottlenecks in resource-restricted devices. Token eviction, a widely adopted post-training methodology designed to alleviate the bottlenecks by evicting less important tokens from the cache, typically uses attention scores as proxy metrics for token importance. However, one major limitation of attention score as a token-wise importance metrics is that it lacks the information about contribution of tokens to the attention output. In this paper, we propose a simple eviction criterion based on the contribution of cached tokens to attention outputs. Our method, CAOTE, optimizes for eviction error due to token eviction, by seamlessly integrating attention scores and value vectors. This is the first method which uses value vector information on top of attention-based eviction scores. Additionally, CAOTE can act as a meta-heuristic method with flexible usage with any token eviction method. We show that CAOTE, when combined with the state-of-the-art attention score-based methods, always improves accuracies on the downstream task, indicating the importance of leveraging information from values during token eviction process.

**Link**: [arxiv](http://arxiv.org/abs/2504.14051v2),  [pdf](http://arxiv.org/pdf/2504.14051v2)

**Tags**: cs.LG 



### ML-based Adaptive Prefetching and Data Placement for US HEP Systems
**Authors**: Venkat Sai Suman Lamba Karanam, Sarat Sasank Barla, Byrav Ramamurthy, Derek Weitzel

**Updated**: 2025-04-23T04:21:49Z

**Summary**: Although benefits from caching in US HEP are well-known, current caching strategies are not adaptive i.e they do not adapt to changing cache access patterns. Newer developments such as the High-Luminosity - Large Hadron Collider (HL-LHC), Deep Underground Neutrino Experiment (DUNE), a steady move toward streaming readout based Data Acquisition systems (DAQs) will increase the data production exponentially and hence burden the storage, compute & network infrastructures. Moreover, existing caching frameworks are optimized to reduce latency, but not optimized for storage. This, in combination with limited cache capacities relative to total data, makes it difficult to achieve data locality.   In this work, we present Machine Learning-aided (ML) caching strategies. Specifically, we first present a Long Short-Term Memory-based (LSTM) hourly and multi-step cache usage prediction. Second, we present an hourly file-level access prediction model based on CatboostRegressor. To date, most ML-based cache prediction strategies in HEP have focused on daily cache usage and limited works tackled hourly cache usage and even fewer strategies addressed hourly file-level access prediction. File-level access prediction allows for the design of intelligent prefetching and data placement strategies with fine-grained control. We validated our cache prediction strategies using data collected from SoCal MINI caches in August 2024. We are currently extending the WRENCH simulator to reflect the US HEP ecosystem at the storage, network and compute levels. We plan to deploy our cache prediction strategies into WRENCH and later perform extensive analysis with complex data access patterns and candidate infrastructure configurations.

**Link**: [arxiv](http://arxiv.org/abs/2503.06015v2),  [pdf](http://arxiv.org/pdf/2503.06015v2)

**Tags**: cs.DC 



### The Dawn of Disaggregation and the Coherence Conundrum: A Call for   Federated Coherence
**Authors**: Jaewan Hong, Marcos K. Aguilera, Emmanuel Amaro, Vincent Liu, Aurojit Panda, Ion Stoica

**Updated**: 2025-04-22T23:52:13Z

**Summary**: Disaggregated memory is an upcoming data center technology that will allow nodes (servers) to share data efficiently. Sharing data creates a debate on the level of cache coherence the system should provide. While current proposals aim to provide coherence for all or parts of the disaggregated memory, we argue that this approach is problematic, because of scalability limitations and hardware complexity. Instead, we propose and formally define federated coherence, a model that provides coherence only within nodes, not across nodes. Federated coherence can use current intra-node coherence provided by processors without requiring expensive mechanisms for inter-node coherence. Developers can use federated coherence with a few simple programming paradigms and a synchronization library. We sketch some potential applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.16324v1),  [pdf](http://arxiv.org/pdf/2504.16324v1)

**Tags**: cs.DC cs.AR 



### Key, Value, Compress: A Systematic Exploration of KV Cache Compression   Techniques
**Authors**: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

**Updated**: 2025-04-22T17:34:34Z

**Summary**: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.

**Link**: [arxiv](http://arxiv.org/abs/2503.11816v3),  [pdf](http://arxiv.org/pdf/2503.11816v3)

**Tags**: cs.CL 



### GainSight: Application-Guided Profiling for Composing Heterogeneous   On-Chip Memories in AI Hardware Accelerators
**Authors**: Peijing Li, Matthew Hung, Yiming Tan, Konstantin Hoßfeld, Jake Cheng Jiajun, Shuhan Liu, Lixian Yan, Xinxin Wang, H. -S. Philip Wong, Thierry Tambe

**Updated**: 2025-04-22T17:23:28Z

**Summary**: As AI workloads drive soaring memory requirements, there is a need for higher-density on-chip memory for domain-specific accelerators that goes beyond what current SRAM technology can provide. We motivate that algorithms and application behavior should guide the composition of heterogeneous on-chip memories. However, there has been little work in factoring dynamic application profiles into such design decisions. We present GainSight, a profiling framework that analyzes fine-grained memory access patterns and computes data lifetimes in domain-specific accelerators. By combining instrumentation and simulation across retargetable hardware backends, GainSight aligns heterogeneous memory designs with workload-specific traffic and lifetime metrics. Case studies on MLPerf Inference and PolyBench workloads using NVIDIA H100 GPUs and systolic arrays reveal key insights: (1) 40% of L1 and 18% of L2 GPU cache accesses, and 79% of systolic array scratchpad accesses across profiled workloads are short-lived and suitable for silicon-based gain cell RAM (Si-GCRAM); (2) Si-GCRAM reduces active energy by 11-28% compared to SRAM; (3) Up to 90% of GPU cache fetches are never reused, highlighting inefficiencies in terms of cache pollution. These insights that GainSight provides can be used to better understand the design spaces of both emerging on-chip memories and software algorithmic optimizations for the next generation of AI accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2504.14866v2),  [pdf](http://arxiv.org/pdf/2504.14866v2)

**Tags**: cs.AR cs.ET B.7.1; B.3.1; C.3; I.6; I.2.6 



### Optimizing SLO-oriented LLM Serving with PD-Multiplexing
**Authors**: Weihao Cui, Yukang Chen, Han Zhao, Ziyi Xu, Quan Chen, Xusheng Chen, Yangjie Zhou, Shixuan Sun, Minyi Guo

**Updated**: 2025-04-22T15:19:48Z

**Summary**: Modern LLM services demand high throughput and stringent SLO guarantees across two distinct inference phases-prefill and decode-and complex multi-turn workflows. However, current systems face a fundamental tradeoff: out-of-place compute partition enables per-phase SLO attainment, while in-place memory sharing maximizes throughput via KV cache reuse. Moreover, existing in-place compute partition also encounters low utilization and high overhead due to phase-coupling design. We present Drift, a new LLM serving framework that resolves this tension via PD multiplexing, enabling in-place and phase-decoupled compute partition. Drift leverages low-level GPU partitioning techniques to multiplex prefill and decode phases spatially and adaptively on shared GPUs, while preserving in-place memory sharing. To fully leverage the multiplexing capability, Drift introduces an adaptive gang scheduling mechanism, a contention-free modeling method, and a SLO-aware dispatching policy. Evaluation shows that Drift achieves an average $5.1\times$ throughput improvement (up to $17.5\times$) over state-of-the-art baselines, while consistently meeting SLO targets under complex LLM workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.14489v2),  [pdf](http://arxiv.org/pdf/2504.14489v2)

**Tags**: cs.OS 



### SeaLLM: Service-Aware and Latency-Optimized Resource Sharing for Large   Language Model Inference
**Authors**: Yihao Zhao, Jiadun Chen, Peng Sun, Lei Li, Xuanzhe Liu, Xin Jin

**Updated**: 2025-04-22T09:08:46Z

**Summary**: Large language models (LLMs) with different architectures and sizes have been developed. Serving each LLM with dedicated GPUs leads to resource waste and service inefficiency due to the varying demand of LLM requests. A common practice is to share multiple LLMs. However, existing sharing systems either do not consider the autoregressive pattern of LLM services, or only focus on improving the throughput, which impairs the sharing performance, especially the serving latency. We present SeaLLM, which enables service-aware and latency-optimized LLM sharing. SeaLLM improves the overall sharing performance by (1) a latency-optimized scheduling algorithm utilizing the characteristics of LLM services, (2) a placement algorithm to determine the placement plan and an adaptive replacement algorithm to decide the replacement interval, and (3) a unified key-value cache to share GPU memory among LLM services efficiently. Our evaluation under real-world traces and LLM services demonstrates that SeaLLM improves the normalized latency by up to $13.60\times$, the tail latency by up to $18.69\times$, and the SLO attainment by up to $3.64\times$ compared to existing solutions.

**Link**: [arxiv](http://arxiv.org/abs/2504.15720v1),  [pdf](http://arxiv.org/pdf/2504.15720v1)

**Tags**: cs.DC 



### Reimagining Memory Access for LLM Inference: Compression-Aware Memory   Controller Design
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-04-21T22:13:07Z

**Summary**: The efficiency of Large Language Model~(LLM) inference is often constrained by substantial memory bandwidth and capacity demands. Existing techniques, such as pruning, quantization, and mixture of experts/depth, reduce memory capacity and/or bandwidth consumption at the cost of slight degradation in inference quality. This paper introduces a design solution that further alleviates memory bottlenecks by enhancing the on-chip memory controller in AI accelerators to achieve two main objectives: (1) significantly reducing memory capacity and bandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of model weights and key-value (KV) cache without compromising inference quality, and (2) enabling memory bandwidth and energy consumption to scale proportionally with context-dependent dynamic quantization. These goals are accomplished by equipping the on-chip memory controller with mechanisms to improve fine-grained bit-level accessibility and compressibility of weights and KV cache through LLM-aware configuration of in-memory placement and representation. Experimental results on publicly available LLMs demonstrate the effectiveness of this approach, showing memory footprint reductions of 25.2\% for model weights and 46.9\% for KV cache. In addition, our hardware prototype at 4\,GHz and 32 lanes (7\,nm) achieves 8\,TB/s throughput with a modest area overhead (under 3.8\,mm\(^2\)), which underscores the viability of LLM-aware memory control as a key to efficient large-scale inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.18869v3),  [pdf](http://arxiv.org/pdf/2503.18869v3)

**Tags**: cs.AR 



### FlashInfer: Efficient and Customizable Attention Engine for LLM   Inference Serving
**Authors**: Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, Luis Ceze

**Updated**: 2025-04-21T20:10:11Z

**Summary**: Transformers, driven by attention mechanisms, form the foundation of large language models (LLMs). As these models scale up, efficient GPU attention kernels become essential for high-throughput and low-latency inference. Diverse LLM applications demand flexible and high-performance attention solutions. We present FlashInfer: a customizable and efficient attention engine for LLM serving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse format and composable formats to optimize memory access and reduce redundancy. It also offers a customizable attention template, enabling adaptation to various settings through Just-In-Time (JIT) compilation. Additionally, FlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user requests while maintaining compatibility with CUDAGraph which requires static configuration. FlashInfer have been integrated into leading LLM serving frameworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and end-to-end evaluations demonstrate FlashInfer's ability to significantly boost kernel performance across diverse inference scenarios: compared to state-of-the-art LLM serving solutions, FlashInfer achieve 29-69% inter-token-latency reduction compared to compiler backends for LLM serving benchmark, 28-30% latency reduction for long-context inference, and 13-17% speedup for LLM serving with parallel generation.

**Link**: [arxiv](http://arxiv.org/abs/2501.01005v2),  [pdf](http://arxiv.org/pdf/2501.01005v2)

**Tags**: cs.DC cs.AI cs.LG 



### Joint Knowledge and Power Management for Secure Semantic Communication   Networks
**Authors**: Xuesong Liu, Yansong Liu, Haoyu Tang, Fangzhou Zhao, Le Xia, Yao Sun

**Updated**: 2025-04-21T17:39:59Z

**Summary**: Recently, semantic communication (SemCom) has shown its great superiorities in resource savings and information exchanges. However, while its unique background knowledge guarantees accurate semantic reasoning and recovery, semantic information security-related concerns are introduced at the same time. Since the potential eavesdroppers may have the same background knowledge to accurately decrypt the private semantic information transmitted between legal SemCom users, this makes the knowledge management in SemCom networks rather challenging in joint consideration with the power control. To this end, this paper focuses on jointly addressing three core issues of power allocation, knowledge base caching (KBC), and device-to-device (D2D) user pairing (DUP) in secure SemCom networks. We first develop a novel performance metric, namely semantic secrecy throughput (SST), to quantify the information security level that can be achieved at each pair of D2D SemCom users. Next, an SST maximization problem is formulated subject to secure SemCom-related delay and reliability constraints. Afterward, we propose a security-aware resource management solution using the Lagrange primal-dual method and a two-stage method. Simulation results demonstrate our proposed solution nearly doubles the SST performance and realizes less than half of the queuing delay performance compared to different benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2504.15260v1),  [pdf](http://arxiv.org/pdf/2504.15260v1)

**Tags**: eess.SP 



### Lance: Efficient Random Access in Columnar Storage through Adaptive   Structural Encodings
**Authors**: Weston Pace, Chang She, Lei Xu, Will Jones, Albert Lockett, Jun Wang, Raunak Shah

**Updated**: 2025-04-21T17:22:18Z

**Summary**: The growing interest in artificial intelligence has created workloads that require both sequential and random access. At the same time, NVMe-backed storage solutions have emerged, providing caching capability for large columnar datasets in cloud storage. Current columnar storage libraries fall short of effectively utilizing an NVMe device's capabilities, especially when it comes to random access. Historically, this has been assumed an implicit weakness in columnar storage formats, but this has not been sufficiently explored. In this paper, we examine the effectiveness of popular columnar formats such as Apache Arrow, Apache Parquet, and Lance in both random access and full scan tasks against NVMe storage.   We argue that effective encoding of a column's structure, such as the repetition and validity information, is the key to unlocking the disk's performance. We show that Parquet, when configured correctly, can achieve over 60x better random access performance than default settings. We also show that this high random access performance requires making minor trade-offs in scan performance and RAM utilization. We then describe the Lance structural encoding scheme, which alternates between two different structural encodings based on data width, and achieves better random access performance without making trade-offs in scan performance or RAM utilization.

**Link**: [arxiv](http://arxiv.org/abs/2504.15247v1),  [pdf](http://arxiv.org/pdf/2504.15247v1)

**Tags**: cs.DB H.3.2 



### A Unified Framework for Quantitative Cache Analysis
**Authors**: Sophie Kahlen, Jan Reineke

**Updated**: 2025-04-21T15:36:53Z

**Summary**: In this work we unify two existing lines of work towards cache analysis for non-LRU policies. To this end, we extend the notion of competitiveness to block competitiveness and systematically analyze the competitiveness and block competitiveness of FIFO and MRU relative to LRU for arbitrary associativities. We show how competitiveness and block competitiveness can be exploited in state-of-the-art WCET analysis based on the results of existing persistence analyses for LRU. Unlike prior work, our approach is applicable to microarchitectures that exhibit timing anomalies. We experimentally evaluate the precision and cost of our approach on benchmarks from TACLeBench. The experiments demonstrate that quantitative cache analysis for FIFO and MRU comes close to the precision of LRU.

**Link**: [arxiv](http://arxiv.org/abs/2503.16588v3),  [pdf](http://arxiv.org/pdf/2503.16588v3)

**Tags**: cs.PL 68 D.3.4 



### LServe: Efficient Long-sequence LLM Serving with Unified Sparse   Attention
**Authors**: Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han

**Updated**: 2025-04-21T15:13:44Z

**Summary**: Large language models (LLMs) have shown remarkable potential in processing long sequences and complex reasoning tasks, yet efficiently serving these models remains challenging due to the quadratic computational complexity of attention in the prefilling stage and the large memory footprint of the KV cache in the decoding stage. To address these issues, we introduce LServe, an efficient system that accelerates long-sequence LLM serving via hybrid sparse attention. This method unifies different hardware-friendly, structured sparsity patterns for both prefilling and decoding attention into a single framework, where computations on less important tokens are skipped block-wise. LServe demonstrates the compatibility of static and dynamic sparsity in long-context LLM attention. This design enables multiplicative speedups by combining these optimizations. Specifically, we convert half of the attention heads to nearly free streaming heads in both the prefilling and decoding stages. Additionally, we find that only a constant number of KV pages is required to preserve long-context and reasoning capabilities, irrespective of context length. We then design a hierarchical KV page selection policy that dynamically prunes KV pages based on query-centric similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is released at https://github.com/mit-han-lab/omniserve.

**Link**: [arxiv](http://arxiv.org/abs/2502.14866v2),  [pdf](http://arxiv.org/pdf/2502.14866v2)

**Tags**: cs.CL cs.AI cs.DC cs.LG cs.PF 



### Is Intelligence the Right Direction in New OS Scheduling for Multiple   Resources in Cloud Environments?
**Authors**: Xinglei Dou, Lei Liu, Limin Xiao

**Updated**: 2025-04-21T11:09:43Z

**Summary**: Making it intelligent is a promising way in System/OS design. This paper proposes OSML+, a new ML-based resource scheduling mechanism for co-located cloud services. OSML+ intelligently schedules the cache and main memory bandwidth resources at the memory hierarchy and the computing core resources simultaneously. OSML+ uses a multi-model collaborative learning approach during its scheduling and thus can handle complicated cases, e.g., avoiding resource cliffs, sharing resources among applications, enabling different scheduling policies for applications with different priorities, etc. OSML+ can converge faster using ML models than previous studies. Moreover, OSML+ can automatically learn on the fly and handle dynamically changing workloads accordingly. Using transfer learning technologies, we show our design can work well across various cloud servers, including the latest off-the-shelf large-scale servers. Our experimental results show that OSML+ supports higher loads and meets QoS targets with lower overheads than previous studies.

**Link**: [arxiv](http://arxiv.org/abs/2504.15021v1),  [pdf](http://arxiv.org/pdf/2504.15021v1)

**Tags**: cs.DC cs.LG cs.PF 



### Context Parallelism for Scalable Million-Token Inference
**Authors**: Amy Yang, Jingyi Yang, Aya Ibrahim, Xinfeng Xie, Bangsheng Tang, Grigory Sizov, Jeremy Reizenstein, Jongsoo Park, Jianyu Huang

**Updated**: 2025-04-21T03:40:10Z

**Summary**: We present context parallelism for long-context large language model inference, which achieves near-linear scaling for long-context prefill latency with up to 128 H100 GPUs across 16 nodes. Particularly, our method achieves 1M context prefill with Llama3 405B model in 77s (93% parallelization efficiency, 63% FLOPS utilization) and 128K context prefill in 3.8s. We develop two lossless exact ring attention variants: pass-KV and pass-Q to cover a wide range of use cases with the state-of-the-art performance: full prefill, persistent KV prefill and decode. Benchmarks on H100 GPU hosts inter-connected with RDMA and TCP both show similar scalability for long-context prefill, demonstrating that our method scales well using common commercial data center with medium-to-low inter-host bandwidth.

**Link**: [arxiv](http://arxiv.org/abs/2411.01783v3),  [pdf](http://arxiv.org/pdf/2411.01783v3)

**Tags**: cs.DC cs.AI cs.LG 



### Splitwiser: Efficient LM inference with constrained resources
**Authors**: Asad Aali, Adney Cardoza, Melissa Capo

**Updated**: 2025-04-21T00:21:08Z

**Summary**: Efficient inference of LLMs remains a crucial challenge, with two main phases: a compute-intensive prompt computation and a memory-intensive token generation. Despite existing batching and scheduling techniques, token generation phases fail to fully utilize compute resources, especially when compared to prompt computation phases. To address these challenges, we propose Splitwiser, a methodology that splits the two phases of an LLM inference request onto the same GPU, thereby reducing overhead and improving memory access and cache utilization. By eliminating the need to transfer data across devices, Splitwiser aims to minimize network-related overheads. In this report, we describe the basic structure of our proposed pipeline while sharing preliminary results and analysis. We implement our proposed multiprocessing design on two widely-used and independent LLM architectures: Huggingface and vLLM. We open-source our code for the respective implementations: 1) Huggingface (https://github.com/asad-aali/splitwiser), and 2) vLLM (https://github.com/adney11/vllm-sysml).

**Link**: [arxiv](http://arxiv.org/abs/2505.03763v1),  [pdf](http://arxiv.org/pdf/2505.03763v1)

**Tags**: cs.AR cs.AI cs.DC cs.LG 



### gLLM: Global Balanced Pipeline Parallelism System for Distributed LLM   Serving with Token Throttling
**Authors**: Tianyu Guo, Xianwei Zhang, Jiangsu Du, Zhiguang Chen, Nong Xiao, Yutong Lu

**Updated**: 2025-04-21T00:07:49Z

**Summary**: Pipeline parallelism has emerged as a predominant approach for deploying large language models (LLMs) across distributed nodes, owing to its lower communication overhead compared to tensor parallelism. While demonstrating high throughput in request serving, pipeline parallelism often suffers from performance limitations caused by pipeline bubbles, which are primarily resulted from imbalanced computation delays across batches. Existing methods like Sarathi-Serve attempt to address this through hybrid scheduling of chunked prefill and decode tokens using a fixed token budget. However, such methods may experience significant fluctuations due to either insufficient prefill tokens or uneven distribution of decode tokens, ultimately leading to computational imbalance. To overcome these inefficiencies, we present gLLM, a globally balanced pipeline parallelism system incorporating Token Throttling to effectively mitigate the pipeline bubbles. Our Token Throttling mechanism is a fine-grained scheduling policy that independently regulates the quantities of prefill and decode tokens, thus enabling balanced computation by leveraging global information from the inference system. Specifically, for decode tokens, gLLM maintains near-consistent token count across processing batches. For prefill tokens, it dynamically adjusts batch sizes based on both total pending tokens and the memory utilization rates of key-value cache (KV cache). Furthermore, gLLM runtime adopts an asynchronous execution and message passing architecture specifically optimized for pipeline parallelism characteristics. Experimental evaluations with representative LLMs show that gLLM achieves significant performance improvements, delivering 11% to 398% higher maximum throughput compared to state-of-the-art pipeline or tensor parallelism systems, while simultaneously maintaining lower latency.

**Link**: [arxiv](http://arxiv.org/abs/2504.14775v1),  [pdf](http://arxiv.org/pdf/2504.14775v1)

**Tags**: cs.DC 



### Star Attention: Efficient LLM Inference over Long Sequences
**Authors**: Shantanu Acharya, Fei Jia, Boris Ginsburg

**Updated**: 2025-04-20T21:50:03Z

**Summary**: Inference with Transformer-based Large Language Models (LLMs) on long sequences is both costly and slow due to the quadratic complexity of the self-attention mechanism. We introduce Star Attention, a two-phase block-sparse approximation that improves computational efficiency by sharding attention across multiple hosts while minimizing communication overhead. In the first phase, the context is processed using blockwise-local attention across hosts, in parallel. In the second phase, query and response tokens attend to all prior cached tokens through sequence-global attention. Star Attention integrates seamlessly with most Transformer-based LLMs trained with global attention, reducing memory requirements and inference time by up to 11x while preserving 97-100% of accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2411.17116v2),  [pdf](http://arxiv.org/pdf/2411.17116v2)

**Tags**: cs.CL cs.AI cs.LG 



### Understanding and Optimizing Multi-Stage AI Inference Pipelines
**Authors**: Abhimanyu Rajeshkumar Bambhaniya, Hanjiang Wu, Suvinay Subramanian, Sudarshan Srinivasan, Souvik Kundu, Amir Yazdanbakhsh, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna

**Updated**: 2025-04-20T19:57:16Z

**Summary**: The rapid evolution of Large Language Models (LLMs) has driven the need for increasingly sophisticated inference pipelines and hardware platforms. Modern LLM serving extends beyond traditional prefill-decode workflows, incorporating multi-stage processes such as Retrieval Augmented Generation (RAG), key-value (KV) cache retrieval, dynamic model routing, and multi step reasoning. These stages exhibit diverse computational demands, requiring distributed systems that integrate GPUs, ASICs, CPUs, and memory-centric architectures. However, existing simulators lack the fidelity to model these heterogeneous, multi-engine workflows, limiting their ability to inform architectural decisions.   To address this gap, we introduce HERMES, a Heterogeneous Multi-stage LLM inference Execution Simulator. HERMES models diverse request stages; including RAG, KV retrieval, reasoning, prefill, and decode across complex hardware hierarchies. HERMES supports heterogeneous clients executing multiple models concurrently unlike prior frameworks while incorporating advanced batching strategies and multi-level memory hierarchies. By integrating real hardware traces with analytical modeling, HERMES captures critical trade-offs such as memory bandwidth contention, inter-cluster communication latency, and batching efficiency in hybrid CPU-accelerator deployments. Through case studies, we explore the impact of reasoning stages on end-to-end latency, optimal batching strategies for hybrid pipelines, and the architectural implications of remote KV cache retrieval. HERMES empowers system designers to navigate the evolving landscape of LLM inference, providing actionable insights into optimizing hardware-software co-design for next-generation AI workloads.

**Link**: [arxiv](http://arxiv.org/abs/2504.09775v3),  [pdf](http://arxiv.org/pdf/2504.09775v3)

**Tags**: cs.AR cs.AI cs.DC cs.LG 



### CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory   Architecture
**Authors**: Riccardo Tedeschi, Gianmarco Ottavi, Côme Allart, Nils Wistoff, Zexin Fu, Filippo Grillotti, Fabio De Ambroggi, Elio Guidetti, Jean-Baptiste Rigaud, Olivier Potin, Jean Roch Coulon, César Fuguet, Luca Benini, Davide Rossi

**Updated**: 2025-05-08T09:05:51Z

**Summary**: Open-source RISC-V cores are increasingly adopted in high-end embedded domains such as automotive, where maximizing instructions per cycle (IPC) is becoming critical. Building on the industry-supported open-source CVA6 core and its superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version incorporating improved branch prediction, register renaming and enhanced operand forwarding. These optimizations enable CVA6S+ to achieve a 43.5% performance improvement over the scalar configuration and 10.9% over CVA6S, with an area overhead of just 9.30% over the scalar core (CVA6). Furthermore, we integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache (HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache subsystem.

**Link**: [arxiv](http://arxiv.org/abs/2505.03762v2),  [pdf](http://arxiv.org/pdf/2505.03762v2)

**Tags**: cs.AR 



### Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink   of an Eye
**Authors**: Bradley Morgan, Gal Horowitz, Sioli O'Connell, Stephan van Schaik, Chitchanok Chuengsatiansup, Daniel Genkin, Olaf Maennel, Paul Montague, Eyal Ronen, Yuval Yarom

**Updated**: 2025-04-20T07:53:09Z

**Summary**: An essential step for mounting cache attacks is finding eviction sets, collections of memory locations that contend on cache space. On Intel processors, one of the main challenges for identifying contending addresses is the sliced cache design, where the processor hashes the physical address to determine where in the cache a memory location is stored. While past works have demonstrated that the hash function can be reversed, they also showed that it depends on physical address bits that the adversary does not know.   In this work, we make three main contributions to the art of finding eviction sets. We first exploit microarchitectural races to compare memory access times and identify the cache slice to which an address maps. We then use the known hash function to both reduce the error rate in our slice identification method and to reduce the work by extrapolating slice mappings to untested memory addresses. Finally, we show how to propagate information on eviction sets across different page offsets for the hitherto unexplored case of non-linear hash functions.   Our contributions allow for entire LLC eviction set generation in 0.7 seconds on the Intel i7-9850H and 1.6 seconds on the i9-10900K, both using non-linear functions. This represents a significant improvement compared to state-of-the-art techniques taking 9x and 10x longer, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2504.11208v2),  [pdf](http://arxiv.org/pdf/2504.11208v2)

**Tags**: cs.CR 



### Deuteronomy 2.0: Record Caching and Latch Freedom
**Authors**: David Lomet

**Updated**: 2025-04-20T00:49:27Z

**Summary**: The Deuteronomy transactional key-value store is unique architecturally in providing separation between transaction functionality -- its Transactional Component (TC) and data management -- its Data Component (DC). It is unique in technology by (1) supporting record caching, a smaller unit than the traditional page; and (2) protecting resources during concurrent execution using a latch-free approach. Both technologies are enabled by delta updating. This paper explains how record caching improves cache cost/performance. It also shows how a new latch-free approach makes implementation easier and improves performance.

**Link**: [arxiv](http://arxiv.org/abs/2504.14435v1),  [pdf](http://arxiv.org/pdf/2504.14435v1)

**Tags**: cs.DB 



### A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated   in a coupled reactive transport HPC simulation
**Authors**: Max Lübke, Marco De Lucia, Stefan Petri, Bettina Schnor

**Updated**: 2025-04-19T18:25:20Z

**Summary**: Surrogate models can play a pivotal role in enhancing performance in contemporary High-Performance Computing applications. Cache-based surrogates use already calculated simulation results to interpolate or extrapolate further simulation output values. But this approach only pays off if the access time to retrieve the needed values is much faster than the actual simulation. While the most existing key-value stores use a Client-Server architecture with dedicated storage nodes, this is not the most suitable architecture for HPC applications. Instead, we propose a distributed architecture where the parallel processes offer a part of their available memory to build a shared distributed hash table based on MPI. This paper presents three DHT approaches with the special requirements of HPC applications in mind. The presented lock-free design outperforms both DHT versions which use explicit synchronization by coarse-grained resp. fine-grained locking. The lock-free DHT shows very good scaling regarding read and write performance. The runtime of a coupled reactive transport simulation was improved between 14% and 42% using the lock-free DHT as a surrogate model.

**Link**: [arxiv](http://arxiv.org/abs/2504.14374v1),  [pdf](http://arxiv.org/pdf/2504.14374v1)

**Tags**: cs.DC 



### Improving the Serving Performance of Multi-LoRA Large Language Models   via Efficient LoRA and KV Cache Management
**Authors**: Hang Zhang, Jiuchen Shi, Yixiao Wang, Quan Chen, Yizhou Shan, Minyi Guo

**Updated**: 2025-04-19T13:17:34Z

**Summary**: Multiple Low-Rank Adapters (Multi-LoRAs) are gaining popularity for task-specific Large Language Model (LLM) applications. For multi-LoRA serving, caching hot KV caches and LoRA adapters in high bandwidth memory of accelerations can improve inference performance. However, existing Multi-LoRA inference systems fail to optimize serving performance like Time-To-First-Toke (TTFT), neglecting usage dependencies when caching LoRAs and KVs. We therefore propose FASTLIBRA, a Multi-LoRA caching system to optimize the serving performance. FASTLIBRA comprises a dependency-aware cache manager and a performance-driven cache swapper. The cache manager maintains the usage dependencies between LoRAs and KV caches during the inference with a unified caching pool. The cache swapper determines the swap-in or out of LoRAs and KV caches based on a unified cost model, when the HBM is idle or busy, respectively. Experimental results show that ELORA reduces the TTFT by 63.4% on average, compared to state-of-the-art works.

**Link**: [arxiv](http://arxiv.org/abs/2505.03756v1),  [pdf](http://arxiv.org/pdf/2505.03756v1)

**Tags**: cs.AR cs.AI cs.LG cs.PF 



### Room-temperature high-average-power strong-field terahertz source based   on industrial high-repetition-rate femtosecond laser
**Authors**: Deyin Kong, Yichen Su, Cheng Song, Xiaojun Wu

**Updated**: 2025-04-19T06:18:56Z

**Summary**: Free-space strong-field terahertz (THz) pulses, generated via optical rectification of femtosecond lasers in nonlinear crystals, are pivotal in various applications. However, conventional Ti:sapphire lasers struggle to produce high-average-power THz due to their limited output power. While kilowatt ytterbium lasers are increasingly adopted, their application in THz generation faces challenges: low optical-to-THz conversion efficiency (attributed to long pulse durations and low energy) and crystal damage under high pumping power. Here, we report a high-average-power strong-field THz source using a lithium niobate crystal pumped by a 1030-nm, 570-fs, 1-mJ, 50-kHz ytterbium femtosecond laser with tilted pulse front pumping (TPFP). By systematically optimizing TPFP implementations and comparing grating- and echelon-type configurations, we achieve a THz source with 64.5 mW average power at 42-W, 50-kHz pumping, and a focused peak electric field of 525 kV/cm at 0.83-mJ, 1-kHz operation. Additionally, we observe Zeeman torque signals in cobalt-iron ferromagnetic nanofilms. This high-repetition-rate, high-average-power THz system, combined with its potential capabilities in high signal-to-noise spectroscopy and imaging, promises transformative impacts in quantum matter manipulation, non-destructive testing, and biomedicine.

**Link**: [arxiv](http://arxiv.org/abs/2504.14196v1),  [pdf](http://arxiv.org/pdf/2504.14196v1)

**Tags**: physics.optics 



### LogicTree: Structured Proof Exploration for Coherent and Rigorous   Logical Reasoning with Large Language Models
**Authors**: Kang He, Kaushik Roy

**Updated**: 2025-04-18T22:10:02Z

**Summary**: Large language models (LLMs) have achieved remarkable multi-step reasoning capabilities across various domains. However, LLMs still face distinct challenges in complex logical reasoning, as (1) proof-finding requires systematic exploration and the maintenance of logical coherence and (2) searching the right combination of premises at each reasoning step is inherently challenging in tasks with large premise space. To address this, we propose LogicTree, an inference-time modular framework employing algorithm-guided search to automate structured proof exploration and ensure logical coherence. Advancing beyond tree-of-thought (ToT), we incorporate caching mechanism into LogicTree to enable effective utilization of historical knowledge, preventing reasoning stagnation and minimizing redundancy. Furthermore, we address the combinatorial complexity of premise search by decomposing it into a linear process. The refined premise selection restricts subsequent inference to at most one derivation per step, enhancing reasoning granularity and enforcing strict step-by-step reasoning. Additionally, we introduce two LLM-free heuristics for premise prioritization, enabling strategic proof search. Experimental results on five datasets demonstrate that LogicTree optimally scales inference-time computation to achieve higher proof accuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6% and 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o outperforms o3-mini by 7.6% on average.

**Link**: [arxiv](http://arxiv.org/abs/2504.14089v1),  [pdf](http://arxiv.org/pdf/2504.14089v1)

**Tags**: cs.CL cs.AI cs.LG 



### Gradual Binary Search and Dimension Expansion : A general method for   activation quantization in LLMs
**Authors**: Lucas Maisonnave, Cyril Moineau, Olivier Bichler, Fabrice Rastello

**Updated**: 2025-04-18T13:46:58Z

**Summary**: Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40\% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley algorithm. We theoretically demonstrates the superiority of Hadamard matrices in reducing outliers.We achieved 3-bit quantization for weights, activations, and KV cache, significantly enhancing model performance. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.

**Link**: [arxiv](http://arxiv.org/abs/2504.13989v1),  [pdf](http://arxiv.org/pdf/2504.13989v1)

**Tags**: cs.LG cs.AI cs.CL 



### CacheFormer: High Attention-Based Segment Caching
**Authors**: Sushant Singh, Ausif Mahmood

**Updated**: 2025-04-18T06:34:57Z

**Summary**: Efficiently handling long contexts in transformer-based language models with low perplexity is an active area of research. Numerous recent approaches like Linformer, Longformer, Performer, and Structured state space models (SSMs)., have not fully resolved this problem. All these models strive to reduce the quadratic time complexity of the attention mechanism while minimizing the loss in quality due to the effective compression of the long context. Inspired by the cache and virtual memory principle in computers, where in case of a cache miss, not only the needed data is retrieved from the memory, but the adjacent data is also obtained, we apply this concept to handling long contexts by dividing it into small segments. In our design, we retrieve the nearby segments in an uncompressed form when high segment-level attention occurs at the compressed level. Our en-hancements for handling long context include aggregating four attention mechanisms consisting of short sliding window attention, long compressed segmented attention, dynamically retrieving top k high attention uncompressed segments, and overlapping segments in long segment attention to avoid segment fragmentation. These enhancements result in an architecture that outperforms ex-isting SOTA architectures with an average perplexity improvement of 8.5% over similar model sizes.

**Link**: [arxiv](http://arxiv.org/abs/2504.13981v1),  [pdf](http://arxiv.org/pdf/2504.13981v1)

**Tags**: cs.LG cs.AI 



## Keyword: LLM Inference 
 ### EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via   Reinforcement Learning
**Authors**: Zhenghao Xing, Xiaowei Hu, Chi-Wing Fu, Wenhai Wang, Jifeng Dai, Pheng-Ann Heng

**Updated**: 2025-05-07T17:59:49Z

**Summary**: Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.

**Link**: [arxiv](http://arxiv.org/abs/2505.04623v1),  [pdf](http://arxiv.org/pdf/2505.04623v1)

**Tags**: eess.AS cs.AI cs.CV cs.MM cs.SD 



### On Path to Multimodal Generalist: General-Level and General-Bench
**Authors**: Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang

**Updated**: 2025-05-07T17:59:32Z

**Summary**: The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/

**Link**: [arxiv](http://arxiv.org/abs/2505.04620v1),  [pdf](http://arxiv.org/pdf/2505.04620v1)

**Tags**: cs.CV 



### From Two Sample Testing to Singular Gaussian Discrimination
**Authors**: Leonardo V. Santoro, Kartik G. Waghmare, Victor M. Panaretos

**Updated**: 2025-05-07T17:56:19Z

**Summary**: We establish that testing for the equality of two probability measures on a general separable and compact metric space is equivalent to testing for the singularity between two corresponding Gaussian measures on a suitable Reproducing Kernel Hilbert Space. The corresponding Gaussians are defined via the notion of kernel mean and covariance embedding of a probability measure. Discerning two singular Gaussians is fundamentally simpler from an information-theoretic perspective than non-parametric two-sample testing, particularly in high-dimensional settings. Our proof leverages the Feldman-Hajek criterion for singularity/equivalence of Gaussians on Hilbert spaces, and shows that discrepancies between distributions are heavily magnified through their corresponding Gaussian embeddings: at a population level, distinct probability measures lead to essentially separated Gaussian embeddings. This appears to be a new instance of the blessing of dimensionality that can be harnessed for the design of efficient inference tools in great generality.

**Link**: [arxiv](http://arxiv.org/abs/2505.04613v1),  [pdf](http://arxiv.org/pdf/2505.04613v1)

**Tags**: stat.ML cs.LG math.ST stat.TH 62G10, 46E22, 60G15 



### Particle Gibbs without the Gibbs bit
**Authors**: Adrien Corenflos

**Updated**: 2025-05-08T11:56:07Z

**Summary**: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.

**Link**: [arxiv](http://arxiv.org/abs/2505.04611v2),  [pdf](http://arxiv.org/pdf/2505.04611v2)

**Tags**: stat.CO eess.SP stat.ME 



### WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via   Weighted-Conformal Martingales
**Authors**: Drew Prinster, Xing Han, Anqi Liu, Suchi Saria

**Updated**: 2025-05-07T17:53:47Z

**Summary**: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria,'' such as data shifts that violate certain exchangeability assumptions, or do not allow for online adaptation in response to shifts. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that accommodate online adaptation to mild covariate shifts (in the marginal input distribution) while raising alarms in response to more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.04608v1),  [pdf](http://arxiv.org/pdf/2505.04608v1)

**Tags**: cs.LG cs.AI stat.ML 



### OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue   Resolution
**Authors**: Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng

**Updated**: 2025-05-07T17:51:10Z

**Summary**: The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.

**Link**: [arxiv](http://arxiv.org/abs/2505.04606v1),  [pdf](http://arxiv.org/pdf/2505.04606v1)

**Tags**: cs.SE 



### Likelihood-Free Adaptive Bayesian Inference via Nonparametric   Distribution Matching
**Authors**: Wenhui Sophia Lu, Wing Hung Wong

**Updated**: 2025-05-07T17:50:14Z

**Summary**: When the likelihood is analytically unavailable and computationally intractable, approximate Bayesian computation (ABC) has emerged as a widely used methodology for approximate posterior inference; however, it suffers from severe computational inefficiency in high-dimensional settings or under diffuse priors. To overcome these limitations, we propose Adaptive Bayesian Inference (ABI), a framework that bypasses traditional data-space discrepancies and instead compares distributions directly in posterior space through nonparametric distribution matching. By leveraging a novel Marginally-augmented Sliced Wasserstein (MSW) distance on posterior measures and exploiting its quantile representation, ABI transforms the challenging problem of measuring divergence between posterior distributions into a tractable sequence of one-dimensional conditional quantile regression tasks. Moreover, we introduce a new adaptive rejection sampling scheme that iteratively refines the posterior approximation by updating the proposal distribution via generative density estimation. Theoretically, we establish parametric convergence rates for the trimmed MSW distance and prove that the ABI posterior converges to the true posterior as the tolerance threshold vanishes. Through extensive empirical evaluation, we demonstrate that ABI significantly outperforms data-based Wasserstein ABC, summary-based ABC, and state-of-the-art likelihood-free simulators, especially in high-dimensional or dependent observation regimes.

**Link**: [arxiv](http://arxiv.org/abs/2505.04603v1),  [pdf](http://arxiv.org/pdf/2505.04603v1)

**Tags**: stat.ME cs.LG stat.CO stat.ML 



### Extracting local velocity from cosmic dipole using simulations
**Authors**: Mohit Panwar, Akash Gandhi, Pankaj Jain

**Updated**: 2025-05-07T17:48:56Z

**Summary**: Our velocity with respect to the cosmic frame of rest leads to a dipole in the number count distribution of galaxies. The dipole depends on the source spectrum, which is usually assumed to be a power law, $S(\nu) \propto \nu^{-\alpha}$ and on the flux dependence of the number density of sources. The latter is also generally assumed to be a power law, parametrised with exponent $x$. The velocity can be extracted from the observed dipole once the two parameters $x$ and $\alpha$ are known. The standard procedure uses the mean value of $\alpha$ across the entire sample, and the parameter $x$ is inferred by fitting the cumulative number count, $\frac{dN}{d\Omega}(>S_*) \propto S_*^{-x}$, near the flux limit $S_*$ of the survey. Here, we introduce a simulation procedure to extract the velocity which directly uses the $\alpha$ values of each source rather than their mean and does not rely on the functional form of the cumulative number count near the flux limit. We apply this to the quasar sample in CatWISE2020 data and find that the final results differ from the standard procedure by approximately one sigma.

**Link**: [arxiv](http://arxiv.org/abs/2505.04602v1),  [pdf](http://arxiv.org/pdf/2505.04602v1)

**Tags**: astro-ph.CO 



### MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection
**Authors**: Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu

**Updated**: 2025-05-08T06:18:31Z

**Summary**: Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.

**Link**: [arxiv](http://arxiv.org/abs/2505.04594v2),  [pdf](http://arxiv.org/pdf/2505.04594v2)

**Tags**: cs.CV 



### ZeroSearch: Incentivize the Search Capability of LLMs without Searching
**Authors**: Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang

**Updated**: 2025-05-07T17:30:22Z

**Summary**: Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2505.04588v1),  [pdf](http://arxiv.org/pdf/2505.04588v1)

**Tags**: cs.CL 



### Active Sampling for MRI-based Sequential Decision Making
**Authors**: Yuning Du, Jingshuai Liu, Rohan Dharmakumar, Sotirios A. Tsaftaris

**Updated**: 2025-05-07T17:27:51Z

**Summary**: Despite the superior diagnostic capability of Magnetic Resonance Imaging (MRI), its use as a Point-of-Care (PoC) device remains limited by high cost and complexity. To enable such a future by reducing the magnetic field strength, one key approach will be to improve sampling strategies. Previous work has shown that it is possible to make diagnostic decisions directly from k-space with fewer samples. Such work shows that single diagnostic decisions can be made, but if we aspire to see MRI as a true PoC, multiple and sequential decisions are necessary while minimizing the number of samples acquired. We present a novel multi-objective reinforcement learning framework enabling comprehensive, sequential, diagnostic evaluation from undersampled k-space data. Our approach during inference actively adapts to sequential decisions to optimally sample. To achieve this, we introduce a training methodology that identifies the samples that contribute the best to each diagnostic objective using a step-wise weighting reward function. We evaluate our approach in two sequential knee pathology assessment tasks: ACL sprain detection and cartilage thickness loss assessment. Our framework achieves diagnostic performance competitive with various policy-based benchmarks on disease detection, severity quantification, and overall sequential diagnosis, while substantially saving k-space samples. Our approach paves the way for the future of MRI as a comprehensive and affordable PoC device. Our code is publicly available at https://github.com/vios-s/MRI_Sequential_Active_Sampling

**Link**: [arxiv](http://arxiv.org/abs/2505.04586v1),  [pdf](http://arxiv.org/pdf/2505.04586v1)

**Tags**: cs.CV cs.LG 



### ACE: A Security Architecture for LLM-Integrated App Systems
**Authors**: Evan Li, Tushin Mallick, Evan Rose, William Robertson, Alina Oprea, Cristina Nita-Rotaru

**Updated**: 2025-05-07T17:26:46Z

**Summary**: LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.   In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that our system is secure against attacks from the INJECAGENT benchmark, a standard benchmark for control flow integrity in the face of indirect prompt injection attacks, and our newly introduced attacks. Our architecture represents a significant advancement towards hardening LLM-based systems containing system facilities of varying levels of trustworthiness.

**Link**: [arxiv](http://arxiv.org/abs/2504.20984v2),  [pdf](http://arxiv.org/pdf/2504.20984v2)

**Tags**: cs.CR cs.LG 



### SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for   Open-Ended Questions
**Authors**: Chloe Qianhui Zhao, Jie Cao, Eason Chen, Kenneth R. Koedinger, Jionghao Lin

**Updated**: 2025-05-07T17:24:40Z

**Summary**: Feedback is important in supporting student learning. While various automated feedback systems have been implemented to make the feedback scalable, many existing solutions only focus on generating text-based feedback. As is indicated in the multimedia learning principle, learning with more modalities could help utilize more separate channels, reduce the cognitive load and facilitate students' learning. Hence, it is important to explore the potential of Artificial Intelligence (AI) in feedback generation from and to different modalities. Our study leverages Large Language Models (LLMs) for textual feedback with the supplementary guidance from other modality - relevant lecture slide retrieved from the slides hub. Through an online crowdsourcing study (N=91), this study investigates learning gains and student perceptions using a 2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant slide), evaluating the clarity, engagement, perceived effectiveness, and reliability) of AI-facilitated multimodal feedback. We observed significant pre-to-post learning gains across all conditions. However, the differences in these gains were not statistically significant between conditions. The post-survey revealed that students found the slide feedback helpful in their learning process, though they reported difficulty in understanding it. Regarding the AI-generated open-ended feedback, students considered it personalized and relevant to their responses, but they expressed lower trust in the AI feedback compared to human-generated feedback.

**Link**: [arxiv](http://arxiv.org/abs/2505.04584v1),  [pdf](http://arxiv.org/pdf/2505.04584v1)

**Tags**: cs.HC 



### Componential Prompt-Knowledge Alignment for Domain Incremental Learning
**Authors**: Kunlun Xu, Xu Zou, Gang Hua, Jiahuan Zhou

**Updated**: 2025-05-07T17:12:15Z

**Summary**: Domain Incremental Learning (DIL) aims to learn from non-stationary data streams across domains while retaining and utilizing past knowledge. Although prompt-based methods effectively store multi-domain knowledge in prompt parameters and obtain advanced performance through cross-domain prompt fusion, we reveal an intrinsic limitation: component-wise misalignment between domain-specific prompts leads to conflicting knowledge integration and degraded predictions. This arises from the random positioning of knowledge components within prompts, where irrelevant component fusion introduces interference.To address this, we propose Componential Prompt-Knowledge Alignment (KA-Prompt), a novel prompt-based DIL method that introduces component-aware prompt-knowledge alignment during training, significantly improving both the learning and inference capacity of the model. KA-Prompt operates in two phases: (1) Initial Componential Structure Configuring, where a set of old prompts containing knowledge relevant to the new domain are mined via greedy search, which is then exploited to initialize new prompts to achieve reusable knowledge transfer and establish intrinsic alignment between new and old prompts. (2) Online Alignment Preservation, which dynamically identifies the target old prompts and applies adaptive componential consistency constraints as new prompts evolve. Extensive experiments on DIL benchmarks demonstrate the effectiveness of our KA-Prompt. Our source code is available at https://github.com/zhoujiahuan1991/ICML2025-KA-Prompt

**Link**: [arxiv](http://arxiv.org/abs/2505.04575v1),  [pdf](http://arxiv.org/pdf/2505.04575v1)

**Tags**: cs.CV cs.LG 



### Federated Generalised Variational Inference: A Robust Probabilistic   Federated Learning Framework
**Authors**: Terje Mildner, Oliver Hamelijnck, Paris Giampouras, Theodoros Damoulas

**Updated**: 2025-05-07T17:06:46Z

**Summary**: We introduce FedGVI, a probabilistic Federated Learning (FL) framework that is robust to both prior and likelihood misspecification. FedGVI addresses limitations in both frequentist and Bayesian FL by providing unbiased predictions under model misspecification, with calibrated uncertainty quantification. Our approach generalises previous FL approaches, specifically Partitioned Variational Inference (Ashman et al., 2022), by allowing robust and conjugate updates, decreasing computational complexity at the clients. We offer theoretical analysis in terms of fixed-point convergence, optimality of the cavity distribution, and provable robustness to likelihood misspecification. Further, we empirically demonstrate the effectiveness of FedGVI in terms of improved robustness and predictive performance on multiple synthetic and real world classification data sets.

**Link**: [arxiv](http://arxiv.org/abs/2502.00846v2),  [pdf](http://arxiv.org/pdf/2502.00846v2)

**Tags**: cs.LG stat.ML 



### Conformal Survival Bands for Risk Screening under Right-Censoring
**Authors**: Matteo Sesia, Vladimir Svetnik

**Updated**: 2025-05-07T17:03:22Z

**Summary**: We propose a method to quantify uncertainty around individual survival distribution estimates using right-censored data, compatible with any survival model. Unlike classical confidence intervals, the survival bands produced by this method offer predictive rather than population-level inference, making them useful for personalized risk screening. For example, in a low-risk screening scenario, they can be applied to flag patients whose survival band at 12 months lies entirely above 50\%, while ensuring that at least half of flagged individuals will survive past that time on average. Our approach builds on recent advances in conformal inference and integrates ideas from inverse probability of censoring weighting and multiple testing with false discovery rate control. We provide asymptotic guarantees and show promising performance in finite samples with both simulated and real data.

**Link**: [arxiv](http://arxiv.org/abs/2505.04568v1),  [pdf](http://arxiv.org/pdf/2505.04568v1)

**Tags**: stat.ME 



### Hierarchical Task Decomposition for Execution Monitoring and Error   Recovery: Understanding the Rationale Behind Task Demonstrations
**Authors**: Christoph Willibald, Dongheui Lee

**Updated**: 2025-05-07T16:57:51Z

**Summary**: Multi-step manipulation tasks where robots interact with their environment and must apply process forces based on the perceived situation remain challenging to learn and prone to execution errors. Accurately simulating these tasks is also difficult. Hence, it is crucial for robust task performance to learn how to coordinate end-effector pose and applied force, monitor execution, and react to deviations. To address these challenges, we propose a learning approach that directly infers both low- and high-level task representations from user demonstrations on the real system. We developed an unsupervised task segmentation algorithm that combines intention recognition and feature clustering to infer the skills of a task. We leverage the inferred characteristic features of each skill in a novel unsupervised anomaly detection approach to identify deviations from the intended task execution. Together, these components form a comprehensive framework capable of incrementally learning task decisions and new behaviors as new situations arise. Compared to state-of-the-art learning techniques, our approach significantly reduces the required amount of training data and computational complexity while efficiently learning complex in-contact behaviors and recovery strategies. Our proposed task segmentation and anomaly detection approaches outperform state-of-the-art methods on force-based tasks evaluated on two different robotic systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.04565v1),  [pdf](http://arxiv.org/pdf/2505.04565v1)

**Tags**: cs.RO 



### Efficiency Meets Fidelity: A Novel Quantization Framework for Stable   Diffusion
**Authors**: Shuaiting Li, Juncan Deng, Zeyu Wang, Kedong Xu, Rongtao Deng, Hong Gu, Haibin Shen, Kejie Huang

**Updated**: 2025-05-07T16:57:47Z

**Summary**: Text-to-image generation via Stable Diffusion models (SDM) have demonstrated remarkable capabilities. However, their computational intensity, particularly in the iterative denoising process, hinders real-time deployment in latency-sensitive applications. While Recent studies have explored post-training quantization (PTQ) and quantization-aware training (QAT) methods to compress Diffusion models, existing methods often overlook the consistency between results generated by quantized models and those from floating-point models. This consistency is paramount for professional applications where both efficiency and output reliability are essential. To ensure that quantized SDM generates high-quality and consistent images, we propose an efficient quantization framework for SDM. Our framework introduces a Serial-to-Parallel pipeline that simultaneously maintains training-inference consistency and ensures optimization stability. Building upon this foundation, we further develop several techniques including multi-timestep activation quantization, time information precalculation, inter-layer distillation, and selective freezing, to achieve high-fidelity generation in comparison to floating-point models while maintaining quantization efficiency.   Through comprehensive evaluation across multiple Stable Diffusion variants (v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over state-of-the-art approaches with shorter training times. Under W4A8 quantization settings, we achieve significant improvements in both distribution similarity and visual fidelity, while preserving a high image quality.

**Link**: [arxiv](http://arxiv.org/abs/2412.06661v2),  [pdf](http://arxiv.org/pdf/2412.06661v2)

**Tags**: cs.CV 



### LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation
**Authors**: Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu

**Updated**: 2025-05-07T16:51:33Z

**Summary**: CLIP is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. Its effectiveness primarily stems from the use of natural language as rich supervision. Motivated by the remarkable advancements in large language models (LLMs), this work explores how LLMs' superior text understanding and extensive open-world knowledge can enhance CLIP's capability, especially for processing longer and more complex image captions. We propose an efficient post-training strategy that integrates LLMs into pretrained CLIP. To address the challenge posed by the autoregressive nature of LLMs, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of LLM outputs. Extensive experiments demonstrate that our approach outperforms LoRA-based methods, achieving nearly fourfold faster training with superior performance. Furthermore, we validate substantial improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2411.04997v4),  [pdf](http://arxiv.org/pdf/2411.04997v4)

**Tags**: cs.CV cs.CL 



### Purity Law for Generalizable Neural TSP Solvers
**Authors**: Wenzhao Liu, Haoran Li, Congying Han, Zicheng Zhang, Anqi Li, Tiande Guo

**Updated**: 2025-05-07T16:46:48Z

**Summary**: Achieving generalization in neural approaches across different scales and distributions remains a significant challenge for the Traveling Salesman Problem~(TSP). A key obstacle is that neural networks often fail to learn robust principles for identifying universal patterns and deriving optimal solutions from diverse instances. In this paper, we first uncover Purity Law (PuLa), a fundamental structural principle for optimal TSP solutions, defining that edge prevalence grows exponentially with the sparsity of surrounding vertices. Statistically validated across diverse instances, PuLa reveals a consistent bias toward local sparsity in global optima. Building on this insight, we propose Purity Policy Optimization~(PUPO), a novel training paradigm that explicitly aligns characteristics of neural solutions with PuLa during the solution construction process to enhance generalization. Extensive experiments demonstrate that PUPO can be seamlessly integrated with popular neural solvers, significantly enhancing their generalization performance without incurring additional computational overhead during inference.

**Link**: [arxiv](http://arxiv.org/abs/2505.04558v1),  [pdf](http://arxiv.org/pdf/2505.04558v1)

**Tags**: cs.LG cs.AI 



### PANCAKE: Python bAsed Numerical Color-magnitude-diagram Analysis pacKagE
**Authors**: Yun Zheng, Yujiao Yang, Yong-kun Zhang, Zheng Zheng, Jing Wang, Lister Staveley-Smith, Chao-Wei Tsai, Di Li, Chao Liu, Jingjing Hu, Huaxi Chen, Donghui Quan, Yinghui Zheng, Hangyuan Li

**Updated**: 2025-05-07T16:09:22Z

**Summary**: Stellar populations serve as a fossil record of galaxy formation and evolution, providing crucial information about the history of star formation and galaxy evolution. The color-magnitude diagram (CMD) stands out as the most accurate tool currently available for inferring the star formation histories (SFHs) of nearby galaxies with stellar-resolved multiband data. The launch of new space telescopes, including JWST, EUCLID, and the upcoming CSST and Roman, will significantly increase the number of stellar-resolved galaxies over the next decade. A user-friendly and customizable CMD fitting package would be valuable for galaxy evolution studies with these data. We develop an open-source Python-based package named \textsc{pancake}, which is fast and accurate in determining SFHs and stellar population parameters in nearby galaxies. We have validated our method via a series of comprehensive tests. First, \textsc{pancake} performs well on mock data, meanwhile the random and systematic uncertainties are quantified. Second, \textsc{pancake} performs well on observational data containing a star cluster and 38 dwarf galaxies (50 fields). Third, the star formation rate (SFR) from \textsc{pancake} is consistent with the SFR from FUV photometry. To ensure compatibility and accuracy, we have included isochrone libraries generated using PARSEC for most of the optical and near-infrared filters used in space telescopes such as HST, JWST, and the upcoming CSST.

**Link**: [arxiv](http://arxiv.org/abs/2505.04534v1),  [pdf](http://arxiv.org/pdf/2505.04534v1)

**Tags**: astro-ph.GA 



### An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground   Integrated Networks
**Authors**: Qi Qin, Xinye Cao, Guoshun Nan, Sihan Chen, Rushan Li, Li Su, Haitao Du, Qimei Cui, Pengxuan Mao, Xiaofeng Tao, Tony Q. S. Quek

**Updated**: 2025-05-07T16:04:25Z

**Summary**: Recently emerged 6G space-air-ground integrated networks (SAGINs), which integrate satellites, aerial networks, and terrestrial communications, offer ubiquitous coverage for various mobile applications. However, the highly dynamic, open, and heterogeneous nature of SAGINs poses severe security issues. Forming a defense line of SAGINs suffers from two preliminary challenges: 1) accurately understanding massive unstructured multi-dimensional threat information to generate defense strategies against various malicious attacks, 2) rapidly adapting to potential unknown threats to yield more effective security strategies. To tackle the above two challenges, we propose a novel security framework for SAGINs based on Large Language Models (LLMs), which consists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG leverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent mechanisms to analyze massive unstructured multi-dimensional threat data and generate comprehensive security strategies, thus addressing the first challenge. Our proposed 6G-INST relies on a novel self-evolving method to automatically update LLM-6GNG, enabling it to accommodate unknown threats under dynamic communication environments, thereby addressing the second challenge. Additionally, we prototype the proposed framework with ns-3, OpenAirInterface (OAI), and software-defined radio (SDR). Experiments on three benchmarks demonstrate the effectiveness of our framework. The results show that our framework produces highly accurate security strategies that remain robust against a variety of unknown attacks. We will release our code to contribute to the community.

**Link**: [arxiv](http://arxiv.org/abs/2505.03161v2),  [pdf](http://arxiv.org/pdf/2505.03161v2)

**Tags**: cs.CR 



### High-Dimensional Interlingual Representations of Large Language Models
**Authors**: Bryan Wilie, Samuel Cahyawijaya, Junxian He, Pascale Fung

**Updated**: 2025-05-07T16:03:47Z

**Summary**: Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.

**Link**: [arxiv](http://arxiv.org/abs/2503.11280v3),  [pdf](http://arxiv.org/pdf/2503.11280v3)

**Tags**: cs.CL 



### Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code   Development
**Authors**: Kuen Sum Cheung, Mayuri Kaul, Gunel Jahangirova, Mohammad Reza Mousavi, Eric Zie

**Updated**: 2025-05-07T15:52:06Z

**Summary**: Large Language Models (LLM) have significantly transformed various domains, including software development. These models assist programmers in generating code, potentially increasing productivity and efficiency. However, the environmental impact of utilising these AI models is substantial, given their high energy consumption during both training and inference stages. This research aims to compare the energy consumption of manual software development versus an LLM-assisted approach, using Codeforces as a simulation platform for software development. The goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using LLM in software development. Our results show that the LLM-assisted code generation leads on average to 32.72 higher carbon footprint than the manual one. Moreover, there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.04521v1),  [pdf](http://arxiv.org/pdf/2505.04521v1)

**Tags**: cs.SE 



### Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs
**Authors**: Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan

**Updated**: 2025-05-07T15:46:36Z

**Summary**: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.

**Link**: [arxiv](http://arxiv.org/abs/2505.04519v1),  [pdf](http://arxiv.org/pdf/2505.04519v1)

**Tags**: cs.CL 



### SecureNT: Smart Topology Obfuscation for Privacy-Aware Network   Monitoring
**Authors**: Chengze Du, Jibin Shi, Hui Xu, Guangzhen Yao

**Updated**: 2025-05-07T15:39:44Z

**Summary**: Network tomography plays a crucial role in network monitoring and management, where network topology serves as the fundamental basis for various tomography tasks including traffic matrix estimation and link performance inference. The topology information, however, can be inferred through end-to-end measurements using various inference algorithms, posing significant security risks to network infrastructure. While existing protection methods attempt to secure topology information by modifying end-to-end measurements, they often require complex computation and sophisticated modification strategies, making real-time protection challenging. Moreover, these modifications typically render the measurements unusable for network monitoring, even by trusted users. This paper presents a novel privacy-preserving framework that addresses these limitations. Our approach provides efficient topology protection while maintaining the utility of measurements for authorized network monitoring. Through extensive evaluation on both simulated and real-world networks, we demonstrate that our framework achieves superior privacy protection compared to existing methods while enabling trusted users to effectively monitor network performance. Our solution offers a practical approach for organizations to protect sensitive topology information without sacrificing their network monitoring capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2412.08177v2),  [pdf](http://arxiv.org/pdf/2412.08177v2)

**Tags**: cs.CR 



### Functional Partial Least-Squares: Adaptive Estimation and Inference
**Authors**: Andrii Babii, Marine Carrasco, Idriss Tsafack

**Updated**: 2025-05-07T15:34:05Z

**Summary**: We study the functional linear regression model with a scalar response and a Hilbert space-valued predictor, a canonical example of an ill-posed inverse problem. We show that the functional partial least squares (PLS) estimator attains nearly minimax-optimal convergence rates over a class of ellipsoids and propose an adaptive early stopping procedure for selecting the number of PLS components. In addition, we develop new test that can detect local alternatives converging at the parametric rate which can be inverted to construct confidence sets. Simulation results demonstrate that the estimator performs favorably relative to several existing methods and the proposed test exhibits good power properties. We apply our methodology to evaluate the nonlinear effects of temperature on corn and soybean yields.

**Link**: [arxiv](http://arxiv.org/abs/2402.11134v2),  [pdf](http://arxiv.org/pdf/2402.11134v2)

**Tags**: math.ST econ.EM stat.CO stat.ME stat.ML stat.TH 62G05, 62G08 



### Provable Accuracy Bounds for Hybrid Dynamical Optimization and Sampling
**Authors**: Matthew X. Burns, Qingyuan Hou, Michael C. Huang

**Updated**: 2025-05-07T15:32:01Z

**Summary**: Analog dynamical accelerators (DXs) are a growing sub-field in computer architecture research, offering order-of-magnitude gains in power efficiency and latency over traditional digital methods in several machine learning, optimization, and sampling tasks. However, limited-capacity accelerators require hybrid analog/digital algorithms to solve real-world problems, commonly using large-neighborhood local search (LNLS) frameworks. Unlike fully digital algorithms, hybrid LNLS has no non-asymptotic convergence guarantees and no principled hyperparameter selection schemes, particularly limiting cross-device training and inference.   In this work, we provide non-asymptotic convergence guarantees for hybrid LNLS by reducing to block Langevin Diffusion (BLD) algorithms. Adapting tools from classical sampling theory, we prove exponential KL-divergence convergence for randomized and cyclic block selection strategies using ideal DXs. With finite device variation, we provide explicit bounds on the 2-Wasserstein bias in terms of step duration, noise strength, and function parameters. Our BLD model provides a key link between established theory and novel computing platforms, and our theoretical results provide a closed-form expression linking device variation, algorithm hyperparameters, and performance.

**Link**: [arxiv](http://arxiv.org/abs/2410.06397v2),  [pdf](http://arxiv.org/pdf/2410.06397v2)

**Tags**: cs.LG cs.DS math.ST stat.TH 60J60 F.2.0 



### OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language   Model Hallucinations in Ontology Matching
**Authors**: Zhangcheng Qiang

**Updated**: 2025-05-07T15:02:02Z

**Summary**: Hallucinations are often inevitable in downstream tasks using large language models (LLMs). To tackle the substantial challenge of addressing hallucinations for LLM-based ontology matching (OM) systems, we introduce a new benchmark dataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching) datasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of different LLMs performing OM tasks. These OM-specific hallucinations are carefully classified into two primary categories and six sub-categories. We showcase the usefulness of the dataset in constructing the LLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.21813v2),  [pdf](http://arxiv.org/pdf/2503.21813v2)

**Tags**: cs.CL cs.IR 



### A Design Space for the Critical Validation of LLM-Generated Tabular Data
**Authors**: Madhav Sachdeva, Christopher Narayanan, Marvin Wiedenkeller, Jana Sedlakova, Jürgen Bernard

**Updated**: 2025-05-07T15:01:23Z

**Summary**: LLM-generated tabular data is creating new opportunities for data-driven applications in academia, business, and society. To leverage benefits like missing value imputation, labeling, and enrichment with context-aware attributes, LLM-generated data needs a critical validation process. The number of pioneering approaches is increasing fast, opening a promising validation space that, so far, remains unstructured. We present a design space for the critical validation of LLM-generated tabular data with two dimensions: First, the Analysis Granularity dimension: from within-attribute (single-item and multi-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second, the Data Source dimension: differentiating between LLM-generated values, ground truth values, explanations, and their combinations. We discuss analysis tasks for each dimension cross-cut, map 19 existing validation approaches, and discuss the characteristics of two approaches in detail, demonstrating descriptive power.

**Link**: [arxiv](http://arxiv.org/abs/2505.04487v1),  [pdf](http://arxiv.org/pdf/2505.04487v1)

**Tags**: cs.HC 



### Efficient Flow Matching using Latent Variables
**Authors**: Anirban Samaddar, Yixuan Sun, Viktor Nilsson, Sandeep Madireddy

**Updated**: 2025-05-07T14:59:23Z

**Summary**: Flow matching models have shown great potential in image generation tasks among probabilistic generative models. Building upon the ideas of continuous normalizing flows, flow matching models generalize the transport path of the diffusion models from a simple prior distribution to the data. Most flow matching models in the literature do not explicitly model the underlying structure/manifold in the target data when learning the flow from a simple source distribution like the standard Gaussian. This leads to inefficient learning, especially for many high-dimensional real-world datasets, which often reside in a low-dimensional manifold. Existing strategies of incorporating manifolds, including data with underlying multi-modal distribution, often require expensive training and hence frequently lead to suboptimal performance. To this end, we present \texttt{Latent-CFM}, which provides simplified training/inference strategies to incorporate multi-modal data structures using pretrained deep latent variable models. Through experiments on multi-modal synthetic data and widely used image benchmark datasets, we show that \texttt{Latent-CFM} exhibits improved generation quality with significantly less training ($\sim 50\%$ less in some cases) and computation than state-of-the-art flow matching models. Using a 2d Darcy flow dataset, we demonstrate that our approach generates more physically accurate samples than competitive approaches. In addition, through latent space analysis, we demonstrate that our approach can be used for conditional image generation conditioned on latent features.

**Link**: [arxiv](http://arxiv.org/abs/2505.04486v1),  [pdf](http://arxiv.org/pdf/2505.04486v1)

**Tags**: cs.CV cs.AI cs.LG 



### CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation
**Authors**: Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou

**Updated**: 2025-05-07T14:52:02Z

**Summary**: Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.04481v1),  [pdf](http://arxiv.org/pdf/2505.04481v1)

**Tags**: cs.CV 



### TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven   Evolution
**Authors**: Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park

**Updated**: 2025-05-07T14:51:43Z

**Summary**: Trajectory prediction is a crucial task in modeling human behavior, especially in fields as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy, while recently proposed deep learning approaches suffer from computational cost, lack of explainability, and generalization issues that limit their practical adoption. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We introduce a Cross-Generation Elite Sampling to promote population diversity and a Statistics Feedback Loop allowing the LLM to analyze alternative predictions. Our evaluations show TrajEvo outperforms previous heuristic methods on the ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning methods when generalizing to the unseen SDD dataset. TrajEvo represents a first step toward automated design of fast, explainable, and generalizable trajectory prediction heuristics. We make our source code publicly available to foster future research at https://github.com/ai4co/trajevo.

**Link**: [arxiv](http://arxiv.org/abs/2505.04480v1),  [pdf](http://arxiv.org/pdf/2505.04480v1)

**Tags**: cs.AI cs.NE cs.RO 



### Mass Modeling the Andromeda Dwarf Galaxies: Andromeda VI and Andromeda   XXIII
**Authors**: Connor S. Pickett, Michelle L. M. Collins, R. Michael Rich, Justin I. Read, Emily J. E. Charles, Nicolas Martin, Scott Chapman, Alan McConnachie, Alessandro Savino, Daniel R. Weisz

**Updated**: 2025-05-07T14:46:22Z

**Summary**: Accurately mapping the mass profiles of low mass dwarf spheroidal (dSph) galaxies allows us to test predictions made by dark matter (DM) models. To date, such analyses have primarily been performed on Milky Way (MW) satellites. Meanwhile, the Andromeda Galaxy (M31) is home to 35 known dwarf galaxies, yet only two have been successfully mass-modeled so far. In order to better understand the nature of dark matter, a more comprehensive study of Local Group dwarfs is necessary. In this study, we have undertaken a dynamical study of two higher-luminosity Andromeda dwarf galaxies: Andromeda VI (And VI) and Andromeda XXIII (And XXIII). We infer an enclosed mass for And VI of M(r < r$_{\rm{h}}$) = (4.9 $\pm$ 1.5) $\times$ 10$^{7}$ M$_{\odot}$, corresponding to a mass-to-light ratio of $[M/L]_{r_{\rm{h}}}$ = (27.1 $\pm$ 8.2) M$_{\odot}$/L$_{\odot}$. We infer an enclosed mass for And XXIII of M(r < r$_{\rm{h}}$) = (3.1 $\pm$ 1.9) $\times$ 10$^{7}$ M$_{\odot}$, corresponding to a mass-to-light ratio of $[M/L]_{\rm{r_{h}}}$ = (90.2 $\pm$ 53.9) M$_{\odot}$/L$_{\odot}$. Using the dynamical Jeans modeling tool, GravSphere, we determine And VI and And XXIII's dark matter density at 150 pc, finding $\rho_{\rm{DM,VI}}$(150 pc) = (1.4 $\pm$ 0.5) $\times$ 10$^{8}$ M$_{\odot}$ kpc$^{-3}$ and $\rho_{\rm{DM,XXIII}}$(150 pc) = 0.5$\substack{+0.4 \\ -0.3} \times$ 10$^{8}$ M$_{\odot}$ kpc$^{-3}$. Our results make And VI the first mass-modeled M31 satellite to fall into the cuspy regime, while And XXIII has a lower density, implying either a more cored central dark matter density, or a lowering of the density through tides. This adds And XXIII to a growing list of M31 dwarfs with a central density lower than most MW dwarfs and lower than expected for isolated dwarfs in the Standard Cosmology. This could be explained by the M31 dwarfs having experienced stronger tides than their Milky Way counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2505.04475v1),  [pdf](http://arxiv.org/pdf/2505.04475v1)

**Tags**: astro-ph.GA 



### Enhancing Statistical Validity and Power in Hybrid Controlled Trials: A   Randomization Inference Approach with Conformal Selective Borrowing
**Authors**: Ke Zhu, Shu Yang, Xiaofei Wang

**Updated**: 2025-05-07T14:40:04Z

**Summary**: External controls from historical trials or observational data can augment randomized controlled trials when large-scale randomization is impractical or unethical, such as in drug evaluation for rare diseases. However, non-randomized external controls can introduce biases, and existing Bayesian and frequentist methods may inflate the type I error rate, particularly in small-sample trials where external data borrowing is most critical. To address these challenges, we propose a randomization inference framework that ensures finite-sample exact and model-free type I error rate control, adhering to the "analyze as you randomize" principle to safeguard against hidden biases. Recognizing that biased external controls reduce the power of randomization tests, we leverage conformal inference to develop an individualized test-then-pool procedure that selectively borrows comparable external controls to improve power. Our approach incorporates selection uncertainty into randomization tests, providing valid post-selection inference. Additionally, we propose an adaptive procedure to optimize the selection threshold by minimizing the mean squared error across a class of estimators encompassing both no-borrowing and full-borrowing approaches. The proposed methods are supported by non-asymptotic theoretical analysis, validated through simulations, and applied to a randomized lung cancer trial that integrates external controls from the National Cancer Database.

**Link**: [arxiv](http://arxiv.org/abs/2410.11713v3),  [pdf](http://arxiv.org/pdf/2410.11713v3)

**Tags**: stat.ME 



### XrayGPT: Chest Radiographs Summarization using Medical Vision-Language   Models
**Authors**: Omkar Thawakar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, Fahad Shahbaz Khan

**Updated**: 2025-05-07T14:26:09Z

**Summary**: The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/XrayGPT.

**Link**: [arxiv](http://arxiv.org/abs/2306.07971v2),  [pdf](http://arxiv.org/pdf/2306.07971v2)

**Tags**: cs.CV 



### Meta-Learning Driven Lightweight Phase Shift Compression for   IRS-Assisted Wireless Systems
**Authors**: Xianhua Yu, Dong Li, Bowen Gu, Xiaoye Jing, Wen Wu, Tuo Wu, Kan Yu

**Updated**: 2025-05-07T14:25:47Z

**Summary**: The phase shift information (PSI) overhead poses a critical challenge to enabling real-time intelligent reflecting surface (IRS)-assisted wireless systems, particularly under dynamic and resource-constrained conditions. In this paper, we propose a lightweight PSI compression framework, termed meta-learning-driven compression and reconstruction network (MCRNet). By leveraging a few-shot adaptation strategy via model-agnostic meta-learning (MAML), MCRNet enables rapid generalization across diverse IRS configurations with minimal retraining overhead. Furthermore, a novel depthwise convolutional gating (DWCG) module is incorporated into the decoder to achieve adaptive local feature modulation with low computational cost, significantly improving decoding efficiency. Extensive simulations demonstrate that MCRNet achieves competitive normalized mean square error performance compared to state-of-the-art baselines across various compression ratios, while substantially reducing model size and inference latency. These results validate the effectiveness of the proposed asymmetric architecture and highlight the practical scalability and real-time applicability of MCRNet for dynamic IRS-assisted wireless deployments.

**Link**: [arxiv](http://arxiv.org/abs/2505.04453v1),  [pdf](http://arxiv.org/pdf/2505.04453v1)

**Tags**: eess.SP 



### M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation
**Authors**: Qianru Zhang, Liang Qu, Honggang Wen, Dong Huang, Siu-Ming Yiu, Nguyen Quoc Viet Hung, Hongzhi Yin

**Updated**: 2025-05-07T14:14:29Z

**Summary**: Sequential recommendation systems aim to predict users' next preferences based on their interaction histories, but existing approaches face critical limitations in efficiency and multi-scale pattern recognition. While Transformer-based methods struggle with quadratic computational complexity, recent Mamba-based models improve efficiency but fail to capture periodic user behaviors, leverage rich semantic information, or effectively fuse multimodal features. To address these challenges, we propose \model, a novel sequential recommendation framework that integrates multi-scale Mamba with Fourier analysis, Large Language Models (LLMs), and adaptive gating. First, we enhance Mamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns in the frequency domain, separating meaningful trends from noise. Second, we incorporate LLM-based text embeddings to enrich sparse interaction data with semantic context from item descriptions. Finally, we introduce a learnable gate mechanism to dynamically balance temporal (Mamba), frequency (FFT), and semantic (LLM) features, ensuring harmonious multimodal fusion. Extensive experiments demonstrate that \model\ achieves state-of-the-art performance, improving Hit Rate@10 by 3.2\% over existing Mamba-based models while maintaining 20\% faster inference than Transformer baselines. Our results highlight the effectiveness of combining frequency analysis, semantic understanding, and adaptive fusion for sequential recommendation. Code and datasets are available at: https://anonymous.4open.science/r/M2Rec.

**Link**: [arxiv](http://arxiv.org/abs/2505.04445v1),  [pdf](http://arxiv.org/pdf/2505.04445v1)

**Tags**: cs.IR 



### Towards Effectively Leveraging Execution Traces for Program Repair with   Code LLMs
**Authors**: Mirazul Haque, Petr Babkin, Farima Farmahinifarahani, Manuela Veloso

**Updated**: 2025-05-07T14:12:41Z

**Summary**: Large Language Models (LLMs) show promising performance on various programming tasks, including Automatic Program Repair (APR). However, most approaches to LLM-based APR are limited to the static analysis of the programs, while disregarding their runtime behavior. Inspired by knowledge-augmented NLP, in this work, we aim to remedy this potential blind spot by augmenting standard APR prompts with program execution traces. We evaluate our approach using the GPT family of models on three popular APR datasets. Our findings suggest that simply incorporating execution traces into the prompt provides a limited performance improvement over trace-free baselines, in only 2 out of 6 tested dataset / model configurations. We further find that the effectiveness of execution traces for APR diminishes as their complexity increases. We explore several strategies for leveraging traces in prompts and demonstrate that LLM-optimized prompts help outperform trace-free prompts more consistently. Additionally, we show trace-based prompting to be superior to finetuning a smaller LLM on a small-scale dataset; and conduct probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.04441v1),  [pdf](http://arxiv.org/pdf/2505.04441v1)

**Tags**: cs.LG 



### Checkification: A Practical Approach for Testing Static Analysis Truths
**Authors**: Daniela Ferreiro, Ignacio Casso, Jose F. Morales, Pedro López-García, Manuel V. Hermenegildo

**Updated**: 2025-05-07T14:05:14Z

**Summary**: Static analysis is an essential component of many modern software development tools. Unfortunately, the ever-increasing complexity of static analyzers makes their coding error-prone. Even analysis tools based on rigorous mathematical techniques, such as abstract interpretation, are not immune to bugs. Ensuring the correctness and reliability of software analyzers is critical if they are to be inserted in production compilers and development environments. While compiler validation has seen notable success, formal validation of static analysis tools remains relatively unexplored. In this paper, we propose a method for testing abstract interpretation-based static analyzers. Broadly, it consists in checking, over a suite of benchmarks, that the properties inferred statically are satisfied dynamically. The main advantage of our approach lies in its simplicity, which stems directly from framing it within the Ciao assertion-based validation framework, and its blended static/dynamic assertion checking approach. We demonstrate that in this setting, the analysis can be tested with little effort by combining the following components already present in the framework: 1) the static analyzer, which outputs its results as the original program source with assertions interspersed; 2) the assertion run-time checking mechanism, which instruments a program to ensure that no assertion is violated at run time; 3) the random test case generator, which generates random test cases satisfying the properties present in assertion preconditions; and 4) the unit-test framework, which executes those test cases. We have applied our approach to the CiaoPP static analyzer, resulting in the identification of many bugs with reasonable overhead. Most of these bugs have been either fixed or confirmed, helping us detect a range of errors not only related to analysis soundness but also within other aspects of the framework.

**Link**: [arxiv](http://arxiv.org/abs/2501.12093v3),  [pdf](http://arxiv.org/pdf/2501.12093v3)

**Tags**: cs.SE cs.PL 



### Gamma-ray Bursts as Distance Indicators by a Statistical Learning   Approach
**Authors**: Maria Giovanna Dainotti, Aditya Narendra, Agnieszka Pollo, Vahe Petrosian, Malgorzata Bogdan, Kazunari Iwasaki, Jason Xavier Prochaska, Enrico Rinaldi, David Zhou

**Updated**: 2025-05-07T14:04:26Z

**Summary**: Gamma-ray bursts (GRBs) can be probes of the early universe, but currently, only 26% of GRBs observed by the Neil Gehrels Swift Observatory GRBs have known redshifts ($z$) due to observational limitations. To address this, we estimated the GRB redshift (distance) via a supervised statistical learning model that uses optical afterglow observed by Swift and ground-based telescopes. The inferred redshifts are strongly correlated (a Pearson coefficient of 0.93) with the observed redshifts, thus proving the reliability of this method. The inferred and observed redshifts allow us to estimate the number of GRBs occurring at a given redshift (GRB rate) to be 8.47-9 $yr^{-1} Gpc^{-1}$ for $1.9<z<2.3$. Since GRBs come from the collapse of massive stars, we compared this rate with the star formation rate highlighting a discrepancy of a factor of 3 at $z<1$.

**Link**: [arxiv](http://arxiv.org/abs/2402.04551v4),  [pdf](http://arxiv.org/pdf/2402.04551v4)

**Tags**: astro-ph.HE astro-ph.CO astro-ph.IM 



### Automatic Debiased Estimation with Machine Learning-Generated Regressors
**Authors**: Juan Carlos Escanciano, Telmo Pérez-Izquierdo

**Updated**: 2025-05-07T13:52:53Z

**Summary**: Many parameters of interest in economics and other social sciences depend on generated regressors. Examples in economics include structural parameters in models with endogenous variables estimated by control functions and in models with sample selection, treatment effect estimation with propensity score matching, and marginal treatment effects. More recently, Machine Learning (ML) generated regressors are becoming ubiquitous for these and other applications such as imputation with missing regressors, dimension reduction, including autoencoders, learned proxies, confounders and treatments, and for feature engineering with unstructured data, among others. We provide the first general method for valid inference with regressors generated from ML. Inference with generated regressors is complicated by the very complex expression for influence functions and asymptotic variances. Additionally, ML-generated regressors may lead to large biases in downstream inferences. To address these problems, we propose Automatic Locally Robust/debiased GMM estimators in a general three-step setting with ML-generated regressors. We illustrate our results with treatment effects and counterfactual parameters in the partially linear and nonparametric models with ML-generated regressors. We provide sufficient conditions for the asymptotic normality of our debiased GMM estimators and investigate their finite-sample performance through Monte Carlo simulations.

**Link**: [arxiv](http://arxiv.org/abs/2301.10643v3),  [pdf](http://arxiv.org/pdf/2301.10643v3)

**Tags**: econ.EM 



### OBLIVIATE: Robust and Practical Machine Unlearning for Large Language   Models
**Authors**: Xiaoyu Xu, Minxin Du, Qingqing Ye, Haibo Hu

**Updated**: 2025-05-07T13:51:42Z

**Summary**: Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.04416v1),  [pdf](http://arxiv.org/pdf/2505.04416v1)

**Tags**: cs.CL cs.AI cs.CR cs.LG 



### YABLoCo: Yet Another Benchmark for Long Context Code Generation
**Authors**: Aidar Valeev, Roman Garaev, Vadim Lomshakov, Irina Piontkovskaya, Vladimir Ivanov, Israel Adewuyi

**Updated**: 2025-05-07T13:42:23Z

**Summary**: Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.

**Link**: [arxiv](http://arxiv.org/abs/2505.04406v1),  [pdf](http://arxiv.org/pdf/2505.04406v1)

**Tags**: cs.CL cs.AI cs.SE 



### Probing the Bounce Energy Scale in Bouncing Cosmologies with Pulsar   Timing Arrays
**Authors**: Junrong Lai, Changhong Li

**Updated**: 2025-05-08T11:09:33Z

**Summary**: In this work we constrain the bounce energy scale $\rho_{s\downarrow}^{1/4}$ in a generic framework of bouncing cosmologies using the nanohertz stochastic gravitational-wave background recently detected by pulsar timing arrays (NANOGrav 15-yr, EPTA DR2, PPTA DR3, IPTA DR2). A full Bayesian fit of the analytic SGWB spectrum for this bounce scenario reveals, for the first time, two distinct posterior branches in $(\rho_{s\downarrow}^{1/4},w_1)$: one near $w_1\approx0.3$ and one at $w_1\gg1$, where $w_1$ is the contraction phase equation of state. We find that the bouncing model attains larger Bayes factors against each of six conventional SGWB sources (SMBHBs, inflationary GWs, cosmic strings, domain walls, first order phase transitions, scalar induced GWs), demonstrating strong preference of current PTA data for the bounce hypothesis. Compared to the more generic dual inflation bounce scenario, the concrete bounce realization yields smaller Bayes factors, indicating that PTA measurements impose tighter constraints when the bounce scale is explicit. Moreover, the two posterior branches illuminate distinct theoretical frontiers. The right branch ($w_1\gg1$) violates the dominant energy condition (DEC), thereby providing direct empirical impetus for models with novel early Universe physics, e.g. ghost condensates, higher-derivative or modified gravity operators, and extra dimensional effects. Independently, both branches infer $\rho_{s\downarrow}^{1/4}$ above the Planck scale $M_\mathrm{pl}$, demonstrating that current PTAs already probe trans-Planckian regimes. Together, these findings offer a rare observational window into UV completions of cosmology. We further describe how normalizing flow based machine learning can accelerate such Bayesian analyses as PTA data volumes increase.

**Link**: [arxiv](http://arxiv.org/abs/2504.19251v3),  [pdf](http://arxiv.org/pdf/2504.19251v3)

**Tags**: astro-ph.CO 



### SwinLip: An Efficient Visual Speech Encoder for Lip Reading Using Swin   Transformer
**Authors**: Young-Hu Park, Rae-Hong Park, Hyung-Min Park

**Updated**: 2025-05-07T13:18:43Z

**Summary**: This paper presents an efficient visual speech encoder for lip reading. While most recent lip reading studies have been based on the ResNet architecture and have achieved significant success, they are not sufficiently suitable for efficiently capturing lip reading features due to high computational complexity in modeling spatio-temporal information. Additionally, using a complex visual model not only increases the complexity of lip reading models but also induces delays in the overall network for multi-modal studies (e.g., audio-visual speech recognition, speech enhancement, and speech separation). To overcome the limitations of Convolutional Neural Network (CNN)-based models, we apply the hierarchical structure and window self-attention of the Swin Transformer to lip reading. We configure a new lightweight scale of the Swin Transformer suitable for processing lip reading data and present the SwinLip visual speech encoder, which efficiently reduces computational load by integrating modified Convolution-augmented Transformer (Conformer) temporal embeddings with conventional spatial embeddings in the hierarchical structure. Through extensive experiments, we have validated that our SwinLip successfully improves the performance and inference speed of the lip reading network when applied to various backbones for word and sentence recognition, reducing computational load. In particular, our SwinLip demonstrated robust performance in both English LRW and Mandarin LRW-1000 datasets and achieved state-of-the-art performance on the Mandarin LRW-1000 dataset with less computation compared to the existing state-of-the-art model.

**Link**: [arxiv](http://arxiv.org/abs/2505.04394v1),  [pdf](http://arxiv.org/pdf/2505.04394v1)

**Tags**: cs.CV eess.AS 



### Large Means Left: Political Bias in Large Language Models Increases with   Their Number of Parameters
**Authors**: David Exler, Mark Schutera, Markus Reischl, Luca Rettenberger

**Updated**: 2025-05-07T13:18:41Z

**Summary**: With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users. Large language models (LLMs) are predominantly used by many as a primary source of information for various topics. LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions. Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification. We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat. This metric measures the alignment between an individual's political views and the positions of German political parties. We compare the models' alignment scores to identify factors influencing their political preferences. Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs. Also, we find that the language we use to communicate with the models affects their political views. Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag. Our results imply that LLMs are prone to exhibiting political bias. Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale.

**Link**: [arxiv](http://arxiv.org/abs/2505.04393v1),  [pdf](http://arxiv.org/pdf/2505.04393v1)

**Tags**: cs.CL 



### Estimating LLM Uncertainty with Logits
**Authors**: Huan Ma, Jingdong Chen, Joey Tianyi Zhou, Guangyu Wang, Changqing Zhang

**Updated**: 2025-05-07T13:13:41Z

**Summary**: Over the past few years, Large Language Models (LLMs) have developed rapidly and are widely applied in various domains. However, LLMs face the issue of hallucinations, generating responses that may be unreliable when the models lack relevant knowledge. To be aware of potential hallucinations, uncertainty estimation methods have been introduced, and most of them have confirmed that reliability lies in critical tokens. However, probability-based methods perform poorly in identifying token reliability, limiting their practical utility. In this paper, we reveal that the probability-based method fails to estimate token reliability due to the loss of evidence strength information which is accumulated in the training stage. Therefore, we present Logits-induced token uncertainty (LogTokU), a framework for estimating decoupled token uncertainty in LLMs, enabling real-time uncertainty estimation without requiring multiple sampling processes. We employ evidence modeling to implement LogTokU and use the estimated uncertainty to guide downstream tasks. The experimental results demonstrate that LogTokU has significant effectiveness and promise.

**Link**: [arxiv](http://arxiv.org/abs/2502.00290v4),  [pdf](http://arxiv.org/pdf/2502.00290v4)

**Tags**: cs.CL cs.AI 



### The Aloe Family Recipe for Open and Specialized Healthcare LLMs
**Authors**: Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés

**Updated**: 2025-05-07T13:13:14Z

**Summary**: Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2505.04388v1),  [pdf](http://arxiv.org/pdf/2505.04388v1)

**Tags**: cs.CL cs.AI 



### Breaking the Lens of the Telescope: Online Relevance Estimation over   Large Retrieval Sets
**Authors**: Mandeep Rathee, V Venktesh, Sean MacAvaney, Avishek Anand

**Updated**: 2025-05-07T12:46:32Z

**Summary**: Advanced relevance models, such as those that use large language models (LLMs), provide highly accurate relevance estimations. However, their computational costs make them infeasible for processing large document corpora. To address this, retrieval systems often employ a telescoping approach, where computationally efficient but less precise lexical and semantic retrievers filter potential candidates for further ranking. However, this approach heavily depends on the quality of early-stage retrieval, which can potentially exclude relevant documents early in the process. In this work, we propose a novel paradigm for re-ranking called online relevance estimation that continuously updates relevance estimates for a query throughout the ranking process. Instead of re-ranking a fixed set of top-k documents in a single step, online relevance estimation iteratively re-scores smaller subsets of the most promising documents while adjusting relevance scores for the remaining pool based on the estimations from the final model using an online bandit-based algorithm. This dynamic process mitigates the recall limitations of telescoping systems by re-prioritizing documents initially deemed less relevant by earlier stages -- including those completely excluded by earlier-stage retrievers. We validate our approach on TREC benchmarks under two scenarios: hybrid retrieval and adaptive retrieval. Experimental results demonstrate that our method is sample-efficient and significantly improves recall, highlighting the effectiveness of our online relevance estimation framework for modern search systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.09353v2),  [pdf](http://arxiv.org/pdf/2504.09353v2)

**Tags**: cs.IR 



### Playing repeated games with Large Language Models
**Authors**: Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz

**Updated**: 2025-05-07T12:44:45Z

**Summary**: LLMs are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour. We let different LLMs play finitely repeated $2\times2$ games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination, like the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We additionally show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a "social chain-of-thought" (SCoT) strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.

**Link**: [arxiv](http://arxiv.org/abs/2305.16867v2),  [pdf](http://arxiv.org/pdf/2305.16867v2)

**Tags**: cs.CL 



### LLM-based Bi-level Multi-interest Learning Framework for Sequential   Recommendation
**Authors**: Shutong Qiao, Chen Gao, Wei Yuan, Yong Li, Hongzhi Yin

**Updated**: 2025-05-07T12:33:27Z

**Summary**: Sequential recommendation (SR) leverages users' dynamic preferences, with recent advances incorporating multi-interest learning to model diverse user interests. However, most multi-interest SR models rely on noisy, sparse implicit feedback, limiting recommendation accuracy. Large language models (LLMs) offer robust reasoning on low-quality data but face high computational costs and latency challenges for SR integration. We propose a novel LLM-based multi-interest SR framework combining implicit behavioral and explicit semantic perspectives. It includes two modules: the Implicit Behavioral Interest Module (IBIM), which learns from user behavior using a traditional SR model, and the Explicit Semantic Interest Module (ESIM), which uses clustering and prompt-engineered LLMs to extract semantic multi-interest representations from informative samples. Semantic insights from ESIM enhance IBIM's behavioral representations via modality alignment and semantic prediction tasks. During inference, only IBIM is used, ensuring efficient, LLM-free recommendations. Experiments on four real-world datasets validate the framework's effectiveness and practicality.

**Link**: [arxiv](http://arxiv.org/abs/2411.09410v3),  [pdf](http://arxiv.org/pdf/2411.09410v3)

**Tags**: cs.IR 



### Benchmarking LLMs' Swarm intelligence
**Authors**: Kai Ruan, Mowen Huang, Ji-Rong Wen, Hao Sun

**Updated**: 2025-05-07T12:32:01Z

**Summary**: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.

**Link**: [arxiv](http://arxiv.org/abs/2505.04364v1),  [pdf](http://arxiv.org/pdf/2505.04364v1)

**Tags**: cs.MA cs.CL 



### Information-Geometric Barycenters for Bayesian Federated Learning
**Authors**: Nour Jamoussi, Giuseppe Serra, Photios A. Stavrou, Marios Kountouris

**Updated**: 2025-05-07T11:54:19Z

**Summary**: Federated learning (FL) is a widely used and impactful distributed optimization framework that achieves consensus through averaging locally trained models. While effective, this approach may not align well with Bayesian inference, where the model space has the structure of a distribution space. Taking an information-geometric perspective, we reinterpret FL aggregation as the problem of finding the barycenter of local posteriors using a prespecified divergence metric, minimizing the average discrepancy across clients. This perspective provides a unifying framework that generalizes many existing methods and offers crisp insights into their theoretical underpinnings. We then propose BA-BFL, an algorithm that retains the convergence properties of Federated Averaging in non-convex settings. In non-independent and identically distributed scenarios, we conduct extensive comparisons with statistical aggregation techniques, showing that BA-BFL achieves performance comparable to state-of-the-art methods while offering a geometric interpretation of the aggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep Learning, exploring the impact of Bayesian layers on uncertainty quantification and model calibration.

**Link**: [arxiv](http://arxiv.org/abs/2412.11646v2),  [pdf](http://arxiv.org/pdf/2412.11646v2)

**Tags**: cs.LG cs.IT cs.NI math.IT 



### Double Cross-fit Doubly Robust Estimators: Beyond Series Regression
**Authors**: Alec McClean, Sivaraman Balakrishnan, Edward H. Kennedy, Larry Wasserman

**Updated**: 2025-05-07T11:48:45Z

**Summary**: Doubly robust estimators with cross-fitting have gained popularity in causal inference due to their favorable structure-agnostic error guarantees. However, when additional structure, such as H\"{o}lder smoothness, is available then more accurate "double cross-fit doubly robust" (DCDR) estimators can be constructed by splitting the training data and undersmoothing nuisance function estimators on independent samples. We study a DCDR estimator of the Expected Conditional Covariance, a functional of interest in causal inference and conditional independence testing. We first provide a structure-agnostic error analysis for the DCDR estimator with no assumptions on the nuisance functions or their estimators. Then, assuming the nuisance functions are H\"{o}lder smooth, but without assuming knowledge of the true smoothness level or the covariate density, we establish that DCDR estimators with several linear smoothers are $\sqrt{n}$-consistent and asymptotically normal under minimal conditions and achieve fast convergence rates in the non-$\sqrt{n}$ regime. When the covariate density and smoothnesses are known, we propose a minimax rate-optimal DCDR estimator based on undersmoothed kernel regression. Moreover, we show an undersmoothed DCDR estimator satisfies a slower-than-$\sqrt{n}$ central limit theorem, and that inference is possible even in the non-$\sqrt{n}$ regime. Finally, we support our theoretical results with simulations, providing intuition for double cross-fitting and undersmoothing, demonstrating where our estimator achieves $\sqrt{n}$-consistency while the usual "single cross-fit" estimator fails, and illustrating asymptotic normality for the undersmoothed DCDR estimator.

**Link**: [arxiv](http://arxiv.org/abs/2403.15175v3),  [pdf](http://arxiv.org/pdf/2403.15175v3)

**Tags**: math.ST stat.ME stat.ML stat.TH 



### A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text   Classification
**Authors**: Junichiro Niimi

**Updated**: 2025-05-07T11:31:37Z

**Summary**: With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%.

**Link**: [arxiv](http://arxiv.org/abs/2504.18884v2),  [pdf](http://arxiv.org/pdf/2504.18884v2)

**Tags**: cs.CL cs.AI 



### Generic Two-Mode Gaussian States as Quantum Sensors
**Authors**: Pritam Chattopadhyay, Saikat Sur, Jonas F. G. Santos

**Updated**: 2025-05-07T11:12:23Z

**Summary**: Gaussian quantum channels constitute a cornerstone of continuous-variable quantum information science, underpinning a wide array of protocols in quantum optics and quantum metrology. While the action of such channels on arbitrary states is well-characterized under full channel knowledge, we address the inverse problem, namely, the precise estimation of fundamental channel parameters, including the beam splitter transmissivity and the two-mode squeezing amplitude. Employing the quantum Fisher information (QFI) as a benchmark for metrological sensitivity, we demonstrate that the symmetry inherent in mode mixing critically governs the amplification of QFI, thereby enabling high-precision parameter estimation. In addition, we investigate quantum thermometry by estimating the average photon number of thermal states, revealing that the transmissivity parameter significantly modulates estimation precision. Our results underscore the metrological utility of two-mode Gaussian states and establish a robust framework for parameter inference in noisy and dynamically evolving quantum systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.04321v1),  [pdf](http://arxiv.org/pdf/2505.04321v1)

**Tags**: quant-ph 



### Detecting Concept Drift in Neural Networks Using Chi-squared Goodness of   Fit Testing
**Authors**: Jacob Glenn Ayers, Buvaneswari A. Ramanan, Manzoor A. Khan

**Updated**: 2025-05-07T11:04:47Z

**Summary**: As the adoption of deep learning models has grown beyond human capacity for verification, meta-algorithms are needed to ensure reliable model inference. Concept drift detection is a field dedicated to identifying statistical shifts that is underutilized in monitoring neural networks that may encounter inference data with distributional characteristics diverging from their training data. Given the wide variety of model architectures, applications, and datasets, it is important that concept drift detection algorithms are adaptable to different inference scenarios. In this paper, we introduce an application of the $\chi^2$ Goodness of Fit Hypothesis Test as a drift detection meta-algorithm applied to a multilayer perceptron, a convolutional neural network, and a transformer trained for machine vision as they are exposed to simulated drift during inference. To that end, we demonstrate how unexpected drops in accuracy due to concept drift can be detected without directly examining the inference outputs. Our approach enhances safety by ensuring models are continually evaluated for reliability across varying conditions.

**Link**: [arxiv](http://arxiv.org/abs/2505.04318v1),  [pdf](http://arxiv.org/pdf/2505.04318v1)

**Tags**: cs.LG cs.AI eess.IV 



### KERAIA: An Adaptive and Explainable Framework for Dynamic Knowledge   Representation and Reasoning
**Authors**: Stephen Richard Varey, Alessandro Di Stefano, The Anh Han

**Updated**: 2025-05-07T10:56:05Z

**Summary**: In this paper, we introduce KERAIA, a novel framework and software platform for symbolic knowledge engineering designed to address the persistent challenges of representing, reasoning with, and executing knowledge in dynamic, complex, and context-sensitive environments. The central research question that motivates this work is: How can unstructured, often tacit, human expertise be effectively transformed into computationally tractable algorithms that AI systems can efficiently utilise? KERAIA seeks to bridge this gap by building on foundational concepts such as Minsky's frame-based reasoning and K-lines, while introducing significant innovations. These include Clouds of Knowledge for dynamic aggregation, Dynamic Relations (DRels) for context-sensitive inheritance, explicit Lines of Thought (LoTs) for traceable reasoning, and Cloud Elaboration for adaptive knowledge transformation. This approach moves beyond the limitations of traditional, often static, knowledge representation paradigms. KERAIA is designed with Explainable AI (XAI) as a core principle, ensuring transparency and interpretability, particularly through the use of LoTs. The paper details the framework's architecture, the KSYNTH representation language, and the General Purpose Paradigm Builder (GPPB) to integrate diverse inference methods within a unified structure. We validate KERAIA's versatility, expressiveness, and practical applicability through detailed analysis of multiple case studies spanning naval warfare simulation, industrial diagnostics in water treatment plants, and strategic decision-making in the game of RISK. Furthermore, we provide a comparative analysis against established knowledge representation paradigms (including ontologies, rule-based systems, and knowledge graphs) and discuss the implementation aspects and computational considerations of the KERAIA platform.

**Link**: [arxiv](http://arxiv.org/abs/2505.04313v1),  [pdf](http://arxiv.org/pdf/2505.04313v1)

**Tags**: cs.AI cs.ET cs.SC 



### Beyond entropic regularization: Debiased Gaussian estimators for   discrete optimal transport and general linear programs
**Authors**: Shuyu Liu, Florentina Bunea, Jonathan Niles-Weed

**Updated**: 2025-05-07T10:55:14Z

**Summary**: This work proposes new estimators for discrete optimal transport plans that enjoy Gaussian limits centered at the true solution. This behavior stands in stark contrast with the performance of existing estimators, including those based on entropic regularization, which are asymptotically biased and only satisfy a CLT centered at a regularized version of the population-level plan. We develop a new regularization approach based on a different class of penalty functions, which can be viewed as the duals of those previously considered in the literature. The key feature of these penalty schemes it that they give rise to preliminary estimates that are asymptotically linear in the penalization strength. Our final estimator is obtained by constructing an appropriate linear combination of two penalized solutions corresponding to two different tuning parameters so that the bias introduced by the penalization cancels out. Unlike classical debiasing procedures, therefore, our proposal entirely avoids the delicate problem of estimating and then subtracting the estimated bias term. Our proofs, which apply beyond the case of optimal transport, are based on a novel asymptotic analysis of penalization schemes for linear programs. As a corollary of our results, we obtain the consistency of the naive bootstrap for fully data-driven inference on the true optimal solution. Simulation results and two data analyses support strongly the benefits of our approach relative to existing techniques.

**Link**: [arxiv](http://arxiv.org/abs/2505.04312v1),  [pdf](http://arxiv.org/pdf/2505.04312v1)

**Tags**: math.ST stat.TH 



### HM-DF SNN: Transcending Conventional Online Learning with Advanced   Training and Deployment
**Authors**: Zecheng Hao, Yifan Huang, Zijie Xu, Wenxuan Liu, Yuanhong Tang, Zhaofei Yu, Tiejun Huang

**Updated**: 2025-05-07T10:08:15Z

**Summary**: Spiking Neural Networks (SNNs) are considered to have enormous potential in the future development of Artificial Intelligence due to their brain-inspired and energy-efficient properties. Compared to vanilla Spatial-Temporal Back-propagation (STBP) training methods, online training can effectively overcome the risk of GPU memory explosion. However, current online learning framework cannot tackle the inseparability problem of temporal dependent gradients and merely aim to optimize the training memory, resulting in no performance advantages compared to the STBP training models in the inference phase. To address the aforementioned challenges, we propose Hybrid Mechanism-Driven Firing (HM-DF) model, which is a family of advanced models that respectively adopt different spiking calculation schemes in the upper-region and lower-region of the firing threshold. We point out that HM-DF model can effectively separate temporal gradients and tackle the mismatch problem of surrogate gradients, as well as achieving full-stage optimization towards computation speed and memory footprint. Experimental results have demonstrated that HM-DF model can be flexibly combined with various techniques to achieve state-of-the-art performance in the field of online learning, without triggering further power consumption.

**Link**: [arxiv](http://arxiv.org/abs/2410.07547v2),  [pdf](http://arxiv.org/pdf/2410.07547v2)

**Tags**: cs.NE cs.AI 



### Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating   the Hallucination for Path Planning
**Authors**: Hourui Deng, Hongjie Zhang, Jie Ou, Chaosheng Feng

**Updated**: 2025-05-07T10:00:50Z

**Summary**: Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied intelligence. However, even in simple maze environments, LLMs still encounter challenges in long-term path-planning, primarily influenced by their spatial hallucination and context inconsistency hallucination by long-term reasoning. To address this challenge, this study proposes an innovative model, Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs, we propose the Spatial-to-Relational approach, which transforms spatial prompts into entity relations and paths representing entity relation chains. This approach fully taps the potential of LLMs in terms of sequential thinking. As a result, we design a path-planning algorithm based on Q-learning to mitigate the context inconsistency hallucination, which enhances the reasoning ability of LLMs. Using the Q-value of state-action as auxiliary information for prompts, we correct the hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally, we propose a reverse curriculum learning technique based on LLMs to further mitigate the context inconsistency hallucination. LLMs can rapidly accumulate successful experiences by reducing task difficulty and leveraging them to tackle more complex tasks. We performed comprehensive experiments based on Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our S2RCQL achieved a 23%--40% improvement in both success and optimality rates compared with advanced prompt engineering.

**Link**: [arxiv](http://arxiv.org/abs/2408.13184v3),  [pdf](http://arxiv.org/pdf/2408.13184v3)

**Tags**: cs.CL 



### Grokking in the Wild: Data Augmentation for Real-World Multi-Hop   Reasoning with Transformers
**Authors**: Roman Abramov, Felix Steinbauer, Gjergji Kasneci

**Updated**: 2025-05-07T09:47:51Z

**Summary**: Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.

**Link**: [arxiv](http://arxiv.org/abs/2504.20752v2),  [pdf](http://arxiv.org/pdf/2504.20752v2)

**Tags**: cs.CL cs.AI cs.LG I.2.7; I.2.6; I.2.3; I.7 



### GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer   Pharmacovigilance
**Authors**: Sofia Jamil, Aryan Dabad, Bollampalli Areen Reddy, Sriparna Saha, Rajiv Misra, Adil A. Shakur

**Updated**: 2025-05-07T09:40:18Z

**Summary**: In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2505.04284v1),  [pdf](http://arxiv.org/pdf/2505.04284v1)

**Tags**: cs.CL cs.AI 



### CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to   Sustainability Data Extraction
**Authors**: Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa

**Updated**: 2025-05-07T09:39:42Z

**Summary**: Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.

**Link**: [arxiv](http://arxiv.org/abs/2501.18504v3),  [pdf](http://arxiv.org/pdf/2501.18504v3)

**Tags**: cs.CV cs.AI cs.NE 68W50, 68T07 G.1.6; I.2.10 



### Test It Before You Trust It: Applying Software Testing for Trustworthy   In-context Learning
**Authors**: Teeradaj Racharak, Chaiyong Ragkhitwetsagul, Chommakorn Sontesadisai, Thanwadee Sunetnanta

**Updated**: 2025-05-07T09:29:45Z

**Summary**: In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.18827v2),  [pdf](http://arxiv.org/pdf/2504.18827v2)

**Tags**: cs.SE cs.AI 



### Weaponizing Language Models for Cybersecurity Offensive Operations:   Automating Vulnerability Assessment Report Validation; A Review Paper
**Authors**: Abdulrahman S Almuhaidib, Azlan Mohd Zain, Zalmiyah Zakaria, Izyan Izzati Kamsani, Abdulaziz S Almuhaidib

**Updated**: 2025-05-07T09:14:55Z

**Summary**: This, with the ever-increasing sophistication of cyberwar, calls for novel solutions. In this regard, Large Language Models (LLMs) have emerged as a highly promising tool for defensive and offensive cybersecurity-related strategies. While existing literature has focused much on the defensive use of LLMs, when it comes to their offensive utilization, very little has been reported-namely, concerning Vulnerability Assessment (VA) report validation. Consequentially, this paper tries to fill that gap by investigating the capabilities of LLMs in automating and improving the validation process of the report of the VA. From the critical review of the related literature, this paper hereby proposes a new approach to using the LLMs in the automation of the analysis and within the validation process of the report of the VA that could potentially reduce the number of false positives and generally enhance efficiency. These results are promising for LLM automatization for improving validation on reports coming from VA in order to improve accuracy while reducing human effort and security postures. The contribution of this paper provides further evidence about the offensive and defensive LLM capabilities and therefor helps in devising more appropriate cybersecurity strategies and tools accordingly.

**Link**: [arxiv](http://arxiv.org/abs/2505.04265v1),  [pdf](http://arxiv.org/pdf/2505.04265v1)

**Tags**: cs.CR cs.AI 



### Steerable Chatbots: Personalizing LLMs with Preference-Based Activation   Steering
**Authors**: Jessica Y. Bo, Tianyu Xu, Ishan Chatterjee, Katrina Passarella-Ward, Achin Kulshrestha, D Shin

**Updated**: 2025-05-07T09:10:51Z

**Summary**: As large language models (LLMs) improve in their capacity to serve as personal AI assistants, their ability to output uniquely tailored, personalized responses that align with the soft preferences of their users is essential for enhancing user satisfaction and retention. However, untrained lay users have poor prompt specification abilities and often struggle with conveying their latent preferences to AI assistants. To address this, we leverage activation steering to guide LLMs to align with interpretable preference dimensions during inference. In contrast to memory-based personalization methods that require longer user history, steering is extremely lightweight and can be easily controlled by the user via an linear strength factor. We embed steering into three different interactive chatbot interfaces and conduct a within-subjects user study (n=14) to investigate how end users prefer to personalize their conversations. The results demonstrate the effectiveness of preference-based steering for aligning real-world conversations with hidden user preferences, and highlight further insights on how diverse values around control, usability, and transparency lead users to prefer different interfaces.

**Link**: [arxiv](http://arxiv.org/abs/2505.04260v1),  [pdf](http://arxiv.org/pdf/2505.04260v1)

**Tags**: cs.HC cs.AI 



### CompileAgent: Automated Real-World Repo-Level Compilation with   Tool-Integrated LLM-based Agent System
**Authors**: Li Hu, Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Benlong Wu, Gangyang Li, Xu Zhu, Weiming Zhang, Nenghai Yu

**Updated**: 2025-05-07T08:59:14Z

**Summary**: With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects.

**Link**: [arxiv](http://arxiv.org/abs/2505.04254v1),  [pdf](http://arxiv.org/pdf/2505.04254v1)

**Tags**: cs.SE 



### LLM-Independent Adaptive RAG: Let the Question Speak for Itself
**Authors**: Maria Marina, Nikolay Ivanov, Sergey Pletenev, Mikhail Salnikov, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii

**Updated**: 2025-05-07T08:58:52Z

**Summary**: Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2505.04253v1),  [pdf](http://arxiv.org/pdf/2505.04253v1)

**Tags**: cs.CL cs.LG 



### Facilitating Trustworthy Human-Agent Collaboration in LLM-based   Multi-Agent System oriented Software Engineering
**Authors**: Krishna Ronanki

**Updated**: 2025-05-07T08:55:15Z

**Summary**: Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular autonomous agents. This holds true within the field of software engineering (SE) as well. The state-of-the-art research on MAS within SE focuses on integrating LLMs at the core of autonomous agents to create LLM-based multi-agent autonomous (LMA) systems. However, the introduction of LMA systems into SE brings a plethora of challenges. One of the major challenges is the strategic allocation of tasks between humans and the LMA system in a trustworthy manner. To address this challenge, a RACI-based framework is proposed in this work in progress article, along with implementation guidelines and an example implementation of the framework. The proposed framework can facilitate efficient collaboration, ensure accountability, and mitigate potential risks associated with LLM-driven automation while aligning with the Trustworthy AI guidelines. The future steps for this work delineating the planned empirical validation method are also presented.

**Link**: [arxiv](http://arxiv.org/abs/2505.04251v1),  [pdf](http://arxiv.org/pdf/2505.04251v1)

**Tags**: cs.SE cs.AI cs.MA 



### Multi-Agent Reinforcement Learning-based Cooperative Autonomous Driving   in Smart Intersections
**Authors**: Taoyuan Yu, Kui Wang, Zongdian Li, Tao Yu, Kei Sakaguchi

**Updated**: 2025-05-07T08:27:52Z

**Summary**: Unsignalized intersections pose significant safety and efficiency challenges due to complex traffic flows. This paper proposes a novel roadside unit (RSU)-centric cooperative driving system leveraging global perception and vehicle-to-infrastructure (V2I) communication. The core of the system is an RSU-based decision-making module using a two-stage hybrid reinforcement learning (RL) framework. At first, policies are pre-trained offline using conservative Q-learning (CQL) combined with behavior cloning (BC) on collected dataset. Subsequently, these policies are fine-tuned in the simulation using multi-agent proximal policy optimization (MAPPO), aligned with a self-attention mechanism to effectively solve inter-agent dependencies. RSUs perform real-time inference based on the trained models to realize vehicle control via V2I communications. Extensive experiments in CARLA environment demonstrate high effectiveness of the proposed system, by: \textit{(i)} achieving failure rates below 0.03\% in coordinating three connected and autonomous vehicles (CAVs) through complex intersection scenarios, significantly outperforming the traditional Autoware control method, and \textit{(ii)} exhibiting strong robustness across varying numbers of controlled agents and shows promising generalization capabilities on other maps.

**Link**: [arxiv](http://arxiv.org/abs/2505.04231v1),  [pdf](http://arxiv.org/pdf/2505.04231v1)

**Tags**: cs.RO cs.MA cs.SY eess.SY 



### To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase   Relevance at eBay
**Authors**: Soumik Dey, Hansi Wu, Binbin Li

**Updated**: 2025-05-07T08:03:25Z

**Summary**: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.

**Link**: [arxiv](http://arxiv.org/abs/2505.04209v1),  [pdf](http://arxiv.org/pdf/2505.04209v1)

**Tags**: cs.IR cs.AI cs.LG 



### InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM   with Optical Circuit Switching Transceivers
**Authors**: Chenchen Shou, Guyue Liu, Hao Nie, Huaiyu Meng, Yu Zhou, Yimin Jiang, Wenqing Lv, Yelong Xu, Yuanwei Lu, Zhang Chen, Yanbo Yu, Yichen Shen, Yibo Zhu, Daxin Jiang

**Updated**: 2025-05-07T08:02:44Z

**Summary**: Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism (TP) and Expert Parallelism (EP). However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such as TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches, but the fault explosion radius remains large at the cube level (e.g., 64 TPUs).   We propose InfiniteHBD, a novel transceiver-centric HBD architecture that unifies connectivity and dynamic switching at the transceiver level using Optical Circuit Switching (OCS). By embedding OCS within each transceiver, InfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing the topology to adapt into variable-size rings. This design provides: i) datacenter-wide scalability without cost explosion; ii) fault resilience by isolating failures to a single node, and iii) full bandwidth utilization for fault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based low-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology co-designed with intra-/inter-node communication, and an HBD-DCN orchestration algorithm maximizing GPU utilization while minimizing cross-ToR datacenter network traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of the cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude lower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault ratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs per Node).

**Link**: [arxiv](http://arxiv.org/abs/2502.03885v3),  [pdf](http://arxiv.org/pdf/2502.03885v3)

**Tags**: cs.NI cs.DC cs.LG 



### Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer   Gate
**Authors**: Zhiyuan Fang, Zicong Hong, Yuegui Huang, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, Zibin Zheng

**Updated**: 2025-05-07T07:57:21Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.

**Link**: [arxiv](http://arxiv.org/abs/2502.12224v2),  [pdf](http://arxiv.org/pdf/2502.12224v2)

**Tags**: cs.AI cs.LG 



### A Computationally Efficient Approach to False Discovery Rate Control and   Power Maximisation via Randomisation and Mirror Statistic
**Authors**: Marco Molinari, Magne Thoresen

**Updated**: 2025-05-07T07:56:43Z

**Summary**: Simultaneously performing variable selection and inference in high-dimensional regression models is an open challenge in statistics and machine learning. The increasing availability of vast amounts of variables requires the adoption of specific statistical procedures to accurately select the most important predictors in a high-dimensional space, while controlling the false discovery rate (FDR) associated with the variable selection procedure. In this paper, we propose the joint adoption of the Mirror Statistic approach to FDR control, coupled with outcome randomisation to maximise the statistical power of the variable selection procedure, measured through the true positive rate. Through extensive simulations, we show how our proposed strategy allows us to combine the benefits of the two techniques. The Mirror Statistic is a flexible method to control FDR, which only requires mild model assumptions, but requires two sets of independent regression coefficient estimates, usually obtained after splitting the original dataset. Outcome randomisation is an alternative to data splitting that allows to generate two independent outcomes, which can then be used to estimate the coefficients that go into the construction of the Mirror Statistic. The combination of these two approaches provides increased testing power in a number of scenarios, such as highly correlated covariates and high percentages of active variables. Moreover, it is scalable to very high-dimensional problems, since the algorithm has a low memory footprint and only requires a single run on the full dataset, as opposed to iterative alternatives such as multiple data splitting.

**Link**: [arxiv](http://arxiv.org/abs/2401.12697v2),  [pdf](http://arxiv.org/pdf/2401.12697v2)

**Tags**: stat.ME stat.AP 



### A Large Language Model for Feasible and Diverse Population Synthesis
**Authors**: Sung Yoo Lim, Hyunsoo Yun, Prateek Bansal, Dong-Kyu Kim, Eui-Jin Kim

**Updated**: 2025-05-07T07:50:12Z

**Summary**: Generating a synthetic population that is both feasible and diverse is crucial for ensuring the validity of downstream activity schedule simulation in activity-based models (ABMs). While deep generative models (DGMs), such as variational autoencoders and generative adversarial networks, have been applied to this task, they often struggle to balance the inclusion of rare but plausible combinations (i.e., sampling zeros) with the exclusion of implausible ones (i.e., structural zeros). To improve feasibility while maintaining diversity, we propose a fine-tuning method for large language models (LLMs) that explicitly controls the autoregressive generation process through topological orderings derived from a Bayesian Network (BN). Experimental results show that our hybrid LLM-BN approach outperforms both traditional DGMs and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically, our approach achieves approximately 95% feasibility, significantly higher than the ~80% observed in DGMs, while maintaining comparable diversity, making it well-suited for practical applications. Importantly, the method is based on a lightweight open-source LLM, enabling fine-tuning and inference on standard personal computing environments. This makes the approach cost-effective and scalable for large-scale applications, such as synthesizing populations in megacities, without relying on expensive infrastructure. By initiating the ABM pipeline with high-quality synthetic populations, our method improves overall simulation reliability and reduces downstream error propagation. The source code for these methods is available for research and practical application.

**Link**: [arxiv](http://arxiv.org/abs/2505.04196v1),  [pdf](http://arxiv.org/pdf/2505.04196v1)

**Tags**: cs.LG cs.MA 



### AutoPatch: Multi-Agent Framework for Patching Real-World CVE   Vulnerabilities
**Authors**: Minjae Seo, Wonwoo Choi, Myoungsung You, Seungwon Shin

**Updated**: 2025-05-07T07:49:05Z

**Summary**: Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.04195v1),  [pdf](http://arxiv.org/pdf/2505.04195v1)

**Tags**: cs.CR 



### Liger: Linearizing Large Language Models to Gated Recurrent Structures
**Authors**: Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, Yu Cheng

**Updated**: 2025-05-07T07:42:11Z

**Summary**: Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\% of the Transformer-based LLM at 0.02\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.

**Link**: [arxiv](http://arxiv.org/abs/2503.01496v2),  [pdf](http://arxiv.org/pdf/2503.01496v2)

**Tags**: cs.CL cs.AI cs.LG 



### Sentiment and Social Signals in the Climate Crisis: A Survey on   Analyzing Social Media Responses to Extreme Weather Events
**Authors**: Pouya Shaeri, Yasaman Mohammadpour, Alimohammad Beigi, Ariane Middel, Huan Liu

**Updated**: 2025-05-07T07:37:47Z

**Summary**: Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.

**Link**: [arxiv](http://arxiv.org/abs/2504.18837v3),  [pdf](http://arxiv.org/pdf/2504.18837v3)

**Tags**: cs.SI 



### RaDialog: A Large Vision-Language Model for Radiology Report Generation   and Conversational Assistance
**Authors**: Chantal Pellegrini, Ege Özsoy, Benjamin Busam, Nassir Navab, Matthias Keicher

**Updated**: 2025-05-07T07:22:02Z

**Summary**: Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog.

**Link**: [arxiv](http://arxiv.org/abs/2311.18681v3),  [pdf](http://arxiv.org/pdf/2311.18681v3)

**Tags**: cs.CV cs.CL 



### On-Device LLM for Context-Aware Wi-Fi Roaming
**Authors**: Ju-Hyung Lee, Yanqing Lu

**Updated**: 2025-05-07T07:04:49Z

**Summary**: Wireless roaming is a critical yet challenging task for maintaining seamless connectivity in dynamic mobile environments. Conventional threshold-based or heuristic schemes often fail, leading to either sticky or excessive handovers. We introduce the first cross-layer use of an on-device large language model (LLM): high-level reasoning in the application layer that issues real-time actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i) context-aware AP selection, where structured prompts fuse environmental cues (e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold adjustment, where the model adaptively decides when to roam. To satisfy the tight latency and resource budgets of edge hardware, we apply a suite of optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and quantization. Experiments on indoor and outdoor datasets show that our approach surpasses legacy heuristics and DRL baselines, achieving a strong balance between roaming stability and signal quality. These findings underscore the promise of application-layer LLM reasoning for lower-layer wireless control in future edge systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.04174v1),  [pdf](http://arxiv.org/pdf/2505.04174v1)

**Tags**: cs.LG cs.AI cs.NI eess.SP 



### Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath
**Authors**: Chris Wise, Akram Youssry, Alberto Peruzzo, Jo Plested, Matt Woolley

**Updated**: 2025-05-07T06:55:17Z

**Summary**: Qubit control protocols have traditionally leveraged a characterisation of the qubit-bath coupling via its power spectral density. Previous work proposed the inference of noise operators that characterise the influence of a classical bath using a grey-box approach that combines deep neural networks with physics-encoded layers. This overall structure is complex and poses challenges in scaling and real-time operations. Here, we show that no expensive neural networks are needed and that this noise operator description admits an efficient parameterisation. We refer to the resulting parameter space as the \textit{quantum feature space} of the qubit dynamics resulting from the coupled bath. We show that the Euclidean distance defined over the quantum feature space provides an effective method for classifying noise processes in the presence of a given set of controls. Using the quantum feature space as the input space for a simple machine learning algorithm (random forest, in this case), we demonstrate that it can effectively classify the stationarity and the broad class of noise processes perturbing a qubit. Finally, we explore how control pulse parameters map to the quantum feature space.

**Link**: [arxiv](http://arxiv.org/abs/2505.03397v2),  [pdf](http://arxiv.org/pdf/2505.03397v2)

**Tags**: quant-ph cs.LG 



### Large Language Models are often politically extreme, usually   ideologically inconsistent, and persuasive even in informational contexts
**Authors**: Nouar Aldahoul, Hazem Ibrahim, Matteo Varvello, Aaron Kaufman, Talal Rahwan, Yasir Zaki

**Updated**: 2025-05-07T06:53:59Z

**Summary**: Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.

**Link**: [arxiv](http://arxiv.org/abs/2505.04171v1),  [pdf](http://arxiv.org/pdf/2505.04171v1)

**Tags**: cs.CY cs.CL 



### Principal Curves In Metric Spaces And The Space Of Probability Measures
**Authors**: Andrew Warren, Anton Afanassiev, Forest Kobayashi, Young-Heon Kim, Geoffrey Schiebinger

**Updated**: 2025-05-07T06:44:34Z

**Summary**: We introduce principal curves in Wasserstein space, and in general compact metric spaces. Our motivation for the Wasserstein case comes from optimal-transport-based trajectory inference, where a developing population of cells traces out a curve in Wasserstein space. Our framework enables new experimental procedures for collecting high-density time-courses of developing populations of cells: time-points can be processed in parallel (making it easier to collect more time-points). However, then the time of collection is unknown, and must be recovered by solving a seriation problem (or one-dimensional manifold learning problem).   We propose an estimator based on Wasserstein principal curves, and prove it is consistent for recovering a curve of probability measures in Wasserstein space from empirical samples. This consistency theorem is obtained via a series of results regarding principal curves in compact metric spaces. In particular, we establish the validity of certain numerical discretization schemes for principal curves, which is a new result even in the Euclidean setting.

**Link**: [arxiv](http://arxiv.org/abs/2505.04168v1),  [pdf](http://arxiv.org/pdf/2505.04168v1)

**Tags**: math.ST stat.TH Primary: 62G05, 49Q20, Secondary: 62P10, 62R20 



### Generative AI in Transportation Planning: A Survey
**Authors**: Longchao Da, Tiejin Chen, Zhuoheng Li, Shreyas Bachiraju, Huaiyuan Yao, Li Li, Yushun Dong, Xiyang Hu, Zhengzhong Tu, Dongjie Wang, Yue Zhao, Ben Zhou, Ram Pendyala, Benjamin Stabler, Yezhou Yang, Xuesong Zhou, Hua Wei

**Updated**: 2025-05-07T06:35:15Z

**Summary**: The integration of generative artificial intelligence (GenAI) into transportation planning has the potential to revolutionize tasks such as demand forecasting, infrastructure design, policy evaluation, and traffic simulation. However, there is a critical need for a systematic framework to guide the adoption of GenAI in this interdisciplinary domain. In this survey, we, a multidisciplinary team of researchers spanning computer science and transportation engineering, present the first comprehensive framework for leveraging GenAI in transportation planning. Specifically, we introduce a new taxonomy that categorizes existing applications and methodologies into two perspectives: transportation planning tasks and computational techniques. From the transportation planning perspective, we examine the role of GenAI in automating descriptive, predictive, generative, simulation, and explainable tasks to enhance mobility systems. From the computational perspective, we detail advancements in data preparation, domain-specific fine-tuning, and inference strategies, such as retrieval-augmented generation and zero-shot learning tailored to transportation applications. Additionally, we address critical challenges, including data scarcity, explainability, bias mitigation, and the development of domain-specific evaluation frameworks that align with transportation goals like sustainability, equity, and system efficiency. This survey aims to bridge the gap between traditional transportation planning methodologies and modern AI techniques, fostering collaboration and innovation. By addressing these challenges and opportunities, we seek to inspire future research that ensures ethical, equitable, and impactful use of generative AI in transportation planning.

**Link**: [arxiv](http://arxiv.org/abs/2503.07158v5),  [pdf](http://arxiv.org/pdf/2503.07158v5)

**Tags**: cs.AI 68T99, 90B06 I.2.6; I.2.8; I.6.3; J.2 



### Can Language Models Understand Social Behavior in Clinical   Conversations?
**Authors**: Manas Satish Bedmutha, Feng Chen, Andrea Hartzler, Trevor Cohen, Nadir Weibel

**Updated**: 2025-05-07T06:03:37Z

**Summary**: Effective communication between providers and their patients influences health and care outcomes. The effectiveness of such conversations has been linked not only to the exchange of clinical information, but also to a range of interpersonal behaviors; commonly referred to as social signals, which are often conveyed through non-verbal cues and shape the quality of the patient-provider relationship. Recent advances in large language models (LLMs) have demonstrated an increasing ability to infer emotional and social behaviors even when analyzing only textual information. As automation increases also in clinical settings, such as for transcription of patient-provider conversations, there is growing potential for LLMs to automatically analyze and extract social behaviors from these interactions. To explore the foundational capabilities of LLMs in tracking social signals in clinical dialogue, we designed task-specific prompts and evaluated model performance across multiple architectures and prompting styles using a highly imbalanced, annotated dataset spanning 20 distinct social signals such as provider dominance, patient warmth, etc. We present the first system capable of tracking all these 20 coded signals, and uncover patterns in LLM behavior. Further analysis of model configurations and clinical context provides insights for enhancing LLM performance on social signal processing tasks in healthcare settings.

**Link**: [arxiv](http://arxiv.org/abs/2505.04152v1),  [pdf](http://arxiv.org/pdf/2505.04152v1)

**Tags**: cs.CL cs.CY cs.HC H.5.2; H.1.2; I.2.7; I.2.m; J.3 



### R^3-VQA: "Read the Room" by Video Social Reasoning
**Authors**: Lixing Niu, Jiapeng Li, Xingping Yu, Shu Wang, Ruining Feng, Bo Wu, Ping Wei, Yisen Wang, Lifeng Fan

**Updated**: 2025-05-07T05:55:45Z

**Summary**: "Read the room" is a significant social reasoning capability in human daily life. Humans can infer others' mental states from subtle social cues. Previous social reasoning tasks and datasets lack complexity (e.g., simple scenes, basic interactions, incomplete mental state variables, single-step reasoning, etc.) and fall far short of the challenges present in real-life social interactions. In this paper, we contribute a valuable, high-quality, and comprehensive video dataset named R^3-VQA with precise and fine-grained annotations of social events and mental states (i.e., belief, intent, desire, and emotion) as well as corresponding social causal chains in complex social scenarios. Moreover, we include human-annotated and model-generated QAs. Our task R^3-VQA includes three aspects: Social Event Understanding, Mental State Estimation, and Social Causal Reasoning. As a benchmark, we comprehensively evaluate the social reasoning capabilities and consistencies of current state-of-the-art large vision-language models (LVLMs). Comprehensive experiments show that (i) LVLMs are still far from human-level consistent social reasoning in complex social scenarios; (ii) Theory of Mind (ToM) prompting can help LVLMs perform better on social reasoning tasks. We provide some of our dataset and codes in supplementary material and will release our full dataset and codes upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2505.04147v1),  [pdf](http://arxiv.org/pdf/2505.04147v1)

**Tags**: cs.CV cs.AI 



### Unmasking the Canvas: A Dynamic Benchmark for Image Generation   Jailbreaking and LLM Content Safety
**Authors**: Variath Madhupal Gautham Nair, Vishal Varma Dantuluri

**Updated**: 2025-05-07T05:54:04Z

**Summary**: Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.   We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.   Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure.

**Link**: [arxiv](http://arxiv.org/abs/2505.04146v1),  [pdf](http://arxiv.org/pdf/2505.04146v1)

**Tags**: cs.CL cs.AI cs.CY 



### NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large   Language Model Guidance
**Authors**: Yuqing Zhang, Yiannis Kantaros

**Updated**: 2025-05-07T05:45:33Z

**Summary**: Several planners have been proposed to compute robot paths that reach desired goal regions while avoiding obstacles. However, these methods fail when all pathways to the goal are blocked. In such cases, the robot must reason about how to reconfigure the environment to access task-relevant regions - a problem known as Navigation Among Movable Objects (NAMO). While various solutions to this problem have been developed, they often struggle to scale to highly cluttered environments. To address this, we propose NAMO-LLM, a sampling-based planner that searches over robot and obstacle configurations to compute feasible plans specifying which obstacles to move, where, and in what order. Its key novelty is a non-uniform sampling strategy guided by Large Language Models (LLMs) biasing the tree construction toward directions more likely to yield a solution. We show that NAMO-LLM is probabilistically complete and demonstrate through experiments that it efficiently scales to cluttered environments, outperforming related works in both runtime and plan quality.

**Link**: [arxiv](http://arxiv.org/abs/2505.04141v1),  [pdf](http://arxiv.org/pdf/2505.04141v1)

**Tags**: cs.RO 



### Enhancing Granular Sentiment Classification with Chain-of-Thought   Prompting in Large Language Models
**Authors**: Vihaan Miriyala, Smrithi Bukkapatnam, Lavanya Prahallad

**Updated**: 2025-05-07T05:13:15Z

**Summary**: We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.04135v1),  [pdf](http://arxiv.org/pdf/2505.04135v1)

**Tags**: cs.CL cs.LG 



### Inference accuracy about an aircraft crash
**Authors**: François Graner, Stefano Matthias Panebianco

**Updated**: 2025-05-07T05:11:35Z

**Summary**: Problem-based learning benefits from situations taken from real life, which arouse student interest. The shooting of Rwanda president aircraft on April 6th, 1994 is still unsolved. We discuss the methods to infer information and conclusions about where the aircraft was shot and its trajectory during its fall, as well as about the place from which the missiles were launched, and their trajectory and type. To this goal, we compiled expert reports, witness indications and other public sources, then translated plain language sentences into quantitative equalities and inequalities applied to geometry and mechanics at undergraduate level. The accuracy of each result is discussed and propagated in order to ensure a proper assessment of the hypotheses and a traceability of their consequences. Overall, the accuracy discussion can train the students critical mind, and teach inference methods which are routinely used in several fields of physics research. In addition, it demonstrates the importance and limits of scientific expertise during a judiciary process.

**Link**: [arxiv](http://arxiv.org/abs/2501.07261v2),  [pdf](http://arxiv.org/pdf/2501.07261v2)

**Tags**: physics.ed-ph physics.soc-ph 



### Re-ReST: Reflection-Reinforced Self-Training for Language Agents
**Authors**: Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, Nanyun Peng

**Updated**: 2025-05-07T05:01:14Z

**Summary**: Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a \textit{reflector} to refine low-quality generated samples during self-training. The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6\% on HotpotQA and 28.4\% on AlfWorld, and Re-ReST further boosting performance by 2.0\% and 14.1\%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work. Our code is released at https://github.com/PlusLabNLP/Re-ReST.

**Link**: [arxiv](http://arxiv.org/abs/2406.01495v3),  [pdf](http://arxiv.org/pdf/2406.01495v3)

**Tags**: cs.CL 



### Bimanual Regrasp Planning and Control for Active Reduction of Object   Pose Uncertainty
**Authors**: Ryuta Nagahama, Weiwei Wan, Zhengtao Hu, Kensuke Harada

**Updated**: 2025-05-07T04:53:51Z

**Summary**: Precisely grasping an object is a challenging task due to pose uncertainties. Conventional methods have used cameras and fixtures to reduce object uncertainty. They are effective but require intensive preparation, such as designing jigs based on the object geometry and calibrating cameras with high-precision tools fabricated using lasers. In this study, we propose a method to reduce the uncertainty of the position and orientation of a grasped object without using a fixture or a camera. Our method is based on the concept that the flat finger pads of a parallel gripper can reduce uncertainty along its opening/closing direction through flat surface contact. Three orthogonal grasps by parallel grippers with flat finger pads collectively constrain an object's position and orientation to a unique state. Guided by the concepts, we develop a regrasp planning and admittance control approach that sequentially finds and leverages three orthogonal grasps of two robotic arms to actively reduce uncertainties in the object pose. We evaluated the proposed method on different initial object uncertainties and verified that it had good repeatability. The deviation levels of the experimental trials were on the same order of magnitude as those of an optical tracking system, demonstrating strong relative inference performance.

**Link**: [arxiv](http://arxiv.org/abs/2503.22240v2),  [pdf](http://arxiv.org/pdf/2503.22240v2)

**Tags**: cs.RO 



### Advancements and limitations of LLMs in replicating human color-word   associations
**Authors**: Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara

**Updated**: 2025-05-07T04:50:21Z

**Summary**: Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT-4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and 80 words (10 word from eight categories) in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category. However, the highest median performance was approximately 50% even for GPT-4o with visual inputs (chance level of 10%). Moreover, we found performance variations across word categories and colors: while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions. Interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. Thus, despite reasonable alignment in basic color discrimination, humans and LLMs still diverge systematically in the words they assign to those colors. Our study highlights both the advancements in LLM capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and LLMs in representing color-word associations.

**Link**: [arxiv](http://arxiv.org/abs/2411.02116v3),  [pdf](http://arxiv.org/pdf/2411.02116v3)

**Tags**: cs.CL cs.CV cs.GR cs.HC 



### Conditional Lagrangian Wasserstein Flow for Time Series Imputation
**Authors**: Weizhu Qian, Dalin Zhang, Yan Zhao, Yunyao Cheng

**Updated**: 2025-05-07T04:26:40Z

**Summary**: Time series imputation is important for numerous real-world applications. To overcome the limitations of diffusion model-based imputation methods, e.g., slow convergence in inference, we propose a novel method for time series imputation in this work, called Conditional Lagrangian Wasserstein Flow (CLWF). Following the principle of least action in Lagrangian mechanics, we learn the velocity by minimizing the corresponding kinetic energy. Moreover, to enhance the model's performance, we estimate the gradient of a task-specific potential function using a time-dependent denoising autoencoder and integrate it into the base estimator to reduce the sampling variance. Finally, the proposed method demonstrates competitive performance compared to other state-of-the-art imputation approaches.

**Link**: [arxiv](http://arxiv.org/abs/2410.07550v2),  [pdf](http://arxiv.org/pdf/2410.07550v2)

**Tags**: cs.LG stat.ML 



### Polynomial-Time Relational Probabilistic Inference in Open Universes
**Authors**: Luise Ge, Brendan Juba, Kris Nilsson

**Updated**: 2025-05-07T04:14:03Z

**Summary**: Reasoning under uncertainty is a fundamental challenge in Artificial Intelligence. As with most of these challenges, there is a harsh dilemma between the expressive power of the language used, and the tractability of the computational problem posed by reasoning. Inspired by human reasoning, we introduce a method of first-order relational probabilistic inference that satisfies both criteria, and can handle hybrid (discrete and continuous) variables. Specifically, we extend sum-of-squares logic of expectation to relational settings, demonstrating that lifted reasoning in the bounded-degree fragment for knowledge bases of bounded quantifier rank can be performed in polynomial time, even with an a priori unknown and/or countably infinite set of objects. Crucially, our notion of tractability is framed in proof-theoretic terms, which extends beyond the syntactic properties of the language or queries. We are able to derive the tightest bounds provable by proofs of a given degree and size and establish completeness in our sum-of-squares refutations for fixed degrees.

**Link**: [arxiv](http://arxiv.org/abs/2505.04115v1),  [pdf](http://arxiv.org/pdf/2505.04115v1)

**Tags**: cs.AI 



### SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub   Issue Resolution
**Authors**: Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, Kai Chen

**Updated**: 2025-05-07T04:06:41Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving competitive performance among open-source models with scores of 22.0% and 30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.

**Link**: [arxiv](http://arxiv.org/abs/2501.05040v3),  [pdf](http://arxiv.org/pdf/2501.05040v3)

**Tags**: cs.CL 



### Alpha Excel Benchmark
**Authors**: David Noever, Forrest McKee

**Updated**: 2025-05-07T03:56:26Z

**Summary**: This study presents a novel benchmark for evaluating Large Language Models (LLMs) using challenges derived from the Financial Modeling World Cup (FMWC) Excel competitions. We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats and use this dataset to compare the performance of several leading LLMs. Our findings demonstrate significant variations in performance across different challenge categories, with models showing specific strengths in pattern recognition tasks but struggling with complex numerical reasoning. The benchmark provides a standardized framework for assessing LLM capabilities in realistic business-oriented tasks rather than abstract academic problems. This research contributes to the growing field of AI benchmarking by establishing proficiency among the 1.5 billion people who daily use Microsoft Excel as a meaningful evaluation metric that bridges the gap between academic AI benchmarks and practical business applications.

**Link**: [arxiv](http://arxiv.org/abs/2505.04110v1),  [pdf](http://arxiv.org/pdf/2505.04110v1)

**Tags**: cs.LG 



### ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language   Modeling Exploitation
**Authors**: Ruixuan Liu, Toan Tran, Tianhao Wang, Hongsheng Hu, Shuo Wang, Li Xiong

**Updated**: 2025-05-07T03:48:31Z

**Summary**: As large language models (LLMs) increasingly depend on web-scraped datasets, concerns arise over their potential to generate verbatim training content with copyrighted or private information. However, current protections against web crawling or sample-specific memorization are inherently limited, as they require compliance from crawlers (e.g., respecting robots.txt) or model trainers (e.g., applying differential privacy). To empower data owners with direct control, we propose ExpShiled, a proactive self-defense mechanism that mitigates sample-specific memorization via imperceptible text perturbations. This approach requires no external collaboration while maintaining original readability. To evaluate individual-level defense efficacy, we first propose the metric of instance exploitation: a zero value indicates perfect defense, achieved when a protected text's log-perplexity ranking aligns with its counterfactual untrained ranking. We then reveal and validate the memorization trigger hypothesis, demonstrating that a model's memorization of a specific text sample stems primarily from its outlier tokens. Leveraging this insight, we design targeted perturbations that (1) prioritize inherent trigger tokens and (2) introduce artificial trigger tokens as pitfalls to disrupt memorization on the protected sample. Experiments validate our defense across model scales, languages, vision-to-language tasks, and fine-tuning methods. Even with privacy backdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55, and instance exploitation approaches zero. This suggests that compared to the ideal no-misuse scenario, the risk of exposing a text instance remains nearly unchanged despite its inclusion in training data.

**Link**: [arxiv](http://arxiv.org/abs/2412.21123v2),  [pdf](http://arxiv.org/pdf/2412.21123v2)

**Tags**: cs.CR 



## Keyword: LLM Deployment 
 ### EchoInk-R1: Exploring Audio-Visual Reasoning in Multimodal LLMs via   Reinforcement Learning
**Authors**: Zhenghao Xing, Xiaowei Hu, Chi-Wing Fu, Wenhai Wang, Jifeng Dai, Pheng-Ann Heng

**Updated**: 2025-05-07T17:59:49Z

**Summary**: Multimodal large language models (MLLMs) have advanced perception across text, vision, and audio, yet they often struggle with structured cross-modal reasoning, particularly when integrating audio and visual signals. We introduce EchoInk-R1, a reinforcement learning framework that enhances such reasoning in MLLMs. Built upon the Qwen2.5-Omni-7B foundation and optimized with Group Relative Policy Optimization (GRPO), EchoInk-R1 tackles multiple-choice question answering over synchronized audio-image pairs. To enable this, we curate AVQA-R1-6K, a dataset pairing such audio-image inputs with multiple-choice questions derived from OmniInstruct-v1. EchoInk-R1-7B achieves 85.77% accuracy on the validation set, outperforming the base model, which scores 80.53%, using only 562 reinforcement learning steps. Beyond accuracy, EchoInk-R1 demonstrates reflective reasoning by revisiting initial interpretations and refining responses when facing ambiguous multimodal inputs. These results suggest that lightweight reinforcement learning fine-tuning enhances cross-modal reasoning in MLLMs. EchoInk-R1 is the first framework to unify audio, visual, and textual modalities for general open-world reasoning via reinforcement learning. Code and data are publicly released to facilitate further research.

**Link**: [arxiv](http://arxiv.org/abs/2505.04623v1),  [pdf](http://arxiv.org/pdf/2505.04623v1)

**Tags**: eess.AS cs.AI cs.CV cs.MM cs.SD 



### On Path to Multimodal Generalist: General-Level and General-Bench
**Authors**: Hao Fei, Yuan Zhou, Juncheng Li, Xiangtai Li, Qingshan Xu, Bobo Li, Shengqiong Wu, Yaoting Wang, Junbao Zhou, Jiahao Meng, Qingyu Shi, Zhiyuan Zhou, Liangtao Shi, Minghe Gao, Daoan Zhang, Zhiqi Ge, Weiming Wu, Siliang Tang, Kaihang Pan, Yaobo Ye, Haobo Yuan, Tao Zhang, Tianjie Ju, Zixiang Meng, Shilin Xu, Liyu Jia, Wentao Hu, Meng Luo, Jiebo Luo, Tat-Seng Chua, Shuicheng Yan, Hanwang Zhang

**Updated**: 2025-05-07T17:59:32Z

**Summary**: The Multimodal Large Language Model (MLLM) is currently experiencing rapid growth, driven by the advanced capabilities of LLMs. Unlike earlier specialists, existing MLLMs are evolving towards a Multimodal Generalist paradigm. Initially limited to understanding multiple modalities, these models have advanced to not only comprehend but also generate across modalities. Their capabilities have expanded from coarse-grained to fine-grained multimodal understanding and from supporting limited modalities to arbitrary ones. While many benchmarks exist to assess MLLMs, a critical question arises: Can we simply assume that higher performance across tasks indicates a stronger MLLM capability, bringing us closer to human-level AI? We argue that the answer is not as straightforward as it seems. This project introduces General-Level, an evaluation framework that defines 5-scale levels of MLLM performance and generality, offering a methodology to compare MLLMs and gauge the progress of existing systems towards more robust multimodal generalists and, ultimately, towards AGI. At the core of the framework is the concept of Synergy, which measures whether models maintain consistent capabilities across comprehension and generation, and across multiple modalities. To support this evaluation, we present General-Bench, which encompasses a broader spectrum of skills, modalities, formats, and capabilities, including over 700 tasks and 325,800 instances. The evaluation results that involve over 100 existing state-of-the-art MLLMs uncover the capability rankings of generalists, highlighting the challenges in reaching genuine AI. We expect this project to pave the way for future research on next-generation multimodal foundation models, providing a robust infrastructure to accelerate the realization of AGI. Project page: https://generalist.top/

**Link**: [arxiv](http://arxiv.org/abs/2505.04620v1),  [pdf](http://arxiv.org/pdf/2505.04620v1)

**Tags**: cs.CV 



### Merging and Disentangling Views in Visual Reinforcement Learning for   Robotic Manipulation
**Authors**: Abdulaziz Almuzairee, Rohan Patil, Dwait Bhatt, Henrik I. Christensen

**Updated**: 2025-05-07T17:59:28Z

**Summary**: Vision is well-known for its use in manipulation, especially using visual servoing. To make it robust, multiple cameras are needed to expand the field of view. That is computationally challenging. Merging multiple views and using Q-learning allows the design of more effective representations and optimization of sample efficiency. Such a solution might be expensive to deploy. To mitigate this, we introduce a Merge And Disentanglement (MAD) algorithm that efficiently merges views to increase sample efficiency while augmenting with single-view features to allow lightweight deployment and ensure robust policies. We demonstrate the efficiency and robustness of our approach using Meta-World and ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad

**Link**: [arxiv](http://arxiv.org/abs/2505.04619v1),  [pdf](http://arxiv.org/pdf/2505.04619v1)

**Tags**: cs.LG cs.CV cs.RO 



### Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting   Content Creators From AI Crawlers
**Authors**: Enze Liu, Elisa Luo, Shawn Shan, Geoffrey M. Voelker, Ben Y. Zhao, Stefan Savage

**Updated**: 2025-05-07T17:55:19Z

**Summary**: The success of generative AI relies heavily on training on data scraped through extensive crawling of the Internet, a practice that has raised significant copyright, privacy, and ethical concerns. While few measures are designed to resist a resource-rich adversary determined to scrape a site, crawlers can be impacted by a range of existing tools such as robots.txt, NoAI meta tags, and active crawler blocking by reverse proxies.   In this work, we seek to understand the ability and efficacy of today's networking tools to protect content creators against AI-related crawling. For targeted populations like human artists, do they have the technical knowledge and agency to utilize crawler-blocking tools such as robots.txt, and can such tools be effective? Using large scale measurements and a targeted user study of 203 professional artists, we find strong demand for tools like robots.txt, but significantly constrained by critical hurdles in technical awareness, agency in deploying them, and limited efficacy against unresponsive crawlers. We further test and evaluate network-level crawler blockers provided by reverse proxies. Despite relatively limited deployment today, they offer stronger protections against AI crawlers, but still come with their own set of limitations.

**Link**: [arxiv](http://arxiv.org/abs/2411.15091v2),  [pdf](http://arxiv.org/pdf/2411.15091v2)

**Tags**: cs.HC 



### WATCH: Weighted Adaptive Testing for Changepoint Hypotheses via   Weighted-Conformal Martingales
**Authors**: Drew Prinster, Xing Han, Anqi Liu, Suchi Saria

**Updated**: 2025-05-07T17:53:47Z

**Summary**: Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria,'' such as data shifts that violate certain exchangeability assumptions, or do not allow for online adaptation in response to shifts. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that accommodate online adaptation to mild covariate shifts (in the marginal input distribution) while raising alarms in response to more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.04608v1),  [pdf](http://arxiv.org/pdf/2505.04608v1)

**Tags**: cs.LG cs.AI stat.ML 



### OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue   Resolution
**Authors**: Lianghong Guo, Wei Tao, Runhan Jiang, Yanlin Wang, Jiachi Chen, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, Zibin Zheng

**Updated**: 2025-05-07T17:51:10Z

**Summary**: The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.

**Link**: [arxiv](http://arxiv.org/abs/2505.04606v1),  [pdf](http://arxiv.org/pdf/2505.04606v1)

**Tags**: cs.SE 



### OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision   Encoders for Multimodal Learning
**Authors**: Xianhang Li, Yanqing Liu, Haoqin Tu, Hongru Zhu, Cihang Xie

**Updated**: 2025-05-07T17:48:35Z

**Summary**: OpenAI's CLIP, released in early 2021, have long been the go-to choice of vision encoder for building multimodal foundation models. Although recent alternatives such as SigLIP have begun to challenge this status quo, to our knowledge none are fully open: their training data remains proprietary and/or their training recipes are not released. This paper fills this gap with OpenVision, a fully-open, cost-effective family of vision encoders that match or surpass the performance of OpenAI's CLIP when integrated into multimodal frameworks like LLaVA. OpenVision builds on existing works -- e.g., CLIPS for training framework and Recap-DataComp-1B for training data -- while revealing multiple key insights in enhancing encoder quality and showcasing practical benefits in advancing multimodal models. By releasing vision encoders spanning from 5.9M to 632.1M parameters, OpenVision offers practitioners a flexible trade-off between capacity and efficiency in building multimodal models: larger models deliver enhanced multimodal performance, while smaller versions enable lightweight, edge-ready multimodal deployments.

**Link**: [arxiv](http://arxiv.org/abs/2505.04601v1),  [pdf](http://arxiv.org/pdf/2505.04601v1)

**Tags**: cs.CV 



### MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection
**Authors**: Zhihao Zhang, Abhinav Kumar, Girish Chandar Ganesan, Xiaoming Liu

**Updated**: 2025-05-08T06:18:31Z

**Summary**: Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.

**Link**: [arxiv](http://arxiv.org/abs/2505.04594v2),  [pdf](http://arxiv.org/pdf/2505.04594v2)

**Tags**: cs.CV 



### AI Governance to Avoid Extinction: The Strategic Landscape and   Actionable Research Questions
**Authors**: Peter Barnett, Aaron Scher

**Updated**: 2025-05-07T17:35:36Z

**Summary**: Humanity appears to be on course to soon develop AI systems that substantially outperform human experts in all cognitive domains and activities. We believe the default trajectory has a high likelihood of catastrophe, including human extinction. Risks come from failure to control powerful AI systems, misuse of AI by malicious rogue actors, war between great powers, and authoritarian lock-in. This research agenda has two aims: to describe the strategic landscape of AI development and to catalog important governance research questions. These questions, if answered, would provide important insight on how to successfully reduce catastrophic risks.   We describe four high-level scenarios for the geopolitical response to advanced AI development, cataloging the research questions most relevant to each. Our favored scenario involves building the technical, legal, and institutional infrastructure required to internationally restrict dangerous AI development and deployment (which we refer to as an Off Switch), which leads into an internationally coordinated Halt on frontier AI activities at some point in the future. The second scenario we describe is a US National Project for AI, in which the US Government races to develop advanced AI systems and establish unilateral control over global AI development. We also describe two additional scenarios: a Light-Touch world similar to that of today and a Threat of Sabotage situation where countries use sabotage and deterrence to slow AI development.   In our view, apart from the Off Switch and Halt scenario, all of these trajectories appear to carry an unacceptable risk of catastrophic harm. Urgent action is needed from the US National Security community and AI governance ecosystem to answer key research questions, build the capability to halt dangerous AI activities, and prepare for international AI agreements.

**Link**: [arxiv](http://arxiv.org/abs/2505.04592v1),  [pdf](http://arxiv.org/pdf/2505.04592v1)

**Tags**: cs.CY cs.AI 



### ZeroSearch: Incentivize the Search Capability of LLMs without Searching
**Authors**: Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Fei Huang, Yan Zhang

**Updated**: 2025-05-07T17:30:22Z

**Summary**: Effective information searching is essential for enhancing the reasoning and generation capabilities of large language models (LLMs). Recent research has explored using reinforcement learning (RL) to improve LLMs' search capabilities by interacting with live search engines in real-world environments. While these approaches show promising results, they face two major challenges: (1) Uncontrolled Document Quality: The quality of documents returned by search engines is often unpredictable, introducing noise and instability into the training process. (2) Prohibitively High API Costs: RL training requires frequent rollouts, potentially involving hundreds of thousands of search requests, which incur substantial API expenses and severely constrain scalability. To address these challenges, we introduce ZeroSearch, a reinforcement learning framework that incentivizes the search capabilities of LLMs without interacting with real search engines. Our approach begins with lightweight supervised fine-tuning to transform the LLM into a retrieval module capable of generating both relevant and noisy documents in response to a query. During RL training, we employ a curriculum-based rollout strategy that incrementally degrades the quality of generated documents, progressively eliciting the model's reasoning ability by exposing it to increasingly challenging retrieval scenarios. Extensive experiments demonstrate that ZeroSearch effectively incentivizes the search capabilities of LLMs using a 3B LLM as the retrieval module. Remarkably, a 7B retrieval module achieves comparable performance to the real search engine, while a 14B retrieval module even surpasses it. Furthermore, it generalizes well across both base and instruction-tuned models of various parameter sizes and is compatible with a wide range of RL algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2505.04588v1),  [pdf](http://arxiv.org/pdf/2505.04588v1)

**Tags**: cs.CL 



### ACE: A Security Architecture for LLM-Integrated App Systems
**Authors**: Evan Li, Tushin Mallick, Evan Rose, William Robertson, Alina Oprea, Cristina Nita-Rotaru

**Updated**: 2025-05-07T17:26:46Z

**Summary**: LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.   In this work, we identify new attacks impacting the integrity of planning, as well as the integrity and availability of execution in LLM-integrated apps, and demonstrate them against IsolateGPT, a recent solution designed to mitigate attacks from malicious apps. We propose Abstract-Concrete-Execute (ACE), a new secure architecture for LLM-integrated app systems that provides security guarantees for system planning and execution. Specifically, ACE decouples planning into two phases by first creating an abstract execution plan using only trusted information, and then mapping the abstract plan to a concrete plan using installed system apps. We verify that the plans generated by our system satisfy user-specified secure information flow constraints via static analysis on the structured plan output. During execution, ACE enforces data and capability barriers between apps, and ensures that the execution is conducted according to the trusted abstract plan. We show experimentally that our system is secure against attacks from the INJECAGENT benchmark, a standard benchmark for control flow integrity in the face of indirect prompt injection attacks, and our newly introduced attacks. Our architecture represents a significant advancement towards hardening LLM-based systems containing system facilities of varying levels of trustworthiness.

**Link**: [arxiv](http://arxiv.org/abs/2504.20984v2),  [pdf](http://arxiv.org/pdf/2504.20984v2)

**Tags**: cs.CR cs.LG 



### SlideItRight: Using AI to Find Relevant Slides and Provide Feedback for   Open-Ended Questions
**Authors**: Chloe Qianhui Zhao, Jie Cao, Eason Chen, Kenneth R. Koedinger, Jionghao Lin

**Updated**: 2025-05-07T17:24:40Z

**Summary**: Feedback is important in supporting student learning. While various automated feedback systems have been implemented to make the feedback scalable, many existing solutions only focus on generating text-based feedback. As is indicated in the multimedia learning principle, learning with more modalities could help utilize more separate channels, reduce the cognitive load and facilitate students' learning. Hence, it is important to explore the potential of Artificial Intelligence (AI) in feedback generation from and to different modalities. Our study leverages Large Language Models (LLMs) for textual feedback with the supplementary guidance from other modality - relevant lecture slide retrieved from the slides hub. Through an online crowdsourcing study (N=91), this study investigates learning gains and student perceptions using a 2x2 design (i.e., human feedback vs. AI feedback and with vs. without relevant slide), evaluating the clarity, engagement, perceived effectiveness, and reliability) of AI-facilitated multimodal feedback. We observed significant pre-to-post learning gains across all conditions. However, the differences in these gains were not statistically significant between conditions. The post-survey revealed that students found the slide feedback helpful in their learning process, though they reported difficulty in understanding it. Regarding the AI-generated open-ended feedback, students considered it personalized and relevant to their responses, but they expressed lower trust in the AI feedback compared to human-generated feedback.

**Link**: [arxiv](http://arxiv.org/abs/2505.04584v1),  [pdf](http://arxiv.org/pdf/2505.04584v1)

**Tags**: cs.HC 



### Efficiency Meets Fidelity: A Novel Quantization Framework for Stable   Diffusion
**Authors**: Shuaiting Li, Juncan Deng, Zeyu Wang, Kedong Xu, Rongtao Deng, Hong Gu, Haibin Shen, Kejie Huang

**Updated**: 2025-05-07T16:57:47Z

**Summary**: Text-to-image generation via Stable Diffusion models (SDM) have demonstrated remarkable capabilities. However, their computational intensity, particularly in the iterative denoising process, hinders real-time deployment in latency-sensitive applications. While Recent studies have explored post-training quantization (PTQ) and quantization-aware training (QAT) methods to compress Diffusion models, existing methods often overlook the consistency between results generated by quantized models and those from floating-point models. This consistency is paramount for professional applications where both efficiency and output reliability are essential. To ensure that quantized SDM generates high-quality and consistent images, we propose an efficient quantization framework for SDM. Our framework introduces a Serial-to-Parallel pipeline that simultaneously maintains training-inference consistency and ensures optimization stability. Building upon this foundation, we further develop several techniques including multi-timestep activation quantization, time information precalculation, inter-layer distillation, and selective freezing, to achieve high-fidelity generation in comparison to floating-point models while maintaining quantization efficiency.   Through comprehensive evaluation across multiple Stable Diffusion variants (v1-4, v2-1, XL 1.0, and v3), our method demonstrates superior performance over state-of-the-art approaches with shorter training times. Under W4A8 quantization settings, we achieve significant improvements in both distribution similarity and visual fidelity, while preserving a high image quality.

**Link**: [arxiv](http://arxiv.org/abs/2412.06661v2),  [pdf](http://arxiv.org/pdf/2412.06661v2)

**Tags**: cs.CV 



### User Selection in Near-Field Gigantic MIMO Systems with Modular Arrays
**Authors**: José P. González-Coma, Santiago Fernández, F. Javier López-Martínez

**Updated**: 2025-05-07T16:55:13Z

**Summary**: Modular Arrays (MAs) are a promising architecture to enable multi-user communications in next-generation multiple-input multiple-output (MIMO) systems based on extra-large (XL) or gigantic MIMO (gMIMO) deployments, trading off improved spatial resolution with characteristic interference patterns associated with grating lobes. In this work, we analyze whether MAs can outperform conventional collocated deployments, in terms of achievable sum-spectral efficiency (SE) and served users in a multi-user downlink set-up. First, we provide a rigorous analytical characterization of the inter-user interference for modular gMIMO systems operating in the near field. Then, we leverage these results to optimize the user selection and precoding mechanisms, designing two algorithms that largely outperform existing alternatives in the literature, with different algorithmic complexities. Results show that the proposed algorithms yield over 70% improvements in achievable sum-spectral efficiencies compared to the state of the art. We also illustrate how MAs allows us to serve a larger number of users thanks to their improved spatial resolution, compared to the collocated counterpart.

**Link**: [arxiv](http://arxiv.org/abs/2501.05820v2),  [pdf](http://arxiv.org/pdf/2501.05820v2)

**Tags**: eess.SP 



### LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation
**Authors**: Weiquan Huang, Aoqi Wu, Yifan Yang, Xufang Luo, Yuqing Yang, Liang Hu, Qi Dai, Chunyu Wang, Xiyang Dai, Dongdong Chen, Chong Luo, Lili Qiu

**Updated**: 2025-05-07T16:51:33Z

**Summary**: CLIP is a foundational multimodal model that aligns image and text features into a shared representation space via contrastive learning on large-scale image-text pairs. Its effectiveness primarily stems from the use of natural language as rich supervision. Motivated by the remarkable advancements in large language models (LLMs), this work explores how LLMs' superior text understanding and extensive open-world knowledge can enhance CLIP's capability, especially for processing longer and more complex image captions. We propose an efficient post-training strategy that integrates LLMs into pretrained CLIP. To address the challenge posed by the autoregressive nature of LLMs, we introduce a caption-to-caption contrastive fine-tuning framework, significantly enhancing the discriminative quality of LLM outputs. Extensive experiments demonstrate that our approach outperforms LoRA-based methods, achieving nearly fourfold faster training with superior performance. Furthermore, we validate substantial improvements over state-of-the-art models such as CLIP, EVA02, and SigLip2 across various zero-shot multimodal retrieval tasks, cross-lingual retrieval tasks, and multimodal language model pretraining.

**Link**: [arxiv](http://arxiv.org/abs/2411.04997v4),  [pdf](http://arxiv.org/pdf/2411.04997v4)

**Tags**: cs.CV cs.CL 



### An LLM-based Self-Evolving Security Framework for 6G Space-Air-Ground   Integrated Networks
**Authors**: Qi Qin, Xinye Cao, Guoshun Nan, Sihan Chen, Rushan Li, Li Su, Haitao Du, Qimei Cui, Pengxuan Mao, Xiaofeng Tao, Tony Q. S. Quek

**Updated**: 2025-05-07T16:04:25Z

**Summary**: Recently emerged 6G space-air-ground integrated networks (SAGINs), which integrate satellites, aerial networks, and terrestrial communications, offer ubiquitous coverage for various mobile applications. However, the highly dynamic, open, and heterogeneous nature of SAGINs poses severe security issues. Forming a defense line of SAGINs suffers from two preliminary challenges: 1) accurately understanding massive unstructured multi-dimensional threat information to generate defense strategies against various malicious attacks, 2) rapidly adapting to potential unknown threats to yield more effective security strategies. To tackle the above two challenges, we propose a novel security framework for SAGINs based on Large Language Models (LLMs), which consists of two key ingredients LLM-6GNG and 6G-INST. Our proposed LLM-6GNG leverages refined chain-of-thought (CoT) reasoning and dynamic multi-agent mechanisms to analyze massive unstructured multi-dimensional threat data and generate comprehensive security strategies, thus addressing the first challenge. Our proposed 6G-INST relies on a novel self-evolving method to automatically update LLM-6GNG, enabling it to accommodate unknown threats under dynamic communication environments, thereby addressing the second challenge. Additionally, we prototype the proposed framework with ns-3, OpenAirInterface (OAI), and software-defined radio (SDR). Experiments on three benchmarks demonstrate the effectiveness of our framework. The results show that our framework produces highly accurate security strategies that remain robust against a variety of unknown attacks. We will release our code to contribute to the community.

**Link**: [arxiv](http://arxiv.org/abs/2505.03161v2),  [pdf](http://arxiv.org/pdf/2505.03161v2)

**Tags**: cs.CR 



### High-Dimensional Interlingual Representations of Large Language Models
**Authors**: Bryan Wilie, Samuel Cahyawijaya, Junxian He, Pascale Fung

**Updated**: 2025-05-07T16:03:47Z

**Summary**: Large language models (LLMs) trained on massive multilingual datasets hint at the formation of interlingual constructs--a shared subspace in the representation space. However, evidence regarding this phenomenon is mixed, leaving it unclear whether these models truly develop unified interlingual representations, or present a partially aligned constructs. We explore 31 diverse languages varying on their resource-levels, typologies, and geographical regions; and find that multilingual LLMs exhibit inconsistent cross-lingual alignments. To address this, we propose an interlingual representation framework identifying both the shared interlingual semantic subspace and fragmented components, existed due to representational limitations. We introduce Interlingual Local Overlap (ILO) score to quantify interlingual alignment by comparing the local neighborhood structures of high-dimensional representations. We utilize ILO to investigate the impact of single-language fine-tuning on the interlingual representations in multilingual LLMs. Our results indicate that training exclusively on a single language disrupts the alignment in early layers, while freezing these layers preserves the alignment of interlingual representations, leading to improved cross-lingual generalization. These results validate our framework and metric for evaluating interlingual representation, and further underscore that interlingual alignment is crucial for scalable multilingual learning.

**Link**: [arxiv](http://arxiv.org/abs/2503.11280v3),  [pdf](http://arxiv.org/pdf/2503.11280v3)

**Tags**: cs.CL 



### RAFT: Robust Augmentation of FeaTures for Image Segmentation
**Authors**: Edward Humes, Xiaomin Lin, Uttej Kallakuri, Tinoosh Mohsenin

**Updated**: 2025-05-07T16:02:46Z

**Summary**: Image segmentation is a powerful computer vision technique for scene understanding. However, real-world deployment is stymied by the need for high-quality, meticulously labeled datasets. Synthetic data provides high-quality labels while reducing the need for manual data collection and annotation. However, deep neural networks trained on synthetic data often face the Syn2Real problem, leading to poor performance in real-world deployments.   To mitigate the aforementioned gap in image segmentation, we propose RAFT, a novel framework for adapting image segmentation models using minimal labeled real-world data through data and feature augmentations, as well as active learning. To validate RAFT, we perform experiments on the synthetic-to-real "SYNTHIA->Cityscapes" and "GTAV->Cityscapes" benchmarks. We managed to surpass the previous state of the art, HALO. SYNTHIA->Cityscapes experiences an improvement in mIoU* upon domain adaptation of 2.1%/79.9%, and GTAV->Cityscapes experiences a 0.4%/78.2% improvement in mIoU. Furthermore, we test our approach on the real-to-real benchmark of "Cityscapes->ACDC", and again surpass HALO, with a gain in mIoU upon adaptation of 1.3%/73.2%. Finally, we examine the effect of the allocated annotation budget and various components of RAFT upon the final transfer mIoU.

**Link**: [arxiv](http://arxiv.org/abs/2505.04529v1),  [pdf](http://arxiv.org/pdf/2505.04529v1)

**Tags**: cs.CV 



### Comparative Analysis of Carbon Footprint in Manual vs. LLM-Assisted Code   Development
**Authors**: Kuen Sum Cheung, Mayuri Kaul, Gunel Jahangirova, Mohammad Reza Mousavi, Eric Zie

**Updated**: 2025-05-07T15:52:06Z

**Summary**: Large Language Models (LLM) have significantly transformed various domains, including software development. These models assist programmers in generating code, potentially increasing productivity and efficiency. However, the environmental impact of utilising these AI models is substantial, given their high energy consumption during both training and inference stages. This research aims to compare the energy consumption of manual software development versus an LLM-assisted approach, using Codeforces as a simulation platform for software development. The goal is to quantify the environmental impact and propose strategies for minimising the carbon footprint of using LLM in software development. Our results show that the LLM-assisted code generation leads on average to 32.72 higher carbon footprint than the manual one. Moreover, there is a significant correlation between task complexity and the difference in the carbon footprint of the two approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.04521v1),  [pdf](http://arxiv.org/pdf/2505.04521v1)

**Tags**: cs.SE 



### Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs
**Authors**: Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, Zongyuan Zhan

**Updated**: 2025-05-07T15:46:36Z

**Summary**: Sparse large language models (LLMs) with Mixture of Experts (MoE) and close to a trillion parameters are dominating the realm of most capable language models. However, the massive model scale poses significant challenges for the underlying software and hardware systems. In this paper, we aim to uncover a recipe to harness such scale on Ascend NPUs. The key goals are better usage of the computing resources under the dynamic sparse model structures and materializing the expected performance gain on the actual hardware. To select model configurations suitable for Ascend NPUs without repeatedly running the expensive experiments, we leverage simulation to compare the trade-off of various model hyperparameters. This study led to Pangu Ultra MoE, a sparse LLM with 718 billion parameters, and we conducted experiments on the model to verify the simulation results. On the system side, we dig into Expert Parallelism to optimize the communication between NPU devices to reduce the synchronization overhead. We also optimize the memory efficiency within the devices to further reduce the parameter and activation management overhead. In the end, we achieve an MFU of 30.0% when training Pangu Ultra MoE, with performance comparable to that of DeepSeek R1, on 6K Ascend NPUs, and demonstrate that the Ascend system is capable of harnessing all the training stages of the state-of-the-art language models. Extensive experiments indicate that our recipe can lead to efficient training of large-scale sparse language models with MoE. We also study the behaviors of such models for future reference.

**Link**: [arxiv](http://arxiv.org/abs/2505.04519v1),  [pdf](http://arxiv.org/pdf/2505.04519v1)

**Tags**: cs.CL 



### OAEI-LLM-T: A TBox Benchmark Dataset for Understanding Large Language   Model Hallucinations in Ontology Matching
**Authors**: Zhangcheng Qiang

**Updated**: 2025-05-07T15:02:02Z

**Summary**: Hallucinations are often inevitable in downstream tasks using large language models (LLMs). To tackle the substantial challenge of addressing hallucinations for LLM-based ontology matching (OM) systems, we introduce a new benchmark dataset called OAEI-LLM-T. The dataset evolves from the TBox (i.e. schema-matching) datasets in the Ontology Alignment Evaluation Initiative (OAEI), capturing hallucinations of different LLMs performing OM tasks. These OM-specific hallucinations are carefully classified into two primary categories and six sub-categories. We showcase the usefulness of the dataset in constructing the LLM leaderboard and fine-tuning foundational LLMs for LLM-based OM systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.21813v2),  [pdf](http://arxiv.org/pdf/2503.21813v2)

**Tags**: cs.CL cs.IR 



### A Design Space for the Critical Validation of LLM-Generated Tabular Data
**Authors**: Madhav Sachdeva, Christopher Narayanan, Marvin Wiedenkeller, Jana Sedlakova, Jürgen Bernard

**Updated**: 2025-05-07T15:01:23Z

**Summary**: LLM-generated tabular data is creating new opportunities for data-driven applications in academia, business, and society. To leverage benefits like missing value imputation, labeling, and enrichment with context-aware attributes, LLM-generated data needs a critical validation process. The number of pioneering approaches is increasing fast, opening a promising validation space that, so far, remains unstructured. We present a design space for the critical validation of LLM-generated tabular data with two dimensions: First, the Analysis Granularity dimension: from within-attribute (single-item and multi-item) to across-attribute perspectives (1 x 1, 1 x m, and n x n). Second, the Data Source dimension: differentiating between LLM-generated values, ground truth values, explanations, and their combinations. We discuss analysis tasks for each dimension cross-cut, map 19 existing validation approaches, and discuss the characteristics of two approaches in detail, demonstrating descriptive power.

**Link**: [arxiv](http://arxiv.org/abs/2505.04487v1),  [pdf](http://arxiv.org/pdf/2505.04487v1)

**Tags**: cs.HC 



### CAD-Llama: Leveraging Large Language Models for Computer-Aided Design   Parametric 3D Model Generation
**Authors**: Jiahao Li, Weijian Ma, Xueyang Li, Yunzhong Lou, Guichun Zhou, Xiangdong Zhou

**Updated**: 2025-05-07T14:52:02Z

**Summary**: Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines.

**Link**: [arxiv](http://arxiv.org/abs/2505.04481v1),  [pdf](http://arxiv.org/pdf/2505.04481v1)

**Tags**: cs.CV 



### TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven   Evolution
**Authors**: Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park

**Updated**: 2025-05-07T14:51:43Z

**Summary**: Trajectory prediction is a crucial task in modeling human behavior, especially in fields as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy, while recently proposed deep learning approaches suffer from computational cost, lack of explainability, and generalization issues that limit their practical adoption. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We introduce a Cross-Generation Elite Sampling to promote population diversity and a Statistics Feedback Loop allowing the LLM to analyze alternative predictions. Our evaluations show TrajEvo outperforms previous heuristic methods on the ETH-UCY datasets, and remarkably outperforms both heuristics and deep learning methods when generalizing to the unseen SDD dataset. TrajEvo represents a first step toward automated design of fast, explainable, and generalizable trajectory prediction heuristics. We make our source code publicly available to foster future research at https://github.com/ai4co/trajevo.

**Link**: [arxiv](http://arxiv.org/abs/2505.04480v1),  [pdf](http://arxiv.org/pdf/2505.04480v1)

**Tags**: cs.AI cs.NE cs.RO 



### XrayGPT: Chest Radiographs Summarization using Medical Vision-Language   Models
**Authors**: Omkar Thawakar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, Fahad Shahbaz Khan

**Updated**: 2025-05-07T14:26:09Z

**Summary**: The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/XrayGPT.

**Link**: [arxiv](http://arxiv.org/abs/2306.07971v2),  [pdf](http://arxiv.org/pdf/2306.07971v2)

**Tags**: cs.CV 



### Meta-Learning Driven Lightweight Phase Shift Compression for   IRS-Assisted Wireless Systems
**Authors**: Xianhua Yu, Dong Li, Bowen Gu, Xiaoye Jing, Wen Wu, Tuo Wu, Kan Yu

**Updated**: 2025-05-07T14:25:47Z

**Summary**: The phase shift information (PSI) overhead poses a critical challenge to enabling real-time intelligent reflecting surface (IRS)-assisted wireless systems, particularly under dynamic and resource-constrained conditions. In this paper, we propose a lightweight PSI compression framework, termed meta-learning-driven compression and reconstruction network (MCRNet). By leveraging a few-shot adaptation strategy via model-agnostic meta-learning (MAML), MCRNet enables rapid generalization across diverse IRS configurations with minimal retraining overhead. Furthermore, a novel depthwise convolutional gating (DWCG) module is incorporated into the decoder to achieve adaptive local feature modulation with low computational cost, significantly improving decoding efficiency. Extensive simulations demonstrate that MCRNet achieves competitive normalized mean square error performance compared to state-of-the-art baselines across various compression ratios, while substantially reducing model size and inference latency. These results validate the effectiveness of the proposed asymmetric architecture and highlight the practical scalability and real-time applicability of MCRNet for dynamic IRS-assisted wireless deployments.

**Link**: [arxiv](http://arxiv.org/abs/2505.04453v1),  [pdf](http://arxiv.org/pdf/2505.04453v1)

**Tags**: eess.SP 



### M2Rec: Multi-scale Mamba for Efficient Sequential Recommendation
**Authors**: Qianru Zhang, Liang Qu, Honggang Wen, Dong Huang, Siu-Ming Yiu, Nguyen Quoc Viet Hung, Hongzhi Yin

**Updated**: 2025-05-07T14:14:29Z

**Summary**: Sequential recommendation systems aim to predict users' next preferences based on their interaction histories, but existing approaches face critical limitations in efficiency and multi-scale pattern recognition. While Transformer-based methods struggle with quadratic computational complexity, recent Mamba-based models improve efficiency but fail to capture periodic user behaviors, leverage rich semantic information, or effectively fuse multimodal features. To address these challenges, we propose \model, a novel sequential recommendation framework that integrates multi-scale Mamba with Fourier analysis, Large Language Models (LLMs), and adaptive gating. First, we enhance Mamba with Fast Fourier Transform (FFT) to explicitly model periodic patterns in the frequency domain, separating meaningful trends from noise. Second, we incorporate LLM-based text embeddings to enrich sparse interaction data with semantic context from item descriptions. Finally, we introduce a learnable gate mechanism to dynamically balance temporal (Mamba), frequency (FFT), and semantic (LLM) features, ensuring harmonious multimodal fusion. Extensive experiments demonstrate that \model\ achieves state-of-the-art performance, improving Hit Rate@10 by 3.2\% over existing Mamba-based models while maintaining 20\% faster inference than Transformer baselines. Our results highlight the effectiveness of combining frequency analysis, semantic understanding, and adaptive fusion for sequential recommendation. Code and datasets are available at: https://anonymous.4open.science/r/M2Rec.

**Link**: [arxiv](http://arxiv.org/abs/2505.04445v1),  [pdf](http://arxiv.org/pdf/2505.04445v1)

**Tags**: cs.IR 



### Towards Effectively Leveraging Execution Traces for Program Repair with   Code LLMs
**Authors**: Mirazul Haque, Petr Babkin, Farima Farmahinifarahani, Manuela Veloso

**Updated**: 2025-05-07T14:12:41Z

**Summary**: Large Language Models (LLMs) show promising performance on various programming tasks, including Automatic Program Repair (APR). However, most approaches to LLM-based APR are limited to the static analysis of the programs, while disregarding their runtime behavior. Inspired by knowledge-augmented NLP, in this work, we aim to remedy this potential blind spot by augmenting standard APR prompts with program execution traces. We evaluate our approach using the GPT family of models on three popular APR datasets. Our findings suggest that simply incorporating execution traces into the prompt provides a limited performance improvement over trace-free baselines, in only 2 out of 6 tested dataset / model configurations. We further find that the effectiveness of execution traces for APR diminishes as their complexity increases. We explore several strategies for leveraging traces in prompts and demonstrate that LLM-optimized prompts help outperform trace-free prompts more consistently. Additionally, we show trace-based prompting to be superior to finetuning a smaller LLM on a small-scale dataset; and conduct probing studies reinforcing the notion that execution traces can complement the reasoning abilities of the LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2505.04441v1),  [pdf](http://arxiv.org/pdf/2505.04441v1)

**Tags**: cs.LG 



### OBLIVIATE: Robust and Practical Machine Unlearning for Large Language   Models
**Authors**: Xiaoyu Xu, Minxin Du, Qingqing Ye, Haibo Hu

**Updated**: 2025-05-07T13:51:42Z

**Summary**: Large language models (LLMs) trained over extensive corpora risk memorizing sensitive, copyrighted, or toxic content. To address this, we propose OBLIVIATE, a robust unlearning framework that removes targeted data while preserving model utility. The framework follows a structured process: extracting target tokens, building retain sets, and fine-tuning with a tailored loss function comprising three components -- masking, distillation, and world fact. Using low-rank adapters (LoRA), it ensures efficiency without compromising unlearning quality. We conduct experiments on multiple datasets, including the Harry Potter series, WMDP, and TOFU, using a comprehensive suite of metrics: forget quality (new document-level memorization score), model utility, and fluency. Results demonstrate its effectiveness in resisting membership inference attacks, minimizing the impact on retained data, and maintaining robustness across diverse scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.04416v1),  [pdf](http://arxiv.org/pdf/2505.04416v1)

**Tags**: cs.CL cs.AI cs.CR cs.LG 



### YABLoCo: Yet Another Benchmark for Long Context Code Generation
**Authors**: Aidar Valeev, Roman Garaev, Vadim Lomshakov, Irina Piontkovskaya, Vladimir Ivanov, Israel Adewuyi

**Updated**: 2025-05-07T13:42:23Z

**Summary**: Large Language Models demonstrate the ability to solve various programming tasks, including code generation. Typically, the performance of LLMs is measured on benchmarks with small or medium-sized context windows of thousands of lines of code. At the same time, in real-world software projects, repositories can span up to millions of LoC. This paper closes this gap by contributing to the long context code generation benchmark (YABLoCo). The benchmark featured a test set of 215 functions selected from four large repositories with thousands of functions. The dataset contained metadata of functions, contexts of the functions with different levels of dependencies, docstrings, functions bodies, and call graphs for each repository. This paper presents three key aspects of the contribution. First, the benchmark aims at function body generation in large repositories in C and C++, two languages not covered by previous benchmarks. Second, the benchmark contains large repositories from 200K to 2,000K LoC. Third, we contribute a scalable evaluation pipeline for efficient computing of the target metrics and a tool for visual analysis of generated code. Overall, these three aspects allow for evaluating code generation in large repositories in C and C++.

**Link**: [arxiv](http://arxiv.org/abs/2505.04406v1),  [pdf](http://arxiv.org/pdf/2505.04406v1)

**Tags**: cs.CL cs.AI cs.SE 



### Large Means Left: Political Bias in Large Language Models Increases with   Their Number of Parameters
**Authors**: David Exler, Mark Schutera, Markus Reischl, Luca Rettenberger

**Updated**: 2025-05-07T13:18:41Z

**Summary**: With the increasing prevalence of artificial intelligence, careful evaluation of inherent biases needs to be conducted to form the basis for alleviating the effects these predispositions can have on users. Large language models (LLMs) are predominantly used by many as a primary source of information for various topics. LLMs frequently make factual errors, fabricate data (hallucinations), or present biases, exposing users to misinformation and influencing opinions. Educating users on their risks is key to responsible use, as bias, unlike hallucinations, cannot be caught through data verification. We quantify the political bias of popular LLMs in the context of the recent vote of the German Bundestag using the score produced by the Wahl-O-Mat. This metric measures the alignment between an individual's political views and the positions of German political parties. We compare the models' alignment scores to identify factors influencing their political preferences. Doing so, we discover a bias toward left-leaning parties, most dominant in larger LLMs. Also, we find that the language we use to communicate with the models affects their political views. Additionally, we analyze the influence of a model's origin and release date and compare the results to the outcome of the recent vote of the Bundestag. Our results imply that LLMs are prone to exhibiting political bias. Large corporations with the necessary means to develop LLMs, thus, knowingly or unknowingly, have a responsibility to contain these biases, as they can influence each voter's decision-making process and inform public opinion in general and at scale.

**Link**: [arxiv](http://arxiv.org/abs/2505.04393v1),  [pdf](http://arxiv.org/pdf/2505.04393v1)

**Tags**: cs.CL 



### Estimating LLM Uncertainty with Logits
**Authors**: Huan Ma, Jingdong Chen, Joey Tianyi Zhou, Guangyu Wang, Changqing Zhang

**Updated**: 2025-05-07T13:13:41Z

**Summary**: Over the past few years, Large Language Models (LLMs) have developed rapidly and are widely applied in various domains. However, LLMs face the issue of hallucinations, generating responses that may be unreliable when the models lack relevant knowledge. To be aware of potential hallucinations, uncertainty estimation methods have been introduced, and most of them have confirmed that reliability lies in critical tokens. However, probability-based methods perform poorly in identifying token reliability, limiting their practical utility. In this paper, we reveal that the probability-based method fails to estimate token reliability due to the loss of evidence strength information which is accumulated in the training stage. Therefore, we present Logits-induced token uncertainty (LogTokU), a framework for estimating decoupled token uncertainty in LLMs, enabling real-time uncertainty estimation without requiring multiple sampling processes. We employ evidence modeling to implement LogTokU and use the estimated uncertainty to guide downstream tasks. The experimental results demonstrate that LogTokU has significant effectiveness and promise.

**Link**: [arxiv](http://arxiv.org/abs/2502.00290v4),  [pdf](http://arxiv.org/pdf/2502.00290v4)

**Tags**: cs.CL cs.AI 



### The Aloe Family Recipe for Open and Specialized Healthcare LLMs
**Authors**: Dario Garcia-Gasulla, Jordi Bayarri-Planas, Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Adrian Tormos, Daniel Hinjos, Pablo Bernabeu-Perez, Anna Arias-Duart, Pablo Agustin Martin-Torres, Marta Gonzalez-Mallo, Sergio Alvarez-Napagao, Eduard Ayguadé-Parra, Ulises Cortés

**Updated**: 2025-05-07T13:13:14Z

**Summary**: Purpose: With advancements in Large Language Models (LLMs) for healthcare, the need arises for competitive open-source models to protect the public interest. This work contributes to the field of open medical LLMs by optimizing key stages of data preprocessing and training, while showing how to improve model safety (through DPO) and efficacy (through RAG). The evaluation methodology used, which includes four different types of tests, defines a new standard for the field. The resultant models, shown to be competitive with the best private alternatives, are released with a permisive license.   Methods: Building on top of strong base models like Llama 3.1 and Qwen 2.5, Aloe Beta uses a custom dataset to enhance public data with synthetic Chain of Thought examples. The models undergo alignment with Direct Preference Optimization, emphasizing ethical and policy-aligned performance in the presence of jailbreaking attacks. Evaluation includes close-ended, open-ended, safety and human assessments, to maximize the reliability of results.   Results: Recommendations are made across the entire pipeline, backed by the solid performance of the Aloe Family. These models deliver competitive performance across healthcare benchmarks and medical fields, and are often preferred by healthcare professionals. On bias and toxicity, the Aloe Beta models significantly improve safety, showing resilience to unseen jailbreaking attacks. For a responsible release, a detailed risk assessment specific to healthcare is attached to the Aloe Family models.   Conclusion: The Aloe Beta models, and the recipe that leads to them, are a significant contribution to the open-source medical LLM field, offering top-of-the-line performance while maintaining high ethical requirements. This work sets a new standard for developing and reporting aligned LLMs in healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2505.04388v1),  [pdf](http://arxiv.org/pdf/2505.04388v1)

**Tags**: cs.CL cs.AI 



### Consensus-Aware AV Behavior: Trade-offs Between Safety, Interaction, and   Performance in Mixed Urban Traffic
**Authors**: Mohammad Elayan, Wissam Kontar

**Updated**: 2025-05-07T12:59:59Z

**Summary**: Transportation systems have long been shaped by complexity and heterogeneity, driven by the interdependency of agent actions and traffic outcomes. The deployment of automated vehicles (AVs) in such systems introduces a new challenge: achieving consensus across safety, interaction quality, and traffic performance. In this work, we position consensus as a fundamental property of the traffic system and aim to quantify it. We use high-resolution trajectory data from the Third Generation Simulation (TGSIM) dataset to empirically analyze AV and human-driven vehicle (HDV) behavior at a signalized urban intersection and around vulnerable road users (VRUs). Key metrics, including Time-to-Collision (TTC), Post-Encroachment Time (PET), deceleration patterns, headways, and string stability, are evaluated across the three performance dimensions. Results show that full consensus across safety, interaction, and performance is rare, with only 1.63% of AV-VRU interaction frames meeting all three conditions. These findings highlight the need for AV models that explicitly balance multi-dimensional performance in mixed-traffic environments. Full reproducibility is supported via our open-source codebase on https://github.com/wissamkontar/Consensus-AV-Analysis.

**Link**: [arxiv](http://arxiv.org/abs/2505.04379v1),  [pdf](http://arxiv.org/pdf/2505.04379v1)

**Tags**: cs.MA cs.AI cs.SY eess.SY 



### Breaking the Lens of the Telescope: Online Relevance Estimation over   Large Retrieval Sets
**Authors**: Mandeep Rathee, V Venktesh, Sean MacAvaney, Avishek Anand

**Updated**: 2025-05-07T12:46:32Z

**Summary**: Advanced relevance models, such as those that use large language models (LLMs), provide highly accurate relevance estimations. However, their computational costs make them infeasible for processing large document corpora. To address this, retrieval systems often employ a telescoping approach, where computationally efficient but less precise lexical and semantic retrievers filter potential candidates for further ranking. However, this approach heavily depends on the quality of early-stage retrieval, which can potentially exclude relevant documents early in the process. In this work, we propose a novel paradigm for re-ranking called online relevance estimation that continuously updates relevance estimates for a query throughout the ranking process. Instead of re-ranking a fixed set of top-k documents in a single step, online relevance estimation iteratively re-scores smaller subsets of the most promising documents while adjusting relevance scores for the remaining pool based on the estimations from the final model using an online bandit-based algorithm. This dynamic process mitigates the recall limitations of telescoping systems by re-prioritizing documents initially deemed less relevant by earlier stages -- including those completely excluded by earlier-stage retrievers. We validate our approach on TREC benchmarks under two scenarios: hybrid retrieval and adaptive retrieval. Experimental results demonstrate that our method is sample-efficient and significantly improves recall, highlighting the effectiveness of our online relevance estimation framework for modern search systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.09353v2),  [pdf](http://arxiv.org/pdf/2504.09353v2)

**Tags**: cs.IR 



### Playing repeated games with Large Language Models
**Authors**: Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, Eric Schulz

**Updated**: 2025-05-07T12:44:45Z

**Summary**: LLMs are increasingly used in applications where they interact with humans and other agents. We propose to use behavioural game theory to study LLM's cooperation and coordination behaviour. We let different LLMs play finitely repeated $2\times2$ games with each other, with human-like strategies, and actual human players. Our results show that LLMs perform particularly well at self-interested games like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination, like the Battle of the Sexes. We verify that these behavioural signatures are stable across robustness checks. We additionally show how GPT-4's behaviour can be modulated by providing additional information about its opponent and by using a "social chain-of-thought" (SCoT) strategy. This also leads to better scores and more successful coordination when interacting with human players. These results enrich our understanding of LLM's social behaviour and pave the way for a behavioural game theory for machines.

**Link**: [arxiv](http://arxiv.org/abs/2305.16867v2),  [pdf](http://arxiv.org/pdf/2305.16867v2)

**Tags**: cs.CL 



### LLM-based Bi-level Multi-interest Learning Framework for Sequential   Recommendation
**Authors**: Shutong Qiao, Chen Gao, Wei Yuan, Yong Li, Hongzhi Yin

**Updated**: 2025-05-07T12:33:27Z

**Summary**: Sequential recommendation (SR) leverages users' dynamic preferences, with recent advances incorporating multi-interest learning to model diverse user interests. However, most multi-interest SR models rely on noisy, sparse implicit feedback, limiting recommendation accuracy. Large language models (LLMs) offer robust reasoning on low-quality data but face high computational costs and latency challenges for SR integration. We propose a novel LLM-based multi-interest SR framework combining implicit behavioral and explicit semantic perspectives. It includes two modules: the Implicit Behavioral Interest Module (IBIM), which learns from user behavior using a traditional SR model, and the Explicit Semantic Interest Module (ESIM), which uses clustering and prompt-engineered LLMs to extract semantic multi-interest representations from informative samples. Semantic insights from ESIM enhance IBIM's behavioral representations via modality alignment and semantic prediction tasks. During inference, only IBIM is used, ensuring efficient, LLM-free recommendations. Experiments on four real-world datasets validate the framework's effectiveness and practicality.

**Link**: [arxiv](http://arxiv.org/abs/2411.09410v3),  [pdf](http://arxiv.org/pdf/2411.09410v3)

**Tags**: cs.IR 



### Benchmarking LLMs' Swarm intelligence
**Authors**: Kai Ruan, Mowen Huang, Ji-Rong Wen, Hao Sun

**Updated**: 2025-05-07T12:32:01Z

**Summary**: Large Language Models (LLMs) show potential for complex reasoning, yet their capacity for emergent coordination in Multi-Agent Systems (MAS) when operating under strict constraints-such as limited local perception and communication, characteristic of natural swarms-remains largely unexplored, particularly concerning the nuances of swarm intelligence. Existing benchmarks often do not fully capture the unique challenges of decentralized coordination that arise when agents operate with incomplete spatio-temporal information. To bridge this gap, we introduce SwarmBench, a novel benchmark designed to systematically evaluate the swarm intelligence capabilities of LLMs acting as decentralized agents. SwarmBench features five foundational MAS coordination tasks within a configurable 2D grid environment, forcing agents to rely primarily on local sensory input (k x k view) and local communication. We propose metrics for coordination effectiveness and analyze emergent group dynamics. Evaluating several leading LLMs in a zero-shot setting, we find significant performance variations across tasks, highlighting the difficulties posed by local information constraints. While some coordination emerges, results indicate limitations in robust planning and strategy formation under uncertainty in these decentralized scenarios. Assessing LLMs under swarm-like conditions is crucial for realizing their potential in future decentralized systems. We release SwarmBench as an open, extensible toolkit-built upon a customizable and scalable physical system with defined mechanical properties. It provides environments, prompts, evaluation scripts, and the comprehensive experimental datasets generated, aiming to foster reproducible research into LLM-based MAS coordination and the theoretical underpinnings of Embodied MAS. Our code repository is available at https://github.com/x66ccff/swarmbench.

**Link**: [arxiv](http://arxiv.org/abs/2505.04364v1),  [pdf](http://arxiv.org/pdf/2505.04364v1)

**Tags**: cs.MA cs.CL 



### EcoWeedNet: A Lightweight and Automated Weed Detection Method for   Sustainable Next-Generation Agricultural Consumer Electronics
**Authors**: Omar H. Khater, Abdul Jabbar Siddiqui, M. Shamim Hossain, Aiman El-Maleh

**Updated**: 2025-05-07T11:40:20Z

**Summary**: Sustainable agriculture plays a crucial role in ensuring world food security for consumers. A critical challenge faced by sustainable precision agriculture is weed growth, as weeds compete for essential resources with crops, such as water, soil nutrients, and sunlight, which notably affect crop yields. The adoption of automated computer vision technologies and ground agricultural consumer electronic vehicles in precision agriculture offers sustainable, low-carbon solutions. However, prior works suffer from issues such as low accuracy and precision, as well as high computational expense. This work proposes EcoWeedNet, a novel model that enhances weed detection performance without introducing significant computational complexity, aligning with the goals of low-carbon agricultural practices. The effectiveness of the proposed model is demonstrated through comprehensive experiments on the CottonWeedDet12 benchmark dataset, which reflects real-world scenarios. EcoWeedNet achieves performance comparable to that of large models (mAP@0.5 = 95.2%), yet with significantly fewer parameters (approximately 4.21% of the parameters of YOLOv4), lower computational complexity and better computational efficiency 6.59% of the GFLOPs of YOLOv4). These key findings indicate EcoWeedNet's deployability on low-power consumer hardware, lower energy consumption, and hence reduced carbon footprint, thereby emphasizing the application prospects of EcoWeedNet in next-generation sustainable agriculture. These findings provide the way forward for increased application of environmentally-friendly agricultural consumer technologies.

**Link**: [arxiv](http://arxiv.org/abs/2502.00205v2),  [pdf](http://arxiv.org/pdf/2502.00205v2)

**Tags**: cs.CV cs.AI 



### A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text   Classification
**Authors**: Junichiro Niimi

**Updated**: 2025-05-07T11:31:37Z

**Summary**: With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%.

**Link**: [arxiv](http://arxiv.org/abs/2504.18884v2),  [pdf](http://arxiv.org/pdf/2504.18884v2)

**Tags**: cs.CL cs.AI 



### DGAR: A Unified Domain Generalization Framework for RF-Enabled Human   Activity Recognition
**Authors**: Junshuo Liu, Xin Shi, Robert C. Qiu

**Updated**: 2025-05-07T11:25:03Z

**Summary**: Radio-frequency (RF)-based human activity recognition (HAR) provides a contactless and privacy-preserving solution for monitoring human behavior in applications such as personalized healthcare, ambient assisted living, and telemedicine. However, real-world deployment is frequently challenged by domain shifts arising from inter-subject variability, heterogeneous physical environments, and unseen activity patterns, resulting in significant performance degradation. To address this issue, we propose DGAR, a domain-generalized activity recognition framework that learns transferable representations without access to target-domain data. DGAR integrates instance-adaptive feature modulation with cross-domain distribution alignment to enhance both personalization and generalization. Specifically, it incorporates a squeeze-and-excitation (SE) block to extract salient spatiotemporal features and employs correlation alignment to mitigate inter-domain discrepancies. Extensive experiments on three public RF-based datasets -- HUST-HAR, Lab-LFM, and Office-LFM -- demonstrate that DGAR consistently outperforms state-of-the-art baselines, achieving up to a 5.81% improvement in weighted F1-score. These results underscore DGAR's potential to enable generalizable, real-time RF sensing systems in dynamic and personalized healthcare scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2503.17667v2),  [pdf](http://arxiv.org/pdf/2503.17667v2)

**Tags**: eess.SP 



### DA-Mamba: Domain Adaptive Hybrid Mamba-Transformer Based One-Stage   Object Detection
**Authors**: A. Enes Doruk, Hasan F. Ates

**Updated**: 2025-05-07T10:18:31Z

**Summary**: Recent 2D CNN-based domain adaptation approaches struggle with long-range dependencies due to limited receptive fields, making it difficult to adapt to target domains with significant spatial distribution changes. While transformer-based domain adaptation methods better capture distant relationships through self-attention mechanisms that facilitate more effective cross-domain feature alignment, their quadratic computational complexity makes practical deployment challenging for object detection tasks across diverse domains. Inspired by the global modeling and linear computation complexity of the Mamba architecture, we present the first domain-adaptive Mamba-based one-stage object detection model, termed DA-Mamba. Specifically, we combine Mamba's efficient state-space modeling with attention mechanisms to address domain-specific spatial and channel-wise variations. Our design leverages domain-adaptive spatial and channel-wise scanning within the Mamba block to extract highly transferable representations for efficient sequential processing, while cross-attention modules generate long-range, mixed-domain spatial features to enable robust soft alignment across domains. Besides, motivated by the observation that hybrid architectures introduce feature noise in domain adaptation tasks, we propose an entropy-based knowledge distillation framework with margin ReLU, which adaptively refines multi-level representations by suppressing irrelevant activations and aligning uncertainty across source and target domains. Finally, to prevent overfitting caused by the mixed-up features generated through cross-attention mechanisms, we propose entropy-driven gating attention with random perturbations that simultaneously refine target features and enhance model generalization.

**Link**: [arxiv](http://arxiv.org/abs/2502.11178v2),  [pdf](http://arxiv.org/pdf/2502.11178v2)

**Tags**: cs.CV 



### FAST: Federated Active Learning with Foundation Models for   Communication-efficient Sampling and Training
**Authors**: Haoyuan Li, Mathias Funk, Jindong Wang, Aaqib Saeed

**Updated**: 2025-05-07T10:09:56Z

**Summary**: Federated Active Learning (FAL) has emerged as a promising framework to leverage large quantities of unlabeled data across distributed clients while preserving data privacy. However, real-world deployments remain limited by high annotation costs and communication-intensive sampling processes, particularly in a cross-silo setting, when clients possess substantial local datasets. This paper addresses the crucial question: What is the best practice to reduce communication costs in human-in-the-loop learning with minimal annotator effort? Existing FAL methods typically rely on iterative annotation processes that separate active sampling from federated updates, leading to multiple rounds of expensive communication and annotation. In response, we introduce FAST, a two-pass FAL framework that harnesses foundation models for weak labeling in a preliminary pass, followed by a refinement pass focused exclusively on the most uncertain samples. By leveraging representation knowledge from foundation models and integrating refinement steps into a streamlined workflow, FAST substantially reduces the overhead incurred by iterative active sampling. Extensive experiments on diverse medical and natural image benchmarks demonstrate that FAST outperforms existing FAL methods by an average of 4.36% while reducing communication rounds eightfold under a limited 5% labeling budget.

**Link**: [arxiv](http://arxiv.org/abs/2504.03783v3),  [pdf](http://arxiv.org/pdf/2504.03783v3)

**Tags**: cs.LG cs.AI cs.CV cs.DC 



### HM-DF SNN: Transcending Conventional Online Learning with Advanced   Training and Deployment
**Authors**: Zecheng Hao, Yifan Huang, Zijie Xu, Wenxuan Liu, Yuanhong Tang, Zhaofei Yu, Tiejun Huang

**Updated**: 2025-05-07T10:08:15Z

**Summary**: Spiking Neural Networks (SNNs) are considered to have enormous potential in the future development of Artificial Intelligence due to their brain-inspired and energy-efficient properties. Compared to vanilla Spatial-Temporal Back-propagation (STBP) training methods, online training can effectively overcome the risk of GPU memory explosion. However, current online learning framework cannot tackle the inseparability problem of temporal dependent gradients and merely aim to optimize the training memory, resulting in no performance advantages compared to the STBP training models in the inference phase. To address the aforementioned challenges, we propose Hybrid Mechanism-Driven Firing (HM-DF) model, which is a family of advanced models that respectively adopt different spiking calculation schemes in the upper-region and lower-region of the firing threshold. We point out that HM-DF model can effectively separate temporal gradients and tackle the mismatch problem of surrogate gradients, as well as achieving full-stage optimization towards computation speed and memory footprint. Experimental results have demonstrated that HM-DF model can be flexibly combined with various techniques to achieve state-of-the-art performance in the field of online learning, without triggering further power consumption.

**Link**: [arxiv](http://arxiv.org/abs/2410.07547v2),  [pdf](http://arxiv.org/pdf/2410.07547v2)

**Tags**: cs.NE cs.AI 



### Massive MIMO: Instantaneous versus Statistical CSI-Based Power   Allocation
**Authors**: Zahra Mobini, Hien Quoc Ngo

**Updated**: 2025-05-07T10:05:23Z

**Summary**: The deployment of instantaneous CSI-based power control schemes necessitates computationally intensive signal processing operations, requiring substantial resources to handle real-time CSI updates and the associated overhead. Conversely, statistical CSIbased schemes enable efficient implementation of advanced power allocation algorithms within large-scale massive MIMO (mMIMO) systems, where the algorithms are updated much less frequently. Nevertheless, these schemes may deviate from optimal results in certain practical mMIMO configurations, necessitating the adoption of instantaneous CSI-based schemes. In addition, they may be limited in practical implementation where instantaneous CSI-based resource allocation and management schemes are widely adopted. This lecture provides a comprehensive comparison between the statistical CSI-based power allocation and instantaneous CSI-based power allocation designs for mMIMO systems from performance, complexity, and practical implementation aspects.

**Link**: [arxiv](http://arxiv.org/abs/2505.04294v1),  [pdf](http://arxiv.org/pdf/2505.04294v1)

**Tags**: cs.IT math.IT 



### Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating   the Hallucination for Path Planning
**Authors**: Hourui Deng, Hongjie Zhang, Jie Ou, Chaosheng Feng

**Updated**: 2025-05-07T10:00:50Z

**Summary**: Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied intelligence. However, even in simple maze environments, LLMs still encounter challenges in long-term path-planning, primarily influenced by their spatial hallucination and context inconsistency hallucination by long-term reasoning. To address this challenge, this study proposes an innovative model, Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs, we propose the Spatial-to-Relational approach, which transforms spatial prompts into entity relations and paths representing entity relation chains. This approach fully taps the potential of LLMs in terms of sequential thinking. As a result, we design a path-planning algorithm based on Q-learning to mitigate the context inconsistency hallucination, which enhances the reasoning ability of LLMs. Using the Q-value of state-action as auxiliary information for prompts, we correct the hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally, we propose a reverse curriculum learning technique based on LLMs to further mitigate the context inconsistency hallucination. LLMs can rapidly accumulate successful experiences by reducing task difficulty and leveraging them to tackle more complex tasks. We performed comprehensive experiments based on Baidu's self-developed LLM: ERNIE-Bot 4.0. The results showed that our S2RCQL achieved a 23%--40% improvement in both success and optimality rates compared with advanced prompt engineering.

**Link**: [arxiv](http://arxiv.org/abs/2408.13184v3),  [pdf](http://arxiv.org/pdf/2408.13184v3)

**Tags**: cs.CL 



### From Incidents to Insights: Patterns of Responsibility following AI   Harms
**Authors**: Isabel Richards, Claire Benn, Miri Zilka

**Updated**: 2025-05-07T09:59:36Z

**Summary**: The AI Incident Database was inspired by aviation safety databases, which enable collective learning from failures to prevent future incidents. The database documents hundreds of AI failures, collected from the news and media. However, criticism highlights that the AIID's reliance on media reporting limits its utility for learning about implementation failures. In this paper, we accept that the AIID falls short in its original mission, but argue that by looking beyond technically-focused learning, the dataset can provide new, highly valuable insights: specifically, opportunities to learn about patterns between developers, deployers, victims, wider society, and law-makers that emerge after AI failures. Through a three-tier mixed-methods analysis of 962 incidents and 4,743 related reports from the AIID, we examine patterns across incidents, focusing on cases with public responses tagged in the database. We identify 'typical' incidents found in the AIID, from Tesla crashes to deepfake scams.   Focusing on this interplay between relevant parties, we uncover patterns in accountability and social expectations of responsibility. We find that the presence of identifiable responsible parties does not necessarily lead to increased accountability. The likelihood of a response and what it amounts to depends highly on context, including who built the technology, who was harmed, and to what extent. Controversy-rich incidents provide valuable data about societal reactions, including insights into social expectations. Equally informative are cases where controversy is notably absent. This work shows that the AIID's value lies not just in preventing technical failures, but in documenting patterns of harms and of institutional response and social learning around AI incidents. These patterns offer crucial insights for understanding how society adapts to and governs emerging AI technologies.

**Link**: [arxiv](http://arxiv.org/abs/2505.04291v1),  [pdf](http://arxiv.org/pdf/2505.04291v1)

**Tags**: cs.CY 



### GASCADE: Grouped Summarization of Adverse Drug Event for Enhanced Cancer   Pharmacovigilance
**Authors**: Sofia Jamil, Aryan Dabad, Bollampalli Areen Reddy, Sriparna Saha, Rajiv Misra, Adil A. Shakur

**Updated**: 2025-05-07T09:40:18Z

**Summary**: In the realm of cancer treatment, summarizing adverse drug events (ADEs) reported by patients using prescribed drugs is crucial for enhancing pharmacovigilance practices and improving drug-related decision-making. While the volume and complexity of pharmacovigilance data have increased, existing research in this field has predominantly focused on general diseases rather than specifically addressing cancer. This work introduces the task of grouped summarization of adverse drug events reported by multiple patients using the same drug for cancer treatment. To address the challenge of limited resources in cancer pharmacovigilance, we present the MultiLabeled Cancer Adverse Drug Reaction and Summarization (MCADRS) dataset. This dataset includes pharmacovigilance posts detailing patient concerns regarding drug efficacy and adverse effects, along with extracted labels for drug names, adverse drug events, severity, and adversity of reactions, as well as summaries of ADEs for each drug. Additionally, we propose the Grouping and Abstractive Summarization of Cancer Adverse Drug events (GASCADE) framework, a novel pipeline that combines the information extraction capabilities of Large Language Models (LLMs) with the summarization power of the encoder-decoder T5 model. Our work is the first to apply alignment techniques, including advanced algorithms like Direct Preference Optimization, to encoder-decoder models using synthetic datasets for summarization tasks. Through extensive experiments, we demonstrate the superior performance of GASCADE across various metrics, validated through both automated assessments and human evaluations. This multitasking approach enhances drug-related decision-making and fosters a deeper understanding of patient concerns, paving the way for advancements in personalized and responsive cancer care. The code and dataset used in this work are publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2505.04284v1),  [pdf](http://arxiv.org/pdf/2505.04284v1)

**Tags**: cs.CL cs.AI 



### CLEAR: Cue Learning using Evolution for Accurate Recognition Applied to   Sustainability Data Extraction
**Authors**: Peter J. Bentley, Soo Ling Lim, Fuyuki Ishikawa

**Updated**: 2025-05-07T09:39:42Z

**Summary**: Large Language Model (LLM) image recognition is a powerful tool for extracting data from images, but accuracy depends on providing sufficient cues in the prompt - requiring a domain expert for specialized tasks. We introduce Cue Learning using Evolution for Accurate Recognition (CLEAR), which uses a combination of LLMs and evolutionary computation to generate and optimize cues such that recognition of specialized features in images is improved. It achieves this by auto-generating a novel domain-specific representation and then using it to optimize suitable textual cues with a genetic algorithm. We apply CLEAR to the real-world task of identifying sustainability data from interior and exterior images of buildings. We investigate the effects of using a variable-length representation compared to fixed-length and show how LLM consistency can be improved by refactoring from categorical to real-valued estimates. We show that CLEAR enables higher accuracy compared to expert human recognition and human-authored prompts in every task with error rates improved by up to two orders of magnitude and an ablation study evincing solution concision.

**Link**: [arxiv](http://arxiv.org/abs/2501.18504v3),  [pdf](http://arxiv.org/pdf/2501.18504v3)

**Tags**: cs.CV cs.AI cs.NE 68W50, 68T07 G.1.6; I.2.10 



### TS-Diff: Two-Stage Diffusion Model for Low-Light RAW Image Enhancement
**Authors**: Yi Li, Zhiyuan Zhang, Jiangnan Xia, Jianghan Cheng, Qilong Wu, Junwei Li, Yibin Tian, Hui Kong

**Updated**: 2025-05-07T09:35:05Z

**Summary**: This paper presents a novel Two-Stage Diffusion Model (TS-Diff) for enhancing extremely low-light RAW images. In the pre-training stage, TS-Diff synthesizes noisy images by constructing multiple virtual cameras based on a noise space. Camera Feature Integration (CFI) modules are then designed to enable the model to learn generalizable features across diverse virtual cameras. During the aligning stage, CFIs are averaged to create a target-specific CFI$^T$, which is fine-tuned using a small amount of real RAW data to adapt to the noise characteristics of specific cameras. A structural reparameterization technique further simplifies CFI$^T$ for efficient deployment. To address color shifts during the diffusion process, a color corrector is introduced to ensure color consistency by dynamically adjusting global color distributions. Additionally, a novel dataset, QID, is constructed, featuring quantifiable illumination levels and a wide dynamic range, providing a comprehensive benchmark for training and evaluation under extreme low-light conditions. Experimental results demonstrate that TS-Diff achieves state-of-the-art performance on multiple datasets, including QID, SID, and ELD, excelling in denoising, generalization, and color consistency across various cameras and illumination levels. These findings highlight the robustness and versatility of TS-Diff, making it a practical solution for low-light imaging applications. Source codes and models are available at https://github.com/CircccleK/TS-Diff

**Link**: [arxiv](http://arxiv.org/abs/2505.04281v1),  [pdf](http://arxiv.org/pdf/2505.04281v1)

**Tags**: cs.CV 



### Test It Before You Trust It: Applying Software Testing for Trustworthy   In-context Learning
**Authors**: Teeradaj Racharak, Chaiyong Ragkhitwetsagul, Chommakorn Sontesadisai, Thanwadee Sunetnanta

**Updated**: 2025-05-07T09:29:45Z

**Summary**: In-context learning (ICL) has emerged as a powerful capability of large language models (LLMs), enabling them to perform new tasks based on a few provided examples without explicit fine-tuning. Despite their impressive adaptability, these models remain vulnerable to subtle adversarial perturbations and exhibit unpredictable behavior when faced with linguistic variations. Inspired by software testing principles, we introduce a software testing-inspired framework, called MMT4NL, for evaluating the trustworthiness of in-context learning by utilizing adversarial perturbations and software testing techniques. It includes diverse evaluation aspects of linguistic capabilities for testing the ICL capabilities of LLMs. MMT4NL is built around the idea of crafting metamorphic adversarial examples from a test set in order to quantify and pinpoint bugs in the designed prompts of ICL. Our philosophy is to treat any LLM as software and validate its functionalities just like testing the software. Finally, we demonstrate applications of MMT4NL on the sentiment analysis and question-answering tasks. Our experiments could reveal various linguistic bugs in state-of-the-art LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.18827v2),  [pdf](http://arxiv.org/pdf/2504.18827v2)

**Tags**: cs.SE cs.AI 



### Weaponizing Language Models for Cybersecurity Offensive Operations:   Automating Vulnerability Assessment Report Validation; A Review Paper
**Authors**: Abdulrahman S Almuhaidib, Azlan Mohd Zain, Zalmiyah Zakaria, Izyan Izzati Kamsani, Abdulaziz S Almuhaidib

**Updated**: 2025-05-07T09:14:55Z

**Summary**: This, with the ever-increasing sophistication of cyberwar, calls for novel solutions. In this regard, Large Language Models (LLMs) have emerged as a highly promising tool for defensive and offensive cybersecurity-related strategies. While existing literature has focused much on the defensive use of LLMs, when it comes to their offensive utilization, very little has been reported-namely, concerning Vulnerability Assessment (VA) report validation. Consequentially, this paper tries to fill that gap by investigating the capabilities of LLMs in automating and improving the validation process of the report of the VA. From the critical review of the related literature, this paper hereby proposes a new approach to using the LLMs in the automation of the analysis and within the validation process of the report of the VA that could potentially reduce the number of false positives and generally enhance efficiency. These results are promising for LLM automatization for improving validation on reports coming from VA in order to improve accuracy while reducing human effort and security postures. The contribution of this paper provides further evidence about the offensive and defensive LLM capabilities and therefor helps in devising more appropriate cybersecurity strategies and tools accordingly.

**Link**: [arxiv](http://arxiv.org/abs/2505.04265v1),  [pdf](http://arxiv.org/pdf/2505.04265v1)

**Tags**: cs.CR cs.AI 



### Steerable Chatbots: Personalizing LLMs with Preference-Based Activation   Steering
**Authors**: Jessica Y. Bo, Tianyu Xu, Ishan Chatterjee, Katrina Passarella-Ward, Achin Kulshrestha, D Shin

**Updated**: 2025-05-07T09:10:51Z

**Summary**: As large language models (LLMs) improve in their capacity to serve as personal AI assistants, their ability to output uniquely tailored, personalized responses that align with the soft preferences of their users is essential for enhancing user satisfaction and retention. However, untrained lay users have poor prompt specification abilities and often struggle with conveying their latent preferences to AI assistants. To address this, we leverage activation steering to guide LLMs to align with interpretable preference dimensions during inference. In contrast to memory-based personalization methods that require longer user history, steering is extremely lightweight and can be easily controlled by the user via an linear strength factor. We embed steering into three different interactive chatbot interfaces and conduct a within-subjects user study (n=14) to investigate how end users prefer to personalize their conversations. The results demonstrate the effectiveness of preference-based steering for aligning real-world conversations with hidden user preferences, and highlight further insights on how diverse values around control, usability, and transparency lead users to prefer different interfaces.

**Link**: [arxiv](http://arxiv.org/abs/2505.04260v1),  [pdf](http://arxiv.org/pdf/2505.04260v1)

**Tags**: cs.HC cs.AI 



### CompileAgent: Automated Real-World Repo-Level Compilation with   Tool-Integrated LLM-based Agent System
**Authors**: Li Hu, Guoqiang Chen, Xiuwei Shang, Shaoyin Cheng, Benlong Wu, Gangyang Li, Xu Zhu, Weiming Zhang, Nenghai Yu

**Updated**: 2025-05-07T08:59:14Z

**Summary**: With open-source projects growing in size and complexity, manual compilation becomes tedious and error-prone, highlighting the need for automation to improve efficiency and accuracy. However, the complexity of compilation instruction search and error resolution makes automatic compilation challenging. Inspired by the success of LLM-based agents in various fields, we propose CompileAgent, the first LLM-based agent framework dedicated to repo-level compilation. CompileAgent integrates five tools and a flow-based agent strategy, enabling interaction with software artifacts for compilation instruction search and error resolution. To measure the effectiveness of our method, we design a public repo-level benchmark CompileAgentBench, and we also design two baselines for comparison by combining two compilation-friendly schemes. The performance on this benchmark shows that our method significantly improves the compilation success rate, ranging from 10% to 71%. Meanwhile, we evaluate the performance of CompileAgent under different agent strategies and verify the effectiveness of the flow-based strategy. Additionally, we emphasize the scalability of CompileAgent, further expanding its application prospects.

**Link**: [arxiv](http://arxiv.org/abs/2505.04254v1),  [pdf](http://arxiv.org/pdf/2505.04254v1)

**Tags**: cs.SE 



### LLM-Independent Adaptive RAG: Let the Question Speak for Itself
**Authors**: Maria Marina, Nikolay Ivanov, Sergey Pletenev, Mikhail Salnikov, Daria Galimzianova, Nikita Krayko, Vasily Konovalov, Alexander Panchenko, Viktor Moskvoretskii

**Updated**: 2025-05-07T08:58:52Z

**Summary**: Large Language Models~(LLMs) are prone to hallucinations, and Retrieval-Augmented Generation (RAG) helps mitigate this, but at a high computational cost while risking misinformation. Adaptive retrieval aims to retrieve only when necessary, but existing approaches rely on LLM-based uncertainty estimation, which remain inefficient and impractical. In this study, we introduce lightweight LLM-independent adaptive retrieval methods based on external information. We investigated 27 features, organized into 7 groups, and their hybrid combinations. We evaluated these methods on 6 QA datasets, assessing the QA performance and efficiency. The results show that our approach matches the performance of complex LLM-based methods while achieving significant efficiency gains, demonstrating the potential of external information for adaptive retrieval.

**Link**: [arxiv](http://arxiv.org/abs/2505.04253v1),  [pdf](http://arxiv.org/pdf/2505.04253v1)

**Tags**: cs.CL cs.LG 



### Facilitating Trustworthy Human-Agent Collaboration in LLM-based   Multi-Agent System oriented Software Engineering
**Authors**: Krishna Ronanki

**Updated**: 2025-05-07T08:55:15Z

**Summary**: Multi-agent autonomous systems (MAS) are better at addressing challenges that spans across multiple domains than singular autonomous agents. This holds true within the field of software engineering (SE) as well. The state-of-the-art research on MAS within SE focuses on integrating LLMs at the core of autonomous agents to create LLM-based multi-agent autonomous (LMA) systems. However, the introduction of LMA systems into SE brings a plethora of challenges. One of the major challenges is the strategic allocation of tasks between humans and the LMA system in a trustworthy manner. To address this challenge, a RACI-based framework is proposed in this work in progress article, along with implementation guidelines and an example implementation of the framework. The proposed framework can facilitate efficient collaboration, ensure accountability, and mitigate potential risks associated with LLM-driven automation while aligning with the Trustworthy AI guidelines. The future steps for this work delineating the planned empirical validation method are also presented.

**Link**: [arxiv](http://arxiv.org/abs/2505.04251v1),  [pdf](http://arxiv.org/pdf/2505.04251v1)

**Tags**: cs.SE cs.AI cs.MA 



### To Judge or not to Judge: Using LLM Judgements for Advertiser Keyphrase   Relevance at eBay
**Authors**: Soumik Dey, Hansi Wu, Binbin Li

**Updated**: 2025-05-07T08:03:25Z

**Summary**: E-commerce sellers are recommended keyphrases based on their inventory on which they advertise to increase buyer engagement (clicks/sales). The relevance of advertiser keyphrases plays an important role in preventing the inundation of search systems with numerous irrelevant items that compete for attention in auctions, in addition to maintaining a healthy seller perception. In this work, we describe the shortcomings of training Advertiser keyphrase relevance filter models on click/sales/search relevance signals and the importance of aligning with human judgment, as sellers have the power to adopt or reject said keyphrase recommendations. In this study, we frame Advertiser keyphrase relevance as a complex interaction between 3 dynamical systems -- seller judgment, which influences seller adoption of our product, Advertising, which provides the keyphrases to bid on, and Search, who holds the auctions for the same keyphrases. This study discusses the practicalities of using human judgment via a case study at eBay Advertising and demonstrate that using LLM-as-a-judge en-masse as a scalable proxy for seller judgment to train our relevance models achieves a better harmony across the three systems -- provided that they are bound by a meticulous evaluation framework grounded in business metrics.

**Link**: [arxiv](http://arxiv.org/abs/2505.04209v1),  [pdf](http://arxiv.org/pdf/2505.04209v1)

**Tags**: cs.IR cs.AI cs.LG 



### InfiniteHBD: Building Datacenter-Scale High-Bandwidth Domain for LLM   with Optical Circuit Switching Transceivers
**Authors**: Chenchen Shou, Guyue Liu, Hao Nie, Huaiyu Meng, Yu Zhou, Yimin Jiang, Wenqing Lv, Yelong Xu, Yuanwei Lu, Zhang Chen, Yanbo Yu, Yichen Shen, Yibo Zhu, Daxin Jiang

**Updated**: 2025-05-07T08:02:44Z

**Summary**: Scaling Large Language Model (LLM) training relies on multi-dimensional parallelism, where High-Bandwidth Domains (HBDs) are critical for communication-intensive parallelism like Tensor Parallelism (TP) and Expert Parallelism (EP). However, existing HBD architectures face fundamental limitations in scalability, cost, and fault resiliency: switch-centric HBDs (e.g., NVL-72) incur prohibitive scaling costs, while GPU-centric HBDs (e.g., TPUv3/Dojo) suffer from severe fault propagation. Switch-GPU hybrid HBDs such as TPUv4 takes a middle-ground approach by leveraging Optical Circuit Switches, but the fault explosion radius remains large at the cube level (e.g., 64 TPUs).   We propose InfiniteHBD, a novel transceiver-centric HBD architecture that unifies connectivity and dynamic switching at the transceiver level using Optical Circuit Switching (OCS). By embedding OCS within each transceiver, InfiniteHBD achieves reconfigurable point-to-multipoint connectivity, allowing the topology to adapt into variable-size rings. This design provides: i) datacenter-wide scalability without cost explosion; ii) fault resilience by isolating failures to a single node, and iii) full bandwidth utilization for fault-free GPUs. Key innovations include a Silicon Photonic (SiPh) based low-cost OCS transceiver (OCSTrx), a reconfigurable k-hop ring topology co-designed with intra-/inter-node communication, and an HBD-DCN orchestration algorithm maximizing GPU utilization while minimizing cross-ToR datacenter network traffic. The evaluation demonstrates that InfiniteHBD achieves 31% of the cost of NVL-72, near-zero GPU waste ratio (over one order of magnitude lower than NVL-72 and TPUv4), near-zero cross-ToR traffic when node fault ratios under 7%, and improves Model FLOPs Utilization by 3.37x compared to NVIDIA DGX (8 GPUs per Node).

**Link**: [arxiv](http://arxiv.org/abs/2502.03885v3),  [pdf](http://arxiv.org/pdf/2502.03885v3)

**Tags**: cs.NI cs.DC cs.LG 



### Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer   Gate
**Authors**: Zhiyuan Fang, Zicong Hong, Yuegui Huang, Yufeng Lyu, Wuhui Chen, Yue Yu, Fan Yu, Zibin Zheng

**Updated**: 2025-05-07T07:57:21Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive performance across various tasks, and their application in edge scenarios has attracted significant attention. However, sparse-activated Mixture-of-Experts (MoE) models, which are well suited for edge scenarios, have received relatively little attention due to their high memory demands. Offload-based methods have been proposed to address this challenge, but they face difficulties with expert prediction. Inaccurate expert predictions can result in prolonged inference delays. To promote the application of MoE models in edge scenarios, we propose Fate, an offloading system designed for MoE models to enable efficient inference in resource-constrained environments. The key insight behind Fate is that gate inputs from adjacent layers can be effectively used for expert prefetching, achieving high prediction accuracy without additional GPU overhead. Furthermore, Fate employs a shallow-favoring expert caching strategy that increases the expert hit rate to 99\%. Additionally, Fate integrates tailored quantization strategies for cache optimization and IO efficiency. Experimental results show that, compared to Load on Demand and Expert Activation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in prefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively, while maintaining inference quality. Moreover, Fate's performance improvements are scalable across different memory budgets.

**Link**: [arxiv](http://arxiv.org/abs/2502.12224v2),  [pdf](http://arxiv.org/pdf/2502.12224v2)

**Tags**: cs.AI cs.LG 



### A Large Language Model for Feasible and Diverse Population Synthesis
**Authors**: Sung Yoo Lim, Hyunsoo Yun, Prateek Bansal, Dong-Kyu Kim, Eui-Jin Kim

**Updated**: 2025-05-07T07:50:12Z

**Summary**: Generating a synthetic population that is both feasible and diverse is crucial for ensuring the validity of downstream activity schedule simulation in activity-based models (ABMs). While deep generative models (DGMs), such as variational autoencoders and generative adversarial networks, have been applied to this task, they often struggle to balance the inclusion of rare but plausible combinations (i.e., sampling zeros) with the exclusion of implausible ones (i.e., structural zeros). To improve feasibility while maintaining diversity, we propose a fine-tuning method for large language models (LLMs) that explicitly controls the autoregressive generation process through topological orderings derived from a Bayesian Network (BN). Experimental results show that our hybrid LLM-BN approach outperforms both traditional DGMs and proprietary LLMs (e.g., ChatGPT-4o) with few-shot learning. Specifically, our approach achieves approximately 95% feasibility, significantly higher than the ~80% observed in DGMs, while maintaining comparable diversity, making it well-suited for practical applications. Importantly, the method is based on a lightweight open-source LLM, enabling fine-tuning and inference on standard personal computing environments. This makes the approach cost-effective and scalable for large-scale applications, such as synthesizing populations in megacities, without relying on expensive infrastructure. By initiating the ABM pipeline with high-quality synthetic populations, our method improves overall simulation reliability and reduces downstream error propagation. The source code for these methods is available for research and practical application.

**Link**: [arxiv](http://arxiv.org/abs/2505.04196v1),  [pdf](http://arxiv.org/pdf/2505.04196v1)

**Tags**: cs.LG cs.MA 



### AutoPatch: Multi-Agent Framework for Patching Real-World CVE   Vulnerabilities
**Authors**: Minjae Seo, Wonwoo Choi, Myoungsung You, Seungwon Shin

**Updated**: 2025-05-07T07:49:05Z

**Summary**: Large Language Models (LLMs) have emerged as promising tools in software development, enabling automated code generation and analysis. However, their knowledge is limited to a fixed cutoff date, making them prone to generating code vulnerable to newly disclosed CVEs. Frequent fine-tuning with new CVE sets is costly, and existing LLM-based approaches focus on oversimplified CWE examples and require providing explicit bug locations to LLMs, limiting their ability to patch complex real-world vulnerabilities. To address these limitations, we propose AutoPatch, a multi-agent framework designed to patch vulnerable LLM-generated code, particularly those introduced after the LLMs' knowledge cutoff. AutoPatch integrates Retrieval-Augmented Generation (RAG) with a structured database of recently disclosed vulnerabilities, comprising 525 code snippets derived from 75 high-severity CVEs across real-world systems such as the Linux kernel and Chrome. AutoPatch combines semantic and taint analysis to identify the most relevant CVE and leverages enhanced Chain-of-Thought (CoT) reasoning to construct enriched prompts for verification and patching. Our unified similarity model, which selects the most relevant vulnerabilities, achieves 90.4 percent accuracy in CVE matching. AutoPatch attains 89.5 percent F1-score for vulnerability verification and 95.0 percent accuracy in patching, while being over 50x more cost-efficient than traditional fine-tuning approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.04195v1),  [pdf](http://arxiv.org/pdf/2505.04195v1)

**Tags**: cs.CR 



### Liger: Linearizing Large Language Models to Gated Recurrent Structures
**Authors**: Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, Yu Cheng

**Updated**: 2025-05-07T07:42:11Z

**Summary**: Transformers with linear recurrent modeling offer linear-time training and constant-memory inference. Despite their demonstrated efficiency and performance, pretraining such non-standard architectures from scratch remains costly and risky. The linearization of large language models (LLMs) transforms pretrained standard models into linear recurrent structures, enabling more efficient deployment. However, current linearization methods typically introduce additional feature map modules that require extensive fine-tuning and overlook the gating mechanisms used in state-of-the-art linear recurrent models. To address these issues, this paper presents Liger, short for Linearizing LLMs to gated recurrent structures. Liger is a novel approach for converting pretrained LLMs into gated linear recurrent models without adding extra parameters. It repurposes the pretrained key matrix weights to construct diverse gating mechanisms, facilitating the formation of various gated recurrent structures while avoiding the need to train additional components from scratch. Using lightweight fine-tuning with Low-Rank Adaptation (LoRA), Liger restores the performance of the linearized gated recurrent models to match that of the original LLMs. Additionally, we introduce Liger Attention, an intra-layer hybrid attention mechanism, which significantly recovers 93\% of the Transformer-based LLM at 0.02\% pre-training tokens during the linearization process, achieving competitive results across multiple benchmarks, as validated on models ranging from 1B to 8B parameters. Code is available at https://github.com/OpenSparseLLMs/Linearization.

**Link**: [arxiv](http://arxiv.org/abs/2503.01496v2),  [pdf](http://arxiv.org/pdf/2503.01496v2)

**Tags**: cs.CL cs.AI cs.LG 



### Sentiment and Social Signals in the Climate Crisis: A Survey on   Analyzing Social Media Responses to Extreme Weather Events
**Authors**: Pouya Shaeri, Yasaman Mohammadpour, Alimohammad Beigi, Ariane Middel, Huan Liu

**Updated**: 2025-05-07T07:37:47Z

**Summary**: Extreme weather events driven by climate change, such as wildfires, floods, and heatwaves, prompt significant public reactions on social media platforms. Analyzing the sentiment expressed in these online discussions can offer valuable insights into public perception, inform policy decisions, and enhance emergency responses. Although sentiment analysis has been widely studied in various fields, its specific application to climate-induced events, particularly in real-time, high-impact situations like the 2025 Los Angeles forest fires, remains underexplored. In this survey, we thoroughly examine the methods, datasets, challenges, and ethical considerations related to sentiment analysis of social media content concerning weather and climate change events. We present a detailed taxonomy of approaches, ranging from lexicon-based and machine learning models to the latest strategies driven by large language models (LLMs). Additionally, we discuss data collection and annotation techniques, including weak supervision and real-time event tracking. Finally, we highlight several open problems, such as misinformation detection, multimodal sentiment extraction, and model alignment with human values. Our goal is to guide researchers and practitioners in effectively understanding sentiment during the climate crisis era.

**Link**: [arxiv](http://arxiv.org/abs/2504.18837v3),  [pdf](http://arxiv.org/pdf/2504.18837v3)

**Tags**: cs.SI 



### Towards Large-scale Generative Ranking
**Authors**: Yanhua Huang, Yuqi Chen, Xiong Cao, Rui Yang, Mingliang Qi, Yinghao Zhu, Qingchang Han, Yaowei Liu, Zhaoyu Liu, Xuefeng Yao, Yuting Jia, Leilei Ma, Yinqi Zhang, Taoyu Zhu, Liujie Zhang, Lei Chen, Weihang Chen, Min Zhu, Ruiwen Xu, Lei Zhang

**Updated**: 2025-05-08T07:51:26Z

**Summary**: Generative recommendation has recently emerged as a promising paradigm in information retrieval. However, generative ranking systems are still understudied, particularly with respect to their effectiveness and feasibility in large-scale industrial settings. This paper investigates this topic at the ranking stage of Xiaohongshu's Explore Feed, a recommender system that serves hundreds of millions of users. Specifically, we first examine how generative ranking outperforms current industrial recommenders. Through theoretical and empirical analyses, we find that the primary improvement in effectiveness stems from the generative architecture, rather than the training paradigm. To facilitate efficient deployment of generative ranking, we introduce GenRank, a novel generative architecture for ranking. We validate the effectiveness and efficiency of our solution through online A/B experiments. The results show that GenRank achieves significant improvements in user satisfaction with nearly equivalent computational resources compared to the existing production system.

**Link**: [arxiv](http://arxiv.org/abs/2505.04180v2),  [pdf](http://arxiv.org/pdf/2505.04180v2)

**Tags**: cs.IR 



### RaDialog: A Large Vision-Language Model for Radiology Report Generation   and Conversational Assistance
**Authors**: Chantal Pellegrini, Ege Özsoy, Benjamin Busam, Nassir Navab, Matthias Keicher

**Updated**: 2025-05-07T07:22:02Z

**Summary**: Conversational AI tools that can generate and discuss clinically correct radiology reports for a given medical image have the potential to transform radiology. Such a human-in-the-loop radiology assistant could facilitate a collaborative diagnostic process, thus saving time and improving the quality of reports. Towards this goal, we introduce RaDialog, the first thoroughly evaluated and publicly available large vision-language model for radiology report generation and interactive dialog. RaDialog effectively integrates visual image features and structured pathology findings with a large language model (LLM) while simultaneously adapting it to a specialized domain using parameter-efficient fine-tuning. To keep the conversational abilities of the underlying LLM, we propose a comprehensive, semi-automatically labeled, image-grounded instruct dataset for chest X-ray radiology tasks. By training with this dataset, our method achieves state-of-the-art clinical correctness in report generation and shows impressive abilities in interactive tasks such as correcting reports and answering questions, serving as a foundational step toward clinical dialog systems. Our code is available on github: https://github.com/ChantalMP/RaDialog.

**Link**: [arxiv](http://arxiv.org/abs/2311.18681v3),  [pdf](http://arxiv.org/pdf/2311.18681v3)

**Tags**: cs.CV cs.CL 



### Developing Assessment Methods for Evaluating Learning Experience
**Authors**: Maneesha

**Updated**: 2025-05-07T07:08:10Z

**Summary**: This research aims to investigate the gender-based learning experiences of engineering students enrolled in the Probability and Statistics course, focusing on the four different assessment methods employed namely direct conceptual learning (DCL), symposium, applied deployment and collaborative learning. The study encompasses 299 engineering students, comprising 90 females and 209 males. Multivariate Analysis of Variance (MANOVA), is used to gain deeper insights into the complex interplay between assessment methods and their influence on student learning. The results of the statistical analysis reveal that there are significant differences in the learning outcomes between female and male engineering students in the assessment methods of direct conceptual learning, symposium, and applied deployment. The findings suggest that there is no significant difference in the learning outcomes between female and male engineering students in the collaborative learning assessment method. The graphical representation visually confirms the significant differences in direct conceptual learning, symposium, and applied deployment, while illustrating no significant difference in collaborative learning between female and male engineering students.

**Link**: [arxiv](http://arxiv.org/abs/2505.04176v1),  [pdf](http://arxiv.org/pdf/2505.04176v1)

**Tags**: stat.OT 



### On-Device LLM for Context-Aware Wi-Fi Roaming
**Authors**: Ju-Hyung Lee, Yanqing Lu

**Updated**: 2025-05-07T07:04:49Z

**Summary**: Wireless roaming is a critical yet challenging task for maintaining seamless connectivity in dynamic mobile environments. Conventional threshold-based or heuristic schemes often fail, leading to either sticky or excessive handovers. We introduce the first cross-layer use of an on-device large language model (LLM): high-level reasoning in the application layer that issues real-time actions executed in the PHY/MAC stack. The LLM addresses two tasks: (i) context-aware AP selection, where structured prompts fuse environmental cues (e.g., location, time) to choose the best BSSID; and (ii) dynamic threshold adjustment, where the model adaptively decides when to roam. To satisfy the tight latency and resource budgets of edge hardware, we apply a suite of optimizations-chain-of-thought prompting, parameter-efficient fine-tuning, and quantization. Experiments on indoor and outdoor datasets show that our approach surpasses legacy heuristics and DRL baselines, achieving a strong balance between roaming stability and signal quality. These findings underscore the promise of application-layer LLM reasoning for lower-layer wireless control in future edge systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.04174v1),  [pdf](http://arxiv.org/pdf/2505.04174v1)

**Tags**: cs.LG cs.AI cs.NI eess.SP 



### Large Language Models are often politically extreme, usually   ideologically inconsistent, and persuasive even in informational contexts
**Authors**: Nouar Aldahoul, Hazem Ibrahim, Matteo Varvello, Aaron Kaufman, Talal Rahwan, Yasir Zaki

**Updated**: 2025-05-07T06:53:59Z

**Summary**: Large Language Models (LLMs) are a transformational technology, fundamentally changing how people obtain information and interact with the world. As people become increasingly reliant on them for an enormous variety of tasks, a body of academic research has developed to examine these models for inherent biases, especially political biases, often finding them small. We challenge this prevailing wisdom. First, by comparing 31 LLMs to legislators, judges, and a nationally representative sample of U.S. voters, we show that LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters. Second, in a randomized experiment, we show that LLMs can promulgate their preferences into political persuasiveness even in information-seeking contexts: voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot. Contrary to expectations, these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics. LLMs, especially those controlled by private companies or governments, may become a powerful and targeted vector for political influence.

**Link**: [arxiv](http://arxiv.org/abs/2505.04171v1),  [pdf](http://arxiv.org/pdf/2505.04171v1)

**Tags**: cs.CY cs.CL 



### Compatibility of Missing Data Handling Methods across the Stages of   Producing Clinical Prediction Models
**Authors**: Antonia Tsvetanova, Matthew Sperrin, David A. Jenkins, Niels Peek, Iain Buchan, Stephanie Hyland, Marcus Taylor, Angela Wood, Richard D. Riley, Glen P. Martin

**Updated**: 2025-05-07T06:38:55Z

**Summary**: Missing data is a challenge when developing, validating and deploying clinical prediction models (CPMs). Traditionally, decisions concerning missing data handling during CPM development and validation havent accounted for whether missingness is allowed at deployment. We hypothesised that the missing data approach used during model development should optimise model performance upon deployment, whilst the approach used during model validation should yield unbiased predictive performance estimates upon deployment; we term this compatibility. We aimed to determine which combinations of missing data handling methods across the CPM life cycle are compatible. We considered scenarios where CPMs are intended to be deployed with missing data allowed or not, and we evaluated the impact of that choice on earlier modelling decisions. Through a simulation study and an empirical analysis of thoracic surgery data, we compared CPMs developed and validated using combinations of complete case analysis, mean imputation, single regression imputation, multiple imputation, and pattern sub-modelling. If planning to deploy a CPM without allowing missing data, then development and validation should use multiple imputation when required. Where missingness is allowed at deployment, the same imputation method must be used during development and validation. Commonly used combinations of missing data handling methods result in biased predictive performance estimates.

**Link**: [arxiv](http://arxiv.org/abs/2504.06799v2),  [pdf](http://arxiv.org/pdf/2504.06799v2)

**Tags**: stat.ME 



### Can Language Models Understand Social Behavior in Clinical   Conversations?
**Authors**: Manas Satish Bedmutha, Feng Chen, Andrea Hartzler, Trevor Cohen, Nadir Weibel

**Updated**: 2025-05-07T06:03:37Z

**Summary**: Effective communication between providers and their patients influences health and care outcomes. The effectiveness of such conversations has been linked not only to the exchange of clinical information, but also to a range of interpersonal behaviors; commonly referred to as social signals, which are often conveyed through non-verbal cues and shape the quality of the patient-provider relationship. Recent advances in large language models (LLMs) have demonstrated an increasing ability to infer emotional and social behaviors even when analyzing only textual information. As automation increases also in clinical settings, such as for transcription of patient-provider conversations, there is growing potential for LLMs to automatically analyze and extract social behaviors from these interactions. To explore the foundational capabilities of LLMs in tracking social signals in clinical dialogue, we designed task-specific prompts and evaluated model performance across multiple architectures and prompting styles using a highly imbalanced, annotated dataset spanning 20 distinct social signals such as provider dominance, patient warmth, etc. We present the first system capable of tracking all these 20 coded signals, and uncover patterns in LLM behavior. Further analysis of model configurations and clinical context provides insights for enhancing LLM performance on social signal processing tasks in healthcare settings.

**Link**: [arxiv](http://arxiv.org/abs/2505.04152v1),  [pdf](http://arxiv.org/pdf/2505.04152v1)

**Tags**: cs.CL cs.CY cs.HC H.5.2; H.1.2; I.2.7; I.2.m; J.3 



### Energy Efficient RSMA-Based LEO Satellite Communications Assisted by   UAV-Mounted BD-Active RIS: A DRL Approach
**Authors**: Rahman Saadat Yeganeh, Hamid Behroozi

**Updated**: 2025-05-07T05:58:31Z

**Summary**: This paper proposes an advanced non-terrestrial communication architecture that integrates Rate-Splitting Multiple Access (RSMA) with a Beyond-Diagonal Active Reconfigurable Intelligent Surface (BD-ARIS) mounted on a UAV under the coverage of a Low Earth Orbit (LEO) satellite. The BD-ARIS adopts a group-connected structure to enhance signal amplification and adaptability, while RSMA enables efficient multi-user access by dividing messages into common and private components. The system jointly optimizes satellite beamforming, UAV positioning, power allocation, and rate-splitting ratios to maximize the overall energy efficiency (EE). To solve the resulting non-convex and high-dimensional problem, we employ three state-of-the-art deep reinforcement learning (DRL) algorithms: Trust Region Policy Optimization (TRPO), Twin Delayed Deep Deterministic Policy Gradient (TD3), and Asynchronous Advantage Actor-Critic (A3C). Moreover, realistic models for the power consumption of both the UAV and the BD-ARIS are considered. Simulation results reveal that TRPO consistently achieves the best performance in terms of EE and sum rate, especially under high transmit powers and challenging deployment scenarios. TD3 converges faster and performs competitively in moderate settings, while A3C suffers from instability due to its high variance. Additionally, the robustness of each algorithm under channel state information (CSI) uncertainty is evaluated, confirming TRPO resilience to imperfect observations. Overall, the proposed RSMA-BD-ARIS framework significantly outperforms conventional RIS-assisted designs and provides a scalable, energy-efficient solution for 6G and massive IoT applications in non-terrestrial networks.

**Link**: [arxiv](http://arxiv.org/abs/2505.04148v1),  [pdf](http://arxiv.org/pdf/2505.04148v1)

**Tags**: eess.SP cs.SY eess.SY 



### Unmasking the Canvas: A Dynamic Benchmark for Image Generation   Jailbreaking and LLM Content Safety
**Authors**: Variath Madhupal Gautham Nair, Vishal Varma Dantuluri

**Updated**: 2025-05-07T05:54:04Z

**Summary**: Existing large language models (LLMs) are advancing rapidly and produce outstanding results in image generation tasks, yet their content safety checks remain vulnerable to prompt-based jailbreaks. Through preliminary testing on platforms such as ChatGPT, MetaAI, and Grok, we observed that even short, natural prompts could lead to the generation of compromising images ranging from realistic depictions of forged documents to manipulated images of public figures.   We introduce Unmasking the Canvas (UTC Benchmark; UTCB), a dynamic and scalable benchmark dataset to evaluate LLM vulnerability in image generation. Our methodology combines structured prompt engineering, multilingual obfuscation (e.g., Zulu, Gaelic, Base64), and evaluation using Groq-hosted LLaMA-3. The pipeline supports both zero-shot and fallback prompting strategies, risk scoring, and automated tagging. All generations are stored with rich metadata and curated into Bronze (non-verified), Silver (LLM-aided verification), and Gold (manually verified) tiers. UTCB is designed to evolve over time with new data sources, prompt templates, and model behaviors.   Warning: This paper includes visual examples of adversarial inputs designed to test model safety. All outputs have been redacted to ensure responsible disclosure.

**Link**: [arxiv](http://arxiv.org/abs/2505.04146v1),  [pdf](http://arxiv.org/pdf/2505.04146v1)

**Tags**: cs.CL cs.AI cs.CY 



### NAMO-LLM: Efficient Navigation Among Movable Obstacles with Large   Language Model Guidance
**Authors**: Yuqing Zhang, Yiannis Kantaros

**Updated**: 2025-05-07T05:45:33Z

**Summary**: Several planners have been proposed to compute robot paths that reach desired goal regions while avoiding obstacles. However, these methods fail when all pathways to the goal are blocked. In such cases, the robot must reason about how to reconfigure the environment to access task-relevant regions - a problem known as Navigation Among Movable Objects (NAMO). While various solutions to this problem have been developed, they often struggle to scale to highly cluttered environments. To address this, we propose NAMO-LLM, a sampling-based planner that searches over robot and obstacle configurations to compute feasible plans specifying which obstacles to move, where, and in what order. Its key novelty is a non-uniform sampling strategy guided by Large Language Models (LLMs) biasing the tree construction toward directions more likely to yield a solution. We show that NAMO-LLM is probabilistically complete and demonstrate through experiments that it efficiently scales to cluttered environments, outperforming related works in both runtime and plan quality.

**Link**: [arxiv](http://arxiv.org/abs/2505.04141v1),  [pdf](http://arxiv.org/pdf/2505.04141v1)

**Tags**: cs.RO 



### Enhancing Granular Sentiment Classification with Chain-of-Thought   Prompting in Large Language Models
**Authors**: Vihaan Miriyala, Smrithi Bukkapatnam, Lavanya Prahallad

**Updated**: 2025-05-07T05:13:15Z

**Summary**: We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.

**Link**: [arxiv](http://arxiv.org/abs/2505.04135v1),  [pdf](http://arxiv.org/pdf/2505.04135v1)

**Tags**: cs.CL cs.LG 



### Advancements and limitations of LLMs in replicating human color-word   associations
**Authors**: Makoto Fukushima, Shusuke Eshita, Hiroshige Fukuhara

**Updated**: 2025-05-07T04:50:21Z

**Summary**: Color-word associations play a fundamental role in human cognition and design applications. Large Language Models (LLMs) have become widely available and have demonstrated intelligent behaviors in various benchmarks with natural conversation skills. However, their ability to replicate human color-word associations remains understudied. We compared multiple generations of LLMs (from GPT-3 to GPT-4o) against human color-word associations using data collected from over 10,000 Japanese participants, involving 17 colors and 80 words (10 word from eight categories) in Japanese. Our findings reveal a clear progression in LLM performance across generations, with GPT-4o achieving the highest accuracy in predicting the best voted word for each color and category. However, the highest median performance was approximately 50% even for GPT-4o with visual inputs (chance level of 10%). Moreover, we found performance variations across word categories and colors: while LLMs tended to excel in categories such as Rhythm and Landscape, they struggled with categories such as Emotions. Interestingly, color discrimination ability estimated from our color-word association data showed high correlation with human color discrimination patterns, consistent with previous studies. Thus, despite reasonable alignment in basic color discrimination, humans and LLMs still diverge systematically in the words they assign to those colors. Our study highlights both the advancements in LLM capabilities and their persistent limitations, raising the possibility of systematic differences in semantic memory structures between humans and LLMs in representing color-word associations.

**Link**: [arxiv](http://arxiv.org/abs/2411.02116v3),  [pdf](http://arxiv.org/pdf/2411.02116v3)

**Tags**: cs.CL cs.CV cs.GR cs.HC 



### SWE-Fixer: Training Open-Source LLMs for Effective and Efficient GitHub   Issue Resolution
**Authors**: Chengxing Xie, Bowen Li, Chang Gao, He Du, Wai Lam, Difan Zou, Kai Chen

**Updated**: 2025-05-07T04:06:41Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable proficiency across a variety of complex tasks. One significant application of LLMs is in tackling software engineering challenges, particularly in resolving real-world tasks on GitHub by fixing code based on the issues reported by the users. However, many current approaches rely on proprietary LLMs, which limits reproducibility, accessibility, and transparency. The critical components of LLMs for addressing software engineering issues and how their capabilities can be effectively enhanced remain unclear. To address these challenges, we introduce SWE-Fixer, a novel open-source framework designed to effectively and efficiently resolve GitHub issues. SWE-Fixer comprises two essential modules: a code file retrieval module and a code editing module. The retrieval module employs BM25 along with a lightweight model to achieve coarse-to-fine file retrieval. Subsequently, the code editing module utilizes the other model to generate patches for the identified files. To mitigate the lack of publicly available datasets, we compile an extensive dataset that includes 110K GitHub issues along with their corresponding patches and train the two models of SWE-Fixer separately. We assess our approach on the SWE-Bench Lite and Verified benchmarks, achieving competitive performance among open-source models with scores of 22.0% and 30.2%. Furthermore, SWE-Fixer reaches state-of-the-art performance (24.7% on Lite and 32.8% on Verified) with PASS_TO_PASS (P2P) filtering. Additionally, our approach requires only two model calls per instance, making it significantly more efficient than existing methods. These results highlight the effectiveness of SWE-Fixer in real-world code-fixing scenarios. We will make our model, dataset, and code publicly available at https://github.com/InternLM/SWE-Fixer.

**Link**: [arxiv](http://arxiv.org/abs/2501.05040v3),  [pdf](http://arxiv.org/pdf/2501.05040v3)

**Tags**: cs.CL 



### Alpha Excel Benchmark
**Authors**: David Noever, Forrest McKee

**Updated**: 2025-05-07T03:56:26Z

**Summary**: This study presents a novel benchmark for evaluating Large Language Models (LLMs) using challenges derived from the Financial Modeling World Cup (FMWC) Excel competitions. We introduce a methodology for converting 113 existing FMWC challenges into programmatically evaluable JSON formats and use this dataset to compare the performance of several leading LLMs. Our findings demonstrate significant variations in performance across different challenge categories, with models showing specific strengths in pattern recognition tasks but struggling with complex numerical reasoning. The benchmark provides a standardized framework for assessing LLM capabilities in realistic business-oriented tasks rather than abstract academic problems. This research contributes to the growing field of AI benchmarking by establishing proficiency among the 1.5 billion people who daily use Microsoft Excel as a meaningful evaluation metric that bridges the gap between academic AI benchmarks and practical business applications.

**Link**: [arxiv](http://arxiv.org/abs/2505.04110v1),  [pdf](http://arxiv.org/pdf/2505.04110v1)

**Tags**: cs.LG 



### ExpShield: Safeguarding Web Text from Unauthorized Crawling and Language   Modeling Exploitation
**Authors**: Ruixuan Liu, Toan Tran, Tianhao Wang, Hongsheng Hu, Shuo Wang, Li Xiong

**Updated**: 2025-05-07T03:48:31Z

**Summary**: As large language models (LLMs) increasingly depend on web-scraped datasets, concerns arise over their potential to generate verbatim training content with copyrighted or private information. However, current protections against web crawling or sample-specific memorization are inherently limited, as they require compliance from crawlers (e.g., respecting robots.txt) or model trainers (e.g., applying differential privacy). To empower data owners with direct control, we propose ExpShiled, a proactive self-defense mechanism that mitigates sample-specific memorization via imperceptible text perturbations. This approach requires no external collaboration while maintaining original readability. To evaluate individual-level defense efficacy, we first propose the metric of instance exploitation: a zero value indicates perfect defense, achieved when a protected text's log-perplexity ranking aligns with its counterfactual untrained ranking. We then reveal and validate the memorization trigger hypothesis, demonstrating that a model's memorization of a specific text sample stems primarily from its outlier tokens. Leveraging this insight, we design targeted perturbations that (1) prioritize inherent trigger tokens and (2) introduce artificial trigger tokens as pitfalls to disrupt memorization on the protected sample. Experiments validate our defense across model scales, languages, vision-to-language tasks, and fine-tuning methods. Even with privacy backdoors, the Membership Inference Attack (MIA) AUC drops from 0.95 to 0.55, and instance exploitation approaches zero. This suggests that compared to the ideal no-misuse scenario, the risk of exposing a text instance remains nearly unchanged despite its inclusion in training data.

**Link**: [arxiv](http://arxiv.org/abs/2412.21123v2),  [pdf](http://arxiv.org/pdf/2412.21123v2)

**Tags**: cs.CR 



### Mitigating Many-Shot Jailbreaking
**Authors**: Christopher M. Ackerman, Nina Panickssery

**Updated**: 2025-05-07T03:39:30Z

**Summary**: Many-shot jailbreaking (MSJ) is an adversarial technique that exploits the long context windows of modern LLMs to circumvent model safety training by including in the prompt many examples of a "fake" assistant responding inappropriately before the final request. With enough examples, the model's in-context learning abilities override its safety training, and it responds as if it were the "fake" assistant. In this work, we probe the effectiveness of different fine-tuning and input sanitization approaches on mitigating MSJ attacks, alone and in combination. We find incremental mitigation effectiveness for each, and show that the combined techniques significantly reduce the effectiveness of MSJ attacks, while retaining model performance in benign in-context learning and conversational tasks. We suggest that our approach could meaningfully ameliorate this vulnerability if incorporated into model safety post-training.

**Link**: [arxiv](http://arxiv.org/abs/2504.09604v2),  [pdf](http://arxiv.org/pdf/2504.09604v2)

**Tags**: cs.LG cs.CR 



### LLMs' Suitability for Network Security: A Case Study of STRIDE Threat   Modeling
**Authors**: AbdulAziz AbdulGhaffar, Ashraf Matrawy

**Updated**: 2025-05-07T03:37:49Z

**Summary**: Artificial Intelligence (AI) is expected to be an integral part of next-generation AI-native 6G networks. With the prevalence of AI, researchers have identified numerous use cases of AI in network security. However, there are almost nonexistent studies that analyze the suitability of Large Language Models (LLMs) in network security. To fill this gap, we examine the suitability of LLMs in network security, particularly with the case study of STRIDE threat modeling. We utilize four prompting techniques with five LLMs to perform STRIDE classification of 5G threats. From our evaluation results, we point out key findings and detailed insights along with the explanation of the possible underlying factors influencing the behavior of LLMs in the modeling of certain threats. The numerical results and the insights support the necessity for adjusting and fine-tuning LLMs for network security use cases.

**Link**: [arxiv](http://arxiv.org/abs/2505.04101v1),  [pdf](http://arxiv.org/pdf/2505.04101v1)

**Tags**: cs.CR cs.AI cs.NI 



### Recursively Summarizing Enables Long-Term Dialogue Memory in Large   Language Models
**Authors**: Qingyue Wang, Yanhe Fu, Yanan Cao, Shuai Wang, Zhiliang Tian, Liang Ding

**Updated**: 2025-05-07T03:31:08Z

**Summary**: Recently, large language models (LLMs), such as GPT-4, stand out remarkable conversational abilities, enabling them to engage in dynamic and contextually relevant dialogues across a wide range of topics. However, given a long conversation, these chatbots fail to recall past information and tend to generate inconsistent responses. To address this, we propose to recursively generate summaries/ memory using large language models (LLMs) to enhance long-term memory ability. Specifically, our method first stimulates LLMs to memorize small dialogue contexts and then recursively produce new memory using previous memory and following contexts. Finally, the chatbot can easily generate a highly consistent response with the help of the latest memory. We evaluate our method on both open and closed LLMs, and the experiments on the widely-used public dataset show that our method can generate more consistent responses in a long-context conversation. Also, we show that our strategy could nicely complement both long-context (e.g., 8K and 16K) and retrieval-enhanced LLMs, bringing further long-term dialogue performance. Notably, our method is a potential solution to enable the LLM to model the extremely long context. The code and scripts will be released later.

**Link**: [arxiv](http://arxiv.org/abs/2308.15022v3),  [pdf](http://arxiv.org/pdf/2308.15022v3)

**Tags**: cs.CL cs.AI 



### SceneLLM: Implicit Language Reasoning in LLM for Dynamic Scene Graph   Generation
**Authors**: Hang Zhang, Zhuoling Li, Jun Liu

**Updated**: 2025-05-07T03:14:53Z

**Summary**: Dynamic scenes contain intricate spatio-temporal information, crucial for mobile robots, UAVs, and autonomous driving systems to make informed decisions. Parsing these scenes into semantic triplets <Subject-Predicate-Object> for accurate Scene Graph Generation (SGG) is highly challenging due to the fluctuating spatio-temporal complexity. Inspired by the reasoning capabilities of Large Language Models (LLMs), we propose SceneLLM, a novel framework that leverages LLMs as powerful scene analyzers for dynamic SGG. Our framework introduces a Video-to-Language (V2L) mapping module that transforms video frames into linguistic signals (scene tokens), making the input more comprehensible for LLMs. To better encode spatial information, we devise a Spatial Information Aggregation (SIA) scheme, inspired by the structure of Chinese characters, which encodes spatial data into tokens. Using Optimal Transport (OT), we generate an implicit language signal from the frame-level token sequence that captures the video's spatio-temporal information. To further improve the LLM's ability to process this implicit linguistic input, we apply Low-Rank Adaptation (LoRA) to fine-tune the model. Finally, we use a transformer-based SGG predictor to decode the LLM's reasoning and predict semantic triplets. Our method achieves state-of-the-art results on the Action Genome (AG) benchmark, and extensive experiments show the effectiveness of SceneLLM in understanding and generating accurate dynamic scene graphs.

**Link**: [arxiv](http://arxiv.org/abs/2412.11026v2),  [pdf](http://arxiv.org/pdf/2412.11026v2)

**Tags**: cs.CV cs.AI 



### Large Language Models Are Struggle to Cope with Unreasonability in Math   Problems
**Authors**: Jingyuan Ma, Damai Dai, Zihang Yuan, Rui li, Weilin Luo, Bin Wang, Qun Liu, Lei Sha, Zhifang Sui

**Updated**: 2025-05-07T03:14:49Z

**Summary**: Recent research have demonstrated LLMs' impressive performance in math and reasoning. However, the capacity of LLMs to address math problems under unconventional conditions, such as internal inconsistencies and flawed assumptions, remains largely unexplored. In this paper, we propose a novel benchmark Unreasonable Math Problem (UMP) designed to assess LLMs' ability to recognize and respond to unreasonability in math problem. The benchmark consists of a carefully curated collection of unreasonable math questions across diverse types. Based on extensive experiments covering 19 LLMs, we observe that even state-of-the-art models such as GPT-4o achieve only limited performance of 0.6 in UMP, while reasoning models such as DeepSeek-R1 are prone to overthinking and unstable. We further explore strategies for improving the recognition of unreasonable inputs, shedding light on both the possibility and limitations of LLMs in this challenging setting.

**Link**: [arxiv](http://arxiv.org/abs/2403.19346v4),  [pdf](http://arxiv.org/pdf/2403.19346v4)

**Tags**: cs.CL 



### An Empirical Study of OpenAI API Discussions on Stack Overflow
**Authors**: Xiang Chen, Jibin Wang, Chaoyang Gao, Xiaolin Ju, Zhanqi Cui

**Updated**: 2025-05-07T02:51:32Z

**Summary**: The rapid advancement of large language models (LLMs), represented by OpenAI's GPT series, has significantly impacted various domains such as natural language processing, software development, education, healthcare, finance, and scientific research. However, OpenAI APIs introduce unique challenges that differ from traditional APIs, such as the complexities of prompt engineering, token-based cost management, non-deterministic outputs, and operation as black boxes. To the best of our knowledge, the challenges developers encounter when using OpenAI APIs have not been explored in previous empirical studies. To fill this gap, we conduct the first comprehensive empirical study by analyzing 2,874 OpenAI API-related discussions from the popular Q&A forum Stack Overflow. We first examine the popularity and difficulty of these posts. After manually categorizing them into nine OpenAI API-related categories, we identify specific challenges associated with each category through topic modeling analysis. Based on our empirical findings, we finally propose actionable implications for developers, LLM vendors, and researchers.

**Link**: [arxiv](http://arxiv.org/abs/2505.04084v1),  [pdf](http://arxiv.org/pdf/2505.04084v1)

**Tags**: cs.SE cs.AI 



### CodeBC: A More Secure Large Language Model for Smart Contract Code   Generation in Blockchain
**Authors**: Lingxiang Wang, Hainan Zhang, Qinnan Zhang, Ziwei Wang, Hongwei Zheng, Jin Dong, Zhiming Zheng

**Updated**: 2025-05-07T02:31:34Z

**Summary**: Large language models (LLMs) excel at generating code from natural language instructions, yet they often lack an understanding of security vulnerabilities. This limitation makes it difficult for LLMs to avoid security risks in generated code, particularly in high-security programming tasks such as smart contract development for blockchain. Researchers have attempted to enhance the vulnerability awareness of these models by training them to differentiate between vulnerable and fixed code snippets. However, this approach relies heavily on manually labeled vulnerability data, which is only available for popular languages like Python and C++. For low-resource languages like Solidity, used in smart contracts, large-scale annotated datasets are scarce and difficult to obtain. To address this challenge, we introduce CodeBC, a code generation model specifically designed for generating secure smart contracts in blockchain. CodeBC employs a three-stage fine-tuning approach based on CodeLlama, distinguishing itself from previous methods by not relying on pairwise vulnerability location annotations. Instead, it leverages vulnerability and security tags to teach the model the differences between vulnerable and secure code. During the inference phase, the model leverages security tags to generate secure and robust code. Experimental results demonstrate that CodeBC outperforms baseline models in terms of BLEU, CodeBLEU, and compilation pass rates, while significantly reducing vulnerability rates. These findings validate the effectiveness and cost-efficiency of our three-stage fine-tuning strategy, making CodeBC a promising solution for generating secure smart contract code.

**Link**: [arxiv](http://arxiv.org/abs/2504.21043v2),  [pdf](http://arxiv.org/pdf/2504.21043v2)

**Tags**: cs.CR cs.AI 



### LLM-e Guess: Can LLMs Capabilities Advance Without Hardware Progress?
**Authors**: Teddy Foley, Spencer Guo, Henry Josephson, Anqi Qu, Jack Sanderson

**Updated**: 2025-05-07T02:26:17Z

**Summary**: This paper examines whether large language model (LLM) capabilities can continue to advance without additional compute by analyzing the development and role of algorithms used in state-of-the-art LLMs. Motivated by regulatory efforts that have largely focused on restricting access to high-performance hardware, we ask: Can LLMs progress in a compute-constrained environment, and how do algorithmic innovations perform under such conditions?   To address these questions, we introduce a novel classification framework that distinguishes between compute-dependent innovations -- which yield disproportionate benefits at high compute levels (e.g., the Transformer architecture and mixture-of-experts models) and compute-independent innovations, which improve efficiency across all compute scales (e.g., rotary positional encoding, FlashAttention, or layer normalization). We quantify these contributions using a metric called compute-equivalent gain (CEG), which estimates the additional compute that would be required to achieve similar improvements without these algorithmic advancements.   To validate this framework, we conduct small-scale training experiments with a scaled-down GPT-2 model. Our results confirm that compute-independent advancements yield meaningful performance gains even in resource-constrained settings, with a CEG of up to $3.5\times$ over a baseline model. By contrast, compute-dependent advancements provided little benefit or even degraded performance at the small scale, reinforcing the importance of compute availability for certain algorithmic gains.

**Link**: [arxiv](http://arxiv.org/abs/2505.04075v1),  [pdf](http://arxiv.org/pdf/2505.04075v1)

**Tags**: cs.LG cs.AI I.2 



### Natural Language Generation in Healthcare: A Review of Methods and   Applications
**Authors**: Mengxian Lyu, Xiaohan Li, Ziyi Chen, Jinqian Pan, Cheng Peng, Sankalp Talankar, Yonghui Wu

**Updated**: 2025-05-07T02:25:29Z

**Summary**: Natural language generation (NLG) is the key technology to achieve generative artificial intelligence (AI). With the breakthroughs in large language models (LLMs), NLG has been widely used in various medical applications, demonstrating the potential to enhance clinical workflows, support clinical decision-making, and improve clinical documentation. Heterogeneous and diverse medical data modalities, such as medical text, images, and knowledge bases, are utilized in NLG. Researchers have proposed many generative models and applied them in a number of healthcare applications. There is a need for a comprehensive review of NLG methods and applications in the medical domain. In this study, we systematically reviewed 113 scientific publications from a total of 3,988 NLG-related articles identified using a literature search, focusing on data modality, model architecture, clinical applications, and evaluation methods. Following PRISMA (Preferred Reporting Items for Systematic reviews and Meta-Analyses) guidelines, we categorize key methods, identify clinical applications, and assess their capabilities, limitations, and emerging challenges. This timely review covers the key NLG technologies and medical applications and provides valuable insights for future studies to leverage NLG to transform medical discovery and healthcare.

**Link**: [arxiv](http://arxiv.org/abs/2505.04073v1),  [pdf](http://arxiv.org/pdf/2505.04073v1)

**Tags**: cs.CL 



### Advancing and Benchmarking Personalized Tool Invocation for LLMs
**Authors**: Xu Huang, Yuefeng Huang, Weiwen Liu, Xingshan Zeng, Yasheng Wang, Ruiming Tang, Hong Xie, Defu Lian

**Updated**: 2025-05-07T02:25:20Z

**Summary**: Tool invocation is a crucial mechanism for extending the capabilities of Large Language Models (LLMs) and has recently garnered significant attention. It enables LLMs to solve complex problems through tool calls while accessing up-to-date world knowledge. However, existing work primarily focuses on the fundamental ability of LLMs to invoke tools for problem-solving, without considering personalized constraints in tool invocation. In this work, we introduce the concept of Personalized Tool Invocation and define two key tasks: Tool Preference and Profile-dependent Query. Tool Preference addresses user preferences when selecting among functionally similar tools, while Profile-dependent Query considers cases where a user query lacks certain tool parameters, requiring the model to infer them from the user profile. To tackle these challenges, we propose PTool, a data synthesis framework designed for personalized tool invocation. Additionally, we construct \textbf{PTBench}, the first benchmark for evaluating personalized tool invocation. We then fine-tune various open-source models, demonstrating the effectiveness of our framework and providing valuable insights. Our benchmark is public at https://github.com/hyfshadow/PTBench.

**Link**: [arxiv](http://arxiv.org/abs/2505.04072v1),  [pdf](http://arxiv.org/pdf/2505.04072v1)

**Tags**: cs.CL cs.AI 



### Shadow Wireless Intelligence: Large Language Model-Driven Reasoning in   Covert Communications
**Authors**: Yuanai Xie, Zhaozhi Liu, Xiao Zhang, Shihua Zhang, Rui Hou, Minrui Xu, Ruichen Zhang, Dusit Niyato

**Updated**: 2025-05-07T02:11:43Z

**Summary**: Covert Communications (CC) can secure sensitive transmissions in industrial, military, and mission-critical applications within 6G wireless networks. However, traditional optimization methods based on Artificial Noise (AN), power control, and channel manipulation might not adapt to dynamic and adversarial environments due to the high dimensionality, nonlinearity, and stringent real-time covertness requirements. To bridge this gap, we introduce Shadow Wireless Intelligence (SWI), which integrates the reasoning capabilities of Large Language Models (LLMs) with retrieval-augmented generation to enable intelligent decision-making in covert wireless systems. Specifically, we utilize DeepSeek-R1, a mixture-of-experts-based LLM with RL-enhanced reasoning, combined with real-time retrieval of domain-specific knowledge to improve context accuracy and mitigate hallucinations. Our approach develops a structured CC knowledge base, supports context-aware retrieval, and performs semantic optimization, allowing LLMs to generate and adapt CC strategies in real time. In a case study on optimizing AN power in a full-duplex CC scenario, DeepSeek-R1 achieves 85% symbolic derivation accuracy and 94% correctness in the generation of simulation code, outperforming baseline models. These results validate SWI as a robust, interpretable, and adaptive foundation for LLM-driven intelligent covert wireless systems in 6G networks.

**Link**: [arxiv](http://arxiv.org/abs/2505.04068v1),  [pdf](http://arxiv.org/pdf/2505.04068v1)

**Tags**: cs.NI 



### SmallPlan: Leverage Small Language Models for Sequential Path Planning   with Simulation-Powered, LLM-Guided Distillation
**Authors**: Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song

**Updated**: 2025-05-07T02:00:27Z

**Summary**: Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan -- a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics.

**Link**: [arxiv](http://arxiv.org/abs/2505.00831v2),  [pdf](http://arxiv.org/pdf/2505.00831v2)

**Tags**: cs.RO cs.CL 



### BuildingBlock: A Hybrid Approach for Structured Building Generation
**Authors**: Junming Huang, Chi Wang, Letian Li, Changxin Huang, Qiang Dai, Weiwei Xu

**Updated**: 2025-05-07T01:44:02Z

**Summary**: Three-dimensional building generation is vital for applications in gaming, virtual reality, and digital twins, yet current methods face challenges in producing diverse, structured, and hierarchically coherent buildings. We propose BuildingBlock, a hybrid approach that integrates generative models, procedural content generation (PCG), and large language models (LLMs) to address these limitations. Specifically, our method introduces a two-phase pipeline: the Layout Generation Phase (LGP) and the Building Construction Phase (BCP).   LGP reframes box-based layout generation as a point-cloud generation task, utilizing a newly constructed architectural dataset and a Transformer-based diffusion model to create globally consistent layouts. With LLMs, these layouts are extended into rule-based hierarchical designs, seamlessly incorporating component styles and spatial structures.   The BCP leverages these layouts to guide PCG, enabling local-customizable, high-quality structured building generation. Experimental results demonstrate BuildingBlock's effectiveness in generating diverse and hierarchically structured buildings, achieving state-of-the-art results on multiple benchmarks, and paving the way for scalable and intuitive architectural workflows.

**Link**: [arxiv](http://arxiv.org/abs/2505.04051v1),  [pdf](http://arxiv.org/pdf/2505.04051v1)

**Tags**: cs.GR 



### Identification and Optimization of Redundant Code Using Large Language   Models
**Authors**: Shamse Tasnim Cynthia

**Updated**: 2025-05-07T00:44:32Z

**Summary**: Redundant code is a persistent challenge in software development that makes systems harder to maintain, scale, and update. It adds unnecessary complexity, hinders bug fixes, and increases technical debt. Despite their impact, removing redundant code manually is risky and error-prone, often introducing new bugs or missing dependencies. While studies highlight the prevalence and negative impact of redundant code, little focus has been given to Artificial Intelligence (AI) system codebases and the common patterns that cause redundancy. Additionally, the reasons behind developers unintentionally introducing redundant code remain largely unexplored. This research addresses these gaps by leveraging large language models (LLMs) to automatically detect and optimize redundant code in AI projects. Our research aims to identify recurring patterns of redundancy and analyze their underlying causes, such as outdated practices or insufficient awareness of best coding principles. Additionally, we plan to propose an LLM agent that will facilitate the detection and refactoring of redundancies on a large scale while preserving original functionality. This work advances the application of AI in identifying and optimizing redundant code, ultimately helping developers maintain cleaner, more readable, and scalable codebases.

**Link**: [arxiv](http://arxiv.org/abs/2505.04040v1),  [pdf](http://arxiv.org/pdf/2505.04040v1)

**Tags**: cs.SE 



### Context-aware LLM-based Safe Control Against Latent Risks
**Authors**: Xiyu Deng, Quan Khanh Luu, Anh Van Ho, Yorie Nakahira

**Updated**: 2025-05-06T23:56:25Z

**Summary**: Autonomous control systems face significant challenges in performing complex tasks in the presence of latent risks. To address this, we propose an integrated framework that combines Large Language Models (LLMs), numerical optimization, and optimization-based control to facilitate efficient subtask learning while ensuring safety against latent risks. The framework decomposes complex tasks into a sequence of context-aware subtasks that account for latent risks. These subtasks and their parameters are then refined through a multi-time-scale process: high-layer multi-turn in-context learning, mid-layer LLM Chain-of-Thought reasoning and numerical optimization, and low-layer model predictive control. The framework iteratively improves decisions by leveraging qualitative feedback and optimized trajectory data from lower-layer optimization processes and a physics simulator. We validate the proposed framework through simulated case studies involving robot arm and autonomous vehicle scenarios. The experiments demonstrate that the proposed framework can mediate actions based on the context and latent risks and learn complex behaviors efficiently.

**Link**: [arxiv](http://arxiv.org/abs/2403.11863v2),  [pdf](http://arxiv.org/pdf/2403.11863v2)

**Tags**: eess.SY cs.RO cs.SY 



### Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving
**Authors**: Shan Yu, Jiarong Xing, Yifan Qiao, Mingyuan Ma, Yangmin Li, Yang Wang, Shuo Yang, Zhiqiang Xie, Shiyi Cao, Ke Bao, Ion Stoica, Harry Xu, Ying Sheng

**Updated**: 2025-05-06T23:38:33Z

**Summary**: Serving large language models (LLMs) is expensive, especially for providers hosting many models, making cost reduction essential. The unique workload patterns of serving multiple LLMs (i.e., multi-LLM serving) create new opportunities and challenges for this task. The long-tail popularity of models and their long idle periods present opportunities to improve utilization through GPU sharing. However, existing GPU sharing systems lack the ability to adjust their resource allocation and sharing policies at runtime, making them ineffective at meeting latency service-level objectives (SLOs) under rapidly fluctuating workloads.   This paper presents Prism, a multi-LLM serving system that unleashes the full potential of GPU sharing to achieve both cost efficiency and SLO attainment. At its core, Prism tackles a key limitation of existing systems$\unicode{x2014}$the lack of $\textit{cross-model memory coordination}$, which is essential for flexibly sharing GPU memory across models under dynamic workloads. Prism achieves this with two key designs. First, it supports on-demand memory allocation by dynamically mapping physical to virtual memory pages, allowing flexible memory redistribution among models that space- and time-share a GPU. Second, it improves memory efficiency through a two-level scheduling policy that dynamically adjusts sharing strategies based on models' runtime demands. Evaluations on real-world traces show that Prism achieves more than $2\times$ cost savings and $3.3\times$ SLO attainment compared to state-of-the-art systems.

**Link**: [arxiv](http://arxiv.org/abs/2505.04021v1),  [pdf](http://arxiv.org/pdf/2505.04021v1)

**Tags**: cs.DC cs.AI cs.LG cs.PF 



### SLOT: Structuring the Output of Large Language Models
**Authors**: Darren Yow-Bang Wang, Zhengyuan Shen, Soumya Smruti Mishra, Zhichao Xu, Yifei Teng, Haibo Ding

**Updated**: 2025-05-06T23:29:43Z

**Summary**: Structured outputs are essential for large language models (LLMs) in critical applications like agents and information extraction. Despite their capabilities, LLMs often generate outputs that deviate from predefined schemas, significantly hampering reliable application development. We present SLOT (Structured LLM Output Transformer), a model-agnostic approach that transforms unstructured LLM outputs into precise structured formats. While existing solutions predominantly rely on constrained decoding techniques or are tightly coupled with specific models, SLOT employs a fine-tuned lightweight language model as a post-processing layer, achieving flexibility across various LLMs and schema specifications. We introduce a systematic pipeline for data curation and synthesis alongside a formal evaluation methodology that quantifies both schema accuracy and content fidelity. Our results demonstrate that fine-tuned Mistral-7B model with constrained decoding achieves near perfect schema accuracy (99.5%) and content similarity (94.0%), outperforming Claude-3.5-Sonnet by substantial margins (+25 and +20 percentage points, respectively). Notably, even compact models like Llama-3.2-1B can match or exceed the structured output capabilities of much larger proprietary models when equipped with SLOT, enabling reliable structured generation in resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2505.04016v1),  [pdf](http://arxiv.org/pdf/2505.04016v1)

**Tags**: cs.CL cs.AI cs.LG 



### Towards Universal and Black-Box Query-Response Only Attack on LLMs with   QROA
**Authors**: Hussein Jawad, Yassine Chenik, Nicolas J. -B. Brunel

**Updated**: 2025-05-06T22:24:50Z

**Summary**: The rapid adoption of Large Language Models (LLMs) has exposed critical security and ethical vulnerabilities, particularly their susceptibility to adversarial manipulations. This paper introduces QROA, a novel black-box jailbreak method designed to identify adversarial suffixes that can bypass LLM alignment safeguards when appended to a malicious instruction. Unlike existing suffix-based jailbreak approaches, QROA does not require access to the model's logit or any other internal information. It also eliminates reliance on human-crafted templates, operating solely through the standard query-response interface of LLMs. By framing the attack as an optimization bandit problem, QROA employs a surrogate model and token level optimization to efficiently explore suffix variations. Furthermore, we propose QROA-UNV, an extension that identifies universal adversarial suffixes for individual models, enabling one-query jailbreaks across a wide range of instructions. Testing on multiple models demonstrates Attack Success Rate (ASR) greater than 80\%. These findings highlight critical vulnerabilities, emphasize the need for advanced defenses, and contribute to the development of more robust safety evaluations for secure AI deployment. The code is made public on the following link: https://github.com/qroa/QROA

**Link**: [arxiv](http://arxiv.org/abs/2406.02044v3),  [pdf](http://arxiv.org/pdf/2406.02044v3)

**Tags**: cs.CL cs.LG 



### An alignment safety case sketch based on debate
**Authors**: Marie Davidsen Buhl, Jacob Pfau, Benjamin Hilton, Geoffrey Irving

**Updated**: 2025-05-06T21:53:44Z

**Summary**: If AI systems match or exceed human capabilities on a wide range of tasks, it may become difficult for humans to efficiently judge their actions -- making it hard to use human feedback to steer them towards desirable traits. One proposed solution is to leverage another superhuman system to point out flaws in the system's outputs via a debate. This paper outlines the value of debate for AI safety, as well as the assumptions and further research required to make debate work. It does so by sketching an ``alignment safety case'' -- an argument that an AI system will not autonomously take actions which could lead to egregious harm, despite being able to do so. The sketch focuses on the risk of an AI R\&D agent inside an AI company sabotaging research, for example by producing false results. To prevent this, the agent is trained via debate, subject to exploration guarantees, to teach the system to be honest. Honesty is maintained throughout deployment via online training. The safety case rests on four key claims: (1) the agent has become good at the debate game, (2) good performance in the debate game implies that the system is mostly honest, (3) the system will not become significantly less honest during deployment, and (4) the deployment context is tolerant of some errors. We identify open research problems that, if solved, could render this a compelling argument that an AI system is safe.

**Link**: [arxiv](http://arxiv.org/abs/2505.03989v1),  [pdf](http://arxiv.org/pdf/2505.03989v1)

**Tags**: cs.AI 



### Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA   Adapters
**Authors**: Roberto Garcia, Jerry Liu, Daniel Sorvisto, Sabri Eyuboglu

**Updated**: 2025-05-06T21:45:53Z

**Summary**: Large Language Models (LLMs) are computationally intensive, particularly during inference. Neuron-adaptive techniques, which selectively activate neurons in Multi-Layer Perceptron (MLP) layers, offer some speedups but suffer from limitations in modern Transformers. These include reliance on sparse activations, incompatibility with attention layers, and the use of costly neuron masking techniques. To address these issues, we propose the Adaptive Rank Allocation framework and introduce the Rank and Neuron Allocator (RaNA) adapter. RaNA adapters leverage rank adapters, which operate on linear layers by applying both low-rank matrix decompositions and adaptive masking to efficiently allocate compute without depending on activation sparsity. This enables RaNA to be generally applied to MLPs and linear components of attention modules, while eliminating the need for expensive maskers found in neuron-adaptive methods. Notably, when compared to neuron adapters, RaNA improves perplexity by up to 7 points and increases accuracy by up to 8 percentage-points when reducing FLOPs by $\sim$44% in state-of-the-art Transformer architectures. These results position RaNA as a robust solution for improving inference efficiency in modern Transformer architectures.

**Link**: [arxiv](http://arxiv.org/abs/2503.18216v2),  [pdf](http://arxiv.org/pdf/2503.18216v2)

**Tags**: cs.LG 



### Towards a HIPAA Compliant Agentic AI System in Healthcare
**Authors**: Subash Neupane, Sudip Mittal, Shahram Rahimi

**Updated**: 2025-05-06T21:45:48Z

**Summary**: Agentic AI systems powered by Large Language Models (LLMs) as their foundational reasoning engine, are transforming clinical workflows such as medical report generation and clinical summarization by autonomously analyzing sensitive healthcare data and executing decisions with minimal human oversight. However, their adoption demands strict compliance with regulatory frameworks such as Health Insurance Portability and Accountability Act (HIPAA), particularly when handling Protected Health Information (PHI). This work-in-progress paper introduces a HIPAA-compliant Agentic AI framework that enforces regulatory compliance through dynamic, context-aware policy enforcement. Our framework integrates three core mechanisms: (1) Attribute-Based Access Control (ABAC) for granular PHI governance, (2) a hybrid PHI sanitization pipeline combining regex patterns and BERT-based model to minimize leakage, and (3) immutable audit trails for compliance verification.

**Link**: [arxiv](http://arxiv.org/abs/2504.17669v2),  [pdf](http://arxiv.org/pdf/2504.17669v2)

**Tags**: cs.MA cs.AI cs.ET 



### Can Large Language Models Predict Parallel Code Performance?
**Authors**: Gregory Bolet, Giorgis Georgakoudis, Harshitha Menon, Konstantinos Parasyris, Niranjan Hasabnis, Hayden Estes, Kirk W. Cameron, Gal Oren

**Updated**: 2025-05-06T21:41:20Z

**Summary**: Accurate determination of the performance of parallel GPU code typically requires execution-time profiling on target hardware -- an increasingly prohibitive step due to limited access to high-end GPUs. This paper explores whether Large Language Models (LLMs) can offer an alternative approach for GPU performance prediction without relying on hardware. We frame the problem as a roofline classification task: given the source code of a GPU kernel and the hardware specifications of a target GPU, can an LLM predict whether the GPU kernel is compute-bound or bandwidth-bound?   For this study, we build a balanced dataset of 340 GPU kernels, obtained from HeCBench benchmark and written in CUDA and OpenMP, along with their ground-truth labels obtained via empirical GPU profiling. We evaluate LLMs across four scenarios: (1) with access to profiling data of the kernel source, (2) zero-shot with source code only, (3) few-shot with code and label pairs, and (4) fine-tuned on a small custom dataset.   Our results show that state-of-the-art LLMs have a strong understanding of the Roofline model, achieving 100% classification accuracy when provided with explicit profiling data. We also find that reasoning-capable LLMs significantly outperform standard LLMs in zero- and few-shot settings, achieving up to 64% accuracy on GPU source codes, without profiling information. Lastly, we find that LLM fine-tuning will require much more data than what we currently have available.   This work is among the first to use LLMs for source-level roofline performance prediction via classification, and illustrates their potential to guide optimization efforts when runtime profiling is infeasible. Our findings suggest that with better datasets and prompt strategies, LLMs could become practical tools for HPC performance analysis and performance portability.

**Link**: [arxiv](http://arxiv.org/abs/2505.03988v1),  [pdf](http://arxiv.org/pdf/2505.03988v1)

**Tags**: cs.DC cs.AI cs.PF 



