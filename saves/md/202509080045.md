# Arxiv Results
## Keyword: kv cache 
 ### PagedEviction: Structured Block-wise KV Cache Pruning for Efficient   Large Language Model Inference
**Authors**: Krishna Teja Chitty-Venkata, Jie Ye, Xian-He Sun, Anthony Kougkas, Murali Emani, Venkatram Vishwanath, Bogdan Nicolae

**Updated**: 2025-09-04T16:40:01Z

**Summary**: KV caching significantly improves the efficiency of Large Language Model (LLM) inference by storing attention states from previously processed tokens, enabling faster generation of subsequent tokens. However, as sequence length increases, the KV cache quickly becomes a major memory bottleneck. To address this, we propose PagedEviction, a novel fine-grained, structured KV cache pruning strategy that enhances the memory efficiency of vLLM's PagedAttention. Unlike existing approaches that rely on attention-based token importance or evict tokens across different vLLM pages, PagedEviction introduces an efficient block-wise eviction algorithm tailored for paged memory layouts. Our method integrates seamlessly with PagedAttention without requiring any modifications to its CUDA attention kernels. We evaluate PagedEviction across Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models on the LongBench benchmark suite, demonstrating improved memory usage with better accuracy than baselines on long context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.04377v1),  [pdf](http://arxiv.org/pdf/2509.04377v1)

**Tags**: cs.LG 



### Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and   Multiple Level Analysis
**Authors**: Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Hongyuan Liu, Qiang Wang, Xiaowen Chu

**Updated**: 2025-09-04T15:21:11Z

**Summary**: This study presents a comprehensive multi-level analysis of the NVIDIA Hopper GPU architecture, focusing on its performance characteristics and novel features. We benchmark Hopper's memory subsystem, highlighting improvements in the L2 partitioned cache and global memory access compared to Ampere and Ada Lovelace. The evaluation of Hopper's fourth-generation tensor cores reveals the benefits of FP8 precision and asynchronous wgmma instructions for matrix operations. Additionally, we investigate the performance of DPX instructions for dynamic programming, distributed shared memory (DSM) for inter-SM communication, and the Tensor Memory Accelerator (TMA) for asynchronous data movement. Through multi-level evaluation, we discover that the Hopper architecture demonstrates significant acceleration potential in real-world applications. For instance, the asynchronous programming model supported by TMA achieves a 1.5x speedup in matrix multiplication, FP8 delivers nearly double the performance of FP16, and DPX instructions accelerate a computational biology algorithm by at least 4.75x. Our findings provide actionable insights for optimizing compute-intensive workloads, from AI training to bioinformatics, on Hopper GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2501.12084v2),  [pdf](http://arxiv.org/pdf/2501.12084v2)

**Tags**: cs.DC cs.AR cs.PF 



### InferLog: Accelerating LLM Inference for Online Log Parsing via   ICL-oriented Prefix Caching
**Authors**: Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng

**Updated**: 2025-09-04T13:14:33Z

**Summary**: Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.   In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.08523v2),  [pdf](http://arxiv.org/pdf/2507.08523v2)

**Tags**: cs.SE 



### Set Block Decoding is a Language Model Inference Accelerator
**Authors**: Itai Gat, Heli Ben-Hamu, Marton Havasi, Daniel Haziza, Jeremy Reizenstein, Gabriel Synnaeve, David Lopez-Paz, Brian Karrer, Yaron Lipman

**Updated**: 2025-09-04T13:02:39Z

**Summary**: Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.

**Link**: [arxiv](http://arxiv.org/abs/2509.04185v1),  [pdf](http://arxiv.org/pdf/2509.04185v1)

**Tags**: cs.LG 



### VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer   Vision
**Authors**: Safouane El Ghazouali, Umberto Michelucci

**Updated**: 2025-09-04T12:54:32Z

**Summary**: AI models rely on annotated data to learn pattern and perform prediction. Annotation is usually a labor-intensive step that require associating labels ranging from a simple classification label to more complex tasks such as object detection, oriented bounding box estimation, and instance segmentation. Traditional tools often require extensive manual input, limiting scalability for large datasets. To address this, we introduce VisioFirm, an open-source web application designed to streamline image labeling through AI-assisted automation. VisioFirm integrates state-of-the-art foundation models into an interface with a filtering pipeline to reduce human-in-the-loop efforts. This hybrid approach employs CLIP combined with pre-trained detectors like Ultralytics models for common classes and zero-shot models such as Grounding DINO for custom labels, generating initial annotations with low-confidence thresholding to maximize recall. Through this framework, when tested on COCO-type of classes, initial prediction have been proven to be mostly correct though the users can refine these via interactive tools supporting bounding boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has on-the-fly segmentation powered by Segment Anything accelerated through WebGPU for browser-side efficiency. The tool supports multiple export formats (YOLO, COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort through benchmarks on diverse datasets, while maintaining high annotation accuracy via clustering of connected CLIP-based disambiguate components and IoU-graph for redundant detection suppression. VisioFirm can be accessed from \href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.

**Link**: [arxiv](http://arxiv.org/abs/2509.04180v1),  [pdf](http://arxiv.org/pdf/2509.04180v1)

**Tags**: cs.CV cs.AI 



### Spotlight Attention: Towards Efficient LLM Generation via Non-linear   Hashing-based KV Cache Retrieval
**Authors**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji

**Updated**: 2025-09-04T09:08:29Z

**Summary**: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.

**Link**: [arxiv](http://arxiv.org/abs/2508.19740v2),  [pdf](http://arxiv.org/pdf/2508.19740v2)

**Tags**: cs.CL 



### Systematic Timing Leakage Analysis of NIST PQDSS Candidates: Tooling and   Lessons Learned
**Authors**: Olivier Adjonyo, Sebastien Bardin, Emanuele Bellini, Gilbert Ndollane Dione, Mahmudul Faisal Al Ameen, Robert Merget, Frederic Recoules, Yanis Sellami

**Updated**: 2025-09-04T08:41:06Z

**Summary**: The PQDSS standardization process requires cryptographic primitives to be free from vulnerabilities, including timing and cache side-channels. Resistance to timing leakage is therefore an essential property, and achieving this typically relies on software implementations that follow constant-time principles. Moreover, ensuring that all implementations are constant-time is crucial for fair performance comparisons, as secure implementations often incur additional overhead. Such analysis also helps identify scheme proposals that are inherently difficult to implement in constant time. Because constant-time properties can be broken during compilation, it is often necessary to analyze the compiled binary directly. Since manual binary analysis is extremely challenging, automated analysis becomes highly important. Although several tools exist to assist with such analysis, they often have usability limitations and are difficult to set up correctly. To support the developers besides the NIST committee in verifying candidates, we developed a toolchain that automates configuration, execution, and result analysis for several widely used constant-time analysis tools. We selected TIMECOP and Binsec/Rel2 to verify constant-time policy compliance at the binary level, and dudect and RTLF to detect side-channel vulnerabilities through statistical analysis of execution time behavior. We demonstrate its effectiveness and practicability by evaluating the NIST PQDSS round 1 and round 2 implementations. We reported 26 issues in total to the respective developers, and 5 of them have already been fixed. We also discuss our different findings, as well as the benefits of shortcomings of the different tools.

**Link**: [arxiv](http://arxiv.org/abs/2509.04010v1),  [pdf](http://arxiv.org/pdf/2509.04010v1)

**Tags**: cs.CR 



### IC-Cache: Efficient Large Language Model Serving via In-context Caching
**Authors**: Yifan Yu, Yu Gan, Nikhil Sarda, Lillian Tsai, Jiaming Shen, Yanqi Zhou, Arvind Krishnamurthy, Fan Lai, Henry M. Levy, David Culler

**Updated**: 2025-09-04T06:20:55Z

**Summary**: Large language models (LLMs) have excelled in various applications, yet serving them at scale is challenging due to their substantial resource demands and high latency. Our real-world studies reveal that over 70% of user requests to LLMs have semantically similar counterparts, suggesting the potential for knowledge transfer among requests. However, naively caching and reusing past responses leads to a big quality drop. In this paper, we introduce IC-Cache, a caching system that enables live LLM capability augmentation to improve serving efficiency: by leveraging historical request-response pairs from larger models as in-context examples, IC-Cache empowers small LLMs to imitate and even exceed the compositional abilities (e.g., reasoning) of their larger counterparts, enabling selective offloading of requests to reduce cost and latency. Achieving this live augmentation at scale introduces intricate trade-offs between response quality, latency, and system throughput. For a new request, IC-Cache efficiently selects similar, high-utility examples to prepend them to the new request's input. At scale, it adaptively routes requests across LLMs of varying capabilities, accounting for response quality and serving loads. IC-Cache employs a cost-aware cache replay mechanism that refines example quality offline to maximize online cache utility and efficiency. Evaluations on millions of realistic requests demonstrate that IC-Cache improves LLM serving throughput by 1.4-5.9x and reduces latency by 28-71% without hurting response quality.

**Link**: [arxiv](http://arxiv.org/abs/2501.12689v3),  [pdf](http://arxiv.org/pdf/2501.12689v3)

**Tags**: cs.LG 



### ConServe: Fine-Grained GPU Harvesting for LLM Online and Offline   Co-Serving
**Authors**: Yifan Qiao, Shu Anzai, Shan Yu, Haoran Ma, Shuo Yang, Yang Wang, Miryung Kim, Yongji Wu, Yang Zhou, Jiarong Xing, Joseph E. Gonzalez, Ion Stoica, Harry Xu

**Updated**: 2025-09-03T20:54:57Z

**Summary**: Large language model (LLM) serving demands low latency and high throughput, but high load variability makes it challenging to achieve high GPU utilization. In this paper, we identify a synergetic but overlooked opportunity to co-serve latency-critical online requests alongside latency-tolerant offline tasks such as model benchmarking. While promising, existing serving systems fail to co-serve them efficiently, as their coarse-grained resource management at the request or iteration level cannot harvest millisecond-level GPU idle cycles without introducing interference that violates online latency objectives. ConServe is a new LLM co-serving system that achieves high throughput and strong online latency guarantees by managing resources at finer granularities. ConServe introduces three techniques: (1) a latency-aware token-level scheduler that precisely sizes offline batches and tokens to fit within online latency objectives; (2) sub-iteration, layer-wise preemption that allows offline tasks to yield to online load spikes; and (3) incremental KV cache management that enables preempting and resuming offline requests at near-zero cost. Evaluations with Llama-3.1 and Qwen-2.5 models on real-world workloads show that ConServe delivers an average of 2.2$\times$ higher throughput and reduces online serving tail latency by 2.9$\times$ on average compared to state-of-the-art systems.

**Link**: [arxiv](http://arxiv.org/abs/2410.01228v2),  [pdf](http://arxiv.org/pdf/2410.01228v2)

**Tags**: cs.DC cs.LG 



### CloudFormer: An Attention-based Performance Prediction for Public Clouds   with Unknown Workload
**Authors**: Amirhossein Shahbazinia, Darong Huang, Luis Costero, David Atienza

**Updated**: 2025-09-03T15:15:44Z

**Summary**: Cloud platforms are increasingly relied upon to host diverse, resource-intensive workloads due to their scalability, flexibility, and cost-efficiency. In multi-tenant cloud environments, virtual machines are consolidated on shared physical servers to improve resource utilization. While virtualization guarantees resource partitioning for CPU, memory, and storage, it cannot ensure performance isolation. Competition for shared resources such as last-level cache, memory bandwidth, and network interfaces often leads to severe performance degradation. Existing management techniques, including VM scheduling and resource provisioning, require accurate performance prediction to mitigate interference. However, this remains challenging in public clouds due to the black-box nature of VMs and the highly dynamic nature of workloads. To address these limitations, we propose CloudFormer, a dual-branch Transformer-based model designed to predict VM performance degradation in black-box environments. CloudFormer jointly models temporal dynamics and system-level interactions, leveraging 206 system metrics at one-second resolution across both static and dynamic scenarios. This design enables the model to capture transient interference effects and adapt to varying workload conditions without scenario-specific tuning. Complementing the methodology, we provide a fine-grained dataset that significantly expands the temporal resolution and metric diversity compared to existing benchmarks. Experimental results demonstrate that CloudFormer consistently outperforms state-of-the-art baselines across multiple evaluation metrics, achieving robust generalization across diverse and previously unseen workloads. Notably, CloudFormer attains a mean absolute error (MAE) of just 7.8%, representing a substantial improvement in predictive accuracy and outperforming existing methods at least by 28%.

**Link**: [arxiv](http://arxiv.org/abs/2509.03394v1),  [pdf](http://arxiv.org/pdf/2509.03394v1)

**Tags**: cs.DC cs.LG cs.PF 



### Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving
**Authors**: Ruoyu Qin, Zheming Li, Weiran He, Mingxing Zhang, Yongwei Wu, Weimin Zheng, Xinran Xu

**Updated**: 2025-09-03T14:56:29Z

**Summary**: Mooncake is the serving platform for Kimi, a leading LLM service provided by Moonshot AI. It features a KVCache-centric disaggregated architecture that separates the prefill and decoding clusters. It also leverages the underutilized CPU, DRAM, and SSD resources of the GPU cluster to implement a disaggregated cache of KVCache. The core of Mooncake is its KVCache-centric scheduler, which balances maximizing overall effective throughput while meeting latency-related Service Level Objectives (SLOs). Unlike traditional studies that assume all requests will be processed, Mooncake faces challenges due to highly overloaded scenarios. To mitigate these, we developed a prediction-based early rejection policy. Experiments show that Mooncake excels in long-context scenarios. Compared to the baseline method, Mooncake can achieve up to a 525% increase in throughput in certain simulated scenarios while adhering to SLOs. Under real workloads, Mooncake's innovative architecture enables Kimi to handle 75% more requests.

**Link**: [arxiv](http://arxiv.org/abs/2407.00079v4),  [pdf](http://arxiv.org/pdf/2407.00079v4)

**Tags**: cs.DC cs.AI cs.AR 



### Amplifying Effective CXL Memory Bandwidth for LLM Inference via   Transparent Near-Data Processing
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-09-03T14:53:45Z

**Summary**: Large language model (LLM) inference is bottlenecked by the limited bandwidth of CXL-based memory used for capacity expansion. We introduce CXL-NDP, a transparent near-data processing architecture that amplifies effective CXL bandwidth without requiring changes to the CXL.mem interface or AI models. CXL-NDP integrates a precision-scalable bit-plane layout for dynamic quantization with transparent lossless compression of weights and KV caches directly within the CXL device. In end-to-end serving, CXL-NDP improves throughput by 43%, extends the maximum context length by 87%, and reduces the KV cache footprint by 46.9% without accuracy loss. Hardware synthesis confirms its practicality with a modest silicon footprint, lowering the barrier for adopting efficient, scalable CXL-based memory in generative AI infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2509.03377v1),  [pdf](http://arxiv.org/pdf/2509.03377v1)

**Tags**: cs.AR 



### RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based   Sequence Modeling
**Authors**: Xiuying Wei, Anunay Yadav, Razvan Pascanu, Caglar Gulcehre

**Updated**: 2025-09-03T14:28:23Z

**Summary**: Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation suffers from memory degradation in long contexts and limits fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a 7x improvement in training speed with 100K token sequences and 9x in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning~(SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at https://github.com/CLAIRE-Labo/RAT.

**Link**: [arxiv](http://arxiv.org/abs/2507.04416v2),  [pdf](http://arxiv.org/pdf/2507.04416v2)

**Tags**: cs.CL 



### A Cegar-centric Bounded Reachability Analysis for Compositional Affine   Hybrid Systems
**Authors**: Atanu Kundu, Pratyay Sarkar, Rajarshi Ray

**Updated**: 2025-09-03T11:23:35Z

**Summary**: Reachability analysis of compositional hybrid systems, where individual components are modeled as hybrid automata, poses unique challenges. In addition to preserving the compositional semantics while computing system behaviors, algorithms have to cater to the explosion in the number of locations in the parallel product automaton. In this paper, we propose a bounded reachability analysis algorithm for compositional hybrid systems with piecewise affine dynamics, based on the principle of counterexample guided abstraction refinement (CEGAR). In particular, the algorithm searches for a counterexample in the discrete abstraction of the composition model, without explicitly computing a product automaton. When a counterexample is discovered in the abstraction, its validity is verified by a refinement of the state-space guided by the abstract counterexample. The state-space refinement is through a symbolic reachability analysis, particularly using a state-of-the-art algorithm with support functions as the continuous state representation. In addition, the algorithm mixes different semantics of composition with the objective of improved efficiency. Step compositional semantics is followed while exploring the abstract (discrete) state-space, while shallow compositional semantics is followed during state-space refinement with symbolic reachability analysis. Optimizations such as caching the results of the symbolic reachability analysis, which can be later reused, have been proposed. We implement this algorithm in the tool SAT-Reach and demonstrate the scalability benefits.

**Link**: [arxiv](http://arxiv.org/abs/2509.03560v1),  [pdf](http://arxiv.org/pdf/2509.03560v1)

**Tags**: cs.LO 



### Adaptive KV-Cache Compression without Manually Setting Budget
**Authors**: Chenxia Tang, Jianchun Liu, Hongli Xu, Liusheng Huang

**Updated**: 2025-09-03T08:38:40Z

**Summary**: Large language models (LLMs) inference relies heavily on KV-caches to accelerate autoregressive decoding, but the resulting memory footprint grows rapidly with sequence length, posing significant efficiency challenges. Current KV-cache compression methods suffer from a Procrustes' bed problem: they force diverse workloads into fixed compression ratios, leading to suboptimal resource allocation and inference performance. To this end, we present GVote, an adaptive KV-cache compression scheme that eliminates manual budget specification while achieving superior accuracy-efficiency trade-offs. GVote operates on the principle that the important keys are the aggregation of keys required by future queries. The method predicts future query attention demands by Monte-Carlo style sampling potential queries and aggregating selected keys to determine the optimal cache budget without manual specification. Experimental evaluation demonstrates GVote's effectiveness across multiple benchmarks, including GSM8K, RULER and Longbench. Compared to baselines, GVote exhibits 2$\times$ memory reduction while the accuracy maintains higher or comparable.

**Link**: [arxiv](http://arxiv.org/abs/2509.03136v1),  [pdf](http://arxiv.org/pdf/2509.03136v1)

**Tags**: cs.DB cs.AI 



### FastCache: Fast Caching for Diffusion Transformer Through Learnable   Linear Approximation
**Authors**: Dong Liu, Yanxuan Yu, Jiayi Zhang, Yifan Li, Ben Lengerich, Ying Nian Wu

**Updated**: 2025-09-03T06:56:21Z

**Summary**: Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.

**Link**: [arxiv](http://arxiv.org/abs/2505.20353v2),  [pdf](http://arxiv.org/pdf/2505.20353v2)

**Tags**: cs.LG cs.AI cs.CV cs.MM cs.PF 



### Digital Network Twins for Next-generation Wireless: Creation,   Optimization, and Challenges
**Authors**: Zifan Zhang, Zhiyuan Peng, Hanzhi Yu, Mingzhe Chen, Yuchen Liu

**Updated**: 2025-09-02T18:10:00Z

**Summary**: Digital network twins (DNTs), by representing a physical network using a virtual model, offer significant benefits such as streamlined network development, enhanced productivity, and cost reduction for next-generation (nextG) communication infrastructure. Existing works mainly describe the deployment of DNT technologies in various service sections.The full life cycle of DNTs for telecommunication has not yet been comprehensively studied, particularly in the aspects of fine-grained creation, real-time adaptation, resource-efficient deployment, and security protection. This article presents an in-depth overview of DNTs, exploring their concrete integration into networks and communication, covering the fundamental designs, the emergent applications, and critical challenges in multiple dimensions. We also include two detailed case studies to illustrate how DNTs can be applied in real-world scenarios such as wireless traffic forecasting and edge caching. Additionally, a forward-looking vision of the research opportunities in tackling the challenges of DNTs is provided, aiming to fully maximize the benefits of DNTs in nextG networks.

**Link**: [arxiv](http://arxiv.org/abs/2410.18002v2),  [pdf](http://arxiv.org/pdf/2410.18002v2)

**Tags**: cs.NI 



### A Novel Coded Caching Scheme for Partially Cooperative Device-to-Device   Networks
**Authors**: Rashid Ummer N. T., K. K. Krishnan Namboodiri, B. Sundar Rajan

**Updated**: 2025-09-02T17:35:42Z

**Summary**: Device-to-device (D2D) communication is one of the most promising techniques for future wireless cellular communication systems. This paper considers coded caching in a partially cooperative wireless D2D network, where only a subset of users transmit during delivery, while all users request files. The non-transmitting users are referred to as selfish users. All existing schemes that do not require knowledge of the identity of selfish users before content placement are limited to the high-memory regime, particularly when the number of selfish users is large. We propose a novel coded caching scheme for a partially cooperative D2D network that operates in all feasible memory regimes, regardless of the number of selfish users. We also derive a lower bound on the transmission load of a partially cooperative D2D coded caching scheme. Using this bound, the proposed scheme is shown to be optimal in the high-memory regime.

**Link**: [arxiv](http://arxiv.org/abs/2509.02532v1),  [pdf](http://arxiv.org/pdf/2509.02532v1)

**Tags**: cs.IT math.IT 



### REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and   Failure Mitigation
**Authors**: Tommaso Bonato, Abdul Kabbani, Ahmad Ghalayini, Michael Papamichael, Mohammad Dohadwala, Lukas Gianinazzi, Mikhail Khalilov, Elias Achermann, Daniele De Sensi, Torsten Hoefler

**Updated**: 2025-09-02T16:39:56Z

**Summary**: Next-generation datacenters require highly efficient network load balancing to manage the growing scale of artificial intelligence (AI) training and general datacenter traffic. However, existing Ethernet-based solutions, such as Equal Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to maintain high network utilization due to both increasing traffic demands and the expanding scale of datacenter topologies, which also exacerbate network failures. To address these limitations, we propose REPS, a lightweight decentralized per-packet adaptive load balancing algorithm designed to optimize network utilization while ensuring rapid recovery from link failures. REPS adapts to network conditions by caching good-performing paths. In case of a network failure, REPS re-routes traffic away from it in less than 100 microseconds. REPS is designed to be deployed with next-generation out-of-order transports, such as Ultra Ethernet, and uses less than 25 bytes of per-connection state regardless of the topology size. We extensively evaluate REPS in large-scale simulations and FPGA-based NICs.

**Link**: [arxiv](http://arxiv.org/abs/2407.21625v5),  [pdf](http://arxiv.org/pdf/2407.21625v5)

**Tags**: cs.NI 



### MLP-Offload: Multi-Level, Multi-Path Offloading for LLM Pre-training to   Break the GPU Memory Wall
**Authors**: Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae

**Updated**: 2025-09-02T16:30:49Z

**Summary**: Training LLMs larger than the aggregated memory of multiple GPUs is increasingly necessary due to the faster growth of LLM sizes compared to GPU memory. To this end, multi-tier host memory or disk offloading techniques are proposed by state of art. Despite advanced asynchronous multi-tier read/write strategies, such offloading strategies result in significant I/O overheads in the critical path of training, resulting in slower iterations. To this end, we propose MLP-Offload, a novel multi-level, multi-path offloading engine specifically designed for optimizing LLM training on resource-constrained setups by mitigating I/O bottlenecks. We make several key observations that drive the design of MLP-Offload, such as I/O overheads during the update dominate the iteration time; I/O bandwidth of the third-level remote storage tier remains unutilized; and, contention due to concurrent offloading amplifies I/O bottlenecks. Driven by these insights, we design and implement MLP-Offload to offload the optimizer states across multiple tiers in a cache-efficient and concurrency-controlled fashion to mitigate I/O bottlenecks during the backward and update phases. Evaluations on models up to 280B parameters shows that MLP-Offload achieves 2.5$\times$ faster iterations compared to the state-of-the-art LLM training runtimes.

**Link**: [arxiv](http://arxiv.org/abs/2509.02480v1),  [pdf](http://arxiv.org/pdf/2509.02480v1)

**Tags**: cs.DC cs.AI cs.LG H.2.0; E.2; I.2.11 



### Cache Management for Mixture-of-Experts LLMs -- extended version
**Authors**: Spyros Angelopoulos, Loris Marchal, Adrien Obrecht, Bertrand Simon

**Updated**: 2025-09-02T15:19:06Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities across a variety of tasks. One of the main challenges towards the successful deployment of LLMs is memory management, since they typically involve billions of parameters. To this end, architectures based on Mixture-of-Experts have been proposed, which aim to reduce the size of the parameters that are activated when producing a token. This raises the equally critical issue of efficiently managing the limited cache of the system, in that frequently used experts should be stored in the fast cache rather than in the slower secondary memory.   In this work, we introduce and study a new paging problem that models expert management optimization. Our formulation captures both the layered architecture of LLMs and the requirement that experts are cached efficiently. We first present lower bounds on the competitive ratio of both deterministic and randomized algorithms, which show that under mild assumptions, LRU-like policies have good theoretical competitive performance. We then propose a layer-based extension of LRU that is tailored to the problem at hand.   Extensive simulations on both synthetic datasets and actual traces of MoE usage show that our algorithm outperforms policies for the classic paging problem, such as the standard LRU.

**Link**: [arxiv](http://arxiv.org/abs/2509.02408v1),  [pdf](http://arxiv.org/pdf/2509.02408v1)

**Tags**: cs.LG cs.DS 



### Leveraging Approximate Caching for Faster Retrieval-Augmented Generation
**Authors**: Shai Bergman, Zhang Ji, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos

**Updated**: 2025-09-02T13:09:37Z

**Summary**: Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing reliance on expensive vector database lookups. To scale efficiently, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically skewed MedRAG workload reduces database calls by 78.9% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our work highlights that approximate caching is a viable and effective strategy for optimizing RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.05530v2),  [pdf](http://arxiv.org/pdf/2503.05530v2)

**Tags**: cs.DB cs.LG cs.PF 



### Efficient Geometry Compression and Communication for 3D Gaussian   Splatting Point Clouds
**Authors**: Liang Xie, Yanting Li, Luyang Tang, Wei Gao

**Updated**: 2025-09-02T11:58:06Z

**Summary**: Storage and transmission challenges in dynamic 3D scene representation based on the i3DV platform, With increasing scene complexity, the explosive growth of 3D Gaussian data volume causes excessive storage space occupancy. To address this issue, we propose adopting the AVS PCRM reference software for efficient compression of Gaussian point cloud geometry data. The strategy deeply integrates the advanced encoding capabilities of AVS PCRM into the i3DV platform, forming technical complementarity with the original rate-distortion optimization mechanism based on binary hash tables. On one hand, the hash table efficiently caches inter-frame Gaussian point transformation relationships, which allows for high-fidelity transmission within a 40 Mbps bandwidth constraint. On the other hand, AVS PCRM performs precise compression on geometry data. Experimental results demonstrate that the joint framework maintains the advantages of fast rendering and high-quality synthesis in 3D Gaussian technology while achieving significant 10\%-25\% bitrate savings on universal test sets. It provides a superior rate-distortion tradeoff solution for the storage, transmission, and interaction of 3D volumetric video.

**Link**: [arxiv](http://arxiv.org/abs/2509.02232v1),  [pdf](http://arxiv.org/pdf/2509.02232v1)

**Tags**: cs.MM 



### SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache   Channel Pruning
**Authors**: Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu

**Updated**: 2025-09-02T11:29:34Z

**Summary**: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.

**Link**: [arxiv](http://arxiv.org/abs/2508.15212v2),  [pdf](http://arxiv.org/pdf/2508.15212v2)

**Tags**: cs.CL cs.AI cs.LG 



### Batch Query Processing and Optimization for Agentic Workflows
**Authors**: Junyi Shen, Noppanat Wadlom, Yao Lu

**Updated**: 2025-09-02T09:17:40Z

**Summary**: Large Language Models (LLMs) in agentic workflows combine multi-step reasoning, tool use, and collaboration across multiple specialized agents. Existing LLM serving engines optimize individual calls in isolation, while multi-agent frameworks focus on orchestration without system-level performance planning. As a result, repeated prompts, overlapping contexts, and concurrent executions create substantial redundancy and poor GPU utilization, especially in batch analytics scenarios. We introduce Halo, a system that brings batch query processing and optimization into agentic LLM workflows. Halo represents each workflow as a structured query plan DAG and constructs a consolidated graph for batched queries that exposes shared computation. Guided by a cost model that jointly considers prefill and decode costs, cache reuse, and GPU placement, Halo performs plan-level optimization to minimize redundant execution. Its runtime integrates adaptive batching, KV-cache sharing and migration, along with compute-communication overlap to maximize hardware efficiency. Evaluation across six benchmarks shows that Halo achieves up to 18.6x speedup for batch inference and 4.7x throughput improvement under online serving, scaling to workloads of tens of thousands of queries and complex graphs. These gains are achieved without compromising output quality. By unifying query optimization with LLM serving, Halo enables efficient agentic workflows in data analytics and decision-making applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.02121v1),  [pdf](http://arxiv.org/pdf/2509.02121v1)

**Tags**: cs.DB cs.DC 



### Augmented Shuffle Differential Privacy Protocols for Large-Domain   Categorical and Key-Value Data
**Authors**: Takao Murakami, Yuichi Sei, Reo Eriguchi

**Updated**: 2025-09-02T06:40:45Z

**Summary**: Shuffle DP (Differential Privacy) protocols provide high accuracy and privacy by introducing a shuffler who randomly shuffles data in a distributed system. However, most shuffle DP protocols are vulnerable to two attacks: collusion attacks by the data collector and users and data poisoning attacks. A recent study addresses this issue by introducing an augmented shuffle DP protocol, where users do not add noise and the shuffler performs random sampling and dummy data addition. However, it focuses on frequency estimation over categorical data with a small domain and cannot be applied to a large domain due to prohibitively high communication and computational costs.   In this paper, we fill this gap by introducing a novel augmented shuffle DP protocol called the FME (Filtering-with-Multiple-Encryption) protocol. Our FME protocol uses a hash function to filter out unpopular items and then accurately calculates frequencies for popular items. To perform this within one round of interaction between users and the shuffler, our protocol carefully communicates within a system using multiple encryption. We also apply our FME protocol to more advanced KV (Key-Value) statistics estimation with an additional technique to reduce bias. For both categorical and KV data, we prove that our protocol provides computational DP, high robustness to the above two attacks, accuracy, and efficiency. We show the effectiveness of our proposals through comparisons with twelve existing protocols.

**Link**: [arxiv](http://arxiv.org/abs/2509.02004v1),  [pdf](http://arxiv.org/pdf/2509.02004v1)

**Tags**: cs.CR 



### BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure   HBM Accelerators
**Authors**: Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, Chenghong Wang

**Updated**: 2025-09-01T19:49:21Z

**Summary**: While Trusted Execution Environments provide a strong foundation for secure cloud computing, they remain vulnerable to access pattern leakages. Oblivious Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high overhead due to randomized remapping and worst-case padding. We argue these costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory (HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that eavesdropping on HBM is difficult -- even for physical attackers -- as its memory channels are sealed together with processor cores inside the same physical package. Later, Hunt et al. [NSDI'20] show that, with proper isolation, HBM can be turned into an unobservable region where both data and memory traces are hidden. This motivates a rethink of OMAP design with HBM-backed solutions to finally overcome their traditional performance limits. Building on these insights, we present BOLT, a Bandwidth Optimized, Lightning-fast OMAP accelerator that, for the first time, achieves O(1) + O((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache to accelerate oblivious access to large host memory; (ii) a self-hosted architecture that offloads execution and memory control from the host to mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs that maximize resource efficiency. We implement a prototype BOLT on a Xilinx U55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in initialization and query time, respectively, over state-of-the-art OMAPs, including an industry implementation from Facebook.

**Link**: [arxiv](http://arxiv.org/abs/2509.01742v1),  [pdf](http://arxiv.org/pdf/2509.01742v1)

**Tags**: cs.CR cs.AR 



### LLMs cannot spot math errors, even when allowed to peek into the   solution
**Authors**: KV Aditya Srivatsa, Kaushal Kumar Maurya, Ekaterina Kochmar

**Updated**: 2025-09-01T11:41:10Z

**Summary**: Large language models (LLMs) demonstrate remarkable performance on math word problems, yet they have been shown to struggle with meta-reasoning tasks such as identifying errors in student solutions. In this work, we investigate the challenge of locating the first error step in stepwise solutions using two error reasoning datasets: VtG and PRM800K. Our experiments show that state-of-the-art LLMs struggle to locate the first error step in student solutions even when given access to the reference solution. To that end, we propose an approach that generates an intermediate corrected student solution, aligning more closely with the original student's solution, which helps improve performance.

**Link**: [arxiv](http://arxiv.org/abs/2509.01395v1),  [pdf](http://arxiv.org/pdf/2509.01395v1)

**Tags**: cs.CL cs.AI 



### Rotate, Clip, and Partition: Towards W2A4KV4 Quantization by Integrating   Rotation and Learnable Non-uniform Quantizer
**Authors**: Euntae Choi, Sumin Song, Woosang Lim, Sungjoo Yoo

**Updated**: 2025-09-01T07:26:57Z

**Summary**: We propose Rotate, Clip, and Partition (RCP), a quantization-aware training (QAT) approach that first realizes extreme compression of LLMs with W2A4KV4(2-bit weight, 4-bit activation, and 4-bit KV cache) configuration. RCP integrates recent rotation techniques with a novel non-uniform weight quantizer design, by quantitatively analyzing the impact of random rotation on 2-bit weight quantization. Our weight quantizer features Learnable Direct Partitioning (LDP), which introduces learnable parameters to directly learn non-uniform intervals jointly with LLM weights. We also present a specialized GPU kernel that supports GEMV on non-uniform W2A4. Experiments show that RCP can compress LLaMA-2-7B to W2A4KV4 with a loss of only 2.84 WikiText2 ppl and 5.29 times reduced memory footprint. Furthermore, RCP can quantize challenging mobile-targeted LLaMA-3.2 models and domain-specific WizardCoder-7B and MetaMath-7B with no critical problems such as convergence failure and repetition. Code is available at https://github.com/ songsm921/RCP.

**Link**: [arxiv](http://arxiv.org/abs/2502.15779v2),  [pdf](http://arxiv.org/pdf/2502.15779v2)

**Tags**: cs.LG cs.AI cs.CL 



### ProMoE: Fast MoE-based LLM Serving using Proactive Caching
**Authors**: Xiaoniu Song, Zihang Zhong, Rong Chen, Haibo Chen

**Updated**: 2025-09-01T03:51:09Z

**Summary**: The promising applications of large language models are often limited by the constrained GPU memory capacity available on edge devices. Mixture-of-Experts (MoE) models help address this issue by activating only a subset of the model's parameters during computation. This approach allows the unused parameters to be offloaded to host memory, thereby reducing the overall GPU memory demand. However, existing cache-based offloading solutions handle cache misses reactively, which significantly impacts system performance. In this paper, we introduce ProMoE, a novel proactive caching system that utilizes intermediate results to predict subsequent expert usage. By proactively fetching experts in advance, ProMoE eliminates passive cache misses, removes loading time from the critical path, and reduces the performance overhead associated with offloading. Our evaluations demonstrate that ProMoE achieves an average speedup of 2.20x (up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages, respectively, compared to existing offloading solutions.

**Link**: [arxiv](http://arxiv.org/abs/2410.22134v3),  [pdf](http://arxiv.org/pdf/2410.22134v3)

**Tags**: cs.DC cs.AI 



### REFRAG: Rethinking RAG based Decoding
**Authors**: Xiaoqiang Lin, Aritra Ghosh, Bryan Kian Hsiang Low, Anshumali Shrivastava, Vijai Mohan

**Updated**: 2025-09-01T03:31:44Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities in leveraging extensive external knowledge to enhance responses in multi-turn and agentic applications, such as retrieval-augmented generation (RAG). However, processing long-context inputs introduces significant system latency and demands substantial memory for the key-value cache, resulting in reduced throughput and a fundamental trade-off between knowledge enrichment and system efficiency. While minimizing latency for long-context inputs is a primary objective for LLMs, we contend that RAG require specialized consideration. In RAG, much of the LLM context consists of concatenated passages from retrieval, with only a small subset directly relevant to the query. These passages often exhibit low semantic similarity due to diversity or deduplication during re-ranking, leading to block-diagonal attention patterns that differ from those in standard LLM generation tasks. Based on this observation, we argue that most computations over the RAG context during decoding are unnecessary and can be eliminated with minimal impact on performance. To this end, we propose REFRAG, an efficient decoding framework that compresses, senses, and expands to improve latency in RAG applications. By exploiting the sparsity structure, we demonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to previous work) without loss in perplexity. In addition, our optimization framework for large context enables REFRAG to extend the context size of LLMs by 16. We provide rigorous validation of REFRAG across diverse long-context tasks, including RAG, multi-turn conversations, and long document summarization, spanning a wide range of datasets. Experimental results confirm that REFRAG delivers substantial speedup with no loss in accuracy compared to LLaMA models and other state-of-the-art baselines across various context sizes.

**Link**: [arxiv](http://arxiv.org/abs/2509.01092v1),  [pdf](http://arxiv.org/pdf/2509.01092v1)

**Tags**: cs.CL cs.AI cs.LG 



### Bidirectional Sparse Attention for Faster Video Diffusion Training
**Authors**: Chenlu Zhan, Wen Li, Chuyu Shen, Jun Zhang, Suhui Wu, Hao Zhang

**Updated**: 2025-09-01T03:16:52Z

**Summary**: Video diffusion Transformer (DiT) models excel in generative quality but hit major computational bottlenecks when producing high-resolution, long-duration videos. The quadratic complexity of full attention leads to prohibitively high training and inference costs. Full attention inefficiency stems from two key challenges: excessive computation due to the inherent sparsity of Queries and Key-Value pairs, and redundant computation as fixed sparse patterns fail to leverage DiT's dynamic attention. To overcome this limitation, we propose a Bidirectional Sparse Attention (BSA) framework for faster video DiT training, the first to dynamically sparsify both Queries and Key-Value pairs within 3D full attention, thereby substantially improving training and inference efficiency. BSA addresses these issues through two key components. Query sparsity is optimized by selecting the most informative query tokens via semantic similarity and with a dynamic spatial-time training strategy, while KV sparsity is achieved by computing a statistical dynamic threshold to retain only the most salient KV blocks for computation. Extensive experiments demonstrate that BSA significantly accelerates DiT training across long sequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention training, while preserving or even surpassing the generative quality of full attention.

**Link**: [arxiv](http://arxiv.org/abs/2509.01085v1),  [pdf](http://arxiv.org/pdf/2509.01085v1)

**Tags**: cs.CV 



### LLM Serving Optimization with Variable Prefill and Decode Lengths
**Authors**: Meixuan Wang, Yinyu Ye, Zijie Zhou

**Updated**: 2025-08-31T15:09:36Z

**Summary**: We study the problem of serving LLM (Large Language Model) requests where each request has heterogeneous prefill and decode lengths. In LLM serving, the prefill length corresponds to the input prompt length, which determines the initial memory usage in the KV cache. The decode length refers to the number of output tokens generated sequentially, with each additional token increasing the KV cache memory usage by one unit. Given a set of n requests, our goal is to schedule and process them to minimize the total completion time. We show that this problem is NP-hard due to the interplay of batching, placement constraints, precedence relationships, and linearly increasing memory usage. We then analyze commonly used scheduling strategies in practice, such as First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their competitive ratios scale up sublinearly with the memory limit-a significant drawback in real-world settings where memory demand is large. To address this, we propose a novel algorithm based on a new selection metric that efficiently forms batches over time. We prove that this algorithm achieves a constant competitive ratio. Finally, we develop and evaluate a few algorithm variants inspired by this approach, including dynamic programming variants, local search methods, and an LP-based scheduler, demonstrating through comprehensive simulations that they outperform standard baselines while maintaining computational efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2508.06133v2),  [pdf](http://arxiv.org/pdf/2508.06133v2)

**Tags**: math.OC cs.AI cs.LG 



### Accelerating Latency-Critical Applications with AI-Powered   Semi-Automatic Fine-Grained Parallelization on SMT Processors
**Authors**: Denis Los, Igor Petushkov

**Updated**: 2025-08-31T14:51:19Z

**Summary**: Latency-critical applications tend to show low utilization of functional units due to frequent cache misses and mispredictions during speculative execution in high-performance superscalar processors. However, due to significant impact on single-thread performance, Simultaneous Multithreading (SMT) technology is rarely used with heavy threads of latency-critical applications. In this paper, we explore utilization of SMT technology to support fine-grained parallelization of latency-critical applications. Following the advancements in the development of Large Language Models (LLMs), we introduce Aira, an AI-powered Parallelization Adviser. To implement Aira, we extend AI Coding Agent in Cursor IDE with additional tools connected through Model Context Protocol, enabling end-to-end AI Agent for parallelization. Additional connected tools enable LLM-guided hotspot detection, collection of dynamic dependencies with Dynamic Binary Instrumentation, SMT-aware performance simulation to estimate performance gains. We apply Aira with Relic parallel framework for fine-grained task parallelism on SMT cores to parallelize latency-critical benchmarks representing real-world applications used in industry. We show 17% geomean performance gain from parallelization of latency-critical benchmarks using Aira with Relic framework.

**Link**: [arxiv](http://arxiv.org/abs/2509.00883v1),  [pdf](http://arxiv.org/pdf/2509.00883v1)

**Tags**: cs.DC cs.AI 



### Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based   Side-Channel Attacks on Fully Associative Randomized Caches
**Authors**: Chris Cao, Gururaj Saileshwar

**Updated**: 2025-08-31T05:43:55Z

**Summary**: Recent work presented at USENIX Security 2025 (SEC'25) claims that occupancy-based attacks can recover AES keys from the MIRAGE randomized cache. In this paper, we examine these claims and find that they arise from a modeling flaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of MIRAGE uses a constant seed to initialize the random number generator used for global evictions in MIRAGE, causing every AES encryption they trace to evict the same deterministic sequence of cache lines. This artificially creates a highly repeatable timing pattern that is not representative of a realistic implementation of MIRAGE, where eviction sequences vary randomly between encryptions. When we instead randomize the eviction seed for each run, reflecting realistic operation, the correlation between AES T-table accesses and attacker runtimes disappears, and the attack fails. These findings show that the reported leakage is an artifact of incorrect modeling, and not an actual vulnerability in MIRAGE.

**Link**: [arxiv](http://arxiv.org/abs/2508.10431v3),  [pdf](http://arxiv.org/pdf/2508.10431v3)

**Tags**: cs.CR 



### NetGent: Agent-Based Automation of Network Application Workflows
**Authors**: Jaber Daneshamooz, Eugene Vuong, Laasya Koduru, Sanjay Chandrasekaran, Arpit Gupta

**Updated**: 2025-08-30T22:47:15Z

**Summary**: We present NetGent, an AI-agent framework for automating complex application workflows to generate realistic network traffic datasets. Developing generalizable ML models for networking requires data collection from network environments with traffic that results from a diverse set of real-world web applications. However, using existing browser automation tools that are diverse, repeatable, realistic, and efficient remains fragile and costly. NetGent addresses this challenge by allowing users to specify workflows as natural-language rules that define state-dependent actions. These abstract specifications are compiled into nondeterministic finite automata (NFAs), which a state synthesis component translates into reusable, executable code. This design enables deterministic replay, reduces redundant LLM calls through state caching, and adapts quickly when application interfaces change. In experiments, NetGent automated more than 50+ workflows spanning video-on-demand streaming, live video streaming, video conferencing, social media, and web scraping, producing realistic traffic traces while remaining robust to UI variability. By combining the flexibility of language-based agents with the reliability of compiled execution, NetGent provides a scalable foundation for generating the diverse, repeatable datasets needed to advance ML in networking.

**Link**: [arxiv](http://arxiv.org/abs/2509.00625v1),  [pdf](http://arxiv.org/pdf/2509.00625v1)

**Tags**: cs.AI 



### KVComp: A High-Performance, LLM-Aware, Lossy Compression Framework for   KV Cache
**Authors**: Bo Jiang, Taolue Yang, Youyuan Liu, Chengming Zhang, Xubin He, Sian Jin

**Updated**: 2025-08-30T18:25:19Z

**Summary**: Transformer-based large language models (LLMs) demonstrate impressive potential in various practical applications. However, long context inference poses a significant challenge due to the enormous memory requirements of the key-value (KV) cache, which can scale to multiple gigabytes as sequence length and batch size increase. In this paper, we present KVComp, a generic and efficient KV cache management framework optimized for long-text generation that synergistically works with both latency-critical and throughput-critical inference systems. KVComp employs novel lossy compression techniques specifically designed for KV cache data characteristics, featuring careful co-design of compression algorithms and system architecture. Our approach maintains compatibility with the growing nature of KV cache while preserving high computational efficiency. Experimental results show that KVComp achieves on average 47\% and up to 83\% higher memory reduction rate compared to existing methods with little/no model accuracy degradation. Furthermore, KVComp achieves extremely high execution throughput, effectively reducing decompression overhead and, in some cases, even accelerating the matrix-vector multiplication operation and outperform cuBLAS-based attention kernels with less data movement.

**Link**: [arxiv](http://arxiv.org/abs/2509.00579v1),  [pdf](http://arxiv.org/pdf/2509.00579v1)

**Tags**: cs.DC cs.AI 



### Discrete and Continuous Caching Games
**Authors**: ron Jnosik, Csenge Mikls, Dniel G. Simon, Kristf Zlomy

**Updated**: 2025-08-30T14:49:34Z

**Summary**: We investigate a discrete search game called the Multiple Caching Game where the searcher's aim is to find all of a set of $d$ treasures hidden in $n$ locations. Allowed queries are sets of locations of size $k$, and the searcher wins if in all $d$ queries, at least one treasure is hidden in one of the $k$ picked locations. P\'alv\"olgyi showed that the value of the game is at most $\frac{k^d}{\binom{n+d-1}{d}}$, with equality for large enough $n$. We conjecture the exact cases of equality. We also investigate variants of the game and show an example where their values are different, answering a question of P\'alv\"olgyi.   This game is closely related to a continuous variant, Alpern's Caching Game, based on which we define other continous variants of the multiple caching game and examine their values.

**Link**: [arxiv](http://arxiv.org/abs/2310.13777v2),  [pdf](http://arxiv.org/pdf/2310.13777v2)

**Tags**: math.OC math.CO 91A05 



### DiffKV: Differentiated Memory Management for Large Language Models with   Parallel KV Compaction
**Authors**: Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C. S. Lui, Haibo Chen

**Updated**: 2025-08-30T09:35:22Z

**Summary**: Large language models (LLMs) demonstrate remarkable capabilities but face substantial serving costs due to their high memory demands, with the key-value (KV) cache being a primary bottleneck. State-of-the-art KV cache compression techniques, such as quantization and pruning, apply uniform treatment to both keys and values, and discard unimportant tokens entirely, overlooking the fine-grained distinctions in the significance of individual KV cache components. To address such limitations, we introduce \textit{DiffKV}, a novel framework for efficient KV cache compression that exploits three levels of differentiation in the KV cache: (1) the differing impact of keys and values on attention computation, (2) the varying importance of tokens, and (3) the diverse dynamic sparsity patterns across attention heads. These levels of differentiation introduce irregular memory usage patterns across different requests and attention heads, posing significant scalability challenges for memory management. To address these challenges, DiffKV proposes an on-GPU memory manager that compacts fragmented free memory list into contiguous regions in parallel, effectively translating sparsity in the KV cache into performance gains. We evaluate DiffKV on several mainstream LLMs, including the emerging thinking models that generate extended chains of thought. DiffKV is able to compress the KV cache by $2.7\times$ to $5.7\times$ with near-lossless accuracy on complex workloads requiring sophisticated reasoning and long-generation capabilities, and enhances throughput by $1.9\times$ to $5.4\times$. Source codes of DiffKV are available at https://github.com/zyqCSL/DiffKV.

**Link**: [arxiv](http://arxiv.org/abs/2412.03131v3),  [pdf](http://arxiv.org/pdf/2412.03131v3)

**Tags**: cs.LG cs.DC 



### LightVLM: Acceleraing Large Multimodal Models with Pyramid Token Merging   and KV Cache Compression
**Authors**: Lianyu Hu, Fanhua Shang, Wei Feng, Liang Wan

**Updated**: 2025-08-30T08:57:53Z

**Summary**: In this paper, we introduce LightVLM, a simple but effective method that can be seamlessly deployed upon existing Vision-Language Models (VLMs) to greatly accelerate the inference process in a training-free manner. We divide the inference procedure of VLMs into two stages, i.e., encoding and decoding, and propose to simultaneously accelerate VLMs in both stages to largely improve model efficiency. During encoding, we propose pyramid token merging to reduce tokens of different LLM layers in a hierarchical manner by finally only keeping a few dominant tokens to achieve high efficiency. During decoding, aimed at reducing the high latency of outputting long sequences, we propose KV Cache compression to remove unnecessary caches to increase the network throughput. Experimental results show that LightVLM successfully retains 100% performance when only preserving 35% image tokens, and maintains around 98% performance when keeping only 3% image tokens. LightVLM could 2.02$\times$ the network throughput and reduce the prefilling time by 3.65$\times$. LightVLM also makes large VLMs faster again by enabling a heavy model (e.g., InternVL2.5 26B) to infer faster than significantly smaller models (e.g., InternVL2.5 8B), hopefully facilitating the real-world deployment. When generating long text sequences (e.g., 4096 tokens), LightVLM could reduce the inference time by 3.21$\times$, largely outperforming existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.00419v1),  [pdf](http://arxiv.org/pdf/2509.00419v1)

**Tags**: cs.CV 



### GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV   Cache Eviction
**Authors**: Xuelin Li, Xiangqi Jin, Linfeng Zhang

**Updated**: 2025-08-30T06:56:28Z

**Summary**: Efficient Key-Value (KV) cache management is essential for processing long text sequences in large language models (LLMs), where memory constraints often limit performance. Conventional KV eviction strategies, such as top-k selection based on attention scores, depend on static heuristics that fail to capture the evolving implicit dependencies among tokens during inference. To overcome this, we propose GraphKV, a graph-based framework that redefines token selection for KV cache compression. In GraphKV, tokens are modeled as nodes with importance scores, and edges represent their similarity relationships. Through a decay-signal-propagation mechanism, token importance is dynamically updated by propagating information across the graph, enabling adaptive retention of the most contextually significant tokens. GraphKV can be seamlessly utilized in existing KV cache eviction methods such as SnapKV and PyramidKV in a plug-and-play manner. Codes will be released on Github.

**Link**: [arxiv](http://arxiv.org/abs/2509.00388v1),  [pdf](http://arxiv.org/pdf/2509.00388v1)

**Tags**: cs.CL 



### Robust Containment Queries over Collections of Trimmed NURBS Surfaces   via Generalized Winding Numbers
**Authors**: Jacob Spainhour, Kenneth Weiss

**Updated**: 2025-08-29T20:39:21Z

**Summary**: We propose a containment query that is robust to the watertightness of regions bound by trimmed NURBS surfaces, as this property is difficult to guarantee for in-the-wild CAD models. Containment is determined through the generalized winding number (GWN), a mathematical construction that is indifferent to the arrangement of surfaces in the shape. Applying contemporary techniques for the 3D GWN to trimmed NURBS surfaces requires some form of geometric discretization, introducing computational inefficiency to the algorithm and even risking containment misclassifications near the surface. In contrast, our proposed method uses a novel reformulation of the relevant surface integral based on Stokes' theorem, which operates on the boundary and trimming curves as provided through rapidly converging adaptive quadrature. Batches of queries are further accelerated by memoizing (i.e.\ caching and reusing) quadrature node positions and tangents as they are evaluated. We demonstrate that our GWN method is robust to complex trimming geometry in a CAD model, and is accurate up to arbitrary precision at arbitrary distances from the surface. The derived containment query is therefore robust to model non-watertightness while respecting all curved features of the input shape.

**Link**: [arxiv](http://arxiv.org/abs/2504.11435v2),  [pdf](http://arxiv.org/pdf/2504.11435v2)

**Tags**: cs.GR cs.CG cs.NA math.NA 68U05 I.3.5 



### From TLinFormer to TConstFormer: The Leap to Constant-Time Transformer   Attention: Achieving O(1) Computation and O(1) KV Cache during Autoregressive   Inference
**Authors**: Zhongpan Tang

**Updated**: 2025-08-29T19:23:35Z

**Summary**: Although the Transformer has become the cornerstone of modern AI, its autoregressive inference suffers from a linearly growing KV Cache and a computational complexity of O(N^2 d), severely hindering its ability to process ultra-long sequences. To overcome this limitation, this paper introduces the TConstFormer architecture, building upon our previous work, TLinFormer. TConstFormer employs an innovative periodic state update mechanism to achieve a truly constant-size O(1) KV Cache. The computational complexity of this mechanism is also O(1) in an amortized sense: it performs purely constant-time computations for $k-1$ consecutive steps (e.g., $k=256$) and executes a single linear-time global information synchronization only on the $k$-th step. Theoretical calculations and experimental results demonstrate that TConstFormer exhibits an overwhelming advantage over baseline models in terms of speed, memory efficiency, and overall performance on long-text inference tasks. This breakthrough paves the way for efficient and robust streaming language model applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.00202v1),  [pdf](http://arxiv.org/pdf/2509.00202v1)

**Tags**: cs.LG 



### Democratizing Agentic AI with Fast Test-Time Scaling on the Edge
**Authors**: Hao Mark Chen, Zhiwen Mo, Guanxi Lu, Shuang Liang, Lingxiao Ma, Wayne Luk, Hongxiang Fan

**Updated**: 2025-08-29T19:12:04Z

**Summary**: Deploying agentic AI on edge devices is crucial for privacy and responsiveness, but memory constraints typically relegate these systems to smaller Large Language Models (LLMs) with inferior reasoning capabilities. Test-Time Scaling (TTS) can bridge this reasoning gap by dedicating more compute during inference, but existing methods incur prohibitive overhead on edge hardware. To overcome this, we introduce FlashTTS, a serving system that makes TTS practical for memory-constrained LLM reasoning. FlashTTS introduces three synergistic optimizations: (i) Speculative Beam Extension to mitigate system stragglers from irregular reasoning paths; (ii) Asymmetric Multi-Model Memory Allocation to dynamically balance memory between generation and verification; and (iii) Dynamic Prefix-Aware Scheduling to maximize KV-cache reuse. Built as a plug-and-play library for vLLM, FlashTTS enables edge LLMs on a single consumer GPU (24 GB) to match the accuracy and latency of large cloud models. Our evaluation demonstrates that FlashTTS achieves an average 2.2x higher goodput and reduces latency by 38%-68% compared to a vLLM baseline, paving the way for democratized, high-performance agentic AI on edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2509.00195v1),  [pdf](http://arxiv.org/pdf/2509.00195v1)

**Tags**: cs.LG 



### Towards Compute-Optimal Many-Shot In-Context Learning
**Authors**: Shahriar Golchin, Yanfei Chen, Rujun Han, Manan Gandhi, Tianli Yu, Swaroop Mishra, Mihai Surdeanu, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister

**Updated**: 2025-08-29T18:45:22Z

**Summary**: Long-context large language models (LLMs) are able to process inputs containing up to several million tokens. In the scope of in-context learning (ICL), this translates into using hundreds/thousands of demonstrations in the input prompt, enabling many-shot ICL. In practice, a fixed set of demonstrations is often selected at random in many-shot settings due to (1) high inference costs, (2) the benefits of caching and reusing computations, and (3) the similar performance offered by this strategy compared to others when scaled. In this work, we propose two straightforward strategies for demonstration selection in many-shot ICL that improve performance with minimal computational overhead. Our first method combines a small number of demonstrations, selected based on their similarity to each test sample, with a disproportionately larger set of random demonstrations that are cached. The second strategy improves the first by replacing random demonstrations with those selected using centroids derived from test sample representations via k-means clustering. Our experiments with Gemini Pro and Flash across several datasets indicate that our strategies consistently outperform random selection and surpass or match the most performant selection approach while supporting caching and reducing inference cost by up to an order of magnitude. We also show that adjusting the proportion of demonstrations selected based on different criteria can balance performance and inference cost in many-shot ICL.

**Link**: [arxiv](http://arxiv.org/abs/2507.16217v2),  [pdf](http://arxiv.org/pdf/2507.16217v2)

**Tags**: cs.CL cs.AI cs.LG 



### Neural Visibility Cache for Real-Time Light Sampling
**Authors**: Jakub Bokansk, Daniel Meister

**Updated**: 2025-08-29T09:58:17Z

**Summary**: Direct illumination with many lights is an inherent component of physically-based rendering, remaining challenging, especially in real-time scenarios. We propose an online-trained neural cache that stores visibility between lights and 3D positions. We feed light visibility to weighted reservoir sampling (WRS) to sample a light source. The cache is implemented as a fully-fused multilayer perceptron (MLP) with multi-resolution hash-grid encoding, enabling online training and efficient inference on modern GPUs in real-time frame rates. The cache can be seamlessly integrated into existing rendering frameworks and can be used in combination with other real-time techniques such as spatiotemporal reservoir sampling (ReSTIR).

**Link**: [arxiv](http://arxiv.org/abs/2506.05930v2),  [pdf](http://arxiv.org/pdf/2506.05930v2)

**Tags**: cs.GR 



### FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting   Framework for Large Language Models
**Authors**: Zishuai Zhang, Hainan zhang, Weihua Li, Qinnan zhang, jin Dong, Yongxin Tong, Zhiming Zheng

**Updated**: 2025-08-29T07:40:34Z

**Summary**: Private data holds promise for improving LLMs due to its high quality, but its scattered distribution across data silos and the high computational demands of LLMs limit their deployment in federated environments. To address this, the transformer-based federated split models are proposed, which offload most model parameters to the server (or distributed clients) while retaining only a small portion on the client to ensure data privacy. Despite this design, they still face three challenges: 1) Peer-to-peer key encryption struggles to secure transmitted vectors effectively; 2) The auto-regressive nature of LLMs means that federated split learning can only train and infer sequentially, causing high communication overhead; 3) Fixed partition points lack adaptability to downstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure, Efficient, and Adaptive Federated splitting framework based on LLaMA2. First, we inject Gaussian noise into forward-pass hidden states to enable secure end-to-end vector transmission. Second, we employ attention-mask compression and KV cache collaboration to reduce communication costs, accelerating training and inference. Third, we allow users to dynamically adjust the partition points for input/output blocks based on specific task requirements. Experiments on natural language understanding, summarization, and conversational QA tasks show that FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and achieves up to 8x speedups in training and inference. Further analysis of privacy attacks and different partition points also demonstrates the effectiveness of FedSEA-LLaMA in security and adaptability.

**Link**: [arxiv](http://arxiv.org/abs/2505.15683v2),  [pdf](http://arxiv.org/pdf/2505.15683v2)

**Tags**: cs.CL cs.AI cs.DC 



### Deep Multiple Quantization Network on Long Behavior Sequence for   Click-Through Rate Prediction
**Authors**: Zhuoxing Wei, Qi Liu, Qingchen Xie

**Updated**: 2025-08-28T14:58:47Z

**Summary**: In Click-Through Rate (CTR) prediction, the long behavior sequence, comprising the user's long period of historical interactions with items has a vital influence on assessing the user's interest in the candidate item. Existing approaches strike efficiency and effectiveness through a two-stage paradigm: first retrieving hundreds of candidate-related items and then extracting interest intensity vector through target attention. However, we argue that the discrepancy in target attention's relevance distribution between the retrieved items and the full long behavior sequence inevitably leads to a performance decline. To alleviate the discrepancy, we propose the Deep Multiple Quantization Network (DMQN) to process long behavior sequence end-to-end through compressing the long behavior sequence. Firstly, the entire spectrum of long behavior sequence will be quantized into multiple codeword sequences based on multiple independent codebooks. Hierarchical Sequential Transduction Unit is incorporated to facilitate the interaction of reduced codeword sequences. Then, attention between the candidate and multiple codeword sequences will output the interest vector. To enable online serving, intermediate representations of the codeword sequences are cached, significantly reducing latency. Our extensive experiments on both industrial and public datasets confirm the effectiveness and efficiency of DMQN. The A/B test in our advertising system shows that DMQN improves CTR by 3.5% and RPM by 2.0%.

**Link**: [arxiv](http://arxiv.org/abs/2508.20865v1),  [pdf](http://arxiv.org/pdf/2508.20865v1)

**Tags**: cs.IR 



### SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study
**Authors**: Yang Xiang, Fernando Garca-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings

**Updated**: 2025-08-28T08:49:24Z

**Summary**: This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.

**Link**: [arxiv](http://arxiv.org/abs/2508.18250v2),  [pdf](http://arxiv.org/pdf/2508.18250v2)

**Tags**: cs.ET 



### Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport   Equation Solver for Fast Scatter Correction in Multi-Spectral CT
**Authors**: Guoxi Zhu, Li Zhang, Zhiqiang Chen, Hewei Gao

**Updated**: 2025-08-28T08:05:42Z

**Summary**: X-ray scatter has been a serious concern in computed tomography (CT), leading to image artifacts and distortion of CT values. The linear Boltzmann transport equation (LBTE) is recognized as a fast and accurate approach for scatter estimation. However, for multi-spectral CT, it is cumbersome to compute multiple scattering components for different spectra separately when applying LBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum Decomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute X-ray scatter distributions from CT acquisitions at two or more different spectra simultaneously, in a unified framework with no sacrifice in accuracy and nearly no increase in computation in theory. First, a matrixed-spectrum solver of LBTE is obtained by introducing an additional label dimension to expand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a principle of selection of basis using the QR decomposition, along with the above solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter correction method can be established for multi-spectral CT. We validate the effectiveness and accuracy of our method by comparing it with the Monte Carlo method, including the computational time. We also evaluate the scatter correction performance using two different phantoms for fast-kV switching based dual-energy CT, and using an elliptical phantom in a numerical simulation for kV-modulation enabled CT scans, validating that our proposed method can significantly reduce the computational cost at multiple spectra and effectively reduce scatter artifact in reconstructed CT images.

**Link**: [arxiv](http://arxiv.org/abs/2508.20524v1),  [pdf](http://arxiv.org/pdf/2508.20524v1)

**Tags**: physics.med-ph 



### MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content   Caching in Emerging Mega-Constellations
**Authors**: Haoyang Shi, Xing Zhang, Sitong Li, Minghang Li, Xinming Lu, Shaoxiang Xu, Guoquan Wang

**Updated**: 2025-08-28T05:22:25Z

**Summary**: Significant latency in global content delivery primarily arises from insufficient terrestrial infrastructure. Deploying space-based content delivery networks within emerging mega-constellations provides an effective means to bridge the digital divide. However, space-based caching faces constraints from physical-layer dynamics, including dynamic topologies, time-varying inter-satellite link conditions, and limited onboard energy. In addition, existing mechanisms often lack fine-grained content categorization and global optimization. This paper proposes MegaCacheX, a cost-effective hierarchical framework for collaborative content distribution that achieves "Earth-independence" by providing cloud services directly from space. Specifically, data centers in Sun-synchronous orbit act as primary content sources, while caching nodes in mega-constellations and ground stations collaboratively form a distributed edge layer. MegaCacheX optimizes caching strategies by integrating content popularity, regional user distribution, and satellite trajectory predictions. Multi-tier caching nodes serve as service anchors, enabling seamless content delivery with low latency. A prototype implemented on a microservices-based, containerized testbed demonstrates that MegaCacheX reduces global content access latency by about 36% compared to baseline approaches, while maintaining cost efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2508.20433v1),  [pdf](http://arxiv.org/pdf/2508.20433v1)

**Tags**: eess.SY cs.SY 



### Breaking Diffusion with Cache: Exploiting Approximate Caches in   Diffusion Models
**Authors**: Desen Sun, Shuncheng Jie, Sihang Liu

**Updated**: 2025-08-28T04:46:44Z

**Summary**: Diffusion models are a powerful class of generative models that produce content, such as images, from user prompts, but they are computationally intensive. To mitigate this cost, recent academic and industry work has adopted approximate caching, which reuses intermediate states from similar prompts in a cache. While efficient, this optimization introduces new security risks by breaking isolation among users. This work aims to comprehensively assess new security vulnerabilities arising from approximate caching. First, we demonstrate a remote covert channel established with the cache, where a sender injects prompts with special keywords into the cache and a receiver can recover that even after days, to exchange information. Second, we introduce a prompt stealing attack using the cache, where an attacker can recover existing cached prompts based on cache hit prompts. Finally, we introduce a poisoning attack that embeds the attacker's logos into the previously stolen prompt, to render them in future user prompts that hit the cache. These attacks are all performed remotely through the serving system, which indicates severe security vulnerabilities in approximate caching.

**Link**: [arxiv](http://arxiv.org/abs/2508.20424v1),  [pdf](http://arxiv.org/pdf/2508.20424v1)

**Tags**: cs.CR 



### Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full   Context-Aware Linear Attention
**Authors**: Zhongpan Tang

**Updated**: 2025-08-28T04:10:19Z

**Summary**: The Transformer architecture has become a cornerstone of modern artificial intelligence, but its core self-attention mechanism suffers from a complexity bottleneck that scales quadratically with sequence length, severely limiting its application in long-sequence tasks. To address this challenge, existing linear attention methods typically sacrifice model performance by relying on data-agnostic kernel approximations or restrictive context selection. This paper returns to the first principles of connectionism, starting from the topological structure of information flow, to introduce a novel linear attention architecture-\textbf{TLinFormer}. By reconfiguring neuron connection patterns, TLinFormer achieves strict linear complexity while computing exact attention scores and ensuring information flow remains aware of the full historical context. This design aims to bridge the performance gap prevalent between existing efficient attention methods and standard attention. Through a series of experiments, we systematically evaluate the performance of TLinFormer against a standard Transformer baseline on long-sequence inference tasks. The results demonstrate that TLinFormer exhibits overwhelming advantages in key metrics such as \textbf{inference latency}, \textbf{KV cache efficiency}, \textbf{memory footprint}, and \textbf{overall speedup}.

**Link**: [arxiv](http://arxiv.org/abs/2508.20407v1),  [pdf](http://arxiv.org/pdf/2508.20407v1)

**Tags**: cs.LG 



### ASVD: Activation-aware Singular Value Decomposition for Compressing   Large Language Models
**Authors**: Zhihang Yuan, Yuzhang Shang, Yue Song, Dawei Yang, Qiang Wu, Yan Yan, Guangyu Sun

**Updated**: 2025-08-28T03:57:52Z

**Summary**: In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from (1) the distribution variance in the LLM activations and (2) the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner.

**Link**: [arxiv](http://arxiv.org/abs/2312.05821v5),  [pdf](http://arxiv.org/pdf/2312.05821v5)

**Tags**: cs.CL 



### Climber: Toward Efficient Scaling Laws for Large Recommendation Models
**Authors**: Songpei Xu, Shijia Wang, Da Guo, Xianwen Guo, Qiang Xiao, Bin Huang, Guanlin Wu, Chuanjiang Luo

**Updated**: 2025-08-28T01:40:30Z

**Summary**: Transformer-based generative models have achieved remarkable success across domains with various scaling law manifestations. However, our extensive experiments reveal persistent challenges when applying Transformer to recommendation systems: (1) Transformer scaling is not ideal with increased computational resources, due to structural incompatibilities with recommendation-specific features such as multi-source data heterogeneity; (2) critical online inference latency constraints (tens of milliseconds) that intensify with longer user behavior sequences and growing computational demands. We propose Climber, an efficient recommendation framework comprising two synergistic components: the model architecture for efficient scaling and the co-designed acceleration techniques. Our proposed model adopts two core innovations: (1) multi-scale sequence extraction that achieves a time complexity reduction by a constant factor, enabling more efficient scaling with sequence length; (2) dynamic temperature modulation adapting attention distributions to the multi-scenario and multi-behavior patterns. Complemented by acceleration techniques, Climber achieves a 5.15$\times$ throughput gain without performance degradation by adopting a "single user, multiple item" batched processing and memory-efficient Key-Value caching. Comprehensive offline experiments on multiple datasets validate that Climber exhibits a more ideal scaling curve. To our knowledge, this is the first publicly documented framework where controlled model scaling drives continuous online metric growth (12.19\% overall lift) without prohibitive resource costs. Climber has been successfully deployed on Netease Cloud Music, one of China's largest music streaming platforms, serving tens of millions of users daily.

**Link**: [arxiv](http://arxiv.org/abs/2502.09888v2),  [pdf](http://arxiv.org/pdf/2502.09888v2)

**Tags**: cs.IR 



### AdaptCache: KV Cache Native Storage Hierarchy for Low-Delay and   High-Quality Language Model Serving
**Authors**: Shaoting Feng, Hanchen Li, Kuntai Du, Zhuohan Gu, Yuhan Liu, Jiayi Yao, Siddhant Ray, Samuel Shen, Yihua Cheng, Ganesh Ananthanarayanan, Junchen Jiang

**Updated**: 2025-08-28T00:46:51Z

**Summary**: Large language model (LLM) applications often reuse previously processed context, such as chat history and documents, which introduces significant redundant computation. Existing LLM serving systems address such redundant computation by storing the KV caches of processed context and loading the corresponding KV cache when a new request reuses the context. Further, as these LLM applications scale, the total size of KV caches becomes excessively large and requires both DRAM and SSD for full storage.   However, prior work that stores KV caches in DRAM and SSD suffers from high loading delays, as most KV cache hits come from SSD, which is slow to load. To increase the KV cache hit rate on DRAM, we identify lossy KV cache compression as a promising approach. We design a lossy compression system that decides the compression algorithm, compression rate and device placement for each KV cache entry to maximise DRAM hits and minimise loading delay without significantly degrading generation quality. Compared to various static compression baselines across three tasks, our system AdaptCache achieves 1.43--2.4 x delay savings at the same quality and 6--55% quality improvements at the same delay.

**Link**: [arxiv](http://arxiv.org/abs/2509.00105v1),  [pdf](http://arxiv.org/pdf/2509.00105v1)

**Tags**: cs.OS 



### DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource   Allocation and Markov Decision Process in Named Data Networking (NDN)
**Authors**: Fatemeh Roshanzadeh, Hamid Barati, Ali Barati

**Updated**: 2025-08-27T21:05:05Z

**Summary**: Named Data Networking (NDN) represents a transformative shift in network architecture, prioritizing content names over host addresses to enhance data dissemination. Efficient queue and resource management are critical to NDN performance, especially under dynamic and high-traffic conditions. This paper introduces DRR-MDPF, a novel hybrid strategy that integrates the Markov Decision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR) algorithm. MDPF enables routers to intelligently predict optimal forwarding decisions based on key metrics such as bandwidth, delay, and the number of unsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation among competing data flows. The proposed method models each router as a learning agent capable of adjusting its strategies through continuous feedback and probabilistic updates. Simulation results using ndnSIM demonstrate that DRR-MDPF significantly outperforms state-of-the-art strategies including SAF, RFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest Satisfaction Rate (ISR), packet drop rate, content retrieval time, and load balancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and heavy traffic, offering enhanced adaptability and lower computational complexity due to its single-path routing design. Furthermore, its multi-metric decision-making capability enables more accurate interface selection, leading to optimized network performance. Overall, DRR-MDPF serves as an intelligent, adaptive, and scalable queue management solution for NDN, effectively addressing core challenges such as resource allocation, congestion control, and route optimization in dynamic networking environments.

**Link**: [arxiv](http://arxiv.org/abs/2508.20272v1),  [pdf](http://arxiv.org/pdf/2508.20272v1)

**Tags**: cs.NI 



### SpeedMalloc: Improving Multi-threaded Applications via a Lightweight   Core for Memory Allocation
**Authors**: Ruihao Li, Qinzhe Wu, Krishna Kavi, Gayatri Mehta, Jonathan C. Beard, Neeraja J. Yadwadkar, Lizy K. John

**Updated**: 2025-08-27T20:18:37Z

**Summary**: Memory allocation, though constituting only a small portion of the executed code, can have a "butterfly effect" on overall program performance, leading to significant and far-reaching impacts. Despite accounting for just approximately 5% of total instructions, memory allocation can result in up to a 2.7x performance variation depending on the allocator used. This effect arises from the complexity of memory allocation in modern multi-threaded multi-core systems, where allocator metadata becomes intertwined with user data, leading to cache pollution or increased cross-thread synchronization overhead. Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a potential direction to improve the allocator performance and mitigate cache pollution. However, these accelerators currently have limited support for multi-threaded applications, and synchronization between cores and accelerators remains a significant challenge.   We present SpeedMalloc, using a lightweight support-core to process memory allocation tasks in multi-threaded applications. The support-core is a lightweight programmable processor with efficient cross-core data synchronization and houses all allocator metadata in its own caches. This design minimizes cache conflicts with user data and eliminates the need for cross-core metadata synchronization. In addition, using a general-purpose core instead of domain-specific accelerators makes SpeedMalloc capable of adopting new allocator designs. We compare SpeedMalloc with state-of-the-art software and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on multithreaded workloads over these five allocators, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2508.20253v1),  [pdf](http://arxiv.org/pdf/2508.20253v1)

**Tags**: cs.DC cs.AR 



### MODE: Mixture of Document Experts for RAG
**Authors**: Rahul Anand

**Updated**: 2025-08-27T17:45:16Z

**Summary**: Retrieval-Augmented Generation (RAG) often relies on large vector databases and cross-encoders tuned for large-scale corpora, which can be excessive for small, domain-specific collections. We present MODE (Mixture of Document Experts), a lightweight alternative that replaces fine-grained nearest-neighbor search with cluster-and-route retrieval. Documents are embedded, grouped into semantically coherent clusters, and represented by cached centroids. At query time, we route to the top centroid(s) and retrieve context only within those clusters, eliminating external vector-database infrastructure and reranking while keeping latency low. On HotpotQA and SQuAD corpora with 100-500 chunks, MODE matches or exceeds a dense-retrieval baseline in answer quality while reducing end-to-end retrieval time. Ablations show that cluster granularity and multi-cluster routing control the recall/precision trade-off, and that tighter clusters improve downstream accuracy. MODE offers a practical recipe for small and medium corpora where simplicity, speed, and topical focus matter.

**Link**: [arxiv](http://arxiv.org/abs/2509.00100v1),  [pdf](http://arxiv.org/pdf/2509.00100v1)

**Tags**: cs.AI 



### Apple Intelligence Foundation Language Models: Tech Report 2025
**Authors**: Ethan Li, Anders Boesen Lindbo Larsen, Chen Zhang, Xiyou Zhou, Jun Qin, Dian Ang Yap, Narendran Raghavan, Xuankai Chang, Margit Bowler, Eray Yildiz, John Peebles, Hannah Gillis Coleman, Matteo Ronchi, Peter Gray, Keen You, Anthony Spalvieri-Kruse, Ruoming Pang, Reed Li, Yuli Yang, Emad Soroush, Zhiyun Lu, Crystal Xiao, Rong Situ, Jordan Huffaker, David Griffiths, Zaid Ahmed, Peng Zhang, Daniel Parilla, Asaf Liberman, Jennifer Mallalieu, Parsa Mazaheri, Qibin Chen, Manjot Bilkhu, Aonan Zhang, Eric Wang, Dave Nelson, Michael FitzMaurice, Thomas Voice, Jeremy Liu, Josh Shaffer, Shiwen Zhao, Prasanth Yadla, Farzin Rasteh, Pengsheng Guo, Arsalan Farooq, Jeremy Snow, Stephen Murphy, Tao Lei, Minsik Cho, George Horrell, Sam Dodge, Lindsay Hislop, Sumeet Singh, Alex Dombrowski, Aiswarya Raghavan, Sasha Sirovica, Mandana Saebi, Faye Lao, Max Lam, TJ Lu, Zhaoyang Xu, Karanjeet Singh, Marc Kirchner, David Mizrahi, Rajat Arora, Haotian Zhang, Henry Mason, Lawrence Zhou, Yi Hua, Ankur Jain, Felix Bai, Joseph Astrauskas, Floris Weers, Josh Gardner, Mira Chiang, Yi Zhang, Pulkit Agrawal, Tony Sun, Quentin Keunebroek, Matthew Hopkins, Bugu Wu, Tao Jia, Chen Chen, Xingyu Zhou, Nanzhu Wang, Peng Liu, Ruixuan Hou, Rene Rauch, Yuan Gao, Afshin Dehghan, Jonathan Janke, Zirui Wang, Cha Chen, Xiaoyi Ren, Feng Nan, Josh Elman, Dong Yin, Yusuf Goren, Jeff Lai, Yiran Fei, Syd Evans, Muyang Yu, Guoli Yin, Yi Qin, Erin Feldman, Isha Garg, Aparna Rajamani, Karla Vega, Walker Cheng, TJ Collins, Hans Han, Raul Rea Menacho, Simon Yeung, Sophy Lee, Phani Mutyala, Ying-Chang Cheng, Zhe Gan, Sprite Chu, Justin Lazarow, Alessandro Pappalardo, Federico Scozzafava, Jing Lu, Erik Daxberger, Laurent Duchesne, Jen Liu, David Gera, Stefano Ligas, Mary Beth Kery, Brent Ramerth, Ciro Sannino, Marcin Eichner, Haoshuo Huang, Rui Qian, Moritz Schwarzer-Becker, David Riazati, Mingfei Gao, Bailin Wang, Jack Cackler, Yang Lu, Ransen Niu, John Dennison, Guillaume Klein, Jeffrey Bigham, Deepak Gopinath, Navid Shiee, Darren Botten, Guillaume Tartavel, Alex Guillen Garcia, Sam Xu, Victoria MnchJuan Haladjian, Zi-Yi Dou, Matthias Paulik, Adolfo Lopez Mendez, Zhen Li, Hong-You Chen, Chao Jia, Dhaval Doshi, Zhengdong Zhang, Raunak Manjani, Aaron Franklin, Zhile Ren, David Chen, Artsiom Peshko, Nandhitha Raghuram, Hans Hao, Jiulong Shan, Kavya Nerella, Ramsey Tantawi, Vivek Kumar, Saiwen Wang, Brycen Wershing, Bhuwan Dhingra, Dhruti Shah, Ob Adaranijo, Xin Zheng, Tait Madsen, Hadas Kotek, Chang Liu, Yin Xia, Hanli Li, Suma Jayaram, Yanchao Sun, Ahmed Fakhry, Vasileios Saveris, Dustin Withers, Yanghao Li, Alp Aygar, Andres Romero Mier Y Teran, Kaiwei Huang, Mark Lee, Xiujun Li, Yuhong Li, Tyler Johnson, Jay Tang, Joseph Yitan Cheng, Futang Peng, Andrew Walkingshaw, Lucas Guibert, Abhishek Sharma, Cheng Shen, Piotr Maj, Yasutaka Tanaka, You-Cyuan Jhang, Vivian Ma, Tommi Vehvilainen, Kelvin Zou, Jeff Nichols, Matthew Lei, David Qiu, Yihao Qian, Gokul Santhanam, Wentao Wu, Yena Han, Dominik Moritz, Haijing Fu, Mingze Xu, Vivek Rathod, Jian Liu, Louis D'hauwe, Qin Ba, Haitian Sun, Haoran Yan, Philipp Dufter, Anh Nguyen, Yihao Feng, Emma Wang, Keyu He, Rahul Nair, Sanskruti Shah, Jiarui Lu, Patrick Sonnenberg, Jeremy Warner, Yuanzhi Li, Bowen Pan, Ziyi Zhong, Joe Zhou, Sam Davarnia, Olli Saarikivi, Irina Belousova, Rachel Burger, Shang-Chen Wu, Di Feng, Bas Straathof, James Chou, Yuanyang Zhang, Marco Zuliani, Eduardo Jimenez, Abhishek Sundararajan, Xianzhi Du, Chang Lan, Nilesh Shahdadpuri, Peter Grasch, Sergiu Sima, Josh Newnham, Varsha Paidi, Jianyu Wang, Kaelen Haag, Alex Braunstein, Daniele Molinari, Richard Wei, Brenda Yang, Nicholas Lusskin, Joanna Arreaza-Taylor, Meng Cao, Nicholas Seidl, Simon Wang, Jiaming Hu, Yiping Ma, Mengyu Li, Kieran Liu, Hang Su, Sachin Ravi, Chong Wang, Xin Wang, Kevin Smith, Haoxuan You, Binazir Karimzadeh, Rui Li, Jinhao Lei, Wei Fang, Alec Doane, Sam Wiseman, Ismael Fernandez, Jane Li, Andrew Hansen, Javier Movellan, Christopher Neubauer, Hanzhi Zhou, Chris Chaney, Nazir Kamaldin, Valentin Wolf, Fernando Bermdez-Medina, Joris Pelemans, Peter Fu, Howard Xing, Xiang Kong, Wayne Shan, Gabriel Jacoby-Cooper, Dongcai Shen, Tom Gunter, Guillaume Seguin, Fangping Shi, Shiyu Li, Yang Xu, Areeba Kamal, Dan Masi, Saptarshi Guha, Qi Zhu, Jenna Thibodeau, Changyuan Zhang, Rebecca Callahan, Charles Maalouf, Wilson Tsao, Boyue Li, Qingqing Cao, Naomy Sabo, Cheng Leong, Yi Wang, Anupama Mann Anupama, Colorado Reed, Kenneth Jung, Zhifeng Chen, Mohana Prasad Sathya Moorthy, Yifei He, Erik Hornberger, Devi Krishna, Senyu Tong, Michael, Lee, David Haldimann, Yang Zhao, Bowen Zhang, Chang Gao, Chris Bartels, Sushma Rao, Nathalie Tran, Simon Lehnerer, Co Giang, Patrick Dong, Junting Pan, Biyao Wang, Dongxu Li, Mehrdad Farajtabar, Dongseong Hwang, Grace Duanmu, Eshan Verma, Sujeeth Reddy, Qi Shan, Hongbin Gao, Nan Du, Pragnya Sridhar, Forrest Huang, Yingbo Wang, Nikhil Bhendawade, Diane Zhu, Sai Aitharaju, Fred Hohman, Lauren Gardiner, Chung-Cheng Chiu, Yinfei Yang, Alper Kokmen, Frank Chu, Ke Ye, Kaan Elgin, Oron Levy, John Park, Donald Zhang, Eldon Schoop, Nina Wenzel, Michael Booker, Hyunjik Kim, Chinguun Erdenebileg, Nan Dun, Eric Liang Yang, Priyal Chhatrapati, Vishaal Mahtani, Haiming Gang, Kohen Chia, Deepa Seshadri, Donghan Yu, Yan Meng, Kelsey Peterson, Zhen Yang, Yongqiang Wang, Carina Peng, Doug Kang, Anuva Agarwal, Albert Antony, Juan Lao Tebar, Albin Madappally Jose, Regan Poston, Andy De Wang, Gerard Casamayor, Elmira Amirloo, Violet Yao, Wojciech Kryscinski, Kun Duan, Lezhi L

**Updated**: 2025-08-27T16:34:47Z

**Summary**: We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines.   A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute.

**Link**: [arxiv](http://arxiv.org/abs/2507.13575v3),  [pdf](http://arxiv.org/pdf/2507.13575v3)

**Tags**: cs.LG cs.AI 



### Re-thinking Memory-Bound Limitations in CGRAs
**Authors**: Xiangfeng Liu, Zhe Jiang, Anzhen Zhu, Xiaomeng Han, Mingsong Lyu, Qingxu Deng, Nan Guan

**Updated**: 2025-08-27T12:13:45Z

**Summary**: Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators commonly employed to boost performance in workloads with iterative structures. Existing research typically focuses on compiler or architecture optimizations aimed at improving CGRA performance, energy efficiency, flexibility, and area utilization, under the idealistic assumption that kernels can access all data from Scratchpad Memory (SPM). However, certain complex workloads-particularly in fields like graph analytics, irregular database operations, and specialized forms of high-performance computing (e.g., unstructured mesh simulations)-exhibit irregular memory access patterns that hinder CGRA utilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To address this challenge, we conduct a thorough analysis of the underlying causes of performance degradation, then propose a redesigned memory subsystem and refine the memory model. With both microarchitectural and theoretical optimization, our solution can effectively manage irregular memory accesses through CGRA-specific runahead execution mechanism and cache reconfiguration techniques. Our results demonstrate that we can achieve performance comparable to the original SPM-only system while requiring only 1.27% of the storage size. The runahead execution mechanism achieves an average 3.04x speedup (up to 6.91x), with cache reconfiguration technique providing an additional 6.02% improvement, significantly enhancing CGRA performance for irregular memory access patterns.

**Link**: [arxiv](http://arxiv.org/abs/2508.09570v2),  [pdf](http://arxiv.org/pdf/2508.09570v2)

**Tags**: cs.AR B.3.0; B.6.0 



### ERTACache: Error Rectification and Timesteps Adjustment for Efficient   Diffusion
**Authors**: Xurui Peng, Hong Liu, Chenqian Yan, Rui Ma, Fangmin Chen, Xing Wang, Zhihua Wu, Songwei Liu, Mingbao Lin

**Updated**: 2025-08-27T10:37:24Z

**Summary**: Diffusion models suffer from substantial computational overhead due to their inherently iterative inference process. While feature caching offers a promising acceleration strategy by reusing intermediate outputs across timesteps, naive reuse often incurs noticeable quality degradation. In this work, we formally analyze the cumulative error introduced by caching and decompose it into two principal components: feature shift error, caused by inaccuracies in cached outputs, and step amplification error, which arises from error propagation under fixed timestep schedules. To address these issues, we propose ERTACache, a principled caching framework that jointly rectifies both error types. Our method employs an offline residual profiling stage to identify reusable steps, dynamically adjusts integration intervals via a trajectory-aware correction coefficient, and analytically approximates cache-induced errors through a closed-form residual linearization model. Together, these components enable accurate and efficient sampling under aggressive cache reuse. Extensive experiments across standard image and video generation benchmarks show that ERTACache achieves up to 2x inference speedup while consistently preserving or even improving visual quality. Notably, on the state-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x acceleration with minimal VBench degradation, effectively maintaining baseline fidelity while significantly improving efficiency. The code is available at https://github.com/bytedance/ERTACache.

**Link**: [arxiv](http://arxiv.org/abs/2508.21091v1),  [pdf](http://arxiv.org/pdf/2508.21091v1)

**Tags**: cs.CV 



### Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed   Criticality Systems
**Authors**: Diogo Costa, Jose Martins, Sandro Pinto

**Updated**: 2025-08-27T08:30:33Z

**Summary**: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate heterogeneous computing platforms, combining general-purpose processors with specialized accelerators such as AI engines, GPUs, and high-speed networking interfaces. This heterogeneity introduces challenges, as these accelerators and DMA-capable devices act as independent bus masters, directly accessing memory. Consequently, ensuring both security and timing predictability in such environments becomes critical. To address these concerns, the Input-Output Memory Management Unit (IOMMU) plays a key role in mediating and regulating memory access, preventing unauthorized transactions while enforcing isolation and access control policies. While prior work has explored IOMMU-related side-channel vulnerabilities from a security standpoint, its role in performance interference remains largely unexplored. Moreover, many of the same architectural properties that enable side-channel leakage, such as shared TLBs, caching effects, and translation overheads, can also introduce timing unpredictability. In this work, we analyze the contention effects within IOMMU structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how their shared nature introduce unpredictable delays. Our findings reveal that IOMMU-induced interference primarily affects small memory transactions, where translation overheads significantly impact execution time. Additionally, we hypothesize that contention effects arising from IOTLBs exhibit similar behavior across architectures due to shared caching principles, such as prefetching and hierarchical TLB structures. Notably, our experiments show that IOMMU interference can delay DMA transactions by up to 1.79x for lower-size transfers on the Arm SMMUv2 implementation.

**Link**: [arxiv](http://arxiv.org/abs/2508.19670v1),  [pdf](http://arxiv.org/pdf/2508.19670v1)

**Tags**: cs.DC cs.SY eess.SY 



### FiRST: Finetuning Router-Selective Transformers for Input-Adaptive   Latency Reduction
**Authors**: Akriti Jain, Saransh Sharma, Koyel Mukherjee, Soumyabrata Pal

**Updated**: 2025-08-27T04:58:58Z

**Summary**: Auto-regressive Large Language Models (LLMs) demonstrate remarkable performance across different domains such as vision and language processing. However, due to sequential processing through a stack of transformer layers, autoregressive decoding faces significant computation/latency challenges, particularly in resource-constrained environments like mobile and edge devices. Existing approaches in literature that aim to improve latency via skipping layers have two distinct flavors - 1) Early exit, and 2) Input-agnostic heuristics where tokens exit at pre-determined layers irrespective of input sequence. Both the above strategies have limitations - the former cannot be applied to handle KV Caching necessary for speed-ups in modern framework and the latter does not capture the variation in layer importance across tasks or more generally, across input sequences. To address both limitations, we propose FiRST, an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence - the prompt (during the prefill stage) decides which layers will be skipped during decoding. FiRST preserves compatibility with KV caching enabling faster inference while being quality-aware. FiRST is model-agnostic and can be easily enabled on any pre-trained LLM. Our approach reveals that input adaptivity is critical - indeed, different task-specific middle layers play a crucial role in evolving hidden representations depending on tasks. Extensive experiments show that FiRST significantly reduces latency while outperforming other layer selection strategies in quality metics. It retains competitive performance to base model (without layer skipping) and in some cases, even improves upon it. FiRST is thus a promising and efficient solution for LLM deployment in low-resource environments.

**Link**: [arxiv](http://arxiv.org/abs/2410.12513v4),  [pdf](http://arxiv.org/pdf/2410.12513v4)

**Tags**: cs.CL 



### VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D   Space
**Authors**: Lin Li, Zehuan Huang, Haoran Feng, Gengxiong Zhuang, Rui Chen, Chunchao Guo, Lu Sheng

**Updated**: 2025-08-26T17:59:47Z

**Summary**: 3D local editing of specified regions is crucial for game industry and robot interaction. Recent methods typically edit rendered multi-view images and then reconstruct 3D models, but they face challenges in precisely preserving unedited regions and overall coherence. Inspired by structured 3D generative models, we propose VoxHammer, a novel training-free approach that performs precise and coherent editing in 3D latent space. Given a 3D model, VoxHammer first predicts its inversion trajectory and obtains its inverted latents and key-value tokens at each timestep. Subsequently, in the denoising and editing phase, we replace the denoising features of preserved regions with the corresponding inverted latents and cached key-value tokens. By retaining these contextual features, this approach ensures consistent reconstruction of preserved areas and coherent integration of edited parts. To evaluate the consistency of preserved regions, we constructed Edit3D-Bench, a human-annotated dataset comprising hundreds of samples, each with carefully labeled 3D editing regions. Experiments demonstrate that VoxHammer significantly outperforms existing methods in terms of both 3D consistency of preserved regions and overall quality. Our method holds promise for synthesizing high-quality edited paired data, thereby laying the data foundation for in-context 3D generation. See our project page at https://huanngzh.github.io/VoxHammer-Page/.

**Link**: [arxiv](http://arxiv.org/abs/2508.19247v1),  [pdf](http://arxiv.org/pdf/2508.19247v1)

**Tags**: cs.CV 



### Enabling MoE on the Edge via Importance-Driven Expert Scheduling
**Authors**: Guoying Zhu, Meng Li, Haipeng Dai, Xuechen Liu, Weijun Wang, Keran Li, Jun xiao, Ligeng Chen, Wei Wang

**Updated**: 2025-08-26T12:32:09Z

**Summary**: The Mixture of Experts (MoE) architecture has emerged as a key technique for scaling Large Language Models by activating only a subset of experts per query. Deploying MoE on consumer-grade edge hardware, however, is constrained by limited device memory, making dynamic expert offloading essential. Unlike prior work that treats offloading purely as a scheduling problem, we leverage expert importance to guide decisions, substituting low-importance activated experts with functionally similar ones already cached in GPU memory, thereby preserving accuracy. As a result, this design reduces memory usage and data transfer, while largely eliminating PCIe overhead. In addition, we introduce a scheduling policy that maximizes the reuse ratio of GPU-cached experts, further boosting efficiency. Extensive evaluations show that our approach delivers 48% lower decoding latency with over 60% expert cache hit rate, while maintaining nearly lossless accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2508.18983v1),  [pdf](http://arxiv.org/pdf/2508.18983v1)

**Tags**: cs.AI 



### Rethinking Caching for LLM Serving Systems: Beyond Traditional   Heuristics
**Authors**: Jungwoo Kim, Minsang Kim, Jaeheon Lee, Chanwoo Moon, Heejin Kim, Taeho Hwang, Woosuk Chung, Yeseong Kim, Sungjin Lee

**Updated**: 2025-08-26T07:09:09Z

**Summary**: Serving Large Language Models (LLMs) at scale requires meeting strict Service Level Objectives (SLOs) under severe computational and memory constraints. Nevertheless, traditional caching strategies fall short: exact-matching and prefix caches neglect query semantics, while state-of-the-art semantic caches remain confined to traditional intuitions, offering little conceptual departure. Building on this, we present SISO, a semantic caching system that redefines efficiency for LLM serving. SISO introduces centroid-based caching to maximize coverage with minimal memory, locality-aware replacement to preserve high-value entries, and dynamic thresholding to balance accuracy and latency under varying workloads. Across diverse datasets, SISO delivers up to 1.71$\times$ higher hit ratios and consistently stronger SLO attainment compared to state-of-the-art systems.

**Link**: [arxiv](http://arxiv.org/abs/2508.18736v1),  [pdf](http://arxiv.org/pdf/2508.18736v1)

**Tags**: cs.DB cs.LG 



### Physical Autoregressive Model for Robotic Manipulation without Action   Pretraining
**Authors**: Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang

**Updated**: 2025-08-26T03:23:53Z

**Summary**: The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining. The project page is here: https://hcplab-sysu.github.io/PhysicalAutoregressiveModel/

**Link**: [arxiv](http://arxiv.org/abs/2508.09822v3),  [pdf](http://arxiv.org/pdf/2508.09822v3)

**Tags**: cs.CV 



### Krul: Efficient State Restoration for Multi-turn Conversations with   Dynamic Cross-layer KV Sharing
**Authors**: Junyi Wen, Junyuan Liang, Zicong Hong, Wuhui Chen, Ting Cai, Zibin Zheng

**Updated**: 2025-08-26T01:55:27Z

**Summary**: Efficient state restoration in multi-turn conversations with large language models (LLMs) remains a critical challenge, primarily due to the overhead of recomputing or loading full key-value (KV) caches for all historical tokens. To address this, existing approaches compress KV caches across adjacent layers with highly similar attention patterns. However, these methods often apply a fixed compression scheme across all conversations, selecting the same layer pairs for compression without considering conversation-specific attention dynamics. This static strategy overlooks variability in attention pattern similarity across different conversations, which can lead to noticeable accuracy degradation.   We present Krul, a multi-turn LLM inference system that enables accurate and efficient KV cache restoration. Krul dynamically selects compression strategies based on attention similarity across layer pairs and uses a recomputation-loading pipeline to restore the KV cache. It introduces three key innovations: 1) a preemptive compression strategy selector to preserve critical context for future conversation turns and selects a customized strategy for the conversation; 2) a token-wise heterogeneous attention similarity estimator to mitigate the attention similarity computation and storage overhead during model generation; 3) a bubble-free restoration scheduler to reduce potential bubbles brought by the imbalance of recomputing and loading stream due to compressed KV caches. Empirical evaluations on real-world tasks demonstrate that Krul achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x reduction in KV cache storage compared to state-of-the-art methods without compromising generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2507.08045v2),  [pdf](http://arxiv.org/pdf/2507.08045v2)

**Tags**: cs.CL cs.AI 



### AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration
**Authors**: Zhuoping Yang, Jinming Zhuang, Xingzhen Chen, Alex K. Jones, Peipei Zhou

**Updated**: 2025-08-26T01:45:34Z

**Summary**: GPUs are critical for compute-intensive applications, yet emerging workloads such as recommender systems, graph analytics, and data analytics often exceed GPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as external memory, and the GPU-centric approach enables GPU threads to directly issue NVMe requests, further avoiding CPU intervention. However, current GPU-centric approaches adopt synchronous I/O, forcing threads to stall during long communication delays.   We propose AGILE, a lightweight asynchronous GPU-centric I/O library that eliminates deadlock risks and integrates a flexible HBM-based software cache. AGILE overlaps computation and I/O, improving performance by up to 1.88$\times$ across workloads with diverse computation-to-communication ratios. Compared to BaM on DLRM, AGILE achieves up to 1.75$\times$ speedup through efficient design and overlapping; on graph applications, AGILE reduces software cache overhead by up to 3.12$\times$ and NVMe I/O overhead by up to 2.85$\times$; AGILE also lowers per-thread register usage by up to 1.32$\times$.

**Link**: [arxiv](http://arxiv.org/abs/2504.19365v3),  [pdf](http://arxiv.org/pdf/2504.19365v3)

**Tags**: cs.DC 



### Strata: Hierarchical Context Caching for Long Context Language Model   Serving
**Authors**: Zhiqiang Xie, Ziyi Xu, Mark Zhao, Yuwei An, Vikram Sharma Mailthody, Scott Mahlke, Michael Garland, Christos Kozyrakis

**Updated**: 2025-08-26T00:09:03Z

**Summary**: Large Language Models (LLMs) with expanding context windows face significant performance hurdles. While caching key-value (KV) states is critical for avoiding redundant computation, the storage footprint of long-context caches quickly exceeds GPU memory capacity, forcing production systems to adopt hierarchical caching across memory hierarchies. However, transferring large cached contexts back to the GPU introduces severe performance bottlenecks: fragmented I/O from paged layouts prevents full bandwidth utilization, and existing schedulers fail to account for cache-loading delays, leaving systems loading-bound rather than compute-bound. We present Strata, a hierarchical context caching framework designed for efficient long context LLM serving. Strata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling GPU and CPU memory layouts and employs cache-aware request scheduling to balance compute with I/O latency and overlapping unavoidable stalls with complementary tasks. Built on SGLang and deployed in production, Strata achieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache and 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without degrading short-context performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.18572v1),  [pdf](http://arxiv.org/pdf/2508.18572v1)

**Tags**: cs.DC 



### Real-time 3D Visualization of Radiance Fields on Light Field Displays
**Authors**: Jonghyun Kim, Cheng Sun, Michael Stengel, Matthew Chan, Andrew Russell, Jaehyun Jung, Wil Braithwaite, Shalini De Mello, David Luebke

**Updated**: 2025-08-25T22:21:04Z

**Summary**: Radiance fields have revolutionized photo-realistic 3D scene visualization by enabling high-fidelity reconstruction of complex environments, making them an ideal match for light field displays. However, integrating these technologies presents significant computational challenges, as light field displays require multiple high-resolution renderings from slightly shifted viewpoints, while radiance fields rely on computationally intensive volume rendering. In this paper, we propose a unified and efficient framework for real-time radiance field rendering on light field displays. Our method supports a wide range of radiance field representations, including NeRFs, 3D Gaussian Splatting, and Sparse Voxels, within a shared architecture based on a single-pass plane sweeping strategy and caching of shared, non-directional components. The framework generalizes across different scene formats without retraining, and avoids redundant computation across views. We further demonstrate a real-time interactive application on a Looking Glass display, achieving 200+ FPS at 512p across 45 views, enabling seamless, immersive 3D interaction. On standard benchmarks, our method achieves up to 22x speedup compared to independently rendering each view, while preserving image quality.

**Link**: [arxiv](http://arxiv.org/abs/2508.18540v1),  [pdf](http://arxiv.org/pdf/2508.18540v1)

**Tags**: cs.GR eess.IV 



### DiskJoin: Large-scale Vector Similarity Join with SSD
**Authors**: Yanqi Chen, Xiao Yan, Alexandra Meliou, Eric Lo

**Updated**: 2025-08-25T21:07:52Z

**Summary**: Similarity join--a widely used operation in data science--finds all pairs of items that have distance smaller than a threshold. Prior work has explored distributed computation methods to scale similarity join to large data volumes but these methods require a cluster deployment, and efficiency suffers from expensive inter-machine communication. On the other hand, disk-based solutions are more cost-effective by using a single machine and storing the large dataset on high-performance external storage, such as NVMe SSDs, but in these methods the disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin, the first disk-based similarity join algorithm that can process billion-scale vector datasets efficiently on a single machine. DiskJoin improves disk I/O by tailoring the data access patterns to avoid repetitive accesses and read amplification. It also uses main memory as a dynamic cache and carefully manages cache eviction to improve cache hit rate and reduce disk retrieval time. For further acceleration, we adopt a probabilistic pruning technique that can effectively prune a large number of vector pairs from computation. Our evaluation on real-world, large-scale datasets shows that DiskJoin significantly outperforms alternatives, achieving speedups from 50x to 1000x.

**Link**: [arxiv](http://arxiv.org/abs/2508.18494v1),  [pdf](http://arxiv.org/pdf/2508.18494v1)

**Tags**: cs.DB 



### MARM: Unlocking the Future of Recommendation Systems through Memory   Augmentation and Scalable Complexity
**Authors**: Xiao Lv, Jiangxia Cao, Shijie Guan, Xiaoyou Zhou, Zhiguang Qi, Yaqiang Zang, Ming Li, Ben Wang, Kun Gai, Guorui Zhou

**Updated**: 2025-08-25T15:48:28Z

**Summary**: Scaling-law has guided the language model designing for past years, however, it is worth noting that the scaling laws of NLP cannot be directly applied to RecSys due to the following reasons: (1) The amount of training samples and model parameters is typically not the bottleneck for the model. Our recommendation system can generate over 50 billion user samples daily, and such a massive amount of training data can easily allow our model parameters to exceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the stability and robustness of the recommendation system, it is essential to control computational complexity FLOPs carefully. Considering the above differences with LLM, we can draw a conclusion that: for a RecSys model, compared to model parameters, the computational complexity FLOPs is a more expensive factor that requires careful control. In this paper, we propose our milestone work, MARM (Memory Augmented Recommendation Model), which explores a new cache scaling-laws successfully.

**Link**: [arxiv](http://arxiv.org/abs/2411.09425v3),  [pdf](http://arxiv.org/pdf/2411.09425v3)

**Tags**: cs.IR N/A 



### ILRe: Intermediate Layer Retrieval for Context Compression in Causal   Language Models
**Authors**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li

**Updated**: 2025-08-25T10:59:02Z

**Summary**: Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance comparable to or better than the full context in the long context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

**Link**: [arxiv](http://arxiv.org/abs/2508.17892v1),  [pdf](http://arxiv.org/pdf/2508.17892v1)

**Tags**: cs.CL cs.LG 



### SuperGen: An Efficient Ultra-high-resolution Video Generation System   with Sketching and Tiling
**Authors**: Fanjiang Ye, Zepeng Zhao, Yi Mu, Jucheng Shen, Renjie Li, Kaijian Wang, Desen Sun, Saurabh Agarwal, Myungjin Lee, Triston Cao, Aditya Akella, Arvind Krishnamurthy, T. S. Eugene Ng, Zhengzhong Tu, Yuke Wang

**Updated**: 2025-08-25T07:49:17Z

**Summary**: Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SuperGen, an efficient tile-based framework for ultra-high-resolution video generation. SuperGen features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SuperGen incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SuperGen also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations demonstrate that SuperGen harvests the maximum performance gains while achieving high output quality across various benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2508.17756v1),  [pdf](http://arxiv.org/pdf/2508.17756v1)

**Tags**: cs.LG cs.SY eess.SY 



### OmniCache: A Trajectory-Oriented Global Perspective on Training-Free   Cache Reuse for Diffusion Transformer Models
**Authors**: Huanpeng Chu, Wei Wu, Guanyu Fen, Yutao Zhang

**Updated**: 2025-08-25T03:07:02Z

**Summary**: Diffusion models have emerged as a powerful paradigm for generative tasks such as image synthesis and video generation, with Transformer architectures further enhancing performance. However, the high computational cost of diffusion Transformers-stemming from a large number of sampling steps and complex per-step computations-presents significant challenges for real-time deployment. In this paper, we introduce OmniCache, a training-free acceleration method that exploits the global redundancy inherent in the denoising process. Unlike existing methods that determine caching strategies based on inter-step similarities and tend to prioritize reusing later sampling steps, our approach originates from the sampling perspective of DIT models. We systematically analyze the model's sampling trajectories and strategically distribute cache reuse across the entire sampling process. This global perspective enables more effective utilization of cached computations throughout the diffusion trajectory, rather than concentrating reuse within limited segments of the sampling procedure. In addition, during cache reuse, we dynamically estimate the corresponding noise and filter it out to reduce its impact on the sampling direction. Extensive experiments demonstrate that our approach accelerates the sampling process while maintaining competitive generative quality, offering a promising and practical solution for efficient deployment of diffusion-based generative models.

**Link**: [arxiv](http://arxiv.org/abs/2508.16212v2),  [pdf](http://arxiv.org/pdf/2508.16212v2)

**Tags**: cs.CV cs.AI cs.LG 



### ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters   at Scale
**Authors**: Ge Shi, Hanieh Sadri, Qian Wang, Yu Zhang, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-08-25T03:05:16Z

**Summary**: Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large language models to enhance their task-specific performance by selectively tuning the top-activated experts for the task. Serving these fine-tuned models at scale is challenging: deploying merged models in isolation is prohibitively resource-hungry, while existing multi-adapter serving systems with LoRA-style additive updates are incompatible with ESFT's expert-oriented paradigm. We present ExpertWeave, a system that serves multiple ESFT adapters concurrently over a single shared MoE base model, drastically reducing the memory footprint and improving resource utilization. To seamlessly integrate into existing inference pipelines for MoE models with non-intrusive modifications and minimal latency overhead, ExpertWeave introduces a virtual-memory-assisted expert weight manager that co-locates base-model and adapter experts without incurring memory overhead from fragmentation, and a fused kernel for batched rerouting to enable lightweight redirection of tokens to the appropriate experts at runtime. Our evaluations show that ExpertWeave can simultaneously serve multiple adapters of a 16B MoE model on a single accelerator where the baseline runs out of memory, or provides up to 94x more KV cache capacity and achieves up to 18% higher throughput while using comparable resources, all without compromising model accuracy. ExpertWeave maintains low overhead even when scaling to 20 adapters, with a 4-11% latency increase compared with serving the base model alone. Source code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.17624v1),  [pdf](http://arxiv.org/pdf/2508.17624v1)

**Tags**: cs.DC 



### Lightning Fast Caching-based Parallel Denoising Prediction for   Accelerating Talking Head Generation
**Authors**: Jianzhi Long, Wenhao Sun, Rongcheng Tu, Dacheng Tao

**Updated**: 2025-08-25T02:58:39Z

**Summary**: Diffusion-based talking head models generate high-quality, photorealistic videos but suffer from slow inference, limiting practical applications. Existing acceleration methods for general diffusion models fail to exploit the temporal and spatial redundancies unique to talking head generation. In this paper, we propose a task-specific framework addressing these inefficiencies through two key innovations. First, we introduce Lightning-fast Caching-based Parallel denoising prediction (LightningCP), caching static features to bypass most model layers in inference time. We also enable parallel prediction using cached features and estimated noisy latents as inputs, efficiently bypassing sequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to further accelerate attention computations, exploiting the spatial decoupling in talking head videos to restrict attention to dynamic foreground regions. Additionally, we remove reference features in certain layers to bring extra speedup. Extensive experiments demonstrate that our framework significantly improves inference speed while preserving video quality.

**Link**: [arxiv](http://arxiv.org/abs/2509.00052v1),  [pdf](http://arxiv.org/pdf/2509.00052v1)

**Tags**: cs.GR cs.AI cs.CV 



### TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated   Prefill and Decode Inference
**Authors**: Xiaojuan Tang, Fanxu Meng, Pingzhi Tang, Yuxuan Wang, Di Yin, Xing Sun, Muhan Zhang

**Updated**: 2025-08-25T02:24:20Z

**Summary**: Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses key-value states into a low-rank latent vector, caching only this vector to reduce memory. In tensor parallelism (TP), however, attention heads are computed across multiple devices, and each device must load the full cache, eroding the advantage of MLA over Grouped Query Attention (GQA). We propose Tensor-Parallel Latent Attention (TPLA): a scheme that partitions both the latent representation and each head's input dimension across devices, performs attention independently per shard, and then combines results with an all-reduce. TPLA preserves the benefits of a compressed KV cache while unlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in TPLA still leverages the full latent representation, maintaining stronger representational capacity. TPLA is drop-in compatible with models pre-trained using MLA: it supports MLA-style prefilling and enables efficient tensor-parallel decoding without retraining. Applying simple orthogonal transforms -- e.g., the Hadamard transform or PCA -- before TP slicing further mitigates cross-shard interference, yielding minimal accuracy degradation. By reducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x and 1.93x speedups, respectively, at a 32K-token context length while maintaining performance on commonsense and LongBench benchmarks. TPLA can be implemented with FlashAttention-3, enabling practical end-to-end acceleration.

**Link**: [arxiv](http://arxiv.org/abs/2508.15881v2),  [pdf](http://arxiv.org/pdf/2508.15881v2)

**Tags**: cs.LG cs.AI 



### Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD   NPUs
**Authors**: Aadesh Deshmukh, Venkata Yaswanth Raparti, Samuel Hsu

**Updated**: 2025-08-25T01:33:18Z

**Summary**: Transformer-based deep learning models are increasingly deployed on energy, and DRAM bandwidth constrained devices such as laptops and gaming consoles, which presents significant challenges in meeting the latency requirements of the models. The industry is turning to neural processing units (NPUs) for superior performance-per-watt (perf/watt); however, efficiently mapping dynamic attention layers to the NPUs remains a challenging task. For optimizing perf/watt, AMD XDNA NPUs employ software managed caches and share system memory with host. This requires substantial engineering effort to unlock efficient tiling, buffer allocation, and data movement to extract the maximum efficiency from the device. This paper introduces Zen-Attention, a framework that optimizes DRAM bandwidth utilization in the attention layer of models by systematically exploring the complex design space of layer folding, tiling, and data-movement on the interconnect, and the tensor layouts to come up with an optimal solution. Our evaluation includes comparative analysis of end-to-end model latency and specific attention latency in each model. We demonstrate how the framework enhances mapping capabilities by varying input dimensions, which require padding and masking in the attention block. For representative transformer models, the Zen-Attention Framework achieves up to 4x improvement in the latency of the attention block and up to 32% improvement in end-to-end network latency compared to the baseline Unfolded- approaches.

**Link**: [arxiv](http://arxiv.org/abs/2508.17593v1),  [pdf](http://arxiv.org/pdf/2508.17593v1)

**Tags**: cs.DC 



### RT-Cache: Training-Free Retrieval for Real-Time Manipulation
**Authors**: Owen Kwon, Abraham George, Alison Bartsch, Amir Barati Farimani

**Updated**: 2025-08-25T00:15:27Z

**Summary**: Real robots are expected to repeat the same behavior in new environments with very little new data, yet modern controllers either incur heavy per-step inference or require deployment-time fine-tuning. We propose RT-Cache, a training-free retrieval-as-control pipeline that caches diverse image action trajectories in a unified vector memory and, at test time, embeds the current frame to retrieve and replay multi-step snippets, replacing per-step model calls. A hierarchical search keeps lookups sub-second at million scale, shifting cost from compute to storage and enabling real-time control on modest GPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher success and lower completion time than strong retrieval baselines (approximately x2 higher success and ~30% faster in our settings), and a single-episode anchoring study shows immediate adaptation to a more complex, contact-rich task without fine-tuning. RT-Cache turns experience into an append-only memory, offering a simple, scalable path to few-shot deployment today and a foundation for multimodal keys and optional integration with high-level policies. Project page: https://rt-cache.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2505.09040v3),  [pdf](http://arxiv.org/pdf/2505.09040v3)

**Tags**: cs.RO cs.AI cs.CV cs.LG 



### PRISM: Efficient Long-Range Reasoning With Short-Context LLMs
**Authors**: Dulhan Jayalath, James Bradley Wendt, Nicholas Monath, Sandeep Tata, Beliz Gunel

**Updated**: 2025-08-24T22:09:57Z

**Summary**: Long-range tasks demand reasoning over long inputs. However, existing solutions are limited, e.g., long-context models require large compute budgets, parameter-efficient fine-tuning (PEFT) needs training data, and retrieval-augmented generation (RAG) entails complex task-specific designs. Though in-context approaches overcome many of these issues, methods with short-context LLMs are inefficient, trading context for processing more tokens. We introduce PRISM, a highly token-efficient in-context method based on structured schemas that outperforms baselines on diverse tasks with 4x shorter contexts. This approach produces concise outputs and efficiently leverages key-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny contexts without increasing costs or sacrificing quality, and generalizes to new tasks with minimal effort by generating schemas from task descriptions.

**Link**: [arxiv](http://arxiv.org/abs/2412.18914v3),  [pdf](http://arxiv.org/pdf/2412.18914v3)

**Tags**: cs.AI 



### Evaluating Compiler Optimization Impacts on zkVM Performance
**Authors**: Thomas Gassmann, Stefanos Chaliasos, Thodoris Sotiropoulos, Zhendong Su

**Updated**: 2025-08-24T20:51:06Z

**Summary**: Zero-knowledge proofs (ZKPs) are the cornerstone of programmable cryptography. They enable (1) privacy-preserving and verifiable computation across blockchains, and (2) an expanding range of off-chain applications such as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the barrier by turning ZKPs into a drop-in backend for standard compilation pipelines. This lets developers write proof-generating programs in conventional languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits. However, these VMs inherit compiler infrastructures tuned for traditional architectures rather than for proof systems. In particular, standard compiler optimizations assume features that are absent in zkVMs, including cache locality, branch prediction, or instruction-level parallelism. Therefore, their impact on proof generation is questionable.   We present the first systematic study of the impact of compiler optimizations on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero and SP1). While standard LLVM optimization levels do improve zkVM performance (over 40\%), their impact is far smaller than on traditional CPUs, since their decisions rely on hardware features rather than proof constraints. Guided by a fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average +4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains. Our work highlights the potential of compiler-level optimizations for zkVM performance and opens new direction for zkVM-specific passes, backends, and superoptimizers.

**Link**: [arxiv](http://arxiv.org/abs/2508.17518v1),  [pdf](http://arxiv.org/pdf/2508.17518v1)

**Tags**: cs.PF cs.PL 



### Practical Insertion-Only Convex Hull
**Authors**: Ivor van der Hoog, Henrik Reinstdtler, Eva Rotenberg

**Updated**: 2025-08-24T19:28:22Z

**Summary**: Convex hull data structures are fundamental in computational geometry. We study insertion-only data structures, supporting various containment and intersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex hulls can be constructed in linear time using classical algorithms such as Graham scan. We investigate a variety of methods tailored to the insertion-only setting. We explore a broad selection of trade-offs involving robustness, memory access patterns, and space usage, providing an extensive evaluation of both existing and novel techniques. Logarithmic-time methods rely on pointer-based tree structures, which suffer in practice due to poor memory locality. Motivated by this, we develop a vector-based solution inspired by Overmars' logarithmic method. Our structure has worse asymptotic bounds, supporting queries in $O(\log^2 n)$ time, but stores data in $O(\log n)$ contiguous vectors, greatly improving cache performance.   Through empirical evaluation on real-world and synthetic data sets, we uncover surprising trends. Let $h$ denote the size of the convex hull. We show that a na\"ive $O(h)$ insertion-only algorithm based on Graham scan consistently outperforms both theoretical and practical state-of-the-art methods under realistic workloads, even on data sets with rather large convex hulls. While tree-based methods with $O(\log h)$ update times offer solid theoretical guarantees, they are never optimal in practice. In contrast, our vector-based logarithmic method, despite its theoretically inferior bounds, is highly competitive across all tested scenarios. It is optimal whenever the convex hull becomes large.

**Link**: [arxiv](http://arxiv.org/abs/2508.17496v1),  [pdf](http://arxiv.org/pdf/2508.17496v1)

**Tags**: cs.CG 



### TreePO: Bridging the Gap of Policy Optimization and Efficacy and   Inference Efficiency with Heuristic Tree-based Modeling
**Authors**: Yizhi Li, Qingshui Gu, Zhoufutu Wen, Ziniu Li, Tianshun Xing, Shuyue Guo, Tianyu Zheng, Xin Zhou, Xingwei Qu, Wangchunshu Zhou, Zheng Zhang, Wei Shen, Qian Liu, Chenghua Lin, Jian Yang, Ge Zhang, Wenhao Huang

**Updated**: 2025-08-24T16:52:37Z

**Summary**: Recent advancements in aligning large language models via reinforcement learning have achieved remarkable gains in solving complex reasoning problems, but at the cost of expensive on-policy rollouts and limited exploration of diverse reasoning paths. In this work, we introduce TreePO, involving a self-guided rollout algorithm that views sequence generation as a tree-structured searching process. Composed of dynamic tree sampling policy and fixed-length segment decoding, TreePO leverages local uncertainty to warrant additional branches. By amortizing computation across common prefixes and pruning low-value paths early, TreePO essentially reduces the per-update compute burden while preserving or enhancing exploration diversity. Key contributions include: (1) a segment-wise sampling algorithm that alleviates the KV cache burden through contiguous segments and spawns new branches along with an early-stop mechanism; (2) a tree-based segment-level advantage estimation that considers both global and local proximal policy optimization. and (3) analysis on the effectiveness of probability and quality-driven dynamic divergence and fallback strategy. We empirically validate the performance gain of TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours from 22\% up to 43\% of the sampling design for the trained models, meanwhile showing up to 40\% reduction at trajectory-level and 35\% at token-level sampling compute for the existing models. While offering a free lunch of inference efficiency, TreePO reveals a practical path toward scaling RL-based post-training with fewer samples and less compute. Home page locates at https://m-a-p.ai/TreePO.

**Link**: [arxiv](http://arxiv.org/abs/2508.17445v1),  [pdf](http://arxiv.org/pdf/2508.17445v1)

**Tags**: cs.LG cs.CL 



### TinySR: Pruning Diffusion for Real-World Image Super-Resolution
**Authors**: Linwei Dong, Qingnan Fan, Yuhang Yu, Qi Zhang, Jinwei Chen, Yawei Luo, Changqing Zou

**Updated**: 2025-08-24T16:17:33Z

**Summary**: Real-world image super-resolution (Real-ISR) focuses on recovering high-quality images from low-resolution inputs that suffer from complex degradations like noise, blur, and compression. Recently, diffusion models (DMs) have shown great potential in this area by leveraging strong generative priors to restore fine details. However, their iterative denoising process incurs high computational overhead, posing challenges for real-time applications. Although one-step distillation methods, such as OSEDiff and TSD-SR, offer faster inference, they remain fundamentally constrained by their large, over-parameterized model architectures. In this work, we present TinySR, a compact yet effective diffusion model specifically designed for Real-ISR that achieves real-time performance while maintaining perceptual quality. We introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy to facilitate more effective decision-making in depth pruning. We achieve VAE compression through channel pruning, attention removal and lightweight SepConv. We eliminate time- and prompt-related modules and perform pre-caching techniques to further speed up the model. TinySR significantly reduces computational cost and model size, achieving up to 5.68x speedup and 83% parameter reduction compared to its teacher TSD-SR, while still providing high quality results.

**Link**: [arxiv](http://arxiv.org/abs/2508.17434v1),  [pdf](http://arxiv.org/pdf/2508.17434v1)

**Tags**: cs.CV 



### DiCache: Let Diffusion Model Determine Its Own Cache
**Authors**: Jiazi Bu, Pengyang Ling, Yujie Zhou, Yibin Wang, Yuhang Zang, Tong Wu, Dahua Lin, Jiaqi Wang

**Updated**: 2025-08-24T13:30:00Z

**Summary**: Recent years have witnessed the rapid development of acceleration techniques for diffusion models, especially caching-based acceleration methods. These studies seek to answer two fundamental questions: "When to cache" and "How to use cache", typically relying on predefined empirical laws or dataset-level priors to determine the timing of caching and utilizing handcrafted rules for leveraging multi-step caches. However, given the highly dynamic nature of the diffusion process, they often exhibit limited generalizability and fail on outlier samples. In this paper, a strong correlation is revealed between the variation patterns of the shallow-layer feature differences in the diffusion model and those of final model outputs. Moreover, we have observed that the features from different model layers form similar trajectories. Based on these observations, we present DiCache, a novel training-free adaptive caching strategy for accelerating diffusion models at runtime, answering both when and how to cache within a unified framework. Specifically, DiCache is composed of two principal components: (1) Online Probe Profiling Scheme leverages a shallow-layer online probe to obtain a stable prior for the caching error in real time, enabling the model to autonomously determine caching schedules. (2) Dynamic Cache Trajectory Alignment combines multi-step caches based on shallow-layer probe feature trajectory to better approximate the current feature, facilitating higher visual quality. Extensive experiments validate DiCache's capability in achieving higher efficiency and improved visual fidelity over state-of-the-art methods on various leading diffusion models including WAN 2.1, HunyuanVideo for video generation, and Flux for image generation.

**Link**: [arxiv](http://arxiv.org/abs/2508.17356v1),  [pdf](http://arxiv.org/pdf/2508.17356v1)

**Tags**: cs.CV 



### TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained   Elastic Long-Context LLM Serving
**Authors**: Bingyang Wu, Zili Zhang, Yinmin Zhong, Guanzhe Huang, Yibo Zhu, Xuanzhe Liu, Xin Jin

**Updated**: 2025-08-24T05:45:16Z

**Summary**: Prefix caching is crucial to accelerate multi-turn interactions and requests with shared prefixes. At the cluster level, existing prefix caching systems are tightly coupled with request scheduling to optimize cache efficiency and computation performance together, leading to load imbalance, data redundancy, and memory fragmentation of caching systems across instances. To address these issues, memory pooling is promising to shield the scheduler from the underlying cache management so that it can focus on the computation optimization. However, because existing prefix caching systems only transfer increasingly longer prefix caches between instances, they cannot achieve low-latency memory pooling.   To address these problems, we propose a unified segment-level prefix cache pool, TokenLake. It uses a declarative cache interface to expose requests' query tensors, prefix caches, and cache-aware operations to TokenLake for efficient pooling. Powered by this abstraction, TokenLake can manage prefix cache at the segment level with a heavy-hitter-aware load balancing algorithm to achieve better cache load balance, deduplication, and defragmentation. TokenLake also transparently minimizes the communication volume of query tensors and new caches. Based on TokenLake, the scheduler can schedule requests elastically by using existing techniques without considering prefix cache management. Evaluations on real-world workloads show that TokenLake can improve throughput by up to 2.6$\times$ and 2.0$\times$ and boost hit rate by 2.0$\times$ and 2.1$\times$, compared to state-of-the-art cache-aware routing and cache-centric PD-disaggregation solutions, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2508.17219v1),  [pdf](http://arxiv.org/pdf/2508.17219v1)

**Tags**: cs.DC cs.LG 



### DPad: Efficient Diffusion Language Models with Suffix Dropout
**Authors**: Xinhua Chen, Sitao Huang, Cong Guo, Chiyue Wei, Yintao He, Jianyi Zhang, Hai "Helen" Li, Yiran Chen

**Updated**: 2025-08-23T20:28:45Z

**Summary**: Diffusion-based Large Language Models (dLLMs) parallelize text generation by framing decoding as a denoising process, but suffer from high computational overhead since they predict all future suffix tokens at each step while retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a training-free method that restricts attention to a small set of nearby suffix tokens, preserving fidelity while eliminating redundancy. DPad integrates two strategies: (i) a sliding window, which maintains a fixed-length suffix window, and (ii) distance-decay dropout, which deterministically removes distant suffix tokens before attention computation. This simple design is compatible with existing optimizations such as prefix caching and can be implemented with only a few lines of code. Comprehensive evaluations across multiple benchmarks on LLaDA-1.5 and Dream models demonstrate that DPad delivers up to $\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable accuracy, highlighting its potential for efficient and scalable long-sequence inference. Our code is available at https://github.com/Crys-Chen/DPad.

**Link**: [arxiv](http://arxiv.org/abs/2508.14148v2),  [pdf](http://arxiv.org/pdf/2508.14148v2)

**Tags**: cs.CL cs.LG 



### MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices
**Authors**: Nishant Gavhane, Arush Mehrotra, Rohit Chawla, Peter Proenca

**Updated**: 2025-08-23T20:28:32Z

**Summary**: The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices presents significant challenges due to memory constraints. While MoE architectures enable efficient utilization of computational resources by activating only a subset of experts per inference, they require careful memory management to operate efficiently in resource-constrained environments. Traditional heuristic-based expert caching strategies such as MoE-Infinity struggle to maintain high cache hit rates as models parameters scale. In this work, we introduce MoE-Beyond, a learning-based expert activation predictor trained to predict expert activations during autoregressive decoding. By framing the task as a multi-label sequence prediction problem, we train a lightweight transformer model on 66 million expert activation traces extracted from LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor generalizes effectively across unseen prompts from WebGLM-QA dataset [6], achieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that MoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts fit in GPU cache, outperforming heuristic baselines.

**Link**: [arxiv](http://arxiv.org/abs/2508.17137v1),  [pdf](http://arxiv.org/pdf/2508.17137v1)

**Tags**: cs.LG 



### VQL: An End-to-End Context-Aware Vector Quantization Attention for   Ultra-Long User Behavior Modeling
**Authors**: Kaiyuan Li, Yongxiang Tang, Yanhua Cheng, Yong Bai, Yanxiang Zeng, Chao Wang, Xialong Liu, Peng Jiang

**Updated**: 2025-08-23T19:58:18Z

**Summary**: In large-scale recommender systems, ultra-long user behavior sequences encode rich signals of evolving interests. Extending sequence length generally improves accuracy, but directly modeling such sequences in production is infeasible due to latency and memory constraints. Existing solutions fall into two categories: (1) top-k retrieval, which truncates the sequence and may discard most attention mass when L >> k; and (2) encoder-based compression, which preserves coverage but often over-compresses and fails to incorporate key context such as temporal gaps or target-aware signals. Neither class achieves a good balance of low-loss compression, context awareness, and efficiency.   We propose VQL, a context-aware Vector Quantization Attention framework for ultra-long behavior modeling, with three innovations. (1) Key-only quantization: only attention keys are quantized, while values remain intact; we prove that softmax normalization yields an error bound independent of sequence length, and a codebook loss directly supervises quantization quality. This also enables L-free inference via offline caches. (2) Multi-scale quantization: attention heads are partitioned into groups, each with its own small codebook, which reduces quantization error while keeping cache size fixed. (3) Efficient context injection: static features (e.g., item category, modality) are directly integrated, and relative position is modeled via a separable temporal kernel. All context is injected without enlarging the codebook, so cached representations remain query-independent.   Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show that VQL consistently outperforms strong baselines, achieving higher accuracy while reducing inference latency, establishing a new state of the art in balancing accuracy and efficiency for ultra-long sequence recommendation.

**Link**: [arxiv](http://arxiv.org/abs/2508.17125v1),  [pdf](http://arxiv.org/pdf/2508.17125v1)

**Tags**: cs.IR 



### Learned Structure in CARTRIDGES: Keys as Shareable Routers in   Self-Studied Representations
**Authors**: Maurizio Diaz

**Updated**: 2025-08-23T14:20:06Z

**Summary**: A bottleneck for long-context LLM inference is the linearly growing KV cache. Recent work has proposed CARTRIDGES, an approach which leverages offline compute to train a much smaller KV cache than is typically required for a full document (up to 40x less memory usage at inference time). In this paper, we present the first mechanistic exploration of the learned CARTRIDGE key-value cache structure. In particular, we propose that (1) CARTRIDGE keys act as stable, shareable retrieval routers for the compressed corpora and (2) most of the learned compression occurs within the CARTRIDGE value vectors. We present empirical evidence of our routing theory across tasks, model families, and model sizes; for example, we can ablate the learned CARTRIDGE key vectors between tasks with little performance loss. Finally, we propose a slight improvement in initialization called Sampled Chunk Initialization (SCI). We suggest that SCI can lead to faster CARTRIDGE convergence than previously demonstrated in the literature. Our findings lay the groundwork for broader empirical study of CARTRIDGE training optimization which may be crucial for further scaling.

**Link**: [arxiv](http://arxiv.org/abs/2508.17032v1),  [pdf](http://arxiv.org/pdf/2508.17032v1)

**Tags**: cs.LG 



### HiCache: Training-free Acceleration of Diffusion Models via Hermite   Polynomial-based Feature Caching
**Authors**: Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, Linfeng Zhang

**Updated**: 2025-08-23T10:35:16Z

**Summary**: Diffusion models have achieved remarkable success in content generation but suffer from prohibitive computational costs due to iterative sampling. While recent feature caching methods tend to accelerate inference through temporal extrapolation, these methods still suffer from server quality loss due to the failure in modeling the complex dynamics of feature evolution. To solve this problem, this paper presents HiCache, a training-free acceleration framework that fundamentally improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature derivative approximations in Diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials-the potentially theoretically optimal basis for Gaussian-correlated processes. Besides, We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy. Extensive experiments demonstrate HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding baseline quality, maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Core implementation is provided in the appendix, with complete code to be released upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2508.16984v1),  [pdf](http://arxiv.org/pdf/2508.16984v1)

**Tags**: cs.CV 



### Enhancing Memory Efficiency in Large Language Model Training Through   Chronos-aware Pipeline Parallelism
**Authors**: Xinyuan Lin, Chenlu Li, Zongle Huang, Chunyu Wang, Bo Xiao, Huazhong Yang, Shishi Duan, Yongpan Liu

**Updated**: 2025-08-23T08:40:52Z

**Summary**: Larger model sizes and longer sequence lengths have empowered the Large Language Model (LLM) to achieve outstanding performance across various domains. However, this progress brings significant storage capacity challenges for LLM pretraining. High Bandwidth Memory (HBM) is expensive and requires more advanced packaging technologies for capacity expansion, creating an urgent need for memory-efficient scheduling strategies. Yet, prior pipeline parallelism schedules have primarily focused on reducing bubble overhead, often neglecting memory efficiency and lacking compatibility with other memory-efficient strategies. Consequently, these methods struggle to meet the storage demands of storage capacity for next-generation LLM. This work presents ChronosPipe, a Chronos-aware pipeline parallelism for memory-efficient LLM pretraining. The core insight of ChronosPipe is to treat HBM as a fast but small 'cache,' optimizing and exploiting temporal locality within LLM pretraining to enhance HBM utilization. ChronosPipe introduces a pipeline scheduling strategy, Chronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal locality of activations. Additionally, it leverages Chronos-Recomp and Chronos-Offload to efficiently harness the intrinsic temporal locality of activations and weights in Deep Neural Networks. Experiment results show that ChronosPipe can expand the trainable model size by 2.4x while maintaining comparable throughput, achieving 1.5x better than the 1F1B strategy combined with recomputation.

**Link**: [arxiv](http://arxiv.org/abs/2503.03182v2),  [pdf](http://arxiv.org/pdf/2503.03182v2)

**Tags**: cs.DC 



### SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long   Sequences
**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

**Updated**: 2025-08-22T08:45:04Z

**Summary**: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. First, SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models. To improve draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. Our code is available at https://github.com/jycha98/SpecExtend .

**Link**: [arxiv](http://arxiv.org/abs/2505.20776v2),  [pdf](http://arxiv.org/pdf/2505.20776v2)

**Tags**: cs.CL cs.AI cs.LG I.2.7; C.4 



### Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion   Transformers
**Authors**: Shikang Zheng, Liang Feng, Xinyu Wang, Qinming Zhou, Peiliang Cai, Chang Zou, Jiacheng Liu, Yuqi Lin, Junjie Chen, Yue Ma, Linfeng Zhang

**Updated**: 2025-08-22T08:34:03Z

**Summary**: Diffusion Transformers (DiTs) have demonstrated exceptional performance in high-fidelity image and video generation. To reduce their substantial computational costs, feature caching techniques have been proposed to accelerate inference by reusing hidden representations from previous timesteps. However, current methods often struggle to maintain generation quality at high acceleration ratios, where prediction errors increase sharply due to the inherent instability of long-step forecasting. In this work, we adopt an ordinary differential equation (ODE) perspective on the hidden-feature sequence, modeling layer representations along the trajectory as a feature-ODE. We attribute the degradation of existing caching strategies to their inability to robustly integrate historical features under large skipping intervals. To address this, we propose FoCa (Forecast-then-Calibrate), which treats feature caching as a feature-ODE solving problem. Extensive experiments on image synthesis, video generation, and super-resolution tasks demonstrate the effectiveness of FoCa, especially under aggressive acceleration. Without additional training, FoCa achieves near-lossless speedups of 5.50 times on FLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high quality with a 4.53 times speedup on DiT.

**Link**: [arxiv](http://arxiv.org/abs/2508.16211v1),  [pdf](http://arxiv.org/pdf/2508.16211v1)

**Tags**: cs.CV 



### Joint Cache Placement and Routing in Satellite-Terrestrial Edge   Computing Network: A GNN-Enabled DRL Approach
**Authors**: Yuhao Zheng, Ting You, Kejia Peng, Chang Liu

**Updated**: 2025-08-22T07:57:28Z

**Summary**: In this letter, we investigate the problem of joint content caching and routing in satellite-terrestrial edge computing networks (STECNs) to improve caching service for geographically distributed users. To handle the challenges arising from dynamic low Earth orbit (LEO) satellite topologies and heterogeneous content demands, we propose a learning-based framework that integrates graph neural networks (GNNs) with deep reinforcement learning (DRL). The satellite network is represented as a dynamic graph, where GNNs are embedded within the DRL agent to capture spatial and topological dependencies and support routing-aware decision-making. The caching strategy is optimized by formulating the problem as a Markov decision process (MDP) and applying soft actor-critic (SAC) algorithm. Simulation results demonstrate that our approach significantly improves the delivery success rate and reduces communication traffic cost.

**Link**: [arxiv](http://arxiv.org/abs/2508.16184v1),  [pdf](http://arxiv.org/pdf/2508.16184v1)

**Tags**: cs.NI 



### CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing
**Authors**: Yixuan Wang, Haoyu Qiao, Lujun Li, Qingfu Zhu, Wanxiang Che

**Updated**: 2025-08-22T06:55:45Z

**Summary**: Large Language Models (LLMs) confront significant memory challenges due to the escalating KV cache with increasing sequence length. As a crucial technique, existing cross-layer KV cache sharing methods either necessitate modified model architectures with subsequent pre-training or incur significant performance degradation at high compression rates. To mitigate these challenges, we propose CommonKV, a training-free method for cross-layer KV cache compression through adjacent parameters sharing. Inspired by the high similarity observed in cross-layer hidden states, we utilize Singular Value Decomposition (SVD) to achieve weight sharing across adjacent parameters, resulting in a more easily mergeable latent KV cache. Furthermore, we also introduce an adaptive budget allocation strategy. It dynamically assigns compression budgets based on cosine similarity, ensuring that dissimilar caches are not over-compressed. Experiments across multiple backbone models and benchmarks including LongBench and Ruler demonstrate that the proposed method consistently outperforms existing low-rank and cross-layer approaches at various compression ratios. Moreover, we find that the benefits of CommonKV are orthogonal to other quantization and eviction methods. By integrating these approaches, we can ultimately achieve a 98\% compression ratio without significant performance loss.

**Link**: [arxiv](http://arxiv.org/abs/2508.16134v1),  [pdf](http://arxiv.org/pdf/2508.16134v1)

**Tags**: cs.LG cs.AI 



### Lightweight and Fast Real-time Image Enhancement via Decomposition of   the Spatial-aware Lookup Tables
**Authors**: Wontae Kim, Keuntek Lee, Nam Ik Cho

**Updated**: 2025-08-22T06:28:24Z

**Summary**: The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently reduce both model size and runtime by interpolating pre-calculated values at the vertices. However, the 3D LUT methods have a limitation due to their lack of spatial information, as they convert color values on a point-by-point basis. Although spatial-aware 3D LUT methods address this limitation, they introduce additional modules that require a substantial number of parameters, leading to increased runtime as image resolution increases. To address this issue, we propose a method for generating image-adaptive LUTs by focusing on the redundant parts of the tables. Our efficient framework decomposes a 3D LUT into a linear sum of low-dimensional LUTs and employs singular value decomposition (SVD). Furthermore, we enhance the modules for spatial feature fusion to be more cache-efficient. Extensive experimental results demonstrate that our model effectively decreases both the number of parameters and runtime while maintaining spatial awareness and performance.

**Link**: [arxiv](http://arxiv.org/abs/2508.16121v1),  [pdf](http://arxiv.org/pdf/2508.16121v1)

**Tags**: eess.IV cs.CV 



## Keyword: LLM Inference 
 ### Delta Activations: A Representation for Finetuned Large Language Models
**Authors**: Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim

**Updated**: 2025-09-04T17:59:06Z

**Summary**: The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.

**Link**: [arxiv](http://arxiv.org/abs/2509.04442v1),  [pdf](http://arxiv.org/pdf/2509.04442v1)

**Tags**: cs.LG cs.AI cs.CL cs.IR 



### ACING: Actor-Critic for Instruction Learning in Black-Box LLMs
**Authors**: Salma Kharrat, Fares Fourati, Marco Canini

**Updated**: 2025-09-04T17:56:24Z

**Summary**: The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at https://github.com/salmakh1/ACING.

**Link**: [arxiv](http://arxiv.org/abs/2411.12736v2),  [pdf](http://arxiv.org/pdf/2411.12736v2)

**Tags**: cs.CL cs.AI cs.LG cs.SY eess.SY math.OC 



### ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory
**Authors**: Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Zhijian Liu, Zhiting Hu, Lianhui Qin

**Updated**: 2025-09-04T17:54:19Z

**Summary**: While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at https://github.com/matt-seb-ho/arc_memo.

**Link**: [arxiv](http://arxiv.org/abs/2509.04439v1),  [pdf](http://arxiv.org/pdf/2509.04439v1)

**Tags**: cs.AI cs.CL cs.LG 



### From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray   Collimators via Hough Transform
**Authors**: Benjamin El-Zein, Dominik Eckert, Andreas Fieselmann, Christopher Syben, Ludwig Ritschl, Steffen Kappler, Sebastian Stober

**Updated**: 2025-09-04T17:53:45Z

**Summary**: Collimation in X-ray imaging restricts exposure to the region-of-interest (ROI) and minimizes the radiation dose applied to the patient. The detection of collimator shadows is an essential image-based preprocessing step in digital radiography posing a challenge when edges get obscured by scattered X-ray radiation. Regardless, the prior knowledge that collimation forms polygonal-shaped shadows is evident. For this reason, we introduce a deep learning-based segmentation that is inherently constrained to its geometry. We achieve this by incorporating a differentiable Hough transform-based network to detect the collimation borders and enhance its capability to extract the information about the ROI center. During inference, we combine the information of both tasks to enable the generation of refined, line-constrained segmentation masks. We demonstrate robust reconstruction of collimated regions achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real Xray images. While this application involves at most four shadow borders, our method is not fundamentally limited by a specific number of edges.

**Link**: [arxiv](http://arxiv.org/abs/2509.04437v1),  [pdf](http://arxiv.org/pdf/2509.04437v1)

**Tags**: cs.CV physics.med-ph 



### Mergers Fall Short: Non-merger Channels Required for Galactic Heavy   Element Production
**Authors**: Muhammed Saleem, Hsin-Yu Chen, Daniel M. Siegel, Philippe Landry, Jocelyn S. Read, Kaile Wang

**Updated**: 2025-09-04T17:50:13Z

**Summary**: Since the discovery of the binary neutron star merger GW170817 and its associated kilonova, neutron star mergers have been established as a key production channel for $r$-process elements in the Universe. However, various lines of evidence, including the $r$-process abundances inferred from stellar spectra of Milky Way disk stars, suggest that additional channels are needed to fully account for the $r$-process enrichment in the Milky Way. Neutron star-black hole mergers and fast-merging binary neutron star systems are among the leading alternative candidates. In this paper, we combine gravitational-wave observations from LIGO-Virgo-KAGRA with data from short gamma-ray bursts, Galactic pulsars, and Galactic [Eu/Fe] versus [Fe/H] abundance observations to assess the contribution of these mergers to $r$-process enrichment in the Galactic disk. We find that neither neutron star-black hole mergers nor fast-merging binary neutron star populations can serve as the dominant additional channel without generating strong tension with existing observations and theoretical expectations. These results constrain the viable sources of Galactic $r$-process enrichment and underscore the necessity of non-merger production channels.

**Link**: [arxiv](http://arxiv.org/abs/2508.06020v2),  [pdf](http://arxiv.org/pdf/2508.06020v2)

**Tags**: astro-ph.HE gr-qc 



### Assessing Large Language Models in Comprehending and Verifying   Concurrent Programs across Memory Models
**Authors**: Ridhi Jain, Rahul Purandare

**Updated**: 2025-09-04T17:49:22Z

**Summary**: As concurrent programming becomes increasingly prevalent, effectively identifying and addressing concurrency issues such as data races and deadlocks is critical. This study evaluates the performance of several leading large language models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini, and Mistral-AI's Large2, in understanding and analyzing concurrency issues within software programs. Given that relaxed memory models, such as Total Store Order (TSO) and Partial Store Order (PSO), are widely implemented and adapted in modern systems, supported even by commodity architectures like ARM and x86, our evaluation focuses not only on sequentially consistent memory models but also on these relaxed memory models. Specifically, we assess two main aspects: the models' capacity to detect concurrency problems under a sequentially consistent memory model and their ability to verify the correctness conditions of concurrent programs across both sequentially consistent and relaxed memory models. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests designed to evaluate Total Store Order (TSO) and Partial Store Order (PSO) memory models. The experimental results reveal that GPT-4, GPT-4o, and Mistral-AI's Large2 demonstrate a robust understanding of concurrency issues, effectively identifying data races and deadlocks when assessed under a sequentially consistent memory model. However, despite its superior performance, all selected LLMs face significant challenges verifying program correctness under relaxed memory models. These LLMs exhibit limitations in accurately capturing memory ordering constraints, and their current capabilities fall short in verifying even small programs in these complex scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.14326v2),  [pdf](http://arxiv.org/pdf/2501.14326v2)

**Tags**: cs.SE 



### AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?
**Authors**: Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan

**Updated**: 2025-09-04T17:49:20Z

**Summary**: Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.03312v2),  [pdf](http://arxiv.org/pdf/2509.03312v2)

**Tags**: cs.CL cs.MA 



### Text2Cypher: Data Pruning using Hard Example Selection
**Authors**: Makbule Gulcin Ozsoy

**Updated**: 2025-09-04T17:41:13Z

**Summary**: Database query languages such as SQL for relational databases and Cypher for graph databases have been widely adopted. Recent advancements in large language models (LLMs) enable natural language interactions with databases through models like Text2SQL and Text2Cypher. Fine-tuning these models typically requires large, diverse datasets containing non-trivial examples. However, as dataset size increases, the cost of fine-tuning also rises. This makes smaller, high-quality datasets essential for reducing costs for the same or better performance. In this paper, we propose five hard-example selection techniques for pruning the Text2Cypher dataset, aiming to preserve or improve performance while reducing resource usage. Our results show that these hard-example selection approaches can halve training time and costs with minimal impact on performance, and demonstrates that hard-example selection provides a cost-effective solution.

**Link**: [arxiv](http://arxiv.org/abs/2505.05122v2),  [pdf](http://arxiv.org/pdf/2505.05122v2)

**Tags**: cs.DB cs.LG 



### Interpretable Clustering with Adaptive Heterogeneous Causal Structure   Learning in Mixed Observational Data
**Authors**: Wenrui Li, Qinghao Zhang, Xiaowo Wang

**Updated**: 2025-09-04T17:37:35Z

**Summary**: Understanding causal heterogeneity is essential for scientific discovery in domains such as biology and medicine. However, existing methods lack causal awareness, with insufficient modeling of heterogeneity, confounding, and observational constraints, leading to poor interpretability and difficulty distinguishing true causal heterogeneity from spurious associations. We propose an unsupervised framework, HCL (Interpretable Causal Mechanism-Aware Clustering with Adaptive Heterogeneous Causal Structure Learning), that jointly infers latent clusters and their associated causal structures from mixed-type observational data without requiring temporal ordering, environment labels, interventions or other prior knowledge. HCL relaxes the homogeneity and sufficiency assumptions by introducing an equivalent representation that encodes both structural heterogeneity and confounding. It further develops a bi-directional iterative strategy to alternately refine causal clustering and structure learning, along with a self-supervised regularization that balance cross-cluster universality and specificity. Together, these components enable convergence toward interpretable, heterogeneous causal patterns. Theoretically, we show identifiability of heterogeneous causal structures under mild conditions. Empirically, HCL achieves superior performance in both clustering and structure learning tasks, and recovers biologically meaningful mechanisms in real-world single-cell perturbation data, demonstrating its utility for discovering interpretable, mechanism-level causal heterogeneity.

**Link**: [arxiv](http://arxiv.org/abs/2509.04415v1),  [pdf](http://arxiv.org/pdf/2509.04415v1)

**Tags**: cs.LG 



### Closed-Loop Neural Operator-Based Observer of Traffic Density
**Authors**: Alice Harting, Karl Henrik Johansson, Matthieu Barreau

**Updated**: 2025-09-04T17:30:11Z

**Summary**: We consider the problem of traffic density estimation with sparse measurements from stationary roadside sensors. Our approach uses Fourier neural operators to learn macroscopic traffic flow dynamics from high-fidelity data. During inference, the operator functions as an open-loop predictor of traffic evolution. To close the loop, we couple the open-loop operator with a correction operator that combines the predicted density with sparse measurements from the sensors. Simulations with the SUMO software indicate that, compared to open-loop observers, the proposed closed-loop observer exhibits classical closed-loop properties such as robustness to noise and ultimate boundedness of the error. This shows the advantages of combining learned physics with real-time corrections, and opens avenues for accurate, efficient, and interpretable data-driven observers.

**Link**: [arxiv](http://arxiv.org/abs/2504.04873v2),  [pdf](http://arxiv.org/pdf/2504.04873v2)

**Tags**: math.OC cs.LG 



### Few-step Flow for 3D Generation via Marginal-Data Transport Distillation
**Authors**: Zanwei Zhou, Taoran Yi, Jiemin Fang, Chen Yang, Lingxi Xie, Xinggang Wang, Wei Shen, Qi Tian

**Updated**: 2025-09-04T17:24:31Z

**Summary**: Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04406v1),  [pdf](http://arxiv.org/pdf/2509.04406v1)

**Tags**: cs.CV 



### Modular Techniques for Synthetic Long-Context Data Generation in   Language Model Training and Evaluation
**Authors**: Seganrasan Subramanian, Abhigya Verma

**Updated**: 2025-09-04T17:22:16Z

**Summary**: The ability of large language models (LLMs) to process and reason over long textual inputs is critical for a wide range of real-world applications. However, progress in this area is significantly constrained by the absence of high-quality, diverse, and verifiable long-context datasets suitable for both training and evaluation. This work introduces a modular, extensible framework for synthetic long-context data generation via prompt-based interaction with LLMs. The framework supports multiple training and alignment objectives, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). It encompasses four core generation paradigms: multi-turn conversational dialogues, document-grounded input-output pairs, verifiable instruction-response tasks, and long-context reasoning examples. Through templated prompting, a model-agnostic architecture, and metadata-enriched outputs, the proposed approach facilitates scalable, controllable, and purpose-aligned dataset creation for advancing long-context capabilities in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.01185v2),  [pdf](http://arxiv.org/pdf/2509.01185v2)

**Tags**: cs.CL cs.AI 



### Generation of Lognormal Synthetic Lyman-$$ Forest Spectra for   $P_{1D}$ Analysis
**Authors**: Meagan Herbold, Naim Gksel Karaayl, Paul Martini

**Updated**: 2025-09-04T17:18:04Z

**Summary**: The one-dimensional flux power spectrum (P1D) of the Lyman-$\alpha$ forest probes small-scale structure in the intergalactic medium (IGM) and is therefore sensitive to a variety of cosmological and astrophysical parameters. These include the amplitude and shape of the matter power spectrum, the thermal history of the IGM, the sum of neutrino masses, and potential small-scale fluctuations due to the nature of dark matter. However, P1D is also highly sensitive to observational and instrumental systematics, making accurate synthetic spectra essential for validating analyses and quantifying these effects, especially in high-volume surveys like the Dark Energy Spectroscopic Instrument (DESI). We present an efficient lognormal mock framework for generating one-dimensional Lyman-$\alpha$ forest spectra tailored for P1D analysis. Our method captures the redshift evolution of the mean transmitted flux and the scale-dependent shape and amplitude of the one-dimensional flux power spectrum by tuning Gaussian field correlations and transformation parameters. Across the DESI Early Data Release (EDR) redshift range ($2.0 \leq z \leq 3.8$), and a wide range of scales ($10^{-4}$ s km$^{-1} \leq k \leq 1.0$ s km$^{-1}$), our mocks recover the mean flux evolution with redshift to sub-percent accuracy, and the P1D at the percent level. Additionally, we discuss potential extensions of this framework, such as the incorporation of astrophysical contaminants, continuum uncertainties, and instrumental effects. Such improvements would expand its utility in ongoing and upcoming surveys and enable a broader range of validation efforts and systematics studies for P1D inference and precision cosmology.

**Link**: [arxiv](http://arxiv.org/abs/2509.04405v1),  [pdf](http://arxiv.org/pdf/2509.04405v1)

**Tags**: astro-ph.CO 



### No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in   Resume Screening
**Authors**: Kyra Wilson, Mattea Sim, Anna-Maria Gueorguieva, Aylin Caliskan

**Updated**: 2025-09-04T17:16:26Z

**Summary**: In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.

**Link**: [arxiv](http://arxiv.org/abs/2509.04404v1),  [pdf](http://arxiv.org/pdf/2509.04404v1)

**Tags**: cs.CY cs.AI cs.CL cs.HC K.4.2 



### Leveraging Equivariances and Symmetries in the Control Barrier Function   Synthesis
**Authors**: Adrian Wiltz, Dimos V. Dimarogonas

**Updated**: 2025-09-04T17:10:15Z

**Summary**: The synthesis of Control Barrier Functions (CBFs) often involves demanding computations or a meticulous construction. However, structural properties of the system dynamics and constraints have the potential to mitigate these challenges. In this paper, we explore how equivariances in the dynamics, loosely speaking a form of symmetry, can be leveraged in the CBF synthesis. Although CBFs are generally not inherently symmetric, we show how equivariances in the dynamics and symmetries in the constraints induce symmetries in CBFs derived through reachability analysis. This insight allows us to infer their CBF values across the entire domain from their values on a subset, leading to significant computational savings. Interestingly, equivariances can be even leveraged to the CBF synthesis for non-symmetric constraints. Specifically, we show how a partially known CBF can be leveraged together with equivariances to construct a CBF for various new constraints. Throughout the paper, we provide examples illustrating the theoretical findings. Furthermore, a numerical study investigates the computational gains from invoking equivariances into the CBF synthesis.

**Link**: [arxiv](http://arxiv.org/abs/2509.04399v1),  [pdf](http://arxiv.org/pdf/2509.04399v1)

**Tags**: eess.SY cs.RO cs.SY 



### IPA: An Information-Preserving Input Projection Framework for Efficient   Foundation Model Adaptation
**Authors**: Yuan Yin, Shashanka Venkataramanan, Tuan-Hung Vu, Andrei Bursuc, Matthieu Cord

**Updated**: 2025-09-04T17:10:01Z

**Summary**: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly preserves information in the reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen.

**Link**: [arxiv](http://arxiv.org/abs/2509.04398v1),  [pdf](http://arxiv.org/pdf/2509.04398v1)

**Tags**: cs.LG cs.AI 



### Denoising GER: A Noise-Robust Generative Error Correction with LLM for   Speech Recognition
**Authors**: Yanyan Liu, Minqiang Xu, Yihao Chen, Liang He, Lei Fang, Sian Fang, Lin Liu

**Updated**: 2025-09-04T17:03:58Z

**Summary**: In recent years, large language models (LLM) have made significant progress in the task of generation error correction (GER) for automatic speech recognition (ASR) post-processing. However, in complex noisy environments, they still face challenges such as poor adaptability and low information utilization, resulting in limited effectiveness of GER. To address these issues, this paper proposes a noise-robust multi-modal GER framework (Denoising GER). The framework enhances the model's adaptability to different noisy scenarios through a noise-adaptive acoustic encoder and optimizes the integration of multi-modal information via a heterogeneous feature compensation dynamic fusion (HFCDF) mechanism, improving the LLM's utilization of multi-modal information. Additionally, reinforcement learning (RL) training strategies are introduced to enhance the model's predictive capabilities. Experimental results demonstrate that Denoising GER significantly improves accuracy and robustness in noisy environments and exhibits good generalization abilities in unseen noise scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.04392v1),  [pdf](http://arxiv.org/pdf/2509.04392v1)

**Tags**: cs.SD 



### PagedEviction: Structured Block-wise KV Cache Pruning for Efficient   Large Language Model Inference
**Authors**: Krishna Teja Chitty-Venkata, Jie Ye, Xian-He Sun, Anthony Kougkas, Murali Emani, Venkatram Vishwanath, Bogdan Nicolae

**Updated**: 2025-09-04T16:40:01Z

**Summary**: KV caching significantly improves the efficiency of Large Language Model (LLM) inference by storing attention states from previously processed tokens, enabling faster generation of subsequent tokens. However, as sequence length increases, the KV cache quickly becomes a major memory bottleneck. To address this, we propose PagedEviction, a novel fine-grained, structured KV cache pruning strategy that enhances the memory efficiency of vLLM's PagedAttention. Unlike existing approaches that rely on attention-based token importance or evict tokens across different vLLM pages, PagedEviction introduces an efficient block-wise eviction algorithm tailored for paged memory layouts. Our method integrates seamlessly with PagedAttention without requiring any modifications to its CUDA attention kernels. We evaluate PagedEviction across Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models on the LongBench benchmark suite, demonstrating improved memory usage with better accuracy than baselines on long context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.04377v1),  [pdf](http://arxiv.org/pdf/2509.04377v1)

**Tags**: cs.LG 



### Spatially resolving superconductivity in type-II superconductors
**Authors**: Donald M. Evans, Michele Conroy, Lukas Puntigam, Dorina Croitori, Lilian Prodan, Marin Alexe, James O. Douglas, Baptiste Gault, Vladimir Tsurkan

**Updated**: 2025-09-04T16:38:32Z

**Summary**: Superconductivity is identified by the emergence of a macroscopic zero-resistance state, typically inferred from a vanishing four-probe voltage at finite current. That inference assumes spatially uniform conduction-e.g., at least one continuous superconducting path between the current leads and voltage electrodes that sample a finite potential gradient-and can fail if the drive current bypasses the electrodes or if narrow filaments short the current contacts. Here we introduce a methodology to test these assumptions in superconductors, by using spatially resolved measurements of local variations in dc using cryogenic conductive atomic-force microscopy (cAFM). Using Fe(Se,Te) as a model system, we find that despite bulk measurements consistent with a homogeneous superconducting state, the material exhibits a heterogeneous conducting landscape: micrometre-scale superconducting regions coexist with relatively insulating areas. We further show that cAFM resolves conductance fluctuations at 20 K (> TC) that vary between repeated scans, consistent with expectations for short-lived, pre-formed Cooper pairs in the BCS-BEC crossover regime. These results establish cAFM as a practical tool to validate assumptions underlying four-probe transport and underscore the need for direct spatial probes in materials whose macroscopic response can conceal nanoscale inhomogeneity. Accurate identification of macroscopic properties is critical for materials classes like superconductors that are defined by their macroscopic properties.

**Link**: [arxiv](http://arxiv.org/abs/2310.20017v2),  [pdf](http://arxiv.org/pdf/2310.20017v2)

**Tags**: cond-mat.supr-con cond-mat.mtrl-sci cond-mat.str-el 



### Measuring Bias or Measuring the Task: Understanding the Brittle Nature   of LLM Gender Biases
**Authors**: Bufan Gao, Elisa Kreiss

**Updated**: 2025-09-04T16:32:18Z

**Summary**: As LLMs are increasingly applied in socially impactful settings, concerns about gender bias have prompted growing efforts both to measure and mitigate such bias. These efforts often rely on evaluation tasks that differ from natural language distributions, as they typically involve carefully constructed task prompts that overtly or covertly signal the presence of gender bias-related content. In this paper, we examine how signaling the evaluative purpose of a task impacts measured gender bias in LLMs. Concretely, we test models under prompt conditions that (1) make the testing context salient, and (2) make gender-focused content salient. We then assess prompt sensitivity across four task formats with both token-probability and discrete-choice metrics. We find that even minor prompt changes can substantially alter bias outcomes, sometimes reversing their direction entirely. Discrete-choice metrics further tend to amplify bias relative to probabilistic measures. These findings do not only highlight the brittleness of LLM gender bias evaluations but open a new puzzle for the NLP benchmarking and development community: To what extent can well-controlled testing designs trigger LLM ``testing mode'' performance, and what does this mean for the ecological validity of future benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2509.04373v1),  [pdf](http://arxiv.org/pdf/2509.04373v1)

**Tags**: cs.CL 



### MiniCPM4: Ultra-Efficient LLMs on End Devices
**Authors**: MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Baoxi Ji, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Xin Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Lushi Pu, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Zheng Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Xiaoyue Xu, Yukun Yan, Jiarui Yuan, Jinqian Zhang, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Chuyue Zhou, Ge Zhou, Jie Zhou, Wei Zhou, Yanghao Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun

**Updated**: 2025-09-04T16:23:02Z

**Summary**: This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Furthermore, we construct a hybrid reasoning model, MiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning mode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform similar-sized open-source models across benchmarks, with the 8B variants showing significant speed improvements on long sequence understanding and generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.07900v2),  [pdf](http://arxiv.org/pdf/2506.07900v2)

**Tags**: cs.CL cs.AI 



### DynaSaur: Large Language Agents Beyond Predefined Actions
**Authors**: Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou

**Updated**: 2025-09-04T16:22:32Z

**Summary**: Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly scoped environments, it presents two major challenges for real-world, open-ended scenarios: (1) it significantly restricts the planning and acting capabilities of LLM agents, and (2) it requires substantial human effort to enumerate and implement all possible actions, which is impractical in complex environments with a vast number of potential actions. To address these limitations, we propose an LLM agent framework that can dynamically create and compose actions as needed. In this framework, the agent interacts with its environment by generating and executing programs written in a general-purpose programming language. Moreover, generated actions are accumulated over time for future reuse. Our extensive experiments across multiple benchmarks show that this framework significantly improves flexibility and outperforms prior methods that rely on a fixed action set. Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases. Our code can be found in https://github.com/adobe-research/dynasaur.

**Link**: [arxiv](http://arxiv.org/abs/2411.01747v3),  [pdf](http://arxiv.org/pdf/2411.01747v3)

**Tags**: cs.CL 



### SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic   Avatars
**Authors**: Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti

**Updated**: 2025-09-04T16:18:04Z

**Summary**: We present SRWToolkit, an open-source Wizard of Oz toolkit designed to facilitate the rapid prototyping of social robotic avatars powered by local large language models (LLMs). Our web-based toolkit enables multimodal interaction through text input, button-activated speech, and wake-word command. The toolkit offers real-time configuration of avatar appearance, behavior, language, and voice via an intuitive control panel. In contrast to prior works that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and ensures on-device functionality through local LLM inference. In our small-scale user study ($n=11$), participants created and interacted with diverse robotic roles (hospital receptionist, mathematics teacher, and driving assistant), which demonstrated positive outcomes in the toolkit's usability, trust, and user experience. The toolkit enables rapid and efficient development of robot characters customized to researchers' needs, supporting scalable research in human-robot interaction.

**Link**: [arxiv](http://arxiv.org/abs/2509.04356v1),  [pdf](http://arxiv.org/pdf/2509.04356v1)

**Tags**: cs.HC cs.RO 



### Moco: A Learnable Meta Optimizer for Combinatorial Optimization
**Authors**: Tim Dernedde, Daniela Thyssens, Sren Dittrich, Maximilian Stubbemann, Lars Schmidt-Thieme

**Updated**: 2025-09-04T16:15:03Z

**Summary**: Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, defines a lightweight solution construction procedure, guided by a single continuous vector $\theta$ (called heatmap) and learns a neural network to update $\theta$ for a single instance of a COP at inference time. The update is based on various features of the current search state. The training procedure is budget aware, targeting the overall best solution found during the entire search. Moco is a fully learnable meta optimizer not utilizing problem specific heuristics or requiring optimal solutions for training. We test Moco on the Traveling Salesman Problem (TSP) and Maximum Independent Set (MIS) and show that it significantly improves over other heatmap based methods.

**Link**: [arxiv](http://arxiv.org/abs/2402.04915v3),  [pdf](http://arxiv.org/pdf/2402.04915v3)

**Tags**: cs.LG 



### GWTC-4.0: Constraints on the Cosmic Expansion Rate and Modified   Gravitational-wave Propagation
**Authors**: The LIGO Scientific Collaboration, the Virgo Collaboration, the KAGRA Collaboration

**Updated**: 2025-09-04T16:05:54Z

**Summary**: We analyze data from 142 of the 218 gravitational-wave (GW) sources in the fourth LIGO-Virgo-KAGRA Collaboration (LVK) Gravitational-Wave Transient Catalog (GWTC-4.0) to estimate the Hubble constant $H_0$ jointly with the population properties of merging compact binaries. We measure the luminosity distance and redshifted masses of GW sources directly; in contrast, we infer GW source redshifts statistically through i) location of features in the compact object mass spectrum and merger rate evolution, and ii) identifying potential host galaxies in the GW localization volume. Probing the relationship between source luminosity distances and redshifts obtained in this way yields constraints on cosmological parameters. We also constrain parameterized deviations from general relativity which affect GW propagation, specifically those modifying the dependence of a GW signal on the source luminosity distance. Assuming our fiducial model for the source-frame mass distribution and using GW candidates detected up to the end of the fourth observing run (O4a), together with the GLADE+ all-sky galaxy catalog, we estimate $H_0 = 76.6^{+13.0}_{-9.5} (76.6^{+25.2}_{-14.0})$ km s$^{-1}$ Mpc$^{-1}$. This value is reported as a median with 68.3% (90%) symmetric credible interval, and includes combination with the $H_0$ measurement from GW170817 and its electromagnetic counterpart. Using a parametrization of modified GW propagation in terms of the magnitude parameter $\Xi_0$, we estimate $\Xi_0 = 1.2^{+0.8}_{-0.4} (1.2^{+2.4}_{-0.5})$, where $\Xi_0 = 1$ recovers the behavior of general relativity.

**Link**: [arxiv](http://arxiv.org/abs/2509.04348v1),  [pdf](http://arxiv.org/pdf/2509.04348v1)

**Tags**: astro-ph.CO gr-qc 



### Psychologically Enhanced AI Agents
**Authors**: Maciej Besta, Shriram Chandran, Robert Gerstenberger, Mathis Lindner, Marcin Chrapek, Sebastian Hermann Martschat, Taraneh Ghandi, Patrick Iff, Hubert Niewiadomski, Piotr Nyczyk, Jrgen Mller, Torsten Hoefler

**Updated**: 2025-09-04T16:03:03Z

**Summary**: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2509.04343v1),  [pdf](http://arxiv.org/pdf/2509.04343v1)

**Tags**: cs.AI cs.CL cs.CY cs.HC cs.MA 



### Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing   Clinical Notes
**Authors**: Kristina L. Kupferschmidt, Kieran O'Doherty, Joshua A. Skorburg

**Updated**: 2025-09-04T15:59:45Z

**Summary**: Large Language Models (LLMs) are often proposed as tools to streamline clinical documentation, a task viewed as both high-volume and low-risk. However, even seemingly straightforward applications of LLMs raise complex sociotechnical considerations to translate into practice. This case study, conducted at KidsAbility, a pediatric rehabilitation facility in Ontario, Canada examined the use of LLMs to support occupational therapists in reducing documentation burden.We conducted a qualitative study involving 20 clinicians who participated in pilot programs using two AI technologies: a general-purpose proprietary LLM and a bespoke model fine-tuned on proprietary historical documentation.   Our findings reveal that documentation challenges are sociotechnical in nature, shaped by clinical workflows, organizational policies, and system constraints. Four key themes emerged: (1) the heterogeneity of workflows, (2) the documentation burden is systemic and not directly linked to the creation of any single type of documentation, (3) the need for flexible tools and clinician autonomy, and (4) effective implementation requires mutual learning between clinicians and AI systems.   While LLMs show promise in easing documentation tasks, their success will depend on flexible, adaptive integration that supports clinician autonomy. Beyond technical performance, sustained adoption will require training programs and implementation strategies that reflect the complexity of clinical environments.

**Link**: [arxiv](http://arxiv.org/abs/2509.04340v1),  [pdf](http://arxiv.org/pdf/2509.04340v1)

**Tags**: cs.HC 



### Transplant Then Regenerate: A New Paradigm for Text Data Augmentation
**Authors**: Guangzhan Wang, Hongyu Zhang, Beijun Shen, Xiaodong Gu

**Updated**: 2025-09-04T15:58:17Z

**Summary**: Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.

**Link**: [arxiv](http://arxiv.org/abs/2508.14723v2),  [pdf](http://arxiv.org/pdf/2508.14723v2)

**Tags**: cs.CL cs.AI 



### Gravitational-wave inference at GPU speed: A bilby-like nested sampling   kernel within blackjax-ns
**Authors**: Metha Prathaban, David Yallup, James Alvey, Ming Yang, Will Templeton, Will Handley

**Updated**: 2025-09-04T15:55:42Z

**Summary**: We present a GPU-accelerated implementation of the gravitational-wave Bayesian inference pipeline for parameter estimation and model comparison. Specifically, we implement the `acceptance-walk' sampling method, a cornerstone algorithm for gravitational-wave inference within the bilby and dynesty framework. By integrating this trusted kernel with the vectorized blackjax-ns framework, we achieve typical speedups of 20-40x for aligned spin binary black hole analyses, while recovering posteriors and evidences that are statistically identical to the original CPU implementation. This faithful re-implementation of a community-standard algorithm establishes a foundational benchmark for gravitational-wave inference. It quantifies the performance gains attributable solely to the architectural shift to GPUs, creating a vital reference against which future parallel sampling algorithms can be rigorously assessed. This allows for a clear distinction between algorithmic innovation and the inherent speedup from hardware. Our work provides a validated community tool for performing GPU-accelerated nested sampling in gravitational-wave data analyses.

**Link**: [arxiv](http://arxiv.org/abs/2509.04336v1),  [pdf](http://arxiv.org/pdf/2509.04336v1)

**Tags**: gr-qc astro-ph.HE astro-ph.IM 



### Small Changes, Large Consequences: Analyzing the Allocational Fairness   of LLMs in Hiring Contexts
**Authors**: Preethi Seshadri, Hongyu Chen, Sameer Singh, Seraphina Goldfarb-Tarrant

**Updated**: 2025-09-04T15:53:10Z

**Summary**: Large language models (LLMs) are increasingly being deployed in high-stakes applications like hiring, yet their potential for unfair decision-making remains understudied in generative and retrieval settings. In this work, we examine the allocational fairness of LLM-based hiring systems through two tasks that reflect actual HR usage: resume summarization and applicant ranking. By constructing a synthetic resume dataset with controlled perturbations and curating job postings, we investigate whether model behavior differs across demographic groups. Our findings reveal that generated summaries exhibit meaningful differences more frequently for race than for gender perturbations. Models also display non-uniform retrieval selection patterns across demographic groups and exhibit high ranking sensitivity to both gender and race perturbations. Surprisingly, retrieval models can show comparable sensitivity to both demographic and non-demographic changes, suggesting that fairness issues may stem from broader model brittleness. Overall, our results indicate that LLM-based hiring systems, especially in the retrieval stage, can exhibit notable biases that lead to discriminatory outcomes in real-world contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.04316v2),  [pdf](http://arxiv.org/pdf/2501.04316v2)

**Tags**: cs.CL 



### LADSG: Label-Anonymized Distillation and Similar Gradient Substitution   for Label Privacy in Vertical Federated Learning
**Authors**: Zeyu Yan, Yanfei Yao, Xuanbing Wen, Shixiong Zhang, Juli Zhang, Kai Fan

**Updated**: 2025-09-04T15:43:39Z

**Summary**: Vertical Federated Learning (VFL) has emerged as a promising paradigm for collaborative model training across distributed feature spaces, which enables privacy-preserving learning without sharing raw data. However, recent studies have confirmed the feasibility of label inference attacks by internal adversaries. By strategically exploiting gradient vectors and semantic embeddings, attackers-through passive, active, or direct attacks-can accurately reconstruct private labels, leading to catastrophic data leakage. Existing defenses, which typically address isolated leakage vectors or are designed for specific types of attacks, remain vulnerable to emerging hybrid attacks that exploit multiple pathways simultaneously. To bridge this gap, we propose Label-Anonymized Defense with Substitution Gradient (LADSG), a unified and lightweight defense framework for VFL. LADSG first anonymizes true labels via soft distillation to reduce semantic exposure, then generates semantically-aligned substitute gradients to disrupt gradient-based leakage, and finally filters anomalous updates through gradient norm detection. It is scalable and compatible with standard VFL pipelines. Extensive experiments on six real-world datasets show that LADSG reduces the success rates of all three types of label inference attacks by 30-60% with minimal computational overhead, demonstrating its practical effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2506.06742v3),  [pdf](http://arxiv.org/pdf/2506.06742v3)

**Tags**: cs.CR 



### Measurement dependence in a Bell inequality arising from the dynamics of   hidden variables
**Authors**: Sophia M. Walls, Ian J. Ford

**Updated**: 2025-09-04T15:43:33Z

**Summary**: Bell inequalities rely on an assumption that the probabilities of adopting configurations of hidden variables describing a system prior to measurement are independent of the choice of measured physical property, also known as measurement independence. Weakening this assumption could alter the inequalities to accommodate experimental data whilst maintaining local interactions. A natural avenue for achieving this would be to model measurement as a dynamical process involving an interaction between the system and its environment (the measurement apparatus), that drives the hidden variables towards attractors representing measurement outcomes of the observable. Implementing such hidden variable dynamics, we can infer from observed correlations the hidden variable probability distributions before measurement, which differ according to which measurement settings were chosen. We explore various models of the dynamics of the hidden variables under measurement, revealing features that can create measurement dependence and others that can not.

**Link**: [arxiv](http://arxiv.org/abs/2307.07655v2),  [pdf](http://arxiv.org/pdf/2307.07655v2)

**Tags**: quant-ph 



### OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent   Detection
**Authors**: Chen Hu, Shan Luo, Letizia Gionfrida

**Updated**: 2025-09-04T15:42:36Z

**Summary**: Grasping assistance is essential for restoring autonomy in individuals with motor impairments, particularly in unstructured environments where object categories and user intentions are diverse and unpredictable. We present OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp assistance that integrates RGB-D vision, open-vocabulary prompts, and voice commands to enable robust multimodal interaction. To enhance generalization in open environments, OVGrasp incorporates a vision-language foundation model with an open-vocabulary mechanism, allowing zero-shot detection of previously unseen objects without retraining. A multimodal decision-maker further fuses spatial and linguistic cues to infer user intent, such as grasp or release, in multi-object scenarios. We deploy the complete framework on a custom egocentric-view wearable exoskeleton and conduct systematic evaluations on 15 objects across three grasp types. Experimental results with ten participants demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%, outperforming state-of-the-art baselines and achieving improved kinematic alignment with natural hand motion.

**Link**: [arxiv](http://arxiv.org/abs/2509.04324v1),  [pdf](http://arxiv.org/pdf/2509.04324v1)

**Tags**: cs.RO cs.CV 



### EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning
**Authors**: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, XiaoLong Hu, Ge Li

**Updated**: 2025-09-04T15:41:36Z

**Summary**: Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models (LLMs) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes sparse, limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on stronger LLMs for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration.   We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way. This enables LLMs to stably learn from initially unsolved hard problems under sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research.

**Link**: [arxiv](http://arxiv.org/abs/2508.07809v2),  [pdf](http://arxiv.org/pdf/2508.07809v2)

**Tags**: cs.LG 



### Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven   Inference-Time-Scaling Algorithm
**Authors**: Siddharth Mansingh, James Amarel, Ragib Arnab, Arvind Mohan, Kamaljeet Singh, Gerd J. Kunde, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Nathan A. Debardeleben, Ayan Biswas, Diane Oyen, Earl Lawrence

**Updated**: 2025-09-04T15:40:37Z

**Summary**: Partial Differential Equations (PDEs) are the bedrock for modern computational sciences and engineering, and inherently computationally expensive. While PDE foundation models have shown much promise for simulating such complex spatio-temporal phenomena, existing models remain constrained by the pretraining datasets and struggle with auto-regressive rollout performance, especially in out-of-distribution (OOD) cases. Furthermore, they have significant compute and training data requirements which hamper their use in many critical applications. Inspired by recent advances in ``thinking" strategies used in large language models (LLMs), we introduce the first test-time computing (TTC) strategy for PDEs that utilizes computational resources during inference to achieve more accurate predictions with fewer training samples and smaller models. We accomplish this with two types of reward models that evaluate predictions of a stochastic based model for spatio-temporal consistency. We demonstrate this method on compressible Euler-equation simulations from the PDEGym benchmark and show that TTC captures improved predictions relative to standard non-adaptive auto-regressive inference. This TTC framework marks a foundational step towards more advanced reasoning algorithms or PDE modeling, inluding building reinforcement-learning-based approaches, potentially transforming computational workflows in physics and engineering.

**Link**: [arxiv](http://arxiv.org/abs/2509.02846v2),  [pdf](http://arxiv.org/pdf/2509.02846v2)

**Tags**: cs.LG physics.comp-ph 



### Plan Verification for LLM-Based Embodied Task Completion Agents
**Authors**: Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-Tr, Gokhan Tur

**Updated**: 2025-09-04T15:30:53Z

**Summary**: Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.02761v2),  [pdf](http://arxiv.org/pdf/2509.02761v2)

**Tags**: cs.AI 



### Perturbation-Robust Predictive Modeling of Social Effects by Network   Subspace Generalized Linear Models
**Authors**: Jianxiang Wang, Can M. Le, Tianxi Li

**Updated**: 2025-09-04T15:26:07Z

**Summary**: Network-linked data, where multivariate observations are interconnected by a network, are becoming increasingly prevalent in fields such as sociology and biology. These data often exhibit inherent noise and complex relational structures, complicating conventional modeling and statistical inference. Motivated by empirical challenges in analyzing such data sets, this paper introduces a family of network subspace generalized linear models designed for analyzing noisy, network-linked data. We propose a model inference method based on subspace-constrained maximum likelihood, which emphasizes flexibility in capturing network effects and provides a robust inference framework against network perturbations. We establish the asymptotic distributions of the estimators under network perturbations, demonstrating the method's accuracy through extensive simulations involving random network models and deep-learning-based embedding algorithms. The proposed methodology is applied to a comprehensive analysis of a large-scale study on school conflicts, where it identifies significant social effects, offering meaningful and interpretable insights into student behaviors.

**Link**: [arxiv](http://arxiv.org/abs/2410.01163v5),  [pdf](http://arxiv.org/pdf/2410.01163v5)

**Tags**: stat.ME 



### EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation
**Authors**: Yunbo Long, Liming Xu, Lukas Beckenbauer, Yuhan Liu, Alexandra Brintrup

**Updated**: 2025-09-04T15:23:58Z

**Summary**: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04310v1),  [pdf](http://arxiv.org/pdf/2509.04310v1)

**Tags**: cs.AI 



### Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with   Adaptive Exploration
**Authors**: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, Jing Tang

**Updated**: 2025-09-04T15:21:27Z

**Summary**: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.

**Link**: [arxiv](http://arxiv.org/abs/2508.13755v2),  [pdf](http://arxiv.org/pdf/2508.13755v2)

**Tags**: cs.LG cs.AI 



### Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge   in Large Language Models
**Authors**: Juraj Vladika, Mahdi Dhaini, Florian Matthes

**Updated**: 2025-09-04T15:17:50Z

**Summary**: The growing capabilities of Large Language Models (LLMs) show significant potential to enhance healthcare by assisting medical researchers and physicians. However, their reliance on static training data is a major risk when medical recommendations evolve with new research and developments. When LLMs memorize outdated medical knowledge, they can provide harmful advice or fail at clinical reasoning tasks. To investigate this problem, we introduce two novel question-answering (QA) datasets derived from systematic reviews: MedRevQA (16,501 QA pairs covering general biomedical knowledge) and MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over time). Our evaluation of eight prominent LLMs on the datasets reveals consistent reliance on outdated knowledge across all models. We additionally analyze the influence of obsolete pre-training data and training strategies to explain this phenomenon and propose future directions for mitigation, laying the groundwork for developing more current and reliable medical AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.04304v1),  [pdf](http://arxiv.org/pdf/2509.04304v1)

**Tags**: cs.CL cs.AI 



### EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming   in Debt Recovery
**Authors**: Yunbo Long, Yuhan Liu, Liming Xu, Alexandra Brintrup

**Updated**: 2025-09-04T15:06:27Z

**Summary**: Large language model-based chatbots have enhanced engagement in financial negotiations, but their overreliance on passive empathy introduces critical risks in credit collection. While empathy-driven approaches preserve client satisfaction in benign cases, they fail catastrophically against dishonest debtors--individuals who exploit conciliatory tactics to manipulate terms or evade repayment. Blindly prioritizing "customer experience" in such scenarios leads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic exploitation. To address this, we propose EQ-Knight, an LLM agent that dynamically optimizes emotional strategy to defend creditor interests. Unlike naive empathy-centric bots, EQ-Knight integrates emotion memory and game-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and predict debtor emotional states. By analyzing both real-time and historical emotional cues, EQ-Knight strategically counters negative emotions (e.g., aggression, feigned distress) while preserving productive debtor relationships. Experiments demonstrate EQ-Knight's superiority over conventional LLM negotiators: it achieves a 32\% reduction in concession losses without compromising recovery rates, particularly in adversarial cases where debtors weaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce concessions. For credit agencies, EQ-Knight transforms LLMs from high-risk "people-pleasers" into strategic emotion-defenders--balancing emotional intelligence with tactical rigor to enforce accountability and deter exploitation.

**Link**: [arxiv](http://arxiv.org/abs/2503.21080v4),  [pdf](http://arxiv.org/pdf/2503.21080v4)

**Tags**: cs.CL 



### Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow   Real Instructions?
**Authors**: Qinyan Zhang, Xinping Lei, Ruijie Miao, Yu Fu, Haojie Fan, Le Chang, Jiafan Hou, Dingling Zhang, Zhongfei Hou, Ziqiang Yang, Changxin Pu, Fei Hu, Jingkai Liu, Mengyun Liu, Yang Liu, Xiang Gao, Jiaheng Liu, Tong Yang, Zaiyuan Wang, Ge Zhang, Wenhao Huang

**Updated**: 2025-09-04T15:03:02Z

**Summary**: Large Language Models (LLMs) achieve strong performance on diverse tasks but often exhibit cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that measures models Counter-intuitive Abilitytheir capacity to override training-induced biases and comply with adversarial instructions. Inverse IFEval introduces eight types of such challenges, including Question Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a dataset of 1012 high-quality Chinese and English questions across 23 domains, evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing leading LLMs demonstrate the necessity of our proposed Inverse IFEval benchmark. Our findings emphasize that future alignment efforts should not only pursue fluency and factual correctness but also account for adaptability under unconventional contexts. We hope that Inverse IFEval serves as both a diagnostic tool and a foundation for developing methods that mitigate cognitive inertia, reduce overfitting to narrow patterns, and ultimately enhance the instruction-following reliability of LLMs in diverse and unpredictable real-world scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.04292v1),  [pdf](http://arxiv.org/pdf/2509.04292v1)

**Tags**: cs.CL 



### Data-driven mean-field within whole-brain models
**Authors**: Martin Breyton, Viktor Sip, Marmaduke Woodman, Meysam Hashemi, Spase Petkoski, Viktor Jirsa

**Updated**: 2025-09-04T14:59:48Z

**Summary**: Mean-field models provide a link between microscopic neuronal activity and macroscopic brain dynamics. Their derivation depends on simplifying assumptions, such as all-to-all connectivity, limiting their biological realism. To overcome this, we introduce a data-driven framework in which a multi-layer perceptron (MLP) learns the macroscopic dynamics directly from simulations of a network of spiking neurons. The network connection probability serves here as a new parameter, inaccessible to purely analytical treatment, which is validated against ground truth analytical solutions. Through bifurcation analysis on the trained MLP, we demonstrate the existence of new cusp bifurcation that systematically reshapes the system's phase diagram in a degenerate manner with synaptic coupling. By integrating this data-driven mean-field model into a whole-brain computational framework, we show that it extends beyond the macroscopic emergent dynamics generated by the analytical model. For validation, we use simulation-based inference on synthetic functional magnetic resonance imaging (fMRI) data and demonstrate accurate parameter recovery for the novel mean-field model, while the current state-of-the-art models lead to biased estimates. This work presents a flexible and generic framework for building more realistic whole-brain models, bridging the gap between microscale mechanisms and macroscopic brain recordings.

**Link**: [arxiv](http://arxiv.org/abs/2509.02799v2),  [pdf](http://arxiv.org/pdf/2509.02799v2)

**Tags**: q-bio.NC 



### Rapid Word Learning Through Meta In-Context Learning
**Authors**: Wentao Wang, Guangyuan Jiang, Tal Linzen, Brenden M. Lake

**Updated**: 2025-09-04T14:58:09Z

**Summary**: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.14791v4),  [pdf](http://arxiv.org/pdf/2502.14791v4)

**Tags**: cs.CL cs.AI cs.LG 



### Synthesizing Sun-as-a-star flare spectra from high-resolution solar   observations
**Authors**: M. De Wilde, A. G. M. Pietrow, M. K. Druett, A. Pastor Yabar, J. Koza, I. Kontogiannis, O. Andriienko, A. Berlicki, A. R. Brunvoll, J. de la Cruz Rodrguez, J. T. Faber, R. Joshi, D. Kuridze, D. Nbrega-Siverio, L. H. M. Rouppe van der Voort, J. Rybk, E. Scullion, A. M. Silva, Z. Vashalomidze, A. Vicente Arvalo, A. Winiewska, R. Yadav, T. V. Zaqarashvili, J. Zbinden, E. S. yre

**Updated**: 2025-09-04T14:43:59Z

**Summary**: Spatially resolved observations of the Sun and the astronomical sample size of stellar bodies are the respective key strengths of solar and stellar observations. However, the large difference in object brightness between the Sun and other stars has led to distinctly different instrumentation and methodologies between the two fields. We produce and analyze synthetic full-disk spectra derived from 19 small area field-of-view optical observations of solar flares acquired by the Swedish 1-m Solar Telescope (SST) between 2011 and 2024. These are used to investigate what can and cannot be inferred about physical processes on the Sun from Sun-as-a-star observations. The recently released Numerical Empirical Sun-as-a-Star Integrator (NESSI) code provides synthetic full-disk integrated spectral line emission based on smaller field-of-view input, accounting for center-to-limb variations and differential rotation. We use this code to generate pseudo-Sun-as-a-star spectra from the SST observations. ...

**Link**: [arxiv](http://arxiv.org/abs/2507.07967v2),  [pdf](http://arxiv.org/pdf/2507.07967v2)

**Tags**: astro-ph.SR 



### Street-Level AI: Are Large Language Models Ready for Real-World   Judgments?
**Authors**: Gaurab Pokharel, Shafkat Farabi, Patrick J. Fowler, Sanmay Das

**Updated**: 2025-09-04T14:42:06Z

**Summary**: A surge of recent work explores the ethical and societal implications of large-scale AI models that make "moral" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2508.08193v2),  [pdf](http://arxiv.org/pdf/2508.08193v2)

**Tags**: cs.CY cs.AI 



### An Empirical Study of Vulnerabilities in Python Packages and Their   Detection
**Authors**: Haowei Quan, Junjie Wang, Xinzhe Li, Terry Yue Zhuo, Xiao Chen, Xiaoning Du

**Updated**: 2025-09-04T14:38:28Z

**Summary**: In the rapidly evolving software development landscape, Python stands out for its simplicity, versatility, and extensive ecosystem. Python packages, as units of organization, reusability, and distribution, have become a pressing concern, highlighted by the considerable number of vulnerability reports. As a scripting language, Python often cooperates with other languages for performance or interoperability. This adds complexity to the vulnerabilities inherent to Python packages, and the effectiveness of current vulnerability detection tools remains underexplored. This paper addresses these gaps by introducing PyVul, the first comprehensive benchmark suite of Python-package vulnerabilities. PyVul includes 1,157 publicly reported, developer-verified vulnerabilities, each linked to its affected packages. To accommodate diverse detection techniques, it provides annotations at both commit and function levels. An LLM-assisted data cleansing method is incorporated to improve label accuracy, achieving 100% commit-level and 94% function-level accuracy, establishing PyVul as the most precise large-scale Python vulnerability benchmark. We further carry out a distribution analysis of PyVul, which demonstrates that vulnerabilities in Python packages involve multiple programming languages and exhibit a wide variety of types. Moreover, our analysis reveals that multi-lingual Python packages are potentially more susceptible to vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark reveals a significant discrepancy between the capabilities of existing tools and the demands of effectively identifying real-world security issues in Python packages. Additionally, we conduct an empirical review of the top-ranked CWEs observed in Python packages, to diagnose the fine-grained limitations of current detection tools and highlight the necessity for future advancements in the field.

**Link**: [arxiv](http://arxiv.org/abs/2509.04260v1),  [pdf](http://arxiv.org/pdf/2509.04260v1)

**Tags**: cs.SE cs.AI cs.CR 



### A Survey of Graph Retrieval-Augmented Generation for Customized Large   Language Models
**Authors**: Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Yi Chang, Xiao Huang

**Updated**: 2025-09-04T14:24:19Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.

**Link**: [arxiv](http://arxiv.org/abs/2501.13958v2),  [pdf](http://arxiv.org/pdf/2501.13958v2)

**Tags**: cs.CL cs.AI cs.IR 



### How many patients could we save with LLM priors?
**Authors**: Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer

**Updated**: 2025-09-04T14:23:35Z

**Summary**: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.

**Link**: [arxiv](http://arxiv.org/abs/2509.04250v1),  [pdf](http://arxiv.org/pdf/2509.04250v1)

**Tags**: stat.ME cs.AI cs.ET cs.IR stat.AP 



### DMN-Guided Prompting: A Framework for Controlling LLM Behavior
**Authors**: Shaghayegh Abedi, Amin Jalali

**Updated**: 2025-09-04T14:12:36Z

**Summary**: Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting in our case study. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.

**Link**: [arxiv](http://arxiv.org/abs/2505.11701v2),  [pdf](http://arxiv.org/pdf/2505.11701v2)

**Tags**: cs.AI 



### Conformalized Multiple Testing under Unknown Null Distribution with   Symmetric Errors
**Authors**: Yang Tian, Zinan Zhao, Wenguang Sun

**Updated**: 2025-09-04T14:08:18Z

**Summary**: This article addresses a fundamental concern, first raised by Efron (2004), regarding the selection of null distributions in large-scale multiple testing. In modern data-intensive applications involving thousands or even millions of hypotheses, the theoretical null distribution of the test statistics often deviates from the true underlying null distribution, severely compromising the false discovery rate (FDR) analysis. We propose a conformalized empirical Bayes method using self-calibrated empirical null samples (SENS) for both one-sample and two-sample multiple testing problems. The new framework not only sidesteps the use of potentially erroneous theoretical null distributions, which is common in conventional practice, but also mitigates the impact of estimation errors in the unknown null distribution on the validity of FDR control, a challenge frequently encountered in the empirical Bayes FDR literature. In contrast to the empirical Bayes approaches (cf. Efron, 2004; Jin and Cai, 2007; Sun and Cai, 2007) that rely on Gaussian assumptions for the null models, SENS imposes only a weak condition on the symmetry of the error distribution, and leverages conformal tools to achieve FDR control in finite samples. Moreover, SENS incorporates structural insights from empirical Bayes into inference, exhibiting higher power compared to frequentist model-free methods. We conduct an in-depth analysis to establish a novel optimality theory for SENS under Efron's two-group model and demonstrate its superiority over existing empirical Bayes FDR methods and recent model-free FDR methods through numerical experiments on both simulated and real data.

**Link**: [arxiv](http://arxiv.org/abs/2509.04231v1),  [pdf](http://arxiv.org/pdf/2509.04231v1)

**Tags**: stat.ME math.ST stat.TH 



### FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via   Flow Variational Inference
**Authors**: Qian Feng, Jianxiang Feng, Zhaopeng Chen, Rudolph Triebel, Alois Knoll

**Updated**: 2025-09-04T14:07:56Z

**Summary**: Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from partial observations remains a critical challenge in robot learning. Prior generative methods struggle to model the intricate grasp distribution of dexterous hands and often fail to reason about shape uncertainty inherent in partial point clouds, leading to unreliable or overly conservative grasps. We propose FFHFlow, a flow-based variational framework that generates diverse, robust multi-finger grasps while explicitly quantifying perceptual uncertainty in the partial point clouds. Our approach leverages a normalizing flow-based deep latent variable model to learn a hierarchical grasp manifold, overcoming the mode collapse and rigid prior limitations of conditional Variational Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of flows, FFHFlow introspects shape uncertainty in partial observations and identifies novel object structures, enabling risk-aware grasp synthesis. To further enhance reliability, we integrate a discriminative grasp evaluator with the flow likelihoods, formulating an uncertainty-aware ranking strategy that prioritizes grasps robust to shape ambiguity. Extensive experiments in simulation and real-world setups demonstrate that FFHFlow outperforms state-of-the-art baselines (including diffusion models) in grasp diversity and success rate, while achieving run-time efficient sampling. We also showcase its practical value in cluttered and confined environments, where diversity-driven sampling excels by mitigating collisions (Project Page: https://sites.google.com/view/ffhflow/home/).

**Link**: [arxiv](http://arxiv.org/abs/2407.15161v4),  [pdf](http://arxiv.org/pdf/2407.15161v4)

**Tags**: cs.RO cs.AI cs.LG 



### Kolb-Based Experiential Learning for Generalist Agents with Human-Level   Kaggle Data Science Performance
**Authors**: Antoine Grosnit, Alexandre Maraval, Refinath S N, Zichao Zhao, James Dora, Giuseppe Paolo, Albert Thomas, Jonas Gonzalez, Abhineet Kumar, Khyati Khandelwal, Abdelhakim Benechehab, Hamza Cherkaoui, Youssef Attia El-Hili, Kun Shao, Jianye Hao, Jun Yao, Balzs Kgl, Jun Wang

**Updated**: 2025-09-04T14:07:26Z

**Summary**: Human expertise emerges through iterative cycles of interaction, reflection, and internal model updating, which are central to cognitive theories such as Kolb's experiential learning and Vygotsky's zone of proximal development. In contrast, current AI systems, particularly LLM agents, rely on static pre-training or rigid workflows, lacking mechanisms for continual adaptation. Recent studies identified early cognitive traits in LLM agents (reflection, revision, and self-correction) suggesting foundational elements of human-like experiential learning. Thus the key question: Can we design LLM agents capable of structured, cognitively grounded learning similar to human processes? In response, we propose a computational framework of Kolb's learning cycle with Vygotsky's ZPD for autonomous agents. Our architecture separates extrinsic (environment interaction) and intrinsic (internal reflection/abstraction) functions, enabling cognitively grounded scaffolded learning, where the agent initially learns within structured environments, followed by open-ended generalisation. This approach empowers agents to master complex tasks ; domains that traditional fine-tuning or simple reflective methods could not tackle effectively. Its potential is powerfully demonstrated via direct comparison with humans in real-world Kaggle data science competitions. Learning fully automated data science code generation across 81 tasks, our system, Agent K, demonstrated the ability to perform the entire workflow autonomously, achieving an Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2% among 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals level performance - including 4 gold and 4 silver on prize-awarding competitions - Agent K is the 1st AI system to successfully integrate Kolb- and Vygotsky-inspired human cognitive learning, marking a major step toward generalist AI.

**Link**: [arxiv](http://arxiv.org/abs/2411.03562v2),  [pdf](http://arxiv.org/pdf/2411.03562v2)

**Tags**: cs.LG cs.AI 



### Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large   Language Models via a Multi-Paradigm Perspective
**Authors**: Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Ziyi Yang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei

**Updated**: 2025-09-04T14:01:21Z

**Summary**: Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.11110v4),  [pdf](http://arxiv.org/pdf/2501.11110v4)

**Tags**: cs.CL 



### Making neural networks understand internal heat transfer using   Fourier-transformed thermal diffusion wave fields
**Authors**: Pengfei Zhu, Hai Zhang, Clemente Ibarra-Castanedo, Xavier Maldague, Andreas Mandelis

**Updated**: 2025-09-04T13:54:25Z

**Summary**: Heat propagation is governed by phonon interactions and mathematically described by partial differential equations (PDEs), which link thermal transport to the intrinsic properties of materials. Conventional experimental techniques infer thermal responses based on surface emissions, limiting their ability to fully resolve subsurface structures and internal heat distribution. Additionally, existing thermal tomographic techniques can only shoot one frame from each layer. Physics-informed neural networks (PINNs) have recently emerged as powerful tools for solving inverse problems in heat transfer by integrating observational data with physical constraints. However, standard PINNs are primarily focused on fitting the given external temperature data, without explicit knowledge of the unknown internal temperature distribution. In this study, we introduce a Helmholtz-informed neural network (HINN) to predict internal temperature distributions without requiring internal measurements. The time-domain heat diffusion equation was converted to the frequency-domain and becomes the pseudo-Helmholtz equation. HINN embeds this pseudo-Helmholtz equation into the learning framework, leveraging both real and imaginary components of the thermal field. Finally, an inverse Fourier transform brings real-part and imagery-part back to the time-domain and can be used to map 3D thermal fields with interior defects. Furthermore, a truncated operation was conducted to improve computational efficiency, and the principle of conjugate symmetry was employed for repairing the discarded data. This approach significantly enhances predictive accuracy and computational efficiency. Our results demonstrate that HINN outperforms state-of-the-art PINNs and inverse heat solvers, offering a novel solution for non-invasive thermography in applications spanning materials science, biomedical diagnostics, and nondestructive evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04223v1),  [pdf](http://arxiv.org/pdf/2509.04223v1)

**Tags**: physics.app-ph 



### PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware   Selection of Generative Models and LLMs
**Authors**: Xiaoyan Hu, Ho-fung Leung, Farzan Farnia

**Updated**: 2025-09-04T13:51:56Z

**Summary**: Selecting a sample generation scheme from multiple prompt-based generative models, including large language models (LLMs) and prompt-guided image and video generation models, is typically addressed by choosing the model that maximizes an averaged evaluation score. However, this score-based selection overlooks the possibility that different models achieve the best generation performance for different types of text prompts. An online identification of the best generation model for various input prompts can reduce the costs associated with querying sub-optimal models. In this work, we explore the possibility of varying rankings of text-based generative models for different text prompts and propose an online learning framework to predict the best data generation model for a given input prompt. The proposed PAK-UCB algorithm addresses a contextual bandit (CB) setting with shared context variables across the arms, utilizing the generated data to update kernel-based functions that predict the score of each model available for unseen text prompts. Additionally, we leverage random Fourier features (RFF) to accelerate the online learning process of PAK-UCB. Our numerical experiments on real and simulated text-to-image and image-to-text generative models show that RFF-UCB performs successfully in identifying the best generation model across different sample types. The code is available at: github.com/yannxiaoyanhu/dgm-online-select.

**Link**: [arxiv](http://arxiv.org/abs/2410.13287v6),  [pdf](http://arxiv.org/pdf/2410.13287v6)

**Tags**: cs.LG 



### Theory of Mind Using Active Inference: A Framework for Multi-Agent   Cooperation
**Authors**: Riddhi J. Pitliya, Ozan atal, Toon Van de Maele, Corrado Pezzato, Tim Verbelen

**Updated**: 2025-09-04T13:51:30Z

**Summary**: Theory of Mind (ToM) -- the ability to understand that others can have differing knowledge and goals -- enables agents to reason about others' beliefs while planning their own actions. We present a novel approach to multi-agent cooperation by implementing ToM within active inference. Unlike previous active inference approaches to multi-agent cooperation, our method neither relies on task-specific shared generative models nor requires explicit communication. In our framework, ToM-equipped agents maintain distinct representations of their own and others' beliefs and goals. ToM agents then use an extended and adapted version of the sophisticated inference tree-based planning algorithm to systematically explore joint policy spaces through recursive reasoning. We evaluate our approach through collision avoidance and foraging simulations. Results suggest that ToM agents cooperate better compared to non-ToM counterparts by being able to avoid collisions and reduce redundant efforts. Crucially, ToM agents accomplish this by inferring others' beliefs solely from observable behaviour and considering them when planning their own actions. Our approach shows potential for generalisable and scalable multi-agent systems while providing computational insights into ToM mechanisms.

**Link**: [arxiv](http://arxiv.org/abs/2508.00401v2),  [pdf](http://arxiv.org/pdf/2508.00401v2)

**Tags**: cs.AI cs.MA 



### Are LLM Agents the New RPA? A Comparative Study with RPA Across   Enterprise Workflows
**Authors**: Petr Prcha, Michaela Matoukov, Jan Strnad

**Updated**: 2025-09-04T13:22:44Z

**Summary**: The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.

**Link**: [arxiv](http://arxiv.org/abs/2509.04198v1),  [pdf](http://arxiv.org/pdf/2509.04198v1)

**Tags**: cs.CY cs.MA 



### InferLog: Accelerating LLM Inference for Online Log Parsing via   ICL-oriented Prefix Caching
**Authors**: Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng

**Updated**: 2025-09-04T13:14:33Z

**Summary**: Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.   In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.08523v2),  [pdf](http://arxiv.org/pdf/2507.08523v2)

**Tags**: cs.SE 



### KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and   Runtime Logs Analysis
**Authors**: Omri Sgan Cohen, Ehud Malul, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai

**Updated**: 2025-09-04T13:13:57Z

**Summary**: The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native applications has introduced significant security challenges, such as misconfigured resources and overly permissive configurations. Failing to address these issues can result in unauthorized access, privilege escalation, and lateral movement within clusters. Most existing K8s security solutions focus on detecting misconfigurations, typically through static analysis or anomaly detection. In contrast, this paper presents KubeGuard, a novel runtime log-driven recommender framework aimed at mitigating risks by addressing overly permissive configurations. KubeGuard is designed to harden K8s environments through two complementary tasks: Resource Creation and Resource Refinement. It leverages large language models (LLMs) to analyze manifests and runtime logs reflecting actual system behavior, using modular prompt-chaining workflows. This approach enables KubeGuard to create least-privilege configurations for new resources and refine existing manifests to reduce the attack surface. KubeGuard's output manifests are presented as recommendations that users (e.g., developers and operators) can review and adopt to enhance cluster security. Our evaluation demonstrates that KubeGuard effectively generates and refines K8s manifests for Roles, NetworkPolicies, and Deployments, leveraging both proprietary and open-source LLMs. The high precision, recall, and F1-scores affirm KubeGuard's practicality as a framework that translates runtime observability into actionable, least-privilege configuration guidance.

**Link**: [arxiv](http://arxiv.org/abs/2509.04191v1),  [pdf](http://arxiv.org/pdf/2509.04191v1)

**Tags**: cs.CR cs.LG 



### The Risk-Neutral Equivalent Pricing of Model-Uncertainty
**Authors**: Ken Kangda Wren

**Updated**: 2025-09-04T13:08:42Z

**Summary**: Existing approaches to asset-pricing under model-uncertainty adapt classical utility-maximization frameworks and seek theoretical comprehensiveness. We move toward practice by considering binary model-risks and by emphasizing 'constraints' over 'preference'. This decomposes viable economic asset-pricing into that of model and non-model risks separately, leading to a unique and convenient model-risk pricing formula. Its parameter, a dynamically conserved constant of model-risk inference, allows an integrated representation of ex-ante risk-pricing and bias such that their ex-post impacts are disentangled via well-known anomalies, Momentum and Low-Risk, whose risk-reward patterns acquire a fresh significance: peak-reward reveals ex-ante risk-premia, and peak-location, bias.

**Link**: [arxiv](http://arxiv.org/abs/2502.13744v8),  [pdf](http://arxiv.org/pdf/2502.13744v8)

**Tags**: q-fin.MF econ.EM 



### Set Block Decoding is a Language Model Inference Accelerator
**Authors**: Itai Gat, Heli Ben-Hamu, Marton Havasi, Daniel Haziza, Jeremy Reizenstein, Gabriel Synnaeve, David Lopez-Paz, Brian Karrer, Yaron Lipman

**Updated**: 2025-09-04T13:02:39Z

**Summary**: Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.

**Link**: [arxiv](http://arxiv.org/abs/2509.04185v1),  [pdf](http://arxiv.org/pdf/2509.04185v1)

**Tags**: cs.LG 



### MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn   Mental Health Counseling Sessions
**Authors**: Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych

**Updated**: 2025-09-04T12:59:24Z

**Summary**: The growing demand for scalable psychological counseling highlights the need for fine-tuning open-source Large Language Models (LLMs) with high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. In addition, we address inconsistencies in prior evaluation protocols by proposing a unified evaluation framework integrating diverse automatic and expert metrics. Furthermore, we expand the expert evaluations from four aspects of counseling in previous works to nine aspects, enabling a more thorough and robust assessment of data quality. Empirical results show that MAGneT significantly outperforms existing methods in quality, diversity, and therapeutic alignment of the generated counseling sessions, improving general counseling skills by 3.2% and CBT-specific skills by 4.3% on average on cognitive therapy rating scale (CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases on average across all aspects. Moreover, fine-tuning an open-source model on MAGneT-generated sessions shows better performance, with improvements of 6.3% on general counseling skills and 7.3% on CBT-specific skills on average on CTRS over those fine-tuned with sessions generated by baseline methods. We also make our code and data public.

**Link**: [arxiv](http://arxiv.org/abs/2509.04183v1),  [pdf](http://arxiv.org/pdf/2509.04183v1)

**Tags**: cs.CL cs.AI 



### Theoretical Radio Signals from Radio-Band Gravitational Waves Converted   from the Neutron Star Magnetic Field
**Authors**: Wei Hong, Zhen-Zhao Tao, Peng He, Tong-Jie Zhang

**Updated**: 2025-09-04T12:56:16Z

**Summary**: Gravitational waves (GWs) can convert into electromagnetic waves in the presence of a magnetic field via the Gertsenshtein-Zeldovich (GZ) effect. The characteristics of the magnetic field substantially affect this conversion probability. This paper confirms that strong magnetic fields in neutron stars significantly enhance the conversion probability, facilitating detectable radio signatures of very high-frequency (VHF, $\left(10^6-10^{11}\mathrm{~Hz}\right)$) gravitational waves. We theoretically identify two distinct signatures using single-dish telescopes (FAST, TMRT, QTT, GBT) and interferometers (SKA1/2-MID): transient signals from burst-like gravitational wave sources and persistent signals from cosmological background gravitational wave sources. These signatures are mapped to graviton spectral lines derived from quantum field theory by incorporating spin-2 and mass constraints, resulting in smooth, featureless profiles that are critical for distinguishing gravitational wave signals from astrophysical foregrounds. FAST attains a characteristic strain bound of $h_c<10^{-23}$, approaching $10^{-24}$ in the frequency range of $1-3\mathrm{~GHz}$ with a 6-hour observation period. This performance exceeds the $5 \sigma$ detection thresholds for GWs originating from primordial black holes (PBHs) and nears the limits set by Big Bang nucleosynthesis. Additionally, projections for SKA2-MID indicate even greater sensitivity. Detecting such gravitational waves would improve our comprehension of cosmological models, refine the parameter spaces for primordial black holes, and function as a test for quantum field theory. This approach addresses significant deficiencies in VHF GW research, improving detection sensitivity and facilitating the advancement of next-generation radio telescopes such as FASTA and SKA, which feature larger fields of view and enhanced gain.

**Link**: [arxiv](http://arxiv.org/abs/2412.05338v3),  [pdf](http://arxiv.org/pdf/2412.05338v3)

**Tags**: astro-ph.HE astro-ph.CO gr-qc 



### Privacy Risks in Time Series Forecasting: User- and Record-Level   Membership Inference
**Authors**: Nicolas Johansson, Tobias Olsson, Daniel Nilsson, Johan stman, Fazeleh Hoseini

**Updated**: 2025-09-04T12:43:45Z

**Summary**: Membership inference attacks (MIAs) aim to determine whether specific data were used to train a model. While extensively studied on classification models, their impact on time series forecasting remains largely unexplored. We address this gap by introducing two new attacks: (i) an adaptation of multivariate LiRA, a state-of-the-art MIA originally developed for classification models, to the time-series forecasting setting, and (ii) a novel end-to-end learning approach called Deep Time Series (DTS) attack. We benchmark these methods against adapted versions of other leading attacks from the classification setting.   We evaluate all attacks in realistic settings on the TUH-EEG and ELD datasets, targeting two strong forecasting architectures, LSTM and the state-of-the-art N-HiTS, under both record- and user-level threat models. Our results show that forecasting models are vulnerable, with user-level attacks often achieving perfect detection. The proposed methods achieve the strongest performance in several settings, establishing new baselines for privacy risk assessment in time series forecasting. Furthermore, vulnerability increases with longer prediction horizons and smaller training populations, echoing trends observed in large language models.

**Link**: [arxiv](http://arxiv.org/abs/2509.04169v1),  [pdf](http://arxiv.org/pdf/2509.04169v1)

**Tags**: cs.LG 



### Uncertainty-Guided Likelihood Tree Search
**Authors**: Julia Grosse, Ruotian Wu, Ahmad Rashid, Cheng Zhang, Philipp Hennig, Pascal Poupart, Agustinus Kristiadi

**Updated**: 2025-09-04T12:43:29Z

**Summary**: Tree search is a fundamental tool for planning, as many sequential decision-making problems can be framed as searching over tree-structured spaces. We propose an uncertainty-guided tree search algorithm for settings where the reward function is a log-likelihood function of the paths. Due to the combinatorial explosion of the tree size, the set of paths for which one can obtain rewards is sparse, particularly when the likelihood is obtained through expensive evaluations, such as by querying a large language model. We address this challenge by deriving an probabilistic search heuristic based on regularity assumptions for the likelihood. Unlike existing tree search methods, the proposed method can perform backtracking and trade-off exploration with exploitation, and yet does not require expensive roll-outs, or sophisticated Bayesian inference. Through extensive on-model and off-model experiments on timely, large-scale practical applications, we demonstrate that our method identifies paths with high likelihood while requiring fewer costly evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2407.03951v3),  [pdf](http://arxiv.org/pdf/2407.03951v3)

**Tags**: cs.LG 



### Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical   Definitions
**Authors**: Lan Zhang, Marco Valentino, Andre Freitas

**Updated**: 2025-09-04T12:43:14Z

**Summary**: Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions: a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalization, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we augment LLMs' formalizations through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2502.12065v3),  [pdf](http://arxiv.org/pdf/2502.12065v3)

**Tags**: cs.CL cs.FL 



### Bayesian Additive Regression Trees for functional ANOVA model
**Authors**: Seokhun Park, Insung Kong, Yongdai Kim

**Updated**: 2025-09-04T12:40:40Z

**Summary**: Bayesian Additive Regression Trees (BART) is a powerful statistical model that leverages the strengths of Bayesian inference and regression trees. It has received significant attention for capturing complex non-linear relationships and interactions among predictors. However, the accuracy of BART often comes at the cost of interpretability. To address this limitation, we propose ANOVA Bayesian Additive Regression Trees (ANOVA-BART), a novel extension of BART based on the functional ANOVA decomposition, which is used to decompose the variability of a function into different interactions, each representing the contribution of a different set of covariates or factors. Our proposed ANOVA-BART enhances interpretability, preserves and extends the theoretical guarantees of BART, and achieves superior predictive performance. Specifically, we establish that the posterior concentration rate of ANOVA-BART is nearly minimax optimal, and further provides the same convergence rates for each interaction that are not available for BART. Moreover, comprehensive experiments confirm that ANOVA-BART surpasses BART in both accuracy and uncertainty quantification, while also demonstrating its effectiveness in component selection. These results suggest that ANOVA-BART offers a compelling alternative to BART by balancing predictive accuracy, interpretability, and theoretical consistency.

**Link**: [arxiv](http://arxiv.org/abs/2509.03317v2),  [pdf](http://arxiv.org/pdf/2509.03317v2)

**Tags**: stat.ML cs.LG 



### Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs   and Optimizations
**Authors**: Safa Mohammed Sali, Mahmoud Meribout, Ashiyana Abdul Majeed

**Updated**: 2025-09-04T12:35:26Z

**Summary**: Transformers and vision-language models (VLMs) have emerged as dominant architectures in computer vision and multimodal AI, offering state-of-the-art performance in tasks such as image classification, object detection, visual question answering, and caption generation. However, their high computational complexity, large memory footprints, and irregular data access patterns present significant challenges for deployment in latency- and power-constrained environments. Field-programmable gate arrays (FPGAs) provide an attractive hardware platform for such workloads due to their reconfigurability, fine-grained parallelism, and potential for energy-efficient acceleration. This paper presents a comprehensive review of design trade-offs, optimization strategies, and implementation challenges for FPGA-based inference of transformers and VLMs. We examine critical factors such as device-class selection, memory subsystem constraints, dataflow orchestration, quantization strategies, sparsity exploitation, and toolchain choices, alongside modality-specific issues unique to VLMs, including heterogeneous compute balancing and cross-attention memory management. Additionally, we discuss emerging trends in hardware-algorithm co-design, highlighting innovations in attention mechanisms, compression, and modular overlays to improve efficiency and adaptability. Practical issues such as runtime flexibility, verification overhead, and the absence of standardized FPGA multimodal benchmarks are also considered. Finally, we outline future directions toward scalable, portable, and reconfigurable FPGA solutions that adapt to evolving model architectures while sustaining high utilization and predictable performance. This synthesis offers both a technical foundation and a forward-looking perspective to help bridge the gap between advanced multimodal AI models and efficient FPGA deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.04162v1),  [pdf](http://arxiv.org/pdf/2509.04162v1)

**Tags**: cs.AR 



### A machine-learning pipeline for real-time detection of gravitational   waves from compact binary coalescences
**Authors**: Ethan Marx, William Benoit, Alec Gunny, Rafia Omer, Deep Chatterjee, Ricco C. Venterea, Lauren Wills, Muhammed Saleem, Eric Moreno, Ryan Raikman, Ekaterina Govorkova, Dylan Rankin, Michael W. Coughlin, Philip Harris, Erik Katsavounidis

**Updated**: 2025-09-04T12:28:54Z

**Summary**: The promise of multi-messenger astronomy relies on the rapid detection of gravitational waves at very low latencies ($\mathcal{O}$(1\,s)) in order to maximize the amount of time available for follow-up observations. In recent years, neural-networks have demonstrated robust non-linear modeling capabilities and millisecond-scale inference at a comparatively small computational footprint, making them an attractive family of algorithms in this context. However, integration of these algorithms into the gravitational-wave astrophysics research ecosystem has proven non-trivial. Here, we present the first fully machine learning-based pipeline for the detection of gravitational waves from compact binary coalescences (CBCs) running in low-latency. We demonstrate this pipeline to have a fraction of the latency of traditional matched filtering search pipelines while achieving state-of-the-art sensitivity to higher-mass stellar binary black holes.

**Link**: [arxiv](http://arxiv.org/abs/2403.18661v2),  [pdf](http://arxiv.org/pdf/2403.18661v2)

**Tags**: gr-qc astro-ph.IM 



### Real Time FPGA Based CNNs for Detection, Classification, and Tracking in   Autonomous Systems: State of the Art Designs and Optimizations
**Authors**: Safa Mohammed Sali, Mahmoud Meribout, Ashiyana Abdul Majeed

**Updated**: 2025-09-04T12:28:36Z

**Summary**: This paper presents a comprehensive review of recent advances in deploying convolutional neural networks (CNNs) for object detection, classification, and tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand for real-time computer vision applications in domains such as autonomous vehicles, robotics, and surveillance, FPGAs have emerged as a powerful alternative to GPUs and ASICs due to their reconfigurability, low power consumption, and deterministic latency. We critically examine state-of-the-art FPGA implementations of CNN-based vision tasks, covering algorithmic innovations, hardware acceleration techniques, and the integration of optimization strategies like pruning, quantization, and sparsity-aware methods to maximize performance within hardware constraints. This survey also explores the landscape of modern FPGA platforms, including classical LUT-DSP based architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration Platforms (ACAPs), comparing their capabilities in handling deep learning workloads. Furthermore, we review available software development tools such as Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the design and deployment of AI models on FPGAs. The paper uniquely discusses hybrid architecture that combine GPUs and FPGAs for collaborative acceleration of AI inference, addressing challenges related to energy efficiency and throughput. Additionally, we highlight hardware-software co-design practices, dataflow optimizations, and pipelined processing techniques essential for real-time inference on resource-constrained devices. Through this survey, researchers and engineers are equipped with insights to develop next-generation, power-efficient, and high-performance vision systems optimized for FPGA deployment in edge and embedded applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.04153v1),  [pdf](http://arxiv.org/pdf/2509.04153v1)

**Tags**: cs.AR 



### TAGAL: Tabular Data Generation using Agentic LLM Methods
**Authors**: Benot Ronval, Pierre Dupont, Siegfried Nijssen

**Updated**: 2025-09-04T12:25:14Z

**Summary**: The generation of data is a common approach to improve the performance of machine learning tasks, among which is the training of models for classification. In this paper, we present TAGAL, a collection of methods able to generate synthetic tabular data using an agentic workflow. The methods leverage Large Language Models (LLMs) for an automatic and iterative process that uses feedback to improve the generated data without any further LLM training. The use of LLMs also allows for the addition of external knowledge in the generation process. We evaluate TAGAL across diverse datasets and different aspects of quality for the generated data. We look at the utility of downstream ML models, both by training classifiers on synthetic data only and by combining real and synthetic data. Moreover, we compare the similarities between the real and the generated data. We show that TAGAL is able to perform on par with state-of-the-art approaches that require LLM training and generally outperforms other training-free approaches. These findings highlight the potential of agentic workflow and open new directions for LLM-based data generation methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.04152v1),  [pdf](http://arxiv.org/pdf/2509.04152v1)

**Tags**: cs.LG cs.AI 



### Replication Study and Benchmarking of Real-Time Object Detection Models
**Authors**: Pierre-Luc Asselin, Vincent Coulombe, William Guimont-Martin, William Larrive-Hardy

**Updated**: 2025-09-04T12:21:13Z

**Summary**: This work examines the reproducibility and benchmarking of state-of-the-art real-time object detection models. As object detection models are often used in real-world contexts, such as robotics, where inference time is paramount, simply measuring models' accuracy is not enough to compare them. We thus compare a large variety of object detection models' accuracy and inference speed on multiple graphics cards. In addition to this large benchmarking attempt, we also reproduce the following models from scratch using PyTorch on the MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we propose a unified training and evaluation pipeline, based on MMDetection's features, to better compare models. Our implementation of DETR and ViTDet could not achieve accuracy or speed performances comparable to what is declared in the original papers. On the other hand, reproduced RTMDet and YOLOv7 could match such performances. Studied papers are also found to be generally lacking for reproducibility purposes. As for MMDetection pretrained models, speed performances are severely reduced with limited computing resources (larger, more accurate models even more so). Moreover, results exhibit a strong trade-off between accuracy and speed, prevailed by anchor-free models - notably RTMDet or YOLOx models. The code used is this paper and all the experiments is available in the repository at https://github.com/willGuimont/segdet_mlcr2024.

**Link**: [arxiv](http://arxiv.org/abs/2405.06911v2),  [pdf](http://arxiv.org/pdf/2405.06911v2)

**Tags**: cs.CV 



### Explicit Learning and the LLM in Machine Translation
**Authors**: Malik Marmonier, Rachel Bawden, Benot Sagot

**Updated**: 2025-09-04T12:20:43Z

**Summary**: This study explores an LLM's ability to learn new languages using explanations found in a grammar book, a process we term "explicit learning." To rigorously assess this ability, we design controlled translation experiments between English and constructed languages generated, through specific cryptographic means, from Latin or French. Contrary to previous studies, our results demonstrate that LLMs do possess a measurable capacity for explicit learning. This ability, however, diminishes as the complexity of the linguistic phenomena to be learned increases. Supervised fine-tuning on ad hoc chains of thought significantly enhances LLM performance but struggles to generalize to typologically novel or more complex linguistic features. These findings point to the need for more diverse training sets and alternative fine-tuning strategies to further improve explicit learning by LLMs, benefiting low-resource languages typically described in grammar books but lacking extensive corpora.

**Link**: [arxiv](http://arxiv.org/abs/2503.09454v4),  [pdf](http://arxiv.org/pdf/2503.09454v4)

**Tags**: cs.CL 



### Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network   Weight Space Diffusion
**Authors**: Dongliang Cao, Guoxing Sun, Marc Habermann, Florian Bernard

**Updated**: 2025-09-04T12:15:55Z

**Summary**: Creating human avatars is a highly desirable yet challenging task. Recent advancements in radiance field rendering have achieved unprecedented photorealism and real-time performance for personalized dynamic human avatars. However, these approaches are typically limited to person-specific rendering models trained on multi-view video data for a single individual, limiting their ability to generalize across different identities. On the other hand, generative approaches leveraging prior knowledge from pre-trained 2D diffusion models can produce cartoonish, static human avatars, which are animated through simple skeleton-based articulation. Therefore, the avatars generated by these methods suffer from lower rendering quality compared to person-specific rendering methods and fail to capture pose-dependent deformations such as cloth wrinkles. In this paper, we propose a novel approach that unites the strengths of person-specific rendering and diffusion-based generative modeling to enable dynamic human avatar generation with both high photorealism and realistic pose-dependent deformations. Our method follows a two-stage pipeline: first, we optimize a set of person-specific UNets, with each network representing a dynamic human avatar that captures intricate pose-dependent deformations. In the second stage, we train a hyper diffusion model over the optimized network weights. During inference, our method generates network weights for real-time, controllable rendering of dynamic human avatars. Using a large-scale, cross-identity, multi-view video dataset, we demonstrate that our approach outperforms state-of-the-art human avatar generation methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.04145v1),  [pdf](http://arxiv.org/pdf/2509.04145v1)

**Tags**: cs.GR cs.CV 



### Bayesian Quantum Amplitude Estimation
**Authors**: Alexandra Rama, Luis Paulo Santos

**Updated**: 2025-09-04T12:13:47Z

**Summary**: We present BAE, a problem-tailored and noise-aware Bayesian algorithm for quantum amplitude estimation. In a fault tolerant scenario, BAE is capable of saturating the Heisenberg limit; if device noise is present, BAE can dynamically characterize it and self-adapt. We further propose aBAE, an annealed variant of BAE drawing on methods from statistical inference, to enhance robustness. Our proposals are parallelizable in both quantum and classical components, offer tools for fast noise model assessment, and can leverage preexisting information. Additionally, they accommodate experimental limitations and preferred cost trade-offs. We propose a robust benchmark for amplitude estimation algorithms and use it to test BAE against other approaches, demonstrating its competitive performance in both noisy and noiseless scenarios. In both cases, it achieves lower error than any other algorithm as a function of the cost. In the presence of decoherence, it is capable of learning when other algorithms fail.

**Link**: [arxiv](http://arxiv.org/abs/2412.04394v4),  [pdf](http://arxiv.org/pdf/2412.04394v4)

**Tags**: quant-ph 



### Enhancing Technical Documents Retrieval for RAG
**Authors**: Songjiang Lai, Tsun-Hin Cheung, Ka-Chun Fung, Kaiwen Xue, Kwan-Ho Lin, Yan-Ming Choi, Vincent Ng, Kin-Man Lam

**Updated**: 2025-09-04T12:11:03Z

**Summary**: In this paper, we introduce Technical-Embeddings, a novel framework designed to optimize semantic retrieval in technical documentation, with applications in both hardware and software development. Our approach addresses the challenges of understanding and retrieving complex technical content by leveraging the capabilities of Large Language Models (LLMs). First, we enhance user queries by generating expanded representations that better capture user intent and improve dataset diversity, thereby enriching the fine-tuning process for embedding models. Second, we apply summary extraction techniques to encode essential contextual information, refining the representation of technical documents. To further enhance retrieval performance, we fine-tune a bi-encoder BERT model using soft prompting, incorporating separate learning parameters for queries and document context to capture fine-grained semantic nuances. We evaluate our approach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that Technical-Embeddings significantly outperforms baseline models in both precision and recall. Our findings highlight the effectiveness of integrating query expansion and contextual summarization to enhance information access and comprehension in technical domains. This work advances the state of Retrieval-Augmented Generation (RAG) systems, offering new avenues for efficient and accurate technical document retrieval in engineering and product development workflows.

**Link**: [arxiv](http://arxiv.org/abs/2509.04139v1),  [pdf](http://arxiv.org/pdf/2509.04139v1)

**Tags**: cs.IR cs.AI 



### Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges
**Authors**: Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding

**Updated**: 2025-09-04T11:57:46Z

**Summary**: Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the "LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2508.00454v2),  [pdf](http://arxiv.org/pdf/2508.00454v2)

**Tags**: cs.CL 



### Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling   Paradigms for Text-to-Music Generation
**Authors**: Or Tal, Felix Kreuk, Yossi Adi

**Updated**: 2025-09-04T11:54:13Z

**Summary**: Recent progress in text-to-music generation has enabled models to synthesize high-quality musical segments, full compositions, and even respond to fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA) systems differ significantly in many dimensions, such as training datasets, modeling paradigms, and architectural choices. This diversity complicates efforts to evaluate models fairly and identify which design choices influence performance the most. While factors like data and architecture are important, in this study we focus exclusively on the modeling paradigm. We conduct a systematic empirical analysis to isolate its effects, offering insights into associated trade-offs and emergent behaviors that can guide future text-to-music generation systems. Specifically, we compare the two arguably most common modeling paradigms: auto-regressive decoding and conditional flow-matching. We conduct a controlled comparison by training all models from scratch using identical datasets, training configurations, and similar backbone architectures. Performance is evaluated across multiple axes, including generation quality, robustness to inference configurations, scalability, adherence to both textual and temporally aligned conditioning, and editing capabilities in the form of audio inpainting. This comparative study sheds light on distinct strengths and limitations of each paradigm, providing actionable insights that can inform future architectural and training decisions in the evolving landscape of text-to-music generation. Audio sampled examples are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

**Link**: [arxiv](http://arxiv.org/abs/2506.08570v3),  [pdf](http://arxiv.org/pdf/2506.08570v3)

**Tags**: cs.SD cs.AI cs.LG eess.AS 



### Oyster-I: Beyond Refusal -- Constructive Safety Alignment for   Responsible Language Models
**Authors**: Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Yitong Yang, Jialing Tao, Hui Xue

**Updated**: 2025-09-04T11:54:06Z

**Summary**: Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.01909v2),  [pdf](http://arxiv.org/pdf/2509.01909v2)

**Tags**: cs.AI cs.CL cs.CY cs.HC cs.SC 



### MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image   Generation
**Authors**: Yuan Zhao, Liu Lin

**Updated**: 2025-09-04T11:44:28Z

**Summary**: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.

**Link**: [arxiv](http://arxiv.org/abs/2509.04126v1),  [pdf](http://arxiv.org/pdf/2509.04126v1)

**Tags**: cs.CV cs.AI 



### Enhancing FKG.in: automating Indian food composition analysis
**Authors**: Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Geeta Trilok-Kumar, Ramesh Jain

**Updated**: 2025-09-04T11:42:04Z

**Summary**: This paper presents a novel approach to compute food composition data for Indian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The primary focus is to provide a broad overview of an automated food composition analysis workflow and describe its core functionalities: nutrition data aggregation, food composition analysis, and LLM-augmented information resolution. This workflow aims to complement FKG[.]in and iteratively supplement food composition data from verified knowledge bases. Additionally, this paper highlights the challenges of representing Indian food and accessing food composition data digitally. It also reviews three key sources of food composition data: the Indian Food Composition Tables, the Indian Nutrient Databank, and the Nutritionix API. Furthermore, it briefly outlines how users can interact with the workflow to obtain diet-based health recommendations and detailed food composition information for numerous recipes. We then explore the complex challenges of analyzing Indian recipe information across dimensions such as structure, multilingualism, and uncertainty as well as present our ongoing work on LLM-based solutions to address these issues. The methods proposed in this workshop paper for AI-driven knowledge curation and information resolution are application-agnostic, generalizable, and replicable for any domain.

**Link**: [arxiv](http://arxiv.org/abs/2412.05248v3),  [pdf](http://arxiv.org/pdf/2412.05248v3)

**Tags**: cs.AI cs.CL cs.IR 



### TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering
**Authors**: Ayan Banerjee, Josep Llads, Umapada Pal, Anjan Dutta

**Updated**: 2025-09-04T11:37:06Z

**Summary**: Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering.

**Link**: [arxiv](http://arxiv.org/abs/2509.04123v1),  [pdf](http://arxiv.org/pdf/2509.04123v1)

**Tags**: cs.CV 



### Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning
**Authors**: Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song

**Updated**: 2025-09-04T11:31:24Z

**Summary**: While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +8.58% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2505.14585v2),  [pdf](http://arxiv.org/pdf/2505.14585v2)

**Tags**: cs.CL 



### EHVC: Efficient Hierarchical Reference and Quality Structure for Neural   Video Coding
**Authors**: Junqi Liao, Yaojun Wu, Chaoyi Lin, Zhipin Deng, Li Li, Dong Liu, Xiaoyan Sun

**Updated**: 2025-09-04T11:31:12Z

**Summary**: Neural video codecs (NVCs), leveraging the power of end-to-end learning, have demonstrated remarkable coding efficiency improvements over traditional video codecs. Recent research has begun to pay attention to the quality structures in NVCs, optimizing them by introducing explicit hierarchical designs. However, less attention has been paid to the reference structure design, which fundamentally should be aligned with the hierarchical quality structure. In addition, there is still significant room for further optimization of the hierarchical quality structure. To address these challenges in NVCs, we propose EHVC, an efficient hierarchical neural video codec featuring three key innovations: (1) a hierarchical multi-reference scheme that draws on traditional video codec design to align reference and quality structures, thereby addressing the reference-quality mismatch; (2) a lookahead strategy to utilize an encoder-side context from future frames to enhance the quality structure; (3) a layer-wise quality scale with random quality training strategy to stabilize quality structures during inference. With these improvements, EHVC achieves significantly superior performance to the state-of-the-art NVCs. Code will be released in: https://github.com/bytedance/NEVC.

**Link**: [arxiv](http://arxiv.org/abs/2509.04118v1),  [pdf](http://arxiv.org/pdf/2509.04118v1)

**Tags**: eess.IV cs.AI 



### Thermal electrons in the radio afterglow of relativistic tidal   disruption event ZTF22aaajecp/AT2022cmc
**Authors**: Lauren Rhodes, Ben Margalit, Joe S. Bright, Hannah Dykaar, Rob Fender, David A. Green, Daryl Haggard, Assaf Horesh, Alexander J. van der Horst, Andrew Hughes, Kunal Mooley, Itai Sfaradi, David Titterington, David WIlliams-Baldwin

**Updated**: 2025-09-04T11:24:58Z

**Summary**: A tidal disruption event (TDE) occurs when a star travels too close to a supermassive black hole. In some cases, accretion of the disrupted material onto the black hole launches a relativistic jet. In this paper, we present a long term observing campaign to study the radio and sub-millimeter emission associated with the fifth jetted/relativistic TDE: AT2022cmc. Our campaign reveals a long lived counterpart. We fit three different models to our data: a non-thermal jet, a spherical outflow consisting of both thermal and non-thermal electrons, and a jet with thermal and non-thermal electrons. We find that the data is best described by a relativistic spherical outflow propagating into an environment with a density profile following R^-1.8. Comparison of AT2022cmc to other TDEs finds agreement in the density profile of the environment but also that AT2022cmc is twice as energetic as the other well-studied relativistic TDE Swift J1644. Our observations of AT2022cmc allow a thermal electron population to be inferred for the first time in a jetted transient providing, new insights into the microphysics of relativistic transients jets.

**Link**: [arxiv](http://arxiv.org/abs/2506.13618v2),  [pdf](http://arxiv.org/pdf/2506.13618v2)

**Tags**: astro-ph.HE 



### Synthetic Counterfactual Labels for Efficient Conformal Counterfactual   Inference
**Authors**: Amirmohammad Farzaneh, Matteo Zecchin, Osvaldo Simeone

**Updated**: 2025-09-04T11:22:08Z

**Summary**: This work addresses the problem of constructing reliable prediction intervals for individual counterfactual outcomes. Existing conformal counterfactual inference (CCI) methods provide marginal coverage guarantees but often produce overly conservative intervals, particularly under treatment imbalance when counterfactual samples are scarce. We introduce synthetic data-powered CCI (SP-CCI), a new framework that augments the calibration set with synthetic counterfactual labels generated by a pre-trained counterfactual model. To ensure validity, SP-CCI incorporates synthetic samples into a conformal calibration procedure based on risk-controlling prediction sets (RCPS) with a debiasing step informed by prediction-powered inference (PPI). We prove that SP-CCI achieves tighter prediction intervals while preserving marginal coverage, with theoretical guarantees under both exact and approximate importance weighting. Empirical results on different datasets confirm that SP-CCI consistently reduces interval width compared to standard CCI across all settings.

**Link**: [arxiv](http://arxiv.org/abs/2509.04112v1),  [pdf](http://arxiv.org/pdf/2509.04112v1)

**Tags**: cs.LG cs.IT math.IT 



### MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages
**Authors**: Dan Saattrup Smart

**Updated**: 2025-09-05T09:12:03Z

**Summary**: We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which covers 306 languages. The context data comes from Wikipedia articles, with questions generated by an LLM and the answers appearing verbatim in the Wikipedia articles. We conduct a crowdsourced human evaluation of the fluency of the generated questions across 30 of the languages, providing evidence that the questions are of good quality. We evaluate 6 different language models, both decoder and encoder models of varying sizes, showing that the benchmark is sufficiently difficult and that there is a large performance discrepancy amongst the languages. The dataset and survey evaluations are freely available.

**Link**: [arxiv](http://arxiv.org/abs/2509.04111v2),  [pdf](http://arxiv.org/pdf/2509.04111v2)

**Tags**: cs.CL 



### Exploiting synergies between JWST and cosmic 21-cm observations to   uncover star formation in the early Universe
**Authors**: Jiten Dhandha, Anastasia Fialkov, Thomas Gessey-Jones, Harry T. J. Bevins, Sandro Tacchella, Simon Pochinda, Eloy de Lera Acedo, Saurabh Singh, Rennan Barkana

**Updated**: 2025-09-04T11:17:46Z

**Summary**: In the current era of JWST, we continue to uncover a wealth of information about the Universe deep into the Epoch of Reionization. In this work, we use a suite of simulations with 21cmSPACE, to explore the astrophysical properties of early galaxies and their imprint on high-redshift observables. Our analysis incorporates a range of multi-wavelength datasets including UV luminosity functions (UVLFs) from HST and JWST spanning $z=6-14.5$, the 21-cm global signal and power spectrum limits from SARAS 3 and HERA respectively, as well as present-day diffuse X-ray and radio backgrounds. We constrain a flexible halo-mass and redshift dependent model of star-formation efficiency (SFE), defined as the fraction of gas converted into stars, and find that it is best described by minimal redshift evolution at $z\approx 6-10$, followed by rapid evolution at $z\approx10-15$. Using Bayesian inference, we derive functional posteriors of the SFE, inferring that halos of mass $M_h=10^{10}\,\mathrm{M}_\odot$ have efficiencies of $\approx 1 - 2\%$ at $z\lesssim10$, $\approx8\%$ at $z=12$ and $\approx21\%$ at $z=15$. We also highlight the synergy between UVLFs and global 21-cm signal from SARAS 3 in constraining the minimum virial conditions required for star-formation in halos. In parallel, we find the X-ray and radio efficiencies of early galaxies to be $f_X = 0.8^{+9.7}_{-0.4}$ and $f_r \lesssim 16.9$ respectively, improving upon previous works that exclude UVLF data. Our results underscore the critical role of UVLFs in constraining early galaxy properties, and their synergy with 21-cm and other multi-wavelength observations.

**Link**: [arxiv](http://arxiv.org/abs/2503.21687v2),  [pdf](http://arxiv.org/pdf/2503.21687v2)

**Tags**: astro-ph.GA astro-ph.CO 



### Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue
**Authors**: Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx

**Updated**: 2025-09-04T11:07:27Z

**Summary**: Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.

**Link**: [arxiv](http://arxiv.org/abs/2509.04104v1),  [pdf](http://arxiv.org/pdf/2509.04104v1)

**Tags**: cs.CL cs.HC 



### TriLiteNet: Lightweight Model for Multi-Task Visual Perception
**Authors**: Quang-Huy Che, Duc-Khai Lam

**Updated**: 2025-09-04T10:48:25Z

**Summary**: Efficient perception models are essential for Advanced Driver Assistance Systems (ADAS), as these applications require rapid processing and response to ensure safety and effectiveness in real-world environments. To address the real-time execution needs of such perception models, this study introduces the TriLiteNet model. This model can simultaneously manage multiple tasks related to panoramic driving perception. TriLiteNet is designed to optimize performance while maintaining low computational costs. Experimental results on the BDD100k dataset demonstrate that the model achieves competitive performance across three key tasks: vehicle detection, drivable area segmentation, and lane line segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of 85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for drivable area segmentation, and an Acc of 82.3% for lane line segmentation with only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed model includes a tiny configuration with just 0.14M parameters, which provides a multi-task solution with minimal computational demand. Evaluated for latency and power consumption on embedded devices, TriLiteNet in both configurations shows low latency and reasonable power during inference. By balancing performance, computational efficiency, and scalability, TriLiteNet offers a practical and deployable solution for real-world autonomous driving applications. Code is available at https://github.com/chequanghuy/TriLiteNet.

**Link**: [arxiv](http://arxiv.org/abs/2509.04092v1),  [pdf](http://arxiv.org/pdf/2509.04092v1)

**Tags**: cs.CV 



### Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its   Implications Across Security Tasks
**Authors**: Jintao Gu, Haolang Lu, Guoshun Nan, Yihan Lin, Kun Wang, Yuchun Guo, Yigui Cao, Yang Liu

**Updated**: 2025-09-05T10:34:25Z

**Summary**: Accurate detection of third-party libraries (TPLs) is fundamental to Android security, supporting vulnerability tracking, malware detection, and supply chain auditing. Despite many proposed tools, their real-world effectiveness remains unclear. We present the first large-scale empirical study of ten state-of-the-art TPL detection techniques across over 6,000 apps, enabled by a new ground truth dataset with precise version-level annotations for both remote and local dependencies. Our evaluation exposes tool fragility to R8-era transformations, weak version discrimination, inaccurate correspondence of candidate libraries, difficulty in generalizing similarity thresholds, and prohibitive runtime/memory overheads at scale. Beyond tool assessment, we further analyze how TPLs shape downstream tasks, including vulnerability analysis, malware detection, secret leakage assessment, and LLM-based evaluation. From this perspective, our study provides concrete insights into how TPL characteristics affect these tasks and informs future improvements in security analysis.

**Link**: [arxiv](http://arxiv.org/abs/2509.04091v2),  [pdf](http://arxiv.org/pdf/2509.04091v2)

**Tags**: cs.CR 68M25 K.6.5; D.2.7 



### Intermediate Languages Matter: Formal Languages and LLMs affect   Neurosymbolic Reasoning
**Authors**: Alexander Beiser, David Penz, Nysret Musliu

**Updated**: 2025-09-04T10:25:50Z

**Summary**: Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, the contributing factors to the success of Neurosymbolic LLM reasoning remain unclear. This paper demonstrates that one previously overlooked factor is the choice of the formal language. We introduce the intermediate language challenge: selecting a suitable formal language for neurosymbolic reasoning. By comparing four formal languages across three datasets and seven LLMs, we show that the choice of formal language affects both syntactic and semantic reasoning capabilities. We also discuss the varying effects across different LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.04083v1),  [pdf](http://arxiv.org/pdf/2509.04083v1)

**Tags**: cs.AI 



### RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging   Evaluation of Large Language Models
**Authors**: Jingjing Liu, Zeming Liu, Zihao Cheng, Mengliang He, Xiaoming Shi, Yuhang Guo, Xiangrong Zhu, Yuanfang Guo, Yunhong Wang, Haifeng Wang

**Updated**: 2025-09-04T10:13:21Z

**Summary**: Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.

**Link**: [arxiv](http://arxiv.org/abs/2509.04078v1),  [pdf](http://arxiv.org/pdf/2509.04078v1)

**Tags**: cs.SE cs.AI 



### ZTF SNe Ia DR2: Towards cosmology-grade ZTF supernova light curves using   scene modeling photometry
**Authors**: L. Lacroix, N. Regnault, T. de Jaeger, M. Le Jeune, M. Betoule, J. -M. Colley, M. Bernard, M. Rigault, M. Smith, A. Goobar, K. Maguire, G. Dimitriadis, J. Nordin, J. Johansson, M. Aubert, C. Barjou, E. C. Bellm, S. Bongard, U. Burgaz, B. Carreres, D. Fouchez, F. Feinstein, L. Galbany, M. Ginolin, M. Graham, D. Kuhn, R. R. Laher, T. E. Mller-Bravo, J. Neveu, M. Osman, B. Popovic, B. Racine, P. Rosnet, D. Rosselli, R. Smith, J. Sollerman, J. H. Terwel, A. Townsend, A. Wold

**Updated**: 2025-09-04T10:08:36Z

**Summary**: The Zwicky Transient Facility (ZTF) is conducting a wide-field survey of the northern sky in three optical bands and the collaboration cosmology working group has released 3628 spectroscopically confirmed Type Ia supernovae (SNe Ia) discovered during its first 2.5 years of operation. This "ZTF SN Ia DR2" sample is the largest SN Ia dataset to date.   Fully exploiting this dataset to improve understanding of the properties of dark energy requires a photometric accuracy of O(0.1%). This can be achieved using Scene Modeling Photometry (SMP), which is optimal to extract a transient signal (SN) from a complex background (its host), while ensuring a common flux estimator with nearby stars used as calibration reference. In this paper, we present the status of the SMP development and use it to assess the precision and accuracy of the ZTF SN Ia DR2 force photometry light curves.   We reach a repeatability of the star observations better than 1%. However, we have identified a new sensor effect, dubbed "pocket-effect", which distorts the Point Spread Function (PSF) in a flux-dependent manner leading to non-linearities in the photometry of a few percent. Correcting for this effect requires time- and sensor-dependent corrections to be applied at the pixel level, which is currently under development. This effects affects all light curve releases to date -- both from forced photometry and scene modelling preventing ZTF SN Ia DR2 to be used for accurate cosmological inference.   Comparing the SMP and forced photometry measurements, we find that stretch and color estimated from both processings are consistent, aside from a 10 mmag shift in color. This assess the robustness of results presented as part of the the ZTF SN Ia DR2 release. The absolute calibration however shifts by 90 mmag. A reprocessing of the full ZTF SN Ia DR2 dataset using the SMP method is currently in progress.

**Link**: [arxiv](http://arxiv.org/abs/2509.04073v1),  [pdf](http://arxiv.org/pdf/2509.04073v1)

**Tags**: astro-ph.CO 



### Arabic Chatbot Technologies in Education: An Overview
**Authors**: Hicham Bourhil, Yacine El Younoussi

**Updated**: 2025-09-04T09:55:16Z

**Summary**: The recent advancements in Artificial Intelligence (AI) in general, and in Natural Language Processing (NLP) in particular, and some of its applications such as chatbots, have led to their implementation in different domains like education, healthcare, tourism, and customer service. Since the COVID-19 pandemic, there has been an increasing interest in these digital technologies to allow and enhance remote access. In education, e-learning systems have been massively adopted worldwide. The emergence of Large Language Models (LLM) such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformers) made chatbots even more popular. In this study, we present a survey on existing Arabic chatbots in education and their different characteristics such as the adopted approaches, language variety, and metrics used to measure their performance. We were able to identified some research gaps when we discovered that, despite the success of chatbots in other languages such as English, only a few educational Arabic chatbots used modern techniques. Finally, we discuss future directions of research in this field.

**Link**: [arxiv](http://arxiv.org/abs/2509.04066v1),  [pdf](http://arxiv.org/pdf/2509.04066v1)

**Tags**: cs.CL 



### Synthesizing Sheet Music Problems for Evaluation and Reinforcement   Learning
**Authors**: Zhilin Wang, Zhe Yang, Yun Luo, Yafu Li, Haoran Zhang, Runzhe Zhan, Derek F. Wong, Jizhe Zhou, Yu Cheng

**Updated**: 2025-09-04T09:42:17Z

**Summary**: Enhancing the ability of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) to interpret sheet music is a crucial step toward building AI musicians. However, current research lacks both evaluation benchmarks and training data for sheet music reasoning. To address this, we propose the idea of synthesizing sheet music problems grounded in music theory, which can serve both as evaluation benchmarks and as training data for reinforcement learning with verifiable rewards (RLVR). We introduce a data synthesis framework that generates verifiable sheet music questions in both textual and visual modalities, leading to the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a complementary training set. Evaluation results on SSMR-Bench show the importance of models' reasoning abilities in interpreting sheet music. At the same time, the poor performance of Gemini 2.5-Pro highlights the challenges that MLLMs still face in interpreting sheet music in a visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and Qwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the trained Qwen3-8B-Base surpasses GPT-4 in overall performance on MusicTheoryBench and achieves reasoning performance comparable to GPT-4 with the strategies of Role play and Chain-of-Thought. Notably, its performance on math problems also improves relative to the original Qwen3-8B-Base. Furthermore, our results show that the enhanced reasoning ability can also facilitate music composition. In conclusion, we are the first to propose the idea of synthesizing sheet music problems based on music theory rules, and demonstrate its effectiveness not only in advancing model reasoning for sheet music understanding but also in unlocking new possibilities for AI-assisted music creation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04059v1),  [pdf](http://arxiv.org/pdf/2509.04059v1)

**Tags**: cs.CL 



### SMooGPT: Stylized Motion Generation using Large Language Models
**Authors**: Lei Zhong, Yi Yang, Changjian Li

**Updated**: 2025-09-04T09:41:18Z

**Summary**: Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04058v1),  [pdf](http://arxiv.org/pdf/2509.04058v1)

**Tags**: cs.GR cs.CV 



### DaMoC: Efficiently Selecting the Optimal Large Language Model for   Fine-tuning Domain Tasks Based on Data and Model Compression
**Authors**: Wei Huang, Huang Wei, Yinggui Wang

**Updated**: 2025-09-04T09:30:16Z

**Summary**: Large language models (LLMs) excel in general tasks but struggle with domain-specific ones, requiring fine-tuning with specific data. With many open-source LLMs available, selecting the best model for fine-tuning downstream tasks is challenging, primarily focusing on how to quickly identify the optimal LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses this challenge by: 1) Data Level: A systematic categorization of data filtering methodologies for LLMs is first established, classifying them into three distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods, and (3) hybrid approaches considering both dimensions. Further, we enhance the density of key tokens in the text achieving token compression. Subsequently, we use an LLM to iterative rewrite the text to optimize its expression. 2) Model Level: We use layer similarity scores to assess each layer's importance and remove those with lower importance. Then, we introduce a sparse merging paradigm to preserve as much of the original model's capability as possible. Extensive experiments on four datasets, medical Q&A, financial Q&A, general Q&A, and reading comprehension, show that we can select the optimal LLM while saving approximately 20-fold in training time.

**Link**: [arxiv](http://arxiv.org/abs/2509.01221v2),  [pdf](http://arxiv.org/pdf/2509.01221v2)

**Tags**: cs.CL cs.AI cs.LG 



### EvolveSignal: A Large Language Model Powered Coding Agent for   Discovering Traffic Signal Control Algorithms
**Authors**: Leizhen Wang, Peibo Duan, Hao Wang, Yue Wang, Jian Xu, Nan Zheng, Zhenliang Ma

**Updated**: 2025-09-04T09:25:05Z

**Summary**: In traffic engineering, the fixed-time traffic signal control remains widely used for its low cost, stability, and interpretability. However, its design depends on hand-crafted formulas (e.g., Webster) and manual re-timing by engineers to adapt to demand changes, which is labor-intensive and often yields suboptimal results under heterogeneous or congested conditions. This paper introduces the EvolveSignal, a large language models (LLMs) powered coding agent to automatically discover new traffic signal control algorithms. We formulate the problem as program synthesis, where candidate algorithms are represented as Python functions with fixed input-output structures, and iteratively optimized through external evaluations (e.g., a traffic simulator) and evolutionary search. Experiments on a signalized intersection demonstrate that the discovered algorithms outperform Webster's baseline, reducing average delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and incremental analyses reveal that EvolveSignal modifications-such as adjusting cycle length bounds, incorporating right-turn demand, and rescaling green allocations-can offer practically meaningful insights for traffic engineers. This work opens a new research direction by leveraging AI for algorithm design in traffic signal control, bridging program synthesis with transportation engineering.

**Link**: [arxiv](http://arxiv.org/abs/2509.03335v2),  [pdf](http://arxiv.org/pdf/2509.03335v2)

**Tags**: cs.LG 



## Keyword: LLM Deployment 
 ### Delta Activations: A Representation for Finetuned Large Language Models
**Authors**: Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim

**Updated**: 2025-09-04T17:59:06Z

**Summary**: The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at https://github.com/OscarXZQ/delta_activations.

**Link**: [arxiv](http://arxiv.org/abs/2509.04442v1),  [pdf](http://arxiv.org/pdf/2509.04442v1)

**Tags**: cs.LG cs.AI cs.CL cs.IR 



### ACING: Actor-Critic for Instruction Learning in Black-Box LLMs
**Authors**: Salma Kharrat, Fares Fourati, Marco Canini

**Updated**: 2025-09-04T17:56:24Z

**Summary**: The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at https://github.com/salmakh1/ACING.

**Link**: [arxiv](http://arxiv.org/abs/2411.12736v2),  [pdf](http://arxiv.org/pdf/2411.12736v2)

**Tags**: cs.CL cs.AI cs.LG cs.SY eess.SY math.OC 



### ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory
**Authors**: Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Zhijian Liu, Zhiting Hu, Lianhui Qin

**Updated**: 2025-09-04T17:54:19Z

**Summary**: While inference-time scaling enables LLMs to carry out increasingly long and capable reasoning traces, the patterns and insights uncovered during these traces are immediately discarded once the context window is reset for a new query. External memory is a natural way to persist these discoveries, and recent work has shown clear benefits for reasoning-intensive tasks. We see an opportunity to make such memories more broadly reusable and scalable by moving beyond instance-based memory entries (e.g. exact query/response pairs, or summaries tightly coupled with the original problem context) toward concept-level memory: reusable, modular abstractions distilled from solution traces and stored in natural language. For future queries, relevant concepts are selectively retrieved and integrated into the prompt, enabling test-time continual learning without weight updates. Our design introduces new strategies for abstracting takeaways from rollouts and retrieving entries for new queries, promoting reuse and allowing memory to expand with additional experiences. On the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over a strong no-memory baseline with performance continuing to scale with inference compute. We find abstract concepts to be the most consistent memory design, outscoring the baseline at all tested inference compute scales. Moreover, we confirm that dynamically updating memory during test-time outperforms an otherwise identical fixed memory setting with additional attempts, supporting the hypothesis that solving more problems and abstracting more patterns to memory enables further solutions in a form of self-improvement. Code available at https://github.com/matt-seb-ho/arc_memo.

**Link**: [arxiv](http://arxiv.org/abs/2509.04439v1),  [pdf](http://arxiv.org/pdf/2509.04439v1)

**Tags**: cs.AI cs.CL cs.LG 



### Assessing Large Language Models in Comprehending and Verifying   Concurrent Programs across Memory Models
**Authors**: Ridhi Jain, Rahul Purandare

**Updated**: 2025-09-04T17:49:22Z

**Summary**: As concurrent programming becomes increasingly prevalent, effectively identifying and addressing concurrency issues such as data races and deadlocks is critical. This study evaluates the performance of several leading large language models (LLMs), including GPT-3.5-turbo, GPT-4, GPT-4o, GPT-4o-mini, and Mistral-AI's Large2, in understanding and analyzing concurrency issues within software programs. Given that relaxed memory models, such as Total Store Order (TSO) and Partial Store Order (PSO), are widely implemented and adapted in modern systems, supported even by commodity architectures like ARM and x86, our evaluation focuses not only on sequentially consistent memory models but also on these relaxed memory models. Specifically, we assess two main aspects: the models' capacity to detect concurrency problems under a sequentially consistent memory model and their ability to verify the correctness conditions of concurrent programs across both sequentially consistent and relaxed memory models. To do this, we leverage SV-COMP's pthread tests and 25 ARM Litmus tests designed to evaluate Total Store Order (TSO) and Partial Store Order (PSO) memory models. The experimental results reveal that GPT-4, GPT-4o, and Mistral-AI's Large2 demonstrate a robust understanding of concurrency issues, effectively identifying data races and deadlocks when assessed under a sequentially consistent memory model. However, despite its superior performance, all selected LLMs face significant challenges verifying program correctness under relaxed memory models. These LLMs exhibit limitations in accurately capturing memory ordering constraints, and their current capabilities fall short in verifying even small programs in these complex scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2501.14326v2),  [pdf](http://arxiv.org/pdf/2501.14326v2)

**Tags**: cs.SE 



### AgenTracer: Who Is Inducing Failure in the LLM Agentic Systems?
**Authors**: Guibin Zhang, Junhao Wang, Junjie Chen, Wangchunshu Zhou, Kun Wang, Shuicheng Yan

**Updated**: 2025-09-04T17:49:20Z

**Summary**: Large Language Model (LLM)-based agentic systems, often comprising multiple models, complex tool invocations, and orchestration protocols, substantially outperform monolithic agents. Yet this very sophistication amplifies their fragility, making them more prone to system failure. Pinpointing the specific agent or step responsible for an error within long execution traces defines the task of agentic system failure attribution. Current state-of-the-art reasoning LLMs, however, remain strikingly inadequate for this challenge, with accuracy generally below 10%. To address this gap, we propose AgenTracer, the first automated framework for annotating failed multi-agent trajectories via counterfactual replay and programmed fault injection, producing the curated dataset TracerTraj. Leveraging this resource, we develop AgenTracer-8B, a lightweight failure tracer trained with multi-granular reinforcement learning, capable of efficiently diagnosing errors in verbose multi-agent interactions. On the Who&When benchmark, AgenTracer-8B outperforms giant proprietary LLMs like Gemini-2.5-Pro and Claude-4-Sonnet by up to 18.18%, setting a new standard in LLM agentic failure attribution. More importantly, AgenTracer-8B delivers actionable feedback to off-the-shelf multi-agent systems like MetaGPT and MaAS with 4.8-14.2% performance gains, empowering self-correcting and self-evolving agentic AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.03312v2),  [pdf](http://arxiv.org/pdf/2509.03312v2)

**Tags**: cs.CL cs.MA 



### Text2Cypher: Data Pruning using Hard Example Selection
**Authors**: Makbule Gulcin Ozsoy

**Updated**: 2025-09-04T17:41:13Z

**Summary**: Database query languages such as SQL for relational databases and Cypher for graph databases have been widely adopted. Recent advancements in large language models (LLMs) enable natural language interactions with databases through models like Text2SQL and Text2Cypher. Fine-tuning these models typically requires large, diverse datasets containing non-trivial examples. However, as dataset size increases, the cost of fine-tuning also rises. This makes smaller, high-quality datasets essential for reducing costs for the same or better performance. In this paper, we propose five hard-example selection techniques for pruning the Text2Cypher dataset, aiming to preserve or improve performance while reducing resource usage. Our results show that these hard-example selection approaches can halve training time and costs with minimal impact on performance, and demonstrates that hard-example selection provides a cost-effective solution.

**Link**: [arxiv](http://arxiv.org/abs/2505.05122v2),  [pdf](http://arxiv.org/pdf/2505.05122v2)

**Tags**: cs.DB cs.LG 



### Modular Techniques for Synthetic Long-Context Data Generation in   Language Model Training and Evaluation
**Authors**: Seganrasan Subramanian, Abhigya Verma

**Updated**: 2025-09-04T17:22:16Z

**Summary**: The ability of large language models (LLMs) to process and reason over long textual inputs is critical for a wide range of real-world applications. However, progress in this area is significantly constrained by the absence of high-quality, diverse, and verifiable long-context datasets suitable for both training and evaluation. This work introduces a modular, extensible framework for synthetic long-context data generation via prompt-based interaction with LLMs. The framework supports multiple training and alignment objectives, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). It encompasses four core generation paradigms: multi-turn conversational dialogues, document-grounded input-output pairs, verifiable instruction-response tasks, and long-context reasoning examples. Through templated prompting, a model-agnostic architecture, and metadata-enriched outputs, the proposed approach facilitates scalable, controllable, and purpose-aligned dataset creation for advancing long-context capabilities in LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.01185v2),  [pdf](http://arxiv.org/pdf/2509.01185v2)

**Tags**: cs.CL cs.AI 



### No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in   Resume Screening
**Authors**: Kyra Wilson, Mattea Sim, Anna-Maria Gueorguieva, Aylin Caliskan

**Updated**: 2025-09-04T17:16:26Z

**Summary**: In this study, we conduct a resume-screening experiment (N=528) where people collaborate with simulated AI models exhibiting race-based preferences (bias) to evaluate candidates for 16 high and low status occupations. Simulated AI bias approximates factual and counterfactual estimates of racial bias in real-world AI systems. We investigate people's preferences for White, Black, Hispanic, and Asian candidates (represented through names and affinity groups on quality-controlled resumes) across 1,526 scenarios and measure their unconscious associations between race and status using implicit association tests (IATs), which predict discriminatory hiring decisions but have not been investigated in human-AI collaboration. When making decisions without AI or with AI that exhibits no race-based preferences, people select all candidates at equal rates. However, when interacting with AI favoring a particular group, people also favor those candidates up to 90% of the time, indicating a significant behavioral shift. The likelihood of selecting candidates whose identities do not align with common race-status stereotypes can increase by 13% if people complete an IAT before conducting resume screening. Finally, even if people think AI recommendations are low quality or not important, their decisions are still vulnerable to AI bias under certain circumstances. This work has implications for people's autonomy in AI-HITL scenarios, AI and work, design and evaluation of AI hiring systems, and strategies for mitigating bias in collaborative decision-making tasks. In particular, organizational and regulatory policy should acknowledge the complex nature of AI-HITL decision making when implementing these systems, educating people who use them, and determining which are subject to oversight.

**Link**: [arxiv](http://arxiv.org/abs/2509.04404v1),  [pdf](http://arxiv.org/pdf/2509.04404v1)

**Tags**: cs.CY cs.AI cs.CL cs.HC K.4.2 



### Denoising GER: A Noise-Robust Generative Error Correction with LLM for   Speech Recognition
**Authors**: Yanyan Liu, Minqiang Xu, Yihao Chen, Liang He, Lei Fang, Sian Fang, Lin Liu

**Updated**: 2025-09-04T17:03:58Z

**Summary**: In recent years, large language models (LLM) have made significant progress in the task of generation error correction (GER) for automatic speech recognition (ASR) post-processing. However, in complex noisy environments, they still face challenges such as poor adaptability and low information utilization, resulting in limited effectiveness of GER. To address these issues, this paper proposes a noise-robust multi-modal GER framework (Denoising GER). The framework enhances the model's adaptability to different noisy scenarios through a noise-adaptive acoustic encoder and optimizes the integration of multi-modal information via a heterogeneous feature compensation dynamic fusion (HFCDF) mechanism, improving the LLM's utilization of multi-modal information. Additionally, reinforcement learning (RL) training strategies are introduced to enhance the model's predictive capabilities. Experimental results demonstrate that Denoising GER significantly improves accuracy and robustness in noisy environments and exhibits good generalization abilities in unseen noise scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.04392v1),  [pdf](http://arxiv.org/pdf/2509.04392v1)

**Tags**: cs.SD 



### PagedEviction: Structured Block-wise KV Cache Pruning for Efficient   Large Language Model Inference
**Authors**: Krishna Teja Chitty-Venkata, Jie Ye, Xian-He Sun, Anthony Kougkas, Murali Emani, Venkatram Vishwanath, Bogdan Nicolae

**Updated**: 2025-09-04T16:40:01Z

**Summary**: KV caching significantly improves the efficiency of Large Language Model (LLM) inference by storing attention states from previously processed tokens, enabling faster generation of subsequent tokens. However, as sequence length increases, the KV cache quickly becomes a major memory bottleneck. To address this, we propose PagedEviction, a novel fine-grained, structured KV cache pruning strategy that enhances the memory efficiency of vLLM's PagedAttention. Unlike existing approaches that rely on attention-based token importance or evict tokens across different vLLM pages, PagedEviction introduces an efficient block-wise eviction algorithm tailored for paged memory layouts. Our method integrates seamlessly with PagedAttention without requiring any modifications to its CUDA attention kernels. We evaluate PagedEviction across Llama-3.1-8B-Instruct, Llama-3.2-1B-Instruct, and Llama-3.2-3B-Instruct models on the LongBench benchmark suite, demonstrating improved memory usage with better accuracy than baselines on long context tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.04377v1),  [pdf](http://arxiv.org/pdf/2509.04377v1)

**Tags**: cs.LG 



### AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval   for Text-Based Person Anomaly Search
**Authors**: Hao Ju, Hu Zhang, Zhedong Zheng

**Updated**: 2025-09-05T02:40:36Z

**Summary**: With growing public safety demands, text-based person anomaly search has emerged as a critical task, aiming to retrieve individuals with abnormal behaviors via natural language descriptions. Unlike conventional person search, this task presents two unique challenges: (1) fine-grained cross-modal alignment between textual anomalies and visual behaviors, and (2) anomaly recognition under sparse real-world samples. While Large Multi-modal Models (LMMs) excel in multi-modal understanding, their potential for fine-grained anomaly retrieval remains underexplored, hindered by: (1) a domain gap between generative knowledge and discriminative retrieval, and (2) the absence of efficient adaptation strategies for deployment. In this work, we propose AnomalyLMM, the first framework that harnesses LMMs for text-based person anomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline integrating LMMs to bridge generative world knowledge with retrieval-centric anomaly detection; (2) A training-free adaptation cookbook featuring masked cross-modal prompting, behavioral saliency prediction, and knowledge-aware re-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study to explore LMMs for this task, we conduct a rigorous evaluation on the PAB dataset, the only publicly available benchmark for text-based person anomaly search, with its curated real-world anomalies covering diverse scenarios (e.g., falling, collision, and being hit). Experiments show the effectiveness of the proposed method, surpassing the competitive baseline by +0.96% Recall@1 accuracy. Notably, our method reveals interpretable alignment between textual anomalies and visual behaviors, validated via qualitative analysis. Our code and models will be released for future research.

**Link**: [arxiv](http://arxiv.org/abs/2509.04376v2),  [pdf](http://arxiv.org/pdf/2509.04376v2)

**Tags**: cs.CV 



### Measuring Bias or Measuring the Task: Understanding the Brittle Nature   of LLM Gender Biases
**Authors**: Bufan Gao, Elisa Kreiss

**Updated**: 2025-09-04T16:32:18Z

**Summary**: As LLMs are increasingly applied in socially impactful settings, concerns about gender bias have prompted growing efforts both to measure and mitigate such bias. These efforts often rely on evaluation tasks that differ from natural language distributions, as they typically involve carefully constructed task prompts that overtly or covertly signal the presence of gender bias-related content. In this paper, we examine how signaling the evaluative purpose of a task impacts measured gender bias in LLMs. Concretely, we test models under prompt conditions that (1) make the testing context salient, and (2) make gender-focused content salient. We then assess prompt sensitivity across four task formats with both token-probability and discrete-choice metrics. We find that even minor prompt changes can substantially alter bias outcomes, sometimes reversing their direction entirely. Discrete-choice metrics further tend to amplify bias relative to probabilistic measures. These findings do not only highlight the brittleness of LLM gender bias evaluations but open a new puzzle for the NLP benchmarking and development community: To what extent can well-controlled testing designs trigger LLM ``testing mode'' performance, and what does this mean for the ecological validity of future benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2509.04373v1),  [pdf](http://arxiv.org/pdf/2509.04373v1)

**Tags**: cs.CL 



### MiniCPM4: Ultra-Efficient LLMs on End Devices
**Authors**: MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Baoxi Ji, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Xin Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Lushi Pu, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Zheng Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Xiaoyue Xu, Yukun Yan, Jiarui Yuan, Jinqian Zhang, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Chuyue Zhou, Ge Zhou, Jie Zhou, Wei Zhou, Yanghao Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun

**Updated**: 2025-09-04T16:23:02Z

**Summary**: This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Furthermore, we construct a hybrid reasoning model, MiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning mode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform similar-sized open-source models across benchmarks, with the 8B variants showing significant speed improvements on long sequence understanding and generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.07900v2),  [pdf](http://arxiv.org/pdf/2506.07900v2)

**Tags**: cs.CL cs.AI 



### DynaSaur: Large Language Agents Beyond Predefined Actions
**Authors**: Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou

**Updated**: 2025-09-04T16:22:32Z

**Summary**: Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly scoped environments, it presents two major challenges for real-world, open-ended scenarios: (1) it significantly restricts the planning and acting capabilities of LLM agents, and (2) it requires substantial human effort to enumerate and implement all possible actions, which is impractical in complex environments with a vast number of potential actions. To address these limitations, we propose an LLM agent framework that can dynamically create and compose actions as needed. In this framework, the agent interacts with its environment by generating and executing programs written in a general-purpose programming language. Moreover, generated actions are accumulated over time for future reuse. Our extensive experiments across multiple benchmarks show that this framework significantly improves flexibility and outperforms prior methods that rely on a fixed action set. Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases. Our code can be found in https://github.com/adobe-research/dynasaur.

**Link**: [arxiv](http://arxiv.org/abs/2411.01747v3),  [pdf](http://arxiv.org/pdf/2411.01747v3)

**Tags**: cs.CL 



### SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic   Avatars
**Authors**: Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti

**Updated**: 2025-09-04T16:18:04Z

**Summary**: We present SRWToolkit, an open-source Wizard of Oz toolkit designed to facilitate the rapid prototyping of social robotic avatars powered by local large language models (LLMs). Our web-based toolkit enables multimodal interaction through text input, button-activated speech, and wake-word command. The toolkit offers real-time configuration of avatar appearance, behavior, language, and voice via an intuitive control panel. In contrast to prior works that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and ensures on-device functionality through local LLM inference. In our small-scale user study ($n=11$), participants created and interacted with diverse robotic roles (hospital receptionist, mathematics teacher, and driving assistant), which demonstrated positive outcomes in the toolkit's usability, trust, and user experience. The toolkit enables rapid and efficient development of robot characters customized to researchers' needs, supporting scalable research in human-robot interaction.

**Link**: [arxiv](http://arxiv.org/abs/2509.04356v1),  [pdf](http://arxiv.org/pdf/2509.04356v1)

**Tags**: cs.HC cs.RO 



### Psychologically Enhanced AI Agents
**Authors**: Maciej Besta, Shriram Chandran, Robert Gerstenberger, Mathis Lindner, Marcin Chrapek, Sebastian Hermann Martschat, Taraneh Ghandi, Patrick Iff, Hubert Niewiadomski, Piotr Nyczyk, Jrgen Mller, Torsten Hoefler

**Updated**: 2025-09-04T16:03:03Z

**Summary**: We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of Large Language Model (LLM) agents through psychologically grounded personality conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method primes agents with distinct personality archetypes via prompt engineering, enabling control over behavior along two foundational axes of human psychology, cognition and affect. We show that such personality priming yields consistent, interpretable behavioral biases across diverse tasks: emotionally expressive agents excel in narrative generation, while analytically primed agents adopt more stable strategies in game-theoretic settings. Our framework supports experimenting with structured multi-agent communication protocols and reveals that self-reflection prior to interaction improves cooperation and reasoning quality. To ensure trait persistence, we integrate the official 16Personalities test for automated verification. While our focus is on MBTI, we show that our approach generalizes seamlessly to other psychological frameworks such as Big Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior design, we establish a foundation for psychologically enhanced AI agents without any fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2509.04343v1),  [pdf](http://arxiv.org/pdf/2509.04343v1)

**Tags**: cs.AI cs.CL cs.CY cs.HC cs.MA 



### Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing   Clinical Notes
**Authors**: Kristina L. Kupferschmidt, Kieran O'Doherty, Joshua A. Skorburg

**Updated**: 2025-09-04T15:59:45Z

**Summary**: Large Language Models (LLMs) are often proposed as tools to streamline clinical documentation, a task viewed as both high-volume and low-risk. However, even seemingly straightforward applications of LLMs raise complex sociotechnical considerations to translate into practice. This case study, conducted at KidsAbility, a pediatric rehabilitation facility in Ontario, Canada examined the use of LLMs to support occupational therapists in reducing documentation burden.We conducted a qualitative study involving 20 clinicians who participated in pilot programs using two AI technologies: a general-purpose proprietary LLM and a bespoke model fine-tuned on proprietary historical documentation.   Our findings reveal that documentation challenges are sociotechnical in nature, shaped by clinical workflows, organizational policies, and system constraints. Four key themes emerged: (1) the heterogeneity of workflows, (2) the documentation burden is systemic and not directly linked to the creation of any single type of documentation, (3) the need for flexible tools and clinician autonomy, and (4) effective implementation requires mutual learning between clinicians and AI systems.   While LLMs show promise in easing documentation tasks, their success will depend on flexible, adaptive integration that supports clinician autonomy. Beyond technical performance, sustained adoption will require training programs and implementation strategies that reflect the complexity of clinical environments.

**Link**: [arxiv](http://arxiv.org/abs/2509.04340v1),  [pdf](http://arxiv.org/pdf/2509.04340v1)

**Tags**: cs.HC 



### Transplant Then Regenerate: A New Paradigm for Text Data Augmentation
**Authors**: Guangzhan Wang, Hongyu Zhang, Beijun Shen, Xiaodong Gu

**Updated**: 2025-09-04T15:58:17Z

**Summary**: Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.

**Link**: [arxiv](http://arxiv.org/abs/2508.14723v2),  [pdf](http://arxiv.org/pdf/2508.14723v2)

**Tags**: cs.CL cs.AI 



### Small Changes, Large Consequences: Analyzing the Allocational Fairness   of LLMs in Hiring Contexts
**Authors**: Preethi Seshadri, Hongyu Chen, Sameer Singh, Seraphina Goldfarb-Tarrant

**Updated**: 2025-09-04T15:53:10Z

**Summary**: Large language models (LLMs) are increasingly being deployed in high-stakes applications like hiring, yet their potential for unfair decision-making remains understudied in generative and retrieval settings. In this work, we examine the allocational fairness of LLM-based hiring systems through two tasks that reflect actual HR usage: resume summarization and applicant ranking. By constructing a synthetic resume dataset with controlled perturbations and curating job postings, we investigate whether model behavior differs across demographic groups. Our findings reveal that generated summaries exhibit meaningful differences more frequently for race than for gender perturbations. Models also display non-uniform retrieval selection patterns across demographic groups and exhibit high ranking sensitivity to both gender and race perturbations. Surprisingly, retrieval models can show comparable sensitivity to both demographic and non-demographic changes, suggesting that fairness issues may stem from broader model brittleness. Overall, our results indicate that LLM-based hiring systems, especially in the retrieval stage, can exhibit notable biases that lead to discriminatory outcomes in real-world contexts.

**Link**: [arxiv](http://arxiv.org/abs/2501.04316v2),  [pdf](http://arxiv.org/pdf/2501.04316v2)

**Tags**: cs.CL 



### FaaSGuard: Secure CI/CD for Serverless Applications -- An OpenFaaS Case   Study
**Authors**: Amine Barrak, Emna Ksontini, Ridouane Atike, Fehmi Jaafar

**Updated**: 2025-09-04T15:48:13Z

**Summary**: Serverless computing significantly alters software development by abstracting infrastructure management and enabling rapid, modular, event-driven deployments. Despite its benefits, the distinct characteristics of serverless functions, such as ephemeral execution and fine-grained scalability, pose unique security challenges, particularly in open-source platforms like OpenFaaS. Existing approaches typically address isolated phases of the DevSecOps lifecycle, lacking an integrated and comprehensive security strategy. To bridge this gap, we propose FaaSGuard, a unified DevSecOps pipeline explicitly designed for open-source serverless environments. FaaSGuard systematically embeds lightweight, fail-closed security checks into every stage of the development lifecycle-planning, coding, building, deployment, and monitoring-effectively addressing threats such as injection attacks, hard-coded secrets, and resource exhaustion. We validate our approach empirically through a case study involving 20 real-world serverless functions from public GitHub repositories. Results indicate that FaaSGuard effectively detects and prevents critical vulnerabilities, demonstrating high precision (95%) and recall (91%) without significant disruption to established CI/CD practices.

**Link**: [arxiv](http://arxiv.org/abs/2509.04328v1),  [pdf](http://arxiv.org/pdf/2509.04328v1)

**Tags**: cs.SE 



### EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning
**Authors**: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, XiaoLong Hu, Ge Li

**Updated**: 2025-09-04T15:41:36Z

**Summary**: Reinforcement learning with verifiable reward (RLVR) has become a promising paradigm for post-training large language models (LLMs) to improve their reasoning capability. However, when the rollout accuracy is low on hard problems, the reward becomes sparse, limiting learning efficiency and causing exploration bottlenecks. Existing approaches either rely on stronger LLMs for distillation or filter out difficult problems, which limits scalability or restricts reasoning improvement through exploration.   We propose EvoCoT, a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way. This enables LLMs to stably learn from initially unsolved hard problems under sparse rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek, and Llama. Experiments show that EvoCoT enables LLMs to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods. We release the source code to support future research.

**Link**: [arxiv](http://arxiv.org/abs/2508.07809v2),  [pdf](http://arxiv.org/pdf/2508.07809v2)

**Tags**: cs.LG 



### Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven   Inference-Time-Scaling Algorithm
**Authors**: Siddharth Mansingh, James Amarel, Ragib Arnab, Arvind Mohan, Kamaljeet Singh, Gerd J. Kunde, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Nathan A. Debardeleben, Ayan Biswas, Diane Oyen, Earl Lawrence

**Updated**: 2025-09-04T15:40:37Z

**Summary**: Partial Differential Equations (PDEs) are the bedrock for modern computational sciences and engineering, and inherently computationally expensive. While PDE foundation models have shown much promise for simulating such complex spatio-temporal phenomena, existing models remain constrained by the pretraining datasets and struggle with auto-regressive rollout performance, especially in out-of-distribution (OOD) cases. Furthermore, they have significant compute and training data requirements which hamper their use in many critical applications. Inspired by recent advances in ``thinking" strategies used in large language models (LLMs), we introduce the first test-time computing (TTC) strategy for PDEs that utilizes computational resources during inference to achieve more accurate predictions with fewer training samples and smaller models. We accomplish this with two types of reward models that evaluate predictions of a stochastic based model for spatio-temporal consistency. We demonstrate this method on compressible Euler-equation simulations from the PDEGym benchmark and show that TTC captures improved predictions relative to standard non-adaptive auto-regressive inference. This TTC framework marks a foundational step towards more advanced reasoning algorithms or PDE modeling, inluding building reinforcement-learning-based approaches, potentially transforming computational workflows in physics and engineering.

**Link**: [arxiv](http://arxiv.org/abs/2509.02846v2),  [pdf](http://arxiv.org/pdf/2509.02846v2)

**Tags**: cs.LG physics.comp-ph 



### Plan Verification for LLM-Based Embodied Task Completion Agents
**Authors**: Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-Tr, Gokhan Tur

**Updated**: 2025-09-04T15:30:53Z

**Summary**: Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.02761v2),  [pdf](http://arxiv.org/pdf/2509.02761v2)

**Tags**: cs.AI 



### EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn   Negotiation
**Authors**: Yunbo Long, Liming Xu, Lukas Beckenbauer, Yuhan Liu, Alexandra Brintrup

**Updated**: 2025-09-04T15:23:58Z

**Summary**: Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models (LLMs) has demonstrated that agents can engage in \textit{complex}, \textit{multi-turn} negotiations, opening new avenues for agentic AI. However, existing LLM agents largely overlook the functional role of emotions in such negotiations, instead generating passive, preference-driven emotional responses that make them vulnerable to manipulation and strategic exploitation by adversarial counterparts. To address this gap, we present EvoEmo, an evolutionary reinforcement learning framework that optimizes dynamic emotional expression in negotiations. EvoEmo models emotional state transitions as a Markov Decision Process and employs population-based genetic optimization to evolve high-reward emotion policies across diverse negotiation scenarios. We further propose an evaluation framework with two baselines -- vanilla strategies and fixed-emotion strategies -- for benchmarking emotion-aware negotiation. Extensive experiments and ablation studies show that EvoEmo consistently outperforms both baselines, achieving higher success rates, higher efficiency, and increased buyer savings. This findings highlight the importance of adaptive emotional expression in enabling more effective LLM agents for multi-turn negotiation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04310v1),  [pdf](http://arxiv.org/pdf/2509.04310v1)

**Tags**: cs.AI 



### Learning Optimal Crew Dispatch for Grid Restoration Following an   Earthquake
**Authors**: Farshad Amani, Faezeh Ardali, Amin Kargarian

**Updated**: 2025-09-04T15:21:54Z

**Summary**: Post-disaster crew dispatch is a critical but computationally intensive task. Traditional mixed-integer linear programming methods often require minutes to several hours to compute solutions, leading to delays that hinder timely decision-making in highly dynamic restoration environments. To address this challenge, we propose a novel learning-based framework that integrates transformer architectures with deep reinforcement learning (DRL) to deliver near real-time decision support without compromising solution quality. Crew dispatch is formulated as a sequential decision-making problem under uncertainty, where transformers capture high-dimensional system states and temporal dependencies, while DRL enables adaptive and scalable decision-making. Earthquake-induced distribution network damage is first characterized using established seismic standards, followed by a scenario generation and reduction pipeline that aggregates probable outcomes into a single geospatial impact map. Conditioned on this map, the proposed framework generates second-level dispatch strategies, trained offline on simulated and historical events and deployed online for rapid response. In addition to substantial runtime improvements, the proposed method enhances system resilience by enabling faster and more effective recovery and restoration. Case studies, particularly on the 2869-bus European gas and power network, demonstrate that the method substantially accelerates restoration while maintaining high-quality solutions, underscoring its potential for practical deployment in large-scale disaster response.

**Link**: [arxiv](http://arxiv.org/abs/2509.04308v1),  [pdf](http://arxiv.org/pdf/2509.04308v1)

**Tags**: eess.SY cs.SY 



### Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with   Adaptive Exploration
**Authors**: Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, Jing Tang

**Updated**: 2025-09-04T15:21:27Z

**Summary**: Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models, yet its full potential is hindered by two under-explored dimensions: Depth-the hardest problem a model can sample; Breadth-the number of instances consumed in a single iteration. We dissect the popular GRPO algorithm and reveal a systematic bias: the cumulative-advantage disproportionately weights samples with medium accuracy, while down-weighting the low-accuracy instances that are crucial for pushing reasoning boundaries. To rectify the depth neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which re-weights hard problems through targeted multi-stage rollouts, thereby increasing the number of positive rollouts for hard problems. Empirically, naively enlarging rollout size only accelerates convergence and even hurts Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra inference cost at convergence. Just as we adaptively expanded the depth of exploration, we now ask whether aggressively scaling the breadth of training data can further amplify reasoning gains. To this end, we intensely scale batch size and replace PPO's mini-batch iterations with full-batch updates over multiple epochs. Increasing breadth significantly enhances Pass@1 performance. Large-breadth training sustains high token-level entropy, indicating continued exploration and reduced gradient noise. We further present DARS-B, which augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K and Pass@1. The results confirm that breadth and adaptive exploration across depth operate as orthogonal dimensions in RLVR, which are key to unleashing the reasoning power of RLVR.

**Link**: [arxiv](http://arxiv.org/abs/2508.13755v2),  [pdf](http://arxiv.org/pdf/2508.13755v2)

**Tags**: cs.LG cs.AI 



### Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge   in Large Language Models
**Authors**: Juraj Vladika, Mahdi Dhaini, Florian Matthes

**Updated**: 2025-09-04T15:17:50Z

**Summary**: The growing capabilities of Large Language Models (LLMs) show significant potential to enhance healthcare by assisting medical researchers and physicians. However, their reliance on static training data is a major risk when medical recommendations evolve with new research and developments. When LLMs memorize outdated medical knowledge, they can provide harmful advice or fail at clinical reasoning tasks. To investigate this problem, we introduce two novel question-answering (QA) datasets derived from systematic reviews: MedRevQA (16,501 QA pairs covering general biomedical knowledge) and MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over time). Our evaluation of eight prominent LLMs on the datasets reveals consistent reliance on outdated knowledge across all models. We additionally analyze the influence of obsolete pre-training data and training strategies to explain this phenomenon and propose future directions for mitigation, laying the groundwork for developing more current and reliable medical AI systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.04304v1),  [pdf](http://arxiv.org/pdf/2509.04304v1)

**Tags**: cs.CL cs.AI 



### An Unsupervised Natural Language Processing Pipeline for Assessing   Referral Appropriateness
**Authors**: Vittorio Torri, Annamaria Bottelli, Michele Ercolanoni, Olivia Leoni, Francesca Ieva

**Updated**: 2025-09-04T15:11:24Z

**Summary**: Objective: Assessing the appropriateness of diagnostic referrals is critical for improving healthcare efficiency and reducing unnecessary procedures. However, this task becomes challenging when referral reasons are recorded only as free text rather than structured codes, like in the Italian NHS. To address this gap, we propose a fully unsupervised Natural Language Processing (NLP) pipeline capable of extracting and evaluating referral reasons without relying on labelled datasets.   Methods: Our pipeline leverages Transformer-based embeddings pre-trained on Italian medical texts to cluster referral reasons and assess their alignment with appropriateness guidelines. It operates in an unsupervised setting and is designed to generalize across different examination types. We analyzed two complete regional datasets from the Lombardy Region (Italy), covering all referrals between 2019 and 2021 for venous echocolordoppler of the lower limbs (ECD;n=496,971; development) and flexible endoscope colonoscopy (FEC; n=407,949; testing only). For both, a random sample of 1,000 referrals was manually annotated to measure performance.   Results: The pipeline achieved high performance in identifying referral reasons (Prec=92.43% (ECD), 93.59% (FEC); Rec=83.28% (ECD), 92.70% (FEC)) and appropriateness (Prec=93.58% (ECD), 94.66% (FEC); Rec=91.52% (ECD), 93.96% (FEC)). At the regional level, the analysis identified relevant inappropriate referral groups and variation across contexts, findings that informed a new Lombardy Region resolution to reinforce guideline adherence.   Conclusions: This study presents a robust, scalable, unsupervised NLP pipeline for assessing referral appropriateness in large, real-world datasets. It demonstrates how such data can be effectively leveraged, providing public health authorities with a deployable AI tool to monitor practices and support evidence-based policy.

**Link**: [arxiv](http://arxiv.org/abs/2501.14701v2),  [pdf](http://arxiv.org/pdf/2501.14701v2)

**Tags**: cs.CL cs.LG 68T50 I.2.7; J.1; J.3 



### Federated Isolation Forest for Efficient Anomaly Detection on Edge IoT   Systems
**Authors**: Pavle Vasiljevic, Milica Matic, Miroslav Popovic

**Updated**: 2025-09-04T15:08:01Z

**Summary**: Recently, federated learning frameworks such as Python TestBed for Federated Learning Algorithms and MicroPython TestBed for Federated Learning Algorithms have emerged to tackle user privacy concerns and efficiency in embedded systems. Even more recently, an efficient federated anomaly detection algorithm, FLiForest, based on Isolation Forests has been developed, offering a low-resource, unsupervised method well-suited for edge deployment and continuous learning. In this paper, we present an application of Isolation Forest-based temperature anomaly detection, developed using the previously mentioned federated learning frameworks, aimed at small edge devices and IoT systems running MicroPython. The system has been experimentally evaluated, achieving over 96% accuracy in distinguishing normal from abnormal readings and above 78% precision in detecting anomalies across all tested configurations, while maintaining a memory usage below 160 KB during model training. These results highlight its suitability for resource-constrained environments and edge systems, while upholding federated learning principles of data privacy and collaborative learning.

**Link**: [arxiv](http://arxiv.org/abs/2506.05138v2),  [pdf](http://arxiv.org/pdf/2506.05138v2)

**Tags**: cs.LG cs.DC 



### EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming   in Debt Recovery
**Authors**: Yunbo Long, Yuhan Liu, Liming Xu, Alexandra Brintrup

**Updated**: 2025-09-04T15:06:27Z

**Summary**: Large language model-based chatbots have enhanced engagement in financial negotiations, but their overreliance on passive empathy introduces critical risks in credit collection. While empathy-driven approaches preserve client satisfaction in benign cases, they fail catastrophically against dishonest debtors--individuals who exploit conciliatory tactics to manipulate terms or evade repayment. Blindly prioritizing "customer experience" in such scenarios leads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic exploitation. To address this, we propose EQ-Knight, an LLM agent that dynamically optimizes emotional strategy to defend creditor interests. Unlike naive empathy-centric bots, EQ-Knight integrates emotion memory and game-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and predict debtor emotional states. By analyzing both real-time and historical emotional cues, EQ-Knight strategically counters negative emotions (e.g., aggression, feigned distress) while preserving productive debtor relationships. Experiments demonstrate EQ-Knight's superiority over conventional LLM negotiators: it achieves a 32\% reduction in concession losses without compromising recovery rates, particularly in adversarial cases where debtors weaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce concessions. For credit agencies, EQ-Knight transforms LLMs from high-risk "people-pleasers" into strategic emotion-defenders--balancing emotional intelligence with tactical rigor to enforce accountability and deter exploitation.

**Link**: [arxiv](http://arxiv.org/abs/2503.21080v4),  [pdf](http://arxiv.org/pdf/2503.21080v4)

**Tags**: cs.CL 



### Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow   Real Instructions?
**Authors**: Qinyan Zhang, Xinping Lei, Ruijie Miao, Yu Fu, Haojie Fan, Le Chang, Jiafan Hou, Dingling Zhang, Zhongfei Hou, Ziqiang Yang, Changxin Pu, Fei Hu, Jingkai Liu, Mengyun Liu, Yang Liu, Xiang Gao, Jiaheng Liu, Tong Yang, Zaiyuan Wang, Ge Zhang, Wenhao Huang

**Updated**: 2025-09-04T15:03:02Z

**Summary**: Large Language Models (LLMs) achieve strong performance on diverse tasks but often exhibit cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that measures models Counter-intuitive Abilitytheir capacity to override training-induced biases and comply with adversarial instructions. Inverse IFEval introduces eight types of such challenges, including Question Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a dataset of 1012 high-quality Chinese and English questions across 23 domains, evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing leading LLMs demonstrate the necessity of our proposed Inverse IFEval benchmark. Our findings emphasize that future alignment efforts should not only pursue fluency and factual correctness but also account for adaptability under unconventional contexts. We hope that Inverse IFEval serves as both a diagnostic tool and a foundation for developing methods that mitigate cognitive inertia, reduce overfitting to narrow patterns, and ultimately enhance the instruction-following reliability of LLMs in diverse and unpredictable real-world scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.04292v1),  [pdf](http://arxiv.org/pdf/2509.04292v1)

**Tags**: cs.CL 



### Rapid Word Learning Through Meta In-Context Learning
**Authors**: Wentao Wang, Guangyuan Jiang, Tal Linzen, Brenden M. Lake

**Updated**: 2025-09-04T14:58:09Z

**Summary**: Humans can quickly learn a new word from a few illustrative examples, and then systematically and flexibly use it in novel contexts. Yet the abilities of current language models for few-shot word learning, and methods for improving these abilities, are underexplored. In this study, we introduce a novel method, Meta-training for IN-context learNing Of Words (Minnow). This method trains language models to generate new examples of a word's usage given a few in-context examples, using a special placeholder token to represent the new word. This training is repeated on many new words to develop a general word-learning ability. We find that training models from scratch with Minnow on human-scale child-directed language enables strong few-shot word learning, comparable to a large language model (LLM) pre-trained on orders of magnitude more data. Furthermore, through discriminative and generative evaluations, we demonstrate that finetuning pre-trained LLMs with Minnow improves their ability to discriminate between new words, identify syntactic categories of new words, and generate reasonable new usages and definitions for new words, based on one or a few in-context examples. These findings highlight the data efficiency of Minnow and its potential to improve language model performance in word learning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2502.14791v4),  [pdf](http://arxiv.org/pdf/2502.14791v4)

**Tags**: cs.CL cs.AI cs.LG 



### Street-Level AI: Are Large Language Models Ready for Real-World   Judgments?
**Authors**: Gaurab Pokharel, Shafkat Farabi, Patrick J. Fowler, Sanmay Das

**Updated**: 2025-09-04T14:42:06Z

**Summary**: A surge of recent work explores the ethical and societal implications of large-scale AI models that make "moral" judgments. Much of this literature focuses either on alignment with human judgments through various thought experiments or on the group fairness implications of AI judgments. However, the most immediate and likely use of AI is to help or fully replace the so-called street-level bureaucrats, the individuals deciding to allocate scarce social resources or approve benefits. There is a rich history underlying how principles of local justice determine how society decides on prioritization mechanisms in such domains. In this paper, we examine how well LLM judgments align with human judgments, as well as with socially and politically determined vulnerability scoring systems currently used in the domain of homelessness resource allocation. Crucially, we use real data on those needing services (maintaining strict confidentiality by only using local large models) to perform our analyses. We find that LLM prioritizations are extremely inconsistent in several ways: internally on different runs, between different LLMs, and between LLMs and the vulnerability scoring systems. At the same time, LLMs demonstrate qualitative consistency with lay human judgments in pairwise testing. Findings call into question the readiness of current generation AI systems for naive integration in high-stakes societal decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2508.08193v2),  [pdf](http://arxiv.org/pdf/2508.08193v2)

**Tags**: cs.CY cs.AI 



### An Empirical Study of Vulnerabilities in Python Packages and Their   Detection
**Authors**: Haowei Quan, Junjie Wang, Xinzhe Li, Terry Yue Zhuo, Xiao Chen, Xiaoning Du

**Updated**: 2025-09-04T14:38:28Z

**Summary**: In the rapidly evolving software development landscape, Python stands out for its simplicity, versatility, and extensive ecosystem. Python packages, as units of organization, reusability, and distribution, have become a pressing concern, highlighted by the considerable number of vulnerability reports. As a scripting language, Python often cooperates with other languages for performance or interoperability. This adds complexity to the vulnerabilities inherent to Python packages, and the effectiveness of current vulnerability detection tools remains underexplored. This paper addresses these gaps by introducing PyVul, the first comprehensive benchmark suite of Python-package vulnerabilities. PyVul includes 1,157 publicly reported, developer-verified vulnerabilities, each linked to its affected packages. To accommodate diverse detection techniques, it provides annotations at both commit and function levels. An LLM-assisted data cleansing method is incorporated to improve label accuracy, achieving 100% commit-level and 94% function-level accuracy, establishing PyVul as the most precise large-scale Python vulnerability benchmark. We further carry out a distribution analysis of PyVul, which demonstrates that vulnerabilities in Python packages involve multiple programming languages and exhibit a wide variety of types. Moreover, our analysis reveals that multi-lingual Python packages are potentially more susceptible to vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark reveals a significant discrepancy between the capabilities of existing tools and the demands of effectively identifying real-world security issues in Python packages. Additionally, we conduct an empirical review of the top-ranked CWEs observed in Python packages, to diagnose the fine-grained limitations of current detection tools and highlight the necessity for future advancements in the field.

**Link**: [arxiv](http://arxiv.org/abs/2509.04260v1),  [pdf](http://arxiv.org/pdf/2509.04260v1)

**Tags**: cs.SE cs.AI cs.CR 



### Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation   via End-to-End Reinforcement Learning
**Authors**: Fan Yang, Per Frivik, David Hoeller, Chen Wang, Cesar Cadena, Marco Hutter

**Updated**: 2025-09-04T14:30:12Z

**Summary**: Recent advancements in robot navigation, particularly with end-to-end learning approaches such as reinforcement learning (RL), have demonstrated strong performance. However, successful navigation still depends on two key capabilities: mapping and planning (explicitly or implicitly). Classical approaches rely on explicit mapping pipelines to register egocentric observations into a coherent map. In contrast, end-to-end learning often achieves this implicitly -- through recurrent neural networks (RNNs) that fuse current and historical observations into a latent space for planning. While existing architectures, such as LSTM and GRU, can capture temporal dependencies, our findings reveal a critical limitation: their inability to effectively perform spatial memorization. This capability is essential for integrating sequential observations from varying perspectives to build spatial representations that support planning. To address this, we propose Spatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective modification to existing RNNs -- that enhance spatial memorization. We further introduce an attention-based network architecture integrated with SRUs, enabling long-range mapless navigation using a single forward-facing stereo camera. We also employ regularization techniques to facilitate robust end-to-end recurrent training via RL. Experimental results show 23.5% overall improvement in long-range navigation compared to existing RNNs. With SRU memory, our method outperforms RL baselines -- one relying on explicit mapping and the other on stacked historical observations -- by 29.6% and 105.0%, respectively, across diverse environments requiring long-horizon mapping and memorization. Finally, we address the sim-to-real gap by leveraging large-scale pretraining on synthetic depth data, enabling zero-shot transfer for deployment across diverse and complex real-world environments.

**Link**: [arxiv](http://arxiv.org/abs/2506.05997v2),  [pdf](http://arxiv.org/pdf/2506.05997v2)

**Tags**: cs.RO 



### Telecommunications fiber-optic and free-space quantum local area   networks at the Air Force Research Laboratory
**Authors**: Erin Sheridan, Nicholas J. Barton, Richard Birrittella, Vedansh Nehra, Zachary Smith, Christopher Tison, Amos Matthew Smith, Shashank Dharanibalan, Vijit Bedi, David Hucul, Benjamin Kyle, Christopher Nadeau, Mary Draper, John Heinig, Scott Faulkner, Randal Scales, Andrew M. Brownell, Stefan Preble, James Schneeloch, Samuel Schwab, Daniel Campbell, Derrick Sica, Peter Ricci, Vladimir Nikulin, John Malowicki, Jacob Hall, Michael Fanto, Matthew D. LaHaye, Laura Wessing, Paul M. Alsing, Kathy-Anne Soderberg, Donald Telesca

**Updated**: 2025-09-04T14:24:40Z

**Summary**: As quantum computing, sensing, timing, and networking technologies mature, quantum network testbeds are being deployed across the United States and around the world. To support the Air Force Research Laboratory (AFRL)'s mission of building heterogeneous quantum networks, we report on the development of Quantum Local Area Networks (QLANs) operating at telecommunications-band frequencies. The multi-node, reconfigurable QLANs include deployed optical fiber and free-space links connected to pristine laboratory environments and rugged outdoor test facilities. Each QLAN is tailored to distinct operating conditions and use cases, with unique environmental characteristics and capabilities. We present network topologies and in-depth link characterization data for three such networks. Using photonic integrated circuit-based sources of entangled photons, we demonstrate entanglement distribution of time-energy Bell states across deployed fiber in a wooded environment. The high quality of the entanglement is confirmed by a Clauser-Horne-Shimony-Holt inequality violation of $S=2.717$, approaching the theoretical maximum of $S=2.828$. We conclude with a discussion of future work aimed at expanding QLAN functionality and enabling entanglement distribution between heterogeneous matter-based quantum systems, including superconducting qubits and trapped ions. These results underscore the practical viability of field-deployable, qubit-agnostic quantum network infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2508.01030v2),  [pdf](http://arxiv.org/pdf/2508.01030v2)

**Tags**: quant-ph 



### A Survey of Graph Retrieval-Augmented Generation for Customized Large   Language Models
**Authors**: Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Yi Chang, Xiao Huang

**Updated**: 2025-09-04T14:24:19Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.

**Link**: [arxiv](http://arxiv.org/abs/2501.13958v2),  [pdf](http://arxiv.org/pdf/2501.13958v2)

**Tags**: cs.CL cs.AI cs.IR 



### How many patients could we save with LLM priors?
**Authors**: Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer

**Updated**: 2025-09-04T14:23:35Z

**Summary**: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.

**Link**: [arxiv](http://arxiv.org/abs/2509.04250v1),  [pdf](http://arxiv.org/pdf/2509.04250v1)

**Tags**: stat.ME cs.AI cs.ET cs.IR stat.AP 



### Integrating Pruning with Quantization for Efficient Deep Neural Networks   Compression
**Authors**: Sara Makenali, Babak Rokh, Ali Azarpeyvand

**Updated**: 2025-09-04T14:17:28Z

**Summary**: Deep Neural Networks (DNNs) have achieved significant advances in a wide range of applications. However, their deployment on resource-constrained devices remains a challenge due to the large number of layers and parameters, which result in considerable computational and memory demands. To address this issue, pruning and quantization are two widely used compression techniques, commonly applied individually in most studies to reduce model size and enhance processing speed. Nevertheless, combining these two techniques can yield even greater compression benefits. Effectively integrating pruning and quantization to harness their complementary advantages poses a challenging task, primarily due to their potential impact on model accuracy and the complexity of jointly optimizing both processes. In this paper, we propose two approaches that integrate similarity-based filter pruning with Adaptive Power-of-Two (APoT) quantization to achieve higher compression efficiency while preserving model accuracy. In the first approach, pruning and quantization are applied simultaneously during training. In the second approach, pruning is performed first to remove less important parameters, followed by quantization of the pruned model using low-bit representations. Experimental results demonstrate that our proposed approaches achieve effective model compression with minimal accuracy degradation, making them well-suited for deployment on devices with limited computational resources.

**Link**: [arxiv](http://arxiv.org/abs/2509.04244v1),  [pdf](http://arxiv.org/pdf/2509.04244v1)

**Tags**: cs.NE 



### DMN-Guided Prompting: A Framework for Controlling LLM Behavior
**Authors**: Shaghayegh Abedi, Amin Jalali

**Updated**: 2025-09-04T14:12:36Z

**Summary**: Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting in our case study. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.

**Link**: [arxiv](http://arxiv.org/abs/2505.11701v2),  [pdf](http://arxiv.org/pdf/2505.11701v2)

**Tags**: cs.AI 



### Kolb-Based Experiential Learning for Generalist Agents with Human-Level   Kaggle Data Science Performance
**Authors**: Antoine Grosnit, Alexandre Maraval, Refinath S N, Zichao Zhao, James Dora, Giuseppe Paolo, Albert Thomas, Jonas Gonzalez, Abhineet Kumar, Khyati Khandelwal, Abdelhakim Benechehab, Hamza Cherkaoui, Youssef Attia El-Hili, Kun Shao, Jianye Hao, Jun Yao, Balzs Kgl, Jun Wang

**Updated**: 2025-09-04T14:07:26Z

**Summary**: Human expertise emerges through iterative cycles of interaction, reflection, and internal model updating, which are central to cognitive theories such as Kolb's experiential learning and Vygotsky's zone of proximal development. In contrast, current AI systems, particularly LLM agents, rely on static pre-training or rigid workflows, lacking mechanisms for continual adaptation. Recent studies identified early cognitive traits in LLM agents (reflection, revision, and self-correction) suggesting foundational elements of human-like experiential learning. Thus the key question: Can we design LLM agents capable of structured, cognitively grounded learning similar to human processes? In response, we propose a computational framework of Kolb's learning cycle with Vygotsky's ZPD for autonomous agents. Our architecture separates extrinsic (environment interaction) and intrinsic (internal reflection/abstraction) functions, enabling cognitively grounded scaffolded learning, where the agent initially learns within structured environments, followed by open-ended generalisation. This approach empowers agents to master complex tasks ; domains that traditional fine-tuning or simple reflective methods could not tackle effectively. Its potential is powerfully demonstrated via direct comparison with humans in real-world Kaggle data science competitions. Learning fully automated data science code generation across 81 tasks, our system, Agent K, demonstrated the ability to perform the entire workflow autonomously, achieving an Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2% among 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals level performance - including 4 gold and 4 silver on prize-awarding competitions - Agent K is the 1st AI system to successfully integrate Kolb- and Vygotsky-inspired human cognitive learning, marking a major step toward generalist AI.

**Link**: [arxiv](http://arxiv.org/abs/2411.03562v2),  [pdf](http://arxiv.org/pdf/2411.03562v2)

**Tags**: cs.LG cs.AI 



### Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large   Language Models via a Multi-Paradigm Perspective
**Authors**: Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Ziyi Yang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei

**Updated**: 2025-09-04T14:01:21Z

**Summary**: Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.

**Link**: [arxiv](http://arxiv.org/abs/2501.11110v4),  [pdf](http://arxiv.org/pdf/2501.11110v4)

**Tags**: cs.CL 



### PAK-UCB Contextual Bandit: An Online Learning Approach to Prompt-Aware   Selection of Generative Models and LLMs
**Authors**: Xiaoyan Hu, Ho-fung Leung, Farzan Farnia

**Updated**: 2025-09-04T13:51:56Z

**Summary**: Selecting a sample generation scheme from multiple prompt-based generative models, including large language models (LLMs) and prompt-guided image and video generation models, is typically addressed by choosing the model that maximizes an averaged evaluation score. However, this score-based selection overlooks the possibility that different models achieve the best generation performance for different types of text prompts. An online identification of the best generation model for various input prompts can reduce the costs associated with querying sub-optimal models. In this work, we explore the possibility of varying rankings of text-based generative models for different text prompts and propose an online learning framework to predict the best data generation model for a given input prompt. The proposed PAK-UCB algorithm addresses a contextual bandit (CB) setting with shared context variables across the arms, utilizing the generated data to update kernel-based functions that predict the score of each model available for unseen text prompts. Additionally, we leverage random Fourier features (RFF) to accelerate the online learning process of PAK-UCB. Our numerical experiments on real and simulated text-to-image and image-to-text generative models show that RFF-UCB performs successfully in identifying the best generation model across different sample types. The code is available at: github.com/yannxiaoyanhu/dgm-online-select.

**Link**: [arxiv](http://arxiv.org/abs/2410.13287v6),  [pdf](http://arxiv.org/pdf/2410.13287v6)

**Tags**: cs.LG 



### An Automated, Scalable Machine Learning Model Inversion Assessment   Pipeline
**Authors**: Tyler Shumaker, Jessica Carpenter, David Saranchak, Nathaniel D. Bastian

**Updated**: 2025-09-04T13:39:37Z

**Summary**: Machine learning (ML) models have the potential to transform military battlefields, presenting a large external pressure to rapidly incorporate them into operational settings. However, it is well-established that these ML models are vulnerable to a number of adversarial attacks throughout the model deployment pipeline that threaten to negate battlefield advantage. One broad category is privacy attacks (such as model inversion) where an adversary can reverse engineer information from the model, such as the sensitive data used in its training. The ability to quantify the risk of model inversion attacks (MIAs) is not well studied, and there is a lack of automated developmental test and evaluation (DT&E) tools and metrics to quantify the effectiveness of privacy loss of the MIA. The current DT&E process is difficult because ML model inversions can be hard for a human to interpret, subjective when they are interpretable, and difficult to quantify in terms of inversion quality. Additionally, scaling the DT&E process is challenging due to many ML model architectures and data modalities that need to be assessed. In this work, we present a novel DT&E tool that quantifies the risk of data privacy loss from MIAs and introduces four adversarial risk dimensions to quantify privacy loss. Our DT&E pipeline combines inversion with vision language models (VLMs) to improve effectiveness while enabling scalable analysis. We demonstrate effectiveness using multiple MIA techniques and VLMs configured for zero-shot classification and image captioning. We benchmark the pipeline using several state-of-the-art MIAs in the computer vision domain with an image classification task that is typical in military applications. In general, our innovative pipeline extends the current model inversion DT&E capabilities by improving the effectiveness and scalability of the privacy loss analysis in an automated fashion.

**Link**: [arxiv](http://arxiv.org/abs/2509.04214v1),  [pdf](http://arxiv.org/pdf/2509.04214v1)

**Tags**: cs.CR 



### Sailing Towards Zero-Shot State Estimation using Foundation Models   Combined with a UKF
**Authors**: Tobin Holtmann, David Stenger, Andres Posada-Moreno, Friedrich Solowjow, Sebastian Trimpe

**Updated**: 2025-09-04T13:38:54Z

**Summary**: State estimation in control and systems engineering traditionally requires extensive manual system identification or data-collection effort. However, transformer-based foundation models in other domains have reduced data requirements by leveraging pre-trained generalist models. Ultimately, developing zero-shot foundation models of system dynamics could drastically reduce manual deployment effort. While recent work shows that transformer-based end-to-end approaches can achieve zero-shot performance on unseen systems, they are limited to sensor models seen during training. We introduce the foundation model unscented Kalman filter (FM-UKF), which combines a transformer-based model of system dynamics with analytically known sensor models via an UKF, enabling generalization across varying dynamics without retraining for new sensor configurations. We evaluate FM-UKF on a new benchmark of container ship models with complex dynamics, demonstrating a competitive accuracy, effort, and robustness trade-off compared to classical methods with approximate system knowledge and to an end-to-end approach. The benchmark and dataset are open sourced to further support future research in zero-shot state estimation via foundation models.

**Link**: [arxiv](http://arxiv.org/abs/2509.04213v1),  [pdf](http://arxiv.org/pdf/2509.04213v1)

**Tags**: eess.SY cs.LG cs.SY 



### COBRA: Multimodal Sensing Deep Learning Framework for Remote Chronic   Obesity Management via Wrist-Worn Activity Monitoring
**Authors**: Zhengyang Shen, Bo Gao, Mayue Shi

**Updated**: 2025-09-04T13:35:49Z

**Summary**: Chronic obesity management requires continuous monitoring of energy balance behaviors, yet traditional self-reported methods suffer from significant underreporting and recall bias, and difficulty in integration with modern digital health systems. This study presents COBRA (Chronic Obesity Behavioral Recognition Architecture), a novel deep learning framework for objective behavioral monitoring using wrist-worn multimodal sensors. COBRA integrates a hybrid D-Net architecture combining U-Net spatial modeling, multi-head self-attention mechanisms, and BiLSTM temporal processing to classify daily activities into four obesity-relevant categories: Food Intake, Physical Activity, Sedentary Behavior, and Daily Living. Validated on the WISDM-Smart dataset with 51 subjects performing 18 activities, COBRA's optimal preprocessing strategy combines spectral-temporal feature extraction, achieving high performance across multiple architectures. D-Net demonstrates 96.86% overall accuracy with category-specific F1-scores of 98.55% (Physical Activity), 95.53% (Food Intake), 94.63% (Sedentary Behavior), and 98.68% (Daily Living), outperforming state-of-the-art baselines by 1.18% in accuracy. The framework shows robust generalizability with low demographic variance (<3%), enabling scalable deployment for personalized obesity interventions and continuous lifestyle monitoring.

**Link**: [arxiv](http://arxiv.org/abs/2509.04210v1),  [pdf](http://arxiv.org/pdf/2509.04210v1)

**Tags**: cs.CE cs.LG 



### Are LLM Agents the New RPA? A Comparative Study with RPA Across   Enterprise Workflows
**Authors**: Petr Prcha, Michaela Matoukov, Jan Strnad

**Updated**: 2025-09-04T13:22:44Z

**Summary**: The emergence of large language models (LLMs) has introduced a new paradigm in automation: LLM agents or Agentic Automation with Computer Use (AACU). Unlike traditional Robotic Process Automation (RPA), which relies on rule-based workflows and scripting, AACU enables intelligent agents to perform tasks through natural language instructions and autonomous interaction with user interfaces. This study investigates whether AACU can serve as a viable alternative to RPA in enterprise workflow automation. We conducted controlled experiments across three standard RPA challenges data entry, monitoring, and document extraction comparing RPA (via UiPath) and AACU (via Anthropic's Computer Use Agent) in terms of speed, reliability, and development effort. Results indicate that RPA outperforms AACU in execution speed and reliability, particularly in repetitive, stable environments. However, AACU significantly reduces development time and adapts more flexibly to dynamic interfaces. While current AACU implementations are not yet production-ready, their promise in rapid prototyping and lightweight automation is evident. Future research should explore multi-agent orchestration, hybrid RPA-AACU architectures, and more robust evaluation across industries and platforms.

**Link**: [arxiv](http://arxiv.org/abs/2509.04198v1),  [pdf](http://arxiv.org/pdf/2509.04198v1)

**Tags**: cs.CY cs.MA 



### InferLog: Accelerating LLM Inference for Online Log Parsing via   ICL-oriented Prefix Caching
**Authors**: Yilun Wang, Pengfei Chen, Haiyu Huang, Zilong He, Gou Tan, Chuanfu Zhang, Jingkai He, Zibin Zheng

**Updated**: 2025-09-04T13:14:33Z

**Summary**: Modern software systems generate massive volumes of runtime logs, necessitating efficient and accurate log parsing to enable critical downstream tasks such as anomaly detection and root cause analysis. Recently, large language models (LLMs) have achieved advanced accuracy on log parsing, but their deployment in production environments faces two major limitations: (1) the privacy risks associated with commercial LLMs, driving the adoption of local deployment, and (2) the stringent latency and throughput requirements imposed by high-volume log streams, which existing LLM-based parsers fail to meet. Although recent efforts have reduced the number of LLM queries, they overlook the high latency of the LLM invocations, where concurrent log parsing requests can cause serve performance degradation of LLM inference system.   In this study, we present InferLog, the first LLM inference optimization method for online log parsing. Our key insight is that the inference efficiency emerges as the vital bottleneck in LLM-based online log parsing, rather than parsing accuracy. InferLog accelerates inference by designing (1) A Prefix-aware ICL Refinement policy to refine the examples and permutation of in-context learning to improve the prefix caching efficiency. (2) A rapid and task-specific configuration tuning pipeline based on meta-learning to find the optimal LLM scheduling-related configuration for dynamic log parsing workloads. The experimental results based on Loghub dataset and vLLM demonstrate that InferLog significantly outperforms existing inference optimization methods and markedly accelerates the state-of-the-art LLM-based log parser without compromising parsing accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2507.08523v2),  [pdf](http://arxiv.org/pdf/2507.08523v2)

**Tags**: cs.SE 



### KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and   Runtime Logs Analysis
**Authors**: Omri Sgan Cohen, Ehud Malul, Yair Meidan, Dudu Mimran, Yuval Elovici, Asaf Shabtai

**Updated**: 2025-09-04T13:13:57Z

**Summary**: The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native applications has introduced significant security challenges, such as misconfigured resources and overly permissive configurations. Failing to address these issues can result in unauthorized access, privilege escalation, and lateral movement within clusters. Most existing K8s security solutions focus on detecting misconfigurations, typically through static analysis or anomaly detection. In contrast, this paper presents KubeGuard, a novel runtime log-driven recommender framework aimed at mitigating risks by addressing overly permissive configurations. KubeGuard is designed to harden K8s environments through two complementary tasks: Resource Creation and Resource Refinement. It leverages large language models (LLMs) to analyze manifests and runtime logs reflecting actual system behavior, using modular prompt-chaining workflows. This approach enables KubeGuard to create least-privilege configurations for new resources and refine existing manifests to reduce the attack surface. KubeGuard's output manifests are presented as recommendations that users (e.g., developers and operators) can review and adopt to enhance cluster security. Our evaluation demonstrates that KubeGuard effectively generates and refines K8s manifests for Roles, NetworkPolicies, and Deployments, leveraging both proprietary and open-source LLMs. The high precision, recall, and F1-scores affirm KubeGuard's practicality as a framework that translates runtime observability into actionable, least-privilege configuration guidance.

**Link**: [arxiv](http://arxiv.org/abs/2509.04191v1),  [pdf](http://arxiv.org/pdf/2509.04191v1)

**Tags**: cs.CR cs.LG 



### Set Block Decoding is a Language Model Inference Accelerator
**Authors**: Itai Gat, Heli Ben-Hamu, Marton Havasi, Daniel Haziza, Jeremy Reizenstein, Gabriel Synnaeve, David Lopez-Paz, Brian Karrer, Yaron Lipman

**Updated**: 2025-09-04T13:02:39Z

**Summary**: Autoregressive next token prediction language models offer powerful capabilities but face significant challenges in practical deployment due to the high computational and memory costs of inference, particularly during the decoding stage. We introduce Set Block Decoding (SBD), a simple and flexible paradigm that accelerates generation by integrating standard next token prediction (NTP) and masked token prediction (MATP) within a single architecture. SBD allows the model to sample multiple, not necessarily consecutive, future tokens in parallel, a key distinction from previous acceleration methods. This flexibility allows the use of advanced solvers from the discrete diffusion literature, offering significant speedups without sacrificing accuracy. SBD requires no architectural changes or extra training hyperparameters, maintains compatibility with exact KV-caching, and can be implemented by fine-tuning existing next token prediction models. By fine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x reduction in the number of forward passes required for generation while achieving same performance as equivalent NTP training.

**Link**: [arxiv](http://arxiv.org/abs/2509.04185v1),  [pdf](http://arxiv.org/pdf/2509.04185v1)

**Tags**: cs.LG 



### MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn   Mental Health Counseling Sessions
**Authors**: Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych

**Updated**: 2025-09-04T12:59:24Z

**Summary**: The growing demand for scalable psychological counseling highlights the need for fine-tuning open-source Large Language Models (LLMs) with high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. In addition, we address inconsistencies in prior evaluation protocols by proposing a unified evaluation framework integrating diverse automatic and expert metrics. Furthermore, we expand the expert evaluations from four aspects of counseling in previous works to nine aspects, enabling a more thorough and robust assessment of data quality. Empirical results show that MAGneT significantly outperforms existing methods in quality, diversity, and therapeutic alignment of the generated counseling sessions, improving general counseling skills by 3.2% and CBT-specific skills by 4.3% on average on cognitive therapy rating scale (CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases on average across all aspects. Moreover, fine-tuning an open-source model on MAGneT-generated sessions shows better performance, with improvements of 6.3% on general counseling skills and 7.3% on CBT-specific skills on average on CTRS over those fine-tuned with sessions generated by baseline methods. We also make our code and data public.

**Link**: [arxiv](http://arxiv.org/abs/2509.04183v1),  [pdf](http://arxiv.org/pdf/2509.04183v1)

**Tags**: cs.CL cs.AI 



### Real-time Object Detection and Associated Hardware Accelerators   Targeting Autonomous Vehicles: A Review
**Authors**: Safa Sali, Anis Meribout, Ashiyana Majeed, Mahmoud Meribout, Juan Pablo, Varun Tiwari, Asma Baobaid

**Updated**: 2025-09-04T12:45:49Z

**Summary**: The efficiency of object detectors depends on factors like detection accuracy, processing time, and computational resources. Processing time is crucial for real-time applications, particularly for autonomous vehicles (AVs), where instantaneous responses are vital for safety. This review paper provides a concise yet comprehensive survey of real-time object detection (OD) algorithms for autonomous cars delving into their hardware accelerators (HAs). Non-neural network-based algorithms, which use statistical image processing, have been entirely substituted by AI algorithms, such as different models of convolutional neural networks (CNNs). Their intrinsically parallel features led them to be deployable into edge-based HAs of various types, where GPUs and, to a lesser extent, ASIC (application-specific integrated circuit) remain the most widely used. Throughputs of hundreds of frames/s (fps) could be reached; however, handling object detection for all the cameras available in a typical AV requires further hardware and algorithmic improvements. The intensive competition between AV providers has limited the disclosure of algorithms, firmware, and even hardware platform details. This remains a hurdle for researchers, as commercial systems provide valuable insights while academics undergo lengthy training and testing on restricted datasets and road scenarios. Consequently, many AV research papers may not be reflected in end products, being developed under limited conditions. This paper surveys state-of-the-art OD algorithms and aims to bridge the gap with technologies in commercial AVs. To our knowledge, this aspect has not been addressed in earlier surveys. Hence, the paper serves as a tangible reference for researchers designing future generations of vehicles, expected to be fully autonomous for comfort and safety.

**Link**: [arxiv](http://arxiv.org/abs/2509.04173v1),  [pdf](http://arxiv.org/pdf/2509.04173v1)

**Tags**: cs.AR 



### Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical   Definitions
**Authors**: Lan Zhang, Marco Valentino, Andre Freitas

**Updated**: 2025-09-04T12:43:14Z

**Summary**: Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions: a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalization, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we augment LLMs' formalizations through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2502.12065v3),  [pdf](http://arxiv.org/pdf/2502.12065v3)

**Tags**: cs.CL cs.FL 



### Real Time FPGA Based Transformers & VLMs for Vision Tasks: SOTA Designs   and Optimizations
**Authors**: Safa Mohammed Sali, Mahmoud Meribout, Ashiyana Abdul Majeed

**Updated**: 2025-09-04T12:35:26Z

**Summary**: Transformers and vision-language models (VLMs) have emerged as dominant architectures in computer vision and multimodal AI, offering state-of-the-art performance in tasks such as image classification, object detection, visual question answering, and caption generation. However, their high computational complexity, large memory footprints, and irregular data access patterns present significant challenges for deployment in latency- and power-constrained environments. Field-programmable gate arrays (FPGAs) provide an attractive hardware platform for such workloads due to their reconfigurability, fine-grained parallelism, and potential for energy-efficient acceleration. This paper presents a comprehensive review of design trade-offs, optimization strategies, and implementation challenges for FPGA-based inference of transformers and VLMs. We examine critical factors such as device-class selection, memory subsystem constraints, dataflow orchestration, quantization strategies, sparsity exploitation, and toolchain choices, alongside modality-specific issues unique to VLMs, including heterogeneous compute balancing and cross-attention memory management. Additionally, we discuss emerging trends in hardware-algorithm co-design, highlighting innovations in attention mechanisms, compression, and modular overlays to improve efficiency and adaptability. Practical issues such as runtime flexibility, verification overhead, and the absence of standardized FPGA multimodal benchmarks are also considered. Finally, we outline future directions toward scalable, portable, and reconfigurable FPGA solutions that adapt to evolving model architectures while sustaining high utilization and predictable performance. This synthesis offers both a technical foundation and a forward-looking perspective to help bridge the gap between advanced multimodal AI models and efficient FPGA deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.04162v1),  [pdf](http://arxiv.org/pdf/2509.04162v1)

**Tags**: cs.AR 



### Real Time FPGA Based CNNs for Detection, Classification, and Tracking in   Autonomous Systems: State of the Art Designs and Optimizations
**Authors**: Safa Mohammed Sali, Mahmoud Meribout, Ashiyana Abdul Majeed

**Updated**: 2025-09-04T12:28:36Z

**Summary**: This paper presents a comprehensive review of recent advances in deploying convolutional neural networks (CNNs) for object detection, classification, and tracking on Field Programmable Gate Arrays (FPGAs). With the increasing demand for real-time computer vision applications in domains such as autonomous vehicles, robotics, and surveillance, FPGAs have emerged as a powerful alternative to GPUs and ASICs due to their reconfigurability, low power consumption, and deterministic latency. We critically examine state-of-the-art FPGA implementations of CNN-based vision tasks, covering algorithmic innovations, hardware acceleration techniques, and the integration of optimization strategies like pruning, quantization, and sparsity-aware methods to maximize performance within hardware constraints. This survey also explores the landscape of modern FPGA platforms, including classical LUT-DSP based architectures, System-on-Chip (SoC) FPGAs, and Adaptive Compute Acceleration Platforms (ACAPs), comparing their capabilities in handling deep learning workloads. Furthermore, we review available software development tools such as Vitis AI, FINN, and Intel FPGA AI Suite, which significantly streamline the design and deployment of AI models on FPGAs. The paper uniquely discusses hybrid architecture that combine GPUs and FPGAs for collaborative acceleration of AI inference, addressing challenges related to energy efficiency and throughput. Additionally, we highlight hardware-software co-design practices, dataflow optimizations, and pipelined processing techniques essential for real-time inference on resource-constrained devices. Through this survey, researchers and engineers are equipped with insights to develop next-generation, power-efficient, and high-performance vision systems optimized for FPGA deployment in edge and embedded applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.04153v1),  [pdf](http://arxiv.org/pdf/2509.04153v1)

**Tags**: cs.AR 



### TAGAL: Tabular Data Generation using Agentic LLM Methods
**Authors**: Benot Ronval, Pierre Dupont, Siegfried Nijssen

**Updated**: 2025-09-04T12:25:14Z

**Summary**: The generation of data is a common approach to improve the performance of machine learning tasks, among which is the training of models for classification. In this paper, we present TAGAL, a collection of methods able to generate synthetic tabular data using an agentic workflow. The methods leverage Large Language Models (LLMs) for an automatic and iterative process that uses feedback to improve the generated data without any further LLM training. The use of LLMs also allows for the addition of external knowledge in the generation process. We evaluate TAGAL across diverse datasets and different aspects of quality for the generated data. We look at the utility of downstream ML models, both by training classifiers on synthetic data only and by combining real and synthetic data. Moreover, we compare the similarities between the real and the generated data. We show that TAGAL is able to perform on par with state-of-the-art approaches that require LLM training and generally outperforms other training-free approaches. These findings highlight the potential of agentic workflow and open new directions for LLM-based data generation methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.04152v1),  [pdf](http://arxiv.org/pdf/2509.04152v1)

**Tags**: cs.LG cs.AI 



### Explicit Learning and the LLM in Machine Translation
**Authors**: Malik Marmonier, Rachel Bawden, Benot Sagot

**Updated**: 2025-09-04T12:20:43Z

**Summary**: This study explores an LLM's ability to learn new languages using explanations found in a grammar book, a process we term "explicit learning." To rigorously assess this ability, we design controlled translation experiments between English and constructed languages generated, through specific cryptographic means, from Latin or French. Contrary to previous studies, our results demonstrate that LLMs do possess a measurable capacity for explicit learning. This ability, however, diminishes as the complexity of the linguistic phenomena to be learned increases. Supervised fine-tuning on ad hoc chains of thought significantly enhances LLM performance but struggles to generalize to typologically novel or more complex linguistic features. These findings point to the need for more diverse training sets and alternative fine-tuning strategies to further improve explicit learning by LLMs, benefiting low-resource languages typically described in grammar books but lacking extensive corpora.

**Link**: [arxiv](http://arxiv.org/abs/2503.09454v4),  [pdf](http://arxiv.org/pdf/2503.09454v4)

**Tags**: cs.CL 



### Quantifying Calibration Error in Neural Networks Through Evidence-Based   Theory
**Authors**: Koffi Ismael Ouattara, Ioannis Krontiris, Theo Dimitrakos, Frank Kargl

**Updated**: 2025-09-04T12:15:53Z

**Summary**: Trustworthiness in neural networks is crucial for their deployment in critical applications, where reliability, confidence, and uncertainty play pivotal roles in decision-making. Traditional performance metrics such as accuracy and precision fail to capture these aspects, particularly in cases where models exhibit overconfidence. To address these limitations, this paper introduces a novel framework for quantifying the trustworthiness of neural networks by incorporating subjective logic into the evaluation of Expected Calibration Error (ECE). This method provides a comprehensive measure of trust, disbelief, and uncertainty by clustering predicted probabilities and fusing opinions using appropriate fusion operators. We demonstrate the effectiveness of this approach through experiments on MNIST and CIFAR-10 datasets, where post-calibration results indicate improved trustworthiness. The proposed framework offers a more interpretable and nuanced assessment of AI models, with potential applications in sensitive domains such as healthcare and autonomous systems.

**Link**: [arxiv](http://arxiv.org/abs/2411.00265v3),  [pdf](http://arxiv.org/pdf/2411.00265v3)

**Tags**: cs.LG cs.AI math.LO 



### Enhancing Technical Documents Retrieval for RAG
**Authors**: Songjiang Lai, Tsun-Hin Cheung, Ka-Chun Fung, Kaiwen Xue, Kwan-Ho Lin, Yan-Ming Choi, Vincent Ng, Kin-Man Lam

**Updated**: 2025-09-04T12:11:03Z

**Summary**: In this paper, we introduce Technical-Embeddings, a novel framework designed to optimize semantic retrieval in technical documentation, with applications in both hardware and software development. Our approach addresses the challenges of understanding and retrieving complex technical content by leveraging the capabilities of Large Language Models (LLMs). First, we enhance user queries by generating expanded representations that better capture user intent and improve dataset diversity, thereby enriching the fine-tuning process for embedding models. Second, we apply summary extraction techniques to encode essential contextual information, refining the representation of technical documents. To further enhance retrieval performance, we fine-tune a bi-encoder BERT model using soft prompting, incorporating separate learning parameters for queries and document context to capture fine-grained semantic nuances. We evaluate our approach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that Technical-Embeddings significantly outperforms baseline models in both precision and recall. Our findings highlight the effectiveness of integrating query expansion and contextual summarization to enhance information access and comprehension in technical domains. This work advances the state of Retrieval-Augmented Generation (RAG) systems, offering new avenues for efficient and accurate technical document retrieval in engineering and product development workflows.

**Link**: [arxiv](http://arxiv.org/abs/2509.04139v1),  [pdf](http://arxiv.org/pdf/2509.04139v1)

**Tags**: cs.IR cs.AI 



### Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges
**Authors**: Yuqi Tang, Kehua Feng, Yunfeng Wang, Zhiwen Chen, Chengfei Lv, Gang Yu, Qiang Zhang, Keyan Ding

**Updated**: 2025-09-04T11:57:46Z

**Summary**: Evaluating the conversational abilities of large language models (LLMs) remains a challenging task. Current mainstream approaches primarily rely on the "LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator to assess dialogue quality. However, such methods often suffer from various biases, which undermine the reliability and consistency of the evaluation results. To mitigate these biases, recent methods employ multiple LLMs as judges and aggregate their judgments to select the optimal assessment. Although effective, this multi-judge approach incurs significant computational overhead during inference. In this paper, we propose an efficient multi-turn dialogue evaluator that captures the collective wisdom of multiple LLM judges by aggregating their preference knowledge into a single model. Our approach preserves the advantages of diverse multi-judge feedback while drastically reducing the evaluation cost, enabling fast and flexible dialogue quality assessment. Extensive experiments on seven single rating and pairwise comparison dialogue evaluation benchmarks demonstrate that our method outperforms existing baselines across diverse scenarios, showcasing its efficiency and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2508.00454v2),  [pdf](http://arxiv.org/pdf/2508.00454v2)

**Tags**: cs.CL 



### Oyster-I: Beyond Refusal -- Constructive Safety Alignment for   Responsible Language Models
**Authors**: Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Yitong Yang, Jialing Tao, Hui Xue

**Updated**: 2025-09-04T11:54:06Z

**Summary**: Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.01909v2),  [pdf](http://arxiv.org/pdf/2509.01909v2)

**Tags**: cs.AI cs.CL cs.CY cs.HC cs.SC 



### MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image   Generation
**Authors**: Yuan Zhao, Liu Lin

**Updated**: 2025-09-04T11:44:28Z

**Summary**: Text-to-image diffusion models have achieved remarkable image quality, but they still struggle with complex, multiele ment prompts, and limited stylistic diversity. To address these limitations, we propose a Multi-Expert Planning and Gen eration Framework (MEPG) that synergistically integrates position- and style-aware large language models (LLMs) with spatial-semantic expert modules. The framework comprises two core components: (1) a Position-Style-Aware (PSA) module that utilizes a supervised fine-tuned LLM to decom pose input prompts into precise spatial coordinates and style encoded semantic instructions; and (2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera tion through dynamic expert routing across both local regions and global areas. During the generation process for each lo cal region, specialized models (e.g., realism experts, styliza tion specialists) are selectively activated for each spatial par tition via attention-based gating mechanisms. The architec ture supports lightweight integration and replacement of ex pert models, providing strong extensibility. Additionally, an interactive interface enables real-time spatial layout editing and per-region style selection from a portfolio of experts. Ex periments show that MEPG significantly outperforms base line models with the same backbone in both image quality   and style diversity.

**Link**: [arxiv](http://arxiv.org/abs/2509.04126v1),  [pdf](http://arxiv.org/pdf/2509.04126v1)

**Tags**: cs.CV cs.AI 



### Enhancing FKG.in: automating Indian food composition analysis
**Authors**: Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Geeta Trilok-Kumar, Ramesh Jain

**Updated**: 2025-09-04T11:42:04Z

**Summary**: This paper presents a novel approach to compute food composition data for Indian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The primary focus is to provide a broad overview of an automated food composition analysis workflow and describe its core functionalities: nutrition data aggregation, food composition analysis, and LLM-augmented information resolution. This workflow aims to complement FKG[.]in and iteratively supplement food composition data from verified knowledge bases. Additionally, this paper highlights the challenges of representing Indian food and accessing food composition data digitally. It also reviews three key sources of food composition data: the Indian Food Composition Tables, the Indian Nutrient Databank, and the Nutritionix API. Furthermore, it briefly outlines how users can interact with the workflow to obtain diet-based health recommendations and detailed food composition information for numerous recipes. We then explore the complex challenges of analyzing Indian recipe information across dimensions such as structure, multilingualism, and uncertainty as well as present our ongoing work on LLM-based solutions to address these issues. The methods proposed in this workshop paper for AI-driven knowledge curation and information resolution are application-agnostic, generalizable, and replicable for any domain.

**Link**: [arxiv](http://arxiv.org/abs/2412.05248v3),  [pdf](http://arxiv.org/pdf/2412.05248v3)

**Tags**: cs.AI cs.CL cs.IR 



### TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering
**Authors**: Ayan Banerjee, Josep Llads, Umapada Pal, Anjan Dutta

**Updated**: 2025-09-04T11:37:06Z

**Summary**: Text-to-story visualization is challenging due to the need for consistent interaction among multiple characters across frames. Existing methods struggle with character consistency, leading to artifact generation and inaccurate dialogue rendering, which results in disjointed storytelling. In response, we introduce TaleDiffusion, a novel framework for generating multi-character stories with an iterative process, maintaining character consistency, and accurate dialogue assignment via postprocessing. Given a story, we use a pre-trained LLM to generate per-frame descriptions, character details, and dialogues via in-context learning, followed by a bounded attention-based per-box mask technique to control character interactions and minimize artifacts. We then apply an identity-consistent self-attention mechanism to ensure character consistency across frames and region-aware cross-attention for precise object placement. Dialogues are also rendered as bubbles and assigned to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion outperforms existing methods in consistency, noise reduction, and dialogue rendering.

**Link**: [arxiv](http://arxiv.org/abs/2509.04123v1),  [pdf](http://arxiv.org/pdf/2509.04123v1)

**Tags**: cs.CV 



### Context Reasoner: Incentivizing Reasoning Capability for Contextualized   Privacy and Safety Compliance via Reinforcement Learning
**Authors**: Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song

**Updated**: 2025-09-04T11:31:24Z

**Summary**: While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +8.58% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2505.14585v2),  [pdf](http://arxiv.org/pdf/2505.14585v2)

**Tags**: cs.CL 



### MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages
**Authors**: Dan Saattrup Smart

**Updated**: 2025-09-05T09:12:03Z

**Summary**: We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which covers 306 languages. The context data comes from Wikipedia articles, with questions generated by an LLM and the answers appearing verbatim in the Wikipedia articles. We conduct a crowdsourced human evaluation of the fluency of the generated questions across 30 of the languages, providing evidence that the questions are of good quality. We evaluate 6 different language models, both decoder and encoder models of varying sizes, showing that the benchmark is sufficiently difficult and that there is a large performance discrepancy amongst the languages. The dataset and survey evaluations are freely available.

**Link**: [arxiv](http://arxiv.org/abs/2509.04111v2),  [pdf](http://arxiv.org/pdf/2509.04111v2)

**Tags**: cs.CL 



### Towards Stable and Personalised Profiles for Lexical Alignment in Spoken   Human-Agent Dialogue
**Authors**: Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx

**Updated**: 2025-09-04T11:07:27Z

**Summary**: Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.

**Link**: [arxiv](http://arxiv.org/abs/2509.04104v1),  [pdf](http://arxiv.org/pdf/2509.04104v1)

**Tags**: cs.CL cs.HC 



### Cloud-Assisted Remote Control for Aerial Robots: From Theory to   Proof-of-Concept Implementation
**Authors**: Achilleas Santi Seisa, Viswa Narayanan Sankaranarayanan, Gerasimos Damigos, Sumeet Gajanan Satpute, George Nikolakopoulos

**Updated**: 2025-09-04T10:53:27Z

**Summary**: Cloud robotics has emerged as a promising technology for robotics applications due to its advantages of offloading computationally intensive tasks, facilitating data sharing, and enhancing robot coordination. However, integrating cloud computing with robotics remains a complex challenge due to network latency, security concerns, and the need for efficient resource management. In this work, we present a scalable and intuitive framework for testing cloud and edge robotic systems. The framework consists of two main components enabled by containerized technology: (a) a containerized cloud cluster and (b) the containerized robot simulation environment. The system incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling bidirectional communication between the cloud cluster container and the robot simulation environment, while simulating realistic network conditions. To achieve this, we consider the use case of cloud-assisted remote control for aerial robots, while utilizing Linux-based traffic control to introduce artificial delay and jitter, replicating variable network conditions encountered in practical cloud-robot deployments.

**Link**: [arxiv](http://arxiv.org/abs/2509.04095v1),  [pdf](http://arxiv.org/pdf/2509.04095v1)

**Tags**: cs.RO cs.DC 



### TriLiteNet: Lightweight Model for Multi-Task Visual Perception
**Authors**: Quang-Huy Che, Duc-Khai Lam

**Updated**: 2025-09-04T10:48:25Z

**Summary**: Efficient perception models are essential for Advanced Driver Assistance Systems (ADAS), as these applications require rapid processing and response to ensure safety and effectiveness in real-world environments. To address the real-time execution needs of such perception models, this study introduces the TriLiteNet model. This model can simultaneously manage multiple tasks related to panoramic driving perception. TriLiteNet is designed to optimize performance while maintaining low computational costs. Experimental results on the BDD100k dataset demonstrate that the model achieves competitive performance across three key tasks: vehicle detection, drivable area segmentation, and lane line segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of 85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for drivable area segmentation, and an Acc of 82.3% for lane line segmentation with only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed model includes a tiny configuration with just 0.14M parameters, which provides a multi-task solution with minimal computational demand. Evaluated for latency and power consumption on embedded devices, TriLiteNet in both configurations shows low latency and reasonable power during inference. By balancing performance, computational efficiency, and scalability, TriLiteNet offers a practical and deployable solution for real-world autonomous driving applications. Code is available at https://github.com/chequanghuy/TriLiteNet.

**Link**: [arxiv](http://arxiv.org/abs/2509.04092v1),  [pdf](http://arxiv.org/pdf/2509.04092v1)

**Tags**: cs.CV 



### Revisiting Third-Party Library Detection: A Ground Truth Dataset and Its   Implications Across Security Tasks
**Authors**: Jintao Gu, Haolang Lu, Guoshun Nan, Yihan Lin, Kun Wang, Yuchun Guo, Yigui Cao, Yang Liu

**Updated**: 2025-09-05T10:34:25Z

**Summary**: Accurate detection of third-party libraries (TPLs) is fundamental to Android security, supporting vulnerability tracking, malware detection, and supply chain auditing. Despite many proposed tools, their real-world effectiveness remains unclear. We present the first large-scale empirical study of ten state-of-the-art TPL detection techniques across over 6,000 apps, enabled by a new ground truth dataset with precise version-level annotations for both remote and local dependencies. Our evaluation exposes tool fragility to R8-era transformations, weak version discrimination, inaccurate correspondence of candidate libraries, difficulty in generalizing similarity thresholds, and prohibitive runtime/memory overheads at scale. Beyond tool assessment, we further analyze how TPLs shape downstream tasks, including vulnerability analysis, malware detection, secret leakage assessment, and LLM-based evaluation. From this perspective, our study provides concrete insights into how TPL characteristics affect these tasks and informs future improvements in security analysis.

**Link**: [arxiv](http://arxiv.org/abs/2509.04091v2),  [pdf](http://arxiv.org/pdf/2509.04091v2)

**Tags**: cs.CR 68M25 K.6.5; D.2.7 



### Intermediate Languages Matter: Formal Languages and LLMs affect   Neurosymbolic Reasoning
**Authors**: Alexander Beiser, David Penz, Nysret Musliu

**Updated**: 2025-09-04T10:25:50Z

**Summary**: Large language models (LLMs) achieve astonishing results on a wide range of tasks. However, their formal reasoning ability still lags behind. A promising approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators from natural to formal languages and symbolic solvers for deriving correct results. Still, the contributing factors to the success of Neurosymbolic LLM reasoning remain unclear. This paper demonstrates that one previously overlooked factor is the choice of the formal language. We introduce the intermediate language challenge: selecting a suitable formal language for neurosymbolic reasoning. By comparing four formal languages across three datasets and seven LLMs, we show that the choice of formal language affects both syntactic and semantic reasoning capabilities. We also discuss the varying effects across different LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.04083v1),  [pdf](http://arxiv.org/pdf/2509.04083v1)

**Tags**: cs.AI 



### Zero-shot Generalization in Inventory Management: Train, then Estimate   and Decide
**Authors**: Tarkan Temizz, Christina Imdahl, Remco Dijkman, Douniel Lamghari-Idrissi, Willem van Jaarsveld

**Updated**: 2025-09-04T10:16:00Z

**Summary**: Deploying deep reinforcement learning (DRL) in real-world inventory management presents challenges, including dynamic environments and uncertain problem parameters, e.g. demand and lead time distributions. These challenges highlight a research gap, suggesting a need for a unifying framework to model and solve sequential decision-making under parameter uncertainty. We address this by exploring an underexplored area of DRL for inventory management: training generally capable agents (GCAs) under zero-shot generalization (ZSG). Here, GCAs are advanced DRL policies designed to handle a broad range of sampled problem instances with diverse inventory challenges. ZSG refers to the ability to successfully apply learned policies to unseen instances with unknown parameters without retraining.   We propose a unifying Super-Markov Decision Process formulation and the Train, then Estimate and Decide (TED) framework to train and deploy a GCA tailored to inventory management applications. The TED framework consists of three phases: training a GCA on varied problem instances, continuously estimating problem parameters during deployment, and making decisions based on these estimates. Applied to periodic review inventory problems with lost sales, cyclic demand patterns, and stochastic lead times, our trained agent, the Generally Capable Lost Sales Network (GC-LSN) consistently outperforms well-known traditional policies when problem parameters are known. Moreover, under conditions where demand and/or lead time distributions are initially unknown and must be estimated, we benchmark against online learning methods that provide worst-case performance guarantees. Our GC-LSN policy, paired with the Kaplan-Meier estimator, is demonstrated to complement these methods by providing superior empirical performance.

**Link**: [arxiv](http://arxiv.org/abs/2411.00515v2),  [pdf](http://arxiv.org/pdf/2411.00515v2)

**Tags**: cs.LG 



### RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging   Evaluation of Large Language Models
**Authors**: Jingjing Liu, Zeming Liu, Zihao Cheng, Mengliang He, Xiaoming Shi, Yuhang Guo, Xiangrong Zhu, Yuanfang Guo, Yunhong Wang, Haifeng Wang

**Updated**: 2025-09-04T10:13:21Z

**Summary**: Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.

**Link**: [arxiv](http://arxiv.org/abs/2509.04078v1),  [pdf](http://arxiv.org/pdf/2509.04078v1)

**Tags**: cs.SE cs.AI 



### Solving Robotics Tasks with Prior Demonstration via   Exploration-Efficient Deep Reinforcement Learning
**Authors**: Chengyandan Shen, Christoffer Sloth

**Updated**: 2025-09-04T10:02:32Z

**Summary**: This paper proposes an exploration-efficient Deep Reinforcement Learning with Reference policy (DRLR) framework for learning robotics tasks that incorporates demonstrations. The DRLR framework is developed based on an algorithm called Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve IBRL by modifying the action selection module. The proposed action selection module provides a calibrated Q-value, which mitigates the bootstrapping error that otherwise leads to inefficient exploration. Furthermore, to prevent the RL policy from converging to a sub-optimal policy, SAC is used as the RL policy instead of TD3. The effectiveness of our method in mitigating bootstrapping error and preventing overfitting is empirically validated by learning two robotics tasks: bucket loading and open drawer, which require extensive interactions with the environment. Simulation results also demonstrate the robustness of the DRLR framework across tasks with both low and high state-action dimensions, and varying demonstration qualities. To evaluate the developed framework on a real-world industrial robotics task, the bucket loading task is deployed on a real wheel loader. The sim2real results validate the successful deployment of the DRLR framework.

**Link**: [arxiv](http://arxiv.org/abs/2509.04069v1),  [pdf](http://arxiv.org/pdf/2509.04069v1)

**Tags**: cs.RO cs.LG 



### Arabic Chatbot Technologies in Education: An Overview
**Authors**: Hicham Bourhil, Yacine El Younoussi

**Updated**: 2025-09-04T09:55:16Z

**Summary**: The recent advancements in Artificial Intelligence (AI) in general, and in Natural Language Processing (NLP) in particular, and some of its applications such as chatbots, have led to their implementation in different domains like education, healthcare, tourism, and customer service. Since the COVID-19 pandemic, there has been an increasing interest in these digital technologies to allow and enhance remote access. In education, e-learning systems have been massively adopted worldwide. The emergence of Large Language Models (LLM) such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformers) made chatbots even more popular. In this study, we present a survey on existing Arabic chatbots in education and their different characteristics such as the adopted approaches, language variety, and metrics used to measure their performance. We were able to identified some research gaps when we discovered that, despite the success of chatbots in other languages such as English, only a few educational Arabic chatbots used modern techniques. Finally, we discuss future directions of research in this field.

**Link**: [arxiv](http://arxiv.org/abs/2509.04066v1),  [pdf](http://arxiv.org/pdf/2509.04066v1)

**Tags**: cs.CL 



### Synthesizing Sheet Music Problems for Evaluation and Reinforcement   Learning
**Authors**: Zhilin Wang, Zhe Yang, Yun Luo, Yafu Li, Haoran Zhang, Runzhe Zhan, Derek F. Wong, Jizhe Zhou, Yu Cheng

**Updated**: 2025-09-04T09:42:17Z

**Summary**: Enhancing the ability of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) to interpret sheet music is a crucial step toward building AI musicians. However, current research lacks both evaluation benchmarks and training data for sheet music reasoning. To address this, we propose the idea of synthesizing sheet music problems grounded in music theory, which can serve both as evaluation benchmarks and as training data for reinforcement learning with verifiable rewards (RLVR). We introduce a data synthesis framework that generates verifiable sheet music questions in both textual and visual modalities, leading to the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a complementary training set. Evaluation results on SSMR-Bench show the importance of models' reasoning abilities in interpreting sheet music. At the same time, the poor performance of Gemini 2.5-Pro highlights the challenges that MLLMs still face in interpreting sheet music in a visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and Qwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the trained Qwen3-8B-Base surpasses GPT-4 in overall performance on MusicTheoryBench and achieves reasoning performance comparable to GPT-4 with the strategies of Role play and Chain-of-Thought. Notably, its performance on math problems also improves relative to the original Qwen3-8B-Base. Furthermore, our results show that the enhanced reasoning ability can also facilitate music composition. In conclusion, we are the first to propose the idea of synthesizing sheet music problems based on music theory rules, and demonstrate its effectiveness not only in advancing model reasoning for sheet music understanding but also in unlocking new possibilities for AI-assisted music creation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04059v1),  [pdf](http://arxiv.org/pdf/2509.04059v1)

**Tags**: cs.CL 



### SMooGPT: Stylized Motion Generation using Large Language Models
**Authors**: Lei Zhong, Yi Yang, Changjian Li

**Updated**: 2025-09-04T09:41:18Z

**Summary**: Stylized motion generation is actively studied in computer graphics, especially benefiting from the rapid advances in diffusion models. The goal of this task is to produce a novel motion respecting both the motion content and the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing research attempts to address this problem via motion style transfer or conditional motion generation. They typically embed the motion style into a latent space and guide the motion implicitly in a latent space as well. Despite the progress, their methods suffer from low interpretability and control, limited generalization to new styles, and fail to produce motions other than ``walking'' due to the strong bias in the public stylization dataset. In this paper, we propose to solve the stylized motion generation problem from a new perspective of reasoning-composition-generation, based on our observations: i) human motion can often be effectively described using natural language in a body-part centric manner, ii) LLMs exhibit a strong ability to understand and reason about human motion, and iii) human motion has an inherently compositional nature, facilitating the new motion content or style generation via effective recomposing. We thus propose utilizing body-part text space as an intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a reasoner, composer, and generator when generating the desired stylized motion. Our method executes in the body-part text space with much higher interpretability, enabling fine-grained motion control, effectively resolving potential conflicts between motion content and style, and generalizes well to new styles thanks to the open-vocabulary ability of LLMs. Comprehensive experiments and evaluations, and a user perceptual study, demonstrate the effectiveness of our approach, especially under the pure text-driven stylized motion generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.04058v1),  [pdf](http://arxiv.org/pdf/2509.04058v1)

**Tags**: cs.GR cs.CV 



### DaMoC: Efficiently Selecting the Optimal Large Language Model for   Fine-tuning Domain Tasks Based on Data and Model Compression
**Authors**: Wei Huang, Huang Wei, Yinggui Wang

**Updated**: 2025-09-04T09:30:16Z

**Summary**: Large language models (LLMs) excel in general tasks but struggle with domain-specific ones, requiring fine-tuning with specific data. With many open-source LLMs available, selecting the best model for fine-tuning downstream tasks is challenging, primarily focusing on how to quickly identify the optimal LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses this challenge by: 1) Data Level: A systematic categorization of data filtering methodologies for LLMs is first established, classifying them into three distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods, and (3) hybrid approaches considering both dimensions. Further, we enhance the density of key tokens in the text achieving token compression. Subsequently, we use an LLM to iterative rewrite the text to optimize its expression. 2) Model Level: We use layer similarity scores to assess each layer's importance and remove those with lower importance. Then, we introduce a sparse merging paradigm to preserve as much of the original model's capability as possible. Extensive experiments on four datasets, medical Q&A, financial Q&A, general Q&A, and reading comprehension, show that we can select the optimal LLM while saving approximately 20-fold in training time.

**Link**: [arxiv](http://arxiv.org/abs/2509.01221v2),  [pdf](http://arxiv.org/pdf/2509.01221v2)

**Tags**: cs.CL cs.AI cs.LG 



### EvolveSignal: A Large Language Model Powered Coding Agent for   Discovering Traffic Signal Control Algorithms
**Authors**: Leizhen Wang, Peibo Duan, Hao Wang, Yue Wang, Jian Xu, Nan Zheng, Zhenliang Ma

**Updated**: 2025-09-04T09:25:05Z

**Summary**: In traffic engineering, the fixed-time traffic signal control remains widely used for its low cost, stability, and interpretability. However, its design depends on hand-crafted formulas (e.g., Webster) and manual re-timing by engineers to adapt to demand changes, which is labor-intensive and often yields suboptimal results under heterogeneous or congested conditions. This paper introduces the EvolveSignal, a large language models (LLMs) powered coding agent to automatically discover new traffic signal control algorithms. We formulate the problem as program synthesis, where candidate algorithms are represented as Python functions with fixed input-output structures, and iteratively optimized through external evaluations (e.g., a traffic simulator) and evolutionary search. Experiments on a signalized intersection demonstrate that the discovered algorithms outperform Webster's baseline, reducing average delay by 20.1% and average stops by 47.1%. Beyond performance, ablation and incremental analyses reveal that EvolveSignal modifications-such as adjusting cycle length bounds, incorporating right-turn demand, and rescaling green allocations-can offer practically meaningful insights for traffic engineers. This work opens a new research direction by leveraging AI for algorithm design in traffic signal control, bridging program synthesis with transportation engineering.

**Link**: [arxiv](http://arxiv.org/abs/2509.03335v2),  [pdf](http://arxiv.org/pdf/2509.03335v2)

**Tags**: cs.LG 



### Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to   Enhance LLM Safety Guardrail to Potential Attacks
**Authors**: Sheng Liu, Qiang Sheng, Danding Wang, Yang Li, Guang Yang, Juan Cao

**Updated**: 2025-09-04T09:23:46Z

**Summary**: Despite advances in improving large language model (LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs' inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.

**Link**: [arxiv](http://arxiv.org/abs/2508.20038v3),  [pdf](http://arxiv.org/pdf/2508.20038v3)

**Tags**: cs.CL 



### CP-Bench: Evaluating Large Language Models for Constraint Modelling
**Authors**: Kostis Michailidis, Dimos Tsouros, Tias Guns

**Updated**: 2025-09-04T09:10:05Z

**Summary**: Constraint Programming (CP) is widely used to solve combinatorial problems, but its core process, namely constraint modelling, requires significant expertise and is considered to be a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for constraint modelling are often limited to small, homogeneous, or domain-specific instances, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing CP-Bench, a novel benchmark that includes a diverse set of well-known combinatorial problems sourced from the CP community, structured explicitly for evaluating LLM-driven CP modelling. With this dataset, and given the variety of constraint modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 70% on this highly challenging benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2506.06052v2),  [pdf](http://arxiv.org/pdf/2506.06052v2)

**Tags**: cs.AI 



### Spotlight Attention: Towards Efficient LLM Generation via Non-linear   Hashing-based KV Cache Retrieval
**Authors**: Wenhao Li, Yuxin Zhang, Gen Luo, Haiyuan Wan, Ziyang Gong, Fei Chao, Rongrong Ji

**Updated**: 2025-09-04T09:08:29Z

**Summary**: Reducing the key-value (KV) cache burden in Large Language Models (LLMs) significantly accelerates inference. Dynamically selecting critical KV caches during decoding helps maintain performance. Existing methods use random linear hashing to identify important tokens, but this approach is inefficient due to the orthogonal distribution of queries and keys within two narrow cones in LLMs. We introduce Spotlight Attention, a novel method that employs non-linear hashing functions to optimize the embedding distribution of queries and keys, enhancing coding efficiency and robustness. We also developed a lightweight, stable training framework using a Bradley-Terry ranking-based loss, enabling optimization of the non-linear hashing module on GPUs with 16GB memory in 8 hours. Experimental results show that Spotlight Attention drastically improves retrieval precision while shortening the length of the hash code at least 5$\times$ compared to traditional linear hashing. Finally, we exploit the computational advantages of bitwise operations by implementing specialized CUDA kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla decoding.

**Link**: [arxiv](http://arxiv.org/abs/2508.19740v2),  [pdf](http://arxiv.org/pdf/2508.19740v2)

**Tags**: cs.CL 



### CoT-Space: A Theoretical Framework for Internal Slow-Thinking via   Reinforcement Learning
**Authors**: Zeyu Gan, Hao Yi, Yong Liu

**Updated**: 2025-09-04T09:02:16Z

**Summary**: Reinforcement Learning (RL) has become a pivotal approach for enhancing the reasoning capabilities of Large Language Models (LLMs). However, a significant theoretical gap persists, as traditional token-level RL frameworks fail to align with the reasoning-level nature of complex, multi-step thought processes like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space, a novel theoretical framework that recasts LLM reasoning from a discrete token-prediction task to an optimization process within a continuous, reasoning-level semantic space. By analyzing this process from both a noise perspective and a risk perspective, we demonstrate that the convergence to an optimal CoT length is a natural consequence of the fundamental trade-off between underfitting and overfitting. Furthermore, extensive experiments provide strong empirical validation for our theoretical findings. Our framework not only provides a coherent explanation for empirical phenomena such as overthinking but also offers a solid theoretical foundation to guide the future development of more effective and generalizable reasoning agents.

**Link**: [arxiv](http://arxiv.org/abs/2509.04027v1),  [pdf](http://arxiv.org/pdf/2509.04027v1)

**Tags**: cs.AI cs.CL 



### FPC-VLA: A Vision-Language-Action Framework with a Supervisor for   Failure Prediction and Correction
**Authors**: Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, Jingtai Liu

**Updated**: 2025-09-04T08:47:26Z

**Summary**: Robotic manipulation is a fundamental component of automation. However, traditional perception-planning pipelines often fall short in open-ended tasks due to limited flexibility, while the architecture of a single end-to-end Vision-Language-Action (VLA) offers promising capabilities but lacks crucial mechanisms for anticipating and recovering from failure. To address these challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with a supervisor for failure prediction and correction. The supervisor evaluates action viability through vision-language queries and generates corrective strategies when risks arise, trained efficiently without manual labeling. A similarity-guided fusion module further refines actions by leveraging past predictions. Evaluation results on multiple simulation platforms (SIMPLER and LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA outperforms state-of-the-art models in both zero-shot and fine-tuned settings. By activating the supervisor only at keyframes, our approach significantly increases task success rates with minimal impact on execution time. Successful real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong generalization and practical utility for building more reliable autonomous systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.04018v1),  [pdf](http://arxiv.org/pdf/2509.04018v1)

**Tags**: cs.RO 



### On Robustness and Reliability of Benchmark-Based Evaluation of LLMs
**Authors**: Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro, Kevin Roitero

**Updated**: 2025-09-04T08:43:27Z

**Summary**: Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.04013v1),  [pdf](http://arxiv.org/pdf/2509.04013v1)

**Tags**: cs.CL cs.AI 



### NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware   Embeddings
**Authors**: Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman

**Updated**: 2025-09-04T08:42:23Z

**Summary**: We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at https://github.com/ShacharOr100/ner_retriever

**Link**: [arxiv](http://arxiv.org/abs/2509.04011v1),  [pdf](http://arxiv.org/pdf/2509.04011v1)

**Tags**: cs.IR cs.AI cs.CL 



### AutoPBO: LLM-powered Optimization for Local Search PBO Solvers
**Authors**: Jinyuan Li, Yi Chu, Yiwen Sun, Mengchuan Zou, Shaowei Cai

**Updated**: 2025-09-04T08:38:42Z

**Summary**: Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling combinatorial problems through pseudo-Boolean (PB) constraints. Local search solvers have shown excellent performance in PBO solving, and their efficiency is highly dependent on their internal heuristics to guide the search. Still, their design often requires significant expert effort and manual tuning in practice. While Large Language Models (LLMs) have demonstrated potential in automating algorithm design, their application to optimizing PBO solvers remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered framework to automatically enhance PBO local search solvers. We conduct experiments on a broad range of four public benchmarks, including one real-world benchmark, a benchmark from PB competition, an integer linear programming optimization benchmark, and a crafted combinatorial benchmark, to evaluate the performance improvement achieved by AutoPBO and compare it with six state-of-the-art competitors, including two local search PBO solvers NuPBO and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates significant improvements over previous local search approaches, while maintaining competitive performance compared to state-of-the-art competitors. The results suggest that AutoPBO offers a promising approach to automating local search solver design.

**Link**: [arxiv](http://arxiv.org/abs/2509.04007v1),  [pdf](http://arxiv.org/pdf/2509.04007v1)

**Tags**: cs.AI 



### Spectral characterization and performance of SPT-SLIM on-chip filterbank   spectrometers
**Authors**: C. S. Benson, K. Fichman, M. Adamic, A. J. Anderson, P. Barry, B. A. Benson, E. Brooks, J. E. Carlstrom, T. Cecil, C. L. Chang, K. R. Dibert, M. Dobbs, K. S. Karkare, G. K. Keating, A. M. Lapuente, M. Lisovenko, D. P. Marrone, J. Montgomery, T. Natoli, Z. Pan, A. Rahlin, G. Robson, M. Rouble, G. Smecher, V. Yefremenko, M. R. Young, C. Yu, J. A. Zebrowski, C. Zhang

**Updated**: 2025-09-04T08:26:18Z

**Summary**: The South Pole Telescope Shirokoff Line Intensity Mapper (SPT-SLIM) experiment is a pathfinder for demonstrating the use of on-chip spectrometers for millimeter Line Intensity Mapping. We present spectral bandpass measurements of the SLIM spectrometer channels made on site using a Fourier Transform Spectrometer during SPT-SLIMs first deployment the 2024-2025 austral summer observing season. Through this we demonstrate a technique for measuring the narrow band passes of the SPT-SLIM filterbanks that improves beyond the intrinsic resolution of a Fourier Transform Spectrometer.

**Link**: [arxiv](http://arxiv.org/abs/2509.02245v2),  [pdf](http://arxiv.org/pdf/2509.02245v2)

**Tags**: astro-ph.IM astro-ph.CO 



### RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question   Answering with Large Language Models
**Authors**: Zhaoyan Gong, Juan Li, Zhiqiang Liu, Lei Liang, Huajun Chen, Wen Zhang

**Updated**: 2025-09-04T08:25:01Z

**Summary**: Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in "Multiple" and "Complex" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA.

**Link**: [arxiv](http://arxiv.org/abs/2509.03995v1),  [pdf](http://arxiv.org/pdf/2509.03995v1)

**Tags**: cs.CL cs.AI 



### Vision-based Manipulation from Single Human Video with Open-World Object   Graphs
**Authors**: Yifeng Zhu, Arisrei Lim, Peter Stone, Yuke Zhu

**Updated**: 2025-09-04T08:23:37Z

**Summary**: This work presents an object-centric approach to learning vision-based manipulation skills from human videos. We investigate the problem of robot manipulation via imitation in the open-world setting, where a robot learns to manipulate novel objects from a single video demonstration. We introduce ORION, an algorithm that tackles the problem by extracting an object-centric manipulation plan from a single RGB or RGB-D video and deriving a policy that conditions on the extracted plan. Our method enables the robot to learn from videos captured by daily mobile devices and to generalize the policies to deployment environments with varying visual backgrounds, camera angles, spatial layouts, and novel object instances. We systematically evaluate our method on both short-horizon and long-horizon tasks, using RGB-D and RGB-only demonstration videos. Across varied tasks and demonstration types (RGB-D / RGB), we observe an average success rate of 74.4%, demonstrating the efficacy of ORION in learning from a single human video in the open world. Additional materials can be found on our project website: https://ut-austin-rpl.github.io/ORION-release.

**Link**: [arxiv](http://arxiv.org/abs/2405.20321v2),  [pdf](http://arxiv.org/pdf/2405.20321v2)

**Tags**: cs.RO cs.CV cs.LG 



### Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility   for Resource-Efficient LLM Agent
**Authors**: Chunlong Wu, Zhibo Qu

**Updated**: 2025-09-04T08:18:39Z

**Summary**: Large language model (LLM) agents achieve impressive single-task performance but commonly exhibit repeated failures, inefficient exploration, and limited cross-task adaptability. Existing reflective strategies (e.g., Reflexion, ReAct) improve per-episode behavior but typically produce ephemeral, task-specific traces that are not reused across tasks. Reinforcement-learning based alternatives can produce transferable policies but require substantial parameter updates and compute. In this work we introduce Meta-Policy Reflexion (MPR): a hybrid framework that consolidates LLM-generated reflections into a structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at inference time through two complementary mechanisms soft memory-guided decoding and hard rule admissibility checks(HAC). MPR (i) externalizes reusable corrective knowledge without model weight updates, (ii) enforces domain constraints to reduce unsafe or invalid actions, and (iii) retains the adaptability of language-based reflection. We formalize the MPM representation, present algorithms for update and decoding, and validate the approach in a text-based agent environment following the experimental protocol described in the provided implementation (AlfWorld-based). Empirical results reported in the supplied material indicate consistent gains in execution accuracy and robustness when compared to Reflexion baselines; rule admissibility further improves stability. We analyze mechanisms that explain these gains, discuss scalability and failure modes, and outline future directions for multimodal and multi?agent extensions.

**Link**: [arxiv](http://arxiv.org/abs/2509.03990v1),  [pdf](http://arxiv.org/pdf/2509.03990v1)

**Tags**: cs.AI 



### FutureGen: A RAG-based Approach to Generate the Future Work of   Scientific Article
**Authors**: Ibrahim Al Azher, Miftahul Jannat Mokarrama, Zhishuai Guo, Sagnik Ray Choudhury, Hamed Alhoori

**Updated**: 2025-09-04T08:12:41Z

**Summary**: The Future Work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from a scientific article. To enrich the generation process with broader insights and reduce the chance of missing important research directions, we use context from related papers using RAG. We experimented with various Large Language Models (LLMs) integrated into Retrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism to enhance the quality of the generated content and introduce an LLM-as-a-judge framework for robust evaluation, assessing key aspects such as novelty, hallucination, and feasibility. Our results demonstrate that the RAG-based approach using GPT-4o mini, combined with an LLM feedback mechanism, outperforms other methods based on both qualitative and quantitative evaluations. Moreover, we conduct a human evaluation to assess the LLM as an extractor, generator, and feedback provider.

**Link**: [arxiv](http://arxiv.org/abs/2503.16561v3),  [pdf](http://arxiv.org/pdf/2503.16561v3)

**Tags**: cs.CL cs.LG 



### NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language   Models
**Authors**: Chuhan Zhang, Ye Zhang, Bowen Shi, Yuyou Gan, Tianyu Du, Shouling Ji, Dazhan Deng, Yingcai Wu

**Updated**: 2025-09-04T08:12:06Z

**Summary**: In deployment and application, large language models (LLMs) typically undergo safety alignment to prevent illegal and unethical outputs. However, the continuous advancement of jailbreak attack techniques, designed to bypass safety mechanisms with adversarial prompts, has placed increasing pressure on the security defenses of LLMs. Strengthening resistance to jailbreak attacks requires an in-depth understanding of the security mechanisms and vulnerabilities of LLMs. However, the vast number of parameters and complex structure of LLMs make analyzing security weaknesses from an internal perspective a challenging task. This paper presents NeuroBreak, a top-down jailbreak analysis system designed to analyze neuron-level safety mechanisms and mitigate vulnerabilities. We carefully design system requirements through collaboration with three experts in the field of AI security. The system provides a comprehensive analysis of various jailbreak attack methods. By incorporating layer-wise representation probing analysis, NeuroBreak offers a novel perspective on the model's decision-making process throughout its generation steps. Furthermore, the system supports the analysis of critical neurons from both semantic and functional perspectives, facilitating a deeper exploration of security mechanisms. We conduct quantitative evaluations and case studies to verify the effectiveness of our system, offering mechanistic insights for developing next-generation defense strategies against evolving jailbreak attacks.

**Link**: [arxiv](http://arxiv.org/abs/2509.03985v1),  [pdf](http://arxiv.org/pdf/2509.03985v1)

**Tags**: cs.CR cs.AI 



### DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool   Interleaved Vision-Language Model
**Authors**: Qian Chen, Xianyin Zhang, Lifan Guo, Feng Chen, Chi Zhang

**Updated**: 2025-09-04T08:05:29Z

**Summary**: Recent advances in large vision-language models (LVLMs) have enabled a new paradigm of end-to-end document image parsing, excelling in Optical Character Recognition (OCR) tasks such as text, table, and formula recognition. However, generative LVLMs, similarly to large language models (LLMs), are prone to hallucinations--generating words that do not exist in input images. Furthermore, LVLMs are designed for general purposes and tend to be less effective on OCR tasks compared to expert models that are trained on domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a reasoning-enhanced framework designed to address these limitations through training reasoning-and-tool interleaved VLMs. Given a recognition instruction, our DianJin-OCR-R1 model first recognizes the content in the input image by its own OCR capabilities, and then calls other tools (i.e., other expert models) to obtain their results as references, finally "looks again" the image and rethinks about the reasoning process to provide the final recognized content. Since architectures of expert models are tailored for specific OCR tasks, which makes them less prone to hallucinations, their results can help VLMs mitigate hallucinations. We evaluate our model on ReST and OmniDocBench, and experimental results show that our DianJin-OCR-R1 models consistently outperform their non-reasoning counterparts and expert OCR models, which proves the effectiveness of our method. Additionally, the results indicate that enhancing expert models, which are typically small and easy to iterate, enable performance improvements for VLMs.

**Link**: [arxiv](http://arxiv.org/abs/2508.13238v2),  [pdf](http://arxiv.org/pdf/2508.13238v2)

**Tags**: cs.CV 



### Harnessing modal fields retrieved from speckle for multi-dimensional   metrology
**Authors**: Qingbo Liu, Zhongyang Xu, Guangkui Tao, Xiuyuan Sun, Min Xue, Weihao Yuan, Shilong Pan

**Updated**: 2025-09-04T08:02:38Z

**Summary**: Although speckle is a powerful tool for high-precision metrology, large datasets and cumbersome training are always required to learn from the encoded speckle patterns, which is unfavorable for rapid deployment and multi-dimensional metrology. To enable high accuracy and fast training, physics-informed machine learning enforces physical laws to address high-dimensional problems. Here, we harness the modal fields in a few-mode fiber, which follow the law of beam propagation, to enable high-accuracy and fast-training parameter estimation. Anti-noise fast mode decomposition is implemented to retrieve the modal fields from the speckles. The accuracy is enhanced since the modal fields enable parameter estimation at random points in the continuous space-time domain. Artificial tactile perception and multi-dimensional metrology are achieved with high accuracy because the modal fields respond diversely to different parameters. Meanwhile, the number of specklegrams for training is reduced by around 5 times. The training time of machine learning is significantly reduced by 800 times, from 9 hours and 45 minutes to 40 seconds. Therefore, harnessing the modal fields paves a new way for the speckle-based metrology to develop efficient, low-cost, multi-dimensional sensors, making it suitable for intelligent wearable devices, industrial robots and healthcare applications.

**Link**: [arxiv](http://arxiv.org/abs/2509.03976v1),  [pdf](http://arxiv.org/pdf/2509.03976v1)

**Tags**: physics.optics 



### Training LLMs to be Better Text Embedders through Bidirectional   Reconstruction
**Authors**: Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, Zhouhan Lin

**Updated**: 2025-09-04T08:02:20Z

**Summary**: Large language models (LLMs) have increasingly been explored as powerful text embedders. Existing LLM-based text embedding approaches often leverage the embedding of the final token, typically a reserved special token such as [EOS]. However, these tokens have not been intentionally trained to capture the semantics of the whole context, limiting their capacity as text embeddings, especially for retrieval and re-ranking tasks. We propose to add a new training stage before contrastive learning to enrich the semantics of the final token embedding. This stage employs bidirectional generative reconstruction tasks, namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-Based Document-to-Query), which interleave to anchor the [EOS] embedding and reconstruct either side of Query-Document pairs. Experimental results demonstrate that our additional training stage significantly improves LLM performance on the Massive Text Embedding Benchmark (MTEB), achieving new state-of-the-art results across different LLM base models and scales.

**Link**: [arxiv](http://arxiv.org/abs/2509.03020v2),  [pdf](http://arxiv.org/pdf/2509.03020v2)

**Tags**: cs.CL cs.IR 



### Expanding Foundational Language Capabilities in Open-Source LLMs through   a Korean Case Study
**Authors**: Junghwan Lim, Gangwon Jo, Sungmin Lee, Jiyoung Park, Dongseok Kim, Jihwan Kim, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Kibong Choi, Jaeyeon Huh, Beomgyu Kim, Jangwoong Kim, Taehyun Kim, Haesol Lee, Jeesoo Lee, Dongpin Oh, Changseok Song, Daewon Suh

**Updated**: 2025-09-04T07:56:24Z

**Summary**: We introduce Llama-3-Motif, a language model consisting of 102 billion parameters, specifically designed to enhance Korean capabilities while retaining strong performance in English. Developed on the Llama 3 architecture, Llama-3-Motif employs advanced training techniques, including LlamaPro and Masked Structure Growth, to effectively scale the model without altering its core Transformer architecture. Using the MoAI platform for efficient training across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully curated dataset that maintains a balanced ratio of Korean and English data. Llama-3-Motif shows decent performance on Korean-specific benchmarks, outperforming existing models and achieving results comparable to GPT-4.

**Link**: [arxiv](http://arxiv.org/abs/2509.03972v1),  [pdf](http://arxiv.org/pdf/2509.03972v1)

**Tags**: cs.CL cs.AI cs.LG 



### MultiGen: Child-Friendly Multilingual Speech Generator with LLMs
**Authors**: Xiaoxue Gao, Huayun Zhang, Nancy F. Chen

**Updated**: 2025-09-04T07:56:00Z

**Summary**: Generative speech models have demonstrated significant potential in improving human-machine interactions, offering valuable real-world applications such as language learning for children. However, achieving high-quality, child-friendly speech generation remains challenging, particularly for low-resource languages across diverse languages and cultural contexts. In this paper, we propose MultiGen, a multilingual speech generation model with child-friendly interaction, leveraging LLM architecture for speech generation tailored for low-resource languages. We propose to integrate age-appropriate multilingual speech generation using LLM architectures, which can be used to facilitate young children's communication with AI systems through culturally relevant context in three low-resource languages: Singaporean accent Mandarin, Malay, and Tamil. Experimental results from both objective metrics and subjective evaluations demonstrate the superior performance of the proposed MultiGen compared to baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2508.08715v3),  [pdf](http://arxiv.org/pdf/2508.08715v3)

**Tags**: eess.AS cs.AI cs.CL eess.SP 



### Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin   Case Study
**Authors**: Yuchen Lei, Yuexin Xiang, Qin Wang, Rafael Dowsley, Tsz Hon Yuen, Kim-Kwang Raymond Choo, Jiangshan Yu

**Updated**: 2025-09-04T07:46:37Z

**Summary**: Cryptocurrencies are widely used, yet current methods for analyzing transactions often rely on opaque, black-box models. While these models may achieve high performance, their outputs are usually difficult to interpret and adapt, making it challenging to capture nuanced behavioral patterns. Large language models (LLMs) have the potential to address these gaps, but their capabilities in this area remain largely unexplored, particularly in cybercrime detection. In this paper, we test this hypothesis by applying LLMs to real-world cryptocurrency transaction graphs, with a focus on Bitcoin, one of the most studied and widely adopted blockchain networks. We introduce a three-tiered framework to assess LLM capabilities: foundational metrics, characteristic overview, and contextual interpretation. This includes a new, human-readable graph representation format, LLM4TG, and a connectivity-enhanced transaction graph sampling algorithm, CETraS. Together, they significantly reduce token requirements, transforming the analysis of multiple moderately large-scale transaction graphs with LLMs from nearly impossible to feasible under strict token limits. Experimental results demonstrate that LLMs have outstanding performance on foundational metrics and characteristic overview, where the accuracy of recognizing most basic information at the node level exceeds 98.50% and the proportion of obtaining meaningful characteristics reaches 95.00%. Regarding contextual interpretation, LLMs also demonstrate strong performance in classification tasks, even with very limited labeled data, where top-3 accuracy reaches 72.43% with explanations. While the explanations are not always fully accurate, they highlight the strong potential of LLMs in this domain. At the same time, several limitations persist, which we discuss along with directions for future research.

**Link**: [arxiv](http://arxiv.org/abs/2501.18158v3),  [pdf](http://arxiv.org/pdf/2501.18158v3)

**Tags**: cs.CR cs.LG 



### Exploring NLP Benchmarks in an Extremely Low-Resource Setting
**Authors**: Ulin Nuha, Adam Jatowt

**Updated**: 2025-09-04T07:41:23Z

**Summary**: The effectiveness of Large Language Models (LLMs) diminishes for extremely low-resource languages, such as indigenous languages, primarily due to the lack of labeled data. Despite growing interest, the availability of high-quality natural language processing (NLP) datasets for these languages remains limited, making it difficult to develop robust language technologies. This paper addresses such gap by focusing on Ladin, an endangered Romance language, specifically targeting the Val Badia variant. Leveraging a small set of parallel Ladin-Italian sentence pairs, we create synthetic datasets for sentiment analysis and multiple-choice question answering (MCQA) by translating monolingual Italian data. To ensure linguistic quality and reliability, we apply rigorous filtering and back-translation procedures in our method. We further demonstrate that incorporating these synthetic datasets into machine translation training leads to substantial improvements over existing Italian-Ladin translation baselines. Our contributions include the first publicly available sentiment analysis and MCQA datasets for Ladin, establishing foundational resources that can support broader NLP research and downstream applications for this underrepresented language.

**Link**: [arxiv](http://arxiv.org/abs/2509.03962v1),  [pdf](http://arxiv.org/pdf/2509.03962v1)

**Tags**: cs.CL 



