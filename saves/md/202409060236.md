# Arxiv Results
## Keyword: kv cache 
 ### SparQ Attention: Bandwidth-Efficient LLM Inference
**Authors**: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr

**Updated**: 2024-09-04T10:04:52Z

**Summary**: The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2312.04985v6),  [pdf](http://arxiv.org/pdf/2312.04985v6)

**Tags**: cs.LG 



### A brown dwarf orbiting around the planetary-nebula central binary KV Vel
**Authors**: S. -B. Qian, L. -Y. Zhu, F. -X. Li, L. -J. Li, Z. -T. Han, J. -J. He, L. Zang, L. -F. Chang, Q. -B. Sun, M. -Y. Li, H. -T. Zhang, F. -Z. Yan

**Updated**: 2024-09-04T07:13:01Z

**Summary**: KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary containing a very hot subdwarf primary (77000 K) and a cool low-mass secondary star (3400 K) that is located at the center of the planetary nebula DS 1. The changes in the orbital period of the close binary were analyzed based on 262 new times of light maximum together with those compiled from the literature. It is discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic period variation with a period of 29.55 years. The explanation by the solar-type magnetic activity cycles of the cool component is ruled out because the required energies are much larger than the total radiant energy of this component in a whole cycle. Therefore, the cyclic variation was plausibly explained as the light-travel time effect via the presence of a tertiary component, which is supported by the periodic changes of the O-C curve and the rather symmetric and stable light curves obtained by TESS. The mass of the tertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third body is coplanar with the central binary (i.e., i' = 62.5{\deg}), the mass of the tertiary component is computed as M_3 ~ 0.068 M\sun, and thus it would be below the stable hydrogen-burning limit and is a brown dwarf. The orbital separation is shorter than 9.35 astronomical units (AU). KV Vel together with its surrounding planetary nebula and the brown-dwarf companion may be formed through the common-envelope evolution after the primary filled its Roche lobe during the early asymptotic giant branch stage.

**Link**: [arxiv](http://arxiv.org/abs/2409.02480v1),  [pdf](http://arxiv.org/pdf/2409.02480v1)

**Tags**: astro-ph.SR 



### SELCC: Coherent Caching over Compute-Limited Disaggregated Memory
**Authors**: Ruihong Wang, Jianguo Wang, Walid G. Aref

**Updated**: 2024-09-05T01:12:04Z

**Summary**: Disaggregating memory from compute offers the opportunity to better utilize stranded memory in data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes to save on round-trip communication cost between the disaggregated memory and the compute nodes. However, the limited computing power on the disaggregated memory servers makes it challenging to maintain cache coherence among multiple compute-side caches over disaggregated shared memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. SELCC builds on a one-sided shared-exclusive latch protocol by introducing lazy latch release and invalidation messages among the compute nodes so that it can guarantee both data access atomicity and cache coherence. SELCC minimizes communication round-trips by embedding the current cache copy holder IDs into RDMA latch words and prioritizes local concurrency control over global concurrency control. We instantiate the SELCC protocol onto compute-sided cache, forming an abstraction layer over disaggregated memory. This abstraction layer provides main-memory-like APIs to upper-level applications, and thus enabling existing data structures and algorithms to function over disaggregated memory with minimal code change. To demonstrate the usability of SELCC, we implement a B-tree and three transaction concurrency control algorithms over SELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves better performance compared to RPC-based cache-coherence protocols. Additionally, YCSB and TPC-C benchmarks indicate that applications over SELCC can achieve comparable or superior performance against competitors over disaggregated memory.

**Link**: [arxiv](http://arxiv.org/abs/2409.02088v2),  [pdf](http://arxiv.org/pdf/2409.02088v2)

**Tags**: cs.DB cs.DC cs.ET 



### Contemporary Model Compression on Large Language Models Inference
**Authors**: Dong Liu

**Updated**: 2024-09-03T15:35:01Z

**Summary**: Large Language Models (LLMs) have revolutionized natural language processing by achieving state-of-the-art results across a variety of tasks. However, the computational demands of LLM inference, including high memory consumption and slow processing speeds, pose significant challenges for real-world applications, particularly on resource-constrained devices. Efficient inference is crucial for scaling the deployment of LLMs to a broader range of platforms, including mobile and edge devices.   This survey explores contemporary techniques in model compression that address these challenges by reducing the size and computational requirements of LLMs while maintaining their performance. We focus on model-level compression methods, including quantization, knowledge distillation, and pruning, as well as system-level optimizations like KV cache efficient design. Each of these methodologies offers a unique approach to optimizing LLMs, from reducing numerical precision to transferring knowledge between models and structurally simplifying neural networks. Additionally, we discuss emerging trends in system-level design that further enhance the efficiency of LLM inference. This survey aims to provide a comprehensive overview of current advancements in model compression and their potential to make LLMs more accessible and practical for diverse applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.01990v1),  [pdf](http://arxiv.org/pdf/2409.01990v1)

**Tags**: cs.DC cs.LG 



### A Fresh Take on Stale Embeddings: Improving Dense Retriever Training   with Corrector Networks
**Authors**: Nicholas Monath, Will Grathwohl, Michael Boratko, Rob Fergus, Andrew McCallum, Manzil Zaheer

**Updated**: 2024-09-03T13:29:13Z

**Summary**: In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring "hard negatives." We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.

**Link**: [arxiv](http://arxiv.org/abs/2409.01890v1),  [pdf](http://arxiv.org/pdf/2409.01890v1)

**Tags**: cs.LG 



### Reward Augmentation in Reinforcement Learning for Testing Distributed   Systems
**Authors**: Andrea Borgarelli, Constantin Enea, Rupak Majumdar, Srinidhi Nagendra

**Updated**: 2024-09-02T15:07:05Z

**Summary**: Bugs in popular distributed protocol implementations have been the source of many downtimes in popular internet services. We describe a randomized testing approach for distributed protocol implementations based on reinforcement learning. Since the natural reward structure is very sparse, the key to successful exploration in reinforcement learning is reward augmentation. We show two different techniques that build on one another. First, we provide a decaying exploration bonus based on the discovery of new states -- the reward decays as the same state is visited multiple times. The exploration bonus captures the intuition from coverage-guided fuzzing of prioritizing new coverage points; in contrast to other schemes, we show that taking the maximum of the bonus and the Q-value leads to more effective exploration. Second, we provide waypoints to the algorithm as a sequence of predicates that capture interesting semantic scenarios. Waypoints exploit designer insight about the protocol and guide the exploration to ``interesting'' parts of the state space. Our reward structure ensures that new episodes can reliably get to deep interesting states even without execution caching. We have implemented our algorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and RSL) shows that our algorithm can significantly outperform baseline approaches in terms of coverage and bug finding.

**Link**: [arxiv](http://arxiv.org/abs/2409.02137v1),  [pdf](http://arxiv.org/pdf/2409.02137v1)

**Tags**: cs.SE cs.DC cs.LG cs.PL 



### Learning in Hybrid Active Inference Models
**Authors**: Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley

**Updated**: 2024-09-02T08:41:45Z

**Summary**: An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work in computational neuroscience has considered this functional integration of discrete and continuous variables during decision-making under the formalism of active inference (Parr, Friston & de Vries, 2017; Parr & Friston, 2018). However, their focus is on the expressive physical implementation of categorical decisions and the hierarchical mixed generative model is assumed to be known. As a consequence, it is unclear how this framework might be extended to learning. We therefore present a novel hierarchical hybrid active inference agent in which a high-level discrete active inference planner sits above a low-level continuous active inference controller. We make use of recent work in recurrent switching linear dynamical systems (rSLDS) which implement end-to-end learning of meaningful discrete representations via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). The representations learned by the rSLDS inform the structure of the hybrid decision-making agent and allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and successful planning through the delineation of abstract sub-goals.

**Link**: [arxiv](http://arxiv.org/abs/2409.01066v1),  [pdf](http://arxiv.org/pdf/2409.01066v1)

**Tags**: cs.AI cs.SY eess.SY 



### Throughput Optimization in Cache-aided Networks: An Opportunistic   Probing and Scheduling Approach
**Authors**: Zhou Zhang, Saman Atapattu, Yizhu Wang, Marco Di Renzo

**Updated**: 2024-09-02T02:36:22Z

**Summary**: This paper addresses the challenges of throughput optimization in wireless cache-aided cooperative networks. We propose an opportunistic cooperative probing and scheduling strategy for efficient content delivery. The strategy involves the base station probing the relaying channels and cache states of multiple cooperative nodes, thereby enabling opportunistic user scheduling for content delivery. Leveraging the theory of Sequentially Planned Decision (SPD) optimization, we dynamically formulate decisions on cooperative probing and stopping time. Our proposed Reward Expected Thresholds (RET)-based strategy optimizes opportunistic probing and scheduling. This approach significantly enhances system throughput by exploiting gains from local caching, cooperative transmission and time diversity. Simulations confirm the effectiveness and practicality of the proposed Media Access Control (MAC) strategy.

**Link**: [arxiv](http://arxiv.org/abs/2409.00905v1),  [pdf](http://arxiv.org/pdf/2409.00905v1)

**Tags**: eess.SP 



### Rapid GPU-Based Pangenome Graph Layout
**Authors**: Jiajie Li, Jan-Niklas Schmelzle, Yixiao Du, Simon Heumos, Andrea Guarracino, Giulia Guidi, Pjotr Prins, Erik Garrison, Zhiru Zhang

**Updated**: 2024-09-02T00:05:20Z

**Summary**: Computational Pangenomics is an emerging field that studies genetic variation using a graph structure encompassing multiple genomes. Visualizing pangenome graphs is vital for understanding genome diversity. Yet, handling large graphs can be challenging due to the high computational demands of the graph layout process.   In this work, we conduct a thorough performance characterization of a state-of-the-art pangenome graph layout algorithm, revealing significant data-level parallelism, which makes GPUs a promising option for compute acceleration. However, irregular data access and the algorithm's memory-bound nature present significant hurdles. To overcome these challenges, we develop a solution implementing three key optimizations: a cache-friendly data layout, coalesced random states, and warp merging. Additionally, we propose a quantitative metric for scalable evaluation of pangenome layout quality.   Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution achieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline without layout quality loss, reducing execution time from hours to minutes.

**Link**: [arxiv](http://arxiv.org/abs/2409.00876v1),  [pdf](http://arxiv.org/pdf/2409.00876v1)

**Tags**: cs.DC cs.CE cs.DS 



### Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,   Communication and Computing Systems
**Authors**: Wanming Hao, Xue Wu, Xingwang Li, Gangcan Sun, Qingqing Wu, Liang Yang

**Updated**: 2024-08-31T06:33:50Z

**Summary**: In this paper, we investigate an intelligent reflecting surface (IRS) assisted full-duplex (FD) integrated sensing, communication and computing system. Specifically, an FD base station (BS) provides service for uplink and downlink transmission, and a local cache is connected to the BS through a backhaul link to store data. Meanwhile, active sensing elements are deployed on the IRS to receive target echo signals. On this basis, in order to evaluate the overall performance of the system under consideration, we propose a system utility maximization problem while ensuring the sensing quality, expressed as the difference between the sum of communication throughput, total computation bits (offloading bits and local computation bits) and the total backhaul cost for content delivery. This makes the problem difficult to solve due to the highly non-convex coupling of the optimization variables. To effectively solve this problem, we first design the most effective caching strategy. Then, we develop an algorithm based on weighted minimum mean square error, alternative direction method of multipliers, majorization-minimization framework, semi-definite relaxation techniques, and several complex transformations to jointly solve the optimization variables. Finally, simulation results are provided to verify the utility performance of the proposed algorithm and demonstrate the advantages of the proposed scheme compared with the baseline scheme.

**Link**: [arxiv](http://arxiv.org/abs/2409.00364v1),  [pdf](http://arxiv.org/pdf/2409.00364v1)

**Tags**: cs.IT eess.SP math.IT 



### >3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction   Termination Extension and Sub-1V Turn-on
**Authors**: Advait Gilankar, Abishek Katta, Nabasindhu Das, Nidhin Kurian Kalarickal

**Updated**: 2024-08-31T04:20:58Z

**Summary**: This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction diodes (HJDs) with a 2-step space-modulated junction termination extension. Distinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown voltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a forward current density (IF) of 1 A-cm-2. The measured devices exhibit excellent turn-on characteristics achieving 100 A-cm-2 current density at a forward bias of 1.5V along with a low differential specific on-resistance (Ron,sp) of 4.4 m{\Omega}-cm2. The SM-JTE was realized using concentric NiO rings with varying widths and spacing that approximates a gradual reduction in JTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and is among the best reported for devices with a sub-1V turn-on. The fabricated devices also displayed minimal change in forward I-V characteristics post reverse bias stress of 3 kV applied during breakdown voltage testing.

**Link**: [arxiv](http://arxiv.org/abs/2409.00344v1),  [pdf](http://arxiv.org/pdf/2409.00344v1)

**Tags**: physics.app-ph 



### Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume   Visualization through Functional Approximation
**Authors**: Jianxin Sun, David Lenz, Hongfeng Yu, Tom Peterka

**Updated**: 2024-08-30T18:04:53Z

**Summary**: Functional approximation as a high-order continuous representation provides a more accurate value and gradient query compared to the traditional discrete volume representation. Volume visualization directly rendered from functional approximation generates high-quality rendering results without high-order artifacts caused by trilinear interpolations. However, querying an encoded functional approximation is computationally expensive, especially when the input dataset is large, making functional approximation impractical for interactive visualization. In this paper, we proposed a novel functional approximation multi-resolution representation, Adaptive-FAM, which is lightweight and fast to query. We also design a GPU-accelerated out-of-core multi-resolution volume visualization framework that directly utilizes the Adaptive-FAM representation to generate high-quality rendering with interactive responsiveness. Our method can not only dramatically decrease the caching time, one of the main contributors to input latency, but also effectively improve the cache hit rate through prefetching. Our approach significantly outperforms the traditional function approximation method in terms of input latency while maintaining comparable rendering quality.

**Link**: [arxiv](http://arxiv.org/abs/2409.00184v1),  [pdf](http://arxiv.org/pdf/2409.00184v1)

**Tags**: cs.GR 



### Modelling the High-Voltage Grid Using Open Data for Europe and Beyond
**Authors**: Bobby Xiong, Davide Fioriti, Fabian Neumann, Iegor Riepin, Tom Brown

**Updated**: 2024-08-30T10:26:50Z

**Summary**: This paper provides the background, methodology and validation for constructing a representation of the European high-voltage grid, including and above 200 kV, based on public data provided by OpenStreetMap. The model-independent grid dataset is published under the Open Data Commons Open Database (ODbL 1.0) licence and can be used for large-scale electricity as well as energy system modelling. The dataset and workflow are provided as part of PyPSA-Eur -- an open-source, sector-coupled optimisation model of the European energy system. By integrating with the codebase for initiatives such as PyPSA-Earth, the value of open and maintainable high-voltage grid data extends to the global context. By accessing the latest data through the the Overpass turbo API, the dataset can be easily reconstructed and updated within minutes. To assess the data quality, this paper further compares the dataset with official statistics and representative model runs using PyPSA-Eur based on different electricity grid representations.

**Link**: [arxiv](http://arxiv.org/abs/2408.17178v1),  [pdf](http://arxiv.org/pdf/2408.17178v1)

**Tags**: physics.soc-ph 



### MemLong: Memory-Augmented Retrieval for Long Text Modeling
**Authors**: Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, Min Zhang

**Updated**: 2024-08-30T02:01:56Z

**Summary**: Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ``ret-mem'' module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong

**Link**: [arxiv](http://arxiv.org/abs/2408.16967v1),  [pdf](http://arxiv.org/pdf/2408.16967v1)

**Tags**: cs.CL cs.AI 



### Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load
**Authors**: Hesameddin Mokhtarzadeh, Mohammed S. Al-Abiad, Md Jahangir Hossain, Julian Cheng

**Updated**: 2024-08-29T17:43:26Z

**Summary**: In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio heads (eRRHs) are connected to a macro base station (MBS) through fronthaul links. Deploying a massive number of eRRHs is not always feasible due to site constraints and the cost of fronthaul links. This paper introduces an innovative concept of using smart helpers (SHs) in F-RANs. These SHs do not require fronthaul links and listen to the nearby eRRHs' communications. Then, they smartly select and cache popular content. This capability enables SHs to serve users with frequent on-demand service requests potentially. As such, network operators have the flexibility to easily deploy SHs in various scenarios, such as dense urban areas and temporary public events, to expand their F-RANs and improve the quality of service (QoS). To study the performance of the proposed SH-aided F-RAN, we formulate an optimization problem of minimizing the average transmission delay that jointly optimizes cache resources and user scheduling. To tackle the formulated problem, we develop an innovative multi-stage algorithm that uses a reinforcement learning (RL) framework. Various performance measures, e.g., the average transmission delay, fronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated numerically and compared with those of traditional F-RANs.

**Link**: [arxiv](http://arxiv.org/abs/2309.07975v2),  [pdf](http://arxiv.org/pdf/2309.07975v2)

**Tags**: cs.IT math.IT 



### VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths   Vision Computation
**Authors**: Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou

**Updated**: 2024-08-29T17:21:58Z

**Summary**: A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens "skipping layers" rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for each transformer layer, we learn to skip the computation for a high proportion (e.g., 80\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately \textasciitilde42\% time and \textasciitilde30\% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets.

**Link**: [arxiv](http://arxiv.org/abs/2408.16730v1),  [pdf](http://arxiv.org/pdf/2408.16730v1)

**Tags**: cs.CV 



### GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless   Generative Inference of LLM
**Authors**: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao

**Updated**: 2024-08-29T16:48:58Z

**Summary**: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR.

**Link**: [arxiv](http://arxiv.org/abs/2403.05527v3),  [pdf](http://arxiv.org/pdf/2403.05527v3)

**Tags**: cs.LG cs.AI cs.CL 



### LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through   Targeted Instruction Hardening
**Authors**: Yiming Zhu, Wenchao Huang, Yan Xiong

**Updated**: 2024-08-29T02:31:28Z

**Summary**: Several software mitigations have been proposed to defend against Spectre vulnerabilities. However, these countermeasures often suffer from high performance overhead, largely due to unnecessary protections. We propose LightSLH, designed to mitigate this overhead by hardening instructions only when they are under threat from Spectre vulnerabilities. LightSLH leverages program analysis techniques based on abstract interpretation to identify all instructions that could potentially lead to Spectre vulnerabilities and provides provable protection. To enhance analysis efficiency and precision, LightSLH employs novel taint and value domains. The taint domain enables bit-level taint tracking, while the value domain allows LightSLH to analyze complex program structures such as pointers and structures. Furthermore, LightSLH uses a two-stage abstract interpretation approach to circumvent potential analysis paralysis issues.   We demonstrate the security guarantees of LightSLH and evaluate its performance on cryptographic algorithm implementations from OpenSSL. LightSLH significantly reduces the overhead associated with speculative-load-hardening techniques. Our results show that LightSLH introduces no protection and thus no overhead on 4 out of the 7 studied algorithms, which contrasts with existing countermeasures that introduce additional overhead due to unnecessary hardening. Additionally, LightSLH performs, for the first time, a rigorous analysis of the security guarantees of RSA against Spectre v1, highlighting that the memory access patterns generated by the scatter-gather algorithm depend on secrets, even for observers at the cache line granularity, necessitating protection for such accesses.

**Link**: [arxiv](http://arxiv.org/abs/2408.16220v1),  [pdf](http://arxiv.org/pdf/2408.16220v1)

**Tags**: cs.CR 



### RIP Linked List
**Authors**: Beno√Æt Sonntag, Dominique Colnet

**Updated**: 2024-08-28T08:41:45Z

**Summary**: Linked lists have long served as a valuable teaching tool in programming. However, the question arises: Are they truly practical for everyday program use? In most cases, it appears that array-based data structures offer distinct advantages, particularly in terms of memory efficiency and,more importantly, execution speed. While it's relatively straightforward to calculate the complexity of operations, gauging actual execution efficiency remains a challenge. This paper addresses this question by introducing a new benchmark. Our study compares various linked list implementations with several array-based alternatives. We also demonstrate the ease of incorporating memory caching for linked lists, enhancing their performance. Additionally, we introduce a new array-based data structure designed to excel in a wide range of operations.

**Link**: [arxiv](http://arxiv.org/abs/2306.06942v3),  [pdf](http://arxiv.org/pdf/2306.06942v3)

**Tags**: cs.DS 



### Efficient LLM Training and Serving with Heterogeneous Context Sharding   among Attention Heads
**Authors**: Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song

**Updated**: 2024-08-27T22:06:20Z

**Summary**: Existing LLM training and inference frameworks struggle in boosting efficiency with sparsity while maintaining the integrity of context and model architecture. Inspired by the sharding concept in database and the fact that attention parallelizes over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention algorithm that allocates heterogeneous context partitions for different attention heads to divide and conquer. S2-Attention enforces each attention head to only attend to a partition of contexts following a strided sparsity pattern, while the full context is preserved as the union of all the shards. As attention heads are processed in separate thread blocks, the context reduction for each head can thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained with S2-Attention can then take the KV cache reduction as free meals with guaranteed model quality preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction in end-to-end training time and 10X inference latency, (2) on-par model training quality compared to default attention, (3)perfect needle retrieval accuracy over 32K context window. On top of the algorithm, we build DKernel, an LLM training and inference kernel library that allows users to customize sparsity patterns for their own models. We open-sourced DKerneland make it compatible with Megatron, Pytorch, and vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2407.17678v2),  [pdf](http://arxiv.org/pdf/2407.17678v2)

**Tags**: cs.CL 



### Styx: Transactional Stateful Functions on Streaming Dataflows
**Authors**: Kyriakos Psarakis, George Siachamis, George Christodoulou, Marios Fragkoulis, Asterios Katsifodimos

**Updated**: 2024-08-27T17:30:41Z

**Summary**: Developing stateful cloud applications, such as low-latency workflows and microservices with strict consistency requirements, remains arduous for programmers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve these use cases. However, existing approaches either provide serializable transactional guarantees at the level of individual functions, or separate application logic from the state and use inefficient transactional protocols. These design choices increase the execution latency, limiting the adoption of SFaaS systems.   In this paper, we present Styx, a novel SFaaS runtime that executes serializable transactions across functions with exactly-once guarantees. Styx extends a deterministic transactional protocol to support an arbitrary call graph of stateful functions. It introduces a transaction-execution acknowledgment scheme that allows tracking a transactional workflow's SFaaS calls, guaranteeing atomicity and exactly-once processing. Finally, Styx features a function-execution caching mechanism and early transactional commit replies for optimized performance. Experiments with the YCSB-T, TPC-C, and Deathstar benchmarks show that Styx outperforms state-of-the-art approaches by achieving at least one order of magnitude higher throughput while exhibiting near-linear scalability and low latency.

**Link**: [arxiv](http://arxiv.org/abs/2312.06893v3),  [pdf](http://arxiv.org/pdf/2312.06893v3)

**Tags**: cs.DC cs.DB 



### Writing in the Margins: Better Inference Pattern for Long Context   Retrieval
**Authors**: Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, Waseem AlShikh

**Updated**: 2024-08-27T09:34:38Z

**Summary**: In this paper, we introduce Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, we observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, we show how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. We release our implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.

**Link**: [arxiv](http://arxiv.org/abs/2408.14906v1),  [pdf](http://arxiv.org/pdf/2408.14906v1)

**Tags**: cs.CL cs.IR 



### PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework   with Correlated Differential Privacy
**Authors**: Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao

**Updated**: 2024-08-27T02:03:36Z

**Summary**: Online video streaming has evolved into an integral component of the contemporary Internet landscape. Yet, the disclosure of user requests presents formidable privacy challenges. As users stream their preferred online videos, their requests are automatically seized by video content providers, potentially leaking users' privacy.   Unfortunately, current protection methods are not well-suited to preserving user request privacy from content providers while maintaining high-quality online video services. To tackle this challenge, we introduce a novel Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge devices to pre-fetch and cache videos, ensuring the privacy of users' requests while optimizing the efficiency of edge caching. More specifically, we design PPVF with three core components: (1) \textit{Online privacy budget scheduler}, which employs a theoretically guaranteed online algorithm to select non-requested videos as candidates with assigned privacy budgets. Alternative videos are chosen by an online algorithm that is theoretically guaranteed to consider both video utilities and available privacy budgets. (2) \textit{Noisy video request generator}, which generates redundant video requests (in addition to original ones) utilizing correlated differential privacy to obfuscate request privacy. (3) \textit{Online video utility predictor}, which leverages federated learning to collaboratively evaluate video utility in an online fashion, aiding in video selection in (1) and noise generation in (2). Finally, we conduct extensive experiments using real-world video request traces from Tencent Video. The results demonstrate that PPVF effectively safeguards user request privacy while upholding high video caching performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.14735v1),  [pdf](http://arxiv.org/pdf/2408.14735v1)

**Tags**: cs.MM cs.CR cs.DC 



### Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference
**Authors**: Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han

**Updated**: 2024-08-26T21:01:02Z

**Summary**: As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .

**Link**: [arxiv](http://arxiv.org/abs/2406.10774v2),  [pdf](http://arxiv.org/pdf/2406.10774v2)

**Tags**: cs.CL cs.LG 



### Employing Artificial Intelligence to Steer Exascale Workflows with   Colmena
**Authors**: Logan Ward, J. Gregory Pauloski, Valerie Hayot-Sasson, Yadu Babuji, Alexander Brace, Ryan Chard, Kyle Chard, Rajeev Thakur, Ian Foster

**Updated**: 2024-08-26T17:21:19Z

**Summary**: Computational workflows are a common class of application on supercomputers, yet the loosely coupled and heterogeneous nature of workflows often fails to take full advantage of their capabilities. We created Colmena to leverage the massive parallelism of a supercomputer by using Artificial Intelligence (AI) to learn from and adapt a workflow as it executes. Colmena allows scientists to define how their application should respond to events (e.g., task completion) as a series of cooperative agents. In this paper, we describe the design of Colmena, the challenges we overcame while deploying applications on exascale systems, and the science workflows we have enhanced through interweaving AI. The scaling challenges we discuss include developing steering strategies that maximize node utilization, introducing data fabrics that reduce communication overhead of data-intensive tasks, and implementing workflow tasks that cache costly operations between invocations. These innovations coupled with a variety of application patterns accessible through our agent-based steering model have enabled science advances in chemistry, biophysics, and materials science using different types of AI. Our vision is that Colmena will spur creative solutions that harness AI across many domains of scientific computing.

**Link**: [arxiv](http://arxiv.org/abs/2408.14434v1),  [pdf](http://arxiv.org/pdf/2408.14434v1)

**Tags**: cs.DC cs.LG 



### Decision-Focused Learning to Predict Action Costs for Planning
**Authors**: Jayanta Mandi, Marco Foschini, Daniel Holler, Sylvie Thiebaux, Jorg Hoffmann, Tias Guns

**Updated**: 2024-08-26T11:29:07Z

**Summary**: In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.

**Link**: [arxiv](http://arxiv.org/abs/2408.06876v2),  [pdf](http://arxiv.org/pdf/2408.06876v2)

**Tags**: cs.AI cs.RO 



### Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems
**Authors**: Yiwei Li, Boyu Tian, Mingyu Gao

**Updated**: 2024-08-26T07:26:27Z

**Summary**: Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity address mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative hybrid memory system with HBM3 and DDR5, Trimma achieves up to 1.68$\times$ and on average 1.33$\times$ speedup benefits, compared to state-of-the-art hybrid memory designs. These results show that Trimma effectively addresses metadata management overheads, especially for future scalable large-scale hybrid memory architectures.

**Link**: [arxiv](http://arxiv.org/abs/2402.16343v2),  [pdf](http://arxiv.org/pdf/2402.16343v2)

**Tags**: cs.AR 



### RollingCache: Using Runtime Behavior to Defend Against Cache Side   Channel Attacks
**Authors**: Divya Ojha, Sandhya Dwarkadas

**Updated**: 2024-08-26T04:32:56Z

**Summary**: Shared caches are vulnerable to side channel attacks through contention in cache sets. Besides being a simple source of information leak, these side channels form useful gadgets for more sophisticated attacks that compromise the security of shared systems.   The fundamental design aspect that contention attacks exploit is the deterministic nature of the set of addresses contending for a cache set. In this paper, we present RollingCache, a cache design that defends against contention attacks by dynamically changing the set of addresses contending for cache sets. Unlike prior defenses, RollingCache does not rely on address encryption/decryption, data relocation, or cache partitioning. We use one level of indirection to implement dynamic mapping controlled by the whole-cache runtime behavior. Our solution does not depend on having defined security domains, and can defend against an attacker running on the same or another core.   We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our security evaluation shows that our dynamic mapping removes the deterministic ability to identify the source of contention. The performance evaluation shows an impact of 1.67\% over a mix of workloads, with a corresponding

**Link**: [arxiv](http://arxiv.org/abs/2408.08795v2),  [pdf](http://arxiv.org/pdf/2408.08795v2)

**Tags**: cs.CR cs.AR 



### Decentralized Federated Learning with Model Caching on Mobile Agents
**Authors**: Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu

**Updated**: 2024-08-26T03:58:20Z

**Summary**: Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching.

**Link**: [arxiv](http://arxiv.org/abs/2408.14001v1),  [pdf](http://arxiv.org/pdf/2408.14001v1)

**Tags**: cs.LG cs.DC 



### Mobile Edge Computing Networks: Online Low-Latency and Fresh Service   Provisioning
**Authors**: Yuhan Yi, Guanglin Zhang, Hai Jiang

**Updated**: 2024-08-24T15:23:32Z

**Summary**: Edge service caching can significantly mitigate latency and reduce communication and computing overhead by fetching and initializing services (applications) from clouds. The freshness of cached service data is critical when providing satisfactory services to users, but has been overlooked in existing research efforts. In this paper, we study the online low-latency and fresh service provisioning in mobile edge computing (MEC) networks. Specifically, we jointly optimize the service caching, task offloading, and resource allocation without knowledge of future system information, which is formulated as a joint online long-term optimization problem. This problem is NP-hard. To solve the problem, we design a Lyapunov-based online framework that decouples the problem at temporal level into a series of per-time-slot subproblems. For each subproblem, we propose an online integrated optimization-deep reinforcement learning (OIODRL) method, which contains an optimization stage including a quadratically constrained quadratic program (QCQP) transformation and a semidefinite relaxation (SDR) method, and a learning stage including a deep reinforcement learning (DRL) algorithm. Extensive simulations show that the proposed OIODRL method achieves a near-optimal solution and outperforms other benchmark methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.13605v1),  [pdf](http://arxiv.org/pdf/2408.13605v1)

**Tags**: cs.IT math.IT 



### MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context   Generation with Speculative Decoding
**Authors**: Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen

**Updated**: 2024-08-23T17:54:34Z

**Summary**: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size. This finding underscores the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available at https://github.com/Infini-AI-Lab/MagicDec/.

**Link**: [arxiv](http://arxiv.org/abs/2408.11049v3),  [pdf](http://arxiv.org/pdf/2408.11049v3)

**Tags**: cs.CL 



### Cyclic Wrap-Around Multi-Access Coded Caching with Private Caches
**Authors**: Dhruv Pratap Singh, Anjana A. Mahesh, B. Sundar Rajan

**Updated**: 2024-08-23T15:39:20Z

**Summary**: We consider a variant of the coded caching problem where users connect to two types of caches, called private caches and access caches. The problem setting consists of a server having a library of files and a set of access caches. Every user, equipped with a private cache, connects to $L$ neighboring access caches in a cyclic wrap-around fashion. The server populates the private and access caches with file contents in either coded or uncoded format. For this setting, we derive a lower bound on the optimal worst-case transmission rate using cut-set arguments. This lower bound applies to both coded and uncoded placements. We then provide an achievable scheme with uncoded placement and show that our scheme specializes to the well-known Maddah-Ali-Niesen scheme for the dedicated cache network in the absence of access caches. Finally, we show that the proposed scheme achieves optimality in large memory regimes and provide numerical plots comparing the rate of the proposed scheme with the derived lower bound, demonstrating the optimality of our scheme.

**Link**: [arxiv](http://arxiv.org/abs/2408.13165v1),  [pdf](http://arxiv.org/pdf/2408.13165v1)

**Tags**: cs.IT math.IT 



### Fundamental Limits of Multi-Message Private Computation
**Authors**: Ali Gholami, Kai Wan, Tayyebeh Jahani-Nezhad, Hua Sun, Mingyue Ji, Giuseppe Caire

**Updated**: 2024-08-23T13:25:07Z

**Summary**: In a typical formulation of the private information retrieval (PIR) problem, a single user wishes to retrieve one out of $ K$ files from $N$ servers without revealing the demanded file index to any server. This paper formulates an extended model of PIR, referred to as multi-message private computation (MM-PC), where instead of retrieving a single file, the user wishes to retrieve $P>1$ linear combinations of files while preserving the privacy of the demand information. The MM-PC problem is a generalization of the private computation (PC) problem (where the user requests one linear combination of the files), and the multi-message private information retrieval (MM-PIR) problem (where the user requests $P>1$ files). A baseline achievable scheme repeats the optimal PC scheme by Sun and Jafar $P$ times, or treats each possible demanded linear combination as an independent file and then uses the near optimal MM-PIR scheme by Banawan and Ulukus. In this paper, we propose a new MM-PC scheme that significantly improves upon the baseline schemes. In doing so, we design the queries inspired by the structure in the cache-aided scalar linear function retrieval scheme by Wan {\it et al.}, which leverages the dependency between linear functions to reduce the amount of communications. To ensure the decodability of our scheme, we propose a new method to benefit from the existing dependency, referred to as the sign assignment step. In the end, we use Maximum Distance Separable matrices to code the queries, which allows the reduction of download from the servers, while preserving privacy. By the proposed schemes, we characterize the capacity within a multiplicative factor of $2$.

**Link**: [arxiv](http://arxiv.org/abs/2305.05332v5),  [pdf](http://arxiv.org/pdf/2305.05332v5)

**Tags**: cs.IT math.IT 



### Which Part of the Heap is Useful? Improving Heap Liveness Analysis
**Authors**: Vini Kanvar, Uday P. Khedker

**Updated**: 2024-08-23T09:54:22Z

**Summary**: With the growing sizes of data structures allocated in heap, understanding the actual use of heap memory is critically important for minimizing cache misses and reclaiming unused memory. A static analysis aimed at this is difficult because the heap locations are unnamed. Using allocation sites to name them creates very few distinctions making it difficult to identify allocated heap locations that are not used. Heap liveness analysis using access graphs solves this problem by (a) using a storeless model of heap memory by naming the locations with access paths, and (b) representing the unbounded sets of access paths (which are regular languages) as finite automata.   We improve the scalability and efficiency of heap liveness analysis, and reduce the amount of computed heap liveness information by using deterministic automata and by minimizing the inclusion of aliased access paths in the language. Practically, our field-, flow-, context-sensitive liveness analysis on SPEC CPU2006 benchmarks scales to 36 kLoC (existing analysis scales to 10.5 kLoC) and improves efficiency even up to 99%. For some of the benchmarks, our technique shows multifold reduction in the computed liveness information, ranging from 2 to 100 times (in terms of the number of live access paths), without compromising on soundness.

**Link**: [arxiv](http://arxiv.org/abs/2408.12947v1),  [pdf](http://arxiv.org/pdf/2408.12947v1)

**Tags**: cs.PL 



### Exposing Shadow Branches
**Authors**: Chrysanthos Pepi, Bhargav Reddy Godala, Krishnam Tibrewala, Gino Chacon, Paul V. Gratz, Daniel A. Jim√©nez, Gilles A. Pokam, David I. August

**Updated**: 2024-08-22T17:56:29Z

**Summary**: Modern processors implement a decoupled front-end in the form of Fetch Directed Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1I). As data center applications become more complex, their code footprints also grow, resulting in an increase in Branch Target Buffer (BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1I cache. We observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched but, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (the taken branch). Branch instructions present in the ignored portion of the cache line we call them "Shadow Branches". Here we present Skeia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss. With a minimal storage state of 12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB across 16 front-end bound applications. Since many branches stored in the SBB are unique compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skeia across all examined sizes until saturation.

**Link**: [arxiv](http://arxiv.org/abs/2408.12592v1),  [pdf](http://arxiv.org/pdf/2408.12592v1)

**Tags**: cs.AR 



### Stable CoO$_2$ Nanoscrolls With Outstanding Electrical Properties
**Authors**: Simon Hettler, Kankona Singha Roy, Raul Arenal, Leela S. Panchakarla

**Updated**: 2024-08-22T17:47:49Z

**Summary**: Layered CoO$_2$ is of great interest for its promising properties but is meta-stable in its bulk form. CoO$_2$ was synthesized by converting the quasi-one-dimensional crystal structure of bulk Ca$_3$Co$_2$O$_6$ via a hydrothermal treatment. The resulting nanostructures were predominantly nanoscrolls with very thin walls, which exhibit long-term stability. A detailed structural investigation reveals that the CoO$_2$ is found to crystallize in monoclinic form, similar to the related CaCoO$_2$-CoO$_2$ misfit structure. Individual nanoscrolls are characterized electrically and show a p-type semiconducting nature with a high current-carrying capacity of 4$\cdot$10$^5$ A cm$^{-2}$ and an extremely high breakdown voltage of up to 270 kV/cm. The results demonstrate the possibility to stabilize meta-stable materials in low-dimensional forms and a promising application of the nanoscrolls as interconnect in high-voltage electronic circuitry.

**Link**: [arxiv](http://arxiv.org/abs/2309.14533v2),  [pdf](http://arxiv.org/pdf/2309.14533v2)

**Tags**: cond-mat.mtrl-sci 



### Rheological behavior of molybdenum disulfide (MoS2) inks under electric   fields: influence of concentration and voltage
**Authors**: Pedro C Rijo, Francisco J. Galindo-Rosales

**Updated**: 2024-08-21T10:26:26Z

**Summary**: This work provides a complete rheological characterization of molybdenum disulfide (MoS2) inks in the presence of electric fields. Several concentrations of MoS2 are studied and dispersed in a viscoelastic fluid. The lubrication effects are present in the ink when the MoS2 concentration is higher than 0.10% w/w. The dielectric properties show the impossibility of a positive electrorheological effect for all MoS2-inks studied. The formation of vortices and electromigration of MoS2 particles occur under the influence of an external electric field. These two phenomena affect the rheological behavior of MoS2-inks under shear flow condition. Relatively to the extensional rheology experiments, the particle migration and the vortex formation promote anisotropy on the rheological properties of the inks which affects the relaxation time, the formation of beads-on-a-string and the uniaxial elongational flow condition is no longer valid. When the electric field strength is 1.5 kV/mm, the formation of Taylor's cone is observed and independent of MoS2 concentration.

**Link**: [arxiv](http://arxiv.org/abs/2408.11506v1),  [pdf](http://arxiv.org/pdf/2408.11506v1)

**Tags**: physics.flu-dyn cond-mat.soft 



### Towards End-to-End GPS Localization with Neural Pseudorange Correction
**Authors**: Xu Weng, KV Ling, Haochen Liu, Kun Cao

**Updated**: 2024-08-21T06:10:02Z

**Summary**: The pseudorange error is one of the root causes of localization inaccuracy in GPS. Previous data-driven methods regress and eliminate pseudorange errors using handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS localization framework, E2E-PrNet, to train a neural network for pseudorange correction (PrNet) directly using the final task loss calculated with the ground truth of GPS receiver states. The gradients of the loss with respect to learnable parameters are backpropagated through a Differentiable Nonlinear Least Squares (DNLS) optimizer to PrNet. The feasibility of fusing the data-driven neural network and the model-based DNLS module is verified with GPS data collected by Android phones, showing that E2E-PrNet outperforms the baseline weighted least squares method and the state-of-the-art end-to-end data-driven approach. Finally, we discuss the explainability of E2E-PrNet.

**Link**: [arxiv](http://arxiv.org/abs/2401.10685v2),  [pdf](http://arxiv.org/pdf/2401.10685v2)

**Tags**: cs.LG cs.AI eess.SP 



### Telepathic Datacenters: Fast RPCs using Shared CXL Memory
**Authors**: Suyash Mahar, Ehsan Hajyjasini, Seungjin Lee, Zifeng Zhang, Mingyao Shen, Steven Swanson

**Updated**: 2024-08-21T04:16:49Z

**Summary**: Datacenter applications often rely on remote procedure calls (RPCs) for fast, efficient, and secure communication. However, RPCs are slow, inefficient, and hard to use as they require expensive serialization and compression to communicate over a packetized serial network link. Compute Express Link 3.0 (CXL) offers an alternative solution, allowing applications to share data using a cache-coherent, shared-memory interface across clusters of machines.   RPCool is a new framework that exploits CXL's shared memory capabilities. RPCool avoids serialization by passing pointers to data structures in shared memory. While avoiding serialization is useful, directly sharing pointer-rich data eliminates the isolation that copying data over traditional networks provides, leaving the receiver vulnerable to invalid pointers and concurrent updates to shared data by the sender. RPCool restores this safety with careful and efficient management of memory permissions. Another significant challenge with CXL shared memory capabilities is that they are unlikely to scale to an entire datacenter. RPCool addresses this by falling back to RDMA-based communication.   Overall, RPCool reduces the round-trip latency by 1.93$\times$ and 7.2$\times$ compared to state-of-the-art RDMA and CXL-based RPC mechanisms, respectively. Moreover, RPCool performs either comparably or better than other RPC mechanisms across a range of workloads.

**Link**: [arxiv](http://arxiv.org/abs/2408.11325v1),  [pdf](http://arxiv.org/pdf/2408.11325v1)

**Tags**: cs.DC cs.OS 



### QET: Enhancing Quantized LLM Parameters and KV cache Compression through   Element Substitution and Residual Clustering
**Authors**: Yanshu Wang, Wang Li, Tong Yang

**Updated**: 2024-08-21T02:32:43Z

**Summary**: Matrix quantization compresses matrix elements into a more compact form to reduce storage requirements, with dequantization enabling reconstruction for use. We define the Quantization Error Minimization (QEM) problem as minimizing the difference between the original and quantized matrices while ensuring the quantized matrix remains within fixed memory constraints. This technique is crucial in applications like Large Language Model (LLM) weight compression and KV cache compression, where large matrix sizes demand efficient storage solutions.   As modern LLMs like GPT-4 and BERT continue to grow, effective matrix compression is increasingly important. These models contain billions of parameters in matrix form, making efficient weight quantization essential for both storage and computational efficiency. Similarly, KV caches, storing intermediate inference results, are matrix-based and benefit significantly from optimized compression techniques.   To address the QEM problem in the context of LLM weight and KV cache compression, we propose Quantum Entanglement Trees (QET). QET leverages the local structure of matrix elements by iteratively swapping elements to create a locally ordered matrix, which is then grouped and quantized column by column. To enhance QET, we introduce two optimizations: residual quantization to further reduce Mean Squared Error (MSE) and masking with batch processing to accelerate the algorithm.   Our experiments demonstrate that QET can reduce MSE to 12.3% of its original value at the same compression ratio, outperforming leading baseline methods. Our contributions include framing the QEM problem specifically for LLM and KV cache compression, developing the QET algorithm, and implementing optimizations that improve accuracy and processing speed.

**Link**: [arxiv](http://arxiv.org/abs/2407.03637v3),  [pdf](http://arxiv.org/pdf/2407.03637v3)

**Tags**: cs.LG cs.CL 



### Hybrid Recurrent Models Support Emergent Descriptions for Hierarchical   Planning and Control
**Authors**: Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley

**Updated**: 2024-08-20T16:02:54Z

**Summary**: An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work has demonstrated that a class of hybrid state-space model known as recurrent switching linear dynamical systems (rSLDS) discover meaningful behavioural units via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). Furthermore, they model how the underlying continuous states drive these discrete mode switches. We propose that the rich representations formed by an rSLDS can provide useful abstractions for planning and control. We present a novel hierarchical model-based algorithm inspired by Active Inference in which a discrete MDP sits above a low-level linear-quadratic controller. The recurrent transition dynamics learned by the rSLDS allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We successfully apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and non-trivial planning through the delineation of abstract sub-goals.

**Link**: [arxiv](http://arxiv.org/abs/2408.10970v1),  [pdf](http://arxiv.org/pdf/2408.10970v1)

**Tags**: cs.AI cs.SY eess.SY 



### Pluto and Charon: A Time and Memory Efficient Collaborative Edge AI   Framework for Personal LLMs Fine-Tuning
**Authors**: Bei Ouyang, Shengyuan Ye, Liekang Zeng, Tianyi Qian, Jingyi Li, Xu Chen

**Updated**: 2024-08-20T11:30:12Z

**Summary**: Large language models (LLMs) have unlocked a plethora of powerful applications at the network edge, such as intelligent personal assistants. Data privacy and security concerns have prompted a shift towards edge-based fine-tuning of personal LLMs, away from cloud reliance. However, this raises issues of computational intensity and resource scarcity, hindering training efficiency and feasibility. While current studies investigate parameter-efficient fine-tuning (PEFT) techniques to mitigate resource constraints, our analysis indicates that these techniques are not sufficiently resource-efficient for edge devices. To tackle these challenges, we propose Pluto and Charon (PAC), a time and memory efficient collaborative edge AI framework for personal LLMs fine-tuning. PAC breaks the resource wall of personal LLMs fine-tuning with a sophisticated algorithm-system co-design. (1) Algorithmically, PAC implements a personal LLMs fine-tuning technique that is efficient in terms of parameters, time, and memory. It utilizes Parallel Adapters to circumvent the need for a full backward pass through the LLM backbone. Additionally, an activation cache mechanism further streamlining the process by negating the necessity for repeated forward passes across multiple epochs. (2) Systematically, PAC leverages edge devices in close proximity, pooling them as a collective resource for in-situ personal LLMs fine-tuning, utilizing a hybrid data and pipeline parallelism to orchestrate distributed training. The use of the activation cache eliminates the need for forward pass through the LLM backbone,enabling exclusive fine-tuning of the Parallel Adapters using data parallelism. Extensive evaluation based on prototype implementation demonstrates that PAC remarkably outperforms state-of-the-art approaches, achieving up to 8.64x end-to-end speedup and up to 88.16% reduction in memory footprint.

**Link**: [arxiv](http://arxiv.org/abs/2408.10746v1),  [pdf](http://arxiv.org/pdf/2408.10746v1)

**Tags**: cs.DC cs.AI cs.LG cs.NI 



### Heta: Distributed Training of Heterogeneous Graph Neural Networks
**Authors**: Yuchen Zhong, Junwei Su, Chuan Wu, Minjie Wang

**Updated**: 2024-08-20T04:46:18Z

**Summary**: Heterogeneous Graph Neural Networks (HGNNs) leverage diverse semantic relationships in Heterogeneous Graphs (HetGs) and have demonstrated remarkable learning performance in various applications. However, current distributed GNN training systems often overlook unique characteristics of HetGs, such as varying feature dimensions and the prevalence of missing features among nodes, leading to suboptimal performance or even incompatibility with distributed HGNN training. We introduce Heta, a framework designed to address the communication bottleneck in distributed HGNN training. Heta leverages the inherent structure of HGNNs - independent relation-specific aggregations for each relation, followed by a cross-relation aggregation - and advocates for a novel Relation-Aggregation-First computation paradigm. It performs relation-specific aggregations within graph partitions and then exchanges partial aggregations. This design, coupled with a new graph partitioning method that divides a HetG based on its graph schema and HGNN computation dependency, substantially reduces communication overhead. Heta further incorporates an innovative GPU feature caching strategy that accounts for the different cache miss-penalties associated with diverse node types. Comprehensive evaluations of various HGNN models and large heterogeneous graph datasets demonstrate that Heta outperforms state-of-the-art systems like DGL and GraphLearn by up to 5.8x and 2.3x in end-to-end epoch time, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.09697v2),  [pdf](http://arxiv.org/pdf/2408.09697v2)

**Tags**: cs.DC 



### Multi-Mode Lens for Momentum Microscopy and XPEEM: Theory
**Authors**: Olena Tkach, Gerd Schoenhense

**Updated**: 2024-08-19T15:47:17Z

**Summary**: The strong electric field between the sample and the extractor is the core of cathode lenses and a pivotal determinant of high resolution. Nevertheless, fields in the range of 3-8 kV/mm can be a source of complications. Local field enhancement at sharp edges or microscopic protrusions of cleaved samples may result in field emission or flashovers. Moreover, slow background electrons are drawn into the microscope column, where they contribute to space charge effects. A novel front lens configuration, optimized through ray-tracing simulations, significantly reduces the field at the sample and allows even for zero field or retarding field, which serves to suppress space charge effects. One or several annular electrodes, situated in a concentric position relative to the extractor, serve to form an additional lens within the gap between the sample and the extractor. The refractory power of this lens, and consequently the field at the sample surface, can be modified by adjusting the potentials of the annular electrodes. The imaging properties and aberrations of this gap lens have been investigated with regard to momentum imaging and XPEEM. The study encompasses the energy range from the few-eV level for laser-ARPES to 6 keV, for hard X-ray ARPES. The additional converging lens situated in close proximity to the sample exhibits a reduced field curvature of the k-image in the backfocal plane. This allows for the acquisition of larger fields of view in both momentum and real-space imaging.

**Link**: [arxiv](http://arxiv.org/abs/2408.10104v1),  [pdf](http://arxiv.org/pdf/2408.10104v1)

**Tags**: physics.app-ph cond-mat.mtrl-sci physics.ins-det 



### Abstract Environment Trimming
**Authors**: Daniel Jurjo-Rivas, Jose F. Morales, Pedro L√≥pez-Garc√≠a, Manuel V. Hermenegildo

**Updated**: 2024-08-19T09:50:35Z

**Summary**: Variable sharing is a fundamental property in the static analysis of logic programs, since it is instrumental for ensuring correctness and increasing precision while inferring many useful program properties. Such properties include modes, determinacy, non-failure, cost, etc. This has motivated significant work on developing abstract domains to improve the precision and performance of sharing analyses. Much of this work has centered around the family of set-sharing domains, because of the high precision they offer. However, this comes at a price: their scalability to a wide set of realistic programs remains challenging and this hinders their wider adoption. In this work, rather than defining new sharing abstract domains, we focus instead on developing techniques which can be incorporated in the analyzers to address aspects that are known to affect the efficiency of these domains, such as the number of variables, without affecting precision. These techniques are inspired in others used in the context of compiler optimizations, such as expression reassociation and variable trimming. We present several such techniques and provide an extensive experimental evaluation of over 1100 program modules taken from both production code and classical benchmarks. This includes the Spectector cache analyzer, the s(CASP) system, the libraries of the Ciao system, the LPdoc documenter, the PLAI analyzer itself, etc. The experimental results are quite encouraging: we have obtained significant speed-ups, and, more importantly, the number of modules that require a timeout was cut in half. As a result, many more programs can be analyzed precisely in reasonable times.

**Link**: [arxiv](http://arxiv.org/abs/2408.09848v1),  [pdf](http://arxiv.org/pdf/2408.09848v1)

**Tags**: cs.PL 



### AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for   Efficient MoE Inference
**Authors**: Shuzhang Zhong, Ling Liang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li

**Updated**: 2024-08-19T03:27:15Z

**Summary**: Mixture-of-Experts (MoE) models are designed to enhance the efficiency of large language models (LLMs) without proportionally increasing the computational demands. However, their deployment on edge devices still faces significant challenges due to high on-demand loading overheads from managing sparsely activated experts. This paper introduces AdapMoE, an algorithm-system co-design framework for efficient MoE inference. AdapMoE features adaptive expert gating and management to reduce the on-demand loading overheads. We observe the heterogeneity of experts loading across layers and tokens, based on which we propose a sensitivity-based strategy to adjust the number of activated experts dynamically. Meanwhile, we also integrate advanced prefetching and cache management techniques to further reduce the loading latency. Through comprehensive evaluations on various platforms, we demonstrate AdapMoE consistently outperforms existing techniques, reducing the average number of activated experts by 25% and achieving a 1.35x speedup without accuracy degradation. Code is available at: https://github.com/PKU-SEC-Lab/AdapMoE.

**Link**: [arxiv](http://arxiv.org/abs/2408.10284v1),  [pdf](http://arxiv.org/pdf/2408.10284v1)

**Tags**: cs.LG 



### Post-Training Sparse Attention with Double Sparsity
**Authors**: Shuo Yang, Ying Sheng, Joseph E. Gonzalez, Ion Stoica, Lianmin Zheng

**Updated**: 2024-08-18T17:27:17Z

**Summary**: The inference process for large language models is slow and memory-intensive, with one of the most critical bottlenecks being excessive Key-Value (KV) cache accesses. This paper introduces "Double Sparsity," a novel post-training sparse attention technique designed to alleviate this bottleneck by reducing KV cache access. Double Sparsity combines token sparsity, which focuses on utilizing only the important tokens for computing self-attention, with channel sparsity, an approach that uses important feature channels for identifying important tokens. Our key insight is that the pattern of channel sparsity is relatively static, allowing us to use offline calibration to make it efficient at runtime, thereby enabling accurate and efficient identification of important tokens. Moreover, this method can be combined with offloading to achieve significant memory usage reduction. Experimental results demonstrate that Double Sparsity can achieve $\frac{1}{16}$ token and channel sparsity with minimal impact on accuracy across various tasks, including wiki-2 perplexity, key-value retrieval, and long context benchmarks with models including Llama-2-7B, Llama-2-70B, and Mixtral-8x7B. It brings up to a 14.1$\times$ acceleration in attention operations and a 1.9$\times$ improvement in end-to-end inference on GPUs. With offloading, it achieves a decoding speed acceleration of 16.3$\times$ compared to state-of-the-art solutions at a sequence length of 256K. Our code is publicly available at https://github.com/andy-yang-1/DoubleSparse.

**Link**: [arxiv](http://arxiv.org/abs/2408.07092v2),  [pdf](http://arxiv.org/pdf/2408.07092v2)

**Tags**: cs.LG cs.AI cs.CL 



### CMD: A Cache-assisted GPU Memory Deduplication Architecture
**Authors**: Wei Zhao, Dan Feng, Wei Tong, Xueliang Wei, Bing Wu

**Updated**: 2024-08-18T13:54:46Z

**Summary**: Massive off-chip accesses in GPUs are the main performance bottleneck, and we divided these accesses into three types: (1) Write, (2) Data-Read, and (3) Read-Only. Besides, We find that many writes are duplicate, and the duplication can be inter-dup and intra-dup. While inter-dup means different memory blocks are identical, and intra-dup means all the 4B elements in a line are the same. In this work, we propose a cache-assisted GPU memory deduplication architecture named CMD to reduce the off-chip accesses via utilizing the data duplication in GPU applications. CMD includes three key design contributions which aim to reduce the three kinds of accesses: (1) A novel GPU memory deduplication architecture that removes the inter-dup and inter-dup lines. As for the inter-dup detection, we reduce the extra read requests caused by the traditional read-verify hash process. Besides, we design several techniques to manage duplicate blocks. (2) We propose a cache-assisted read scheme to reduce the reads to duplicate data. When an L2 cache miss wants to read the duplicate block, if the reference block has been fetched to L2 and it is clean, we can copy it to the L2 missed block without accessing off-chip DRAM. As for the reads to intra-dup data, CMD uses the on-chip metadata cache to get the data. (3) When a cache line is evicted, the clean sectors in the line are invalidated while the dirty sectors are written back. However, most read-only victims are re-referenced from DRAM more than twice. Therefore, we add a full-associate FIFO to accommodate the read-only (it is also clean) victims to reduce the re-reference counts. Experiments show that CMD can decrease the off-chip accesses by 31.01%, reduce the energy by 32.78% and improve performance by 37.79%. Besides, CMD can improve the performance of memory-intensive workloads by 50.18%.

**Link**: [arxiv](http://arxiv.org/abs/2408.09483v1),  [pdf](http://arxiv.org/pdf/2408.09483v1)

**Tags**: cs.AR 



### Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for   Efficient LLM Inference
**Authors**: Yuan Feng, Junlin Lv, Yukun Cao, Xike Xie, S. Kevin Zhou

**Updated**: 2024-08-16T08:46:33Z

**Summary**: Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks.

**Link**: [arxiv](http://arxiv.org/abs/2407.11550v3),  [pdf](http://arxiv.org/pdf/2407.11550v3)

**Tags**: cs.CL cs.AI 



### SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language   Models
**Authors**: Kaushal Kumar Maurya, KV Aditya Srivatsa, Ekaterina Kochmar

**Updated**: 2024-08-16T06:11:21Z

**Summary**: Large language models (LLMs) have gained increased popularity due to their remarkable success across various tasks, which has led to the active development of a large set of diverse LLMs. However, individual LLMs have limitations when applied to complex tasks because of such factors as training biases, model sizes, and the datasets used. A promising approach is to efficiently harness the diverse capabilities of LLMs to overcome these individual limitations. Towards this goal, we introduce a novel LLM selection algorithm called SelectLLM. This algorithm directs input queries to the most suitable subset of LLMs from a large pool, ensuring they collectively provide the correct response efficiently. SelectLLM uses a multi-label classifier, utilizing the classifier's predictions and confidence scores to design optimal policies for selecting an optimal, query-aware, and lightweight subset of LLMs. Our findings show that the proposed model outperforms individual LLMs and achieves competitive performance compared to similarly sized, computationally expensive top-performing LLM subsets. Specifically, with a similarly sized top-performing LLM subset, we achieve a significant reduction in latency on two standard reasoning benchmarks: 13% lower latency for GSM8K and 70% lower latency for MMLU. Additionally, we conduct comprehensive analyses and ablation studies, which validate the robustness of the proposed model.

**Link**: [arxiv](http://arxiv.org/abs/2408.08545v1),  [pdf](http://arxiv.org/pdf/2408.08545v1)

**Tags**: cs.CL 



### Symmetric Locality: Definition and Initial Results
**Authors**: Giordan Escalona, Dylan McKellips, Chen Ding

**Updated**: 2024-08-16T04:12:25Z

**Summary**: In this short paper, we characterize symmetric locality. In designing algorithms, compilers, and systems, data movement is a common bottleneck in high-performance computation, in which we improve cache and memory performance. We study a special type of data reuse in the form of repeated traversals, or re-traversals, which are based on the symmetric group. The cyclic and sawtooth traces are previously known results in symmetric locality, and in this work, we would like to generalize this result for any re-traversal. Then, we also provide an abstract framework for applications in compiler design and machine learning models to improve the memory performance of certain programs.

**Link**: [arxiv](http://arxiv.org/abs/2407.19291v2),  [pdf](http://arxiv.org/pdf/2407.19291v2)

**Tags**: eess.SY cs.SY 



### ConfusedPilot: Confused Deputy Risks in RAG-based LLMs
**Authors**: Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari

**Updated**: 2024-08-15T05:24:19Z

**Summary**: Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.04870v3),  [pdf](http://arxiv.org/pdf/2408.04870v3)

**Tags**: cs.CR cs.AI 



### A Case for Enabling Delegation of 5G Core Decisions to the RAN
**Authors**: Lucas Vancina, Geoffrey Xie

**Updated**: 2024-08-14T23:42:46Z

**Summary**: Under conventional 5G system design, the authentication and continuous monitoring of user equipment (UE) demands a reliable backhaul connection between the radio access network (RAN) and the core network functions (AMF, AUSF, UDM, etc.). This is not a given, especially in disaster response and military operations. We propose that, in these scenarios, decisions made by core functions can be effectively delegated to the RAN by leveraging the RAN's computing resources and the micro-service programmability of the O-RAN system architecture. This paper presents several concrete designs of core-RAN decision delegation, including caching of core decisions and replicating some of the core decision logic. Each design has revealed interesting performance and security trade-offs that warrant further investigation.

**Link**: [arxiv](http://arxiv.org/abs/2408.07853v1),  [pdf](http://arxiv.org/pdf/2408.07853v1)

**Tags**: cs.NI 



### The Bicameral Cache: a split cache for vector architectures
**Authors**: Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu

**Updated**: 2024-08-14T09:18:02Z

**Summary**: The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.

**Link**: [arxiv](http://arxiv.org/abs/2407.15440v2),  [pdf](http://arxiv.org/pdf/2407.15440v2)

**Tags**: cs.AR cs.PF 



### At Least Factor-of-Two Optimization for RWLE-Based Homomorphic   Encryption
**Authors**: Jonathan Ly

**Updated**: 2024-08-14T05:42:35Z

**Summary**: Many modern applications that deal with sensitive data, such as healthcare and government services, outsource computation to cloud platforms. In such untrusted environments, privacy is of vital importance. One solution to this problem is homomorphic encryption (HE), a family of cryptographic schemes that support certain algebraic operations on encrypted data without the need for decryption. However, despite major advancements, encryption in modern HE schemes still comes with a non-trivial computational overhead that can hamper data-intensive workloads. To resolve this, recent research has shown that leveraging caching techniques, such as Rache, can significantly enhance the performance of HE schemes while maintaining security. Rache unfortunately displays a key limitation in the time complexity of its caching procedure, which scales with the size of the plaintext space. Smuche is another caching scheme that simultaneously improves the scalability of the caching procedure and turns the encryption process into a constant-time operation, utilizing only a single scalar multiplication. Even still, more can be done. In this paper, we present an encryption method we call ``Zinc" which entirely forgoes the multiple caching process, replacing it with a single scalar addition, and then injecting randomness that takes constant time with respect to the plaintext space. This injection of randomness is similar to Smuche, and a great improvement from Rache, allowing Zinc to achieve efficiency without compromising security. We implement the scheme using Microsoft SEAL and compare its performance to vanilla CKKS.

**Link**: [arxiv](http://arxiv.org/abs/2408.07304v1),  [pdf](http://arxiv.org/pdf/2408.07304v1)

**Tags**: cs.CR 



### Cache-Aided MIMO Communications: DoF Analysis and Transmitter   Optimization
**Authors**: Mohammad NaseriTehrani, MohammadJavad Salehi, Antti T√∂lli

**Updated**: 2024-08-13T13:56:14Z

**Summary**: Cache-aided MIMO communications aims to jointly exploit both coded caching~(CC) and spatial multiplexing gains to enhance communication efficiency. In this paper, we first analyze the achievable degrees of freedom~(DoF) in a MIMO-CC system with CC gain \(t\), where a server with \(L\) transmit antennas communicates with \(K\) users, each equipped with \(G\) receive antennas. We demonstrate that the enhanced achievable DoF is \(\max_{\beta, \Omega} \Omega \beta\), where the number of users \(\Omega\) served in each transmission is fine-tuned to maximize DoF, and \(\beta \le \min\big(G, \nicefrac{L \binom{\Omega-1}{t}}{1 + (\Omega - t - 1)\binom{\Omega-1}{t}}\big)\) represents the number of parallel streams decoded by each user. Second, we introduce an effective transmit covariance matrix design aimed at maximizing the symmetric rate, solved iteratively via successive convex approximation. Third, we propose a new class of MIMO-CC schemes using a novel scheduling mechanism leveraging maximal multicasting opportunities to maximize delivery rates at given SNR levels while adhering to linear processing constraints. Lastly, we devise linear multicast beamforming strategies tailored for the flexible scheduling schemes in MIMO-CC systems and present an iterative solution for the efficient design of beamformers. Extensive numerical simulations are used to verify the results of the paper.

**Link**: [arxiv](http://arxiv.org/abs/2407.15743v2),  [pdf](http://arxiv.org/pdf/2407.15743v2)

**Tags**: cs.IT eess.SP math.IT 



### Ownership in low-level intermediate representation
**Authors**: Siddharth Priya, Arie Gurfinkel

**Updated**: 2024-08-13T13:31:34Z

**Summary**: The concept of ownership in high level languages can aid both the programmer and the compiler to reason about the validity of memory operations. Previously, ownership semantics has been used successfully in high level automatic program verification to model a reference to data by a first order logic (FOL) representation of data instead of maintaining an address map. However, ownership semantics is not used in low level program verification. We have identified two challenges. First, ownership information is lost when a program is compiled to a low level intermediate representation (e.g., in LLVM IR). Second, pointers in low level programs point to bytes using an address map (e.g., in unsafe Rust) and thus the verification condition (VC) cannot always replace a pointer by its FOL abstraction. To remedy the situation, we develop ownership semantics for an LLVM like low level intermediate representation. Using these semantics, the VC can opportunistically model some memory accesses by a direct access of a pointer cache that stores byte representation of data. This scheme reduces instances where an address map must be maintained, especially for mostly safe programs that follow ownership semantics. For unsafe functionality, memory accesses are modelled by operations on an address map and we provide mechanisms to keep the address map and pointer cache in sync. We implement these semantics in SEABMC, a bit precise bounded model checker for LLVM. For evaluation, the source programs are assumed to be written in C. Since C does not have ownership built in, suitable macros are added that introduce and preserve ownership during translation to LLVM like IR for verification. This approach is evaluated on mature open source C code. For both handcrafted benchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT solving.

**Link**: [arxiv](http://arxiv.org/abs/2408.04043v3),  [pdf](http://arxiv.org/pdf/2408.04043v3)

**Tags**: cs.PL cs.SE D.2.4 



### Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache   Consumption
**Authors**: Luohe Shi, Hongyi Zhang, Yao Yao, Zuchao Li, Hai Zhao

**Updated**: 2024-08-13T09:55:43Z

**Summary**: Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022, have revolutionized various industries with their advanced language comprehension. However, their efficiency is challenged by the Transformer architecture' s struggle with handling long texts. KV-Cache has emerged as a pivotal solution to this issue, converting the time complexity of token generation from quadratic to linear, albeit with increased GPU memory overhead proportional to conversation length. With the development of the LLM community and academia, various KV-Cache compression methods have been proposed. In this review, we dissect the various properties of KV-Cache and elaborate on various methods currently used to optimize the KV-Cache space usage of LLMs. These methods span the pre-training phase, deployment phase, and inference phase, and we summarize the commonalities and differences among these methods. Additionally, we list some metrics for evaluating the long-text capabilities of large language models, from both efficiency and capability perspectives. Our review thus sheds light on the evolving landscape of LLM optimization, offering insights into future advancements in this dynamic field.

**Link**: [arxiv](http://arxiv.org/abs/2407.18003v3),  [pdf](http://arxiv.org/pdf/2407.18003v3)

**Tags**: cs.CL 



### Finch: Prompt-guided Key-Value Cache Compression
**Authors**: Giulio Corallo, Paolo Papotti

**Updated**: 2024-08-13T09:08:55Z

**Summary**: Recent large language model applications, such as Retrieval-Augmented Generation and chatbots, have led to an increased need to process longer input contexts. However, this requirement is hampered by inherent limitations. Architecturally, models are constrained by a context window defined during training. Additionally, processing extensive texts requires substantial GPU memory. We propose a novel approach, Finch, to compress the input context by leveraging the pre-trained model weights of the self-attention. Given a prompt and a long text, Finch iteratively identifies the most relevant Key (K) and Value (V) pairs over chunks of the text conditioned on the prompt. Only such pairs are stored in the KV cache, which, within the space constrained by the context window, ultimately contains a compressed version of the long text. Our proposal enables models to consume large inputs even with high compression (up to 93x) while preserving semantic integrity without the need for fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2408.00167v2),  [pdf](http://arxiv.org/pdf/2408.00167v2)

**Tags**: cs.AI 



### Value-based Proactive Caching for Sensing Data in Internet of Vehicles
**Authors**: Yantong Wang, Ke Liu, Hui Ji, Jiande Sun

**Updated**: 2024-08-12T08:46:30Z

**Summary**: Sensing data (SD) plays an important role in safe-related applications for Internet of Vehicles. Proactively caching required sensing data (SD) is a pivotal strategy for alleviating network congestion and improving data accessibility. Despite merits, existing studies predominantly address SD caching within a single time slot, which may not be scalable to scenarios involving multi-slots. Furthermore, the oversight of service capacity at caching nodes could lead to significant queuing delays in SD reception. To tackle these limitations, we jointly consider the problem of anchoring caching placement and requests allocation for SD. A value model incorporating both temporal and spacial characteristics is first proposed to estimate the significance of different caching decisions. Subsequently, a stochastic integer nonlinear programming model is provided to optimize the long-term system performance, which is converted into a series of online optimization problem by leveraging the Lyapunov method and linearized via introducing auxiliary variables. To expedite the solution, we provide a binary quantum particle swarm optimization based algorithm with quadratic time complexity. Numerical investigations demonstrate the superiority of proposed algorithms compared with other schemes in terms of energy consumption, response latency, and cache-hit ratio.

**Link**: [arxiv](http://arxiv.org/abs/2408.05996v1),  [pdf](http://arxiv.org/pdf/2408.05996v1)

**Tags**: cs.NI 



### Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open   Source RISC-V application processor
**Authors**: Riccardo Tedeschi, Luca Valente, Gianmarco Ottavi, Enrico Zelioli, Nils Wistoff, Massimiliano Giacometti, Abdul Basit Sajjad, Luca Benini, Davide Rossi

**Updated**: 2024-08-12T07:47:28Z

**Summary**: Symmetric Multi-Processing (SMP) based on cache coherency is crucial for high-end embedded systems like automotive applications. RISC-V is gaining traction, and open-source hardware (OSH) platforms offer solutions to issues such as IP costs and vendor dependency. Existing multi-core cache-coherent RISC-V platforms are complex and not efficient for small embedded core clusters. We propose an open-source SystemVerilog implementation of a lightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our design uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with Splash-3 benchmarks, our solution shows up to 32.87% faster performance in a dual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized using GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of the system area.

**Link**: [arxiv](http://arxiv.org/abs/2407.19895v2),  [pdf](http://arxiv.org/pdf/2407.19895v2)

**Tags**: eess.SY cs.SY 



### Correct Wrong Path
**Authors**: Bhargav Reddy Godala, Sankara Prasad Ramesh, Krishnam Tibrewala, Chrysanthos Pepi, Gino Chacon, Svilen Kanev, Gilles A. Pokam, Daniel A. Jim√©nez, Paul V. Gratz, David I. August

**Updated**: 2024-08-12T03:53:51Z

**Summary**: Modern OOO CPUs have very deep pipelines with large branch misprediction recovery penalties. Speculatively executed instructions on the wrong path can significantly change cache state, depending on speculation levels. Architects often employ trace-driven simulation models in the design exploration stage, which sacrifice precision for speed. Trace-driven simulators are orders of magnitude faster than execution-driven models, reducing the often hundreds of thousands of simulation hours needed to explore new micro-architectural ideas. Despite this strong benefit of trace-driven simulation, these often fail to adequately model the consequences of wrong path because obtaining them is nontrivial. Prior works consider either a positive or negative impact of wrong path but not both. Here, we examine wrong path execution in simulation results and design a set of infrastructure for enabling wrong-path execution in a trace driven simulator. Our analysis shows the wrong path affects structures on both the instruction and data sides extensively, resulting in performance variations ranging from $-3.05$\% to $20.9$\% when ignoring wrong path. To benefit the research community and enhance the accuracy of simulators, we opened our traces and tracing utility in the hopes that industry can provide wrong-path traces generated by their internal simulators, enabling academic simulation without exposing industry IP.

**Link**: [arxiv](http://arxiv.org/abs/2408.05912v1),  [pdf](http://arxiv.org/pdf/2408.05912v1)

**Tags**: cs.AR 



### Hierarchical Coded Caching with Low Subpacketization and Coding Delay
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2024-08-11T16:35:10Z

**Summary**: Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN) considered a broadcast network consisting of a single server connected to a set of users each having a cache memory. Motivated by practical scenarios, Karamchandani \textit{et al.} in [16] proposed a coded caching scheme for a two-layer hierarchical network consisting of a single server connected to multiple mirror sites and each mirror site connected to a distinct set of users, in which both mirror sites and users having cache memories. Low subpacketization level coded caching schemes are desirable for practical implementations. Placement delivery array (PDA) was proposed as a tool to design coded caching schemes with reduced subpacketization level by Yan \textit{et al.} in [4]. Schemes with reduced subpacketization levels are studied extensively in the literature for single-layer networks. Kong \textit{et al.} in [17] proposed a structure called hierarchical placement delivery arrays (HPDA), which characterizes a hierarchical coded caching system and also proposed a class of HPDAs that gives low subpacketization level schemes by using two PDAs. Low subpacketization level hierarchical schemes using combinatorial $t$-designs is proposed in [20]. Apart from that there is no other existing work that discusses the subpacketization problem in a hierarchical network. This paper proposes a class of HPDA construction that gives low subpacketization level hierarchical coded caching schemes, by first constructing a new class of PDAs. Compared with the existing schemes, in cases where the system parameters and subpacketization level are the same, the proposed hierarchical scheme has a better coding delay. Further, the new class of PDAs constructed either subsumes several known PDA constructions or achieves better transmission load for the same system parameters.

**Link**: [arxiv](http://arxiv.org/abs/2405.12747v2),  [pdf](http://arxiv.org/pdf/2405.12747v2)

**Tags**: cs.IT math.IT 



### Genie: Smart ROS-based Caching for Connected Autonomous Robots
**Authors**: Zexin Li, Soroush Bateni, Cong Liu

**Updated**: 2024-08-11T08:07:28Z

**Summary**: Despite the promising future of autonomous robots, several key issues currently remain that can lead to compromised performance and safety. One such issue is latency, where we find that even the latest embedded platforms from NVIDIA fail to execute intelligence tasks (e.g., object detection) of autonomous vehicles in a real-time fashion. One remedy to this problem is the promising paradigm of edge computing. Through collaboration with our industry partner, we identify key prohibitive limitations of the current edge mindset: (1) servers are not distributed enough and thus, are not close enough to vehicles, (2) current proposed edge solutions do not provide substantially better performance and extra information specific to autonomous vehicles to warrant their cost to the user, and (3) the state-of-the-art solutions are not compatible with popular frameworks used in autonomous systems, particularly the Robot Operating System (ROS).   To remedy these issues, we provide Genie, an encapsulation technique that can enable transparent caching in ROS in a non-intrusive way (i.e., without modifying the source code), can build the cache in a distributed manner (in contrast to traditional central caching methods), and can construct a collective three-dimensional object map to provide substantially better latency (even on low-power edge servers) and higher quality data to all vehicles in a certain locality. We fully implement our design on state-of-the-art industry-adopted embedded and edge platforms, using the prominent autonomous driving software Autoware, and find that Genie can enhance the latency of Autoware Vision Detector by 82% on average, enable object reusability 31% of the time on average and as much as 67% for the incoming requests, and boost the confidence in its object map considerably over time.

**Link**: [arxiv](http://arxiv.org/abs/2402.19410v2),  [pdf](http://arxiv.org/pdf/2402.19410v2)

**Tags**: cs.RO cs.SY eess.SY 



### Eigen Attention: Attention in Low-Rank Space for KV Cache Compression
**Authors**: Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy

**Updated**: 2024-08-10T22:47:12Z

**Summary**: Large language models (LLMs) represent a groundbreaking advancement in the domain of natural language processing due to their impressive reasoning abilities. Recently, there has been considerable interest in increasing the context lengths for these models to enhance their applicability to complex tasks. However, at long context lengths and large batch sizes, the key-value (KV) cache, which stores the attention keys and values, emerges as the new bottleneck in memory usage during inference. To address this, we propose Eigen Attention, which performs the attention operation in a low-rank space, thereby reducing the KV cache memory overhead. Our proposed approach is orthogonal to existing KV cache compression techniques and can be used synergistically with them. Through extensive experiments over OPT, MPT, and Llama model families, we demonstrate that Eigen Attention results in up to 40% reduction in KV cache sizes and up to 60% reduction in attention operation latency with minimal drop in performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.05646v1),  [pdf](http://arxiv.org/pdf/2408.05646v1)

**Tags**: cs.LG cs.AI cs.CL 



### ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using   Gaussian Mixture Model
**Authors**: Hanqiu Chen, Yitu Wang, Luis Vitorio Cargnini, Mohammadreza Soltaniyeh, Dongyang Li, Gongjin Sun, Pradeep Subedi, Andrew Chang, Yiran Chen, Cong Hao

**Updated**: 2024-08-10T19:17:46Z

**Summary**: Compute Express Link (CXL) emerges as a solution for wide gap between computational speed and data communication rates among host and multiple devices. It fosters a unified and coherent memory space between host and CXL storage devices such as such as Solid-state drive (SSD) for memory expansion, with a corresponding DRAM implemented as the device cache. However, this introduces challenges such as substantial cache miss penalties, sub-optimal caching due to data access granularity mismatch between the DRAM "cache" and SSD "memory", and inefficient hardware cache management. To address these issues, we propose a novel solution, named ICGMM, which optimizes caching and eviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based approach. We prototype our solution on an FPGA board, which demonstrates a noteworthy improvement compared to the classic Least Recently Used (LRU) cache strategy. We observe a decrease in the cache miss rate ranging from 0.32% to 6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD access latency. Furthermore, when compared to the state-of-the-art Long Short-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA showcases an impressive latency reduction of over 10,000 times. Remarkably, this is achieved while demanding much fewer hardware resources.

**Link**: [arxiv](http://arxiv.org/abs/2408.05614v1),  [pdf](http://arxiv.org/pdf/2408.05614v1)

**Tags**: cs.AR cs.ET cs.SY eess.SY 



### Time-resolved measurement of neutron energy isotropy in a   sheared-flow-stabilized Z pinch
**Authors**: R. A. Ryan, P. E. Tsai, A. R. Johansen, A. Youmans, D. P. Higginson, J. M. Mitrani, C. S. Adams, D. A. Sutherland, B. Levitt, U. Shumlak

**Updated**: 2024-08-09T16:48:01Z

**Summary**: Previous measurements of neutron energy using fast plastic scintillators while operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of any yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been operated at increasingly higher input power, resulting in increased plasma current and larger fusion neutron yields. A detailed experimental study of the neutron energy isotropy in these regimes applies more stringent limits to possible contributions from beam-target fusion. The FuZE device operated at $-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and D-D fusion neutron yields of $4\times10^7$ neutrons per discharge. Measurements of the neutron energy isotropy under these operating conditions demonstrates the energy of deuteron beams is less than $7.4 \pm 5.6^\mathrm{(stat)} \pm 3.7^\mathrm{(syst)}~keV$. Characterization of the detector response has reduced the number of free parameters in the fit of the neutron energy distribution, improving the confidence in the forward-fit method. Gamma backgrounds have been measured and the impact of these contributions on the isotropy results have been studied. Additionally, a time dependent measurement of the isotropy has been resolved for the first time, indicating increases to possible deuteron beam energies at late times. This suggests the possible growth of $m$=0 instabilities at the end of the main radiation event but confirms that the majority of the neutron production exhibits isotropy consistent with thermonuclear origin.

**Link**: [arxiv](http://arxiv.org/abs/2408.05171v1),  [pdf](http://arxiv.org/pdf/2408.05171v1)

**Tags**: physics.plasm-ph nucl-ex 



### NACL: A General and Effective KV Cache Eviction Framework for LLMs at   Inference Time
**Authors**: Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu

**Updated**: 2024-08-08T01:20:13Z

**Summary**: Large Language Models (LLMs) have ignited an innovative surge of AI applications, marking a new era of exciting possibilities equipped with extended context windows. However, hosting these models is cost-prohibitive mainly due to the extensive memory consumption of KV Cache involving long-context modeling. Despite several works proposing to evict unnecessary tokens from the KV Cache, most of them rely on the biased local statistics of accumulated attention scores and report performance using unconvincing metric like perplexity on inadequate short-text evaluation. In this paper, we propose NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase. Due to NACL's efficiency, we combine more accurate attention score statistics in PROXY TOKENS EVICTION with the diversified random eviction strategy of RANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance the robustness in maintaining pivotal tokens for long-context modeling tasks. Notably, our method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance. The code is available at https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.

**Link**: [arxiv](http://arxiv.org/abs/2408.03675v2),  [pdf](http://arxiv.org/pdf/2408.03675v2)

**Tags**: cs.CL 



### A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals   and Future Trends
**Authors**: Yao Zhao, Youyang Qu, Yong Xiang, Md Palash Uddin, Dezhong Peng, Longxiang Gao

**Updated**: 2024-08-07T23:48:59Z

**Summary**: Recent advances in edge computing~(EC) have pushed cloud-based data caching services to edge, however, such emerging edge storage comes with numerous challenging and unique security issues. One of them is the problem of edge data integrity verification (EDIV) which coordinates multiple participants (e.g., data owners and edge nodes) to inspect whether data cached on edge is authentic. To date, various solutions have been proposed to address the EDIV problem, while there is no systematic review. Thus, we offer a comprehensive survey for the first time, aiming to show current research status, open problems, and potentially promising insights for readers to further investigate this under-explored field. Specifically, we begin by stating the significance of the EDIV problem, the integrity verification difference between data cached on cloud and edge, and three typical system models with corresponding inspection processes. To thoroughly assess prior research efforts, we synthesize a universal criteria framework that an effective verification approach should satisfy. On top of it, a schematic development timeline is developed to reveal the research advance on EDIV in a sequential manner, followed by a detailed review of the existing EDIV solutions. Finally, we highlight intriguing research challenges and possible directions for future work, along with a discussion on how forthcoming technology, e.g., machine learning and context-aware security, can augment security in EC. Given our findings, some major observations are: there is a noticeable trend to equip EDIV solutions with various functions and diversify study scenarios; completing EDIV within two types of participants (i.e., data owner and edge nodes) is garnering escalating interest among researchers; although the majority of existing methods rely on cryptography, emerging technology is being explored to handle the EDIV problem.

**Link**: [arxiv](http://arxiv.org/abs/2210.10978v2),  [pdf](http://arxiv.org/pdf/2210.10978v2)

**Tags**: cs.CR 



### Zero-Delay QKV Compression for Mitigating KV Cache and Network   Bottlenecks in LLM Inference
**Authors**: Zeyu Zhang, Haiying Shen

**Updated**: 2024-08-07T22:10:26Z

**Summary**: In large-language models, memory constraints in the key-value cache (KVC) pose a challenge during inference, especially with long prompts. In this work, we observed that compressing KV values is more effective than compressing the model regarding accuracy and job completion time (JCT). However, quantizing KV values and dropping less-important tokens incur significant runtime computational time overhead, delaying JCT. These methods also cannot reduce computation time or high network communication time overhead in sequence-parallelism (SP) frameworks for long prompts. To tackle these issues, based on our insightful observations from experimental analysis, we propose ZeroC, a Zero-delay QKV Compression system that eliminates time overhead and even reduces computation and communication time of the model operations. ZeroC innovatively embeds compression and decompression operations within model operations and adaptively determines compression ratios at a hybrid layer-token level. Further, it enables a communication-efficient SP inference framework. Trace-driven experiments demonstrate that ZeroC achieves up to 80% lower average JCT, 35% lower average perplexity, and 2.8x higher throughput with the same latency compared to state-of-the-art compression methods. ZeroC also reduces the average JCT of current LLM serving systems by up to 91% with the constraint of 0.1 perplexity increase. We open-sourced the code.

**Link**: [arxiv](http://arxiv.org/abs/2408.04107v1),  [pdf](http://arxiv.org/pdf/2408.04107v1)

**Tags**: cs.LG cs.DC 



### Temporal Feature Matters: A Framework for Diffusion Model Quantization
**Authors**: Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, Dacheng Tao

**Updated**: 2024-08-07T20:43:10Z

**Summary**: The Diffusion models, widely used for image generation, face significant challenges related to their broad applicability due to prolonged inference times and high memory demands. Efficient Post-Training Quantization (PTQ) is crucial to address these issues. However, unlike traditional models, diffusion models critically rely on the time-step for the multi-round denoising. Typically, each time-step is encoded into a hypersensitive temporal feature by several modules. Despite this, existing PTQ methods do not optimize these modules individually. Instead, they employ unsuitable reconstruction objectives and complex calibration methods, leading to significant disturbances in the temporal feature and denoising trajectory, as well as reduced compression efficiency. To address these challenges, we introduce a novel quantization framework that includes three strategies: 1) TIB-based Maintenance: Based on our innovative Temporal Information Block (TIB) definition, Temporal Information-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are developed to efficiently align original temporal features. 2) Cache-based Maintenance: Instead of indirect and complex optimization for the related modules, pre-computing and caching quantized counterparts of temporal features are developed to minimize errors. 3) Disturbance-aware Selection: Employ temporal feature errors to guide a fine-grained selection between the two maintenance strategies for further disturbance reduction. This framework preserves most of the temporal information and ensures high-quality end-to-end generation. Extensive testing on various datasets, diffusion models and hardware confirms our superior performance and acceleration..

**Link**: [arxiv](http://arxiv.org/abs/2407.19547v2),  [pdf](http://arxiv.org/pdf/2407.19547v2)

**Tags**: cs.CV 



### mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest   Neighbor Search
**Authors**: Ahmed Abdou, Tasneem Mohsen

**Updated**: 2024-08-07T09:34:55Z

**Summary**: Named Entity Recognition (NER) is a task in Natural Language Processing (NLP) that aims to identify and classify entities in text into predefined categories. However, when applied to Arabic data, NER encounters unique challenges stemming from the language's rich morphological inflections, absence of capitalization cues, and spelling variants, where a single word can comprise multiple morphemes. In this paper, we introduce Arabic KNN-NER, our submission to the Wojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the shared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained flat-entity recognition for Arabic text, where we identify a single main entity and possibly zero or multiple sub-entities for each word. Arabic KNN-NER augments the probability distribution of a fine-tuned model with another label probability distribution derived from performing a KNN search over the cached training data. Our submission achieved 91% on the test set on the WojoodFine dataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.

**Link**: [arxiv](http://arxiv.org/abs/2408.03652v1),  [pdf](http://arxiv.org/pdf/2408.03652v1)

**Tags**: cs.CL cs.LG 



### Potential and Limitation of High-Frequency Cores and Caches
**Authors**: Kunal Pai, Anusheel Nand, Jason Lowe-Power

**Updated**: 2024-08-06T17:16:19Z

**Summary**: This paper explores the potential of cryogenic computing and superconducting electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Cryogenic computing operates at ultra-low temperatures near 77 K, leading to lower leakage currents and improved electron mobility. On the other hand, superconducting electronics, operating near 0 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconducting electronics and cryogenic computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconducting technologies, laying the foundation for future research in this field using gem5.

**Link**: [arxiv](http://arxiv.org/abs/2408.03308v1),  [pdf](http://arxiv.org/pdf/2408.03308v1)

**Tags**: cs.AR 



### LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning
**Authors**: Lekai Chen, Ashutosh Trivedi, Alvaro Velasquez

**Updated**: 2024-08-06T07:12:09Z

**Summary**: The emergence of intelligence in large language models (LLMs) has inspired investigations into their integration into automata learning. This paper introduces the probabilistic Minimally Adequate Teacher (pMAT) formulation, which leverages a probabilistic oracle that could give persistent errors randomly during answering the membership queries for deterministic finite automata (DFA) learning. Given the tendency of LLMs to produce hallucinatory content, we have developed techniques to improve answer accuracy and ensure the correctness of the learned automata. We propose the $\mathtt{Discrimination}$ prompt as well as the $\mathtt{Verification}$ prompt and explore their advantages over common prompts. Additionally, we compare DFA learning performance between the TTT algorithm and common active learning algorithms. To address the exponential number of persistent errors, we implement a dynamic query cache refinement algorithm that identifies and corrects conflicting queries by combining the active and passive learning algorithms. The empirical results demonstrate the robustness and efficiency of our approach, providing a theoretical foundation for automata learning with LLMs in the loop.

**Link**: [arxiv](http://arxiv.org/abs/2408.02999v1),  [pdf](http://arxiv.org/pdf/2408.02999v1)

**Tags**: cs.FL cs.AI 



### NVPC: A Transparent NVM Page Cache
**Authors**: Guoyu Wang, Xilong Che, Haoyang Wei, Shuo Chen, Puyi He, Juncheng Hu

**Updated**: 2024-08-06T02:51:22Z

**Summary**: Towards a compatible utilization of NVM, NVM-specialized kernel file systems and NVM-based disk file system accelerators have been proposed. However, these studies only focus on one or several characteristics of NVM, while failing to exploit its best practice by putting NVM in the proper position of the whole storage stack. In this paper, we present NVPC, a transparent acceleration to existing kernel file systems with an NVM-enhanced page cache. The acceleration lies in two aspects, respectively matching the desperate needs of existing disk file systems: sync writes and cache-missed operations. Besides, the fast DRAM page cache is preserved for cache-hit operations. For sync writes, a high-performance log-based sync absorbing area is provided to redirect data destination from the slow disk to the fast NVM. Meanwhile, the byte-addressable feature of NVM is used to prevent write amplification. For cache-missed operations, NVPC makes use of the idle space on NVM to extend the DRAM page cache, so that more and larger workloads can fit into the cache. NVPC is entirely implemented as a page cache, thus can provide efficient speed-up to disk file systems with full transparency to users and full compatibility to lower file systems.   In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x faster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger than DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and SPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in 62.5% of the tested cases in our read/write/sync mixed evaluation, demonstrating that NVPC is more balanced and adaptive to complex real-world workloads. Experimental results also show that NVPC is the only method that accelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to any other use cases.

**Link**: [arxiv](http://arxiv.org/abs/2408.02911v1),  [pdf](http://arxiv.org/pdf/2408.02911v1)

**Tags**: cs.OS 



### Electron-beam-induced modification of gold microparticles in an SEM
**Authors**: Kristina Weinel, Marc Benjamin Hahn, Axel Lubk, Wen Feng, Ignacio Gonzalez Martinez, Bernd B√ºchner, Leonardo Agudo J√°come

**Updated**: 2024-08-05T12:09:50Z

**Summary**: Electron-beam-induced conversion of materials in a transmission electron microscope uses the high power density of a localized electron beam of acceleration voltages above 100 kV as an energy source to transform matter at the sub-micron scale. Here, the e-beam-induced transformation of precursor microparticles employing a low-energy e-beam with an acceleration voltage of 30 kV in a scanning electron microscope is developed to increase the versatility and efficiency of the technique. Under these conditions, the technique can be classified between e-beam lithography, where the e-beam is used to mill holes in or grow some different material onto a substrate, and e-beam welding, where matter can be welded together when overcoming the melting phase. Modifying gold microparticles on an amorphous SiOx substrate reveals the dominant role of inelastic electron-matter interaction and subsequent localized heating for the observed melting and vaporization of the precursor microparticles under the electron beam. Monte-Carlo scattering simulations and thermodynamic modeling further support the findings.

**Link**: [arxiv](http://arxiv.org/abs/2408.02409v1),  [pdf](http://arxiv.org/pdf/2408.02409v1)

**Tags**: cond-mat.mtrl-sci 



### SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference   Serving
**Authors**: Andreas Kosmas Kakolyris, Dimosthenis Masouros, Petros Vavaroutsos, Sotirios Xydis, Dimitrios Soudris

**Updated**: 2024-08-05T09:07:06Z

**Summary**: As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \textit{throttLL'eM} achieves up to 43.8\% lower energy consumption and an energy efficiency improvement of at least $1.71\times$ under SLOs, when compared to NVIDIA's Triton server.

**Link**: [arxiv](http://arxiv.org/abs/2408.05235v1),  [pdf](http://arxiv.org/pdf/2408.05235v1)

**Tags**: cs.DC cs.AI cs.AR cs.LG 



### TriForce: Lossless Acceleration of Long Sequence Generation with   Hierarchical Speculative Decoding
**Authors**: Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, Beidi Chen

**Updated**: 2024-08-04T00:58:04Z

**Summary**: With large language models (LLMs) widely deployed in long content generation recently, there has emerged an increasing demand for efficient long-sequence inference support. However, key-value (KV) cache, which is stored to avoid re-computation, has emerged as a critical bottleneck by growing linearly in size with the sequence length. Due to the auto-regressive nature of LLMs, the entire KV cache will be loaded for every generated token, resulting in low utilization of computational cores and high latency. While various compression methods for KV cache have been proposed to alleviate this issue, they suffer from degradation in generation quality. We introduce TriForce, a hierarchical speculative decoding system that is scalable for long sequence generation. This approach leverages the original model weights and dynamic sparse KV cache via retrieval as a draft model, which serves as an intermediate layer in the hierarchy and is further speculated by a smaller model to reduce its drafting latency. TriForce not only facilitates impressive speedups for Llama2-7B-128K, achieving up to 2.31$\times$ on an A100 GPU but also showcases scalability in handling even longer contexts. For the offloading setting on two RTX 4090 GPUs, TriForce achieves 0.108s/token$\unicode{x2014}$only half as slow as the auto-regressive baseline on an A100, which attains 7.78$\times$ on our optimized offloading system. Additionally, TriForce performs 4.86$\times$ than DeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is highlighted by its consistently outstanding performance across various temperatures. The code is available at https://github.com/Infini-AI-Lab/TriForce.

**Link**: [arxiv](http://arxiv.org/abs/2404.11912v3),  [pdf](http://arxiv.org/pdf/2404.11912v3)

**Tags**: cs.CL cs.LG 



### Cross-layer Attention Sharing for Large Language Models
**Authors**: Yongyu Mu, Yuzhang Wu, Yuchun Fan, Chenglong Wang, Hengyu Li, Qiaozhi He, Murun Yang, Tong Xiao, Jingbo Zhu

**Updated**: 2024-08-04T00:38:34Z

**Summary**: As large language models (LLMs) evolve, the increase in model depth and parameter number leads to substantial redundancy. To enhance the efficiency of the attention mechanism, previous works primarily compress the KV cache or group attention heads, while largely overlooking redundancy between layers. Our comprehensive analyses across various LLMs show that highly similar attention patterns persist within most layers. It's intuitive to save the computation by sharing attention weights across layers. However, further analysis reveals two challenges: (1) Directly sharing the weight matrix without carefully rearranging the attention heads proves to be ineffective; (2) Shallow layers are vulnerable to small deviations in attention weights. Driven by these insights, we introduce LiSA, a lightweight substitute for self-attention in well-trained LLMs. LiSA employs tiny feed-forward networks to align attention heads between adjacent layers and low-rank matrices to approximate differences in layer-wise attention weights. Evaluations encompassing 13 typical benchmarks demonstrate that LiSA maintains high response quality in terms of accuracy and perplexity while reducing redundant attention calculations within 53-84% of the total layers. Our implementations of LiSA achieve a 6X compression of Q and K, with maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for LLaMA2-7B.

**Link**: [arxiv](http://arxiv.org/abs/2408.01890v1),  [pdf](http://arxiv.org/pdf/2408.01890v1)

**Tags**: cs.CL 



### Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling
**Authors**: Xiao Jiang, Grace J. Gang, J. Webster Stayman

**Updated**: 2024-08-02T18:25:57Z

**Summary**: Many spectral CT applications require accurate material decomposition. Existing material decomposition algorithms are often susceptible to significant noise magnification or, in the case of one-step model-based approaches, hampered by slow convergence rates and large computational requirements. In this work, we proposed a novel framework - spectral diffusion posterior sampling (spectral DPS) - for one-step reconstruction and multi-material decomposition, which combines sophisticated prior information captured by one-time unsupervised learning and an arbitrary analytic physical system model. Spectral DPS is built upon a general DPS framework for nonlinear inverse problems. Several strategies developed in previous work, including jumpstart sampling, Jacobian approximation, and multi-step likelihood updates are applied facilitate stable and accurate decompositions. The effectiveness of spectral DPS was evaluated on a simulated dual-layer and a kV-switching spectral system as well as on a physical cone-beam CT (CBCT) test bench. In simulation studies, spectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53% to 57.30% over MBMD, depending on the the region of interest. In physical phantom study, spectral DPS achieved a <1% error in estimating the mean density in a homogeneous region. Compared with baseline DPS, spectral DPS effectively avoided generating false structures in the homogeneous phantom and reduced the variability around edges. Both simulation and physical phantom studies demonstrated the superior performance of spectral DPS for stable and accurate material decomposition.

**Link**: [arxiv](http://arxiv.org/abs/2408.01519v1),  [pdf](http://arxiv.org/pdf/2408.01519v1)

**Tags**: physics.med-ph 



### Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching   in SSD's NAND Flash Memory Chip for Data Indexing Acceleration
**Authors**: Yun-Chih Chen, Yuan-Hao Chang, Tei-Wei Kuo

**Updated**: 2024-08-02T07:37:51Z

**Summary**: To index the increasing volume of data, modern data indexes are typically stored on SSDs and cached in DRAM. However, searching such an index has resulted in significant I/O traffic due to limited access locality and inefficient cache utilization. At the heart of index searching is the operation of filtering through vast data spans to isolate a small, relevant subset, which involves basic equality tests rather than the complex arithmetic provided by modern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which demonstrates the feasibility of performing data filtering directly within a NAND flash memory chip, transmitting only relevant search results rather than complete pages. Instead of adding complex circuits, we propose repurposing existing circuitry for efficient and accurate bitwise parallel matching. We demonstrate how different data structures can use our flexible SIMD command interface to offload index searches. This strategy not only frees up the CPU for more computationally demanding tasks, but it also optimizes DRAM usage for write buffering, significantly lowering energy consumption associated with I/O transmission between the CPU and DRAM. Extensive testing across a wide range of workloads reveals up to a 9X speedup in write-heavy workloads and up to 45% energy savings due to reduced read and write I/O. Furthermore, we achieve significant reductions in median and tail read latencies of up to 89% and 85% respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.00327v2),  [pdf](http://arxiv.org/pdf/2408.00327v2)

**Tags**: cs.AR 



### Caching Aided Multi-Tenant Serverless Computing
**Authors**: Chu Qiao, Cong Wang, Zhenkai Zhang, Yuede Ji, Xing Gao

**Updated**: 2024-08-01T23:52:43Z

**Summary**: One key to enabling high-performance serverless computing is to mitigate cold-starts. Current solutions utilize a warm pool to keep function alive: a warm-start can be analogous to a CPU cache-hit. However, modern cache has multiple hierarchies and the last-level cache is shared among cores, whereas the warm pool is limited to a single tenant for security concerns. Also, the warm pool keep-alive policy can be further optimized using cache replacement algorithms. In this paper, we borrow practical optimizations from caching, and design FaasCamp, a caching-aided multi-tenant serverless computing framework. FaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim pool introduced enabling secure function instance sharing among tenants. Also, FaasCamp leverages machine learning to approximate the optimal cache replacement policy to improve the warm rate. We have implemented a prototype and conducted extensive experiments under multiple scenarios. The results show that FaasCamp can outperform existing platforms with minimal overhead.

**Link**: [arxiv](http://arxiv.org/abs/2408.00957v1),  [pdf](http://arxiv.org/pdf/2408.00957v1)

**Tags**: cs.DC 



### Do language models plan ahead for future tokens?
**Authors**: Wilson Wu, John X. Morris, Lionel Levine

**Updated**: 2024-08-01T21:21:28Z

**Summary**: Do transformers "think ahead" during inference at a given position? It is known transformers prepare information in the hidden states of the forward pass at time step $t$ that is then used in future forward passes $t+\tau$. We posit two explanations for this phenomenon: pre-caching, in which off-diagonal gradient terms present during training result in the model computing features at $t$ irrelevant to the present inference task but useful for the future, and breadcrumbs, in which features most relevant to time step $t$ are already the same as those that would most benefit inference at time $t+\tau$. We test these hypotheses by training language models without propagating gradients to past timesteps, a scheme we formalize as myopic training. In a constructed synthetic data setting, we find clear evidence for pre-caching. In the autoregressive language modeling setting, our experiments are more suggestive of the breadcrumbs hypothesis, though pre-caching increases with model scale.

**Link**: [arxiv](http://arxiv.org/abs/2404.00859v2),  [pdf](http://arxiv.org/pdf/2404.00859v2)

**Tags**: cs.LG cs.CL 



### Intermittent Semi-working Mask: A New Masking Paradigm for LLMs
**Authors**: Mingcong Lu, Jiangcai Zhu, Wang Hao, Zheng Li, Shusheng Zhang, Kailai Shao, Chao Chen, Nan Li, Feng Wang, Xin Lu

**Updated**: 2024-08-01T13:22:01Z

**Summary**: Multi-turn dialogues are a key interaction method between humans and Large Language Models (LLMs), as conversations extend over multiple rounds, keeping LLMs' high generation quality and low latency is a challenge. Mainstream LLMs can be grouped into two categories based on masking strategy: causal LLM and prefix LLM. Several works have demonstrated that prefix LLMs tend to outperform causal ones in scenarios that heavily depend on historical context such as multi-turn dialogues or in-context learning, thanks to their bidirectional attention on prefix sequences. However, prefix LLMs have an inherent inefficient training problem in multi-turn dialogue datasets. In addition, the attention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV Cache) across dialogue rounds to reduce generation latency. In this paper, we propose a novel masking scheme called Intermittent Semi-working Mask (ISM) to address these problems. Specifically, we apply alternate bidirectional and unidirectional attention on queries and answers in the dialogue history. In this way, ISM is able to maintain the high quality of prefix LLM and low generation latency of causal LLM, simultaneously. Extensive experiments illustrate that our ISM achieves significant performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.00539v1),  [pdf](http://arxiv.org/pdf/2408.00539v1)

**Tags**: cs.CL cs.AI 



### MoE-Infinity: Offloading-Efficient MoE Model Serving
**Authors**: Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh Marina

**Updated**: 2024-08-01T13:21:24Z

**Summary**: This paper presents MoE-Infinity, an offloading-efficient serving system for sparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity achieves novel request-level tracing for expert activation, capturing MoE's sparse execution patterns such as selective activation, group activation, and skewed reuse. Leveraging the request-level trace, MoE-Infinity performs effective expert prefetching and expert caching, achieving high efficiency in transferring model parameters from host memory to GPU memory. Experimental results demonstrate that MoE-Infinity achieves low latency comparable to expensive full-GPU deployments, which require up to 4X more GPU resources than MoE-Infinity. Compared to offloading-supporting LLM serving systems such as DeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm, MoE-Infinity exhibits superior latency performance, providing 2-20X improvements when serving various MoE models for a large collection of LLM tasks. MoE-Infinity's source code is publicly available a https://github.com/TorchMoE/MoE-Infinity

**Link**: [arxiv](http://arxiv.org/abs/2401.14361v2),  [pdf](http://arxiv.org/pdf/2401.14361v2)

**Tags**: cs.LG cs.PF 



### ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and   Two-Phase Partition
**Authors**: Lu Ye, Ze Tao, Yong Huang, Yang Li

**Updated**: 2024-08-01T07:51:25Z

**Summary**: Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096.

**Link**: [arxiv](http://arxiv.org/abs/2402.15220v4),  [pdf](http://arxiv.org/pdf/2402.15220v4)

**Tags**: cs.LG cs.CL 



### CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph   Neural Network Training with Communication Reduction
**Authors**: Shuai Zhang, Zite Jiang, Haihang You

**Updated**: 2024-08-01T01:57:09Z

**Summary**: Graph neural network training is mainly categorized into mini-batch and full-batch training methods. The mini-batch training method samples subgraphs from the original graph in each iteration. This sampling operation introduces extra computation overhead and reduces the training accuracy. Meanwhile, the full-batch training method calculates the features and corresponding gradients of all vertices in each iteration, and therefore has higher convergence accuracy. However, in the distributed cluster, frequent remote accesses of vertex features and gradients lead to huge communication overhead, thus restricting the overall training efficiency.   In this paper, we introduce the cached-based distributed full-batch graph neural network training framework (CDFGNN). We propose the adaptive cache mechanism to reduce the remote vertex access by caching the historical features and gradients of neighbor vertices. Besides, we further optimize the communication overhead by quantifying the messages and designing the graph partition algorithm for the hierarchical communication architecture. Experiments show that the adaptive cache mechanism reduces remote vertex accesses by 63.14% on average. Combined with communication quantization and hierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed full-batch training frameworks by 30.39% in our experiments. Our results indicate that CDFGNN has great potential in accelerating distributed full-batch GNN training tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.00232v1),  [pdf](http://arxiv.org/pdf/2408.00232v1)

**Tags**: cs.DC cs.LG 



### Towards Variable-Length In-Network Caching
**Authors**: Gyuyeong Kim

**Updated**: 2024-08-01T00:41:52Z

**Summary**: We present StarCache, a new in-network caching architecture that can cache variable-length items to balance a wide range of key-value workloads. Unlike existing works, StarCache does not cache hot items in the switch memory. Instead, we make hot items revisit the switch data plane continuously by exploiting packet recirculation. Our approach keeps cached key-value pairs in the switch data plane while freeing them from item size limitations caused by hardware constraints. We implement a StarCache prototype on an Intel Tofino switch. Our experimental results show that StarCache can balance highly skewed workloads with various key and value sizes.

**Link**: [arxiv](http://arxiv.org/abs/2407.21324v2),  [pdf](http://arxiv.org/pdf/2407.21324v2)

**Tags**: cs.NI 



### A2SF: Accumulative Attention Scoring with Forgetting Factor for Token   Pruning in Transformer Decoder
**Authors**: Hyun-rae Jo, Dongkun Shin

**Updated**: 2024-07-31T02:02:40Z

**Summary**: Recently, large language models (LLM) based on transformers are facing memory bottleneck issues due to KV cache, especially in long sequence handling. Previous researches proposed KV cache compression techniques that identify insignificant tokens based on Accumulative Attention Scores and removes their items from KV cache, noting that only few tokens play an important role in attention operations. However, we have observed that the existing Accumulative Attention Score is not suitable for the transformer decoder structure. In the decoder model, the number of times the Attention Score accumulates varies depending on the order of token appearance due to the effect of masking, causing an uneven comparison between tokens. To solve this, we propose Accumulative Attention Score with Forgetting Factor (A2SF) technique, which introduces a Forgetting Factor in the Attention Score accumulation process. A2SF applies a penalty to the past Attention Score generated from old tokens by repeatedly multiplying the Forgetting Factor to the Attention Score over time. Therefore, older tokens receive a larger penalty, providing fairness among different ages of tokens. Through the fair comparison among tokens, we can more effectively select important tokens. We have verified the accuracy improvement through A2SF in the OPT and LLaMA models and A2SF improves the accuracy of LLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.

**Link**: [arxiv](http://arxiv.org/abs/2407.20485v2),  [pdf](http://arxiv.org/pdf/2407.20485v2)

**Tags**: cs.CL cs.LG 



### Electric field control of magnetocaloric effect in cylindrical MnAs/PZT   magnetoelectric composite
**Authors**: Abdulkarim A. Amirov, Maksim A. Koliushenkov, Abdula A. Mukhuchev, Dibir M. Yusupov, Valeriya V. Govorina, Dmitriy S. Neznakhin, Gennady A. Govor, Akhmed M. Aliev

**Updated**: 2024-07-30T21:27:00Z

**Summary**: The possibility of electric field control of magnetocaloric effect through quasi-isostatic compression as a result of the converse piezoelectric effect was demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was shown that an electric voltage of 100 V corresponding to an electric field of E ~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the MnAs/PZT composite contributes to an increase in the maximum adiabatic temperature change by 0.2 K in the temperature range of the magnetostructural phase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations using the finite element method have shown that an electric field voltage of 100 V is capable of creating a quasi-isostatic mechanical stress in the region inside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak pressures up to 10 MPa, the contribution to the MCE from piezo compression linearly depends on the electrical voltage that can be used for control the MCE

**Link**: [arxiv](http://arxiv.org/abs/2407.21201v1),  [pdf](http://arxiv.org/pdf/2407.21201v1)

**Tags**: cond-mat.mtrl-sci 



### Palu: Compressing KV-Cache with Low-Rank Projection
**Authors**: Chi-Chih Chang, Wei-Cheng Lin, Chien-Yu Lin, Chong-Yan Chen, Yu-Fang Hu, Pei-Shuo Wang, Ning-Chi Huang, Luis Ceze, Kai-Chiang Wu

**Updated**: 2024-07-30T18:19:38Z

**Summary**: KV-Cache compression methods generally sample a KV-Cache of effectual tokens or quantize it into lower bits. However, these methods cannot exploit the redundancy of the hidden dimension of KV tensors. This paper investigates a unique hidden dimension approach called Palu, a novel KV-Cache compression framework that utilizes low-rank projection. Palu decomposes the linear layers into low-rank matrices, caches the smaller intermediate states, and reconstructs the full keys and values on the fly. To improve accuracy, compression rate, and efficiency, Palu further encompasses (1) a medium-grained low-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a low-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU kernels. Our extensive experiments with popular LLMs show that Palu can compress KV-Cache by more than 91.25% while maintaining a significantly better accuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache quantization methods at a similar or even higher memory usage. When compressing KV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the attention module. Our code is publicly available at https://github.com/shadowpa0327/Palu.

**Link**: [arxiv](http://arxiv.org/abs/2407.21118v1),  [pdf](http://arxiv.org/pdf/2407.21118v1)

**Tags**: cs.AI cs.LG 



### ThinK: Thinner Key Cache by Query-Driven Pruning
**Authors**: Yuhui Xu, Zhanming Jie, Hanze Dong, Lei Wang, Xudong Lu, Aojun Zhou, Amrita Saha, Caiming Xiong, Doyen Sahoo

**Updated**: 2024-07-30T17:59:08Z

**Summary**: Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.

**Link**: [arxiv](http://arxiv.org/abs/2407.21018v1),  [pdf](http://arxiv.org/pdf/2407.21018v1)

**Tags**: cs.CL cs.AI 



### SpChar: Characterizing the Sparse Puzzle via Decision Trees
**Authors**: Francesco Sgherzi, Marco Siracusa, Ivan Fernandez, Adri√† Armejach, Miquel Moret√≥

**Updated**: 2024-07-30T13:06:36Z

**Summary**: Sparse matrix computation is crucial in various modern applications, including large-scale graph analytics, deep learning, and recommender systems. The performance of sparse kernels varies greatly depending on the structure of the input matrix, making it difficult to gain a comprehensive understanding of sparse computation and its relationship to inputs, algorithms, and target machine architecture. Despite extensive research on certain sparse kernels, such as Sparse Matrix-Vector Multiplication (SpMV), the overall family of sparse algorithms has yet to be investigated as a whole. This paper introduces SpChar, a workload characterization methodology for general sparse computation. SpChar employs tree-based models to identify the most relevant hardware and input characteristics, starting from hardware and input-related metrics gathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis enables the creation of a characterization loop that facilitates the optimization of sparse computation by mapping the impact of architectural features to inputs and algorithmic choices. We apply SpChar to more than 600 matrices from the SuiteSparse Matrix collection and three state-of-the-art Arm CPUs to determine the critical hardware and software characteristics that affect sparse computation. In our analysis, we determine that the biggest limiting factors for high-performance sparse computation are (1) the latency of the memory system, (2) the pipeline flush overhead resulting from branch misprediction, and (3) the poor reuse of cached elements. Additionally, we propose software and hardware optimizations that designers can implement to create a platform suitable for sparse computation. We then investigate these optimizations using the gem5 simulator to achieve a significant speedup of up to 2.63x compared to a CPU where the optimizations are not applied.

**Link**: [arxiv](http://arxiv.org/abs/2304.06944v2),  [pdf](http://arxiv.org/pdf/2304.06944v2)

**Tags**: cs.AR B.8.2 



### UpDown: Programmable fine-grained Events for Scalable Performance on   Irregular Applications
**Authors**: Andronicus Rajasukumar, Jiya Su, Yuqing, Wang, Tianshuo Su, Marziyeh Nourian, Jose M Monsalve Diaz, Tianchi Zhang, Jianru Ding, Wenyi Wang, Ziyi Zhang, Moubarak Jeje, Henry Hoffmann, Yanjing Li, Andrew A. Chien

**Updated**: 2024-07-30T12:16:39Z

**Summary**: Applications with irregular data structures, data-dependent control flows and fine-grained data transfers (e.g., real-world graph computations) perform poorly on cache-based systems. We propose the UpDown accelerator that supports fine-grained execution with novel architecture mechanisms - lightweight threading, event-driven scheduling, efficient ultra-short threads, and split-transaction DRAM access with software-controlled synchronization. These hardware primitives support software programmable events, enabling high performance on diverse data structures and algorithms. UpDown also supports scalable performance; hardware replication enables programs to scale up performance. Evaluation results show UpDown's flexibility and scalability enable it to outperform CPUs on graph mining and analytics computations by up to 116-195x geomean speedup and more than 4x speedup over prior accelerators. We show that UpDown generates high memory parallelism (~4.6x over CPU) required for memory intensive graph computations. We present measurements that attribute the performance of UpDown (23x architectural advantage) to its individual architectural mechanisms. Finally, we also analyze the area and power cost of UpDown's mechanisms for software programmability.

**Link**: [arxiv](http://arxiv.org/abs/2407.20773v1),  [pdf](http://arxiv.org/pdf/2407.20773v1)

**Tags**: cs.AR 



### Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models
**Authors**: Eman Ali, Muhammad Haris Khan

**Updated**: 2024-07-30T08:39:52Z

**Summary**: Recent advances in large-scale vision-language models have achieved impressive performance in various zero-shot image classification tasks. While prior studies have demonstrated significant improvements by introducing few-shot labelled target samples, they still require labelling of target samples, which greatly degrades their scalability and generalizability while handling various visual recognition tasks. We design NtUA, a Noise-tolerant Unsupervised Adapter that allows the learning of effective target models with few unlabelled target samples. NtUA works as a key-value cache that formulates visual features and predicted pseudo-labels of the few unlabelled target samples as key-value pairs. It consists of two complementary designs. The first is adaptive cache formation that combats pseudo-label noises by weighting the key-value pairs according to their prediction confidence. The second is knowledge-guided cache refinement, which refines pair values (i.e., pseudo-labels) and cache weights by leveraging knowledge distillation from large-scale vision language models. Extensive experiments show that NtUA achieves superior performance consistently across multiple widely adopted benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2309.14928v3),  [pdf](http://arxiv.org/pdf/2309.14928v3)

**Tags**: cs.CV cs.LG 



### Robust Federated Learning for Wireless Networks: A Demonstration with   Channel Estimation
**Authors**: Zexin Fang, Bin Han, Hans D. Schotten

**Updated**: 2024-07-30T08:19:53Z

**Summary**: Federated learning (FL) offers a privacy-preserving collaborative approach for training models in wireless networks, with channel estimation emerging as a promising application. Despite extensive studies on FL-empowered channel estimation, the security concerns associated with FL require meticulous attention. In a scenario where small base stations (SBSs) serve as local models trained on cached data, and a macro base station (MBS) functions as the global model setting, an attacker can exploit the vulnerability of FL, launching attacks with various adversarial attacks or deployment tactics. In this paper, we analyze such vulnerabilities, corresponding solutions were brought forth, and validated through simulation.

**Link**: [arxiv](http://arxiv.org/abs/2404.03088v2),  [pdf](http://arxiv.org/pdf/2404.03088v2)

**Tags**: cs.LG cs.AI cs.NI eess.SP 



### Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)
**Authors**: Ziyue Qiu, Juncheng Yang, Mor Harchol-Balter

**Updated**: 2024-07-30T04:01:25Z

**Summary**: Software caches are an intrinsic component of almost every computer system. Consequently, caching algorithms, particularly eviction policies, are the topic of many papers. Almost all these prior papers evaluate the caching algorithm based on its hit ratio, namely the fraction of requests that are found in the cache, as opposed to disk. The hit ratio is viewed as a proxy for traditional performance metrics like system throughput or response time. Intuitively it makes sense that higher hit ratio should lead to higher throughput (and lower response time), since more requests are found in the cache (low access time) as opposed to the disk (high access time).   This paper challenges this intuition. We show that increasing the hit ratio can actually hurt the throughput (and response time) for many caching algorithms. Our investigation follows a three-pronged approach involving (i) queueing modeling and analysis, (ii) implementation and measurement, and (iii) simulation to validate the accuracy of the queueing model. We also show that the phenomenon of throughput decreasing at higher hit ratios is likely to be more pronounced in future systems, where the trend is towards faster disks and higher numbers of cores per CPU.

**Link**: [arxiv](http://arxiv.org/abs/2404.16219v3),  [pdf](http://arxiv.org/pdf/2404.16219v3)

**Tags**: cs.PF 



### STT-RAM-based Hierarchical In-Memory Computing
**Authors**: Dhruv Gajaria, Kevin Antony Gomez, Tosiron Adegbija

**Updated**: 2024-07-29T01:43:26Z

**Summary**: In-memory computing promises to overcome the von Neumann bottleneck in computer systems by performing computations directly within the memory. Previous research has suggested using Spin-Transfer Torque RAM (STT-RAM) for in-memory computing due to its non-volatility, low leakage power, high density, endurance, and commercial viability. This paper explores hierarchical in-memory computing, where different levels of the memory hierarchy are augmented with processing elements to optimize workload execution. The paper investigates processing in memory (PiM) using non-volatile STT-RAM and processing in cache (PiC) using volatile STT-RAM with relaxed retention, which helps mitigate STT-RAM's write latency and energy overheads. We analyze tradeoffs and overheads associated with data movement for PiC versus write overheads for PiM using STT-RAMs for various workloads. We examine workload characteristics, such as computational intensity and CPU-dependent workloads with limited instruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using these workloads, we evaluate computing in STT-RAM versus SRAM at different cache hierarchy levels and explore the potential of heterogeneous STT-RAM cache architectures with various retention times for PiC and CPU-based computing. Our experiments reveal significant advantages of STT-RAM-based PiC over PiM for specific workloads. Finally, we describe open research problems in hierarchical in-memory computing architectures to further enhance this paradigm.

**Link**: [arxiv](http://arxiv.org/abs/2407.19637v1),  [pdf](http://arxiv.org/pdf/2407.19637v1)

**Tags**: cs.CY cs.AR 



### CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory   Processing
**Authors**: Dhruv Gajaria, Tosiron Adegbija, Kevin Gomez

**Updated**: 2024-07-29T01:17:54Z

**Summary**: Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures, especially those utilizing bit-line computing, offer promising solutions to mitigate data movement bottlenecks within the memory hierarchy. While previous studies have explored the integration of compute units within individual memory levels, the complexity and potential overheads associated with these designs have often limited their capabilities. This paper introduces a novel PiC/PiM architecture, Concurrent Hierarchical In-Memory Processing (CHIME), which strategically incorporates heterogeneous compute units across multiple levels of the memory hierarchy. This design targets the efficient execution of diverse, domain-specific workloads by placing computations closest to the data where it optimizes performance, energy consumption, data movement costs, and area. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing, such as high density, low leakage, and better resiliency to data corruption from activating multiple word lines. We demonstrate that CHIME enhances concurrency and improves compute unit utilization at each level of the memory hierarchy. We present strategies for exploring the design space, grouping, and placing the compute units across the memory hierarchy. Experiments reveal that, compared to the state-of-the-art bit-line computing approaches, CHIME achieves significant speedup and energy savings of 57.95% and 78.23% for various domain-specific workloads, while reducing the overheads associated with single-level compute designs.

**Link**: [arxiv](http://arxiv.org/abs/2407.19627v1),  [pdf](http://arxiv.org/pdf/2407.19627v1)

**Tags**: cs.CY 



### ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient   Multicore Processors
**Authors**: Dhruv Gajaria, Tosiron Adegbija

**Updated**: 2024-07-28T23:43:59Z

**Summary**: Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been widely studied as a way to reduce STT-RAM's write energy and latency overheads. Given a relaxed retention time STT-RAM level one (L1) cache, we analyze the impacts of dynamic voltage and frequency scaling (DVFS) -- a common optimization in modern processors -- on STT-RAM L1 cache design. Our analysis reveals that, apart from the fact that different applications may require different retention times, the clock frequency, which is typically ignored in most STT-RAM studies, may also significantly impact applications' retention time needs. Based on our findings, we propose an asymmetric-retention core (ARC) design for multicore architectures. ARC features retention time heterogeneity to specialize STT-RAM retention times to applications' needs. We also propose a runtime prediction model to determine the best core on which to run an application, based on the applications' characteristics, their retention time requirements, and available DVFS settings. Results reveal that the proposed approach can reduce the average cache energy by 20.19% and overall processor energy by 7.66%, compared to a homogeneous STT-RAM cache design.

**Link**: [arxiv](http://arxiv.org/abs/2407.19612v1),  [pdf](http://arxiv.org/pdf/2407.19612v1)

**Tags**: cs.CY cs.AR 



## Keyword: LLM Inference 
 ### pop-cosmos: Scaleable inference of galaxy properties and redshifts with   a data-driven population model
**Authors**: Stephen Thorp, Justin Alsing, Hiranya V. Peiris, Sinan Deger, Daniel J. Mortlock, Boris Leistedt, Joel Leja, Arthur Loureiro

**Updated**: 2024-09-04T17:59:53Z

**Summary**: We present an efficient Bayesian method for estimating individual photometric redshifts and galaxy properties under a pre-trained population model (pop-cosmos) that was calibrated using purely photometric data. This model specifies a prior distribution over 16 stellar population synthesis (SPS) parameters using a score-based diffusion model, and includes a data model with detailed treatment of nebular emission. We use a GPU-accelerated affine invariant ensemble sampler to achieve fast posterior sampling under this model for 292,300 individual galaxies in the COSMOS2020 catalog, leveraging a neural network emulator (Speculator) to speed up the SPS calculations. We apply both the pop-cosmos population model and a baseline prior inspired by Prospector-$\alpha$, and compare these results to published COSMOS2020 redshift estimates from the widely-used EAZY and LePhare codes. For the $\sim 12,000$ galaxies with spectroscopic redshifts, we find that pop-cosmos yields redshift estimates that have minimal bias ($\sim10^{-4}$), high accuracy ($\sigma_\text{MAD}=7\times10^{-3}$), and a low outlier rate ($1.6\%$). We show that the pop-cosmos population model generalizes well to galaxies fainter than its $r<25$ mag training set. The sample we have analyzed is $\gtrsim3\times$ larger than has previously been possible via posterior sampling with a full SPS model, with average throughput of 15 GPU-sec per galaxy under the pop-cosmos prior, and 0.6 GPU-sec per galaxy under the Prospector prior. This paves the way for principled modeling of the huge catalogs expected from upcoming Stage IV galaxy surveys.

**Link**: [arxiv](http://arxiv.org/abs/2406.19437v2),  [pdf](http://arxiv.org/pdf/2406.19437v2)

**Tags**: astro-ph.CO astro-ph.GA astro-ph.IM 



### Enhancing Graph Neural Networks with Limited Labeled Data by Actively   Distilling Knowledge from Large Language Models
**Authors**: Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, Suhang Wang

**Updated**: 2024-09-04T17:52:37Z

**Summary**: Graphs are pervasive in the real-world, such as social network analysis, bioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great ability in node classification, a fundamental task on graphs. Unfortunately, conventional GNNs still face challenges in scenarios with few labeled nodes, despite the prevalence of few-shot node classification tasks in real-world applications. To address this challenge, various approaches have been proposed, including graph meta-learning, transfer learning, and methods based on Large Language Models (LLMs). However, traditional meta-learning and transfer learning methods often require prior knowledge from base classes or fail to exploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based methods may overlook the zero-shot capabilities of LLMs and rely heavily on the quality of generated contexts. In this paper, we propose a novel approach that integrates LLMs and GNNs, leveraging the zero-shot inference and reasoning capabilities of LLMs and employing a Graph-LLM-based active learning paradigm to enhance GNNs' performance. Extensive experiments demonstrate the effectiveness of our model in improving node classification accuracy with considerably limited labeled data, surpassing state-of-the-art baselines by significant margins.

**Link**: [arxiv](http://arxiv.org/abs/2407.13989v3),  [pdf](http://arxiv.org/pdf/2407.13989v3)

**Tags**: cs.LG cs.AI 



### Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR
**Authors**: Reinhard Wiesmayr, Sebastian Cammerer, Fay√ßal A√Øt Aoudia, Jakob Hoydis, Jakub Zakrzewski, Alexander Keller

**Updated**: 2024-09-04T17:51:18Z

**Summary**: We detail the steps required to deploy a multi-user multiple-input multiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular communication system. This raises several exciting research challenges, including the need for real-time inference and compatibility with the 5G NR standard. As the network configuration in a practical setup can change dynamically within milliseconds, we propose an adaptive NRX architecture capable of supporting dynamic modulation and coding scheme (MCS) configurations without the need for any re-training and without additional inference cost. We optimize the latency of the neural network (NN) architecture to achieve inference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT inference library. These latency constraints effectively limit the size of the NN and we quantify the resulting signal-to-noise ratio (SNR) degradation as less than 0.7 dB when compared to a preliminary non-real-time NRX architecture. Finally, we explore the potential for site-specific adaptation of the receiver by investigating the required size of the training dataset and the number of fine-tuning iterations to optimize the NRX for specific radio environments using a ray tracing-based channel model. The resulting NRX is ready for deployment in a real-time 5G NR system and the source code including the TensorRT experiments is available online.

**Link**: [arxiv](http://arxiv.org/abs/2409.02912v1),  [pdf](http://arxiv.org/pdf/2409.02912v1)

**Tags**: cs.IT eess.SP math.IT 



### LongCite: Enabling LLMs to Generate Fine-grained Citations in   Long-context QA
**Authors**: Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li

**Updated**: 2024-09-05T03:53:13Z

**Summary**: Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering user questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to their potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations, improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs' performance in Long-Context Question Answering with Citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically generate long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the LongCite-45k dataset, successfully enabling their generation of accurate responses and fine-grained sentence-level citations in a single output. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o.

**Link**: [arxiv](http://arxiv.org/abs/2409.02897v2),  [pdf](http://arxiv.org/pdf/2409.02897v2)

**Tags**: cs.CL 



### Follow the Mass -- A Concordance Picture of Tidal Disruption Events
**Authors**: Julian Krolik, Tsvi Piran, Taeho Ryu

**Updated**: 2024-09-04T17:38:13Z

**Summary**: Three recent global simulations of tidal disruption events (TDEs) have produced, using different numerical techniques and parameters, very similar pictures of their dynamics. In typical TDEs, after the star is disrupted by a supermassive black hole, the bound portion of the stellar debris follows highly eccentric trajectories, reaching apocenters of several thousand gravitational radii. Only a very small fraction is captured upon returning to the vicinity of the supermassive black hole. Nearly all the debris returns to the apocenter, where shocks produce a thick irregular cloud on this radial scale and power the optical/UV flare. These simulation results imply that over a few years, the thick cloud settles into an accretion flow responsible for the long term emission. Despite not being designed to match observations, the dynamical picture given by the three simulations aligns well with observations of typical events, correctly predicting the flares' total radiated energy, luminosity, temperature and emission line width. On the basis of these predictions, we provide an updated method ({\sc TDEmass}) to infer the stellar and black hole masses from a flare's peak luminosity and temperature. This picture also correctly predicts the luminosity observed years after the flare. In addition, we show that in a magnitude-limited survey, if the intrinsic rate of TDEs is independent of black hole mass, the detected events will preferentially have black hole masses $\sim 10^{6 \pm 0.3} M_\odot$ and stellar masses of $\sim 1-1.5 M_\odot$.

**Link**: [arxiv](http://arxiv.org/abs/2409.02894v1),  [pdf](http://arxiv.org/pdf/2409.02894v1)

**Tags**: astro-ph.HE astro-ph.SR 



### LADDER: Language Driven Slice Discovery and Error Rectification
**Authors**: Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Kayhan Batmanghelich

**Updated**: 2024-09-04T17:31:00Z

**Summary**: Error slice discovery associates structured patterns with model errors. Existing methods discover error slices by clustering the error-prone samples with similar patterns or assigning discrete attributes to each sample for post-hoc analysis. While these methods aim for interpretability and easier mitigation through reweighting or rebalancing, they may not capture the full complexity of error patterns due to incomplete or missing attributes. Contrary to the existing approach, this paper utilizes the reasoning capabilities of the Large Language Model (LLM) to analyze complex error patterns and generate testable hypotheses. This paper proposes LADDER: Language Driven slice Discovery and Error Rectification. It first projects the model's representation into a language-aligned feature space (eg CLIP) to preserve semantics in the original model feature space. This ensures the accurate retrieval of sentences that highlight the model's errors. Next, the LLM utilizes the sentences and generates hypotheses to discover error slices. Finally, we mitigate the error by fine-tuning the classification head by creating a group-balanced dataset using the hypotheses. Our entire method does not require any attribute annotation, either explicitly or through external tagging models. We validate our method with \textbf{five} image classification datasets. The code is available (https://github.com/batmanlab/Ladder).

**Link**: [arxiv](http://arxiv.org/abs/2408.07832v3),  [pdf](http://arxiv.org/pdf/2408.07832v3)

**Tags**: cs.CL cs.CV 



### LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via   Hybrid Architecture
**Authors**: Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang

**Updated**: 2024-09-04T17:25:21Z

**Summary**: Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as \textit{degraded performance with more images} and \textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks.

**Link**: [arxiv](http://arxiv.org/abs/2409.02889v1),  [pdf](http://arxiv.org/pdf/2409.02889v1)

**Tags**: cs.CL cs.AI cs.CV cs.MM 



### Kinetic Interacting Particle Langevin Monte Carlo
**Authors**: Paul Felix Valsecchi Oliva, O. Deniz Akyildiz

**Updated**: 2024-09-04T17:23:39Z

**Summary**: This paper introduces and analyses interacting underdamped Langevin algorithms, termed Kinetic Interacting Particle Langevin Monte Carlo (KIPLMC) methods, for statistical inference in latent variable models. We propose a diffusion process that evolves jointly in the space of parameters and latent variables and exploit the fact that the stationary distribution of this diffusion concentrates around the maximum marginal likelihood estimate of the parameters. We then provide two explicit discretisations of this diffusion as practical algorithms to estimate parameters of statistical models. For each algorithm, we obtain nonasymptotic rates of convergence for the case where the joint log-likelihood is strongly concave with respect to latent variables and parameters. In particular, we provide convergence analysis for the diffusion together with the discretisation error, providing convergence rate estimates for the algorithms in Wasserstein-2 distance. We achieve accelerated convergence rates clearly demonstrating improvement in dimension dependence, similar to the underdamped samplers. To demonstrate the utility of the introduced methodology, we provide numerical experiments that demonstrate the effectiveness of the proposed diffusion for statistical inference and the stability of the numerical integrators utilised for discretisation. Our setting covers a broad number of applications, including unsupervised learning, statistical inference, and inverse problems.

**Link**: [arxiv](http://arxiv.org/abs/2407.05790v2),  [pdf](http://arxiv.org/pdf/2407.05790v2)

**Tags**: stat.CO stat.ML 



### The Need for Guardrails with Large Language Models in Medical   Safety-Critical Settings: An Artificial Intelligence Application in the   Pharmacovigilance Ecosystem
**Authors**: Joe B Hakim, Jeffery L Painter, Darmendra Ramcharran, Vijay Kara, Greg Powell, Paulina Sobczak, Chiho Sato, Andrew Bate, Andrew Beam

**Updated**: 2024-09-04T17:16:05Z

**Summary**: Large language models (LLMs) are useful tools with the capacity for performing specific types of knowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical domains pose unique challenges, notably the issue of ``hallucination,'' where LLMs can generate fabricated information. This is particularly concerning in settings such as drug safety, where inaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety, and potentially applicable to other medical safety-critical contexts. These guardrails include mechanisms to detect anomalous documents to prevent the ingestion of inappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty in generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, which involves converting both structured and unstructured data within adverse event reports into natural language. This method was applied to translate individual case safety reports, demonstrating effective application in a pharmacovigilance processing task. Our guardrail framework offers a set of tools with broad applicability across various domains, ensuring LLMs can be safely used in high-risk situations by eliminating the occurrence of key errors, including the generation of incorrect pharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in medical safety-critical environments.

**Link**: [arxiv](http://arxiv.org/abs/2407.18322v2),  [pdf](http://arxiv.org/pdf/2407.18322v2)

**Tags**: cs.CL cs.AI cs.CY cs.LG I.2.1; I.2.7; I.7.1 



### Hyft: A Reconfigurable Softmax Accelerator with Hybrid Numeric Format   for both Training and Inference
**Authors**: Tianhua Xia, Sai Qian Zhang

**Updated**: 2024-09-04T17:11:04Z

**Summary**: The attention mechanism is a pivotal element within the transformer architecture, making a substantial contribution to its exceptional performance. Within this attention mechanism, Softmax is an imperative component that enables the model to assess the degree of correlation between various segments of the input. Yet, prior research has shown that Softmax operations can significantly increase processing latency and energy consumption in the transformer network due to their internal nonlinear operations and data dependencies. In this work, we proposed Hyft, a hardware efficient floating point Softmax accelerator for both training and inference. Hyft aims to reduce the implementation cost of different nonlinear arithmetic operations within softmax by adaptively converting intermediate results into the most suitable numeric format for each specific operation, leading to reconfigurable accelerator with hybrid numeric format. The evaluation results highlight that Hyft achieves a remarkable 10x reduction in hardware resource utilization and a 6x reduction in processing latency, all while maintaining a negligible impact on transformer accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2311.13290v2),  [pdf](http://arxiv.org/pdf/2311.13290v2)

**Tags**: cs.AR 



### Configurable Foundation Models: Building LLMs from a Modular Perspective
**Authors**: Chaojun Xiao, Zhengyan Zhang, Chenyang Song, Dazhi Jiang, Feng Yao, Xu Han, Xiaozhi Wang, Shuo Wang, Yufei Huang, Guanyu Lin, Yingfa Chen, Weilin Zhao, Yuge Tu, Zexuan Zhong, Ao Zhang, Chenglei Si, Khai Hao Moo, Chenyang Zhao, Huimin Chen, Yankai Lin, Zhiyuan Liu, Jingbo Shang, Maosong Sun

**Updated**: 2024-09-04T17:01:02Z

**Summary**: Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.

**Link**: [arxiv](http://arxiv.org/abs/2409.02877v1),  [pdf](http://arxiv.org/pdf/2409.02877v1)

**Tags**: cs.AI cs.CL cs.LG 



### $Œº$GUIDE: a framework for quantitative imaging via generalized   uncertainty-driven inference using deep learning
**Authors**: Ma√´liss Jallais, Marco Palombo

**Updated**: 2024-09-04T16:59:27Z

**Summary**: This work proposes $\mu$GUIDE: a general Bayesian framework to estimate posterior distributions of tissue microstructure parameters from any given biophysical model or MRI signal representation, with exemplar demonstration in diffusion-weighted MRI. Harnessing a new deep learning architecture for automatic signal feature selection combined with simulation-based inference and efficient sampling of the posterior distributions, $\mu$GUIDE bypasses the high computational and time cost of conventional Bayesian approaches and does not rely on acquisition constraints to define model-specific summary statistics. The obtained posterior distributions allow to highlight degeneracies present in the model definition and quantify the uncertainty and ambiguity of the estimated parameters.

**Link**: [arxiv](http://arxiv.org/abs/2312.17293v4),  [pdf](http://arxiv.org/pdf/2312.17293v4)

**Tags**: eess.IV cs.LG physics.med-ph 



### Automating Pharmacovigilance Evidence Generation: Using Large Language   Models to Produce Context-Aware SQL
**Authors**: Jeffery L. Painter, Venkateswara Rao Chalamalasetti, Raymond Kassekert, Andrew Bate

**Updated**: 2024-09-04T16:58:25Z

**Summary**: Objective: To enhance the efficiency and accuracy of information retrieval from pharmacovigilance (PV) databases by employing Large Language Models (LLMs) to convert natural language queries (NLQs) into Structured Query Language (SQL) queries, leveraging a business context document.   Materials and Methods: We utilized OpenAI's GPT-4 model within a retrieval-augmented generation (RAG) framework, enriched with a business context document, to transform NLQs into syntactically precise SQL queries. Each NLQ was presented to the LLM randomly and independently to prevent memorization. The study was conducted in three phases, varying query complexity, and assessing the LLM's performance both with and without the business context document.   Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing from 8.3\% with the database schema alone to 78.3\% with the business context document. This enhancement was consistent across low, medium, and high complexity queries, indicating the critical role of contextual knowledge in query generation.   Discussion: The integration of a business context document markedly improved the LLM's ability to generate accurate and contextually relevant SQL queries. Performance achieved a maximum of 85\% when high complexity queries are excluded, suggesting promise for routine deployment.   Conclusion: This study presents a novel approach to employing LLMs for safety data retrieval and analysis, demonstrating significant advancements in query generation accuracy. The methodology offers a framework applicable to various data-intensive domains, enhancing the accessibility and efficiency of information retrieval for non-technical users.

**Link**: [arxiv](http://arxiv.org/abs/2406.10690v3),  [pdf](http://arxiv.org/pdf/2406.10690v3)

**Tags**: cs.AI cs.DB H.3.3; I.2.7 



### Four-dimensional phase space tomography from one-dimensional   measurements in a high-power hadron ring
**Authors**: Austin Hoover

**Updated**: 2024-09-04T16:41:56Z

**Summary**: In this paper, we use one-dimensional measurements to infer the four-dimensional phase space density of an accumulated 1 GeV proton beam in the Spallation Neutron Source (SNS) accelerator. The reconstruction was performed using MENT, an exact maximum-entropy tomography algorithm, and thus represents the most reasonable inference from the data. The reconstructed distribution reproduces the measured profiles with the same dynamic range as the measurement devices, and simulations indicate that the problem is well-constrained. Similar measurements could serve as benchmarks for simulations of intense, coupled beam dynamics in the SNS or other hadron rings.

**Link**: [arxiv](http://arxiv.org/abs/2409.02862v1),  [pdf](http://arxiv.org/pdf/2409.02862v1)

**Tags**: physics.acc-ph 



### The Design of an LLM-powered Unstructured Analytics System
**Authors**: Eric Anderson, Jonathan Fritz, Austin Lee, Bohou Li, Mark Lindblad, Henry Lindeman, Alex Meyer, Parth Parmar, Tanvi Ranade, Mehul A. Shah, Benjamin Sowell, Dan Tecuci, Vinayak Thapliyal, Matt Welsh

**Updated**: 2024-09-04T16:39:22Z

**Summary**: LLMs demonstrate an uncanny ability to process unstructured data, and as such, have the potential to go beyond search and run complex, semantic analyses at scale. We describe the design of an unstructured analytics system, Aryn, and the tenets and use cases that motivate its design. With Aryn, users can specify queries in natural language and the system automatically determines a semantic plan and executes it to compute an answer from a large collection of unstructured documents using LLMs. At the core of Aryn is Sycamore, a declarative document processing engine, built using Ray, that provides a reliable distributed abstraction called DocSets. Sycamore allows users to analyze, enrich, and transform complex documents at scale. Aryn also comprises Luna, a query planner that translates natural language queries to Sycamore scripts, and the Aryn Partitioner, which takes raw PDFs and document images, and converts them to DocSets for downstream processing. Using Aryn, we demonstrate a real world use case for analyzing accident reports from the National Transportation Safety Board (NTSB), and discuss some of the major challenges we encountered in deploying Aryn in the wild.

**Link**: [arxiv](http://arxiv.org/abs/2409.00847v2),  [pdf](http://arxiv.org/pdf/2409.00847v2)

**Tags**: cs.DB cs.AI cs.IR 



### Domain Decomposition-based coupling of Operator Inference reduced order   models via the Schwarz alternating method
**Authors**: Ian Moore, Christopher Wentland, Anthony Gruber, Irina Tezaur

**Updated**: 2024-09-04T16:28:36Z

**Summary**: This paper presents and evaluates an approach for coupling together subdomain-local reduced order models (ROMs) constructed via non-intrusive operator inference (OpInf) with each other and with subdomain-local full order models (FOMs), following a domain decomposition of the spatial geometry on which a given partial differential equation (PDE) is posed. Joining subdomain-local models is accomplished using the overlapping Schwarz alternating method, a minimally-intrusive multiscale coupling technique that works by transforming a monolithic problem into a sequence of subdomain-local problems, which communicate through transmission boundary conditions imposed on the subdomain interfaces. After formulating the overlapping Schwarz alternating method for OpInf ROMs, termed OpInf-Schwarz, we evaluate the method's accuracy and efficiency on several test cases involving the heat equation in two spatial dimensions. We demonstrate that the method is capable of coupling together arbitrary combinations of OpInf ROMs and FOMs, and that speed-ups over a monolithic FOM are possible when performing OpInf ROM coupling.

**Link**: [arxiv](http://arxiv.org/abs/2409.01433v2),  [pdf](http://arxiv.org/pdf/2409.01433v2)

**Tags**: math.NA cs.LG cs.NA math-ph math.MP 



### Simple and Scalable Strategies to Continually Pre-train Large Language   Models
**Authors**: Adam Ibrahim, Benjamin Th√©rien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timoth√©e Lesort, Eugene Belilovsky, Irina Rish

**Updated**: 2024-09-04T16:13:18Z

**Summary**: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.

**Link**: [arxiv](http://arxiv.org/abs/2403.08763v4),  [pdf](http://arxiv.org/pdf/2403.08763v4)

**Tags**: cs.LG cs.AI cs.CL 



### CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the   Mathematics Reasoning of Large Multimodal Models
**Authors**: Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, Liang He

**Updated**: 2024-09-04T16:00:21Z

**Summary**: Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.

**Link**: [arxiv](http://arxiv.org/abs/2409.02834v1),  [pdf](http://arxiv.org/pdf/2409.02834v1)

**Tags**: cs.CL 



### LongRecipe: Recipe for Efficient Long Context Generalization in Large   Language Models
**Authors**: Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh Tuan Luu, See-Kiong Ng, Zhiwei Jiang, Bryan Hooi

**Updated**: 2024-09-04T15:55:22Z

**Summary**: Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive. To address this, we introduce LongRecipe, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model's understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks. Ultimately, we can extend the effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory. Our code is released at https://github.com/zhiyuanhubj/LongRecipe.

**Link**: [arxiv](http://arxiv.org/abs/2409.00509v2),  [pdf](http://arxiv.org/pdf/2409.00509v2)

**Tags**: cs.CL 



### Group Difference in Differences can Identify Effect Heterogeneity in   Non-Canonical Settings
**Authors**: Zach Shahn, Laura Hatfield

**Updated**: 2024-09-04T15:52:03Z

**Summary**: Consider a very general setting in which data on an outcome of interest is collected in two `groups' at two time periods, with certain group-periods deemed `treated' and others `untreated'. A special case is the canonical Difference-in-Differences (DiD) setting in which one group is treated only in the second period while the other is treated in neither period. Then it is well known that under a parallel trends assumption across the two groups the classic DiD formula (subtracting the average change in the outcome across periods in the treated group by the average change in the outcome across periods in the untreated group) identifies the average treatment effect on the treated in the second period. But other relations between group, period, and treatment are possible. For example, the groups might be demographic (or other baseline covariate) categories with all units in both groups treated in the second period and none treated in the first, i.e. a pre-post design. Or one group might be treated in both periods while the other is treated in neither. In these non-canonical settings (lacking a control group or a pre-period), some researchers still compute DiD estimators, while others avoid causal inference altogether. In this paper, we will elucidate the group-period-treatment scenarios and corresponding parallel trends assumptions under which a DiD formula identifies meaningful causal estimands and what those causal estimands are. We find that in non-canonical settings, under a group parallel trends assumption the DiD formula identifies effect heterogeneity in the treated across groups or across time periods (depending on the setting).

**Link**: [arxiv](http://arxiv.org/abs/2408.16039v2),  [pdf](http://arxiv.org/pdf/2408.16039v2)

**Tags**: stat.ME 



### Design Contradictions: Help or Hindrance?
**Authors**: Aron E. Owen, Jonathan C. Roberts

**Updated**: 2024-09-04T15:42:59Z

**Summary**: The need for innovative ideas in data visualisation drives us to explore new creative approaches. Combining two or more creative words, particularly those that contradict each other, can positively impact the creative process, sparking novel ideas and designs. As we move towards AI-driven design, an open question arises: do these design contradictions work positively with AI tools? Currently, the answer is no. AI systems, like large language models (LLMs), rely on algorithms that engender similarity, whereas creativity often requires divergence and novelty. This poster initiates a conversation on how to drive AI systems to be more creative and generate new ideas. This research invites us to reconsider traditional design methods and explore new approaches in an AI-driven world. Can we apply the same techniques used in traditional design, like the double diamond model, or do we need new methods for design engineering? How can we quickly design visualisations and craft new ideas with generative AI? This paper seeks to start this critical conversation and offers practical insights into the potential of AI in driving creativity in data visualisation.

**Link**: [arxiv](http://arxiv.org/abs/2409.02823v1),  [pdf](http://arxiv.org/pdf/2409.02823v1)

**Tags**: cs.HC 



### Language Understanding as a Constraint on Consensus Size in LLM   Societies
**Authors**: Giordano De Marzo, Claudio Castellano, David Garcia

**Updated**: 2024-09-04T15:42:29Z

**Summary**: The applications of Large Language Models (LLMs) are going towards collaborative tasks where several agents interact with each other like in an LLM society. In such a setting, large groups of LLMs could reach consensus about arbitrary norms for which there is no information supporting one option over another, regulating their own behavior in a self-organized way. In human societies, the ability to reach consensus without institutions has a limit in the cognitive capacities of humans. To understand if a similar phenomenon characterizes also LLMs, we apply methods from complexity science and principles from behavioral sciences in a new approach of AI anthropology. We find that LLMs are able to reach consensus in groups and that the opinion dynamics of LLMs can be understood with a function parametrized by a majority force coefficient that determines whether consensus is possible. This majority force is stronger for models with higher language understanding capabilities and decreases for larger groups, leading to a critical group size beyond which, for a given LLM, consensus is unfeasible. This critical group size grows exponentially with the language understanding capabilities of models and for the most advanced models, it can reach an order of magnitude beyond the typical size of informal human groups.

**Link**: [arxiv](http://arxiv.org/abs/2409.02822v1),  [pdf](http://arxiv.org/pdf/2409.02822v1)

**Tags**: physics.soc-ph 



### The Future of Open Human Feedback
**Authors**: Shachar Don-Yehiya, Ben Burtenshaw, Ramon Fernandez Astudillo, Cailean Osborne, Mimansa Jaiswal, Tzu-Sheng Kuo, Wenting Zhao, Idan Shenfeld, Andi Peng, Mikhail Yurochkin, Atoosa Kasirzadeh, Yangsibo Huang, Tatsunori Hashimoto, Yacine Jernite, Daniel Vila-Suero, Omri Abend, Jennifer Ding, Sara Hooker, Hannah Rose Kirk, Leshem Choshen

**Updated**: 2024-09-04T15:39:47Z

**Summary**: Human feedback on conversations with language language models (LLMs) is central to how these systems learn about the world, improve their capabilities, and are steered toward desirable and safe behaviors. However, this feedback is mostly collected by frontier AI labs and kept behind closed doors. In this work, we bring together interdisciplinary experts to assess the opportunities and challenges to realizing an open ecosystem of human feedback for AI. We first look for successful practices in peer production, open source, and citizen science communities. We then characterize the main challenges for open human feedback. For each, we survey current approaches and offer recommendations. We end by envisioning the components needed to underpin a sustainable and open human feedback ecosystem. In the center of this ecosystem are mutually beneficial feedback loops, between users and specialized models, incentivizing a diverse stakeholders community of model trainers and feedback providers to support a general open feedback pool.

**Link**: [arxiv](http://arxiv.org/abs/2408.16961v2),  [pdf](http://arxiv.org/pdf/2408.16961v2)

**Tags**: cs.HC cs.AI 



### Bayesian Analysis of Generalized Hierarchical Indian Buffet Processes   for Within and Across Group Sharing of Latent Features
**Authors**: Lancelot Fitzgerald James, Juho Lee, Abhinav Pandey

**Updated**: 2024-09-04T15:37:09Z

**Summary**: Bayesian nonparametric hierarchical priors are highly effective in providing flexible models for latent data structures exhibiting sharing of information within and across groups. In this work, we focus on latent feature allocation models, where the data structures correspond to multi-sets or unbounded sparse matrices, which we refer to as generalized hierarchical Indian Buffet processes (HIBP). These are based on hierarchical versions of generalized spike and slab Indian Buffet processes (IBP), where the fundamental development in this regard is the Bernoulli-based HIBP, devised by Thibaux-Jordan (2007), as a hierarchical extension of the IBP devised by Griffiths-Ghahramani (2005). With a focus on Bayesian inference, we provide novel explicit descriptions of the joint, marginal, and posterior distributions of the HIBP, significantly advancing our understanding of these processes. Our results allow for exact sampling for the otherwise complex joint marginal distributions. We provide a general characterization of their posterior distributions as well as highlight bottlenecks for practical implementation. Our main focus then shifts to specific tractable results for the remarkable case of Poisson HIBP, which correspond to generalizations of mixed Poisson random count models arising in genetics, imaging, topic modeling, random occupancy, and species sampling models. We show they also have important relations to Bayesian nonparametric latent class models appearing in the literature. Furthermore, we show that all general HIBP may be coupled to Poisson HIBP, allowing for further analysis of such processes.

**Link**: [arxiv](http://arxiv.org/abs/2304.05244v2),  [pdf](http://arxiv.org/pdf/2304.05244v2)

**Tags**: math.ST stat.TH 60C05, 60G09, 60G57, 60E99 



### Obsidian: Cooperative State-Space Exploration for Performant Inference   on Secure ML Accelerators
**Authors**: Sarbartha Banerjee, Shijia Wei, Prakash Ramrakhyani, Mohit Tiwari

**Updated**: 2024-09-04T15:35:18Z

**Summary**: Trusted execution environments (TEEs) for machine learning accelerators are indispensable in secure and efficient ML inference. Optimizing workloads through state-space exploration for the accelerator architectures improves performance and energy consumption. However, such explorations are expensive and slow due to the large search space. Current research has to use fast analytical models that forego critical hardware details and cross-layer opportunities unique to the hardware security primitives. While cycle-accurate models can theoretically reach better designs, their high runtime cost restricts them to a smaller state space.   We present Obsidian, an optimization framework for finding the optimal mapping from ML kernels to a secure ML accelerator. Obsidian addresses the above challenge by exploring the state space using analytical and cycle-accurate models cooperatively. The two main exploration components include: (1) A secure accelerator analytical model, that includes the effect of secure hardware while traversing the large mapping state space and produce the best m model mappings; (2) A compiler profiling step on a cycle-accurate model, that captures runtime bottlenecks to further improve execution runtime, energy and resource utilization and find the optimal model mapping.   We compare our results to a baseline secure accelerator, comprising of the state-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11]. The analytical model reduces the inference latency by 20.5% for a cloud and 8.4% for an edge deployment with an energy improvement of 24% and 19% respectively. The cycle-accurate model, further reduces the latency by 9.1% for a cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%.

**Link**: [arxiv](http://arxiv.org/abs/2409.02817v1),  [pdf](http://arxiv.org/pdf/2409.02817v1)

**Tags**: cs.CR cs.LG 



### LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language   Models
**Authors**: Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang

**Updated**: 2024-09-05T10:30:39Z

**Summary**: Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce LogicGame, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment of rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. LogicGame defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing LogicGame, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.

**Link**: [arxiv](http://arxiv.org/abs/2408.15778v3),  [pdf](http://arxiv.org/pdf/2408.15778v3)

**Tags**: cs.AI cs.CL 



### Towards a Unified View of Preference Learning for Large Language Models:   A Survey
**Authors**: Bofei Gao, Feifan Song, Yibo Miao, Zefan Cai, Zhe Yang, Liang Chen, Helan Hu, Runxin Xu, Qingxiu Dong, Ce Zheng, Wen Xiao, Ge Zhang, Daoguang Zan, Keming Lu, Bowen Yu, Dayiheng Liu, Zeyu Cui, Jian Yang, Lei Sha, Houfeng Wang, Zhifang Sui, Peiyi Wang, Tianyu Liu, Baobao Chang

**Updated**: 2024-09-04T15:11:55Z

**Summary**: Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to efficiently enhance the LLM's performance. While effective, research in this area spans multiple domains, and the methods involved are relatively complex to understand. The relationships between different methods have been under-explored, limiting the development of the preference alignment. In light of this, we break down the existing popular alignment strategies into different components and provide a unified framework to study the current alignment strategies, thereby establishing connections among them. In this survey, we decompose all the strategies in preference learning into four components: model, data, feedback, and algorithm. This unified view offers an in-depth understanding of existing alignment algorithms and also opens up possibilities to synergize the strengths of different strategies. Furthermore, we present detailed working examples of prevalent existing algorithms to facilitate a comprehensive understanding for the readers. Finally, based on our unified perspective, we explore the challenges and future research directions for aligning large language models with human preferences.

**Link**: [arxiv](http://arxiv.org/abs/2409.02795v1),  [pdf](http://arxiv.org/pdf/2409.02795v1)

**Tags**: cs.CL 



### Negation Blindness in Large Language Models: Unveiling the NO Syndrome   in Image Generation
**Authors**: Mohammad Nadeem, Shahab Saquib Sohail, Erik Cambria, Bj√∂rn W. Schuller, Amir Hussain

**Updated**: 2024-09-04T14:40:14Z

**Summary**: Foundational Large Language Models (LLMs) have changed the way we perceive technology. They have been shown to excel in tasks ranging from poem writing and coding to essay generation and puzzle solving. With the incorporation of image generation capability, they have become more comprehensive and versatile AI tools. At the same time, researchers are striving to identify the limitations of these tools to improve them further. Currently identified flaws include hallucination, biases, and bypassing restricted commands to generate harmful content. In the present work, we have identified a fundamental limitation related to the image generation ability of LLMs, and termed it The NO Syndrome. This negation blindness refers to LLMs inability to correctly comprehend NO related natural language prompts to generate the desired images. Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found to be suffering from this syndrome. To demonstrate the generalization of this limitation, we carried out simulation experiments and conducted entropy-based and benchmark statistical analysis tests on various LLMs in multiple languages, including English, Hindi, and French. We conclude that the NO syndrome is a significant flaw in current LLMs that needs to be addressed. A related finding of this study showed a consistent discrepancy between image and textual responses as a result of this NO syndrome. We posit that the introduction of a negation context-aware reinforcement learning based feedback loop between the LLMs textual response and generated image could help ensure the generated text is based on both the LLMs correct contextual understanding of the negation query and the generated visual output.

**Link**: [arxiv](http://arxiv.org/abs/2409.00105v2),  [pdf](http://arxiv.org/pdf/2409.00105v2)

**Tags**: cs.CL cs.AI cs.LG 



### Federated Quantum-Train with Batched Parameter Generation
**Authors**: Chen-Yu Liu, Samuel Yen-Chi Chen

**Updated**: 2024-09-04T14:39:11Z

**Summary**: In this work, we introduce the Federated Quantum-Train (QT) framework, which integrates the QT model into federated learning to leverage quantum computing for distributed learning systems. Quantum client nodes employ Quantum Neural Networks (QNNs) and a mapping model to generate local target model parameters, which are updated and aggregated at a central node. Testing with a VGG-like convolutional neural network on the CIFAR-10 dataset, our approach significantly reduces qubit usage from 19 to as low as 8 qubits while reducing generalization error. The QT method mitigates overfitting observed in classical models, aligning training and testing accuracy and improving performance in highly compressed models. Notably, the Federated QT framework does not require a quantum computer during inference, enhancing practicality given current quantum hardware limitations. This work highlights the potential of integrating quantum techniques into federated learning, paving the way for advancements in quantum machine learning and distributed learning systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.02763v1),  [pdf](http://arxiv.org/pdf/2409.02763v1)

**Tags**: quant-ph 



### A Comparative Study of Pre-training and Self-training
**Authors**: Yiheng Wang, Jiayu Lin, Zuoquan Lin

**Updated**: 2024-09-04T14:30:13Z

**Summary**: Pre-training and self-training are two approaches to semi-supervised learning. The comparison between pre-training and self-training has been explored. However, the previous works led to confusing findings: self-training outperforms pre-training experienced on some tasks in computer vision, and contrarily, pre-training outperforms self-training experienced on some tasks in natural language processing, under certain conditions of incomparable settings. We propose, comparatively and exhaustively, an ensemble method to empirical study all feasible training paradigms combining pre-training, self-training, and fine-tuning within consistent foundational settings comparable to data augmentation. We conduct experiments on six datasets, four data augmentation, and imbalanced data for sentiment analysis and natural language inference tasks. Our findings confirm that the pre-training and fine-tuning paradigm yields the best overall performances. Moreover, self-training offers no additional benefits when combined with semi-supervised pre-training.

**Link**: [arxiv](http://arxiv.org/abs/2409.02751v1),  [pdf](http://arxiv.org/pdf/2409.02751v1)

**Tags**: cs.CL 



### Self-intercalation as origin of high-temperature ferromagnetism in   epitaxially grown Fe5GeTe2 thin films
**Authors**: M. Silinskas, S. Senz, P. Gargiani, A. M. Ruiz, B. Kalkofen, I. Kostanovskiy, K. Mohseni, J. J. Baldov√≠, H. L. Meyerheim, S. S. P. Parkin, A. Bedoya-Pinto

**Updated**: 2024-09-04T14:20:21Z

**Summary**: The role of self-intercalation in 2D van der Waals materials is key to the understanding of many of their properties. Here we show that the magnetic ordering temperature of thin films of the 2D ferromagnet Fe5GeTe2 is substantially increased by self-intercalated Fe that resides in the van der Waals gaps. The epitaxial films were prepared by molecular beam epitaxy and their magnetic properties explored by element-specific x-ray magnetic circular dichroism that showed ferromagnetic ordering up to 375 K. Both surface and bulk sensitive x-ray absorption modes were used to confirm that the magnetic signal is of an intrinsic nature. Fe occupation within the van der Waals gap was determined by x-ray diffraction which showed a notably higher occupation with respect to bulk crystals. We thus infer, supported by first-principles calculations, that the higher magnetic ordering temperature results from an increased exchange interaction between the individual Fe5GeTe2 layers mediated by Fe atoms residing within the van der Waals gaps. Our findings establish self-intercalation during epitaxial growth as an efficient mechanism to achieve high-temperature magnetism in a broad class of van der Waals materials.

**Link**: [arxiv](http://arxiv.org/abs/2309.17439v4),  [pdf](http://arxiv.org/pdf/2309.17439v4)

**Tags**: cond-mat.mtrl-sci 



### The future of cosmological likelihood-based inference: accelerated   high-dimensional parameter estimation and model comparison
**Authors**: Davide Piras, Alicja Polanska, Alessio Spurio Mancini, Matthew A. Price, Jason D. McEwen

**Updated**: 2024-09-04T14:10:43Z

**Summary**: We advocate for a new paradigm of cosmological likelihood-based inference, leveraging recent developments in machine learning and its underlying technology, to accelerate Bayesian inference in high-dimensional settings. Specifically, we combine (i) emulation, where a machine learning model is trained to mimic cosmological observables, e.g. CosmoPower-JAX; (ii) differentiable and probabilistic programming, e.g. JAX and NumPyro, respectively; (iii) scalable Markov chain Monte Carlo (MCMC) sampling techniques that exploit gradients, e.g. Hamiltonian Monte Carlo; and (iv) decoupled and scalable Bayesian model selection techniques that compute the Bayesian evidence purely from posterior samples, e.g. the learned harmonic mean implemented in harmonic. This paradigm allows us to carry out a complete Bayesian analysis, including both parameter estimation and model selection, in a fraction of the time of traditional approaches. First, we demonstrate the application of this paradigm on a simulated cosmic shear analysis for a Stage IV survey in 37- and 39-dimensional parameter spaces, comparing $\Lambda$CDM and a dynamical dark energy model ($w_0w_a$CDM). We recover posterior contours and evidence estimates that are in excellent agreement with those computed by the traditional nested sampling approach while reducing the computational cost from 8 months on 48 CPU cores to 2 days on 12 GPUs. Second, we consider a joint analysis between three simulated next-generation surveys, each performing a 3x2pt analysis, resulting in 157- and 159-dimensional parameter spaces. Standard nested sampling techniques are simply unlikely to be feasible in this high-dimensional setting, requiring a projected 12 years of compute time on 48 CPU cores; on the other hand, the proposed approach only requires 8 days of compute time on 24 GPUs. All packages used in our analyses are publicly available.

**Link**: [arxiv](http://arxiv.org/abs/2405.12965v2),  [pdf](http://arxiv.org/pdf/2405.12965v2)

**Tags**: astro-ph.CO astro-ph.IM cs.LG 



### Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality   Norms
**Authors**: Joshua Ashkinaze, Ruijia Guan, Laura Kurek, Eytan Adar, Ceren Budak, Eric Gilbert

**Updated**: 2024-09-04T14:07:07Z

**Summary**: Large language models (LLMs) are trained on broad corpora and then used in communities with specialized norms. Is providing LLMs with community rules enough for models to follow these norms? We evaluate LLMs' capacity to detect (Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's Neutral Point of View (NPOV) policy. LLMs struggled with bias detection, achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting biases (some under- and others over-predicted bias), suggesting distinct priors about neutrality. LLMs performed better at generation, removing 79% of words removed by Wikipedia editors. However, LLMs made additional changes beyond Wikipedia editors' simpler neutralizations, resulting in high-recall but low-precision editing. Interestingly, crowdworkers rated AI rewrites as more neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia editors but often made extraneous non-NPOV-related changes (such as grammar). LLMs may apply rules in ways that resonate with the public but diverge from community experts. While potentially effective for generation, LLMs may reduce editor agency and increase moderation workload (e.g., verifying additions). Even when rules are easy to articulate, having LLMs apply them like community members may still be difficult.

**Link**: [arxiv](http://arxiv.org/abs/2407.04183v2),  [pdf](http://arxiv.org/pdf/2407.04183v2)

**Tags**: cs.CL cs.AI cs.CY cs.HC 



### Pooling And Attention: What Are Effective Designs For LLM-Based   Embedding Models?
**Authors**: Yixuan Tang, Yi Yang

**Updated**: 2024-09-05T07:17:59Z

**Summary**: The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.

**Link**: [arxiv](http://arxiv.org/abs/2409.02727v2),  [pdf](http://arxiv.org/pdf/2409.02727v2)

**Tags**: cs.CL cs.IR 



### Alignment-Aware Model Extraction Attacks on Large Language Models
**Authors**: Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu

**Updated**: 2024-09-04T13:54:38Z

**Summary**: Model extraction attacks (MEAs) on large language models (LLMs) have received increasing research attention lately. Existing attack methods on LLMs inherit the extraction strategies from those designed for deep neural networks (DNNs) yet neglect the inconsistency of training tasks between MEA and LLMs' alignments. As such, they result in poor attack performances. To tackle this issue, we present Locality Reinforced Distillation (LoRD), a novel model extraction attack algorithm specifically for LLMs. In particular, we design a policy-gradient-style training task, which utilizes victim models' responses as a signal to guide the crafting of preference for the local model. Theoretical analysis has shown that i) LoRD's convergence procedure in MEAs is consistent with the alignments of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions demonstrate the superiority of our method by examining the extraction of various state-of-the-art commercial LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.02718v1),  [pdf](http://arxiv.org/pdf/2409.02718v1)

**Tags**: cs.CR cs.CL 



### Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for   PostNL
**Authors**: Mohammad Reshadati

**Updated**: 2024-09-04T13:49:19Z

**Summary**: The developments in the field of generative AI has brought a lot of opportunities for companies, for instance to improve efficiency in customer service and automating tasks. PostNL, the biggest parcel and E-commerce corporation of the Netherlands wants to use generative AI to enhance the communication around track and trace of parcels. During the internship a Minimal Viable Product (MVP) is created to showcase the value of using generative AI technologies, to enhance parcel tracking, analyzing the parcel's journey and being able to communicate about it in an easy to understand manner. The primary goal was to develop an in-house LLM-based system, reducing dependency on external platforms and establishing the feasibility of a dedicated generative AI team within the company. This multi-agent LLM based system aimed to construct parcel journey stories and identify logistical disruptions with heightened efficiency and accuracy. The research involved deploying a sophisticated AI-driven communication system, employing Retrieval-Augmented Generation (RAG) for enhanced response precision, and optimizing large language models (LLMs) tailored to domain specific tasks.   The MVP successfully implemented a multi-agent open-source LLM system, called SuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of user inquiries and improving internal knowledge handling. Results and evaluation demonstrated technological innovation and feasibility, notably in communication about the track and trace of a parcel, which exceeded initial expectations. These advancements highlight the potential of AI-driven solutions in logistics, suggesting many opportunities for further refinement and broader implementation within PostNL operational framework.

**Link**: [arxiv](http://arxiv.org/abs/2409.02711v1),  [pdf](http://arxiv.org/pdf/2409.02711v1)

**Tags**: cs.AI 



### A family of toroidal diffusions with exact likelihood inference
**Authors**: Eduardo Garc√≠a-Portugu√©s, Michael S√∏rensen

**Updated**: 2024-09-04T13:41:46Z

**Summary**: We provide a class of diffusion processes for continuous time-varying multivariate angular data with explicit transition probability densities, enabling exact likelihood inference. The presented diffusions are time-reversible and can be constructed for any pre-specified stationary distribution on the torus, including highly-multimodal mixtures. We give results on asymptotic likelihood theory allowing one-sample inference and tests of linear hypotheses for $k$ groups of diffusions, including homogeneity. We show that exact and direct diffusion bridge simulation is possible too. A class of circular jump processes with similar properties is also proposed. Several numerical experiments illustrate the methodology for the circular and two-dimensional torus cases. The new family of diffusions is applied (i) to test several homogeneity hypotheses on the movement of ants and (ii) to simulate bridges between the three-dimensional backbones of two related proteins.

**Link**: [arxiv](http://arxiv.org/abs/2409.02705v1),  [pdf](http://arxiv.org/pdf/2409.02705v1)

**Tags**: stat.ME 60J60, 62H11, 62M02 



### Decision Transformer for Enhancing Neural Local Search on the Job Shop   Scheduling Problem
**Authors**: Constantin Waubert de Puiseau, Fabian Wolz, Merlin Montag, Jannik Peters, Hasan Tercan, Tobias Meisen

**Updated**: 2024-09-04T13:33:38Z

**Summary**: The job shop scheduling problem (JSSP) and its solution algorithms have been of enduring interest in both academia and industry for decades. In recent years, machine learning (ML) is playing an increasingly important role in advancing existing and building new heuristic solutions for the JSSP, aiming to find better solutions in shorter computation times. In this paper we build on top of a state-of-the-art deep reinforcement learning (DRL) agent, called Neural Local Search (NLS), which can efficiently and effectively control a large local neighborhood search on the JSSP. In particular, we develop a method for training the decision transformer (DT) algorithm on search trajectories taken by a trained NLS agent to further improve upon the learned decision-making sequences. Our experiments show that the DT successfully learns local search strategies that are different and, in many cases, more effective than those of the NLS agent itself. In terms of the tradeoff between solution quality and acceptable computational time needed for the search, the DT is particularly superior in application scenarios where longer computational times are acceptable. In this case, it makes up for the longer inference times required per search step, which are caused by the larger neural network architecture, through better quality decisions per step. Thereby, the DT achieves state-of-the-art results for solving the JSSP with ML-enhanced search.

**Link**: [arxiv](http://arxiv.org/abs/2409.02697v1),  [pdf](http://arxiv.org/pdf/2409.02697v1)

**Tags**: cs.AI cs.LG 



### A possible late-time transition of $M_B$ inferred via neural networks
**Authors**: Purba Mukherjee, Konstantinos F. Dialektopoulos, Jackson Levi Said, Jurgen Mifsud

**Updated**: 2024-09-04T13:31:31Z

**Summary**: The strengthening of tensions in the cosmological parameters has led to a reconsideration of fundamental aspects of standard cosmology. The tension in the Hubble constant can also be viewed as a tension between local and early Universe constraints on the absolute magnitude $M_B$ of Type Ia supernova. In this work, we reconsider the possibility of a variation of this parameter in a model-independent way. We employ neural networks to agnostically constrain the value of the absolute magnitude as well as assess the impact and statistical significance of a variation in $M_B$ with redshift from the Pantheon+ compilation, together with a thorough analysis of the neural network architecture. We find an indication for a possible transition redshift at the $z\approx 1$ region.

**Link**: [arxiv](http://arxiv.org/abs/2402.10502v2),  [pdf](http://arxiv.org/pdf/2402.10502v2)

**Tags**: astro-ph.CO cs.LG gr-qc 



### A Causal Explainable Guardrails for Large Language Models
**Authors**: Zhixuan Chu, Yan Wang, Longfei Li, Zhibo Wang, Zhan Qin, Kui Ren

**Updated**: 2024-09-04T13:29:56Z

**Summary**: Large Language Models (LLMs) have shown impressive performance in natural language tasks, but their outputs can exhibit undesirable attributes or biases. Existing methods for steering LLMs toward desired attributes often assume unbiased representations and rely solely on steering prompts. However, the representations learned from pre-training can introduce semantic biases that influence the steering process, leading to suboptimal results. We propose LLMGuardrail, a novel framework that incorporates causal analysis and adversarial learning to obtain unbiased steering representations in LLMs. LLMGuardrail systematically identifies and blocks the confounding effects of biases, enabling the extraction of unbiased steering representations. Additionally, it includes an explainable component that provides insights into the alignment between the generated output and the desired direction. Experiments demonstrate LLMGuardrail's effectiveness in steering LLMs toward desired attributes while mitigating biases. Our work contributes to the development of safe and reliable LLMs that align with desired attributes.

**Link**: [arxiv](http://arxiv.org/abs/2405.04160v2),  [pdf](http://arxiv.org/pdf/2405.04160v2)

**Tags**: cs.CL 



### Optimal combination of composite likelihoods using approximate Bayesian   computation with application to state-space models
**Authors**: Wentao Li, Rosabeth White, Dennis Prangle

**Updated**: 2024-09-04T13:25:00Z

**Summary**: Composite likelihood provides approximate inference when the full likelihood is intractable and sub-likelihood functions of marginal events can be evaluated relatively easily. It has been successfully applied for many complex models. However, its wider application is limited by two issues. First, weight selection of marginal likelihood can have a significant impact on the information efficiency and is currently an open question. Second, calibrated Bayesian inference with composite likelihood requires curvature adjustment which is difficult for dependent data. This work shows that approximate Bayesian computation (ABC) can properly address these two issues by using multiple composite score functions as summary statistics. First, the summary-based posterior distribution gives the optimal Godambe information among a wide class of estimators defined by linear combinations of estimating functions. Second, to make ABC computationally feasible for models where marginal likelihoods have no closed form, a novel approach is proposed to estimate all simulated marginal scores using a Monte Carlo sample with size N. Sufficient conditions are given for the additional noise to be negligible with N fixed as the data size n goes to infinity, and the computational cost is O(n). Third, asymptotic properties of ABC with summary statistics having heterogeneous convergence rates is derived, and an adaptive scheme to choose the component composite scores is proposed. Numerical studies show that the new method significantly outperforms the existing Bayesian composite likelihood methods, and the efficiency of adaptively combined composite scores well approximates the efficiency of particle MCMC using the full likelihood.

**Link**: [arxiv](http://arxiv.org/abs/2404.02313v2),  [pdf](http://arxiv.org/pdf/2404.02313v2)

**Tags**: stat.ME stat.CO 



### LLM-Assisted Visual Analytics: Opportunities and Challenges
**Authors**: Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Pranava Madhyastha

**Updated**: 2024-09-04T13:24:03Z

**Summary**: We explore the integration of large language models (LLMs) into visual analytics (VA) systems to transform their capabilities through intuitive natural language interactions. We survey current research directions in this emerging field, examining how LLMs are integrated into data management, language interaction, visualisation generation, and language generation processes. We highlight the new possibilities that LLMs bring to VA, especially how they can change VA processes beyond the usual use cases. We especially highlight building new visualisation-language models, allowing access of a breadth of domain knowledge, multimodal interaction, and opportunities with guidance. Finally, we carefully consider the prominent challenges of using current LLMs in VA tasks. Our discussions in this paper aim to guide future researchers working on LLM-assisted VA systems and help them navigate common obstacles when developing these systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.02691v1),  [pdf](http://arxiv.org/pdf/2409.02691v1)

**Tags**: cs.HC cs.AI 



### Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for   Problem-Solving Improvement of LLMs
**Authors**: Ruoyu Wang, Xiaoxuan Li, Lina Yao

**Updated**: 2024-09-04T13:17:09Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but recent studies reveal that these models often fail to achieve satisfactory results on questions involving reasoning, such as mathematics or physics questions. This phenomenon is usually attributed to the uncertainty regarding whether these models could genuinely comprehend the knowledge embedded in the text or merely learn to replicate the token distribution without a true understanding of the content. In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs. First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level. Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems we observe in the visualization. Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions. Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods. This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.02686v1),  [pdf](http://arxiv.org/pdf/2409.02686v1)

**Tags**: cs.CL cs.AI cs.LG 



### Parallel Speculative Decoding with Adaptive Draft Length
**Authors**: Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu

**Updated**: 2024-09-04T13:14:57Z

**Summary**: Speculative decoding (SD), where an extra draft model is employed to provide multiple \textit{draft} tokens first and then the original target model verifies these tokens in parallel, has shown great power for LLM inference acceleration. However, existing SD methods suffer from the mutual waiting problem, i.e., the target model gets stuck when the draft model is \textit{guessing} tokens, and vice versa. This problem is directly incurred by the asynchronous execution of the draft model and the target model, and is exacerbated due to the fixed draft length in speculative decoding. To address these challenges, we propose a conceptually simple, flexible, and general framework to boost speculative decoding, namely \textbf{P}arallel sp\textbf{E}culative decoding with \textbf{A}daptive d\textbf{R}aft \textbf{L}ength (PEARL). Specifically, PEARL proposes \textit{pre-verify} to verify the first draft token in advance during the drafting phase, and \textit{post-verify} to generate more draft tokens during the verification phase. PEARL parallels the drafting phase and the verification phase via applying the two strategies, and achieves adaptive draft length for different scenarios, which effectively alleviates the mutual waiting problem. Moreover, we theoretically demonstrate that the mean accepted tokens of PEARL is more than existing \textit{draft-then-verify} works. Experiments on various text generation benchmarks demonstrate the effectiveness of our \name, leading to a superior speedup performance up to \textbf{3.79$\times$} and \textbf{1.52$\times$}, compared to auto-regressive decoding and vanilla speculative decoding, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.11850v2),  [pdf](http://arxiv.org/pdf/2408.11850v2)

**Tags**: cs.CL 



### Symmetries and synchronization from whole-neural activity in {\it C.   elegans} connectome: Integration of functional and structural networks
**Authors**: Bryant Avila, Pedro Augusto, David Phillips, Tommaso Gili, Manuel Zimmer, Hern√°n A. Makse

**Updated**: 2024-09-04T13:12:58Z

**Summary**: Understanding the dynamical behavior of complex systems from their underlying network architectures is a long-standing question in complexity theory. Therefore, many metrics have been devised to extract network features like motifs, centrality, and modularity measures. It has previously been proposed that network symmetries are of particular importance since they are expected to underly the synchronization of a system's units, which is ubiquitously observed in nervous system activity patterns. However, perfectly symmetrical structures are difficult to assess in noisy measurements of biological systems, like neuronal connectomes. Here, we devise a principled method to infer network symmetries from combined connectome and neuronal activity data. Using nervous system-wide population activity recordings of the \textit{C.elegans} backward locomotor system, we infer structures in the connectome called fibration symmetries, which can explain which group of neurons synchronize their activity. Our analysis suggests functional building blocks in the animal's motor periphery, providing new testable hypotheses on how descending interneuron circuits communicate with the motor periphery to control behavior. Our approach opens a new door to exploring the structure-function relations in other complex systems, like the nervous systems of larger animals.

**Link**: [arxiv](http://arxiv.org/abs/2409.02682v1),  [pdf](http://arxiv.org/pdf/2409.02682v1)

**Tags**: q-bio.NC physics.app-ph 



### Nonparametric Bayesian estimation in a multidimensional diffusion model   with high frequency data
**Authors**: Marc Hoffmann, Kolyan Ray

**Updated**: 2024-09-04T13:12:53Z

**Summary**: We consider nonparametric Bayesian inference in a multidimensional diffusion model with reflecting boundary conditions based on discrete high-frequency observations. We prove a general posterior contraction rate theorem in $L^2$-loss, which is applied to Gaussian priors. The resulting posteriors, as well as their posterior means, are shown to converge to the ground truth at the minimax optimal rate over H\"older smoothness classes in any dimension. Of independent interest and as part of our proofs, we show that certain frequentist penalized least squares estimators are also minimax optimal.

**Link**: [arxiv](http://arxiv.org/abs/2211.12267v3),  [pdf](http://arxiv.org/pdf/2211.12267v3)

**Tags**: math.ST math.PR stat.TH 62G20, 62F15, 60J60 



### Improved Single Camera BEV Perception Using Multi-Camera Training
**Authors**: Daniel Busch, Ido Freeman, Richard Meyes, Tobias Meisen

**Updated**: 2024-09-04T13:06:40Z

**Summary**: Bird's Eye View (BEV) map prediction is essential for downstream autonomous driving tasks like trajectory prediction. In the past, this was accomplished through the use of a sophisticated sensor configuration that captured a surround view from multiple cameras. However, in large-scale production, cost efficiency is an optimization goal, so that using fewer cameras becomes more relevant. But the consequence of fewer input images correlates with a performance drop. This raises the problem of developing a BEV perception model that provides a sufficient performance on a low-cost sensor setup. Although, primarily relevant for inference time on production cars, this cost restriction is less problematic on a test vehicle during training. Therefore, the objective of our approach is to reduce the aforementioned performance drop as much as possible using a modern multi-camera surround view model reduced for single-camera inference. The approach includes three features, a modern masking technique, a cyclic Learning Rate (LR) schedule, and a feature reconstruction loss for supervising the transition from six-camera inputs to one-camera input during training. Our method outperforms versions trained strictly with one camera or strictly with six-camera surround view for single-camera inference resulting in reduced hallucination and better quality of the BEV map.

**Link**: [arxiv](http://arxiv.org/abs/2409.02676v1),  [pdf](http://arxiv.org/pdf/2409.02676v1)

**Tags**: cs.CV 



### A variational inference framework for inverse problems
**Authors**: Luca Maestrini, Robert G. Aykroyd, Matt P. Wand

**Updated**: 2024-09-04T13:05:21Z

**Summary**: A framework is presented for fitting inverse problem models via variational Bayes approximations. This methodology guarantees flexibility to statistical model specification for a broad range of applications, good accuracy and reduced model fitting times. The message passing and factor graph fragment approach to variational Bayes that is also described facilitates streamlined implementation of approximate inference algorithms and allows for supple inclusion of numerous response distributions and penalizations into the inverse problem model. Models for one- and two-dimensional response variables are examined and an infrastructure is laid down where efficient algorithm updates based on nullifying weak interactions between variables can also be derived for inverse problems in higher dimensions. An image processing application and a simulation exercise motivated by biomedical problems reveal the computational advantage offered by efficient implementation of variational Bayes over Markov chain Monte Carlo.

**Link**: [arxiv](http://arxiv.org/abs/2103.05909v4),  [pdf](http://arxiv.org/pdf/2103.05909v4)

**Tags**: stat.ME stat.AP stat.ML 



### Preregistration does not improve the transparent evaluation of severity   in Popper's philosophy of science or when deviations are allowed
**Authors**: Mark Rubin

**Updated**: 2024-09-04T12:49:34Z

**Summary**: One justification for preregistering research hypotheses, methods, and analyses is that it improves the transparent evaluation of the severity of hypothesis tests. In this article, I consider two cases in which preregistration does not improve this evaluation. First, I argue that, although preregistration can facilitate the transparent evaluation of severity in Mayo's error statistical philosophy of science, it does not facilitate this evaluation in Popper's theory-centric approach. To illustrate, I show that associated concerns about Type I error rate inflation are only relevant in the error statistical approach and not in a theory-centric approach. Second, I argue that a preregistered test procedure that allows deviations in its implementation does not provide a more transparent evaluation of Mayoian severity than a non-preregistered procedure. In particular, I argue that sample-based validity-enhancing deviations cause an unknown inflation of the test procedure's Type I (familywise) error rate and, consequently, an unknown reduction in its capability to license inferences severely. I conclude that preregistration does not improve the transparent evaluation of severity in Popper's philosophy of science or when deviations are allowed.

**Link**: [arxiv](http://arxiv.org/abs/2408.12347v4),  [pdf](http://arxiv.org/pdf/2408.12347v4)

**Tags**: stat.ME 



### HIRO: Hierarchical Information Retrieval Optimization
**Authors**: Krish Goel, Mahek Chandak

**Updated**: 2024-09-04T12:33:24Z

**Summary**: Retrieval-Augmented Generation (RAG) has revolutionized natural language processing by dynamically integrating external knowledge into Large Language Models (LLMs), addressing their limitation of static training datasets. Recent implementations of RAG leverage hierarchical data structures, which organize documents at various levels of summarization and information density. This complexity, however, can cause LLMs to "choke" on information overload, necessitating more sophisticated querying mechanisms. In this context, we introduce Hierarchical Information Retrieval Optimization (HIRO), a novel querying approach that employs a Depth-First Search (DFS)-based recursive similarity score calculation and branch pruning. This method uniquely minimizes the context delivered to the LLM without informational loss, effectively managing the challenge of excessive data. HIRO's refined approach is validated by a 10.85% improvement in performance on the NarrativeQA dataset.

**Link**: [arxiv](http://arxiv.org/abs/2406.09979v2),  [pdf](http://arxiv.org/pdf/2406.09979v2)

**Tags**: cs.CL cs.AI cs.IR 



### Conformal Prediction in Dynamic Biological Systems
**Authors**: Alberto Portela, Julio R. Banga, Marcos Matabuena

**Updated**: 2024-09-04T12:20:27Z

**Summary**: Uncertainty quantification (UQ) is the process of systematically determining and characterizing the degree of confidence in computational model predictions. In the context of systems biology, especially with dynamic models, UQ is crucial because it addresses the challenges posed by nonlinearity and parameter sensitivity, allowing us to properly understand and extrapolate the behavior of complex biological systems. Here, we focus on dynamic models represented by deterministic nonlinear ordinary differential equations. Many current UQ approaches in this field rely on Bayesian statistical methods. While powerful, these methods often require strong prior specifications and make parametric assumptions that may not always hold in biological systems. Additionally, these methods face challenges in domains where sample sizes are limited, and statistical inference becomes constrained, with computational speed being a bottleneck in large models of biological systems. As an alternative, we propose the use of conformal inference methods, introducing two novel algorithms that, in some instances, offer non-asymptotic guarantees, enhancing robustness and scalability across various applications. We demonstrate the efficacy of our proposed algorithms through several scenarios, highlighting their advantages over traditional Bayesian approaches. The proposed methods show promising results for diverse biological data structures and scenarios, offering a general framework to quantify uncertainty for dynamic models of biological systems.The software for the methodology and the reproduction of the results is available at https://zenodo.org/doi/10.5281/zenodo.13644870.

**Link**: [arxiv](http://arxiv.org/abs/2409.02644v1),  [pdf](http://arxiv.org/pdf/2409.02644v1)

**Tags**: stat.ML cs.LG q-bio.QM 



### Graph Retrieval Augmented Trustworthiness Reasoning
**Authors**: Ying Zhu, Shengchang Li, Ziqian Kong, Peilan Xu

**Updated**: 2024-09-04T12:00:25Z

**Summary**: Trustworthiness reasoning is crucial in multiplayer games with incomplete information, enabling agents to identify potential allies and adversaries, thereby enhancing reasoning and decision-making processes. Traditional approaches relying on pre-trained models necessitate extensive domain-specific data and considerable reward feedback, with their lack of real-time adaptability hindering their effectiveness in dynamic environments. In this paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework, leveraging the Retrieval-Augmented Generation (RAG) technique to bolster trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness graph, updating it in real-time with evidential information, and retrieves relevant trust data to augment the reasoning capabilities of Large Language Models (LLMs). We validate our approach through experiments on the multiplayer game "Werewolf," comparing GRATR against baseline LLM and LLM enhanced with Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the baseline methods by over 30\% in winning rate, with superior reasoning performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as identity and objective amnesia, and crucially, it renders the reasoning process more transparent and traceable through the use of the trustworthiness graph.

**Link**: [arxiv](http://arxiv.org/abs/2408.12333v2),  [pdf](http://arxiv.org/pdf/2408.12333v2)

**Tags**: cs.AI 



### Mamba as a motion encoder for robotic imitation learning
**Authors**: Toshiaki Tsuji

**Updated**: 2024-09-04T11:59:53Z

**Summary**: Recent advancements in imitation learning, particularly with the integration of LLM techniques, are set to significantly improve robots' dexterity and adaptability. In this study, we propose using Mamba, a state-of-the-art architecture with potential applications in LLMs, for robotic imitation learning, highlighting its ability to function as an encoder that effectively captures contextual information. By reducing the dimensionality of the state space, Mamba operates similarly to an autoencoder. It effectively compresses the sequential information into state variables while preserving the essential temporal dynamics necessary for accurate motion prediction. Experimental results in tasks such as cup placing and case loading demonstrate that despite exhibiting higher estimation errors, Mamba achieves superior success rates compared to Transformers in practical task execution. This performance is attributed to Mamba's structure, which encompasses the state space model. Additionally, the study investigates Mamba's capacity to serve as a real-time motion generator with a limited amount of training data.

**Link**: [arxiv](http://arxiv.org/abs/2409.02636v1),  [pdf](http://arxiv.org/pdf/2409.02636v1)

**Tags**: cs.RO cs.SY eess.SY 



### Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion   Dependency
**Authors**: Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng

**Updated**: 2024-09-05T09:11:25Z

**Summary**: With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2409.02634v2),  [pdf](http://arxiv.org/pdf/2409.02634v2)

**Tags**: cs.CV 



### EIGER VI. The Correlation Function, Host Halo Mass and Duty Cycle of   Luminous Quasars at $z\gtrsim6$
**Authors**: Anna-Christina Eilers, Ruari Mackenzie, Elia Pizzati, Jorryt Matthee, Joseph F. Hennawi, Haowen Zhang, Rongmon Bordoloi, Daichi Kashino, Simon J. Lilly, Rohan P. Naidu, Robert A. Simcoe, Minghao Yue, Carlos S. Frenk, John C. Helly, Matthieu Schaller, Joop Schaye

**Updated**: 2024-09-04T11:50:23Z

**Summary**: We expect luminous ($M_{1450}\lesssim-26.5$) high-redshift quasars to trace the highest density peaks in the early universe. Here, we present observations of four $z\gtrsim6$ quasar fields using JWST/NIRCam in imaging and widefield slitless spectroscopy mode and report a wide range in the number of detected [OIII]-emitting galaxies in the quasars' environments, ranging between a density enhancement of $\delta\approx65$ within a $2$ cMpc radius - one of the largest proto-clusters during the Epoch of Reionization discovered to date - to a density contrast consistent with zero, indicating the presence of a UV-luminous quasar in a region comparable to the average density of the universe. By measuring the two-point cross-correlation function of quasars and their surrounding galaxies, as well as the galaxy auto-correlation function, we infer a correlation length of quasars at $\langle z\rangle=6.25$ of $r_0^{\rm QQ}=22.0^{+3.0}_{-2.9}~{\rm cMpc}\,h^{-1}$, while we obtain a correlation length of the [OIII]-emitting galaxies of $r_0^{\rm GG}=4.1\pm0.3~{\rm cMpc}\,h^{-1}$. By comparing the correlation functions to dark-matter-only simulations we estimate the minimum mass of the quasars' host dark matter halos to be $\log_{10}(M_{\rm halo, min}/M_\odot)=12.43^{+0.13}_{-0.15}$ (and $\log_{10}(M_{\rm halo, min}^{\rm [OIII]}/M_\odot) = 10.56^{+0.05}_{-0.03}$ for the [OIII]-emitters), indicating that (a) luminous quasars do not necessarily reside within the most overdense regions in the early universe, and that (b) the UV-luminous duty cycle of quasar activity at these redshifts is $f_{\rm duty}\ll1$. Such short quasar activity timescales challenge our understanding of early supermassive black hole growth and provide evidence for highly dust-obscured growth phases or episodic, radiatively inefficient accretion rates.

**Link**: [arxiv](http://arxiv.org/abs/2403.07986v2),  [pdf](http://arxiv.org/pdf/2403.07986v2)

**Tags**: astro-ph.GA astro-ph.CO 



### Large Language Models for Information Retrieval: A Survey
**Authors**: Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, Ji-Rong Wen

**Updated**: 2024-09-04T11:39:56Z

**Summary**: As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.

**Link**: [arxiv](http://arxiv.org/abs/2308.07107v4),  [pdf](http://arxiv.org/pdf/2308.07107v4)

**Tags**: cs.CL cs.IR 



### Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?
**Authors**: Marcel Hallgarten, Julian Zapata, Martin Stoll, Katrin Renz, Andreas Zell

**Updated**: 2024-09-04T11:34:33Z

**Summary**: Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios. Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop). In particular, nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios. This makes it difficult to judge a planner's capabilities to generalize to rarely-seen situations. Therefore, we propose a novel closed-loop benchmark interPlan containing several edge cases and challenging driving scenarios. We assess existing state-of-the-art planners on our benchmark and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios. A recently evolving direction is the usage of foundation models like large language models (LLM) to handle generalization. We evaluate an LLM-only planner and introduce a novel hybrid planner that combines an LLM-based behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2404.07569v2),  [pdf](http://arxiv.org/pdf/2404.07569v2)

**Tags**: cs.RO cs.AI cs.LG 



### PUB: Plot Understanding Benchmark and Dataset for Evaluating Large   Language Models on Synthetic Visual Data Interpretation
**Authors**: Aneta Pawelec, Victoria Sara Weso≈Çowska, Zuzanna BƒÖczek, Piotr Sankowski

**Updated**: 2024-09-04T11:19:17Z

**Summary**: The ability of large language models (LLMs) to interpret visual representations of data is crucial for advancing their application in data analysis and decision-making processes. This paper presents a novel synthetic dataset designed to evaluate the proficiency of LLMs in interpreting various forms of data visualizations, including plots like time series, histograms, violins, boxplots, and clusters. Our dataset is generated using controlled parameters to ensure comprehensive coverage of potential real-world scenarios. We employ multimodal text prompts with questions related to visual data in images to benchmark several state-of-the-art models like ChatGPT or Gemini, assessing their understanding and interpretative accuracy.   To ensure data integrity, our benchmark dataset is generated automatically, making it entirely new and free from prior exposure to the models being tested. This strategy allows us to evaluate the models' ability to truly interpret and understand the data, eliminating possibility of pre-learned responses, and allowing for an unbiased evaluation of the models' capabilities. We also introduce quantitative metrics to assess the performance of the models, providing a robust and comprehensive evaluation tool.   Benchmarking several state-of-the-art LLMs with this dataset reveals varying degrees of success, highlighting specific strengths and weaknesses in interpreting diverse types of visual data. The results provide valuable insights into the current capabilities of LLMs and identify key areas for improvement. This work establishes a foundational benchmark for future research and development aimed at enhancing the visual interpretative abilities of language models. In the future, improved LLMs with robust visual interpretation skills can significantly aid in automated data analysis, scientific research, educational tools, and business intelligence applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.02617v1),  [pdf](http://arxiv.org/pdf/2409.02617v1)

**Tags**: cs.CL 



### Unveiling the Vulnerability of Private Fine-Tuning in Split-Based   Frameworks for Large Language Models: A Bidirectionally Enhanced Attack
**Authors**: Guanzhong Chen, Zhenghan Qin, Mingxin Yang, Yajie Zhou, Tao Fan, Tianyu Du, Zenglin Xu

**Updated**: 2024-09-04T10:58:26Z

**Summary**: Recent advancements in pre-trained large language models (LLMs) have significantly influenced various domains. Adapting these models for specific tasks often involves fine-tuning (FT) with private, domain-specific data. However, privacy concerns keep this data undisclosed, and the computational demands for deploying LLMs pose challenges for resource-limited data holders. This has sparked interest in split learning (SL), a Model-as-a-Service (MaaS) paradigm that divides LLMs into smaller segments for distributed training and deployment, transmitting only intermediate activations instead of raw data. SL has garnered substantial interest in both industry and academia as it aims to balance user data privacy, model ownership, and resource challenges in the private fine-tuning of LLMs. Despite its privacy claims, this paper reveals significant vulnerabilities arising from the combination of SL and LLM-FT: the Not-too-far property of fine-tuning and the auto-regressive nature of LLMs. Exploiting these vulnerabilities, we propose Bidirectional Semi-white-box Reconstruction (BiSR), the first data reconstruction attack (DRA) designed to target both the forward and backward propagation processes of SL. BiSR utilizes pre-trained weights as prior knowledge, combining a learning-based attack with a bidirectional optimization-based approach for highly effective data reconstruction. Additionally, it incorporates a Noise-adaptive Mixture of Experts (NaMoE) model to enhance reconstruction performance under perturbation. We conducted systematic experiments on various mainstream LLMs and different setups, empirically demonstrating BiSR's state-of-the-art performance. Furthermore, we thoroughly examined three representative defense mechanisms, showcasing our method's capability to reconstruct private data even in the presence of these defenses.

**Link**: [arxiv](http://arxiv.org/abs/2409.00960v2),  [pdf](http://arxiv.org/pdf/2409.00960v2)

**Tags**: cs.CR K.6.5 



### Tensor product algorithms for inference of contact network from   epidemiological data
**Authors**: Sergey Dolgov, Dmitry Savostyanov

**Updated**: 2024-09-04T10:42:36Z

**Summary**: We consider a problem of inferring contact network from nodal states observed during an epidemiological process. In a black--box Bayesian optimisation framework this problem reduces to a discrete likelihood optimisation over the set of possible networks. The cardinality of this set grows combinatorially with the number of network nodes, which makes this optimisation computationally challenging. For each network, its likelihood is the probability for the observed data to appear during the evolution of the epidemiological process on this network. This probability can be very small, particularly if the network is significantly different from the ground truth network, from which the observed data actually appear. A commonly used stochastic simulation algorithm struggles to recover rare events and hence to estimate small probabilities and likelihoods. In this paper we replace the stochastic simulation with solving the chemical master equation for the probabilities of all network states. Since this equation also suffers from the curse of dimensionality, we apply tensor train approximations to overcome it and enable fast and accurate computations. Numerical simulations demonstrate efficient black--box Bayesian inference of the network.

**Link**: [arxiv](http://arxiv.org/abs/2401.15031v2),  [pdf](http://arxiv.org/pdf/2401.15031v2)

**Tags**: stat.CO cs.NA math.NA math.PR physics.soc-ph 15A69, 34A30, 37N25, 60J28, 62F15, 65F55, 90B15, 95C42 



### Hypothesizing Missing Causal Variables with LLMs
**Authors**: Ivaxi Sheth, Sahar Abdelnabi, Mario Fritz

**Updated**: 2024-09-04T10:37:44Z

**Summary**: Scientific discovery is a catalyst for human intellectual advances, driven by the cycle of hypothesis generation, experimental design, data evaluation, and iterative assumption refinement. This process, while crucial, is expensive and heavily dependent on the domain knowledge of scientists to generate hypotheses and navigate the scientific cycle. Central to this is causality, the ability to establish the relationship between the cause and the effect. Motivated by the scientific discovery process, in this work, we formulate a novel task where the input is a partial causal graph with missing variables, and the output is a hypothesis about the missing variables to complete the partial graph. We design a benchmark with varying difficulty levels and knowledge assumptions about the causal graph. With the growing interest in using Large Language Models (LLMs) to assist in scientific discovery, we benchmark open-source and closed models on our testbed. We show the strong ability of LLMs to hypothesize the mediation variables between a cause and its effect. In contrast, they underperform in hypothesizing the cause and effect variables themselves. We also observe surprising results where some of the open-source models outperform the closed GPT-4 model.

**Link**: [arxiv](http://arxiv.org/abs/2409.02604v1),  [pdf](http://arxiv.org/pdf/2409.02604v1)

**Tags**: cs.LG stat.ME 



### ChatGPT vs Social Surveys: Probing the Objective and Subjective Human   Society
**Authors**: Muzhi Zhou, Lu Yu, Xiaomin Geng, Lan Luo

**Updated**: 2024-09-04T10:33:37Z

**Summary**: The extent to which Large Language Models (LLMs) can simulate the data-generating process for social surveys remains unclear. Current research has not thoroughly assessed potential biases in the sociodemographic population represented within the language model's framework. Additionally, the subjective worlds of LLMs often show inconsistencies in how closely their responses match those of groups of human respondents. In this paper, we used ChatGPT-3.5 to simulate the sampling process and generated six socioeconomic characteristics from the 2020 US population. We also analyzed responses to questions about income inequality and gender roles to explore GPT's subjective attitudes. By using repeated random sampling, we created a sampling distribution to identify the parameters of the GPT-generated population and compared these with Census data. Our findings show some alignment in gender and age means with the actual 2020 US population, but we also found mismatches in the distributions of racial and educational groups. Furthermore, there were significant differences between the distribution of GPT's responses and human self-reported attitudes. While the overall point estimates of GPT's income attitudinal responses seem to align with the mean of the population occasionally, their response distributions follow a normal distribution that diverges from human responses. In terms of gender relations, GPT's answers tend to cluster in the most frequently answered category, demonstrating a deterministic pattern. We conclude by emphasizing the distinct design philosophies of LLMs and social surveys: LLMs aim to predict the most suitable answers, while social surveys seek to reveal the heterogeneity among social groups.

**Link**: [arxiv](http://arxiv.org/abs/2409.02601v1),  [pdf](http://arxiv.org/pdf/2409.02601v1)

**Tags**: cs.CY 



### Rate-Adaptive Generative Semantic Communication Using Conditional   Diffusion Models
**Authors**: Pujing Yang, Guangyi Zhang, Yunlong Cai

**Updated**: 2024-09-04T10:28:05Z

**Summary**: Recent advances in deep learning-based joint source-channel coding (DJSCC) have shown promise for end-to-end semantic image transmission. However, most existing schemes primarily focus on optimizing pixel-wise metrics, which often fail to align with human perception, leading to lower perceptual quality. In this letter, we propose a novel generative DJSCC approach using conditional diffusion models to enhance the perceptual quality of transmitted images. Specifically, by utilizing entropy models, we effectively manage transmission bandwidth based on the estimated entropy of transmitted sym-bols. These symbols are then used at the receiver as conditional information to guide a conditional diffusion decoder in image reconstruction. Our model is built upon the emerging advanced mamba-like linear attention (MLLA) skeleton, which excels in image processing tasks while also offering fast inference speed. Besides, we introduce a multi-stage training strategy to ensure the stability and improve the overall performance of the model. Simulation results demonstrate that our proposed method significantly outperforms existing approaches in terms of perceptual quality.

**Link**: [arxiv](http://arxiv.org/abs/2409.02597v1),  [pdf](http://arxiv.org/pdf/2409.02597v1)

**Tags**: eess.SP 



### Prompt Compression with Context-Aware Sentence Encoding for Fast and   Improved LLM Inference
**Authors**: Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke

**Updated**: 2024-09-04T10:20:59Z

**Summary**: Large language models (LLMs) have triggered a new stream of research focusing on compressing the context length to reduce the computational cost while ensuring the retention of helpful information for LLMs to answer the given question. Token-based removal methods are one of the most prominent approaches in this direction, but risk losing the semantics of the context caused by intermediate token removal, especially under high compression ratios, while also facing challenges in computational efficiency. In this work, we propose context-aware prompt compression (CPC), a sentence-level prompt compression technique where its key innovation is a novel context-aware sentence encoder that provides a relevance score for each sentence for a given question. To train this encoder, we generate a new dataset consisting of questions, positives, and negative pairs where positives are sentences relevant to the question, while negatives are irrelevant context sentences. We train the encoder in a contrastive setup to learn context-aware sentence representations. Our method considerably outperforms prior works on prompt compression on benchmark datasets and is up to 10.93x faster at inference compared to the best token-level compression method. We also find better improvement for shorter length constraints in most benchmarks, showing the effectiveness of our proposed solution in the compression of relevant information in a shorter context. Finally, we release the code and the dataset for quick reproducibility and further development: https://github.com/Workday/cpc.

**Link**: [arxiv](http://arxiv.org/abs/2409.01227v2),  [pdf](http://arxiv.org/pdf/2409.01227v2)

**Tags**: cs.CL cs.LG 



### SparQ Attention: Bandwidth-Efficient LLM Inference
**Authors**: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr

**Updated**: 2024-09-04T10:04:52Z

**Summary**: The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2312.04985v6),  [pdf](http://arxiv.org/pdf/2312.04985v6)

**Tags**: cs.LG 



### AlignGroup: Learning and Aligning Group Consensus with Member   Preferences for Group Recommendation
**Authors**: Jinfeng Xu, Zheyu Chen, Jinze Li, Shuo Yang, Hewei Wang, Edith C. -H. Ngai

**Updated**: 2024-09-04T10:03:09Z

**Summary**: Group activities are important behaviors in human society, providing personalized recommendations for groups is referred to as the group recommendation task. Existing methods can usually be categorized into two strategies to infer group preferences: 1) determining group preferences by aggregating members' personalized preferences, and 2) inferring group consensus by capturing group members' coherent decisions after common compromises. However, the former would suffer from the lack of group-level considerations, and the latter overlooks the fine-grained preferences of individual users. To this end, we propose a novel group recommendation method AlignGroup, which focuses on both group consensus and individual preferences of group members to infer the group decision-making. Specifically, AlignGroup explores group consensus through a well-designed hypergraph neural network that efficiently learns intra- and inter-group relationships. Moreover, AlignGroup innovatively utilizes a self-supervised alignment task to capture fine-grained group decision-making by aligning the group consensus with members' common preferences. Extensive experiments on two real-world datasets validate that our AlignGroup outperforms the state-of-the-art on both the group recommendation task and the user recommendation task, as well as outperforms the efficiency of most baselines.

**Link**: [arxiv](http://arxiv.org/abs/2409.02580v1),  [pdf](http://arxiv.org/pdf/2409.02580v1)

**Tags**: cs.IR cs.AI 



### Advancing Cyber Incident Timeline Analysis Through Rule Based AI and   Large Language Models
**Authors**: Fatma Yasmine Loumachi, Mohamed Chahine Ghanem

**Updated**: 2024-09-04T09:46:33Z

**Summary**: Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital Forensics (DF), focusing primarily on examining and analysing temporal digital artefacts such as timestamps, derived from event logs, file metadata, and other related data to correlate events resulting from cyber incidents and reconstruct their chronological timeline. Traditional tools often struggle to efficiently process the vast volume and variety of data acquired during DF investigations and Incident Response (IR) processes. This paper presents a novel framework, GenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to advance and automate the TA process. Our approach consists of two main stages (1) We use R-BAI to identify and select anomalous digital artefacts based on predefined rules. (2) The selected artefacts are then converted into embeddings for processing by an LLM with the help of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently leverages its capabilities to perform automated TA on the artefacts and predict potential incident scenarios. To validate our framework, we evaluate GenDFIR performance, efficiency, and reliability using various metrics across synthetic cyber incident simulation scenarios. This paper presents a proof of concept, where the findings demonstrate the significant potential of integrating R-BAI and LLMs for TA. This novel approach highlights the power of Generative AI (GenAI), specifically LLMs, and opens new avenues for advanced threat detection and incident reconstruction, representing a significant step forward in the field.

**Link**: [arxiv](http://arxiv.org/abs/2409.02572v1),  [pdf](http://arxiv.org/pdf/2409.02572v1)

**Tags**: cs.CR cs.AI cs.ET cs.LG 



### More is More: Addition Bias in Large Language Models
**Authors**: Luca Santagata, Cristiano De Nobili

**Updated**: 2024-09-04T09:39:07Z

**Summary**: In this paper, we investigate the presence of additive bias in Large Language Models (LLMs), drawing a parallel to the cognitive bias observed in humans where individuals tend to favor additive over subtractive changes. Using a series of controlled experiments, we tested various LLMs, including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, Math$\Sigma$tral, and Llama 3.1, on tasks designed to measure their propensity for additive versus subtractive modifications. Our findings demonstrate a significant preference for additive changes across all tested models. For example, in a palindrome creation task, Llama 3.1 favored adding letters 97.85% of the time over removing them. Similarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick 76.38% of the time rather than remove one. In a text summarization task, Mistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to improve its own or others' writing. These results indicate that, similar to humans, LLMs exhibit a marked additive bias, which might have implications when LLMs are used on a large scale. Addittive bias might increase resource use and environmental impact, leading to higher economic costs due to overconsumption and waste. This bias should be considered in the development and application of LLMs to ensure balanced and efficient problem-solving approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.02569v1),  [pdf](http://arxiv.org/pdf/2409.02569v1)

**Tags**: cs.CL cs.AI cs.CY cs.HC 



### A Sentence is Worth a Thousand Pictures: Can Large Language Models   Understand Hum4n L4ngu4ge and the W0rld behind W0rds?
**Authors**: Evelina Leivada, Gary Marcus, Fritz G√ºnther, Elliot Murphy

**Updated**: 2024-09-04T09:27:05Z

**Summary**: Modern Artificial Intelligence applications show great potential for language-related tasks that rely on next-word prediction. The current generation of Large Language Models (LLMs) have been linked to claims about human-like linguistic performance and their applications are hailed both as a step towards artificial general intelligence and as a major advance in understanding the cognitive, and even neural basis of human language. To assess these claims, first we analyze the contribution of LLMs as theoretically informative representations of a target cognitive system vs. atheoretical mechanistic tools. Second, we evaluate the models' ability to see the bigger picture, through top-down feedback from higher levels of processing, which requires grounding in previous expectations and past world experience. We hypothesize that since models lack grounded cognition, they cannot take advantage of these features and instead solely rely on fixed associations between represented words and word vectors. To assess this, we designed and ran a novel 'leet task' (l33t t4sk), which requires decoding sentences in which letters are systematically replaced by numbers. The results suggest that humans excel in this task whereas models struggle, confirming our hypothesis. We interpret the results by identifying the key abilities that are still missing from the current state of development of these models, which require solutions that go beyond increased system scaling.

**Link**: [arxiv](http://arxiv.org/abs/2308.00109v2),  [pdf](http://arxiv.org/pdf/2308.00109v2)

**Tags**: cs.CL 



### Computational Efficient Width-Wise Early Exiting in Wireless   Communication Systems
**Authors**: Dieter Verbruggen, Hazem Sallouha, Sofie Pollin

**Updated**: 2024-09-04T09:25:21Z

**Summary**: Deep learning (DL) techniques are increasingly pervasive across various domains, including wireless communication, where they extract insights from raw radio signals. However, the computational demands of DL pose significant challenges, particularly in distributed wireless networks like Cell-free networks, where deploying DL models on edge devices becomes hard due to heightened computational loads. These computational loads escalate with larger input sizes, often correlating with improved model performance. To mitigate this challenge, Early Exiting (EE) techniques have been introduced in DL, primarily targeting the depth of the model. This approach enables models to exit during inference based on specified criteria, leveraging entropy measures at intermediate exits. Doing so makes less complex samples exit early, reducing the average computational load and inference time. In our contribution, we propose a novel width-wise exiting strategy for Convolutional Neural Network (CNN)-based architectures. By selectively adjusting the input size, we aim to regulate computational demands effectively. Our approach aims to decrease the average computational load during inference while maintaining performance levels comparable to conventional models. We specifically investigate Modulation Classification, a well-established application of DL in wireless communication. Our experimental results show substantial reductions in computational load, with an average decrease of 26%, and particularly notable reductions of 60% in high-SNR scenarios. Through this work, we present a practical solution for reducing computational demands in deep learning applications, particularly within the domain of wireless communication.

**Link**: [arxiv](http://arxiv.org/abs/2405.03222v2),  [pdf](http://arxiv.org/pdf/2405.03222v2)

**Tags**: eess.SP 



### Theory-agnostic searches for non-gravitational modes in black hole   ringdown
**Authors**: Francesco Crescimbeni, Xisco Jimenez Forteza, Swetha Bhagwat, Julian Westerweck, Paolo Pani

**Updated**: 2024-09-04T08:52:47Z

**Summary**: In any extension of General Relativity (GR), extra fundamental degrees of freedom couple to gravity. Besides deforming GR forecasts in a theory-dependent way, this coupling generically introduces extra modes in the gravitational-wave signal. We propose a novel theory-agnostic test of gravity to search for these nongravitational modes in black hole merger ringdown signals. To leading order in the GR deviations, their frequencies and damping times match those of a test scalar or vector field in a Kerr background, with only amplitudes and phases as free parameters. This test will be highly valuable for future detectors, which will achieve signal-to-noise ratios higher than 100 (and as high as 1000 for space-based detectors such as LISA). Such sensitivity will allow measurement of these modes with amplitude ratios as low as 0.05 for ground-based detectors (and as low as 0.008 for LISA), relative to the fundamental mode, enabling stringent agnostic constraints or detection of scalar/vector modes. By applying this test to GW150914, GW190521, and GW200129, we find that the current evidence for an extra mode is comparable to that for the first gravitational overtone, but its inclusion modifies the inferred remnant spin.

**Link**: [arxiv](http://arxiv.org/abs/2408.08956v2),  [pdf](http://arxiv.org/pdf/2408.08956v2)

**Tags**: gr-qc astro-ph.HE hep-ph 



### Understanding eGFR Trajectories and Kidney Function Decline via Large   Multimodal Models
**Authors**: Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

**Updated**: 2024-09-04T08:44:36Z

**Summary**: The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.

**Link**: [arxiv](http://arxiv.org/abs/2409.02530v1),  [pdf](http://arxiv.org/pdf/2409.02530v1)

**Tags**: cs.LG cs.AI 



### Sample what you cant compress
**Authors**: Vighnesh Birodkar, Gabriel Barcik, James Lyon, Sergey Ioffe, David Minnen, Joshua V. Dillon

**Updated**: 2024-09-04T08:42:42Z

**Summary**: For learned image representations, basic autoencoders often produce blurry results. Reconstruction quality can be improved by incorporating additional penalties such as adversarial (GAN) and perceptual losses. Arguably, these approaches lack a principled interpretation. Concurrently, in generative settings diffusion has demonstrated a remarkable ability to create crisp, high quality results and has solid theoretical underpinnings (from variational inference to direct study as the Fisher Divergence). Our work combines autoencoder representation learning with diffusion and is, to our knowledge, the first to demonstrate the efficacy of jointly learning a continuous encoder and decoder under a diffusion-based loss. We demonstrate that this approach yields better reconstruction quality as compared to GAN-based autoencoders while being easier to tune. We also show that the resulting representation is easier to model with a latent diffusion model as compared to the representation obtained from a state-of-the-art GAN-based loss. Since our decoder is stochastic, it can generate details not encoded in the otherwise deterministic latent representation; we therefore name our approach "Sample what you can't compress", or SWYCC for short.

**Link**: [arxiv](http://arxiv.org/abs/2409.02529v1),  [pdf](http://arxiv.org/pdf/2409.02529v1)

**Tags**: cs.LG cs.CV 



### Cog-GA: A Large Language Models-based Generative Agent for   Vision-Language Navigation in Continuous Environments
**Authors**: Zhiyuan Li, Yanfeng Lu, Yao Mu, Hong Qiao

**Updated**: 2024-09-04T08:30:03Z

**Summary**: Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.

**Link**: [arxiv](http://arxiv.org/abs/2409.02522v1),  [pdf](http://arxiv.org/pdf/2409.02522v1)

**Tags**: cs.AI cs.RO 



### Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic   Reasoning with Argumentation Theory-Driven Prompts
**Authors**: Arianna Muti, Federico Ruggeri, Khalid Al-Khatib, Alberto Barr√≥n-Cede√±o, Tommaso Caselli

**Updated**: 2024-09-04T08:27:43Z

**Summary**: We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2409.02519v1),  [pdf](http://arxiv.org/pdf/2409.02519v1)

**Tags**: cs.CL cs.SI 



### Training Universal Vocoders with Feature Smoothing-Based Augmentation   Methods for High-Quality TTS Systems
**Authors**: Jeongmin Liu, Eunwoo Song

**Updated**: 2024-09-04T08:25:54Z

**Summary**: While universal vocoders have achieved proficient waveform generation across diverse voices, their integration into text-to-speech (TTS) tasks often results in degraded synthetic quality. To address this challenge, we present a novel augmentation technique for training universal vocoders. Our training scheme randomly applies linear smoothing filters to input acoustic features, facilitating vocoder generalization across a wide range of smoothings. It significantly mitigates the training-inference mismatch, enhancing the naturalness of synthetic output even when the acoustic model produces overly smoothed features. Notably, our method is applicable to any vocoder without requiring architectural modifications or dependencies on specific acoustic models. The experimental results validate the superiority of our vocoder over conventional methods, achieving 11.99% and 12.05% improvements in mean opinion scores when integrated with Tacotron 2 and FastSpeech 2 TTS acoustic models, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2409.02517v1),  [pdf](http://arxiv.org/pdf/2409.02517v1)

**Tags**: cs.SD cs.LG eess.AS 



### Vision-Language and Large Language Model Performance in   Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized   Models
**Authors**: Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush

**Updated**: 2024-09-04T08:22:28Z

**Summary**: Background and Aims: This study evaluates the medical reasoning performance of large language models (LLMs) and vision language models (VLMs) in gastroenterology.   Methods: We used 300 gastroenterology board exam-style multiple-choice questions, 138 of which contain images to systematically assess the impact of model configurations and parameters and prompt engineering strategies utilizing GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs (versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0), Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces (web and API), computing environments (cloud and local), and model precisions (with and without quantization). Finally, we assessed accuracy using a semiautomated pipeline.   Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet (74.0%) achieved the highest accuracy, outperforming the top open-source models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%). Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%) performed best. The scores of the quantized models were comparable to those of the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM performance on image-containing questions did not improve when the images were provided and worsened when LLM-generated captions were provided. In contrast, a 10% increase in accuracy was observed when images were accompanied by human-crafted image descriptions.   Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in medical reasoning, the integration of visual data remains a challenge for VLMs. Effective deployment involves carefully determining optimal model configurations, encouraging users to consider either the high performance of proprietary models or the flexible adaptability of open-source models.

**Link**: [arxiv](http://arxiv.org/abs/2409.00084v2),  [pdf](http://arxiv.org/pdf/2409.00084v2)

**Tags**: cs.CL cs.AI 92C50, 68T50 J.3 



### Is Difficulty Calibration All We Need? Towards More Practical Membership   Inference Attacks
**Authors**: Yu He, Boheng Li, Yao Wang, Mengda Yang, Juan Wang, Hongxin Hu, Xingyu Zhao

**Updated**: 2024-09-04T08:21:48Z

**Summary**: The vulnerability of machine learning models to Membership Inference Attacks (MIAs) has garnered considerable attention in recent years. These attacks determine whether a data sample belongs to the model's training set or not. Recent research has focused on reference-based attacks, which leverage difficulty calibration with independently trained reference models. While empirical studies have demonstrated its effectiveness, there is a notable gap in our understanding of the circumstances under which it succeeds or fails. In this paper, we take a further step towards a deeper understanding of the role of difficulty calibration. Our observations reveal inherent limitations in calibration methods, leading to the misclassification of non-members and suboptimal performance, particularly on high-loss samples. We further identify that these errors stem from an imperfect sampling of the potential distribution and a strong dependence of membership scores on the model parameters. By shedding light on these issues, we propose RAPID: a query-efficient and computation-efficient MIA that directly \textbf{R}e-lever\textbf{A}ges the original membershi\textbf{P} scores to m\textbf{I}tigate the errors in \textbf{D}ifficulty calibration. Our experimental results, spanning 9 datasets and 5 model architectures, demonstrate that RAPID outperforms previous state-of-the-art attacks (e.g., LiRA and Canary offline) across different metrics while remaining computationally efficient. Our observations and analysis challenge the current de facto paradigm of difficulty calibration in high-precision inference, encouraging greater attention to the persistent risks posed by MIAs in more practical scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2409.00426v2),  [pdf](http://arxiv.org/pdf/2409.00426v2)

**Tags**: cs.CR 



### BiKC: Keypose-Conditioned Consistency Policy for Bimanual Robotic   Manipulation
**Authors**: Dongjie Yu, Hang Xu, Yizhou Chen, Yi Ren, Jia Pan

**Updated**: 2024-09-04T08:20:40Z

**Summary**: Bimanual manipulation tasks typically involve multiple stages which require efficient interactions between two arms, posing step-wise and stage-wise challenges for imitation learning systems. Specifically, failure and delay of one step will broadcast through time, hinder success and efficiency of each sub-stage task, and thereby overall task performance. Although recent works have made strides in addressing certain challenges, few approaches explicitly consider the multi-stage nature of bimanual tasks while simultaneously emphasizing the importance of inference speed. In this paper, we introduce a novel keypose-conditioned consistency policy tailored for bimanual manipulation. It is a hierarchical imitation learning framework that consists of a high-level keypose predictor and a low-level trajectory generator. The predicted keyposes provide guidance for trajectory generation and also mark the completion of one sub-stage task. The trajectory generator is designed as a consistency model trained from scratch without distillation, which generates action sequences conditioning on current observations and predicted keyposes with fast inference speed. Simulated and real-world experimental results demonstrate that the proposed approach surpasses baseline methods in terms of success rate and operational efficiency. Codes are available at https://github.com/ManUtdMoon/BiKC.

**Link**: [arxiv](http://arxiv.org/abs/2406.10093v2),  [pdf](http://arxiv.org/pdf/2406.10093v2)

**Tags**: cs.RO cs.LG 



### Not-so-little Red Dots: Two massive and dusty starbursts at z~5-7   pushing the limits of star formation discovered by JWST in the COSMOS-Web   survey
**Authors**: Fabrizio Gentile, Caitlin M. Casey, Hollis B. Akins, Maximilien Franco, Jed McKinney, Edward Berman, Olivia R. Cooper, Nicole E. Drakos, Michaela Hirschmann, Arianna S. Long, Georgios Magdis, Anton M. Koekemoer, Vasily Kokorev, Marko Shuntov, Margherita Talia, Natalie Allen, Santosh Harish, Olivier Ilbert, Henry J. McCracken, Jeyhan S. Kartaltepe, Daizhong Liu, Louise Paquereau, Jason Rhodes, Michael R. Rich, Brant Robertson, Sune Toft, Ghassem Gozaliasl

**Updated**: 2024-09-04T08:16:01Z

**Summary**: We present the properties of two candidate massive ($M_\star\sim10^{11}M_\odot$) and dusty ($A_{\rm v}>2.5$ mag) galaxies at $z=5-7$ in the first 0.28 deg$^2$ of the COSMOS-Web survey. One object is spectroscopically confirmed at $z_{\rm spec}=5.051$, while the other has a robust $z_{\rm phot}=6.7\pm0.3$. Thanks to their extremely red colors ($F277W-F444W\sim1.7$ mag), these galaxies satisfy the nominal color-selection for the widely-studied ``little red dot" (LRD) population with the exception of their spatially-resolved morphologies. The morphology of our targets allows us to conclude that their red continuum is dominated by highly obscured stellar emission and not by reddened nuclear activity. Using a variety of SED-fitting tools and star formation histories, we estimate the stellar masses to be $\log(M_\star)=11.32^{+0.07}_{-0.15}$ $M_\odot$ and $\log(M_\star)=11.2^{+0.1}_{-0.2}$ $M_\odot$, respectively, with a red continuum emission dominated by a recent episode of star formation. We then compare their number density to the halo mass function to infer stellar baryon fractions of $\epsilon_\star\sim0.25$ and $\epsilon_\star\sim0.5$. Both are significantly higher than what is commonly observed in lower-z galaxies or more dust-obscured galaxies at similar redshifts. With very bright ultra-high-z Lyman-Break Galaxies and some non-AGN dominated LRDs, such ``extended" LRDs represent another population that may require very efficient star formation at early times.

**Link**: [arxiv](http://arxiv.org/abs/2408.10305v2),  [pdf](http://arxiv.org/pdf/2408.10305v2)

**Tags**: astro-ph.GA astro-ph.CO 



### Ubiquitous Late Radio Emission from Tidal Disruption Events
**Authors**: Yvette Cendes, Edo Berger, Kate D. Alexander, Ryan Chornock, Raffaella Margutti, Brian Metzger, Mark H. Wieringa, Michael F. Bietenholz, Aprajita Hajela, Tanmoy Laskar, Michael C. Stroh, Giacomo Terreran

**Updated**: 2024-09-04T08:06:27Z

**Summary**: We present radio observations of 23 optically discovered tidal disruption events (TDEs) on timescales of 500-3200 days post discovery. We detect nine new TDEs that did not have detectable radio emission at earlier times, indicating a late-time brightening after several hundred (and up to 2300) days; an additional seven TDEs exhibit radio emission whose origin is ambiguous or may be attributed to the host galaxy or an active galactic nucleus. We also report a new rising component in one TDE previously detected in the radio at 10^3 days. While the radio emission in some of the detected TDEs peaked on a timescale 2-4 yr, over half of the sample still show rising emission. The range of luminosities for the sample is 10^37-10^39 erg/s, about 2 orders of magnitude below the radio luminosity of the relativistic TDE Sw J1644+57. Our data set indicates 40% of all optical TDEs are detected in radio hundreds to thousands of days after discovery, and that this is probably more common than early radio emission peaking at 10^2 days. Using an equipartition analysis, we find evidence for a delayed launch of the radio-emitting outflows, with delay timescales of 500-2000 days, inferred velocities of 0.02-0.15c, and kinetic energies of 10^47-10^49 erg. We rule out off axis relativistic jets as a viable explanation for this population, and conclude delayed outflows are a more likely explanation, possibly from delayed disk formation. We conclude late radio emission marks a fairly ubiquitous but heretofore overlooked phase of TDE evolution.

**Link**: [arxiv](http://arxiv.org/abs/2308.13595v2),  [pdf](http://arxiv.org/pdf/2308.13595v2)

**Tags**: astro-ph.HE 



### NetMamba: Efficient Network Traffic Classification via Pre-training   Unidirectional Mamba
**Authors**: Tongze Wang, Xiaohui Xie, Wenduo Wang, Chuyi Wang, Youjian Zhao, Yong Cui

**Updated**: 2024-09-04T08:03:27Z

**Summary**: Network traffic classification is a crucial research area aiming to enhance service quality, streamline network management, and bolster cybersecurity. To address the growing complexity of transmission encryption techniques, various machine learning and deep learning methods have been proposed. However, existing approaches face two main challenges. Firstly, they struggle with model inefficiency due to the quadratic complexity of the widely used Transformer architecture. Secondly, they suffer from inadequate traffic representation because of discarding important byte information while retaining unwanted biases. To address these challenges, we propose NetMamba, an efficient linear-time state space model equipped with a comprehensive traffic representation scheme. We adopt a specially selected and improved unidirectional Mamba architecture for the networking field, instead of the Transformer, to address efficiency issues. In addition, we design a traffic representation scheme to extract valid information from massive traffic data while removing biased information. Evaluation experiments on six public datasets encompassing three main classification tasks showcase NetMamba's superior classification performance compared to state-of-the-art baselines. It achieves an accuracy rate of nearly 99% (some over 99%) in all tasks. Additionally, NetMamba demonstrates excellent efficiency, improving inference speed by up to 60 times while maintaining comparably low memory usage. Furthermore, NetMamba exhibits superior few-shot learning abilities, achieving better classification performance with fewer labeled data. To the best of our knowledge, NetMamba is the first model to tailor the Mamba architecture for networking.

**Link**: [arxiv](http://arxiv.org/abs/2405.11449v3),  [pdf](http://arxiv.org/pdf/2405.11449v3)

**Tags**: cs.LG cs.NI 



### Growth of nonconvex functionals at strict local minimizers
**Authors**: Alberto Dom√≠nguez Corella, Tr√≠ Minh L√™

**Updated**: 2024-09-04T07:50:40Z

**Summary**: In this paper, we present new equivalent conditions for the growth of proper lower semicontinuous functionals at strict local minimizers. The main conditions are a variant of the so-called tilt stability property of local minimizers and an analog of the classic Polyak-{\L}ojasiewicz condition, where the gradient is replaced by linear perturbations. We derive the following tilting principle: stability of minimizers under linear perturbations can infer their stability under nonlinear ones. We show how growth conditions can be used to give convergence rates for the proximal point algorithm. Finally, we give an application to elliptic tracking problems, establishing a novel equivalence between second-order conditions and the sensitivity of solutions with respect to uncertainty in data.

**Link**: [arxiv](http://arxiv.org/abs/2409.01833v2),  [pdf](http://arxiv.org/pdf/2409.01833v2)

**Tags**: math.OC 49J52, 49K40, 90C31, 90C48 



### Plane2Depth: Hierarchical Adaptive Plane Guidance for Monocular Depth   Estimation
**Authors**: Li Liu, Ruijie Zhu, Jiacheng Deng, Ziyang Song, Wenfei Yang, Tianzhu Zhang

**Updated**: 2024-09-04T07:45:06Z

**Summary**: Monocular depth estimation aims to infer a dense depth map from a single image, which is a fundamental and prevalent task in computer vision. Many previous works have shown impressive depth estimation results through carefully designed network structures, but they usually ignore the planar information and therefore perform poorly in low-texture areas of indoor scenes. In this paper, we propose Plane2Depth, which adaptively utilizes plane information to improve depth prediction within a hierarchical framework. Specifically, in the proposed plane guided depth generator (PGDG), we design a set of plane queries as prototypes to softly model planes in the scene and predict per-pixel plane coefficients. Then the predicted plane coefficients can be converted into metric depth values with the pinhole camera model. In the proposed adaptive plane query aggregation (APGA) module, we introduce a novel feature interaction approach to improve the aggregation of multi-scale plane features in a top-down manner. Extensive experiments show that our method can achieve outstanding performance, especially in low-texture or repetitive areas. Furthermore, under the same backbone network, our method outperforms the state-of-the-art methods on the NYU-Depth-v2 dataset, achieves competitive results with state-of-the-art methods KITTI dataset and can be generalized to unseen scenes effectively.

**Link**: [arxiv](http://arxiv.org/abs/2409.02494v1),  [pdf](http://arxiv.org/pdf/2409.02494v1)

**Tags**: cs.CV 



### Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image   Indoor Depth by Meta-Initialization
**Authors**: Cho-Ying Wu, Yiqi Zhong, Junying Wang, Ulrich Neumann

**Updated**: 2024-09-04T07:25:50Z

**Summary**: Indoor robots rely on depth to perform tasks like navigation or obstacle detection, and single-image depth estimation is widely used to assist perception. Most indoor single-image depth prediction focuses less on model generalizability to unseen datasets, concerned with in-the-wild robustness for system deployment. This work leverages gradient-based meta-learning to gain higher generalizability on zero-shot cross-dataset inference. Unlike the most-studied meta-learning of image classification associated with explicit class labels, no explicit task boundaries exist for continuous depth values tied to highly varying indoor environments regarding object arrangement and scene composition. We propose fine-grained task that treats each RGB-D mini-batch as a task in our meta-learning formulation. We first show that our method on limited data induces a much better prior (max 27.8% in RMSE). Then, finetuning on meta-learned initialization consistently outperforms baselines without the meta approach. Aiming at generalization, we propose zero-shot cross-dataset protocols and validate higher generalizability induced by our meta-initialization, as a simple and useful plugin to many existing depth estimation methods. The work at the intersection of depth and meta-learning potentially drives both research to step closer to practical robotic and machine perception usage.

**Link**: [arxiv](http://arxiv.org/abs/2409.02486v1),  [pdf](http://arxiv.org/pdf/2409.02486v1)

**Tags**: cs.CV cs.AI 



### Adversarial Attacks on Machine Learning-Aided Visualizations
**Authors**: Takanori Fujiwara, Kostiantyn Kucher, Junpeng Wang, Rafael M. Martins, Andreas Kerren, Anders Ynnerman

**Updated**: 2024-09-04T07:23:12Z

**Summary**: Research in ML4VIS investigates how to use machine learning (ML) techniques to generate visualizations, and the field is rapidly growing with high societal impact. However, as with any computational pipeline that employs ML processes, ML4VIS approaches are susceptible to a range of ML-specific adversarial attacks. These attacks can manipulate visualization generations, causing analysts to be tricked and their judgments to be impaired. Due to a lack of synthesis from both visualization and ML perspectives, this security aspect is largely overlooked by the current ML4VIS literature. To bridge this gap, we investigate the potential vulnerabilities of ML-aided visualizations from adversarial attacks using a holistic lens of both visualization and ML perspectives. We first identify the attack surface (i.e., attack entry points) that is unique in ML-aided visualizations. We then exemplify five different adversarial attacks. These examples highlight the range of possible attacks when considering the attack surface and multiple different adversary capabilities. Our results show that adversaries can induce various attacks, such as creating arbitrary and deceptive visualizations, by systematically identifying input attributes that are influential in ML inferences. Based on our observations of the attack surface characteristics and the attack examples, we underline the importance of comprehensive studies of security issues and defense mechanisms as a call of urgency for the ML4VIS community.

**Link**: [arxiv](http://arxiv.org/abs/2409.02485v1),  [pdf](http://arxiv.org/pdf/2409.02485v1)

**Tags**: cs.CR cs.AI cs.HC cs.LG stat.ML 



### A Comparative Study on Large Language Models for Log Parsing
**Authors**: Merve Astekin, Max Hort, Leon Moonen

**Updated**: 2024-09-04T06:46:31Z

**Summary**: Background: Log messages provide valuable information about the status of software systems. This information is provided in an unstructured fashion and automated approaches are applied to extract relevant parameters. To ease this process, log parsing can be applied, which transforms log messages into structured log templates. Recent advances in language models have led to several studies that apply ChatGPT to the task of log parsing with promising results. However, the performance of other state-of-the-art large language models (LLMs) on the log parsing task remains unclear.   Aims: In this study, we investigate the current capability of state-of-the-art LLMs to perform log parsing.   Method: We select six recent LLMs, including both paid proprietary (GPT-3.5, Claude 2.1) and four free-to-use open models, and compare their performance on system logs obtained from a selection of mature open-source projects. We design two different prompting approaches and apply the LLMs on 1, 354 log templates across 16 different projects. We evaluate their effectiveness, in the number of correctly identified templates, and the syntactic similarity between the generated templates and the ground truth.   Results: We found that free-to-use models are able to compete with paid models, with CodeLlama extracting 10% more log templates correctly than GPT-3.5. Moreover, we provide qualitative insights into the usability of language models (e.g., how easy it is to use their responses).   Conclusions: Our results reveal that some of the smaller, free-to-use LLMs can considerably assist log parsing compared to their paid proprietary competitors, especially code-specialized models.

**Link**: [arxiv](http://arxiv.org/abs/2409.02474v1),  [pdf](http://arxiv.org/pdf/2409.02474v1)

**Tags**: cs.SE cs.CL 



### Evaluating Named Entity Recognition Using Few-Shot Prompting with Large   Language Models
**Authors**: H√©di Zeghidi, Ludovic Moncla

**Updated**: 2024-09-04T06:36:22Z

**Summary**: This paper evaluates Few-Shot Prompting with Large Language Models for Named Entity Recognition (NER). Traditional NER systems rely on extensive labeled datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples. We assess state-of-the-art models like GPT-4 in NER tasks, comparing their few-shot performance to fully supervised benchmarks. Results show that while there is a performance gap, large models excel in adapting to new entity types and domains with very limited data. We also explore the effects of prompt engineering, guided output format and context length on performance. This study underscores Few-Shot Learning's potential to reduce the need for large labeled datasets, enhancing NER scalability and accessibility.

**Link**: [arxiv](http://arxiv.org/abs/2408.15796v2),  [pdf](http://arxiv.org/pdf/2408.15796v2)

**Tags**: cs.IR cs.AI 



### DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels
**Authors**: Zhe Xu, Jiasheng Ye, Xiangyang Liu, Tianxiang Sun, Xiaoran Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, Xipeng Qiu

**Updated**: 2024-09-04T06:28:22Z

**Summary**: With the rapid advancement of Large Language Models (LLMs), long-context information understanding and processing have become a hot topic in academia and industry. However, benchmarks for evaluating the ability of LLMs to handle long-context information do not seem to have kept pace with the development of LLMs. Despite the emergence of various long-context evaluation benchmarks, the types of capability assessed are still limited, without new capability dimensions. In this paper, we introduce DetectiveQA, a narrative reasoning benchmark featured with an average context length of over 100K tokens. DetectiveQA focuses on evaluating the long-context reasoning ability of LLMs, which not only requires a full understanding of context but also requires extracting important evidences from the context and reasoning according to extracted evidences to answer the given questions. This is a new dimension of capability evaluation, which is more in line with the current intelligence level of LLMs. We use detective novels as data sources, which naturally have various reasoning elements. Finally, we manually annotated 600 questions in Chinese and then also provided an English edition of the context information and questions. We evaluate many long-context LLMs on DetectiveQA, including commercial and open-sourced models, and the results indicate that existing long-context LLMs still require significant advancements to effectively process true long-context dependency questions.

**Link**: [arxiv](http://arxiv.org/abs/2409.02465v1),  [pdf](http://arxiv.org/pdf/2409.02465v1)

**Tags**: cs.CL 



### MarsCode Agent: AI-native Automated Bug Fixing
**Authors**: Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, Chao Peng

**Updated**: 2024-09-04T06:19:08Z

**Summary**: Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.00899v2),  [pdf](http://arxiv.org/pdf/2409.00899v2)

**Tags**: cs.SE cs.AI 



### The Fault in our Stars: Quality Assessment of Code Generation Benchmarks
**Authors**: Mohammed Latif Siddiq, Simantika Dristi, Joy Saha, Joanna C. S. Santos

**Updated**: 2024-09-04T06:10:47Z

**Summary**: Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.

**Link**: [arxiv](http://arxiv.org/abs/2404.10155v3),  [pdf](http://arxiv.org/pdf/2404.10155v3)

**Tags**: cs.SE cs.LG 



### Large Language Models for Explainable Decisions in Dynamic Digital Twins
**Authors**: Nan Zhang, Christian Vergara-Marcillo, Georgios Diamantopoulos, Jingran Shen, Nikos Tziritas, Rami Bahsoon, Georgios Theodoropoulos

**Updated**: 2024-09-04T06:00:56Z

**Summary**: Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making and provide an optimisation platform for the underlying system. By leveraging principles of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can formulate computational modalities for feedback loops, model updates and decision-making, including autonomous ones. However, understanding autonomous decision-making often requires technical and domain-specific knowledge. This paper explores using large language models (LLMs) to provide an explainability platform for DDTs, generating natural language explanations of the system's decision-making by leveraging domain-specific knowledge bases. A case study from smart agriculture is presented.

**Link**: [arxiv](http://arxiv.org/abs/2405.14411v2),  [pdf](http://arxiv.org/pdf/2405.14411v2)

**Tags**: cs.AI cs.SY eess.SY 



### Multi-Sources Fusion Learning for Multi-Points NLOS Localization in OFDM   System
**Authors**: Bohao Wang, Zitao Shuai, Chongwen Huang, Qianqian Yang, Zhaohui Yang, Richeng Jin, Ahmed Al Hammadi, Zhaoyang Zhang, Chau Yuen, M√©rouane Debbah

**Updated**: 2024-09-04T05:34:27Z

**Summary**: Accurate localization of mobile terminals is a pivotal aspect of integrated sensing and communication systems. Traditional fingerprint-based localization methods, which infer coordinates from channel information within pre-set rectangular areas, often face challenges due to the heterogeneous distribution of fingerprints inherent in non-line-of-sight (NLOS) scenarios, particularly within orthogonal frequency division multiplexing systems. To overcome this limitation, we develop a novel multi-sources information fusion learning framework referred to as the Autosync Multi-Domains NLOS Localization (AMDNLoc). Specifically, AMDNLoc employs a two-stage matched filter fused with a target tracking algorithm and iterative centroid-based clustering to automatically and irregularly segment NLOS regions, ensuring uniform distribution within channel state information across frequency, power, and time-delay domains. Additionally, the framework utilizes a segment-specific linear classifier array, coupled with deep residual network-based feature extraction and fusion, to establish the correlation function between fingerprint features and coordinates within these regions. Simulation results reveal that AMDNLoc achieves an impressive NLOS localization accuracy of 1.46 meters on typical wireless artificial intelligence research datasets and demonstrates significant improvements in interpretability, adaptability, and scalability.

**Link**: [arxiv](http://arxiv.org/abs/2409.02454v1),  [pdf](http://arxiv.org/pdf/2409.02454v1)

**Tags**: cs.IT eess.SP math.IT 



### Towards Measuring and Modeling "Culture" in LLMs: A Survey
**Authors**: Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Alham Fikri Aji, Jacki O'Neill, Ashutosh Modi, Monojit Choudhury

**Updated**: 2024-09-04T05:12:54Z

**Summary**: We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define "culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture". We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of ``culture,'' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.

**Link**: [arxiv](http://arxiv.org/abs/2403.15412v5),  [pdf](http://arxiv.org/pdf/2403.15412v5)

**Tags**: cs.CY cs.AI cs.CL 



### Fast, High-Quality and Parameter-Efficient Articulatory Synthesis using   Differentiable DSP
**Authors**: Yisi Liu, Bohan Yu, Drake Lin, Peter Wu, Cheol Jun Cho, Gopala Krishna Anumanchipalli

**Updated**: 2024-09-04T05:12:15Z

**Summary**: Articulatory trajectories like electromagnetic articulography (EMA) provide a low-dimensional representation of the vocal tract filter and have been used as natural, grounded features for speech synthesis. Differentiable digital signal processing (DDSP) is a parameter-efficient framework for audio synthesis. Therefore, integrating low-dimensional EMA features with DDSP can significantly enhance the computational efficiency of speech synthesis. In this paper, we propose a fast, high-quality, and parameter-efficient DDSP articulatory vocoder that can synthesize speech from EMA, F0, and loudness. We incorporate several techniques to solve the harmonics / noise imbalance problem, and add a multi-resolution adversarial loss for better synthesis quality. Our model achieves a transcription word error rate (WER) of 6.67% and a mean opinion score (MOS) of 3.74, with an improvement of 1.63% and 0.16 compared to the state-of-the-art (SOTA) baseline. Our DDSP vocoder is 4.9x faster than the baseline on CPU during inference, and can generate speech of comparable quality with only 0.4M parameters, in contrast to the 9M parameters required by the SOTA.

**Link**: [arxiv](http://arxiv.org/abs/2409.02451v1),  [pdf](http://arxiv.org/pdf/2409.02451v1)

**Tags**: eess.AS cs.AI cs.SD 



### Jina-ColBERT-v2: A General-Purpose Multilingual Late Interaction   Retriever
**Authors**: Rohan Jha, Bo Wang, Michael G√ºnther, Georgios Mastrapas, Saba Sturua, Isabelle Mohr, Andreas Koukounas, Mohammad Kalim Akram, Nan Wang, Han Xiao

**Updated**: 2024-09-04T05:09:00Z

**Summary**: Multi-vector dense models, such as ColBERT, have proven highly effective in information retrieval. ColBERT's late interaction scoring approximates the joint query-document attention seen in cross-encoders while maintaining inference efficiency closer to traditional dense retrieval models, thanks to its bi-encoder architecture and recent optimizations in indexing and search. In this paper, we introduce a novel architecture and a training framework to support long context window and multilingual retrieval. Our new model, Jina-ColBERT-v2, demonstrates strong performance across a range of English and multilingual retrieval tasks,

**Link**: [arxiv](http://arxiv.org/abs/2408.16672v3),  [pdf](http://arxiv.org/pdf/2408.16672v3)

**Tags**: cs.IR cs.AI cs.CL 68T50 I.2.7 



### Identification of Gravitational-waves from Extreme Mass Ratio Inspirals
**Authors**: Chang-Qing Ye, Hui-Min Fan, Alejandro Torres-Orjuela, Jian-dong Zhang, Yi-Ming Hu

**Updated**: 2024-09-04T04:51:36Z

**Summary**: Space-based gravitational wave detectors like TianQin or LISA could observe extreme-mass-ratio-inspirals (EMRIs) at millihertz frequencies. The accurate identification of these EMRI signals from the data plays a crucial role in enabling in-depth study of astronomy and physics. We aim at the identification stage of the data analysis, with the aim to extract key features of the signal from the data, such as the evolution of the orbital frequency, as well as to pinpoint the parameter range that can fit the data well for the subsequent parameter inference stage. In this manuscript, we demonstrated the identification of EMRI signals without any additional prior information on physical parameters. High-precision measurements of EMRI signals have been achieved, using a hierarchical search. It combines the search for physical parameters that guide the subsequent parameter inference, and a semi-coherent search with phenomenological waveforms that reaches precision levels down to $10^{-4}$ for the phenomenological waveform parameters $\omega_{0}$, $\dot{\omega}_{0}$, and $\ddot{\omega}_{0}$. As a result, we obtain measurement relative errors of less than 4% for the mass of the massive black hole, while keeping the relative errors of the other parameters within as small as 0.5%.

**Link**: [arxiv](http://arxiv.org/abs/2310.03520v2),  [pdf](http://arxiv.org/pdf/2310.03520v2)

**Tags**: gr-qc astro-ph.GA astro-ph.HE 



### Exploring the applicability of Large Language Models to citation context   analysis
**Authors**: Kai Nishikawa, Hitoshi Koshiba

**Updated**: 2024-09-04T04:41:15Z

**Summary**: Unlike traditional citation analysis -- which assumes that all citations in a paper are equivalent -- citation context analysis considers the contextual information of individual citations. However, citation context analysis requires creating large amounts of data through annotation, which hinders the widespread use of this methodology. This study explored the applicability of Large Language Models (LLMs) -- particularly ChatGPT -- to citation context analysis by comparing LLMs and human annotation results. The results show that the LLMs annotation is as good as or better than the human annotation in terms of consistency but poor in terms of predictive performance. Thus, having LLMs immediately replace human annotators in citation context analysis is inappropriate. However, the annotation results obtained by LLMs can be used as reference information when narrowing the annotation results obtained by multiple human annotators to one, or LLMs can be used as one of the annotators when it is difficult to prepare sufficient human annotators. This study provides basic findings important for the future development of citation context analyses.

**Link**: [arxiv](http://arxiv.org/abs/2409.02443v1),  [pdf](http://arxiv.org/pdf/2409.02443v1)

**Tags**: cs.DL 



### Rank-transformed subsampling: inference for multiple data splitting and   exchangeable p-values
**Authors**: F. Richard Guo, Rajen D. Shah

**Updated**: 2024-09-04T04:39:30Z

**Summary**: Many testing problems are readily amenable to randomised tests such as those employing data splitting. However despite their usefulness in principle, randomised tests have obvious drawbacks. Firstly, two analyses of the same dataset may lead to different results. Secondly, the test typically loses power because it does not fully utilise the entire sample. As a remedy to these drawbacks, we study how to combine the test statistics or p-values resulting from multiple random realisations such as through random data splits. We develop rank-transformed subsampling as a general method for delivering large sample inference about the combined statistic or p-value under mild assumptions. We apply our methodology to a wide range of problems, including testing unimodality in high-dimensional data, testing goodness-of-fit of parametric quantile regression models, testing no direct effect in a sequentially randomised trial and calibrating cross-fit double machine learning confidence intervals. In contrast to existing p-value aggregation schemes that can be highly conservative, our method enjoys type-I error control that asymptotically approaches the nominal level. Moreover, compared to using the ordinary subsampling, we show that our rank transform can remove the first-order bias in approximating the null under alternatives and greatly improve power.

**Link**: [arxiv](http://arxiv.org/abs/2301.02739v3),  [pdf](http://arxiv.org/pdf/2301.02739v3)

**Tags**: stat.ME math.ST stat.TH 



### Fuzzy Logic Control for Indoor Navigation of Mobile Robots
**Authors**: Akshay Kumar, Ashwin Sahasrabudhe, Sanjuksha Nirgude

**Updated**: 2024-09-04T04:27:14Z

**Summary**: Autonomous mobile robots have many applications in indoor unstructured environment, wherein optimal movement of the robot is needed. The robot therefore needs to navigate in unknown and dynamic environments. This paper presents an implementation of fuzzy logic controller for navigation of mobile robot in an unknown dynamically cluttered environment. Fuzzy logic controller is used here as it is capable of making inferences even under uncertainties. It helps in rule generation and decision making process in order to reach the goal position under various situations. Sensor readings from the robot and the desired direction of motion are inputs to the fuzz logic controllers and the acceleration of the respective wheels are the output of the controller. Hence, the mobile robot avoids obstacles and reaches the goal position. Keywords: Fuzzy Logic Controller, Membership Functions, Takagi-Sugeno-Kang FIS, Centroid Defuzzification

**Link**: [arxiv](http://arxiv.org/abs/2409.02437v1),  [pdf](http://arxiv.org/pdf/2409.02437v1)

**Tags**: cs.RO 



## Keyword: LLM Deployment 
 ### Enhancing Graph Neural Networks with Limited Labeled Data by Actively   Distilling Knowledge from Large Language Models
**Authors**: Quan Li, Tianxiang Zhao, Lingwei Chen, Junjie Xu, Suhang Wang

**Updated**: 2024-09-04T17:52:37Z

**Summary**: Graphs are pervasive in the real-world, such as social network analysis, bioinformatics, and knowledge graphs. Graph neural networks (GNNs) have great ability in node classification, a fundamental task on graphs. Unfortunately, conventional GNNs still face challenges in scenarios with few labeled nodes, despite the prevalence of few-shot node classification tasks in real-world applications. To address this challenge, various approaches have been proposed, including graph meta-learning, transfer learning, and methods based on Large Language Models (LLMs). However, traditional meta-learning and transfer learning methods often require prior knowledge from base classes or fail to exploit the potential advantages of unlabeled nodes. Meanwhile, LLM-based methods may overlook the zero-shot capabilities of LLMs and rely heavily on the quality of generated contexts. In this paper, we propose a novel approach that integrates LLMs and GNNs, leveraging the zero-shot inference and reasoning capabilities of LLMs and employing a Graph-LLM-based active learning paradigm to enhance GNNs' performance. Extensive experiments demonstrate the effectiveness of our model in improving node classification accuracy with considerably limited labeled data, surpassing state-of-the-art baselines by significant margins.

**Link**: [arxiv](http://arxiv.org/abs/2407.13989v3),  [pdf](http://arxiv.org/pdf/2407.13989v3)

**Tags**: cs.LG cs.AI 



### Design of a Standard-Compliant Real-Time Neural Receiver for 5G NR
**Authors**: Reinhard Wiesmayr, Sebastian Cammerer, Fay√ßal A√Øt Aoudia, Jakob Hoydis, Jakub Zakrzewski, Alexander Keller

**Updated**: 2024-09-04T17:51:18Z

**Summary**: We detail the steps required to deploy a multi-user multiple-input multiple-output (MU-MIMO) neural receiver (NRX) in an actual cellular communication system. This raises several exciting research challenges, including the need for real-time inference and compatibility with the 5G NR standard. As the network configuration in a practical setup can change dynamically within milliseconds, we propose an adaptive NRX architecture capable of supporting dynamic modulation and coding scheme (MCS) configurations without the need for any re-training and without additional inference cost. We optimize the latency of the neural network (NN) architecture to achieve inference times of less than 1ms on an NVIDIA A100 GPU using the TensorRT inference library. These latency constraints effectively limit the size of the NN and we quantify the resulting signal-to-noise ratio (SNR) degradation as less than 0.7 dB when compared to a preliminary non-real-time NRX architecture. Finally, we explore the potential for site-specific adaptation of the receiver by investigating the required size of the training dataset and the number of fine-tuning iterations to optimize the NRX for specific radio environments using a ray tracing-based channel model. The resulting NRX is ready for deployment in a real-time 5G NR system and the source code including the TensorRT experiments is available online.

**Link**: [arxiv](http://arxiv.org/abs/2409.02912v1),  [pdf](http://arxiv.org/pdf/2409.02912v1)

**Tags**: cs.IT eess.SP math.IT 



### LongCite: Enabling LLMs to Generate Fine-grained Citations in   Long-context QA
**Authors**: Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li

**Updated**: 2024-09-05T03:53:13Z

**Summary**: Though current long-context large language models (LLMs) have demonstrated impressive capacities in answering user questions based on extensive text, the lack of citations in their responses makes user verification difficult, leading to concerns about their trustworthiness due to their potential hallucinations. In this work, we aim to enable long-context LLMs to generate responses with fine-grained sentence-level citations, improving their faithfulness and verifiability. We first introduce LongBench-Cite, an automated benchmark for assessing current LLMs' performance in Long-Context Question Answering with Citations (LQAC), revealing considerable room for improvement. To this end, we propose CoF (Coarse to Fine), a novel pipeline that utilizes off-the-shelf LLMs to automatically generate long-context QA instances with precise sentence-level citations, and leverage this pipeline to construct LongCite-45k, a large-scale SFT dataset for LQAC. Finally, we train LongCite-8B and LongCite-9B using the LongCite-45k dataset, successfully enabling their generation of accurate responses and fine-grained sentence-level citations in a single output. The evaluation results on LongBench-Cite show that our trained models achieve state-of-the-art citation quality, surpassing advanced proprietary models including GPT-4o.

**Link**: [arxiv](http://arxiv.org/abs/2409.02897v2),  [pdf](http://arxiv.org/pdf/2409.02897v2)

**Tags**: cs.CL 



### LADDER: Language Driven Slice Discovery and Error Rectification
**Authors**: Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Kayhan Batmanghelich

**Updated**: 2024-09-04T17:31:00Z

**Summary**: Error slice discovery associates structured patterns with model errors. Existing methods discover error slices by clustering the error-prone samples with similar patterns or assigning discrete attributes to each sample for post-hoc analysis. While these methods aim for interpretability and easier mitigation through reweighting or rebalancing, they may not capture the full complexity of error patterns due to incomplete or missing attributes. Contrary to the existing approach, this paper utilizes the reasoning capabilities of the Large Language Model (LLM) to analyze complex error patterns and generate testable hypotheses. This paper proposes LADDER: Language Driven slice Discovery and Error Rectification. It first projects the model's representation into a language-aligned feature space (eg CLIP) to preserve semantics in the original model feature space. This ensures the accurate retrieval of sentences that highlight the model's errors. Next, the LLM utilizes the sentences and generates hypotheses to discover error slices. Finally, we mitigate the error by fine-tuning the classification head by creating a group-balanced dataset using the hypotheses. Our entire method does not require any attribute annotation, either explicitly or through external tagging models. We validate our method with \textbf{five} image classification datasets. The code is available (https://github.com/batmanlab/Ladder).

**Link**: [arxiv](http://arxiv.org/abs/2408.07832v3),  [pdf](http://arxiv.org/pdf/2408.07832v3)

**Tags**: cs.CL cs.CV 



### LongLLaVA: Scaling Multi-modal LLMs to 1000 Images Efficiently via   Hybrid Architecture
**Authors**: Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, Benyou Wang

**Updated**: 2024-09-04T17:25:21Z

**Summary**: Expanding the long-context capabilities of Multi-modal Large Language Models~(MLLMs) is crucial for video understanding, high-resolution image understanding, and multi-modal agents. This involves a series of systematic optimizations, including model architecture, data construction and training strategy, particularly addressing challenges such as \textit{degraded performance with more images} and \textit{high computational costs}. In this paper, we adapt the model architecture to a hybrid of Mamba and Transformer blocks, approach data construction with both temporal and spatial dependencies among multiple images and employ a progressive training strategy. The released model \textbf{LongLLaVA}~(\textbf{Long}-Context \textbf{L}arge \textbf{L}anguage \textbf{a}nd \textbf{V}ision \textbf{A}ssistant) is the first hybrid MLLM, which achieved a better balance between efficiency and effectiveness. LongLLaVA not only achieves competitive results across various benchmarks, but also maintains high throughput and low memory consumption. Especially, it could process nearly a thousand images on a single A100 80GB GPU, showing promising application prospects for a wide range of tasks.

**Link**: [arxiv](http://arxiv.org/abs/2409.02889v1),  [pdf](http://arxiv.org/pdf/2409.02889v1)

**Tags**: cs.CL cs.AI cs.CV cs.MM 



### The Need for Guardrails with Large Language Models in Medical   Safety-Critical Settings: An Artificial Intelligence Application in the   Pharmacovigilance Ecosystem
**Authors**: Joe B Hakim, Jeffery L Painter, Darmendra Ramcharran, Vijay Kara, Greg Powell, Paulina Sobczak, Chiho Sato, Andrew Bate, Andrew Beam

**Updated**: 2024-09-04T17:16:05Z

**Summary**: Large language models (LLMs) are useful tools with the capacity for performing specific types of knowledge work at an effective scale. However, LLM deployments in high-risk and safety-critical domains pose unique challenges, notably the issue of ``hallucination,'' where LLMs can generate fabricated information. This is particularly concerning in settings such as drug safety, where inaccuracies could lead to patient harm. To mitigate these risks, we have developed and demonstrated a proof of concept suite of guardrails specifically designed to mitigate certain types of hallucinations and errors for drug safety, and potentially applicable to other medical safety-critical contexts. These guardrails include mechanisms to detect anomalous documents to prevent the ingestion of inappropriate data, identify incorrect drug names or adverse event terms, and convey uncertainty in generated content. We integrated these guardrails with an LLM fine-tuned for a text-to-text task, which involves converting both structured and unstructured data within adverse event reports into natural language. This method was applied to translate individual case safety reports, demonstrating effective application in a pharmacovigilance processing task. Our guardrail framework offers a set of tools with broad applicability across various domains, ensuring LLMs can be safely used in high-risk situations by eliminating the occurrence of key errors, including the generation of incorrect pharmacovigilance-related terms, thus adhering to stringent regulatory and quality standards in medical safety-critical environments.

**Link**: [arxiv](http://arxiv.org/abs/2407.18322v2),  [pdf](http://arxiv.org/pdf/2407.18322v2)

**Tags**: cs.CL cs.AI cs.CY cs.LG I.2.1; I.2.7; I.7.1 



### Configurable Foundation Models: Building LLMs from a Modular Perspective
**Authors**: Chaojun Xiao, Zhengyan Zhang, Chenyang Song, Dazhi Jiang, Feng Yao, Xu Han, Xiaozhi Wang, Shuo Wang, Yufei Huang, Guanyu Lin, Yingfa Chen, Weilin Zhao, Yuge Tu, Zexuan Zhong, Ao Zhang, Chenglei Si, Khai Hao Moo, Chenyang Zhao, Huimin Chen, Yankai Lin, Zhiyuan Liu, Jingbo Shang, Maosong Sun

**Updated**: 2024-09-04T17:01:02Z

**Summary**: Advancements in LLMs have recently unveiled challenges tied to computational efficiency and continual scalability due to their requirements of huge parameters, making the applications and evolution of these models on devices with limited computation resources and scenarios requiring various abilities increasingly cumbersome. Inspired by modularity within the human brain, there is a growing tendency to decompose LLMs into numerous functional modules, allowing for inference with part of modules and dynamic assembly of modules to tackle complex tasks, such as mixture-of-experts. To highlight the inherent efficiency and composability of the modular approach, we coin the term brick to represent each functional module, designating the modularized structure as configurable foundation models. In this paper, we offer a comprehensive overview and investigation of the construction, utilization, and limitation of configurable foundation models. We first formalize modules into emergent bricks - functional neuron partitions that emerge during the pre-training phase, and customized bricks - bricks constructed via additional post-training to improve the capabilities and knowledge of LLMs. Based on diverse functional bricks, we further present four brick-oriented operations: retrieval and routing, merging, updating, and growing. These operations allow for dynamic configuration of LLMs based on instructions to handle complex tasks. To verify our perspective, we conduct an empirical analysis on widely-used LLMs. We find that the FFN layers follow modular patterns with functional specialization of neurons and functional neuron partitions. Finally, we highlight several open issues and directions for future research. Overall, this paper aims to offer a fresh modular perspective on existing LLM research and inspire the future creation of more efficient and scalable foundational models.

**Link**: [arxiv](http://arxiv.org/abs/2409.02877v1),  [pdf](http://arxiv.org/pdf/2409.02877v1)

**Tags**: cs.AI cs.CL cs.LG 



### Automating Pharmacovigilance Evidence Generation: Using Large Language   Models to Produce Context-Aware SQL
**Authors**: Jeffery L. Painter, Venkateswara Rao Chalamalasetti, Raymond Kassekert, Andrew Bate

**Updated**: 2024-09-04T16:58:25Z

**Summary**: Objective: To enhance the efficiency and accuracy of information retrieval from pharmacovigilance (PV) databases by employing Large Language Models (LLMs) to convert natural language queries (NLQs) into Structured Query Language (SQL) queries, leveraging a business context document.   Materials and Methods: We utilized OpenAI's GPT-4 model within a retrieval-augmented generation (RAG) framework, enriched with a business context document, to transform NLQs into syntactically precise SQL queries. Each NLQ was presented to the LLM randomly and independently to prevent memorization. The study was conducted in three phases, varying query complexity, and assessing the LLM's performance both with and without the business context document.   Results: Our approach significantly improved NLQ-to-SQL accuracy, increasing from 8.3\% with the database schema alone to 78.3\% with the business context document. This enhancement was consistent across low, medium, and high complexity queries, indicating the critical role of contextual knowledge in query generation.   Discussion: The integration of a business context document markedly improved the LLM's ability to generate accurate and contextually relevant SQL queries. Performance achieved a maximum of 85\% when high complexity queries are excluded, suggesting promise for routine deployment.   Conclusion: This study presents a novel approach to employing LLMs for safety data retrieval and analysis, demonstrating significant advancements in query generation accuracy. The methodology offers a framework applicable to various data-intensive domains, enhancing the accessibility and efficiency of information retrieval for non-technical users.

**Link**: [arxiv](http://arxiv.org/abs/2406.10690v3),  [pdf](http://arxiv.org/pdf/2406.10690v3)

**Tags**: cs.AI cs.DB H.3.3; I.2.7 



### The Design of an LLM-powered Unstructured Analytics System
**Authors**: Eric Anderson, Jonathan Fritz, Austin Lee, Bohou Li, Mark Lindblad, Henry Lindeman, Alex Meyer, Parth Parmar, Tanvi Ranade, Mehul A. Shah, Benjamin Sowell, Dan Tecuci, Vinayak Thapliyal, Matt Welsh

**Updated**: 2024-09-04T16:39:22Z

**Summary**: LLMs demonstrate an uncanny ability to process unstructured data, and as such, have the potential to go beyond search and run complex, semantic analyses at scale. We describe the design of an unstructured analytics system, Aryn, and the tenets and use cases that motivate its design. With Aryn, users can specify queries in natural language and the system automatically determines a semantic plan and executes it to compute an answer from a large collection of unstructured documents using LLMs. At the core of Aryn is Sycamore, a declarative document processing engine, built using Ray, that provides a reliable distributed abstraction called DocSets. Sycamore allows users to analyze, enrich, and transform complex documents at scale. Aryn also comprises Luna, a query planner that translates natural language queries to Sycamore scripts, and the Aryn Partitioner, which takes raw PDFs and document images, and converts them to DocSets for downstream processing. Using Aryn, we demonstrate a real world use case for analyzing accident reports from the National Transportation Safety Board (NTSB), and discuss some of the major challenges we encountered in deploying Aryn in the wild.

**Link**: [arxiv](http://arxiv.org/abs/2409.00847v2),  [pdf](http://arxiv.org/pdf/2409.00847v2)

**Tags**: cs.DB cs.AI cs.IR 



### Anomaly Detection in Offshore Open Radio Access Network Using Long   Short-Term Memory Models on a Novel Artificial Intelligence-Driven   Cloud-Native Data Platform
**Authors**: Abdelrahim Ahmad, Peizheng Li, Robert Piechocki, Rui Inacio

**Updated**: 2024-09-04T16:19:55Z

**Summary**: The radio access network (RAN) is a critical component of modern telecom infrastructure, currently undergoing significant transformation towards disaggregated and open architectures. These advancements are pivotal for integrating intelligent, data-driven applications aimed at enhancing network reliability and operational autonomy through the introduction of cognition capabilities, exemplified by the set of enhancements proposed by the emerging Open radio access network (O-RAN) standards. Despite its potential, the nascent nature of O-RAN technology presents challenges, primarily due to the absence of mature operational standards. This complicates the management of data and applications, particularly in integrating with traditional network management and operational support systems. Divergent vendor-specific design approaches further hinder migration and limit solution reusability. Addressing the skills gap in telecom business-oriented engineering is crucial for the effective deployment of O-RAN and the development of robust data-driven applications. To address these challenges, Boldyn Networks, a global Neutral Host provider, has implemented a novel cloud-native data analytics platform. This platform underwent rigorous testing in real-world scenarios of using advanced artificial intelligence (AI) techniques, significantly improving operational efficiency, and enhancing customer experience. Implementation involved adopting development operations (DevOps) practices, leveraging data lakehouse architectures tailored for AI applications, and employing sophisticated data engineering strategies. The platform successfully addresses connectivity challenges inherent in offshore windfarm deployments using long short-term memory (LSTM) Models for anomaly detection of the connectivity, providing detailed insights into its specialized architecture developed for this purpose.

**Link**: [arxiv](http://arxiv.org/abs/2409.02849v1),  [pdf](http://arxiv.org/pdf/2409.02849v1)

**Tags**: cs.NI 



### SNNAX -- Spiking Neural Networks in JAX
**Authors**: Jamie Lohoff, Jan Finkbeiner, Emre Neftci

**Updated**: 2024-09-04T16:14:14Z

**Summary**: Spiking Neural Networks (SNNs) simulators are essential tools to prototype biologically inspired models and neuromorphic hardware architectures and predict their performance. For such a tool, ease of use and flexibility are critical, but so is simulation speed especially given the complexity inherent to simulating SNN. Here, we present SNNAX, a JAX-based framework for simulating and training such models with PyTorch-like intuitiveness and JAX-like execution speed. SNNAX models are easily extended and customized to fit the desired model specifications and target neuromorphic hardware. Additionally, SNNAX offers key features for optimizing the training and deployment of SNNs such as flexible automatic differentiation and just-in-time compilation. We evaluate and compare SNNAX to other commonly used machine learning (ML) frameworks used for programming SNNs. We provide key performance metrics, best practices, documented examples for simulating SNNs in SNNAX, and implement several benchmarks used in the literature.

**Link**: [arxiv](http://arxiv.org/abs/2409.02842v1),  [pdf](http://arxiv.org/pdf/2409.02842v1)

**Tags**: cs.NE cs.LG 



### Simple and Scalable Strategies to Continually Pre-train Large Language   Models
**Authors**: Adam Ibrahim, Benjamin Th√©rien, Kshitij Gupta, Mats L. Richter, Quentin Anthony, Timoth√©e Lesort, Eugene Belilovsky, Irina Rish

**Updated**: 2024-09-04T16:13:18Z

**Summary**: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by the final loss and the average score on several language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at the $405$M parameter model scale with large dataset sizes (hundreds of billions of tokens). Selecting the weak but realistic shift for larger-scale experiments, we also find that our continual learning strategies match the re-training baseline for a 10B parameter LLM. Our results demonstrate that LLMs can be successfully updated via simple and scalable continual learning strategies, matching the re-training baseline using only a fraction of the compute. Finally, inspired by previous work, we propose alternatives to the cosine learning rate schedule that help circumvent forgetting induced by LR re-warming and that are not bound to a fixed token budget.

**Link**: [arxiv](http://arxiv.org/abs/2403.08763v4),  [pdf](http://arxiv.org/pdf/2403.08763v4)

**Tags**: cs.LG cs.AI cs.CL 



### J√§ger: Automated Telephone Call Traceback
**Authors**: David Adei, Varun Madathil, Sathvik Prasad, Bradley Reaves, Alessandra Scafuro

**Updated**: 2024-09-04T16:09:28Z

**Summary**: Unsolicited telephone calls that facilitate fraud or unlawful telemarketing continue to overwhelm network users and the regulators who prosecute them. The first step in prosecuting phone abuse is traceback -- identifying the call originator. This fundamental investigative task currently requires hours of manual effort per call. In this paper, we introduce J\"ager, a distributed secure call traceback system. J\"ager can trace a call in a few seconds, even with partial deployment, while cryptographically preserving the privacy of call parties, carrier trade secrets like peers and call volume, and limiting the threat of bulk analysis. We establish definitions and requirements of secure traceback, then develop a suite of protocols that meet these requirements using witness encryption, oblivious pseudorandom functions, and group signatures. We prove these protocols secure in the universal composibility framework. We then demonstrate that J\"ager has low compute and bandwidth costs per call, and these costs scale linearly with call volume. J\"ager provides an efficient, secure, privacy-preserving system to revolutionize telephone abuse investigation with minimal costs to operators.

**Link**: [arxiv](http://arxiv.org/abs/2409.02839v1),  [pdf](http://arxiv.org/pdf/2409.02839v1)

**Tags**: cs.CR cs.CY cs.NI 



### CMM-Math: A Chinese Multimodal Math Dataset To Evaluate and Enhance the   Mathematics Reasoning of Large Multimodal Models
**Authors**: Wentao Liu, Qianjun Pan, Yi Zhang, Zhuo Liu, Ji Wu, Jie Zhou, Aimin Zhou, Qin Chen, Bo Jiang, Liang He

**Updated**: 2024-09-04T16:00:21Z

**Summary**: Large language models (LLMs) have obtained promising results in mathematical reasoning, which is a foundational skill for human intelligence. Most previous studies focus on improving and measuring the performance of LLMs based on textual math reasoning datasets (e.g., MATH, GSM8K). Recently, a few researchers have released English multimodal math datasets (e.g., MATHVISTA and MATH-V) to evaluate the effectiveness of large multimodal models (LMMs). In this paper, we release a Chinese multimodal math (CMM-Math) dataset, including benchmark and training parts, to evaluate and enhance the mathematical reasoning of LMMs. CMM-Math contains over 28,000 high-quality samples, featuring a variety of problem types (e.g., multiple-choice, fill-in-the-blank, and so on) with detailed solutions across 12 grade levels from elementary to high school in China. Specifically, the visual context may be present in the questions or opinions, which makes this dataset more challenging. Through comprehensive analysis, we discover that state-of-the-art LMMs on the CMM-Math dataset face challenges, emphasizing the necessity for further improvements in LMM development. We also propose a Multimodal Mathematical LMM (Math-LMM) to handle the problems with mixed input of multiple images and text segments. We train our model using three stages, including foundational pre-training, foundational fine-tuning, and mathematical fine-tuning. The extensive experiments indicate that our model effectively improves math reasoning performance by comparing it with the SOTA LMMs over three multimodal mathematical datasets.

**Link**: [arxiv](http://arxiv.org/abs/2409.02834v1),  [pdf](http://arxiv.org/pdf/2409.02834v1)

**Tags**: cs.CL 



### LongRecipe: Recipe for Efficient Long Context Generalization in Large   Language Models
**Authors**: Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh Tuan Luu, See-Kiong Ng, Zhiwei Jiang, Bryan Hooi

**Updated**: 2024-09-04T15:55:22Z

**Summary**: Large language models (LLMs) face significant challenges in handling long-context tasks because of their limited effective context window size during pretraining, which restricts their ability to generalize over extended sequences. Meanwhile, extending the context window in LLMs through post-pretraining is highly resource-intensive. To address this, we introduce LongRecipe, an efficient training strategy for extending the context window of LLMs, including impactful token analysis, position index transformation, and training optimization strategies. It simulates long-sequence inputs while maintaining training efficiency and significantly improves the model's understanding of long-range dependencies. Experiments on three types of LLMs show that LongRecipe can utilize long sequences while requiring only 30% of the target context window size, and reduces computational training resource over 85% compared to full sequence training. Furthermore, LongRecipe also preserves the original LLM's capabilities in general tasks. Ultimately, we can extend the effective context window of open-source LLMs from 8k to 128k, achieving performance close to GPT-4 with just one day of dedicated training using a single GPU with 80G memory. Our code is released at https://github.com/zhiyuanhubj/LongRecipe.

**Link**: [arxiv](http://arxiv.org/abs/2409.00509v2),  [pdf](http://arxiv.org/pdf/2409.00509v2)

**Tags**: cs.CL 



### Design Contradictions: Help or Hindrance?
**Authors**: Aron E. Owen, Jonathan C. Roberts

**Updated**: 2024-09-04T15:42:59Z

**Summary**: The need for innovative ideas in data visualisation drives us to explore new creative approaches. Combining two or more creative words, particularly those that contradict each other, can positively impact the creative process, sparking novel ideas and designs. As we move towards AI-driven design, an open question arises: do these design contradictions work positively with AI tools? Currently, the answer is no. AI systems, like large language models (LLMs), rely on algorithms that engender similarity, whereas creativity often requires divergence and novelty. This poster initiates a conversation on how to drive AI systems to be more creative and generate new ideas. This research invites us to reconsider traditional design methods and explore new approaches in an AI-driven world. Can we apply the same techniques used in traditional design, like the double diamond model, or do we need new methods for design engineering? How can we quickly design visualisations and craft new ideas with generative AI? This paper seeks to start this critical conversation and offers practical insights into the potential of AI in driving creativity in data visualisation.

**Link**: [arxiv](http://arxiv.org/abs/2409.02823v1),  [pdf](http://arxiv.org/pdf/2409.02823v1)

**Tags**: cs.HC 



### Language Understanding as a Constraint on Consensus Size in LLM   Societies
**Authors**: Giordano De Marzo, Claudio Castellano, David Garcia

**Updated**: 2024-09-04T15:42:29Z

**Summary**: The applications of Large Language Models (LLMs) are going towards collaborative tasks where several agents interact with each other like in an LLM society. In such a setting, large groups of LLMs could reach consensus about arbitrary norms for which there is no information supporting one option over another, regulating their own behavior in a self-organized way. In human societies, the ability to reach consensus without institutions has a limit in the cognitive capacities of humans. To understand if a similar phenomenon characterizes also LLMs, we apply methods from complexity science and principles from behavioral sciences in a new approach of AI anthropology. We find that LLMs are able to reach consensus in groups and that the opinion dynamics of LLMs can be understood with a function parametrized by a majority force coefficient that determines whether consensus is possible. This majority force is stronger for models with higher language understanding capabilities and decreases for larger groups, leading to a critical group size beyond which, for a given LLM, consensus is unfeasible. This critical group size grows exponentially with the language understanding capabilities of models and for the most advanced models, it can reach an order of magnitude beyond the typical size of informal human groups.

**Link**: [arxiv](http://arxiv.org/abs/2409.02822v1),  [pdf](http://arxiv.org/pdf/2409.02822v1)

**Tags**: physics.soc-ph 



### The Future of Open Human Feedback
**Authors**: Shachar Don-Yehiya, Ben Burtenshaw, Ramon Fernandez Astudillo, Cailean Osborne, Mimansa Jaiswal, Tzu-Sheng Kuo, Wenting Zhao, Idan Shenfeld, Andi Peng, Mikhail Yurochkin, Atoosa Kasirzadeh, Yangsibo Huang, Tatsunori Hashimoto, Yacine Jernite, Daniel Vila-Suero, Omri Abend, Jennifer Ding, Sara Hooker, Hannah Rose Kirk, Leshem Choshen

**Updated**: 2024-09-04T15:39:47Z

**Summary**: Human feedback on conversations with language language models (LLMs) is central to how these systems learn about the world, improve their capabilities, and are steered toward desirable and safe behaviors. However, this feedback is mostly collected by frontier AI labs and kept behind closed doors. In this work, we bring together interdisciplinary experts to assess the opportunities and challenges to realizing an open ecosystem of human feedback for AI. We first look for successful practices in peer production, open source, and citizen science communities. We then characterize the main challenges for open human feedback. For each, we survey current approaches and offer recommendations. We end by envisioning the components needed to underpin a sustainable and open human feedback ecosystem. In the center of this ecosystem are mutually beneficial feedback loops, between users and specialized models, incentivizing a diverse stakeholders community of model trainers and feedback providers to support a general open feedback pool.

**Link**: [arxiv](http://arxiv.org/abs/2408.16961v2),  [pdf](http://arxiv.org/pdf/2408.16961v2)

**Tags**: cs.HC cs.AI 



### Obsidian: Cooperative State-Space Exploration for Performant Inference   on Secure ML Accelerators
**Authors**: Sarbartha Banerjee, Shijia Wei, Prakash Ramrakhyani, Mohit Tiwari

**Updated**: 2024-09-04T15:35:18Z

**Summary**: Trusted execution environments (TEEs) for machine learning accelerators are indispensable in secure and efficient ML inference. Optimizing workloads through state-space exploration for the accelerator architectures improves performance and energy consumption. However, such explorations are expensive and slow due to the large search space. Current research has to use fast analytical models that forego critical hardware details and cross-layer opportunities unique to the hardware security primitives. While cycle-accurate models can theoretically reach better designs, their high runtime cost restricts them to a smaller state space.   We present Obsidian, an optimization framework for finding the optimal mapping from ML kernels to a secure ML accelerator. Obsidian addresses the above challenge by exploring the state space using analytical and cycle-accurate models cooperatively. The two main exploration components include: (1) A secure accelerator analytical model, that includes the effect of secure hardware while traversing the large mapping state space and produce the best m model mappings; (2) A compiler profiling step on a cycle-accurate model, that captures runtime bottlenecks to further improve execution runtime, energy and resource utilization and find the optimal model mapping.   We compare our results to a baseline secure accelerator, comprising of the state-of-the-art security schemes obtained from guardnn [ 33 ] and sesame [11]. The analytical model reduces the inference latency by 20.5% for a cloud and 8.4% for an edge deployment with an energy improvement of 24% and 19% respectively. The cycle-accurate model, further reduces the latency by 9.1% for a cloud and 12.2% for an edge with an energy improvement of 13.8% and 13.1%.

**Link**: [arxiv](http://arxiv.org/abs/2409.02817v1),  [pdf](http://arxiv.org/pdf/2409.02817v1)

**Tags**: cs.CR cs.LG 



### LogicGame: Benchmarking Rule-Based Reasoning Abilities of Large Language   Models
**Authors**: Jiayi Gui, Yiming Liu, Jiale Cheng, Xiaotao Gu, Xiao Liu, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang

**Updated**: 2024-09-05T10:30:39Z

**Summary**: Large Language Models (LLMs) have demonstrated notable capabilities across various tasks, showcasing complex problem-solving abilities. Understanding and executing complex rules, along with multi-step planning, are fundamental to logical reasoning and critical for practical LLM agents and decision-making systems. However, evaluating LLMs as effective rule-based executors and planners remains underexplored. In this paper, we introduce LogicGame, a novel benchmark designed to evaluate the comprehensive rule understanding, execution, and planning capabilities of LLMs. Unlike traditional benchmarks, LogicGame provides diverse games that contain a series of rules with an initial state, requiring models to comprehend and apply predefined regulations to solve problems. We create simulated scenarios in which models execute or plan operations to achieve specific outcomes. These game scenarios are specifically designed to distinguish logical reasoning from mere knowledge by relying exclusively on predefined rules. This separation allows for a pure assessment of rule-based reasoning capabilities. The evaluation considers not only final outcomes but also intermediate steps, providing a comprehensive assessment of model performance. Moreover, these intermediate steps are deterministic and can be automatically verified. LogicGame defines game scenarios with varying difficulty levels, from simple rule applications to complex reasoning chains, in order to offer a precise evaluation of model performance on rule understanding and multi-step execution. Utilizing LogicGame, we test various LLMs and identify notable shortcomings in their rule-based logical reasoning abilities.

**Link**: [arxiv](http://arxiv.org/abs/2408.15778v3),  [pdf](http://arxiv.org/pdf/2408.15778v3)

**Tags**: cs.AI cs.CL 



### Towards a Unified View of Preference Learning for Large Language Models:   A Survey
**Authors**: Bofei Gao, Feifan Song, Yibo Miao, Zefan Cai, Zhe Yang, Liang Chen, Helan Hu, Runxin Xu, Qingxiu Dong, Ce Zheng, Wen Xiao, Ge Zhang, Daoguang Zan, Keming Lu, Bowen Yu, Dayiheng Liu, Zeyu Cui, Jian Yang, Lei Sha, Houfeng Wang, Zhifang Sui, Peiyi Wang, Tianyu Liu, Baobao Chang

**Updated**: 2024-09-04T15:11:55Z

**Summary**: Large Language Models (LLMs) exhibit remarkably powerful capabilities. One of the crucial factors to achieve success is aligning the LLM's output with human preferences. This alignment process often requires only a small amount of data to efficiently enhance the LLM's performance. While effective, research in this area spans multiple domains, and the methods involved are relatively complex to understand. The relationships between different methods have been under-explored, limiting the development of the preference alignment. In light of this, we break down the existing popular alignment strategies into different components and provide a unified framework to study the current alignment strategies, thereby establishing connections among them. In this survey, we decompose all the strategies in preference learning into four components: model, data, feedback, and algorithm. This unified view offers an in-depth understanding of existing alignment algorithms and also opens up possibilities to synergize the strengths of different strategies. Furthermore, we present detailed working examples of prevalent existing algorithms to facilitate a comprehensive understanding for the readers. Finally, based on our unified perspective, we explore the challenges and future research directions for aligning large language models with human preferences.

**Link**: [arxiv](http://arxiv.org/abs/2409.02795v1),  [pdf](http://arxiv.org/pdf/2409.02795v1)

**Tags**: cs.CL 



### Negation Blindness in Large Language Models: Unveiling the NO Syndrome   in Image Generation
**Authors**: Mohammad Nadeem, Shahab Saquib Sohail, Erik Cambria, Bj√∂rn W. Schuller, Amir Hussain

**Updated**: 2024-09-04T14:40:14Z

**Summary**: Foundational Large Language Models (LLMs) have changed the way we perceive technology. They have been shown to excel in tasks ranging from poem writing and coding to essay generation and puzzle solving. With the incorporation of image generation capability, they have become more comprehensive and versatile AI tools. At the same time, researchers are striving to identify the limitations of these tools to improve them further. Currently identified flaws include hallucination, biases, and bypassing restricted commands to generate harmful content. In the present work, we have identified a fundamental limitation related to the image generation ability of LLMs, and termed it The NO Syndrome. This negation blindness refers to LLMs inability to correctly comprehend NO related natural language prompts to generate the desired images. Interestingly, all tested LLMs including GPT-4, Gemini, and Copilot were found to be suffering from this syndrome. To demonstrate the generalization of this limitation, we carried out simulation experiments and conducted entropy-based and benchmark statistical analysis tests on various LLMs in multiple languages, including English, Hindi, and French. We conclude that the NO syndrome is a significant flaw in current LLMs that needs to be addressed. A related finding of this study showed a consistent discrepancy between image and textual responses as a result of this NO syndrome. We posit that the introduction of a negation context-aware reinforcement learning based feedback loop between the LLMs textual response and generated image could help ensure the generated text is based on both the LLMs correct contextual understanding of the negation query and the generated visual output.

**Link**: [arxiv](http://arxiv.org/abs/2409.00105v2),  [pdf](http://arxiv.org/pdf/2409.00105v2)

**Tags**: cs.CL cs.AI cs.LG 



### Seeing Like an AI: How LLMs Apply (and Misapply) Wikipedia Neutrality   Norms
**Authors**: Joshua Ashkinaze, Ruijia Guan, Laura Kurek, Eytan Adar, Ceren Budak, Eric Gilbert

**Updated**: 2024-09-04T14:07:07Z

**Summary**: Large language models (LLMs) are trained on broad corpora and then used in communities with specialized norms. Is providing LLMs with community rules enough for models to follow these norms? We evaluate LLMs' capacity to detect (Task 1) and correct (Task 2) biased Wikipedia edits according to Wikipedia's Neutral Point of View (NPOV) policy. LLMs struggled with bias detection, achieving only 64% accuracy on a balanced dataset. Models exhibited contrasting biases (some under- and others over-predicted bias), suggesting distinct priors about neutrality. LLMs performed better at generation, removing 79% of words removed by Wikipedia editors. However, LLMs made additional changes beyond Wikipedia editors' simpler neutralizations, resulting in high-recall but low-precision editing. Interestingly, crowdworkers rated AI rewrites as more neutral (70%) and fluent (61%) than Wikipedia-editor rewrites. Qualitative analysis found LLMs sometimes applied NPOV more comprehensively than Wikipedia editors but often made extraneous non-NPOV-related changes (such as grammar). LLMs may apply rules in ways that resonate with the public but diverge from community experts. While potentially effective for generation, LLMs may reduce editor agency and increase moderation workload (e.g., verifying additions). Even when rules are easy to articulate, having LLMs apply them like community members may still be difficult.

**Link**: [arxiv](http://arxiv.org/abs/2407.04183v2),  [pdf](http://arxiv.org/pdf/2407.04183v2)

**Tags**: cs.CL cs.AI cs.CY cs.HC 



### Pooling And Attention: What Are Effective Designs For LLM-Based   Embedding Models?
**Authors**: Yixuan Tang, Yi Yang

**Updated**: 2024-09-05T07:17:59Z

**Summary**: The significant advancements of Large Language Models (LLMs) in generative tasks have led to a growing body of work exploring LLM-based embedding models. While these models, employing different pooling and attention strategies, have achieved state-of-the-art performance on public embedding benchmarks, questions still arise about what constitutes an effective design for LLM-based embedding models. However, these models are often trained on different datasets, using different LLM base models or training settings. Moreover, evaluations on public embedding benchmarks often fail to report statistical significance, making it difficult to determine which designs truly contribute to final performance. This complicates the process for practitioners seeking optimal training recipes for LLM-based embedding models. In this study, we conduct a large-scale experiment by training a series of LLM-based embedding models using the same training data and base model but differing in their pooling and attention strategies. The results show that there is no one-size-fits-all solution: while bidirectional attention and an additional trainable pooling layer outperform in text similarity and information retrieval tasks, they do not significantly surpass simpler designs like EOS-last token pooling and default causal attention in clustering and classification tasks. Furthermore, we propose a new pooling strategy, Multi-Layers Trainable Pooling, which transforms the outputs of all hidden layers, rather than just the last layer, using a cross-attention network. This method proves to be statistically superior in text similarity and retrieval tasks compared to existing pooling methods. Overall, this paper sheds light on effective training strategies for LLM-based embedding models.

**Link**: [arxiv](http://arxiv.org/abs/2409.02727v2),  [pdf](http://arxiv.org/pdf/2409.02727v2)

**Tags**: cs.CL cs.IR 



### Alignment-Aware Model Extraction Attacks on Large Language Models
**Authors**: Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu

**Updated**: 2024-09-04T13:54:38Z

**Summary**: Model extraction attacks (MEAs) on large language models (LLMs) have received increasing research attention lately. Existing attack methods on LLMs inherit the extraction strategies from those designed for deep neural networks (DNNs) yet neglect the inconsistency of training tasks between MEA and LLMs' alignments. As such, they result in poor attack performances. To tackle this issue, we present Locality Reinforced Distillation (LoRD), a novel model extraction attack algorithm specifically for LLMs. In particular, we design a policy-gradient-style training task, which utilizes victim models' responses as a signal to guide the crafting of preference for the local model. Theoretical analysis has shown that i) LoRD's convergence procedure in MEAs is consistent with the alignments of LLMs, and ii) LoRD can reduce query complexity while mitigating watermark protection through exploration-based stealing. Extensive experiments on domain-specific extractions demonstrate the superiority of our method by examining the extraction of various state-of-the-art commercial LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.02718v1),  [pdf](http://arxiv.org/pdf/2409.02718v1)

**Tags**: cs.CR cs.CL 



### Creating a Gen-AI based Track and Trace Assistant MVP (SuperTracy) for   PostNL
**Authors**: Mohammad Reshadati

**Updated**: 2024-09-04T13:49:19Z

**Summary**: The developments in the field of generative AI has brought a lot of opportunities for companies, for instance to improve efficiency in customer service and automating tasks. PostNL, the biggest parcel and E-commerce corporation of the Netherlands wants to use generative AI to enhance the communication around track and trace of parcels. During the internship a Minimal Viable Product (MVP) is created to showcase the value of using generative AI technologies, to enhance parcel tracking, analyzing the parcel's journey and being able to communicate about it in an easy to understand manner. The primary goal was to develop an in-house LLM-based system, reducing dependency on external platforms and establishing the feasibility of a dedicated generative AI team within the company. This multi-agent LLM based system aimed to construct parcel journey stories and identify logistical disruptions with heightened efficiency and accuracy. The research involved deploying a sophisticated AI-driven communication system, employing Retrieval-Augmented Generation (RAG) for enhanced response precision, and optimizing large language models (LLMs) tailored to domain specific tasks.   The MVP successfully implemented a multi-agent open-source LLM system, called SuperTracy. SuperTracy is capable of autonomously managing a broad spectrum of user inquiries and improving internal knowledge handling. Results and evaluation demonstrated technological innovation and feasibility, notably in communication about the track and trace of a parcel, which exceeded initial expectations. These advancements highlight the potential of AI-driven solutions in logistics, suggesting many opportunities for further refinement and broader implementation within PostNL operational framework.

**Link**: [arxiv](http://arxiv.org/abs/2409.02711v1),  [pdf](http://arxiv.org/pdf/2409.02711v1)

**Tags**: cs.AI 



### CLDA: Collaborative Learning for Enhanced Unsupervised Domain Adaptation
**Authors**: Minhee Cho, Hyesong Choi, Hayeon Jo, Dongbo Min

**Updated**: 2024-09-04T13:35:15Z

**Summary**: Unsupervised Domain Adaptation (UDA) endeavors to bridge the gap between a model trained on a labeled source domain and its deployment in an unlabeled target domain. However, current high-performance models demand significant resources, resulting in prohibitive deployment costs and highlighting the need for small yet effective models. For UDA of lightweight models, Knowledge Distillation (KD) in a Teacher-Student framework can be a common approach, but we find that domain shift in UDA leads to a significant increase in non-salient parameters in the teacher model, degrading model's generalization ability and transferring misleading information to the student model. Interestingly, we observed that this phenomenon occurs considerably less in the student model. Driven by this insight, we introduce Collaborative Learning, a method that updates the teacher's non-salient parameters using the student model and at the same time enhance the student's performance using the updated teacher model. Experiments across various tasks and datasets show consistent performance improvements for both student and teacher models. For example, in semantic segmentation, CLDA achieves an improvement of +0.7% mIoU for teacher and +1.4% mIoU for student compared to the baseline model in the GTA to Cityscapes. In the Synthia to Cityscapes, it achieves an improvement of +0.8% mIoU for teacher and +2.0% mIoU for student.

**Link**: [arxiv](http://arxiv.org/abs/2409.02699v1),  [pdf](http://arxiv.org/pdf/2409.02699v1)

**Tags**: cs.CV 



### A Causal Explainable Guardrails for Large Language Models
**Authors**: Zhixuan Chu, Yan Wang, Longfei Li, Zhibo Wang, Zhan Qin, Kui Ren

**Updated**: 2024-09-04T13:29:56Z

**Summary**: Large Language Models (LLMs) have shown impressive performance in natural language tasks, but their outputs can exhibit undesirable attributes or biases. Existing methods for steering LLMs toward desired attributes often assume unbiased representations and rely solely on steering prompts. However, the representations learned from pre-training can introduce semantic biases that influence the steering process, leading to suboptimal results. We propose LLMGuardrail, a novel framework that incorporates causal analysis and adversarial learning to obtain unbiased steering representations in LLMs. LLMGuardrail systematically identifies and blocks the confounding effects of biases, enabling the extraction of unbiased steering representations. Additionally, it includes an explainable component that provides insights into the alignment between the generated output and the desired direction. Experiments demonstrate LLMGuardrail's effectiveness in steering LLMs toward desired attributes while mitigating biases. Our work contributes to the development of safe and reliable LLMs that align with desired attributes.

**Link**: [arxiv](http://arxiv.org/abs/2405.04160v2),  [pdf](http://arxiv.org/pdf/2405.04160v2)

**Tags**: cs.CL 



### LLM-Assisted Visual Analytics: Opportunities and Challenges
**Authors**: Maeve Hutchinson, Radu Jianu, Aidan Slingsby, Pranava Madhyastha

**Updated**: 2024-09-04T13:24:03Z

**Summary**: We explore the integration of large language models (LLMs) into visual analytics (VA) systems to transform their capabilities through intuitive natural language interactions. We survey current research directions in this emerging field, examining how LLMs are integrated into data management, language interaction, visualisation generation, and language generation processes. We highlight the new possibilities that LLMs bring to VA, especially how they can change VA processes beyond the usual use cases. We especially highlight building new visualisation-language models, allowing access of a breadth of domain knowledge, multimodal interaction, and opportunities with guidance. Finally, we carefully consider the prominent challenges of using current LLMs in VA tasks. Our discussions in this paper aim to guide future researchers working on LLM-assisted VA systems and help them navigate common obstacles when developing these systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.02691v1),  [pdf](http://arxiv.org/pdf/2409.02691v1)

**Tags**: cs.HC cs.AI 



### Deconfounded Causality-aware Parameter-Efficient Fine-Tuning for   Problem-Solving Improvement of LLMs
**Authors**: Ruoyu Wang, Xiaoxuan Li, Lina Yao

**Updated**: 2024-09-04T13:17:09Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable efficiency in tackling various tasks based on human instructions, but recent studies reveal that these models often fail to achieve satisfactory results on questions involving reasoning, such as mathematics or physics questions. This phenomenon is usually attributed to the uncertainty regarding whether these models could genuinely comprehend the knowledge embedded in the text or merely learn to replicate the token distribution without a true understanding of the content. In this paper, we delve into this problem and aim to enhance the reasoning capabilities of LLMs. First, we investigate if the model has genuine reasoning capabilities by visualizing the text generation process at the attention and representation level. Then, we formulate the reasoning process of LLMs into a causal framework, which provides a formal explanation of the problems we observe in the visualization. Finally, building upon this causal framework, we propose Deconfounded Causal Adaptation (DCA), a novel parameter-efficient fine-tuning (PEFT) method to enhance the model's reasoning capabilities by encouraging the model to extract the general problem-solving skills and apply these skills to different questions. Experiments show that our method outperforms the baseline consistently across multiple benchmarks, and with only 1.2M tunable parameters, we achieve better or comparable results to other fine-tuning methods. This demonstrates the effectiveness and efficiency of our method in improving the overall accuracy and reliability of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.02686v1),  [pdf](http://arxiv.org/pdf/2409.02686v1)

**Tags**: cs.CL cs.AI cs.LG 



### Parallel Speculative Decoding with Adaptive Draft Length
**Authors**: Tianyu Liu, Yun Li, Qitan Lv, Kai Liu, Jianchen Zhu, Winston Hu

**Updated**: 2024-09-04T13:14:57Z

**Summary**: Speculative decoding (SD), where an extra draft model is employed to provide multiple \textit{draft} tokens first and then the original target model verifies these tokens in parallel, has shown great power for LLM inference acceleration. However, existing SD methods suffer from the mutual waiting problem, i.e., the target model gets stuck when the draft model is \textit{guessing} tokens, and vice versa. This problem is directly incurred by the asynchronous execution of the draft model and the target model, and is exacerbated due to the fixed draft length in speculative decoding. To address these challenges, we propose a conceptually simple, flexible, and general framework to boost speculative decoding, namely \textbf{P}arallel sp\textbf{E}culative decoding with \textbf{A}daptive d\textbf{R}aft \textbf{L}ength (PEARL). Specifically, PEARL proposes \textit{pre-verify} to verify the first draft token in advance during the drafting phase, and \textit{post-verify} to generate more draft tokens during the verification phase. PEARL parallels the drafting phase and the verification phase via applying the two strategies, and achieves adaptive draft length for different scenarios, which effectively alleviates the mutual waiting problem. Moreover, we theoretically demonstrate that the mean accepted tokens of PEARL is more than existing \textit{draft-then-verify} works. Experiments on various text generation benchmarks demonstrate the effectiveness of our \name, leading to a superior speedup performance up to \textbf{3.79$\times$} and \textbf{1.52$\times$}, compared to auto-regressive decoding and vanilla speculative decoding, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2408.11850v2),  [pdf](http://arxiv.org/pdf/2408.11850v2)

**Tags**: cs.CL 



### Nickel and Diming Your GAN: A Dual-Method Approach to Enhancing GAN   Efficiency via Knowledge Distillation
**Authors**: Sangyeop Yeo, Yoojin Jang, Jaejun Yoo

**Updated**: 2024-09-04T13:02:15Z

**Summary**: In this paper, we address the challenge of compressing generative adversarial networks (GANs) for deployment in resource-constrained environments by proposing two novel methodologies: Distribution Matching for Efficient compression (DiME) and Network Interactive Compression via Knowledge Exchange and Learning (NICKEL). DiME employs foundation models as embedding kernels for efficient distribution matching, leveraging maximum mean discrepancy to facilitate effective knowledge distillation. Simultaneously, NICKEL employs an interactive compression method that enhances the communication between the student generator and discriminator, achieving a balanced and stable compression process. Our comprehensive evaluation on the StyleGAN2 architecture with the FFHQ dataset shows the effectiveness of our approach, with NICKEL & DiME achieving FID scores of 10.45 and 15.93 at compression rates of 95.73% and 98.92%, respectively. Remarkably, our methods sustain generative quality even at an extreme compression rate of 99.69%, surpassing the previous state-of-the-art performance by a large margin. These findings not only demonstrate our methodologies' capacity to significantly lower GANs' computational demands but also pave the way for deploying high-quality GAN models in settings with limited resources. Our code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2405.11614v2),  [pdf](http://arxiv.org/pdf/2405.11614v2)

**Tags**: cs.CV eess.IV 



### HIRO: Hierarchical Information Retrieval Optimization
**Authors**: Krish Goel, Mahek Chandak

**Updated**: 2024-09-04T12:33:24Z

**Summary**: Retrieval-Augmented Generation (RAG) has revolutionized natural language processing by dynamically integrating external knowledge into Large Language Models (LLMs), addressing their limitation of static training datasets. Recent implementations of RAG leverage hierarchical data structures, which organize documents at various levels of summarization and information density. This complexity, however, can cause LLMs to "choke" on information overload, necessitating more sophisticated querying mechanisms. In this context, we introduce Hierarchical Information Retrieval Optimization (HIRO), a novel querying approach that employs a Depth-First Search (DFS)-based recursive similarity score calculation and branch pruning. This method uniquely minimizes the context delivered to the LLM without informational loss, effectively managing the challenge of excessive data. HIRO's refined approach is validated by a 10.85% improvement in performance on the NarrativeQA dataset.

**Link**: [arxiv](http://arxiv.org/abs/2406.09979v2),  [pdf](http://arxiv.org/pdf/2406.09979v2)

**Tags**: cs.CL cs.AI cs.IR 



### Graph Retrieval Augmented Trustworthiness Reasoning
**Authors**: Ying Zhu, Shengchang Li, Ziqian Kong, Peilan Xu

**Updated**: 2024-09-04T12:00:25Z

**Summary**: Trustworthiness reasoning is crucial in multiplayer games with incomplete information, enabling agents to identify potential allies and adversaries, thereby enhancing reasoning and decision-making processes. Traditional approaches relying on pre-trained models necessitate extensive domain-specific data and considerable reward feedback, with their lack of real-time adaptability hindering their effectiveness in dynamic environments. In this paper, we introduce the Graph Retrieval Augmented Reasoning (GRATR) framework, leveraging the Retrieval-Augmented Generation (RAG) technique to bolster trustworthiness reasoning in agents. GRATR constructs a dynamic trustworthiness graph, updating it in real-time with evidential information, and retrieves relevant trust data to augment the reasoning capabilities of Large Language Models (LLMs). We validate our approach through experiments on the multiplayer game "Werewolf," comparing GRATR against baseline LLM and LLM enhanced with Native RAG and Rerank RAG. Our results demonstrate that GRATR surpasses the baseline methods by over 30\% in winning rate, with superior reasoning performance. Moreover, GRATR effectively mitigates LLM hallucinations, such as identity and objective amnesia, and crucially, it renders the reasoning process more transparent and traceable through the use of the trustworthiness graph.

**Link**: [arxiv](http://arxiv.org/abs/2408.12333v2),  [pdf](http://arxiv.org/pdf/2408.12333v2)

**Tags**: cs.AI 



### Mamba as a motion encoder for robotic imitation learning
**Authors**: Toshiaki Tsuji

**Updated**: 2024-09-04T11:59:53Z

**Summary**: Recent advancements in imitation learning, particularly with the integration of LLM techniques, are set to significantly improve robots' dexterity and adaptability. In this study, we propose using Mamba, a state-of-the-art architecture with potential applications in LLMs, for robotic imitation learning, highlighting its ability to function as an encoder that effectively captures contextual information. By reducing the dimensionality of the state space, Mamba operates similarly to an autoencoder. It effectively compresses the sequential information into state variables while preserving the essential temporal dynamics necessary for accurate motion prediction. Experimental results in tasks such as cup placing and case loading demonstrate that despite exhibiting higher estimation errors, Mamba achieves superior success rates compared to Transformers in practical task execution. This performance is attributed to Mamba's structure, which encompasses the state space model. Additionally, the study investigates Mamba's capacity to serve as a real-time motion generator with a limited amount of training data.

**Link**: [arxiv](http://arxiv.org/abs/2409.02636v1),  [pdf](http://arxiv.org/pdf/2409.02636v1)

**Tags**: cs.RO cs.SY eess.SY 



### Large Language Models for Information Retrieval: A Survey
**Authors**: Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, Ji-Rong Wen

**Updated**: 2024-09-04T11:39:56Z

**Summary**: As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.

**Link**: [arxiv](http://arxiv.org/abs/2308.07107v4),  [pdf](http://arxiv.org/pdf/2308.07107v4)

**Tags**: cs.CL cs.IR 



### Can Vehicle Motion Planning Generalize to Realistic Long-tail Scenarios?
**Authors**: Marcel Hallgarten, Julian Zapata, Martin Stoll, Katrin Renz, Andreas Zell

**Updated**: 2024-09-04T11:34:33Z

**Summary**: Real-world autonomous driving systems must make safe decisions in the face of rare and diverse traffic scenarios. Current state-of-the-art planners are mostly evaluated on real-world datasets like nuScenes (open-loop) or nuPlan (closed-loop). In particular, nuPlan seems to be an expressive evaluation method since it is based on real-world data and closed-loop, yet it mostly covers basic driving scenarios. This makes it difficult to judge a planner's capabilities to generalize to rarely-seen situations. Therefore, we propose a novel closed-loop benchmark interPlan containing several edge cases and challenging driving scenarios. We assess existing state-of-the-art planners on our benchmark and show that neither rule-based nor learning-based planners can safely navigate the interPlan scenarios. A recently evolving direction is the usage of foundation models like large language models (LLM) to handle generalization. We evaluate an LLM-only planner and introduce a novel hybrid planner that combines an LLM-based behavior planner with a rule-based motion planner that achieves state-of-the-art performance on our benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2404.07569v2),  [pdf](http://arxiv.org/pdf/2404.07569v2)

**Tags**: cs.RO cs.AI cs.LG 



### PUB: Plot Understanding Benchmark and Dataset for Evaluating Large   Language Models on Synthetic Visual Data Interpretation
**Authors**: Aneta Pawelec, Victoria Sara Weso≈Çowska, Zuzanna BƒÖczek, Piotr Sankowski

**Updated**: 2024-09-04T11:19:17Z

**Summary**: The ability of large language models (LLMs) to interpret visual representations of data is crucial for advancing their application in data analysis and decision-making processes. This paper presents a novel synthetic dataset designed to evaluate the proficiency of LLMs in interpreting various forms of data visualizations, including plots like time series, histograms, violins, boxplots, and clusters. Our dataset is generated using controlled parameters to ensure comprehensive coverage of potential real-world scenarios. We employ multimodal text prompts with questions related to visual data in images to benchmark several state-of-the-art models like ChatGPT or Gemini, assessing their understanding and interpretative accuracy.   To ensure data integrity, our benchmark dataset is generated automatically, making it entirely new and free from prior exposure to the models being tested. This strategy allows us to evaluate the models' ability to truly interpret and understand the data, eliminating possibility of pre-learned responses, and allowing for an unbiased evaluation of the models' capabilities. We also introduce quantitative metrics to assess the performance of the models, providing a robust and comprehensive evaluation tool.   Benchmarking several state-of-the-art LLMs with this dataset reveals varying degrees of success, highlighting specific strengths and weaknesses in interpreting diverse types of visual data. The results provide valuable insights into the current capabilities of LLMs and identify key areas for improvement. This work establishes a foundational benchmark for future research and development aimed at enhancing the visual interpretative abilities of language models. In the future, improved LLMs with robust visual interpretation skills can significantly aid in automated data analysis, scientific research, educational tools, and business intelligence applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.02617v1),  [pdf](http://arxiv.org/pdf/2409.02617v1)

**Tags**: cs.CL 



### Unveiling the Vulnerability of Private Fine-Tuning in Split-Based   Frameworks for Large Language Models: A Bidirectionally Enhanced Attack
**Authors**: Guanzhong Chen, Zhenghan Qin, Mingxin Yang, Yajie Zhou, Tao Fan, Tianyu Du, Zenglin Xu

**Updated**: 2024-09-04T10:58:26Z

**Summary**: Recent advancements in pre-trained large language models (LLMs) have significantly influenced various domains. Adapting these models for specific tasks often involves fine-tuning (FT) with private, domain-specific data. However, privacy concerns keep this data undisclosed, and the computational demands for deploying LLMs pose challenges for resource-limited data holders. This has sparked interest in split learning (SL), a Model-as-a-Service (MaaS) paradigm that divides LLMs into smaller segments for distributed training and deployment, transmitting only intermediate activations instead of raw data. SL has garnered substantial interest in both industry and academia as it aims to balance user data privacy, model ownership, and resource challenges in the private fine-tuning of LLMs. Despite its privacy claims, this paper reveals significant vulnerabilities arising from the combination of SL and LLM-FT: the Not-too-far property of fine-tuning and the auto-regressive nature of LLMs. Exploiting these vulnerabilities, we propose Bidirectional Semi-white-box Reconstruction (BiSR), the first data reconstruction attack (DRA) designed to target both the forward and backward propagation processes of SL. BiSR utilizes pre-trained weights as prior knowledge, combining a learning-based attack with a bidirectional optimization-based approach for highly effective data reconstruction. Additionally, it incorporates a Noise-adaptive Mixture of Experts (NaMoE) model to enhance reconstruction performance under perturbation. We conducted systematic experiments on various mainstream LLMs and different setups, empirically demonstrating BiSR's state-of-the-art performance. Furthermore, we thoroughly examined three representative defense mechanisms, showcasing our method's capability to reconstruct private data even in the presence of these defenses.

**Link**: [arxiv](http://arxiv.org/abs/2409.00960v2),  [pdf](http://arxiv.org/pdf/2409.00960v2)

**Tags**: cs.CR K.6.5 



### Hypothesizing Missing Causal Variables with LLMs
**Authors**: Ivaxi Sheth, Sahar Abdelnabi, Mario Fritz

**Updated**: 2024-09-04T10:37:44Z

**Summary**: Scientific discovery is a catalyst for human intellectual advances, driven by the cycle of hypothesis generation, experimental design, data evaluation, and iterative assumption refinement. This process, while crucial, is expensive and heavily dependent on the domain knowledge of scientists to generate hypotheses and navigate the scientific cycle. Central to this is causality, the ability to establish the relationship between the cause and the effect. Motivated by the scientific discovery process, in this work, we formulate a novel task where the input is a partial causal graph with missing variables, and the output is a hypothesis about the missing variables to complete the partial graph. We design a benchmark with varying difficulty levels and knowledge assumptions about the causal graph. With the growing interest in using Large Language Models (LLMs) to assist in scientific discovery, we benchmark open-source and closed models on our testbed. We show the strong ability of LLMs to hypothesize the mediation variables between a cause and its effect. In contrast, they underperform in hypothesizing the cause and effect variables themselves. We also observe surprising results where some of the open-source models outperform the closed GPT-4 model.

**Link**: [arxiv](http://arxiv.org/abs/2409.02604v1),  [pdf](http://arxiv.org/pdf/2409.02604v1)

**Tags**: cs.LG stat.ME 



### ChatGPT vs Social Surveys: Probing the Objective and Subjective Human   Society
**Authors**: Muzhi Zhou, Lu Yu, Xiaomin Geng, Lan Luo

**Updated**: 2024-09-04T10:33:37Z

**Summary**: The extent to which Large Language Models (LLMs) can simulate the data-generating process for social surveys remains unclear. Current research has not thoroughly assessed potential biases in the sociodemographic population represented within the language model's framework. Additionally, the subjective worlds of LLMs often show inconsistencies in how closely their responses match those of groups of human respondents. In this paper, we used ChatGPT-3.5 to simulate the sampling process and generated six socioeconomic characteristics from the 2020 US population. We also analyzed responses to questions about income inequality and gender roles to explore GPT's subjective attitudes. By using repeated random sampling, we created a sampling distribution to identify the parameters of the GPT-generated population and compared these with Census data. Our findings show some alignment in gender and age means with the actual 2020 US population, but we also found mismatches in the distributions of racial and educational groups. Furthermore, there were significant differences between the distribution of GPT's responses and human self-reported attitudes. While the overall point estimates of GPT's income attitudinal responses seem to align with the mean of the population occasionally, their response distributions follow a normal distribution that diverges from human responses. In terms of gender relations, GPT's answers tend to cluster in the most frequently answered category, demonstrating a deterministic pattern. We conclude by emphasizing the distinct design philosophies of LLMs and social surveys: LLMs aim to predict the most suitable answers, while social surveys seek to reveal the heterogeneity among social groups.

**Link**: [arxiv](http://arxiv.org/abs/2409.02601v1),  [pdf](http://arxiv.org/pdf/2409.02601v1)

**Tags**: cs.CY 



### Prompt Compression with Context-Aware Sentence Encoding for Fast and   Improved LLM Inference
**Authors**: Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke

**Updated**: 2024-09-04T10:20:59Z

**Summary**: Large language models (LLMs) have triggered a new stream of research focusing on compressing the context length to reduce the computational cost while ensuring the retention of helpful information for LLMs to answer the given question. Token-based removal methods are one of the most prominent approaches in this direction, but risk losing the semantics of the context caused by intermediate token removal, especially under high compression ratios, while also facing challenges in computational efficiency. In this work, we propose context-aware prompt compression (CPC), a sentence-level prompt compression technique where its key innovation is a novel context-aware sentence encoder that provides a relevance score for each sentence for a given question. To train this encoder, we generate a new dataset consisting of questions, positives, and negative pairs where positives are sentences relevant to the question, while negatives are irrelevant context sentences. We train the encoder in a contrastive setup to learn context-aware sentence representations. Our method considerably outperforms prior works on prompt compression on benchmark datasets and is up to 10.93x faster at inference compared to the best token-level compression method. We also find better improvement for shorter length constraints in most benchmarks, showing the effectiveness of our proposed solution in the compression of relevant information in a shorter context. Finally, we release the code and the dataset for quick reproducibility and further development: https://github.com/Workday/cpc.

**Link**: [arxiv](http://arxiv.org/abs/2409.01227v2),  [pdf](http://arxiv.org/pdf/2409.01227v2)

**Tags**: cs.CL cs.LG 



### SparQ Attention: Bandwidth-Efficient LLM Inference
**Authors**: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr

**Updated**: 2024-09-04T10:04:52Z

**Summary**: The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2312.04985v6),  [pdf](http://arxiv.org/pdf/2312.04985v6)

**Tags**: cs.LG 



### Advancing Cyber Incident Timeline Analysis Through Rule Based AI and   Large Language Models
**Authors**: Fatma Yasmine Loumachi, Mohamed Chahine Ghanem

**Updated**: 2024-09-04T09:46:33Z

**Summary**: Timeline Analysis (TA) is a key part of Timeline Forensics (TF) in Digital Forensics (DF), focusing primarily on examining and analysing temporal digital artefacts such as timestamps, derived from event logs, file metadata, and other related data to correlate events resulting from cyber incidents and reconstruct their chronological timeline. Traditional tools often struggle to efficiently process the vast volume and variety of data acquired during DF investigations and Incident Response (IR) processes. This paper presents a novel framework, GenDFIR, that combines Rule-Based Artificial Intelligence (R-BAI) algorithms with Large Language Models (LLMs) to advance and automate the TA process. Our approach consists of two main stages (1) We use R-BAI to identify and select anomalous digital artefacts based on predefined rules. (2) The selected artefacts are then converted into embeddings for processing by an LLM with the help of a Retrieval-Augmented Generation (RAG) agent. The LLM consequently leverages its capabilities to perform automated TA on the artefacts and predict potential incident scenarios. To validate our framework, we evaluate GenDFIR performance, efficiency, and reliability using various metrics across synthetic cyber incident simulation scenarios. This paper presents a proof of concept, where the findings demonstrate the significant potential of integrating R-BAI and LLMs for TA. This novel approach highlights the power of Generative AI (GenAI), specifically LLMs, and opens new avenues for advanced threat detection and incident reconstruction, representing a significant step forward in the field.

**Link**: [arxiv](http://arxiv.org/abs/2409.02572v1),  [pdf](http://arxiv.org/pdf/2409.02572v1)

**Tags**: cs.CR cs.AI cs.ET cs.LG 



### More is More: Addition Bias in Large Language Models
**Authors**: Luca Santagata, Cristiano De Nobili

**Updated**: 2024-09-04T09:39:07Z

**Summary**: In this paper, we investigate the presence of additive bias in Large Language Models (LLMs), drawing a parallel to the cognitive bias observed in humans where individuals tend to favor additive over subtractive changes. Using a series of controlled experiments, we tested various LLMs, including GPT-3.5 Turbo, Claude 3.5 Sonnet, Mistral, Math$\Sigma$tral, and Llama 3.1, on tasks designed to measure their propensity for additive versus subtractive modifications. Our findings demonstrate a significant preference for additive changes across all tested models. For example, in a palindrome creation task, Llama 3.1 favored adding letters 97.85% of the time over removing them. Similarly, in a Lego tower balancing task, GPT-3.5 Turbo chose to add a brick 76.38% of the time rather than remove one. In a text summarization task, Mistral 7B produced longer summaries in 59.40% to 75.10% of cases when asked to improve its own or others' writing. These results indicate that, similar to humans, LLMs exhibit a marked additive bias, which might have implications when LLMs are used on a large scale. Addittive bias might increase resource use and environmental impact, leading to higher economic costs due to overconsumption and waste. This bias should be considered in the development and application of LLMs to ensure balanced and efficient problem-solving approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.02569v1),  [pdf](http://arxiv.org/pdf/2409.02569v1)

**Tags**: cs.CL cs.AI cs.CY cs.HC 



### A Sentence is Worth a Thousand Pictures: Can Large Language Models   Understand Hum4n L4ngu4ge and the W0rld behind W0rds?
**Authors**: Evelina Leivada, Gary Marcus, Fritz G√ºnther, Elliot Murphy

**Updated**: 2024-09-04T09:27:05Z

**Summary**: Modern Artificial Intelligence applications show great potential for language-related tasks that rely on next-word prediction. The current generation of Large Language Models (LLMs) have been linked to claims about human-like linguistic performance and their applications are hailed both as a step towards artificial general intelligence and as a major advance in understanding the cognitive, and even neural basis of human language. To assess these claims, first we analyze the contribution of LLMs as theoretically informative representations of a target cognitive system vs. atheoretical mechanistic tools. Second, we evaluate the models' ability to see the bigger picture, through top-down feedback from higher levels of processing, which requires grounding in previous expectations and past world experience. We hypothesize that since models lack grounded cognition, they cannot take advantage of these features and instead solely rely on fixed associations between represented words and word vectors. To assess this, we designed and ran a novel 'leet task' (l33t t4sk), which requires decoding sentences in which letters are systematically replaced by numbers. The results suggest that humans excel in this task whereas models struggle, confirming our hypothesis. We interpret the results by identifying the key abilities that are still missing from the current state of development of these models, which require solutions that go beyond increased system scaling.

**Link**: [arxiv](http://arxiv.org/abs/2308.00109v2),  [pdf](http://arxiv.org/pdf/2308.00109v2)

**Tags**: cs.CL 



### Understanding eGFR Trajectories and Kidney Function Decline via Large   Multimodal Models
**Authors**: Chih-Yuan Li, Jun-Ting Wu, Chan Hsu, Ming-Yen Lin, Yihuang Kang

**Updated**: 2024-09-04T08:44:36Z

**Summary**: The estimated Glomerular Filtration Rate (eGFR) is an essential indicator of kidney function in clinical practice. Although traditional equations and Machine Learning (ML) models using clinical and laboratory data can estimate eGFR, accurately predicting future eGFR levels remains a significant challenge for nephrologists and ML researchers. Recent advances demonstrate that Large Language Models (LLMs) and Large Multimodal Models (LMMs) can serve as robust foundation models for diverse applications. This study investigates the potential of LMMs to predict future eGFR levels with a dataset consisting of laboratory and clinical values from 50 patients. By integrating various prompting techniques and ensembles of LMMs, our findings suggest that these models, when combined with precise prompts and visual representations of eGFR trajectories, offer predictive performance comparable to existing ML models. This research extends the application of foundation models and suggests avenues for future studies to harness these models in addressing complex medical forecasting challenges.

**Link**: [arxiv](http://arxiv.org/abs/2409.02530v1),  [pdf](http://arxiv.org/pdf/2409.02530v1)

**Tags**: cs.LG cs.AI 



### A design of magnetic tunnel junctions for the deployment of neuromorphic   hardware for edge computing
**Authors**: Davi Rodrigues, Eleonora Raimondo, Riccardo Tomasello, Mario Carpentieri, Giovanni Finocchio

**Updated**: 2024-09-04T08:40:27Z

**Summary**: The electrically readable complex dynamics of robust and scalable magnetic tunnel junctions (MTJs) offer promising opportunities for advancing neuromorphic computing. In this work, we present an MTJ design with a free layer and two polarizers capable of computing the sigmoidal activation function and its gradient at the device level. This design enables both feedforward and backpropagation computations within a single device, extending neuromorphic computing frameworks previously explored in the literature by introducing the ability to perform backpropagation directly in hardware. Our algorithm implementation reveals two key findings: (i) the small discrepancies between the MTJ-generated curves and the exact software-generated curves have a negligible impact on the performance of the backpropagation algorithm, (ii) the device implementation is highly robust to inter-device variation and noise, and (iii) the proposed method effectively supports transfer learning and knowledge distillation. To demonstrate this, we evaluated the performance of an edge computing network using weights from a software-trained model implemented with our MTJ design. The results show a minimal loss of accuracy of only 0.1% for the Fashion MNIST dataset and 2% for the CIFAR-100 dataset compared to the original software implementation. These results highlight the potential of our MTJ design for compact, hardware-based neural networks in edge computing applications, particularly for transfer learning.

**Link**: [arxiv](http://arxiv.org/abs/2409.02528v1),  [pdf](http://arxiv.org/pdf/2409.02528v1)

**Tags**: physics.app-ph cs.ET 



### Cog-GA: A Large Language Models-based Generative Agent for   Vision-Language Navigation in Continuous Environments
**Authors**: Zhiyuan Li, Yanfeng Lu, Yao Mu, Hong Qiao

**Updated**: 2024-09-04T08:30:03Z

**Summary**: Vision Language Navigation in Continuous Environments (VLN-CE) represents a frontier in embodied AI, demanding agents to navigate freely in unbounded 3D spaces solely guided by natural language instructions. This task introduces distinct challenges in multimodal comprehension, spatial reasoning, and decision-making. To address these challenges, we introduce Cog-GA, a generative agent founded on large language models (LLMs) tailored for VLN-CE tasks. Cog-GA employs a dual-pronged strategy to emulate human-like cognitive processes. Firstly, it constructs a cognitive map, integrating temporal, spatial, and semantic elements, thereby facilitating the development of spatial memory within LLMs. Secondly, Cog-GA employs a predictive mechanism for waypoints, strategically optimizing the exploration trajectory to maximize navigational efficiency. Each waypoint is accompanied by a dual-channel scene description, categorizing environmental cues into 'what' and 'where' streams as the brain. This segregation enhances the agent's attentional focus, enabling it to discern pertinent spatial information for navigation. A reflective mechanism complements these strategies by capturing feedback from prior navigation experiences, facilitating continual learning and adaptive replanning. Extensive evaluations conducted on VLN-CE benchmarks validate Cog-GA's state-of-the-art performance and ability to simulate human-like navigation behaviors. This research significantly contributes to the development of strategic and interpretable VLN-CE agents.

**Link**: [arxiv](http://arxiv.org/abs/2409.02522v1),  [pdf](http://arxiv.org/pdf/2409.02522v1)

**Tags**: cs.AI cs.RO 



### Language is Scary when Over-Analyzed: Unpacking Implied Misogynistic   Reasoning with Argumentation Theory-Driven Prompts
**Authors**: Arianna Muti, Federico Ruggeri, Khalid Al-Khatib, Alberto Barr√≥n-Cede√±o, Tommaso Caselli

**Updated**: 2024-09-04T08:27:43Z

**Summary**: We propose misogyny detection as an Argumentative Reasoning task and we investigate the capacity of large language models (LLMs) to understand the implicit reasoning used to convey misogyny in both Italian and English. The central aim is to generate the missing reasoning link between a message and the implied meanings encoding the misogyny. Our study uses argumentation theory as a foundation to form a collection of prompts in both zero-shot and few-shot settings. These prompts integrate different techniques, including chain-of-thought reasoning and augmented knowledge. Our findings show that LLMs fall short on reasoning capabilities about misogynistic comments and that they mostly rely on their implicit knowledge derived from internalized common stereotypes about women to generate implied assumptions, rather than on inductive reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2409.02519v1),  [pdf](http://arxiv.org/pdf/2409.02519v1)

**Tags**: cs.CL cs.SI 



### Vision-Language and Large Language Model Performance in   Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized   Models
**Authors**: Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush

**Updated**: 2024-09-04T08:22:28Z

**Summary**: Background and Aims: This study evaluates the medical reasoning performance of large language models (LLMs) and vision language models (VLMs) in gastroenterology.   Methods: We used 300 gastroenterology board exam-style multiple-choice questions, 138 of which contain images to systematically assess the impact of model configurations and parameters and prompt engineering strategies utilizing GPT-3.5. Next, we assessed the performance of proprietary and open-source LLMs (versions), including GPT (3.5, 4, 4o, 4omini), Claude (3, 3.5), Gemini (1.0), Mistral, Llama (2, 3, 3.1), Mixtral, and Phi (3), across different interfaces (web and API), computing environments (cloud and local), and model precisions (with and without quantization). Finally, we assessed accuracy using a semiautomated pipeline.   Results: Among the proprietary models, GPT-4o (73.7%) and Claude3.5-Sonnet (74.0%) achieved the highest accuracy, outperforming the top open-source models: Llama3.1-405b (64%), Llama3.1-70b (58.3%), and Mixtral-8x7b (54.3%). Among the quantized open-source models, the 6-bit quantized Phi3-14b (48.7%) performed best. The scores of the quantized models were comparable to those of the full-precision models Llama2-7b, Llama2--13b, and Gemma2-9b. Notably, VLM performance on image-containing questions did not improve when the images were provided and worsened when LLM-generated captions were provided. In contrast, a 10% increase in accuracy was observed when images were accompanied by human-crafted image descriptions.   Conclusion: In conclusion, while LLMs exhibit robust zero-shot performance in medical reasoning, the integration of visual data remains a challenge for VLMs. Effective deployment involves carefully determining optimal model configurations, encouraging users to consider either the high performance of proprietary models or the flexible adaptability of open-source models.

**Link**: [arxiv](http://arxiv.org/abs/2409.00084v2),  [pdf](http://arxiv.org/pdf/2409.00084v2)

**Tags**: cs.CL cs.AI 92C50, 68T50 J.3 



### Reuse and Blend: Energy-Efficient Optical Neural Network Enabled by   Weight Sharing
**Authors**: Bo Xu, Yuetong Fang, Shaoliang Yu, Renjing Xu

**Updated**: 2024-09-04T08:01:32Z

**Summary**: Optical neural networks (ONN) based on micro-ring resonators (MRR) have emerged as a promising alternative to significantly accelerating the massive matrix-vector multiplication (MVM) operations in artificial intelligence (AI) applications. However, the limited scale of MRR arrays presents a challenge for AI acceleration. The disparity between the small MRR arrays and the large weight matrices in AI necessitates extensive MRR writings, including reprogramming and calibration, resulting in considerable latency and energy overheads. To address this problem, we propose a novel design methodology to lessen the need for frequent weight reloading. Specifically, we propose a reuse and blend (R&B) architecture to support efficient layer-wise and block-wise weight sharing, which allows weights to be reused several times between layers/blocks. Experimental results demonstrate the R&B system can maintain comparable accuracy with 69% energy savings and 57% latency improvement. These results highlight the promise of the R&B to enable the efficient deployment of advanced deep learning models on photonic accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2409.01836v2),  [pdf](http://arxiv.org/pdf/2409.01836v2)

**Tags**: cs.AR 



### Boosting Generalizability towards Zero-Shot Cross-Dataset Single-Image   Indoor Depth by Meta-Initialization
**Authors**: Cho-Ying Wu, Yiqi Zhong, Junying Wang, Ulrich Neumann

**Updated**: 2024-09-04T07:25:50Z

**Summary**: Indoor robots rely on depth to perform tasks like navigation or obstacle detection, and single-image depth estimation is widely used to assist perception. Most indoor single-image depth prediction focuses less on model generalizability to unseen datasets, concerned with in-the-wild robustness for system deployment. This work leverages gradient-based meta-learning to gain higher generalizability on zero-shot cross-dataset inference. Unlike the most-studied meta-learning of image classification associated with explicit class labels, no explicit task boundaries exist for continuous depth values tied to highly varying indoor environments regarding object arrangement and scene composition. We propose fine-grained task that treats each RGB-D mini-batch as a task in our meta-learning formulation. We first show that our method on limited data induces a much better prior (max 27.8% in RMSE). Then, finetuning on meta-learned initialization consistently outperforms baselines without the meta approach. Aiming at generalization, we propose zero-shot cross-dataset protocols and validate higher generalizability induced by our meta-initialization, as a simple and useful plugin to many existing depth estimation methods. The work at the intersection of depth and meta-learning potentially drives both research to step closer to practical robotic and machine perception usage.

**Link**: [arxiv](http://arxiv.org/abs/2409.02486v1),  [pdf](http://arxiv.org/pdf/2409.02486v1)

**Tags**: cs.CV cs.AI 



### A Comparative Study on Large Language Models for Log Parsing
**Authors**: Merve Astekin, Max Hort, Leon Moonen

**Updated**: 2024-09-04T06:46:31Z

**Summary**: Background: Log messages provide valuable information about the status of software systems. This information is provided in an unstructured fashion and automated approaches are applied to extract relevant parameters. To ease this process, log parsing can be applied, which transforms log messages into structured log templates. Recent advances in language models have led to several studies that apply ChatGPT to the task of log parsing with promising results. However, the performance of other state-of-the-art large language models (LLMs) on the log parsing task remains unclear.   Aims: In this study, we investigate the current capability of state-of-the-art LLMs to perform log parsing.   Method: We select six recent LLMs, including both paid proprietary (GPT-3.5, Claude 2.1) and four free-to-use open models, and compare their performance on system logs obtained from a selection of mature open-source projects. We design two different prompting approaches and apply the LLMs on 1, 354 log templates across 16 different projects. We evaluate their effectiveness, in the number of correctly identified templates, and the syntactic similarity between the generated templates and the ground truth.   Results: We found that free-to-use models are able to compete with paid models, with CodeLlama extracting 10% more log templates correctly than GPT-3.5. Moreover, we provide qualitative insights into the usability of language models (e.g., how easy it is to use their responses).   Conclusions: Our results reveal that some of the smaller, free-to-use LLMs can considerably assist log parsing compared to their paid proprietary competitors, especially code-specialized models.

**Link**: [arxiv](http://arxiv.org/abs/2409.02474v1),  [pdf](http://arxiv.org/pdf/2409.02474v1)

**Tags**: cs.SE cs.CL 



### Evaluating Named Entity Recognition Using Few-Shot Prompting with Large   Language Models
**Authors**: H√©di Zeghidi, Ludovic Moncla

**Updated**: 2024-09-04T06:36:22Z

**Summary**: This paper evaluates Few-Shot Prompting with Large Language Models for Named Entity Recognition (NER). Traditional NER systems rely on extensive labeled datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples. We assess state-of-the-art models like GPT-4 in NER tasks, comparing their few-shot performance to fully supervised benchmarks. Results show that while there is a performance gap, large models excel in adapting to new entity types and domains with very limited data. We also explore the effects of prompt engineering, guided output format and context length on performance. This study underscores Few-Shot Learning's potential to reduce the need for large labeled datasets, enhancing NER scalability and accessibility.

**Link**: [arxiv](http://arxiv.org/abs/2408.15796v2),  [pdf](http://arxiv.org/pdf/2408.15796v2)

**Tags**: cs.IR cs.AI 



### DetectiveQA: Evaluating Long-Context Reasoning on Detective Novels
**Authors**: Zhe Xu, Jiasheng Ye, Xiangyang Liu, Tianxiang Sun, Xiaoran Liu, Qipeng Guo, Linlin Li, Qun Liu, Xuanjing Huang, Xipeng Qiu

**Updated**: 2024-09-04T06:28:22Z

**Summary**: With the rapid advancement of Large Language Models (LLMs), long-context information understanding and processing have become a hot topic in academia and industry. However, benchmarks for evaluating the ability of LLMs to handle long-context information do not seem to have kept pace with the development of LLMs. Despite the emergence of various long-context evaluation benchmarks, the types of capability assessed are still limited, without new capability dimensions. In this paper, we introduce DetectiveQA, a narrative reasoning benchmark featured with an average context length of over 100K tokens. DetectiveQA focuses on evaluating the long-context reasoning ability of LLMs, which not only requires a full understanding of context but also requires extracting important evidences from the context and reasoning according to extracted evidences to answer the given questions. This is a new dimension of capability evaluation, which is more in line with the current intelligence level of LLMs. We use detective novels as data sources, which naturally have various reasoning elements. Finally, we manually annotated 600 questions in Chinese and then also provided an English edition of the context information and questions. We evaluate many long-context LLMs on DetectiveQA, including commercial and open-sourced models, and the results indicate that existing long-context LLMs still require significant advancements to effectively process true long-context dependency questions.

**Link**: [arxiv](http://arxiv.org/abs/2409.02465v1),  [pdf](http://arxiv.org/pdf/2409.02465v1)

**Tags**: cs.CL 



### MarsCode Agent: AI-native Automated Bug Fixing
**Authors**: Yizhou Liu, Pengfei Gao, Xinchen Wang, Jie Liu, Yexuan Shi, Zhao Zhang, Chao Peng

**Updated**: 2024-09-04T06:19:08Z

**Summary**: Recent advances in large language models (LLMs) have shown significant potential to automate various software development tasks, including code completion, test generation, and bug fixing. However, the application of LLMs for automated bug fixing remains challenging due to the complexity and diversity of real-world software systems. In this paper, we introduce MarsCode Agent, a novel framework that leverages LLMs to automatically identify and repair bugs in software code. MarsCode Agent combines the power of LLMs with advanced code analysis techniques to accurately localize faults and generate patches. Our approach follows a systematic process of planning, bug reproduction, fault localization, candidate patch generation, and validation to ensure high-quality bug fixes. We evaluated MarsCode Agent on SWE-bench, a comprehensive benchmark of real-world software projects, and our results show that MarsCode Agent achieves a high success rate in bug fixing compared to most of the existing automated approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.00899v2),  [pdf](http://arxiv.org/pdf/2409.00899v2)

**Tags**: cs.SE cs.AI 



### The Fault in our Stars: Quality Assessment of Code Generation Benchmarks
**Authors**: Mohammed Latif Siddiq, Simantika Dristi, Joy Saha, Joanna C. S. Santos

**Updated**: 2024-09-04T06:10:47Z

**Summary**: Large Language Models (LLMs) are gaining popularity among software engineers. A crucial aspect of developing effective code generation LLMs is to evaluate these models using a robust benchmark. Evaluation benchmarks with quality issues can provide a false sense of performance. In this work, we conduct the first-of-its-kind study of the quality of prompts within benchmarks used to compare the performance of different code generation models. To conduct this study, we analyzed 3,566 prompts from 9 code generation benchmarks to identify quality issues in them. We also investigated whether fixing the identified quality issues in the benchmarks' prompts affects a model's performance. We also studied memorization issues of the evaluation dataset, which can put into question a benchmark's trustworthiness. We found that code generation evaluation benchmarks mainly focused on Python and coding exercises and had very limited contextual dependencies to challenge the model. These datasets and the developers' prompts suffer from quality issues like spelling and grammatical errors, unclear sentences to express developers' intent, and not using proper documentation style. Fixing all these issues in the benchmarks can lead to a better performance for Python code generation, but not a significant improvement was observed for Java code generation. We also found evidence that GPT-3.5-Turbo and CodeGen-2.5 models may have data contamination issues.

**Link**: [arxiv](http://arxiv.org/abs/2404.10155v3),  [pdf](http://arxiv.org/pdf/2404.10155v3)

**Tags**: cs.SE cs.LG 



### Large Language Models for Explainable Decisions in Dynamic Digital Twins
**Authors**: Nan Zhang, Christian Vergara-Marcillo, Georgios Diamantopoulos, Jingran Shen, Nikos Tziritas, Rami Bahsoon, Georgios Theodoropoulos

**Updated**: 2024-09-04T06:00:56Z

**Summary**: Dynamic data-driven Digital Twins (DDTs) can enable informed decision-making and provide an optimisation platform for the underlying system. By leveraging principles of Dynamic Data-Driven Applications Systems (DDDAS), DDTs can formulate computational modalities for feedback loops, model updates and decision-making, including autonomous ones. However, understanding autonomous decision-making often requires technical and domain-specific knowledge. This paper explores using large language models (LLMs) to provide an explainability platform for DDTs, generating natural language explanations of the system's decision-making by leveraging domain-specific knowledge bases. A case study from smart agriculture is presented.

**Link**: [arxiv](http://arxiv.org/abs/2405.14411v2),  [pdf](http://arxiv.org/pdf/2405.14411v2)

**Tags**: cs.AI cs.SY eess.SY 



### Towards Measuring and Modeling "Culture" in LLMs: A Survey
**Authors**: Muhammad Farid Adilazuarda, Sagnik Mukherjee, Pradhyumna Lavania, Siddhant Singh, Alham Fikri Aji, Jacki O'Neill, Ashutosh Modi, Monojit Choudhury

**Updated**: 2024-09-04T05:12:54Z

**Summary**: We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define "culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture". We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of ``culture,'' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.

**Link**: [arxiv](http://arxiv.org/abs/2403.15412v5),  [pdf](http://arxiv.org/pdf/2403.15412v5)

**Tags**: cs.CY cs.AI cs.CL 



### Exploring the applicability of Large Language Models to citation context   analysis
**Authors**: Kai Nishikawa, Hitoshi Koshiba

**Updated**: 2024-09-04T04:41:15Z

**Summary**: Unlike traditional citation analysis -- which assumes that all citations in a paper are equivalent -- citation context analysis considers the contextual information of individual citations. However, citation context analysis requires creating large amounts of data through annotation, which hinders the widespread use of this methodology. This study explored the applicability of Large Language Models (LLMs) -- particularly ChatGPT -- to citation context analysis by comparing LLMs and human annotation results. The results show that the LLMs annotation is as good as or better than the human annotation in terms of consistency but poor in terms of predictive performance. Thus, having LLMs immediately replace human annotators in citation context analysis is inappropriate. However, the annotation results obtained by LLMs can be used as reference information when narrowing the annotation results obtained by multiple human annotators to one, or LLMs can be used as one of the annotators when it is difficult to prepare sufficient human annotators. This study provides basic findings important for the future development of citation context analyses.

**Link**: [arxiv](http://arxiv.org/abs/2409.02443v1),  [pdf](http://arxiv.org/pdf/2409.02443v1)

**Tags**: cs.DL 



### Large Language Models as Efficient Reward Function Searchers for   Custom-Environment Multi-Objective Reinforcement Learning
**Authors**: Guanwen Xie, Jingzehua Xu, Yiyuan Yang, Shuai Zhang

**Updated**: 2024-09-04T04:15:14Z

**Summary**: Leveraging large language models (LLMs) for designing reward functions demonstrates significant potential. However, achieving effective design and improvement of reward functions in reinforcement learning (RL) tasks with complex custom environments and multiple requirements presents considerable challenges. In this paper, we enable LLMs to be effective white-box searchers, highlighting their advanced semantic understanding capabilities. Specifically, we generate reward components for each explicit user requirement and employ the reward critic to identify the correct code form. Then, LLMs assign weights to the reward components to balance their values and iteratively search and optimize these weights based on the context provided by the training log analyzer, while adaptively determining the search step size. We applied the framework to an underwater information collection RL task without direct human feedback or reward examples (zero-shot). The reward critic successfully correct the reward code with only one feedback for each requirement, effectively preventing irreparable errors that can occur when reward function feedback is provided in aggregate. The effective initialization of weights enables the acquisition of different reward functions within the Pareto solution set without weight search. Even in the case where a weight is 100 times off, fewer than four iterations are needed to obtain solutions that meet user requirements. The framework also works well with most prompts utilizing GPT-3.5 Turbo, since it does not require advanced numerical understanding or calculation.

**Link**: [arxiv](http://arxiv.org/abs/2409.02428v1),  [pdf](http://arxiv.org/pdf/2409.02428v1)

**Tags**: cs.LG cs.AI cs.CL cs.SY eess.SY 



### Accelerating Large Language Model Training with Hybrid GPU-based   Compression
**Authors**: Lang Xu, Quentin Anthony, Qinghua Zhou, Nawras Alnaasan, Radha R. Gulhane, Aamir Shafi, Hari Subramoni, Dhabaleswar K. Panda

**Updated**: 2024-09-04T04:05:30Z

**Summary**: Data Parallelism (DP), Tensor Parallelism (TP), and Pipeline Parallelism (PP) are the three strategies widely adopted to enable fast and efficient Large Language Model (LLM) training. However, these approaches rely on data-intensive communication routines to collect, aggregate, and re-distribute gradients, activations, and other important model information, which pose significant overhead. Co-designed with GPU-based compression libraries, MPI libraries have been proven to reduce message size significantly, and leverage interconnect bandwidth, thus increasing training efficiency while maintaining acceptable accuracy.   In this work, we investigate the efficacy of compression-assisted MPI collectives under the context of distributed LLM training using 3D parallelism and ZeRO optimizations. We scaled up to 192 V100 GPUs on the Lassen supercomputer. First, we enabled a na\"ive compression scheme across all collectives and observed a 22.5\% increase in TFLOPS per GPU and a 23.6\% increase in samples per second for GPT-NeoX-20B training. Nonetheless, such a strategy ignores the sparsity discrepancy among messages communicated in each parallelism degree, thus introducing more errors and causing degradation in training loss. Therefore, we incorporated hybrid compression settings toward each parallel dimension and adjusted the compression intensity accordingly. Given their low-rank structure (arXiv:2301.02654), we apply aggressive compression on gradients when performing DP All-reduce. We adopt milder compression to preserve precision while communicating activations, optimizer states, and model parameters in TP and PP. Using the adjusted hybrid compression scheme, we demonstrate a 17.3\% increase in TFLOPS per GPU and a 12.7\% increase in samples per second while reaching baseline loss convergence.

**Link**: [arxiv](http://arxiv.org/abs/2409.02423v1),  [pdf](http://arxiv.org/pdf/2409.02423v1)

**Tags**: cs.DC cs.AI 



### An Empirical Study on Information Extraction using Large Language Models
**Authors**: Ridong Han, Chaohao Yang, Tao Peng, Prayag Tiwari, Xiang Wan, Lu Liu, Benyou Wang

**Updated**: 2024-09-04T03:50:38Z

**Summary**: Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks. Therefore, various attempts have been made to apply LLMs to information extraction (IE), which is a fundamental NLP task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in LLMs' information extraction ability, we assess the information extraction ability of GPT-4 (the latest version of GPT at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between GPT-4 and state-of-the-art (SOTA) IE methods. To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.

**Link**: [arxiv](http://arxiv.org/abs/2409.00369v2),  [pdf](http://arxiv.org/pdf/2409.00369v2)

**Tags**: cs.CL 



### Can AI Replace Human Subjects? A Large-Scale Replication of   Psychological Experiments with LLMs
**Authors**: Ziyan Cui, Ning Li, Huaikang Zhou

**Updated**: 2024-09-04T03:21:07Z

**Summary**: Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) like GPT-4 have shown promise in replicating human-like responses in various psychological experiments. However, the extent to which LLMs can effectively replace human subjects across diverse experimental contexts remains unclear. Here, we conduct a large-scale study replicating 154 psychological experiments from top social science journals with 618 main effects and 138 interaction effects using GPT-4 as a simulated participant. We find that GPT-4 successfully replicates 76.0 percent of main effects and 47.0 percent of interaction effects observed in the original studies, closely mirroring human responses in both direction and significance. However, only 19.44 percent of GPT-4's replicated confidence intervals contain the original effect sizes, with the majority of replicated effect sizes exceeding the 95 percent confidence interval of the original studies. Additionally, there is a 71.6 percent rate of unexpected significant results where the original studies reported null findings, suggesting potential overestimation or false positives. Our results demonstrate the potential of LLMs as powerful tools in psychological research but also emphasize the need for caution in interpreting AI-driven findings. While LLMs can complement human studies, they cannot yet fully replace the nuanced insights provided by human subjects.

**Link**: [arxiv](http://arxiv.org/abs/2409.00128v2),  [pdf](http://arxiv.org/pdf/2409.00128v2)

**Tags**: cs.CL cs.AI econ.GN q-fin.EC 



### Learning Privacy-Preserving Student Networks via   Discriminative-Generative Distillation
**Authors**: Shiming Ge, Bochao Liu, Pengju Wang, Yong Li, Dan Zeng

**Updated**: 2024-09-04T03:06:13Z

**Summary**: While deep models have proved successful in learning rich knowledge from massive well-annotated data, they may pose a privacy leakage risk in practical deployment. It is necessary to find an effective trade-off between high utility and strong privacy. In this work, we propose a discriminative-generative distillation approach to learn privacy-preserving deep models. Our key idea is taking models as bridge to distill knowledge from private data and then transfer it to learn a student network via two streams. First, discriminative stream trains a baseline classifier on private data and an ensemble of teachers on multiple disjoint private subsets, respectively. Then, generative stream takes the classifier as a fixed discriminator and trains a generator in a data-free manner. After that, the generator is used to generate massive synthetic data which are further applied to train a variational autoencoder (VAE). Among these synthetic data, a few of them are fed into the teacher ensemble to query labels via differentially private aggregation, while most of them are embedded to the trained VAE for reconstructing synthetic data. Finally, a semi-supervised student learning is performed to simultaneously handle two tasks: knowledge transfer from the teachers with distillation on few privately labeled synthetic data, and knowledge enhancement with tangent-normal adversarial regularization on many triples of reconstructed synthetic data. In this way, our approach can control query cost over private data and mitigate accuracy degradation in a unified manner, leading to a privacy-preserving student model. Extensive experiments and analysis clearly show the effectiveness of the proposed approach.

**Link**: [arxiv](http://arxiv.org/abs/2409.02404v1),  [pdf](http://arxiv.org/pdf/2409.02404v1)

**Tags**: cs.LG cs.AI cs.CR 



### Building Math Agents with Multi-Turn Iterative Preference Learning
**Authors**: Wei Xiong, Chengshuai Shi, Jiaming Shen, Aviv Rosenberg, Zhen Qin, Daniele Calandriello, Misha Khalman, Rishabh Joshi, Bilal Piot, Mohammad Saleh, Chi Jin, Tong Zhang, Tianqi Liu

**Updated**: 2024-09-04T02:41:04Z

**Summary**: Recent studies have shown that large language models' (LLMs) mathematical problem-solving capabilities can be enhanced by integrating external tools, such as code interpreters, and employing multi-turn Chain-of-Thought (CoT) reasoning. While current methods focus on synthetic data generation and Supervised Fine-Tuning (SFT), this paper studies the complementary direct preference learning approach to further improve model performance. However, existing direct preference learning algorithms are originally designed for the single-turn chat task, and do not fully address the complexities of multi-turn reasoning and external tool integration required for tool-integrated mathematical reasoning tasks. To fill in this gap, we introduce a multi-turn direct preference learning framework, tailored for this context, that leverages feedback from code interpreters and optimizes trajectory-level preferences. This framework includes multi-turn DPO and multi-turn KTO as specific implementations. The effectiveness of our framework is validated through training of various language models using an augmented prompt set from the GSM8K and MATH datasets. Our results demonstrate substantial improvements: a supervised fine-tuned Gemma-1.1-it-7B model's performance increased from 77.5% to 83.9% on GSM8K and from 46.1% to 51.2% on MATH. Similarly, a Gemma-2-it-9B model improved from 84.1% to 86.3% on GSM8K and from 51.0% to 54.5% on MATH.

**Link**: [arxiv](http://arxiv.org/abs/2409.02392v1),  [pdf](http://arxiv.org/pdf/2409.02392v1)

**Tags**: cs.LG stat.ML 



### Scaling Laws for Economic Productivity: Experimental Evidence in   LLM-Assisted Translation
**Authors**: Ali Merali

**Updated**: 2024-09-04T02:39:31Z

**Summary**: This paper derives 'scaling laws' -- empirical relationships between the amount of training compute used for a Large Language Model (LLM) and its performance -- for economic outcomes. In a preregistered experiment, 300 professional translators completed 1800 tasks with access to one of thirteen LLMs with differing model training compute sizes (or a control). Our results show that model scaling substantially raises productivity: for every 10x increase in model compute, translators completed tasks 12.3% quicker, received 0.18 s.d. higher grades, and earned 16.1% more per minute (including bonus payments). Further, the gains from model scaling are much higher for lower-skilled workers who gain a 4x larger improvement in task completion speed. These results imply further frontier model scaling -- which is currently estimated at 4x increase per year -- may have significant economic implications.

**Link**: [arxiv](http://arxiv.org/abs/2409.02391v1),  [pdf](http://arxiv.org/pdf/2409.02391v1)

**Tags**: econ.GN cs.AI q-fin.EC 



### Large Language Models and Cognitive Science: A Comprehensive Review of   Similarities, Differences, and Challenges
**Authors**: Qian Niu, Junyu Liu, Ziqian Bi, Pohsun Feng, Benji Peng, Keyu Chen

**Updated**: 2024-09-05T05:36:10Z

**Summary**: This comprehensive review explores the intersection of Large Language Models (LLMs) and cognitive science, examining similarities and differences between LLMs and human cognitive processes. We analyze methods for evaluating LLMs cognitive abilities and discuss their potential as cognitive models. The review covers applications of LLMs in various cognitive fields, highlighting insights gained for cognitive science research. We assess cognitive biases and limitations of LLMs, along with proposed methods for improving their performance. The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities. Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition. This review provides a balanced perspective on the current state and future potential of LLMs in advancing our understanding of both artificial and human intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2409.02387v2),  [pdf](http://arxiv.org/pdf/2409.02387v2)

**Tags**: cs.AI cs.CL 



### Enhancing Dialogue Generation in Werewolf Game Through Situation   Analysis and Persuasion Strategies
**Authors**: Zhiyang Qi, Michimasa Inaba

**Updated**: 2024-09-04T02:24:08Z

**Summary**: Recent advancements in natural language processing, particularly with large language models (LLMs) like GPT-4, have significantly enhanced dialogue systems, enabling them to generate more natural and fluent conversations. Despite these improvements, challenges persist, such as managing continuous dialogues, memory retention, and minimizing hallucinations. The AIWolfDial2024 addresses these challenges by employing the Werewolf Game, an incomplete information game, to test the capabilities of LLMs in complex interactive environments. This paper introduces a LLM-based Werewolf Game AI, where each role is supported by situation analysis to aid response generation. Additionally, for the werewolf role, various persuasion strategies, including logical appeal, credibility appeal, and emotional appeal, are employed to effectively persuade other players to align with its actions.

**Link**: [arxiv](http://arxiv.org/abs/2408.16586v2),  [pdf](http://arxiv.org/pdf/2408.16586v2)

**Tags**: cs.CL cs.AI 



### STAB: Speech Tokenizer Assessment Benchmark
**Authors**: Shikhar Vashishth, Harman Singh, Shikhar Bharadwaj, Sriram Ganapathy, Chulayuth Asawaroengchai, Kartik Audhkhasi, Andrew Rosenberg, Ankur Bapna, Bhuvana Ramabhadran

**Updated**: 2024-09-04T02:20:59Z

**Summary**: Representing speech as discrete tokens provides a framework for transforming speech into a format that closely resembles text, thus enabling the use of speech as an input to the widely successful large language models (LLMs). Currently, while several speech tokenizers have been proposed, there is ambiguity regarding the properties that are desired from a tokenizer for specific downstream tasks and its overall generalizability. Evaluating the performance of tokenizers across different downstream tasks is a computationally intensive effort that poses challenges for scalability. To circumvent this requirement, we present STAB (Speech Tokenizer Assessment Benchmark), a systematic evaluation framework designed to assess speech tokenizers comprehensively and shed light on their inherent characteristics. This framework provides a deeper understanding of the underlying mechanisms of speech tokenization, thereby offering a valuable resource for expediting the advancement of future tokenizer models and enabling comparative analysis using a standardized benchmark. We evaluate the STAB metrics and correlate this with downstream task performance across a range of speech tasks and tokenizer choices.

**Link**: [arxiv](http://arxiv.org/abs/2409.02384v1),  [pdf](http://arxiv.org/pdf/2409.02384v1)

**Tags**: cs.CL cs.SD eess.AS 



### SELF-[IN]CORRECT: LLMs Struggle with Discriminating Self-Generated   Responses
**Authors**: Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir, Benjamin Van Durme, Daniel Khashabi

**Updated**: 2024-09-04T02:00:58Z

**Summary**: Can LLMs consistently improve their previous outputs for better results? For this to be true, LLMs would need to be better at discriminating among previously-generated alternatives, than generating initial responses. We explore the validity of this hypothesis in practice. We first formulate a unified framework that allows us to compare the generative and discriminative capability of any model on any task. In our resulting experimental analysis of several open-source and industrial LLMs, we observe that models are not reliably better at discriminating among previously-generated alternatives than generating initial responses. This finding challenges the notion that LLMs may be able to enhance their performance only through their own judgment.

**Link**: [arxiv](http://arxiv.org/abs/2404.04298v2),  [pdf](http://arxiv.org/pdf/2404.04298v2)

**Tags**: cs.AI cs.CL cs.LG 



### How Privacy-Savvy Are Large Language Models? A Case Study on Compliance   and Privacy Technical Review
**Authors**: Xichou Zhu, Yang Liu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Bolong Yang, Manman Wang, Zongxing Xie, Peng Liu, Dan Cai, Junhui Wang

**Updated**: 2024-09-04T01:51:37Z

**Summary**: The recent advances in large language models (LLMs) have significantly expanded their applications across various fields such as language generation, summarization, and complex question answering. However, their application to privacy compliance and technical privacy reviews remains under-explored, raising critical concerns about their ability to adhere to global privacy standards and protect sensitive user data. This paper seeks to address this gap by providing a comprehensive case study evaluating LLMs' performance in privacy-related tasks such as privacy information extraction (PIE), legal and regulatory key point detection (KPD), and question answering (QA) with respect to privacy policies and data protection regulations. We introduce a Privacy Technical Review (PTR) framework, highlighting its role in mitigating privacy risks during the software development life-cycle. Through an empirical assessment, we investigate the capacity of several prominent LLMs, including BERT, GPT-3.5, GPT-4, and custom models, in executing privacy compliance checks and technical privacy reviews. Our experiments benchmark the models across multiple dimensions, focusing on their precision, recall, and F1-scores in extracting privacy-sensitive information and detecting key regulatory compliance points. While LLMs show promise in automating privacy reviews and identifying regulatory discrepancies, significant gaps persist in their ability to fully comply with evolving legal standards. We provide actionable recommendations for enhancing LLMs' capabilities in privacy compliance, emphasizing the need for robust model improvements and better integration with legal and regulatory requirements. This study underscores the growing importance of developing privacy-aware LLMs that can both support businesses in compliance efforts and safeguard user privacy rights.

**Link**: [arxiv](http://arxiv.org/abs/2409.02375v1),  [pdf](http://arxiv.org/pdf/2409.02375v1)

**Tags**: cs.CL 



### Do Large Language Models Possess Sensitive to Sentiment?
**Authors**: Yang Liu, Xichou Zhu, Zhou Shen, Yi Liu, Min Li, Yujun Chen, Benzi John, Zhenzhen Ma, Tao Hu, Zhiyang Xu, Wei Luo, Junhui Wang

**Updated**: 2024-09-04T01:40:20Z

**Summary**: Large Language Models (LLMs) have recently displayed their extraordinary capabilities in language understanding. However, how to comprehensively assess the sentiment capabilities of LLMs continues to be a challenge. This paper investigates the ability of LLMs to detect and react to sentiment in text modal. As the integration of LLMs into diverse applications is on the rise, it becomes highly critical to comprehend their sensitivity to emotional tone, as it can influence the user experience and the efficacy of sentiment-driven tasks. We conduct a series of experiments to evaluate the performance of several prominent LLMs in identifying and responding appropriately to sentiments like positive, negative, and neutral emotions. The models' outputs are analyzed across various sentiment benchmarks, and their responses are compared with human evaluations. Our discoveries indicate that although LLMs show a basic sensitivity to sentiment, there are substantial variations in their accuracy and consistency, emphasizing the requirement for further enhancements in their training processes to better capture subtle emotional cues. Take an example in our findings, in some cases, the models might wrongly classify a strongly positive sentiment as neutral, or fail to recognize sarcasm or irony in the text. Such misclassifications highlight the complexity of sentiment analysis and the areas where the models need to be refined. Another aspect is that different LLMs might perform differently on the same set of data, depending on their architecture and training datasets. This variance calls for a more in-depth study of the factors that contribute to the performance differences and how they can be optimized.

**Link**: [arxiv](http://arxiv.org/abs/2409.02370v1),  [pdf](http://arxiv.org/pdf/2409.02370v1)

**Tags**: cs.CL cs.AI 



### The Hidden Costs of Automation: An Empirical Study on GitHub Actions   Workflow Maintenance
**Authors**: Pablo Valenzuela-Toledo, Alexandre Bergel, Timo Kehrer, Oscar Nierstrasz

**Updated**: 2024-09-04T01:33:16Z

**Summary**: GitHub Actions (GA) is an orchestration platform that streamlines the automatic execution of software engineering tasks such as building, testing, and deployment. Although GA workflows are the primary means for automation, according to our experience and observations, human intervention is necessary to correct defects, update dependencies, or refactor existing workflow files. In fact, previous research has shown that software artifacts similar to workflows, such as build files and bots, can introduce additional maintenance tasks in software projects. This suggests that workflow files, which are also used to automate repetitive tasks in professional software production, may generate extra workload for developers. However, the nature of such effort has not been well studied. This paper presents a large-scale empirical investigation towards characterizing the maintenance of GA workflows by studying the evolution of workflow files in almost 200 mature GitHub projects across ten programming languages. Our findings largely confirm the results of previous studies on the maintenance of similar artifacts, while also revealing GA-specific insights such as bug fixing and CI/CD improvement being among the major drivers of GA maintenance. A direct implication is that practitioners should be aware of proper resource planning and allocation for maintaining GA workflows, thus exposing the ``hidden costs of automation.'' Our findings also call for identifying and documenting best practices for such maintenance, and for enhanced tool features supporting dependency tracking and better error reporting of workflow specifications.

**Link**: [arxiv](http://arxiv.org/abs/2409.02366v1),  [pdf](http://arxiv.org/pdf/2409.02366v1)

**Tags**: cs.SE 



### $\textit{MMJ-Bench}$: A Comprehensive Study on Jailbreak Attacks and   Defenses for Vision Language Models
**Authors**: Fenghua Weng, Yue Xu, Chengyan Fu, Wenjie Wang

**Updated**: 2024-09-04T01:30:23Z

**Summary**: As deep learning advances, Large Language Models (LLMs) and their multimodal counterparts, Vision-Language Models (VLMs), have shown exceptional performance in many real-world tasks. However, VLMs face significant security challenges, such as jailbreak attacks, where attackers attempt to bypass the model's safety alignment to elicit harmful responses. The threat of jailbreak attacks on VLMs arises from both the inherent vulnerabilities of LLMs and the multiple information channels that VLMs process. While various attacks and defenses have been proposed, there is a notable gap in unified and comprehensive evaluations, as each method is evaluated on different dataset and metrics, making it impossible to compare the effectiveness of each method. To address this gap, we introduce \textit{MMJ-Bench}, a unified pipeline for evaluating jailbreak attacks and defense techniques for VLMs. Through extensive experiments, we assess the effectiveness of various attack methods against SoTA VLMs and evaluate the impact of defense mechanisms on both defense effectiveness and model utility for normal tasks. Our comprehensive evaluation contribute to the field by offering a unified and systematic evaluation framework and the first public-available benchmark for VLM jailbreak research. We also demonstrate several insightful findings that highlights directions for future studies.

**Link**: [arxiv](http://arxiv.org/abs/2408.08464v2),  [pdf](http://arxiv.org/pdf/2408.08464v2)

**Tags**: cs.CR 



### LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet
**Authors**: Nathaniel Li, Ziwen Han, Ian Steneker, Willow Primack, Riley Goodside, Hugh Zhang, Zifan Wang, Cristina Menghini, Summer Yue

**Updated**: 2024-09-04T00:58:59Z

**Summary**: Recent large language model (LLM) defenses have greatly improved models' ability to refuse harmful queries, even when adversarially attacked. However, LLM defenses are primarily evaluated against automated adversarial attacks in a single turn of conversation, an insufficient threat model for real-world malicious use. We demonstrate that multi-turn human jailbreaks uncover significant vulnerabilities, exceeding 70% attack success rate (ASR) on HarmBench against defenses that report single-digit ASRs with automated single-turn attacks. Human jailbreaks also reveal vulnerabilities in machine unlearning defenses, successfully recovering dual-use biosecurity knowledge from unlearned models. We compile these results into Multi-Turn Human Jailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks. We publicly release MHJ alongside a compendium of jailbreak tactics developed across dozens of commercial red teaming engagements, supporting research towards stronger LLM defenses.

**Link**: [arxiv](http://arxiv.org/abs/2408.15221v2),  [pdf](http://arxiv.org/pdf/2408.15221v2)

**Tags**: cs.LG cs.CL cs.CR cs.CY 



### Anchored Preference Optimization and Contrastive Revisions: Addressing   Underspecification in Alignment
**Authors**: Karel D'Oosterlinck, Winnie Xu, Chris Develder, Thomas Demeester, Amanpreet Singh, Christopher Potts, Douwe Kiela, Shikib Mehri

**Updated**: 2024-09-04T00:22:45Z

**Summary**: Large Language Models (LLMs) are often aligned using contrastive alignment objectives and preference pair datasets. The interaction between model, paired data, and objective makes alignment a complicated procedure, sometimes producing subpar results. We study this and find that (i) preference data gives a better learning signal when the underlying responses are contrastive, and (ii) alignment objectives lead to better performance when they specify more control over the model during training. Based on these insights, we introduce Contrastive Learning from AI Revisions (CLAIR), a data-creation method which leads to more contrastive preference pairs, and Anchored Preference Optimization (APO), a controllable and more stable alignment objective. We align Llama-3-8B-Instruct using various comparable datasets and alignment objectives and measure MixEval-Hard scores, which correlate highly with human judgments. The CLAIR preferences lead to the strongest performance out of all datasets, and APO consistently outperforms less controllable objectives. Our best model, trained on 32K CLAIR preferences with APO, improves Llama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code is available at https://github.com/ContextualAI/CLAIR_and_APO.

**Link**: [arxiv](http://arxiv.org/abs/2408.06266v4),  [pdf](http://arxiv.org/pdf/2408.06266v4)

**Tags**: cs.LG cs.AI cs.CL 



### From Lab to Field: Real-World Evaluation of an AI-Driven Smart Video   Solution to Enhance Community Safety
**Authors**: Shanle Yao, Babak Rahimi Ardabili, Armin Danesh Pazho, Ghazal Alinezhad Noghre, Christopher Neff, Lauren Bourque, Hamed Tabkhi

**Updated**: 2024-09-04T00:06:20Z

**Summary**: This article adopts and evaluates an AI-enabled Smart Video Solution (SVS) designed to enhance safety in the real world. The system integrates with existing infrastructure camera networks, leveraging recent advancements in AI for easy adoption. Prioritizing privacy and ethical standards, pose based data is used for downstream AI tasks such as anomaly detection. Cloud-based infrastructure and mobile app are deployed, enabling real-time alerts within communities. The SVS employs innovative data representation and visualization techniques, such as the Occupancy Indicator, Statistical Anomaly Detection, Bird's Eye View, and Heatmaps, to understand pedestrian behaviors and enhance public safety. Evaluation of the SVS demonstrates its capacity to convert complex computer vision outputs into actionable insights for stakeholders, community partners, law enforcement, urban planners, and social scientists. This article presents a comprehensive real-world deployment and evaluation of the SVS, implemented in a community college environment across 16 cameras. The system integrates AI-driven visual processing, supported by statistical analysis, database management, cloud communication, and user notifications. Additionally, the article evaluates the end-to-end latency from the moment an AI algorithm detects anomalous behavior in real-time at the camera level to the time stakeholders receive a notification. The results demonstrate the system's robustness, effectively managing 16 CCTV cameras with a consistent throughput of 16.5 frames per second (FPS) over a 21-hour period and an average end-to-end latency of 26.76 seconds between anomaly detection and alert issuance.

**Link**: [arxiv](http://arxiv.org/abs/2312.02078v2),  [pdf](http://arxiv.org/pdf/2312.02078v2)

**Tags**: cs.CV cs.AI cs.LG 



### The Responsible Foundation Model Development Cheatsheet: A Review of   Tools & Resources
**Authors**: Shayne Longpre, Stella Biderman, Alon Albalak, Hailey Schoelkopf, Daniel McDuff, Sayash Kapoor, Kevin Klyman, Kyle Lo, Gabriel Ilharco, Nay San, Maribeth Rauh, Aviya Skowron, Bertie Vidgen, Laura Weidinger, Arvind Narayanan, Victor Sanh, David Adelani, Percy Liang, Rishi Bommasani, Peter Henderson, Sasha Luccioni, Yacine Jernite, Luca Soldaini

**Updated**: 2024-09-03T23:03:41Z

**Summary**: Foundation model development attracts a rapidly expanding body of contributors, scientists, and applications. To help shape responsible development practices, we introduce the Foundation Model Development Cheatsheet: a growing collection of 250+ tools and resources spanning text, vision, and speech modalities. We draw on a large body of prior work to survey resources (e.g. software, documentation, frameworks, guides, and practical tools) that support informed data selection, processing, and understanding, precise and limitation-aware artifact documentation, efficient model training, advance awareness of the environmental impact from training, careful model evaluation of capabilities, risks, and claims, as well as responsible model release, licensing and deployment practices. We hope this curated collection of resources helps guide more responsible development. The process of curating this list, enabled us to review the AI development ecosystem, revealing what tools are critically missing, misused, or over-used in existing practices. We find that (i) tools for data sourcing, model evaluation, and monitoring are critically under-serving ethical and real-world needs, (ii) evaluations for model safety, capabilities, and environmental impact all lack reproducibility and transparency, (iii) text and particularly English-centric analyses continue to dominate over multilingual and multi-modal analyses, and (iv) evaluation of systems, rather than just models, is needed so that capabilities and impact are assessed in context.

**Link**: [arxiv](http://arxiv.org/abs/2406.16746v3),  [pdf](http://arxiv.org/pdf/2406.16746v3)

**Tags**: cs.LG cs.AI cs.CL 



### Site Selection for the Second Flyeye Telescope: A Simulation Study for   Optimizing Near-Earth Object Discovery
**Authors**: D. F√∂hring, L. Conversi, M. Micheli, E. D√∂lling, P. Ramirez Moreta

**Updated**: 2024-09-03T22:39:27Z

**Summary**: The European Space Agency (ESA) is developing a network of wide-field survey telescopes, named Flyeye, to improve the discovery of Near-Earth Objects (NEOs). The first telescope in the network will be located in the Northern Hemisphere on Mount Mufara (Italy), and a second Flyeye telescope, featuring increased detection capabilities, has just started the critical design phase. The potential location for the second Flyeye telescope is investigated by performing simulations of NEOs on impacting trajectories. Approximately 3000 impacting asteroids of two absolute magnitudes (H=25 and H=28) were propagated and tested for detectability by major existing surveys (Catalina, Pan-STARRS, ATLAS), the upcoming Vera Rubin Observatory (LSST), and possible Flyeye locations. Chile, South Africa, and a second facility in the Northern Hemisphere were considered. For each observatory, their past or planned pointing strategies were taken into account in the simulation. Before LSST deployment, a single Flyeye in the Southern Hemisphere performs similarly to a telescope in the Northern Hemisphere. When combined, having one telescope in the north and one in the south maximizes detections and number of unique objects detected. After LSST, southern and northern Flyeye telescopes remain complementary. Overall, simulations show that a second Flyeye in the south complements a Flyeye telescope in the north both before and after LSST. A Flyeye located at La Silla would take advantage of the excellent atmospheric conditions, while allowing a balance of assets across hemispheres.

**Link**: [arxiv](http://arxiv.org/abs/2409.02329v1),  [pdf](http://arxiv.org/pdf/2409.02329v1)

**Tags**: astro-ph.EP astro-ph.IM 



### Initial Development and Evaluation of the Creative Artificial   Intelligence through Recurring Developments and Determinations (CAIRDD)   System
**Authors**: Jeremy Straub, Zach Johnson

**Updated**: 2024-09-03T21:04:07Z

**Summary**: Computer system creativity is a key step on the pathway to artificial general intelligence (AGI). It is elusive, however, due to the fact that human creativity is not fully understood and, thus, it is difficult to develop this capability in software. Large language models (LLMs) provide a facsimile of creativity and the appearance of sentience, while not actually being either creative or sentient. While LLMs have created bona fide new content, in some cases - such as with harmful hallucinations - inadvertently, their deliberate creativity is seen by some to not match that of humans. In response to this challenge, this paper proposes a technique for enhancing LLM output creativity via an iterative process of concept injection and refinement. Initial work on the development of the Creative Artificial Intelligence through Recurring Developments and Determinations (CAIRDD) system is presented and the efficacy of key system components is evaluated.

**Link**: [arxiv](http://arxiv.org/abs/2409.02291v1),  [pdf](http://arxiv.org/pdf/2409.02291v1)

**Tags**: cs.AI cs.HC 



### SlipNet: Slip Cost Map for Autonomous Navigation on Heterogeneous   Deformable Terrains
**Authors**: Mubarak Yakubu, Yahya Zweiri, Ahmad Abubakar, Rana Azzam, Ruqayya Alhammadi, Lakmal Seneviratne

**Updated**: 2024-09-03T20:09:07Z

**Summary**: Autonomous space rovers face significant challenges when navigating deformable and heterogeneous terrains during space exploration. The variability in terrain types, influenced by different soil properties, often results in severe wheel slip, compromising navigation efficiency and potentially leading to entrapment. This paper proposes SlipNet, an approach for predicting slip in segmented regions of heterogeneous deformable terrain surfaces to enhance navigation algorithms. Unlike previous methods, SlipNet does not depend on prior terrain classification, reducing prediction errors and misclassifications through dynamic terrain segmentation and slip assignment during deployment while maintaining a history of terrain classes. This adaptive reclassification mechanism has improved prediction performance. Extensive simulation results demonstrate that our model (DeepLab v3+ + SlipNet) achieves better slip prediction performance than the TerrainNet, with a lower mean absolute error (MAE) in five terrain sample tests.

**Link**: [arxiv](http://arxiv.org/abs/2409.02273v1),  [pdf](http://arxiv.org/pdf/2409.02273v1)

**Tags**: cs.RO 



### Taking the Next Step with Generative Artificial Intelligence: The   Transformative Role of Multimodal Large Language Models in Science Education
**Authors**: Arne Bewersdorff, Christian Hartmann, Marie Hornberger, Kathrin Se√üler, Maria Bannert, Enkelejda Kasneci, Gjergji Kasneci, Xiaoming Zhai, Claudia Nerdel

**Updated**: 2024-09-03T19:43:53Z

**Summary**: The integration of Artificial Intelligence (AI), particularly Large Language Model (LLM)-based systems, in education has shown promise in enhancing teaching and learning experiences. However, the advent of Multimodal Large Language Models (MLLMs) like GPT-4 with vision (GPT-4V), capable of processing multimodal data including text, sound, and visual inputs, opens a new era of enriched, personalized, and interactive learning landscapes in education. Grounded in theory of multimedia learning, this paper explores the transformative role of MLLMs in central aspects of science education by presenting exemplary innovative learning scenarios. Possible applications for MLLMs could range from content creation to tailored support for learning, fostering competencies in scientific practices, and providing assessment and feedback. These scenarios are not limited to text-based and uni-modal formats but can be multimodal, increasing thus personalization, accessibility, and potential learning effectiveness. Besides many opportunities, challenges such as data protection and ethical considerations become more salient, calling for robust frameworks to ensure responsible integration. This paper underscores the necessity for a balanced approach in implementing MLLMs, where the technology complements rather than supplants the educator's role, ensuring thus an effective and ethical use of AI in science education. It calls for further research to explore the nuanced implications of MLLMs on the evolving role of educators and to extend the discourse beyond science education to other disciplines. Through the exploration of potentials, challenges, and future implications, we aim to contribute to a preliminary understanding of the transformative trajectory of MLLMs in science education and beyond.

**Link**: [arxiv](http://arxiv.org/abs/2401.00832v2),  [pdf](http://arxiv.org/pdf/2401.00832v2)

**Tags**: cs.AI cs.CY 



### Preference Learning Algorithms Do Not Learn Preference Rankings
**Authors**: Angelica Chen, Sadhika Malladi, Lily H. Zhang, Xinyi Chen, Qiuyi Zhang, Rajesh Ranganath, Kyunghyun Cho

**Updated**: 2024-09-03T19:37:27Z

**Summary**: Preference learning algorithms (e.g., RLHF and DPO) are frequently used to steer LLMs to produce generations that are more preferred by humans, but our understanding of their inner workings is still limited. In this work, we study the conventional wisdom that preference learning trains models to assign higher likelihoods to more preferred outputs than less preferred outputs, measured via $\textit{ranking accuracy}$. Surprisingly, we find that most state-of-the-art preference-tuned models achieve a ranking accuracy of less than 60% on common preference datasets. We furthermore derive the $\textit{idealized ranking accuracy}$ that a preference-tuned LLM would achieve if it optimized the DPO or RLHF objective perfectly. We demonstrate that existing models exhibit a significant $\textit{alignment gap}$ -- $\textit{i.e.}$, a gap between the observed and idealized ranking accuracies. We attribute this discrepancy to the DPO objective, which is empirically and theoretically ill-suited to fix even mild ranking errors in the reference model, and derive a simple and efficient formula for quantifying the difficulty of learning a given preference datapoint. Finally, we demonstrate that ranking accuracy strongly correlates with the empirically popular win rate metric when the model is close to the reference model used in the objective, shedding further light on the differences between on-policy (e.g., RLHF) and off-policy (e.g., DPO) preference learning algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2405.19534v2),  [pdf](http://arxiv.org/pdf/2405.19534v2)

**Tags**: cs.LG cs.AI cs.CL 



### MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in   LLMs
**Authors**: Saeid Asgari Taghanaki, Aliasgahr Khani, Amir Khasahmadi

**Updated**: 2024-09-03T19:31:03Z

**Summary**: Existing benchmarks for large language models (LLMs) increasingly struggle to differentiate between top-performing models, underscoring the need for more challenging evaluation frameworks. We introduce MMLU-Pro+, an enhanced benchmark building upon MMLU-Pro to assess shortcut learning and higher-order reasoning in LLMs. By incorporating questions with multiple correct answers across diverse domains, MMLU-Pro+ tests LLMs' ability to engage in complex reasoning and resist simplistic problem-solving strategies. Our results show that MMLU-Pro+ maintains MMLU-Pro's difficulty while providing a more rigorous test of model discrimination, particularly in multi-correct answer scenarios. We introduce novel metrics like shortcut selection ratio and correct pair identification ratio, offering deeper insights into model behavior and anchoring bias. Evaluations of five state-of-the-art LLMs reveal significant performance gaps, highlighting variations in reasoning abilities and bias susceptibility. We release the dataset and evaluation codes at \url{https://github.com/asgsaeid/mmlu-pro-plus}.

**Link**: [arxiv](http://arxiv.org/abs/2409.02257v1),  [pdf](http://arxiv.org/pdf/2409.02257v1)

**Tags**: cs.CL cs.LG 



### A Voter-Based Stochastic Rejection-Method Framework for Asymptotically   Safe Language Model Outputs
**Authors**: Jake R. Watts, Joel Sokol

**Updated**: 2024-09-03T19:28:39Z

**Summary**: This paper proposes a new method for preventing unsafe or otherwise low quality large language model (LLM) outputs, by leveraging the stochasticity of LLMs. We propose a system whereby LLM checkers vote on the acceptability of a generated output, regenerating it if a threshold of disapproval is reached, until sufficient checkers approve. We further propose estimators for cost and failure rate, and based on those estimators and experimental data tailored to the application, we propose an algorithm that achieves a desired failure rate at the least possible cost. We demonstrate that, under these models, failure rate decreases exponentially as a function of cost when voter count and threshold are chosen according to the algorithm, and that the models reasonably estimate the actual performance of such a system in action, even with limited data.

**Link**: [arxiv](http://arxiv.org/abs/2407.16994v2),  [pdf](http://arxiv.org/pdf/2407.16994v2)

**Tags**: cs.AI cs.CL cs.LG 



### ARN: Analogical Reasoning on Narratives
**Authors**: Zhivar Sourati, Filip Ilievski, Pia Sommerauer, Yifan Jiang

**Updated**: 2024-09-03T19:27:19Z

**Summary**: As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and chain-of-thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners.

**Link**: [arxiv](http://arxiv.org/abs/2310.00996v4),  [pdf](http://arxiv.org/pdf/2310.00996v4)

**Tags**: cs.CL 



### Therapy as an NLP Task: Psychologists' Comparison of LLMs and Human   Peers in CBT
**Authors**: Zainab Iftikhar, Sean Ransom, Amy Xiao, Jeff Huang

**Updated**: 2024-09-03T19:19:13Z

**Summary**: Wider access to therapeutic care is one of the biggest challenges in mental health treatment. Due to institutional barriers, some people seeking mental health support have turned to large language models (LLMs) for personalized therapy, even though these models are largely unsanctioned and untested. We investigate the potential and limitations of using LLMs as providers of evidence-based therapy by using mixed methods clinical metrics. Using HELPERT, a prompt run on a large language model using the same process and training as a comparative group of peer counselors, we replicated publicly accessible mental health conversations rooted in Cognitive Behavioral Therapy (CBT) to compare session dynamics and counselor's CBT-based behaviors between original peer support sessions and their reconstructed HELPERT sessions. Two licensed, CBT-trained clinical psychologists evaluated the sessions using the Cognitive Therapy Rating Scale and provided qualitative feedback. Our findings show that the peer sessions are characterized by empathy, small talk, therapeutic alliance, and shared experiences but often exhibit therapist drift. Conversely, HELPERT reconstructed sessions exhibit minimal therapist drift and higher adherence to CBT methods but display a lack of collaboration, empathy, and cultural understanding. Through CTRS ratings and psychologists' feedback, we highlight the importance of human-AI collaboration for scalable mental health. Our work outlines the ethical implication of imparting human-like subjective qualities to LLMs in therapeutic settings, particularly the risk of deceptive empathy, which may lead to unrealistic patient expectations and potential harm.

**Link**: [arxiv](http://arxiv.org/abs/2409.02244v1),  [pdf](http://arxiv.org/pdf/2409.02244v1)

**Tags**: cs.HC cs.CL I.2.7; J.4 



### Zyda: A 1.3T Dataset for Open Language Modeling
**Authors**: Yury Tokpanov, Beren Millidge, Paolo Glorioso, Jonathan Pilault, Adam Ibrahim, James Whittington, Quentin Anthony

**Updated**: 2024-09-03T19:11:11Z

**Summary**: The size of large language models (LLMs) has scaled dramatically in recent years and their computational and data requirements have surged correspondingly. State-of-the-art language models, even at relatively smaller sizes, typically require training on at least a trillion tokens. This rapid advancement has eclipsed the growth of open-source datasets available for large-scale LLM pretraining. In this paper, we introduce Zyda (Zyphra Dataset), a dataset under a permissive license comprising 1.3 trillion tokens, assembled by integrating several major respected open-source datasets into a single, high-quality corpus. We apply rigorous filtering and deduplication processes, both within and across datasets, to maintain and enhance the quality derived from the original datasets. Our evaluations show that Zyda not only competes favorably with other open datasets like Dolma, FineWeb, and RefinedWeb, but also substantially improves the performance of comparable models from the Pythia suite. Our rigorous data processing methods significantly enhance Zyda's effectiveness, outperforming even the best of its constituent datasets when used independently.

**Link**: [arxiv](http://arxiv.org/abs/2406.01981v2),  [pdf](http://arxiv.org/pdf/2406.01981v2)

**Tags**: cs.CL cs.AI 



### Open6G OTIC: A Blueprint for Programmable O-RAN and 3GPP Testing   Infrastructure
**Authors**: Gabriele Gemmi, Michele Polese, Pedram Johari, Stefano Maxenti, Michael Seltser, Tommaso Melodia

**Updated**: 2024-09-03T19:07:53Z

**Summary**: Softwarized and programmable Radio Access Networks (RANs) come with virtualized and disaggregated components, increasing the supply chain robustness and the flexibility and dynamism of the network deployments. This is a key tenet of Open RAN, with open interfaces across disaggregated components specified by the O-RAN ALLIANCE. It is mandatory, however, to validate that all components are compliant with the specifications and can successfully interoperate, without performance gaps with traditional, monolithic appliances. Open Testing & Integration Centers (OTICs) are entities that can verify such interoperability and adherence to the standard through rigorous testing. However, how to design, instrument, and deploy an OTIC which can offer testing for multiple tenants, heterogeneous devices, and is ready to support automated testing is still an open challenge. In this paper, we introduce a blueprint for a programmable OTIC testing infrastructure, based on the design and deployment of the Open6G OTIC at Northeastern University, Boston, and provide insights on technical challenges and solutions for O-RAN testing at scale.

**Link**: [arxiv](http://arxiv.org/abs/2409.02237v1),  [pdf](http://arxiv.org/pdf/2409.02237v1)

**Tags**: cs.NI cs.SY eess.SY 



### SmileyLlama: Modifying Large Language Models for Directed Chemical Space   Exploration
**Authors**: Joseph M. Cavanagh, Kunyang Sun, Andrew Gritsevskiy, Dorian Bagni, Thomas D. Bannister, Teresa Head-Gordon

**Updated**: 2024-09-03T18:59:20Z

**Summary**: Here we show that a Large Language Model (LLM) can serve as a foundation model for a Chemical Language Model (CLM) which performs at or above the level of CLMs trained solely on chemical SMILES string data. Using supervised fine-tuning (SFT) and direct preference optimization (DPO) on the open-source Llama LLM, we demonstrate that we can train an LLM to respond to prompts such as generating molecules with properties of interest to drug development. This overall framework allows an LLM to not just be a chatbot client for chemistry and materials tasks, but can be adapted to speak more directly as a CLM which can generate molecules with user-specified properties.

**Link**: [arxiv](http://arxiv.org/abs/2409.02231v1),  [pdf](http://arxiv.org/pdf/2409.02231v1)

**Tags**: physics.chem-ph cs.LG 



### CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through   Corpus Retrieval and Augmentation
**Authors**: Ingo Ziegler, Abdullatif K√∂ksal, Desmond Elliott, Hinrich Sch√ºtze

**Updated**: 2024-09-03T17:54:40Z

**Summary**: Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given the few-shot examples, we use large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology question-answering (QA), medicine QA and commonsense QA as well as summarization. Our experiments show that CRAFT-based models outperform or achieve comparable performance to general LLMs for QA tasks, while CRAFT-based summarization models outperform models trained on human-curated data by 46 preference points.

**Link**: [arxiv](http://arxiv.org/abs/2409.02098v1),  [pdf](http://arxiv.org/pdf/2409.02098v1)

**Tags**: cs.CL cs.AI cs.LG 



### Investigating the Robustness of LLMs on Math Word Problems
**Authors**: Ujjwala Anantheswaran, Himanshu Gupta, Kevin Scaria, Shreyas Verma, Chitta Baral, Swaroop Mishra

**Updated**: 2024-09-03T17:48:55Z

**Summary**: Large Language Models (LLMs) excel at various tasks, including solving math word problems (MWPs), but struggle with real-world problems containing irrelevant information. To address this, we propose a prompting framework that generates adversarial variants of MWPs by adding irrelevant variables. We introduce a dataset, ProbleMATHIC, containing both adversarial and non-adversarial MWPs. Our experiments reveal that LLMs are susceptible to distraction by numerical noise, resulting in an average relative performance drop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2, Mistral) on the adversarial samples from our dataset. Fine-tuning on adversarial training instances improves performance on adversarial MWPs by ~8%, indicating increased robustness to noise and better ability to identify relevant data for reasoning. Finally, to assess the generalizability of our prompting framework, we introduce GSM-8K-Adv, an adversarial variant of the GSM-8K benchmark. LLMs continue to struggle when faced with adversarial information, reducing performance by up to ~6%.

**Link**: [arxiv](http://arxiv.org/abs/2406.15444v2),  [pdf](http://arxiv.org/pdf/2406.15444v2)

**Tags**: cs.CL 



### Physical Rule-Guided Convolutional Neural Network
**Authors**: Kishor Datta Gupta, Marufa Kamal, Rakib Hossain Rifat, Mohd Ariful Haque, Roy George

**Updated**: 2024-09-03T17:32:35Z

**Summary**: The black-box nature of Convolutional Neural Networks (CNNs) and their reliance on large datasets limit their use in complex domains with limited labeled data. Physics-Guided Neural Networks (PGNNs) have emerged to address these limitations by integrating scientific principles and real-world knowledge, enhancing model interpretability and efficiency. This paper proposes a novel Physics-Guided CNN (PGCNN) architecture that incorporates dynamic, trainable, and automated LLM-generated, widely recognized rules integrated into the model as custom layers to address challenges like limited data and low confidence scores. The PGCNN is evaluated on multiple datasets, demonstrating superior performance compared to a baseline CNN model. Key improvements include a significant reduction in false positives and enhanced confidence scores for true detection. The results highlight the potential of PGCNNs to improve CNN performance for broader application areas.

**Link**: [arxiv](http://arxiv.org/abs/2409.02081v1),  [pdf](http://arxiv.org/pdf/2409.02081v1)

**Tags**: cs.CV 



### Synthetic Data Generation and Automated Multidimensional Data Labeling   for AI/ML in General and Circular Coordinates
**Authors**: Alice Williams, Boris Kovalerchuk

**Updated**: 2024-09-03T17:26:50Z

**Summary**: Insufficient amounts of available training data is a critical challenge for both development and deployment of artificial intelligence and machine learning (AI/ML) models. This paper proposes a unified approach to both synthetic data generation (SDG) and automated data labeling (ADL) with a unified SDG-ADL algorithm. SDG-ADL uses multidimensional (n-D) representations of data visualized losslessly with General Line Coordinates (GLCs), relying on reversible GLC properties to visualize n-D data in multiple GLCs. This paper demonstrates use of the new Circular Coordinates in Static and Dynamic forms, used with Parallel Coordinates and Shifted Paired Coordinates, since each GLC exemplifies unique data properties, such as interattribute n-D distributions and outlier detection. The approach is interactively implemented in computer software with the Dynamic Coordinates Visualization system (DCVis). Results with real data are demonstrated in case studies, evaluating impact on classifiers.

**Link**: [arxiv](http://arxiv.org/abs/2409.02079v1),  [pdf](http://arxiv.org/pdf/2409.02079v1)

**Tags**: cs.LG 



### RACONTEUR: A Knowledgeable, Insightful, and Portable LLM-Powered Shell   Command Explainer
**Authors**: Jiangyi Deng, Xinfeng Li, Yanjiao Chen, Yijie Bai, Haiqin Weng, Yan Liu, Tao Wei, Wenyuan Xu

**Updated**: 2024-09-03T17:22:00Z

**Summary**: Malicious shell commands are linchpins to many cyber-attacks, but may not be easy to understand by security analysts due to complicated and often disguised code structures. Advances in large language models (LLMs) have unlocked the possibility of generating understandable explanations for shell commands. However, existing general-purpose LLMs suffer from a lack of expert knowledge and a tendency to hallucinate in the task of shell command explanation. In this paper, we present Raconteur, a knowledgeable, expressive and portable shell command explainer powered by LLM. Raconteur is infused with professional knowledge to provide comprehensive explanations on shell commands, including not only what the command does (i.e., behavior) but also why the command does it (i.e., purpose). To shed light on the high-level intent of the command, we also translate the natural-language-based explanation into standard technique & tactic defined by MITRE ATT&CK, the worldwide knowledge base of cybersecurity. To enable Raconteur to explain unseen private commands, we further develop a documentation retriever to obtain relevant information from complementary documentations to assist the explanation process. We have created a large-scale dataset for training and conducted extensive experiments to evaluate the capability of Raconteur in shell command explanation. The experiments verify that Raconteur is able to provide high-quality explanations and in-depth insight of the intent of the command.

**Link**: [arxiv](http://arxiv.org/abs/2409.02074v1),  [pdf](http://arxiv.org/pdf/2409.02074v1)

**Tags**: cs.CR cs.HC cs.LG cs.SE 



### A Deployed Online Reinforcement Learning Algorithm In An Oral Health   Clinical Trial
**Authors**: Anna L. Trella, Kelly W. Zhang, Hinal Jajal, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, Susan A. Murphy

**Updated**: 2024-09-03T17:16:01Z

**Summary**: Dental disease is a prevalent chronic condition associated with substantial financial burden, personal suffering, and increased risk of systemic diseases. Despite widespread recommendations for twice-daily tooth brushing, adherence to recommended oral self-care behaviors remains sub-optimal due to factors such as forgetfulness and disengagement. To address this, we developed Oralytics, a mHealth intervention system designed to complement clinician-delivered preventative care for marginalized individuals at risk for dental disease. Oralytics incorporates an online reinforcement learning algorithm to determine optimal times to deliver intervention prompts that encourage oral self-care behaviors. We have deployed Oralytics in a registered clinical trial. The deployment required careful design to manage challenges specific to the clinical trials setting in the U.S. In this paper, we (1) highlight key design decisions of the RL algorithm that address these challenges and (2) conduct a re-sampling analysis to evaluate algorithm design decisions. A second phase (randomized control trial) of Oralytics is planned to start in spring 2025.

**Link**: [arxiv](http://arxiv.org/abs/2409.02069v1),  [pdf](http://arxiv.org/pdf/2409.02069v1)

**Tags**: cs.AI cs.HC 



### Lagrangian approach to origami vertex analysis: Kinematics
**Authors**: Matthew Grasinger, Andrew Gillman, Philip Buskohl

**Updated**: 2024-09-03T16:56:16Z

**Summary**: The use of origami in engineering has significantly expanded in recent years, spanning deployable structures across scales, folding robotics, and mechanical metamaterials. However, finding foldable paths can be a formidable task as the kinematics are determined by a nonlinear system of equations, often with several degrees of freedom. In this work, we leverage a Lagrangian approach to derive reduced-order compatibility conditions for rigid-facet origami vertices with reflection and rotational symmetries. Then, using the reduced-order conditions, we derive exact, multi-degree of freedom solutions for degree 6 and degree 8 vertices with prescribed symmetries. The exact kinematic solutions allow us to efficiently investigate the topology of allowable kinematics, including the consideration of a self-contact constraint, and then visually interpret the role of geometric design parameters on these admissible fold paths by monitoring the change in the kinematic topology. We then introduce a procedure to construct lower symmetry kinematic solutions by breaking symmetry of higher order kinematic solutions in a systematic way that preserves compatibility. The multi-degree of freedom solutions discovered here should assist with building intuition of the kinematic feasibility of higher degree origami vertices and also facilitate the development of new algorithmic procedures for origami-engineering design.

**Link**: [arxiv](http://arxiv.org/abs/2408.15460v2),  [pdf](http://arxiv.org/pdf/2408.15460v2)

**Tags**: cond-mat.soft math-ph math.MP 



### BEAVER: An Enterprise Benchmark for Text-to-SQL
**Authors**: Peter Baile Chen, Fabian Wenz, Yi Zhang, Moe Kayali, Nesime Tatbul, Michael Cafarella, √áaƒüatay Demiralp, Michael Stonebraker

**Updated**: 2024-09-03T16:37:45Z

**Summary**: Existing text-to-SQL benchmarks have largely been constructed using publicly available tables from the web with human-generated tests containing question and SQL statement pairs. They typically show very good results and lead people to think that LLMs are effective at text-to-SQL tasks. In this paper, we apply off-the-shelf LLMs to a benchmark containing enterprise data warehouse data. In this environment, LLMs perform poorly, even when standard prompt engineering and RAG techniques are utilized. As we will show, the reasons for poor performance are largely due to three characteristics: (1) public LLMs cannot train on enterprise data warehouses because they are largely in the "dark web", (2) schemas of enterprise tables are more complex than the schemas in public data, which leads the SQL-generation task innately harder, and (3) business-oriented questions are often more complex, requiring joins over multiple tables and aggregations. As a result, we propose a new dataset BEAVER, sourced from real enterprise data warehouses together with natural language queries and their correct SQL statements which we collected from actual user history. We evaluated this dataset using recent LLMs and demonstrated their poor performance on this task. We hope this dataset will facilitate future researchers building more sophisticated text-to-SQL systems which can do better on this important class of data.

**Link**: [arxiv](http://arxiv.org/abs/2409.02038v1),  [pdf](http://arxiv.org/pdf/2409.02038v1)

**Tags**: cs.CL cs.AI cs.DB 



