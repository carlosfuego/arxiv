# Arxiv Results
## Keyword: kv cache 
 ### Performance Analysis of HPC applications on the Aurora Supercomputer:   Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs
**Authors**: Huda Ibeid, Vikram Narayana, Jeongnim Kim, Anthony Nguyen, Vitali Morozov, Ye Luo

**Updated**: 2025-04-04T17:56:44Z

**Summary**: The Aurora supercomputer is an exascale-class system designed to tackle some of the most demanding computational workloads. Equipped with both High Bandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in performance, latency, and capacity. This paper presents a comprehensive analysis of the memory systems on the Aurora supercomputer, with a focus on evaluating the trade-offs between HBM and DDR memory systems. We explore how different memory configurations, including memory modes (Flat and Cache) and clustering modes (Quad and SNC4), influence key system performance metrics such as memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication bandwidth. Additionally, we examine the performance of three representative HPC applications -- HACC, QMCPACK, and BFS -- each illustrating the impact of memory configurations on performance. By using microbenchmarks and application-level analysis, we provide insights into how to select the optimal memory system and configuration to maximize performance based on the application characteristics. The findings presented in this paper offer guidance for users of the Aurora system and similar exascale systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.03632v1),  [pdf](http://arxiv.org/pdf/2504.03632v1)

**Tags**: cs.DC cs.AR cs.PF 



### Adaptive Semantic Prompt Caching with VectorQ
**Authors**: Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez

**Updated**: 2025-04-04T16:51:15Z

**Summary**: Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different embeddings. We propose VectorQ, an online framework with a threshold convergence guarantee to learn embedding-specific threshold regions that adapt to the uncertainty of an embedding. Through evaluations on a combination of three diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 26x increases in cache hit rate and error rate reductions up to 74%.

**Link**: [arxiv](http://arxiv.org/abs/2502.03771v2),  [pdf](http://arxiv.org/pdf/2502.03771v2)

**Tags**: cs.LG cs.CL 



### Performance Modeling of Data Storage Systems using Generative Models
**Authors**: Abdalaziz Rashid Al-Maeeni, Aziz Temirkhanov, Artem Ryzhikov, Mikhail Hushchyn

**Updated**: 2025-04-04T15:30:20Z

**Summary**: High-precision modeling of systems is one of the main areas of industrial data analysis. Models of systems, their digital twins, are used to predict their behavior under various conditions. We have developed several models of a storage system using machine learning-based generative models. The system consists of several components: hard disk drive (HDD) and solid-state drive (SSD) storage pools with different RAID schemes and cache. Each storage component is represented by a probabilistic model that describes the probability distribution of the component performance in terms of IOPS and latency, depending on their configuration and external data load parameters. The results of the experiments demonstrate the errors of 4-10 % for IOPS and 3-16 % for latency predictions depending on the components and models of the system. The predictions show up to 0.99 Pearson correlation with Little's law, which can be used for unsupervised reliability checks of the models. In addition, we present novel data sets that can be used for benchmarking regression algorithms, conditional generative models, and uncertainty estimation methods in machine learning.

**Link**: [arxiv](http://arxiv.org/abs/2307.02073v2),  [pdf](http://arxiv.org/pdf/2307.02073v2)

**Tags**: cs.LG cs.AI cs.PF 



### Optimistic Learning for Communication Networks
**Authors**: George Iosifidis, Naram Mhaisen, Douglas J. Leith

**Updated**: 2025-04-04T14:55:27Z

**Summary**: AI/ML-based tools are at the forefront of resource management solutions for communication networks. Deep learning, in particular, is highly effective in facilitating fast and high-performing decision-making whenever representative training data is available to build offline accurate models. Conversely, online learning solutions do not require training and enable adaptive decisions based on runtime observations, alas are often overly conservative. This extensive tutorial proposes the use of optimistic learning (OpL) as a decision engine for resource management frameworks in modern communication systems. When properly designed, such solutions can achieve fast and high-performing decisions -- comparable to offline-trained models -- while preserving the robustness and performance guarantees of the respective online learning approaches. We introduce the fundamental concepts, algorithms and results of OpL, discuss the roots of this theory and present different approaches to defining and achieving optimism. We proceed to showcase how OpL can enhance resource management in communication networks for several key problems such as caching, edge computing, network slicing, and workload assignment in decentralized O-RAN platforms. Finally, we discuss the open challenges that must be addressed to unlock the full potential of this new resource management approach.

**Link**: [arxiv](http://arxiv.org/abs/2504.03499v1),  [pdf](http://arxiv.org/pdf/2504.03499v1)

**Tags**: cs.NI cs.LG 



### EVOS: Efficient Implicit Neural Training via EVOlutionary Selector
**Authors**: Weixiang Zhang, Shuzhao Xie, Chengwei Ren, Siyi Xie, Chen Tang, Shijia Ge, Mingzi Wang, Zhi Wang

**Updated**: 2025-04-04T13:27:49Z

**Summary**: We propose EVOlutionary Selector (EVOS), an efficient training paradigm for accelerating Implicit Neural Representation (INR). Unlike conventional INR training that feeds all samples through the neural network in each iteration, our approach restricts training to strategically selected points, reducing computational overhead by eliminating redundant forward passes. Specifically, we treat each sample as an individual in an evolutionary process, where only those fittest ones survive and merit inclusion in training, adaptively evolving with the neural network dynamics. While this is conceptually similar to Evolutionary Algorithms, their distinct objectives (selection for acceleration vs. iterative solution optimization) require a fundamental redefinition of evolutionary mechanisms for our context. In response, we design sparse fitness evaluation, frequency-guided crossover, and augmented unbiased mutation to comprise EVOS. These components respectively guide sample selection with reduced computational cost, enhance performance through frequency-domain balance, and mitigate selection bias from cached evaluation. Extensive experiments demonstrate that our method achieves approximately 48%-66% reduction in training time while ensuring superior convergence without additional cost, establishing state-of-the-art acceleration among recent sampling-based strategies.

**Link**: [arxiv](http://arxiv.org/abs/2412.10153v3),  [pdf](http://arxiv.org/pdf/2412.10153v3)

**Tags**: cs.CV cs.MM cs.NE 



### ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions
**Authors**: Vincenzo Petrolo, Flavia Guella, Michele Caon, Pasquale Davide Schiavone, Guido Masera, Maurizio Martina

**Updated**: 2025-04-04T11:12:18Z

**Summary**: Modern data-driven applications expose limitations of von Neumann architectures - extensive data movement, low throughput, and poor energy efficiency. Accelerators improve performance but lack flexibility and require data transfers. Existing compute in- and near-memory solutions mitigate these issues but face usability challenges due to data placement constraints. We propose a novel cache architecture that doubles as a tightly-coupled compute-near-memory coprocessor. Our RISC-V cache controller executes custom instructions from the host CPU using vector operations dispatched to near-memory vector processing units within the cache memory subsystem. This architecture abstracts memory synchronization and data mapping from application software while offering software-based ISA extensibility. Our implementation shows $30\times$ to $84\times$ performance improvement when operating on 8-bit data over the same system with a traditional cache when executing a worst-case 32-bit CNN workload, with only $41.3\%$ area overhead.

**Link**: [arxiv](http://arxiv.org/abs/2504.02533v2),  [pdf](http://arxiv.org/pdf/2504.02533v2)

**Tags**: cs.AR 



### Model Reveals What to Cache: Profiling-Based Feature Reuse for Video   Diffusion Models
**Authors**: Xuran Ma, Yexin Liu, Yaofu Liu, Xianfeng Wu, Mingzhe Zheng, Zihao Wang, Ser-Nam Lim, Harry Yang

**Updated**: 2025-04-04T03:30:15Z

**Summary**: Recent advances in diffusion models have demonstrated remarkable capabilities in video generation. However, the computational intensity remains a significant challenge for practical applications. While feature caching has been proposed to reduce the computational burden of diffusion models, existing methods typically overlook the heterogeneous significance of individual blocks, resulting in suboptimal reuse and degraded output quality. To this end, we address this gap by introducing ProfilingDiT, a novel adaptive caching strategy that explicitly disentangles foreground and background-focused blocks. Through a systematic analysis of attention distributions in diffusion models, we reveal a key observation: 1) Most layers exhibit a consistent preference for either foreground or background regions. 2) Predicted noise shows low inter-step similarity initially, which stabilizes as denoising progresses. This finding inspires us to formulate a selective caching strategy that preserves full computation for dynamic foreground elements while efficiently caching static background features. Our approach substantially reduces computational overhead while preserving visual fidelity. Extensive experiments demonstrate that our framework achieves significant acceleration (e.g., 2.01 times speedup for Wan2.1) while maintaining visual fidelity across comprehensive quality metrics, establishing a viable method for efficient video generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.03140v1),  [pdf](http://arxiv.org/pdf/2504.03140v1)

**Tags**: cs.CV 



### CacheBlend: Fast Large Language Model Serving for RAG with Cached   Knowledge Fusion
**Authors**: Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang

**Updated**: 2025-04-03T22:49:22Z

**Summary**: Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text's cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized.   This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache.

**Link**: [arxiv](http://arxiv.org/abs/2405.16444v3),  [pdf](http://arxiv.org/pdf/2405.16444v3)

**Tags**: cs.LG 



### LLM Library Learning Fails: A LEGO-Prover Case Study
**Authors**: Ian Berlot-Attwell, Frank Rudzicz, Xujie Si

**Updated**: 2025-04-03T21:53:51Z

**Summary**: Recent advancements in the coding, reasoning, and tool-using abilities of LLMs have spurred interest in library learning (i.e., online learning through the creation, storage, and retrieval of reusable and composable functions, knowledge, checklists, or lemmas). Such systems often promise improved task performance through the automatic creation of broadly applicable tools, as well as superior computational performance through the caching of reasoning (i.e., the storage of generated tools). However, we find strong reason to be skeptical. We perform a deep dive into one such system, LEGO-Prover, which purports to learn reusable lemmas for mathematical reasoning. We find no evidence of the direct reuse of learned lemmas, and find evidence against the soft reuse of learned lemmas (i.e., reuse by modifying relevant examples). Crucially, we find that LEGO-Prover does not in fact improve over the simple baseline of prompting the model - the improvements in task accuracy vanish once computational cost is accounted for. Our findings suggest that serious misconceptions exist as to the effectiveness of these techniques, that a serious re-examination of the state of LLM-based library learning is required, and that we require much stronger standards for evaluation including behavioural analysis and ensuring that an equal computational budget is used for baselines.

**Link**: [arxiv](http://arxiv.org/abs/2504.03048v1),  [pdf](http://arxiv.org/pdf/2504.03048v1)

**Tags**: cs.LG cs.CL 



### Localized Definitions and Distributed Reasoning: A Proof-of-Concept   Mechanistic Interpretability Study via Activation Patching
**Authors**: Nooshin Bahador

**Updated**: 2025-04-03T18:54:50Z

**Summary**: This study investigates the localization of knowledge representation in fine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching (CLAP), a method that identifies critical neural layers responsible for correct answer generation. The model was fine-tuned on 9,958 PubMed abstracts (epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions) using two configurations with validation loss monitoring for early stopping. CLAP involved (1) caching clean (correct answer) and corrupted (incorrect answer) activations, (2) computing logit difference to quantify model preference, and (3) patching corrupted activations with clean ones to assess recovery. Results revealed three findings: First, patching the first feedforward layer recovered 56% of correct preference, demonstrating that associative knowledge is distributed across multiple layers. Second, patching the final output layer completely restored accuracy (100% recovery), indicating that definitional knowledge is localised. The stronger clean logit difference for definitional questions further supports this localized representation. Third, minimal recovery from convolutional layer patching (13.6%) suggests low-level features contribute marginally to high-level reasoning. Statistical analysis confirmed significant layer-specific effects (p<0.01). These findings demonstrate that factual knowledge is more localized and associative knowledge depends on distributed representations. We also showed that editing efficacy depends on task type. Our findings not only reconcile conflicting observations about localization in model editing but also emphasize on using task-adaptive techniques for reliable, interpretable updates.

**Link**: [arxiv](http://arxiv.org/abs/2504.02976v1),  [pdf](http://arxiv.org/pdf/2504.02976v1)

**Tags**: cs.LG cs.AI 



### Improved Compact Genetic Algorithms with Efficient Caching
**Authors**: Prasanta Dutta, Anirban Mukhopadhyay

**Updated**: 2025-04-03T18:47:26Z

**Summary**: Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic Algorithms (GAs) that use a probability vector representation of the population instead of the complete population. cGAs have been shown to significantly reduce the number of function evaluations required while producing outcomes similar to those of classical GAs. However, cGAs have a tendency to repeatedly generate the same chromosomes as they approach convergence, resulting in unnecessary evaluations of identical chromosomes. This article introduces the concept of caching in cGAs as a means of avoiding redundant evaluations of the same chromosomes. Our proposed approach operates equivalently to cGAs, but enhances the algorithm's time efficiency by reducing the number of function evaluations. We also present a data structure for efficient cache maintenance to ensure low overhead. The proposed caching approach has an asymptotically constant time complexity on average. The proposed method further generalizes the caching mechanism with higher selection pressure for elitism-based cGAs. We conduct a rigorous analysis based on experiments on benchmark optimization problems using two well-known cache replacement strategies. The results demonstrate that caching significantly reduces the number of function evaluations required while maintaining the same level of performance accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.02972v1),  [pdf](http://arxiv.org/pdf/2504.02972v1)

**Tags**: cs.NE cs.AI 



### HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented   Generation with Reranker KV-Cache Reuse
**Authors**: Yuwei An, Yihua Cheng, Seo Jin Park, Junchen Jiang

**Updated**: 2025-04-03T17:08:42Z

**Summary**: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing the performance of large language models (LLMs) by integrating external knowledge into the generation process. A key component of RAG pipelines is the reranker, which selects the most relevant documents from a pool of retrieved candidates and significantly improves the quality of the generated responses. While rerankers refine the selection of retrieved documents in RAG pipelines, they introduce computational challenges that hinder high throughput and low latency. To address this problem, we propose HyperRAG, a system that optimizes the trade-off between quality and efficiency in RAG pipelines by leveraging KV-cache reuse for efficient reranker inference. By reusing document-side KV-cache, HyperRAG achieves both high-quality generation and system-level efficiency. To fully realize the benefits of KV-cache reuse, HyperRAG incorporates a range of system-level optimizations designed to enhance efficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3 throughput improvement with decoder-only rerankers while also delivering higher downstream performance compared with traditional RAG service.

**Link**: [arxiv](http://arxiv.org/abs/2504.02921v1),  [pdf](http://arxiv.org/pdf/2504.02921v1)

**Tags**: cs.CL 



### Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware   Masking
**Authors**: Marco Federici, Davide Belli, Mart van Baalen, Amir Jalalirad, Andrii Skliar, Bence Major, Markus Nagel, Paul Whatmough

**Updated**: 2025-04-03T13:28:51Z

**Summary**: While mobile devices provide ever more compute power, improvements in DRAM bandwidth are much slower. This is unfortunate for large language model (LLM) token generation, which is heavily memory-bound. Previous work has proposed to leverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce effective DRAM bandwidth per token. However, more recent LLMs use SwiGLU instead of ReLU, which results in little inherent sparsity. While SwiGLU activations can be pruned based on magnitude, the resulting sparsity patterns are difficult to predict, rendering previous approaches ineffective. To circumvent this issue, our work introduces Dynamic Input Pruning (DIP): a predictor-free dynamic sparsification approach, which preserves accuracy with minimal fine-tuning. DIP can further use lightweight LoRA adapters to regain some performance lost during sparsification. Lastly, we describe a novel cache-aware masking strategy, which considers the cache state and activation magnitude to further increase cache hit rate, improving LLM token rate on mobile devices. DIP outperforms other methods in terms of accuracy, memory and throughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP achieves a 46\% reduction in memory and 40\% increase in throughput with $<$ 0.1 loss in perplexity when compared to streaming the dense model from Flash. The open source code for HW simulator, methods, and experiments in this paper is available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .

**Link**: [arxiv](http://arxiv.org/abs/2412.01380v2),  [pdf](http://arxiv.org/pdf/2412.01380v2)

**Tags**: cs.LG cs.CL 



### Cognitive Memory in Large Language Models
**Authors**: Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, Yong Wu

**Updated**: 2025-04-03T09:58:19Z

**Summary**: This paper examines memory mechanisms in Large Language Models (LLMs), emphasizing their importance for context-rich responses, reduced hallucinations, and improved efficiency. It categorizes memory into sensory, short-term, and long-term, with sensory memory corresponding to input prompts, short-term memory processing immediate context, and long-term memory implemented via external databases or structures. The text-based memory section covers acquisition (selection and summarization), management (updating, accessing, storing, and resolving conflicts), and utilization (full-text search, SQL queries, semantic search). The KV cache-based memory section discusses selection methods (regularity-based summarization, score-based approaches, special token embeddings) and compression techniques (low-rank compression, KV merging, multimodal compression), along with management strategies like offloading and shared attention mechanisms. Parameter-based memory methods (LoRA, TTT, MoE) transform memories into model parameters to enhance efficiency, while hidden-state-based memory approaches (chunk mechanisms, recurrent transformers, Mamba model) improve long-text processing by combining RNN hidden states with current methods. Overall, the paper offers a comprehensive analysis of LLM memory mechanisms, highlighting their significance and future research directions.

**Link**: [arxiv](http://arxiv.org/abs/2504.02441v1),  [pdf](http://arxiv.org/pdf/2504.02441v1)

**Tags**: cs.CL cs.AI 



### Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and   Synthetic Data
**Authors**: Waris Gill, Justin Cechmanek, Tyler Hutcherson, Srijith Rajamohan, Jen Agarwal, Muhammad Ali Gulzar, Manvinder Singh, Benoit Dion

**Updated**: 2025-04-03T04:27:02Z

**Summary**: This report investigates enhancing semantic caching effectiveness by employing specialized, fine-tuned embedding models. Semantic caching relies on embedding similarity rather than exact key matching, presenting unique challenges in balancing precision, query latency, and computational efficiency. We propose leveraging smaller, domain-specific embedding models, fine-tuned with targeted real-world and synthetically generated datasets. Our empirical evaluations demonstrate that compact embedding models fine-tuned for just one epoch on specialized datasets significantly surpass both state-of-the-art open-source and proprietary alternatives in precision and recall. Moreover, we introduce a novel synthetic data generation pipeline for the semantic cache that mitigates the challenge of limited domain-specific annotated data, further boosting embedding performance. Our approach effectively balances computational overhead and accuracy, establishing a viable and efficient strategy for practical semantic caching implementations.

**Link**: [arxiv](http://arxiv.org/abs/2504.02268v1),  [pdf](http://arxiv.org/pdf/2504.02268v1)

**Tags**: cs.LG cs.CL 



### Comparative Analysis of Distributed Caching Algorithms: Performance   Metrics and Implementation Considerations
**Authors**: Helen Mayer, James Richards

**Updated**: 2025-04-03T02:24:21Z

**Summary**: This paper presents a comprehensive comparison of distributed caching algorithms employed in modern distributed systems. We evaluate various caching strategies including Least Recently Used (LRU), Least Frequently Used (LFU), Adaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU) against metrics such as hit ratio, latency reduction, memory overhead, and scalability. Our analysis reveals that while traditional algorithms like LRU remain prevalent, hybrid approaches incorporating machine learning techniques demonstrate superior performance in dynamic environments. Additionally, we analyze implementation patterns across different distributed architectures and provide recommendations for algorithm selection based on specific workload characteristics.

**Link**: [arxiv](http://arxiv.org/abs/2504.02220v1),  [pdf](http://arxiv.org/pdf/2504.02220v1)

**Tags**: cs.DC 



### Scaling Test-Time Inference with Policy-Optimized, Dynamic   Retrieval-Augmented Generation via KV Caching and Decoding
**Authors**: Sakhinana Sagar Srinivas, Venkataramana Runkana

**Updated**: 2025-04-03T01:23:22Z

**Summary**: We present a comprehensive framework for enhancing Retrieval-Augmented Generation (RAG) systems through dynamic retrieval strategies and reinforcement fine-tuning. This approach significantly improves large language models on knowledge-intensive tasks, including opendomain question answering and complex reasoning. Our framework integrates two complementary techniques: Policy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use of retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS), which dynamically determines retrieval timing and content based on contextual needs. Together, these techniques enhance both the utilization and relevance of retrieved content, improving factual accuracy and response quality. Designed as a lightweight solution compatible with any Transformer-based LLM without requiring additional training, our framework excels in knowledge-intensive tasks, boosting output accuracy in RAG settings. We further propose CRITIC, a novel method to selectively compress key-value caches by token importance, mitigating memory bottlenecks in long-context applications. The framework also incorporates test-time scaling techniques to dynamically balance reasoning depth and computational resources, alongside optimized decoding strategies for faster inference. Experiments on benchmark datasets show that our framework reduces hallucinations, strengthens domain-specific reasoning, and achieves significant efficiency and scalability gains over traditional RAG systems. This integrated approach advances the development of robust, efficient, and scalable RAG systems across diverse applications.

**Link**: [arxiv](http://arxiv.org/abs/2504.01281v2),  [pdf](http://arxiv.org/pdf/2504.01281v2)

**Tags**: cs.LG cs.AI cs.CL cs.IR 



### A Pilot Study on Tunable Precision Emulation via Automatic BLAS   Offloading
**Authors**: Hang Liu, Junjie Li, Yinzhi Wang

**Updated**: 2025-04-02T18:51:53Z

**Summary**: This study explores the use of automatic BLAS offloading and INT8-based emulation for accelerating traditional HPC workloads on modern GPU architectures. Through the use of low-bitwidth integer units and cache-coherent Unified Memory Architecture, we emulate double-precision matrix multiplications in the MuST application without code changes. We find that accuracy depends on both arithmetic precision and the properties of the operator, which can be dealt with through tunable precision emulation. Unlike traditional mixed-precision approaches, this method preserves original algorithms while optimizing hardware utilization. We showcases the potential of improving accuracy and performance at the same time. This work highlights the potential of AI-driven hardware to transform HPC, advocating for adaptive precision strategies in future scientific computing.

**Link**: [arxiv](http://arxiv.org/abs/2503.22875v2),  [pdf](http://arxiv.org/pdf/2503.22875v2)

**Tags**: cs.DC cs.PF 



### MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in   Embedded Processors
**Authors**: Dean You, Jieyu Jiang, Xiaoxuan Wang, Yushu Du, Zhihang Tan, Wenbo Xu, Hui Wang, Jiapeng Guan, Zhenyuan Wang, Ran Wei, Shuai Zhao, Zhe Jiang

**Updated**: 2025-04-02T10:38:25Z

**Summary**: Runahead execution is a technique to mask memory latency caused by irregular memory accesses. By pre-executing the application code during occurrences of long-latency operations and prefetching anticipated cache-missed data into the cache hierarchy, runahead effectively masks memory latency for subsequent cache misses and achieves high prefetching accuracy; however, this technique has been limited to superscalar out-of-order and superscalar in-order cores. For implementation in scalar in-order cores, the challenges of area-/energy-constraint and severe cache contention remain.   Here, we build the first full-stack system featuring runahead, MERE, from SoC and a dedicated ISA to the OS and programming model. Through this deployment, we show that enabling runahead in scalar in-order cores is possible, with minimal area and power overheads, while still achieving high performance. By re-constructing the sequential runahead employing a hardware/software co-design approach, the system can be implemented on a mature processor and SoC. Building on this, an adaptive runahead mechanism is proposed to mitigate the severe cache contention in scalar in-order cores. Combining this, we provide a comprehensive solution for embedded processors managing irregular workloads. Our evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide out-of-order core's performance while constraining area and power overheads below 5%, with the adaptive runahead mechanism delivering an additional 20.1% performance gain through mitigating the severe cache contention issues.

**Link**: [arxiv](http://arxiv.org/abs/2504.01582v1),  [pdf](http://arxiv.org/pdf/2504.01582v1)

**Tags**: cs.AR 



### Hexa-MoE: Efficient and Heterogeneous-aware Training for   Mixture-of-Experts
**Authors**: Shuqing Luo, Jie Peng, Pingzhi Li, Hanrui Wang, Tianlong Chen

**Updated**: 2025-04-02T04:57:15Z

**Summary**: Mixture-of-Experts (MoE) has emerged as a practical approach to scale up parameters for the Transformer model to achieve better generalization while maintaining a sub-linear increase in computation overhead. Current MoE models are mainly built with expert parallelism on distributed devices. However, it usually depends on homogeneous devices to deploy and suffers from heavy communication overhead and computation redundancy. In this paper, we explore developing a \texttt{H}eterogeneous-aware \texttt{EX}pert \texttt{A}llocation framework, \textbf{\texttt{HEXA-MoE}}, with significantly enhanced computing efficiency. It contains two components: ($1$) \textit{Expert-Specific Operators}. We replace the typical general matrix multiplication or grouped matrix multiplication interfaces with our operators, which allows the computing to be performed in an in-place manner with \textbf{ZERO} redundancy. ($2$) \textit{Adaptive Data- and Model-Centric Configurations} for different workload scales. Specifically, we introduce a pipeline-shared cache on each device to tackle the heavy memory consumption in the existing data-centric MoE library. Comprehensive experiments on the Swin-MoE benchmark consistently reveal the effectiveness of our \texttt{HEXA-MoE} framework, i.e., reducing $10\%\sim48\%$ memory consumption and achieving $0.5\sim4.3\times$ speed up compared to current state-of-the-art MoE libraries. Furthermore, we examine our \texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric settings. Promising results show that employing optimal parallel configuration with \texttt{HEXA-MoE} on heterogeneous devices can substantially minimize overall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.

**Link**: [arxiv](http://arxiv.org/abs/2411.01288v4),  [pdf](http://arxiv.org/pdf/2411.01288v4)

**Tags**: cs.DC 



### MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context   Generation with Speculative Decoding
**Authors**: Ranajoy Sadhukhan, Jian Chen, Zhuoming Chen, Vashisth Tiwari, Ruihang Lai, Jinyuan Shi, Ian En-Hsu Yen, Avner May, Tianqi Chen, Beidi Chen

**Updated**: 2025-04-02T01:58:38Z

**Summary**: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency losslessly, but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy SD more effectively for high throughput inference. We leverage draft model with sparse KV cache to address the KV bottleneck, which scales with both sequence length and batch size. Additionally, we propose a theoretical model to select the optimal drafting strategy for maximum speedup. Our work highlights the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B when serving batch sizes ranging from 32 to 256 on various types of hardware and tasks.

**Link**: [arxiv](http://arxiv.org/abs/2408.11049v5),  [pdf](http://arxiv.org/pdf/2408.11049v5)

**Tags**: cs.CL 



### Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN   Heterostructures
**Authors**: Seungheon Shin, Kyle Liddy, Yinxuan Zhu, Chandan Joishi, Brianna A. Klein, Andrew Armstrong, Andrew A. Allerman, Siddharth Rajan

**Updated**: 2025-04-02T01:49:58Z

**Summary**: We report on energy bands and breakdown characteristics of Al2O3 dielectrics on ultra-wide bandgap (UWBG) AlGaN heterostructures. Metal-dielectric-semiconductor structures are important to sustain high fields needed for future high-performance UWBG transistors. Using systematic experiments, we determined the fixed charge density (> 1013 cm-2), the dielectric/interface, and electric fields in the oxide of under flat-band conditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x 10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In lateral metal-semiconductor-insulator test structures, breakdown voltage exceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013 cm-2. The effective peak electric field and average breakdown field were estimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings demonstrate the potential of Al2O2 integration for enhancing the breakdown performance of UWBG AlGaN HEMTs.

**Link**: [arxiv](http://arxiv.org/abs/2504.01291v1),  [pdf](http://arxiv.org/pdf/2504.01291v1)

**Tags**: cond-mat.mtrl-sci physics.app-ph 



### Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB
**Authors**: Anas Dorbani, Sunny Yasser, Jimmy Lin, Amine Mhedhbi

**Updated**: 2025-04-01T19:48:17Z

**Summary**: Knowledge-intensive analytical applications retrieve context from both structured tabular data and unstructured, text-free documents for effective decision-making. Large language models (LLMs) have made it significantly easier to prototype such retrieval and reasoning data pipelines. However, implementing these pipelines efficiently still demands significant effort and has several challenges. This often involves orchestrating heterogeneous data systems, managing data movement, and handling low-level implementation details, e.g., LLM context management.   To address these challenges, we introduce FlockMTL: an extension for DBMSs that deeply integrates LLM capabilities and retrieval-augmented generation (RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling chained predictions through tuple-level mappings and reductions. Drawing inspiration from the relational model, FlockMTL incorporates: (i) cost-based optimizations, which seamlessly apply techniques such as batching and caching; and (ii) resource independence, enabled through novel SQL DDL abstractions: PROMPT and MODEL, introduced as first-class schema objects alongside TABLE. FlockMTL streamlines the development of knowledge-intensive analytical applications, and its optimizations ease the implementation burden.

**Link**: [arxiv](http://arxiv.org/abs/2504.01157v1),  [pdf](http://arxiv.org/pdf/2504.01157v1)

**Tags**: cs.DB cs.IR 



### Fundamentals of Caching Layered Data objects
**Authors**: Agrim Bari, Gustavo de Veciana, George Kesidis

**Updated**: 2025-04-01T18:21:43Z

**Summary**: The effective management of large amounts of data processed or required by today's cloud or edge computing systems remains a fundamental challenge. This paper focuses on cache management for applications where data objects can be stored in layered representations. In such representations, each additional data layer enhances the "quality" of the object's version but comes with an incremental cost of memory space. This layered approach proves beneficial in various scenarios, including the delivery of zoomable maps, video coding, future Virtual Reality gaming, and layered neural network models where additional data layers improve inference accuracy. In systems where users or devices demand different versions of a data object, layered representations offer flexibility for caching policies to achieve improved hit rates.   In this paper, we explore the performance of various traditionally studied caching policies, such as Belady, LRU, and LFU, both with and without layering. To this end, we develop an asymptotically accurate analytical model for Layered LRU (LLRU). We study how the performance of LLRU is impacted by factors such as the number of layers, the popularity of different objects and layers, and overheads associated with storing layered representations. For instance, we show that, for LLRU, more layers are not always beneficial and indeed performance depends in subtle ways on the popularity and size profiles of layers.

**Link**: [arxiv](http://arxiv.org/abs/2504.01104v1),  [pdf](http://arxiv.org/pdf/2504.01104v1)

**Tags**: cs.NI 



### Surfactants Screen Slide Electrification
**Authors**: Xiaomei Li, Zhongyuan Ni, Xiaoteng Zhou, Lisa S. Bauer, Diego Diaz, Gabriele Schäfer, Hans-Jürgen Butt

**Updated**: 2025-04-01T18:00:48Z

**Summary**: Water drops spontaneously accumulate charges when they move on hydrophobic dielectric surfaces by slide electrification. On the one hand, slide electrification generates electricity with possible applications on tiny devices. On the other hand, the potential of up to 1 KV generated by slide electrification alters wetting and drop motion. Therefore, it is important to know the factors that affect slide electrification. To find out how surfactants affect slide electrification, we measured drop charges of aqueous drops containing cationic CTAB, anionic SDS and neutral C8E3 sliding on different hydrophobic surfaces. The result is: addition of surfactant significantly reduces the spontaneous charging of moving water drops. Based on zeta potential measurements, confocal microscopy of deposited surface-active dyes and drop impact studies, we propose that several factors contribute to this suppression of charge separation: (1) Surfactants tend to lower the contact angles, which reduces charge separation. (2) Surfactant adsorption at the solid-liquid interface can reduce the density of primary ions, particularly for anionic surfactants. (3) Anionic and neutral surfactants are mostly transferred to the liquid-air interface at the rear of the sliding drop, retaining primary ions within the drop. (4) Deposited cationic surfactant directly reduces the charge of the drop.

**Link**: [arxiv](http://arxiv.org/abs/2504.01084v1),  [pdf](http://arxiv.org/pdf/2504.01084v1)

**Tags**: cond-mat.soft cond-mat.mtrl-sci physics.chem-ph 



### MergeVQ: A Unified Framework for Visual Generation and Representation   with Disentangled Token Merging and Quantization
**Authors**: Siyuan Li, Luyuan Zhang, Zedong Wang, Juanxi Tian, Cheng Tan, Zicheng Liu, Chang Yu, Qingsong Xie, Haonan Lu, Haoqian Wang, Zhen Lei

**Updated**: 2025-04-01T17:39:19Z

**Summary**: Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for the second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ.

**Link**: [arxiv](http://arxiv.org/abs/2504.00999v1),  [pdf](http://arxiv.org/pdf/2504.00999v1)

**Tags**: cs.CV cs.AI 



### SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV   Caching
**Authors**: Yuxuan Zhu, Ali Falahati, David H. Yang, Mohammad Mohammadi Amiri

**Updated**: 2025-04-01T17:08:57Z

**Summary**: Large language models face significant computational and memory challenges when processing long contexts. During inference, efficient management of the key-value (KV) cache, which stores intermediate activations for autoregressive generation, is critical to reducing memory overhead and improving computational efficiency. Traditional token-level efficient KV caching methods overlook semantic information, treating tokens independently without considering their semantic relationships. Meanwhile, existing semantic-preserving KV cache management approaches often suffer from substantial memory usage and high time-to-first-token. To address these limitations, we propose SentenceKV, a novel sentence-level semantic KV caching approach designed to enhance inference efficiency while preserving semantic coherence. During prefilling, SentenceKV groups tokens based on sentence-level semantic similarity, compressing sentence representations into concise semantic vectors stored directly on the GPU, while individual KV pairs are offloaded to CPU. During decoding, SentenceKV generates tokens by selectively retrieving semantically relevant sentence-level KV entries, leveraging the semantic similarity between the prefilling-stage semantic vectors and decoding-stage queries. This ensures efficient and contextually accurate predictions, minimizing the loading of redundant or irrelevant data into GPU memory and significantly reducing memory overhead while maintaining stable inference latency, even for extremely long contexts. Extensive evaluations on benchmarks including PG-19, LongBench, and Needle-In-A-Haystack demonstrate that SentenceKV significantly outperforms state-of-the-art methods in both efficiency and memory usage, without compromising model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.00970v1),  [pdf](http://arxiv.org/pdf/2504.00970v1)

**Tags**: cs.CL cs.AI cs.LG 



### Knowledge-Aware Iterative Retrieval for Multi-Agent Systems
**Authors**: Seyoung Song

**Updated**: 2025-04-01T14:21:15Z

**Summary**: We introduce a novel large language model (LLM)-driven agent framework, which iteratively refines queries and filters contextual evidence by leveraging dynamically evolving knowledge. A defining feature of the system is its decoupling of external sources from an internal knowledge cache that is progressively updated to guide both query generation and evidence selection. This design mitigates bias-reinforcement loops and enables dynamic, trackable search exploration paths, thereby optimizing the trade-off between exploring diverse information and maintaining accuracy through autonomous agent decision-making. Our approach is evaluated on a broad range of open-domain question answering benchmarks, including multi-step tasks that mirror real-world scenarios where integrating information from multiple sources is critical, especially given the vulnerabilities of LLMs that lack explicit reasoning or planning capabilities. The results show that the proposed system not only outperforms single-step baselines regardless of task difficulty but also, compared to conventional iterative retrieval methods, demonstrates pronounced advantages in complex tasks through precise evidence-based reasoning and enhanced efficiency. The proposed system supports both competitive and collaborative sharing of updated context, enabling multi-agent extension. The benefits of multi-agent configurations become especially prominent as task difficulty increases. The number of convergence steps scales with task difficulty, suggesting cost-effective scalability.

**Link**: [arxiv](http://arxiv.org/abs/2503.13275v2),  [pdf](http://arxiv.org/pdf/2503.13275v2)

**Tags**: cs.AI cs.IR I.2.0; I.2.7; I.2.11; H.3.3 



### EMO: Edge Model Overlays to Scale Model Size in Federated Learning
**Authors**: Di Wu, Weibo He, Wanglei Feng, Zhenyu Wen, Bin Qian, Blesson Varghese

**Updated**: 2025-04-01T12:34:58Z

**Summary**: Federated Learning (FL) trains machine learning models on edge devices with distributed data. However, the computational and memory limitations of these devices restrict the training of large models using FL. Split Federated Learning (SFL) addresses this challenge by distributing the model across the device and server, but it introduces a tightly coupled data flow, leading to computational bottlenecks and high communication costs. We propose EMO as a solution to enable the training of large models in FL while mitigating the challenges of SFL. EMO introduces Edge Model Overlay(s) between the device and server, enabling the creation of a larger ensemble model without modifying the FL workflow. The key innovation in EMO is Augmented Federated Learning (AFL), which builds an ensemble model by connecting the original (smaller) FL model with model(s) trained in the overlay(s) to facilitate horizontal or vertical scaling. This is accomplished through three key modules: a hierarchical activation replay cache to decouple AFL from FL, a convergence-aware communication controller to optimize communication overhead, and an ensemble inference module. Evaluations on a real-world prototype show that EMO improves accuracy by up to 17.77% compared to FL, and reduces communication costs by up to 7.17x and decreases training time by up to 6.9x compared to SFL.

**Link**: [arxiv](http://arxiv.org/abs/2504.00726v1),  [pdf](http://arxiv.org/pdf/2504.00726v1)

**Tags**: cs.LG cs.DC 



### Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features
**Authors**: Jewon Lee, Ki-Ung Song, Seungmin Yang, Donguk Lim, Jaeyeon Kim, Wooksu Shin, Bo-Kyeong Kim, Yong Jae Lee, Tae-Ho Kim

**Updated**: 2025-04-01T09:10:32Z

**Summary**: Visual token reduction lowers inference costs caused by extensive image features in large vision-language models (LVLMs). Unlike relevant studies that prune tokens in self-attention-only LVLMs, our work uniquely addresses cross-attention-based models, which achieve superior performance. We identify that the key-value (KV) cache size for image tokens in cross-attention layers significantly exceeds that of text tokens in self-attention layers, posing a major compute bottleneck. To mitigate this issue, we exploit the sparse nature in cross-attention maps to selectively prune redundant visual features. Our Trimmed Llama effectively reduces KV cache demands without requiring additional training. By benefiting from 50%-reduced visual features, our model can reduce inference latency and memory usage while achieving benchmark parity.

**Link**: [arxiv](http://arxiv.org/abs/2504.00557v1),  [pdf](http://arxiv.org/pdf/2504.00557v1)

**Tags**: cs.CV cs.LG 



### High specific impulse electrospray propulsion with small capillary   emitters
**Authors**: Manel Caballero-Pérez, Marc Galobardes-Esteban, Manuel Gamero-Castaño

**Updated**: 2025-04-01T07:04:30Z

**Summary**: This study demonstrates the feasibility of using smaller capillary emitters to achieve higher specific impulse ($I_\text{sp}$) in electrospray propulsion. Four ionic liquids were characterized using capillary emitters with tip diameters from 15 to 50 $\mu$m. Smaller diameter capillaries produced smaller and more stable Taylor cones. This stabilization enabled steady cone-jet operation at significantly lower flow rates compared to larger emitters. This was unexpected because when the jet diameter is much smaller than far-field geometric features, the minimum flow rate is thought to be solely determined by the physical properties of the propellant. Using the smaller emitters and acceleration voltages of 10 kV, specific impulses up to 3000 s could be achieved with efficiencies above 50%, approximately doubling the $I_\text{sp}$ observed with larger emitters. For one of the liquids and the smallest emitters, the beam consisted solely of ions at the lowest flow rates, similarly to studies using externally wetted and porous emitters. Another important finding was that at sufficiently low flow rates, a significant fraction of the propellant fed to the emitter is not accelerated by the electrostatic field. These propellant losses make the time-of-flight technique unreliable for determining the $I_\text{sp}$.

**Link**: [arxiv](http://arxiv.org/abs/2504.00474v1),  [pdf](http://arxiv.org/pdf/2504.00474v1)

**Tags**: physics.flu-dyn 



### SQuat: Subspace-orthogonal KV Cache Quantization
**Authors**: Hao Wang, Ligong Han, Kai Xu, Akash Srivastava

**Updated**: 2025-03-31T17:37:32Z

**Summary**: The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from previously generated tokens. It reduces redundant computation at the cost of increased memory usage. To mitigate this overhead, existing approaches compress KV tensors into lower-bit representations; however, quantization errors can accumulate as more tokens are generated, potentially resulting in undesired outputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache quantization). It first constructs a subspace spanned by query tensors to capture the most critical task-related information. During key tensor quantization, it enforces that the difference between the (de)quantized and original keys remains orthogonal to this subspace, minimizing the impact of quantization errors on the attention mechanism's outputs. SQuat requires no model fine-tuning, no additional calibration dataset for offline learning, and is grounded in a theoretical framework we develop. Through numerical experiments, we show that our method reduces peak memory by 2.17 to 2.82, improves throughput by 2.45 to 3.60, and achieves more favorable benchmark scores than existing KV cache quantization algorithms.

**Link**: [arxiv](http://arxiv.org/abs/2503.24358v1),  [pdf](http://arxiv.org/pdf/2503.24358v1)

**Tags**: cs.LG cs.AI cs.CL cs.IT math.IT 



### CITRAS: Covariate-Informed Transformer for Time Series Forecasting
**Authors**: Yosuke Yamaguchi, Issei Suemitsu, Wenpeng Wei

**Updated**: 2025-03-31T12:32:23Z

**Summary**: Covariates play an indispensable role in practical time series forecasting, offering rich context from the past and sometimes extending into the future. However, their availability varies depending on the scenario, and situations often involve multiple target variables simultaneously. Moreover, the cross-variate dependencies between them are multi-granular, with some covariates having a short-term impact on target variables and others showing long-term correlations. This heterogeneity and the intricate dependencies arising in covariate-informed forecasting present significant challenges to existing deep models. To address these issues, we propose CITRAS, a patch-based Transformer that flexibly leverages multiple targets and covariates covering both the past and the future forecasting horizon. While preserving the strong autoregressive capabilities of the canonical Transformer, CITRAS introduces two novel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift and Attention Score Smoothing. KV Shift seamlessly incorporates future known covariates into the forecasting of target variables based on their concurrent dependencies. Additionally, Attention Score Smoothing transforms locally accurate patch-wise cross-variate dependencies into global variate-level dependencies by smoothing the past series of attention scores. Experimentally, CITRAS achieves state-of-the-art performance in both covariate-informed and multivariate forecasting, demonstrating its versatile ability to leverage cross-variate dependency for improved forecasting accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.24007v1),  [pdf](http://arxiv.org/pdf/2503.24007v1)

**Tags**: cs.LG cs.AI 



### Rethinking Key-Value Cache Compression Techniques for Large Language   Model Serving
**Authors**: Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen

**Updated**: 2025-03-31T12:23:31Z

**Summary**: Key-Value cache (\texttt{KV} \texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \texttt{KV} \texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \texttt{KV} \texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \texttt{KV} \texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \texttt{KV} \texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \texttt{KV} \texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \texttt{KV} \texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \texttt{KV} \texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \texttt{KV} \texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.

**Link**: [arxiv](http://arxiv.org/abs/2503.24000v1),  [pdf](http://arxiv.org/pdf/2503.24000v1)

**Tags**: cs.LG cs.AI 



### Deep Learning Model Deployment in Multiple Cloud Providers: an   Exploratory Study Using Low Computing Power Environments
**Authors**: Elayne Lemos, Rodrigo Oliveira, Jairson Rodrigues, Rosalvo F. Oliveira Neto

**Updated**: 2025-03-31T11:58:37Z

**Summary**: The deployment of Machine Learning models at cloud have grown by tech companies. Hardware requirements are higher when these models involve Deep Learning (DL) techniques and the cloud providers' costs may be a barrier. We explore deploying DL models using for experiments the GECToR model, a DL solution for Grammatical Error Correction, across three of the major cloud platforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware usage and cost at each cloud provider by 7 execution environments with 10 experiments reproduced. We found that while GPUs excel in performance, they had an average cost 300% higher than solutions without GPU. Our analysis also identifies that processor cache size is crucial for cost-effective CPU deployments, enabling over 50% of cost reduction compared to GPUs. This study demonstrates the feasibility and affordability of cloud-based DL inference solutions without GPUs, benefiting resource-constrained users like startups.

**Link**: [arxiv](http://arxiv.org/abs/2503.23988v1),  [pdf](http://arxiv.org/pdf/2503.23988v1)

**Tags**: cs.DC cs.AI cs.PF 68T07, 68U01 C.4; I.2.0; B.8.2 



### AirCache: Activating Inter-modal Relevancy KV Cache Compression for   Efficient Large Vision-Language Model Inference
**Authors**: Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang

**Updated**: 2025-03-31T11:13:18Z

**Summary**: Recent advancements in Large Visual Language Models (LVLMs) have gained significant attention due to their remarkable reasoning capabilities and proficiency in generalization. However, processing a large number of visual tokens and generating long-context outputs impose substantial computational overhead, leading to excessive demands for key-value (KV) cache. To address this critical bottleneck, we propose AirCache, a novel KV cache compression method aimed at accelerating LVLMs inference. This work systematically investigates the correlations between visual and textual tokens within the attention mechanisms of LVLMs. Our empirical analysis reveals considerable redundancy in cached visual tokens, wherein strategically eliminating these tokens preserves model performance while significantly accelerating context generation. Inspired by these findings, we introduce an elite observation window for assessing the importance of visual components in the KV cache, focusing on stable inter-modal relevancy modeling with enhanced multi-perspective consistency. Additionally, we develop an adaptive layer-wise budget allocation strategy that capitalizes on the strength and skewness of token importance distribution, showcasing superior efficiency compared to uniform allocation. Comprehensive evaluations across multiple LVLMs and benchmarks demonstrate that our method achieves comparable performance to the full cache while retaining only 10% of visual KV cache, thereby reducing decoding latency by 29% to 66% across various batch size and prompt length of inputs. Notably, as cache retention rates decrease, our method exhibits increasing performance advantages over existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.23956v1),  [pdf](http://arxiv.org/pdf/2503.23956v1)

**Tags**: cs.CV cs.AI 



### Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language   Models
**Authors**: Haotian Zhai, Xinyu Chen, Can Zhang, Tianming Sha, Ruirui Li

**Updated**: 2025-03-31T10:28:04Z

**Summary**: Test-time adaptation (TTA) of visual language models has recently attracted significant attention as a solution to the performance degradation caused by distribution shifts in downstream tasks. However, existing cache-based TTA methods have certain limitations. They mainly rely on the accuracy of cached feature labels, and the presence of noisy pseudo-labels can cause these features to deviate from their true distribution. This makes cache retrieval methods based on similarity matching highly sensitive to outliers or extreme samples. Moreover, current methods lack effective mechanisms to model class distributions, which limits their ability to fully exploit the potential of cached information. To address these challenges, we introduce a comprehensive and reliable caching mechanism and propose a novel zero-shot TTA method called "Cache, Residual, Gaussian" (CRG). This method not only employs learnable residual parameters to better align positive and negative visual prototypes with text prototypes, thereby optimizing the quality of cached features, but also incorporates Gaussian Discriminant Analysis (GDA) to dynamically model intra-class feature distributions, further mitigating the impact of noisy features. Experimental results on 13 benchmarks demonstrate that CRG outperforms state-of-the-art TTA methods, showcasing exceptional robustness and adaptability.

**Link**: [arxiv](http://arxiv.org/abs/2503.18334v2),  [pdf](http://arxiv.org/pdf/2503.18334v2)

**Tags**: cs.CV 



### Training-Free Text-Guided Image Editing with Visual Autoregressive Model
**Authors**: Yufei Wang, Lanqing Guo, Zhihao Li, Jiaxing Huang, Pichao Wang, Bihan Wen, Jian Wang

**Updated**: 2025-03-31T09:46:56Z

**Summary**: Text-guided image editing is an essential task that enables users to modify images through natural language descriptions. Recent advances in diffusion models and rectified flows have significantly improved editing quality, primarily relying on inversion techniques to extract structured noise from input images. However, inaccuracies in inversion can propagate errors, leading to unintended modifications and compromising fidelity. Moreover, even with perfect inversion, the entanglement between textual prompts and image features often results in global changes when only local edits are intended. To address these challenges, we propose a novel text-guided image editing framework based on VAR (Visual AutoRegressive modeling), which eliminates the need for explicit inversion while ensuring precise and controlled modifications. Our method introduces a caching mechanism that stores token indices and probability distributions from the original image, capturing the relationship between the source prompt and the image. Using this cache, we design an adaptive fine-grained masking strategy that dynamically identifies and constrains modifications to relevant regions, preventing unintended changes. A token reassembling approach further refines the editing process, enhancing diversity, fidelity, and control. Our framework operates in a training-free manner and achieves high-fidelity editing with faster inference speeds, processing a 1K resolution image in as fast as 1.2 seconds. Extensive experiments demonstrate that our method achieves performance comparable to, or even surpassing, existing diffusion- and rectified flow-based approaches in both quantitative metrics and visual quality. The code will be released.

**Link**: [arxiv](http://arxiv.org/abs/2503.23897v1),  [pdf](http://arxiv.org/pdf/2503.23897v1)

**Tags**: cs.CV cs.AI 



### Training-Free Exponential Context Extension via Cascading KV Cache
**Authors**: Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang

**Updated**: 2025-03-31T03:28:44Z

**Summary**: The transformer's context window is vital for tasks such as few-shot learning and conditional generation as it preserves previous tokens for active memory. However, as the context lengths increase, the computational costs grow quadratically, hindering the deployment of large language models (LLMs) in real-world, long sequence scenarios. Although some recent key-value caching (KV Cache) methods offer linear inference complexity, they naively manage the stored context, prematurely evicting tokens and losing valuable information. Moreover, they lack an optimized prefill/prompt stage strategy, resulting in higher latency than even quadratic attention for realistic context sizes. In response, we introduce a novel mechanism that leverages cascading sub-cache buffers to selectively retain the most relevant tokens, enabling the model to maintain longer context histories without increasing the cache size. Our approach outperforms linear caching baselines across key benchmarks, including streaming perplexity, question answering, book summarization, and passkey retrieval, where it retains better retrieval accuracy at 1M tokens after four doublings of the cache size of 65K. Additionally, our method reduces prefill stage latency by a factor of 6.8 when compared to flash attention on 1M tokens. These innovations not only enhance the computational efficiency of LLMs but also pave the way for their effective deployment in resource-constrained environments, enabling large-scale, real-time applications with significantly reduced latency.

**Link**: [arxiv](http://arxiv.org/abs/2406.17808v4),  [pdf](http://arxiv.org/pdf/2406.17808v4)

**Tags**: cs.CL cs.AI cs.LG 



### Skip-Vision: Efficient and Scalable Acceleration of Vision-Language   Models via Adaptive Token Skipping
**Authors**: Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan

**Updated**: 2025-03-31T02:19:29Z

**Summary**: Transformer-based models have driven significant advancements in Multimodal Large Language Models (MLLMs), yet their computational costs surge drastically when scaling resolution, training data, and model parameters. A key bottleneck stems from the proliferation of visual tokens required for fine-grained image understanding. We propose Skip-Vision, a unified framework addressing both training and inference inefficiencies in vision-language models. On top of conventional token compression approaches, our method introduces two complementary acceleration strategies. For training acceleration, we observe that Feed-Forward Network (FFN) computations on visual tokens induce marginal feature updates. This motivates our Skip-FFN strategy, which bypasses FFN layers for redundant visual tokens. For inference acceleration, we design a selective KV-cache removal mechanism that prunes the skipped key-value pairs during decoding while preserving model performance. Experimental results demonstrate that Skip-Vision reduces training time by up to 35\%, inference FLOPs by 75\%, and latency by 45\%, while achieving comparable or superior performance to existing methods. Our work provides a practical solution for scaling high-performance MLLMs with enhanced efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2503.21817v2),  [pdf](http://arxiv.org/pdf/2503.21817v2)

**Tags**: cs.CV 



### EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient   Image Editing
**Authors**: Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, Linfeng Zhang

**Updated**: 2025-03-30T11:14:17Z

**Summary**: Inversion-based image editing is rapidly gaining momentum while suffering from significant computation overhead, hindering its application in real-time interactive scenarios. In this paper, we rethink that the redundancy in inversion-based image editing exists in both the spatial and temporal dimensions, such as the unnecessary computation in unedited regions and the redundancy in the inversion progress. To tackle these challenges, we propose a practical framework, named EEdit, to achieve efficient image editing. Specifically, we introduce three techniques to solve them one by one. For spatial redundancy, spatial locality caching is introduced to compute the edited region and its neighboring regions while skipping the unedited regions, and token indexing preprocessing is designed to further accelerate the caching. For temporal redundancy, inversion step skipping is proposed to reuse the latent for efficient editing. Our experiments demonstrate an average of 2.46 $\times$ acceleration without performance drop in a wide range of editing tasks including prompt-guided image editing, dragging and image composition. Our codes are available at https://github.com/yuriYanZeXuan/EEdit

**Link**: [arxiv](http://arxiv.org/abs/2503.10270v2),  [pdf](http://arxiv.org/pdf/2503.10270v2)

**Tags**: cs.CV 



### FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update
**Authors**: Yuan Chen, Ao Li, Wenhai Li, Lingfeng Deng

**Updated**: 2025-03-30T11:09:06Z

**Summary**: B$^+$-trees are prevalent in traditional database systems due to their versatility and balanced structure. While binary search is typically utilized for branch operations, it may lead to inefficient cache utilization in main-memory scenarios. In contrast, trie-based index structures drive branch operations through prefix matching. While these structures generally produce fewer cache misses and are thus increasingly popular, they may underperform in range scans because of frequent pointer chasing. This paper proposes a new high-performance B$^+$-tree variant called \textbf{Feature B$^+$-tree (FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries, FB$^+$-tree progressively considers several bytes following the common prefix on each level of its inner nodes\textemdash referred to as features, which allows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines between B$^+$-trees and tries, while still retaining balance. In the best case, FB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to function as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that combines the link technique and optimistic lock is designed to support efficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle atomic operations seamlessly coordinated with optimistic lock to facilitate latch-free updates, which can be easily extended to other structures. Intensive experiments on multiple workload-dataset combinations demonstrate that FB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based indexes and outperforms popular B$^+$-trees by 2.3x$\ \sim\ $3.7x under 96 threads. FB$^+$-tree also exhibits significant potential on other workloads, especially update workloads under contention and scan workloads.

**Link**: [arxiv](http://arxiv.org/abs/2503.23397v1),  [pdf](http://arxiv.org/pdf/2503.23397v1)

**Tags**: cs.DB cs.DS cs.PF 



### COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP   Test-Time Adaptation
**Authors**: Fanding Huang, Jingyan Jiang, Qinting Jiang, Hebei Li, Faisal Nadeem Khan, Zhi Wang

**Updated**: 2025-03-30T10:34:45Z

**Summary**: Recent vision-language models (VLMs) face significant challenges in test-time adaptation to novel domains. While cache-based methods show promise by leveraging historical information, they struggle with both caching unreliable feature-label pairs and indiscriminately using single-class information during querying, significantly compromising adaptation accuracy. To address these limitations, we propose COSMIC (Clique-Oriented Semantic Multi-space Integration for CLIP), a robust test-time adaptation framework that enhances adaptability through multi-granular, cross-modal semantic caching and graph-based querying mechanisms. Our framework introduces two key innovations: Dual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual Semantics Graph constructs complementary semantic spaces by incorporating textual features, coarse-grained CLIP features, and fine-grained DINOv2 features to capture rich semantic relationships. Building upon these dual graphs, the Clique Guided Hyper-class component leverages structured class relationships to enhance prediction robustness through correlated class selection. Extensive experiments demonstrate COSMIC's superior performance across multiple benchmarks, achieving significant improvements over state-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on cross-domain generation with CLIP RN-50. Code is available at github.com/hf618/COSMIC.

**Link**: [arxiv](http://arxiv.org/abs/2503.23388v1),  [pdf](http://arxiv.org/pdf/2503.23388v1)

**Tags**: cs.CV cs.AI cs.LG cs.MM 



### A Unified Framework for Quantitative Cache Analysis
**Authors**: Sophie Kahlen, Jan Reineke

**Updated**: 2025-03-30T09:46:34Z

**Summary**: In this work we unify two existing lines of work towards cache analysis for non-LRU policies. To this end, we extend the notion of competitiveness to block competitiveness and systematically analyze the competitiveness and block competitiveness of FIFO and MRU relative to LRU for arbitrary associativities. We show how competitiveness and block competitiveness can be exploited in state-of-the-art WCET analysis based on the results of existing persistence analyses for LRU. Unlike prior work, our approach is applicable to microarchitectures that exhibit timing anomalies. We experimentally evaluate the precision and cost of our approach on benchmarks from TACLeBench. The experiments demonstrate that quantitative cache analysis for FIFO and MRU comes close to the precision of LRU.

**Link**: [arxiv](http://arxiv.org/abs/2503.16588v2),  [pdf](http://arxiv.org/pdf/2503.16588v2)

**Tags**: cs.PL 68 D.3.4 



### MVREC: A General Few-shot Defect Classification Model Using Multi-View   Region-Context
**Authors**: Shuai Lyu, Rongchen Zhang, Zeqi Ma, Fangjian Liao, Dongmei Mo, Waikeung Wong

**Updated**: 2025-03-30T09:19:53Z

**Summary**: Few-shot defect multi-classification (FSDMC) is an emerging trend in quality control within industrial manufacturing. However, current FSDMC research often lacks generalizability due to its focus on specific datasets. Additionally, defect classification heavily relies on contextual information within images, and existing methods fall short of effectively extracting this information. To address these challenges, we propose a general FSDMC framework called MVREC, which offers two primary advantages: (1) MVREC extracts general features for defect instances by incorporating the pre-trained AlphaCLIP model. (2) It utilizes a region-context framework to enhance defect features by leveraging mask region input and multi-view context augmentation. Furthermore, Few-shot Zip-Adapter(-F) classifiers within the model are introduced to cache the visual features of the support set and perform few-shot classification. We also introduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes 1228 defect images with instance-level mask annotations and 46 defect types. Extensive experiments conducted on MVTec-FS and four additional datasets demonstrate its effectiveness in general defect classification and its ability to incorporate contextual information to improve classification performance. Code: https://github.com/ShuaiLYU/MVREC

**Link**: [arxiv](http://arxiv.org/abs/2412.16897v2),  [pdf](http://arxiv.org/pdf/2412.16897v2)

**Tags**: cs.CV cs.AI 



### FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning
**Authors**: Hang Guo, Yawei Li, Taolin Zhang, Jiangshan Wang, Tao Dai, Shu-Tao Xia, Luca Benini

**Updated**: 2025-03-30T08:51:19Z

**Summary**: Visual Autoregressive (VAR) modeling has gained popularity for its shift towards next-scale prediction. However, existing VAR paradigms process the entire token map at each scale step, leading to the complexity and runtime scaling dramatically with image resolution. To address this challenge, we propose FastVAR, a post-training acceleration method for efficient resolution scaling with VARs. Our key finding is that the majority of latency arises from the large-scale step where most tokens have already converged. Leveraging this observation, we develop the cached token pruning strategy that only forwards pivotal tokens for scale-specific modeling while using cached tokens from previous scale steps to restore the pruned slots. This significantly reduces the number of forwarded tokens and improves the efficiency at larger resolutions. Experiments show the proposed FastVAR can further speedup FlashAttention-accelerated VAR by 2.7$\times$ with negligible performance drop of <1%. We further extend FastVAR to zero-shot generation of higher resolution images. In particular, FastVAR can generate one 2K image with 15GB memory footprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at https://github.com/csguoh/FastVAR.

**Link**: [arxiv](http://arxiv.org/abs/2503.23367v1),  [pdf](http://arxiv.org/pdf/2503.23367v1)

**Tags**: cs.CV 



### PQCache: Product Quantization-based KVCache for Long Context LLM   Inference
**Authors**: Hailin Zhang, Xiaodong Ji, Yilin Chen, Fangcheng Fu, Xupeng Miao, Xiaonan Nie, Weipeng Chen, Bin Cui

**Updated**: 2025-03-30T08:13:50Z

**Summary**: As the field of Large Language Models (LLMs) continues to evolve, the context length in inference is steadily growing. Key-Value Cache (KVCache), the intermediate representations of tokens within LLM inference, has now become the primary memory bottleneck due to limited GPU memory. Current methods selectively determine suitable keys and values for self-attention computation in LLMs to address the issue. However, they either fall short in maintaining model quality or result in high serving latency. Drawing inspiration from advanced embedding retrieval techniques prevalent in the data management community, we consider the storage and retrieval of KVCache as a typical embedding retrieval problem. We propose PQCache, which employs Product Quantization (PQ) to manage KVCache, maintaining model quality while ensuring low serving latency. During the prefilling phase, we apply PQ to tokens' keys for each LLM layer and head. During the autoregressive decoding phase, we use PQ codes and centroids to approximately identify important preceding tokens, then fetch the corresponding key-value pairs for self-attention computation. Through meticulous design of overlapping and caching, we minimize any additional computation and communication overhead during both phases. Extensive experiments demonstrate that PQCache achieves both effectiveness and efficiency, with 4.60% score improvement over existing methods on InfiniteBench and low system latency in both prefilling and decoding.

**Link**: [arxiv](http://arxiv.org/abs/2407.12820v2),  [pdf](http://arxiv.org/pdf/2407.12820v2)

**Tags**: cs.CL cs.AI cs.LG 



### Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context   LLM Inference
**Authors**: Wei Tao, Bin Zhang, Xiaoyang Qu, Jiguang Wan, Jianzong Wang

**Updated**: 2025-03-30T03:20:34Z

**Summary**: Recently, large language models (LLMs) have been able to handle longer and longer contexts. However, a context that is too long may cause intolerant inference latency and GPU memory usage. Existing methods propose mixed-precision quantization to the key-value (KV) cache in LLMs based on token granularity, which is time-consuming in the search process and hardware inefficient during computation. This paper introduces a novel approach called Cocktail, which employs chunk-adaptive mixed-precision quantization to optimize the KV cache. Cocktail consists of two modules: chunk-level quantization search and chunk-level KV cache computation. Chunk-level quantization search determines the optimal bitwidth configuration of the KV cache chunks quickly based on the similarity scores between the corresponding context chunks and the query, maintaining the model accuracy. Furthermore, chunk-level KV cache computation reorders the KV cache chunks before quantization, avoiding the hardware inefficiency caused by mixed-precision quantization in inference computation. Extensive experiments demonstrate that Cocktail outperforms state-of-the-art KV cache quantization methods on various models and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2503.23294v1),  [pdf](http://arxiv.org/pdf/2503.23294v1)

**Tags**: cs.CL 



### Key, Value, Compress: A Systematic Exploration of KV Cache Compression   Techniques
**Authors**: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

**Updated**: 2025-03-30T02:45:00Z

**Summary**: Large language models (LLMs) have demonstrated exceptional capabilities in generating text, images, and video content. However, as context length grows, the computational cost of attention increases quadratically with the number of tokens, presenting significant efficiency challenges. This paper presents an analysis of various Key-Value (KV) cache compression strategies, offering a comprehensive taxonomy that categorizes these methods by their underlying principles and implementation techniques. Furthermore, we evaluate their impact on performance and inference latency, providing critical insights into their effectiveness. Our findings highlight the trade-offs involved in KV cache compression and its influence on handling long-context scenarios, paving the way for more efficient LLM implementations.

**Link**: [arxiv](http://arxiv.org/abs/2503.11816v2),  [pdf](http://arxiv.org/pdf/2503.11816v2)

**Tags**: cs.CL 



### TopV: Compatible Token Pruning with Inference Time Optimization for Fast   and Low-Memory Multimodal Vision Language Model
**Authors**: Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Chendi Li, Jinghua Yan, Yu Bai, Ponnuswamy Sadayappan, Xia Hu, Bo Yuan

**Updated**: 2025-03-29T23:00:27Z

**Summary**: Vision-Language Models (VLMs) demand substantial computational resources during inference, largely due to the extensive visual input tokens for representing visual information. Previous studies have noted that visual tokens tend to receive less attention than text tokens, suggesting their lower importance during inference and potential for pruning. However, their methods encounter several challenges: reliance on greedy heuristic criteria for token importance and incompatibility with FlashAttention and KV cache. To address these issues, we introduce \textbf{TopV}, a compatible \textbf{TO}ken \textbf{P}runing with inference Time Optimization for fast and low-memory \textbf{V}LM, achieving efficient pruning without additional training or fine-tuning. Instead of relying on attention scores, we formulate token pruning as an optimization problem, accurately identifying important visual tokens while remaining compatible with FlashAttention. Additionally, since we only perform this pruning once during the prefilling stage, it effectively reduces KV cache size. Our optimization framework incorporates a visual-aware cost function considering factors such as Feature Similarity, Relative Spatial Distance, and Absolute Central Distance, to measure the importance of each source visual token, enabling effective pruning of low-importance tokens. Extensive experiments demonstrate that our method outperforms previous token pruning methods, validating the effectiveness and efficiency of our approach.

**Link**: [arxiv](http://arxiv.org/abs/2503.18278v2),  [pdf](http://arxiv.org/pdf/2503.18278v2)

**Tags**: cs.CV cs.AI 



### X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and   Extreme KV Compression
**Authors**: Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum

**Updated**: 2025-03-29T04:43:11Z

**Summary**: Multi-head latent attention (MLA) is designed to optimize KV cache memory through low-rank key-value joint compression. Rather than caching keys and values separately, MLA stores their compressed latent representations, reducing memory overhead while maintaining the performance. While MLA improves memory efficiency without compromising language model accuracy, its major limitation lies in its integration during the pre-training phase, requiring models to be trained from scratch. This raises a key question: can we use MLA's benefits fully or partially in models that have already been pre-trained with different attention mechanisms? In this paper, we propose X-EcoMLA to deploy post training distillation to enable the upcycling of Transformer-based attention into an efficient hybrid MLA variant through lightweight post-training adaptation, bypassing the need for extensive pre-training. We demonstrate that leveraging the dark knowledge of a well-trained model can enhance training accuracy and enable extreme KV cache compression in MLA without compromising model performance. The experimental results show that our proposed method can effectively compress the KV cache while preserving the performance on the benchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression achieves the same average score by using only 3.6B training tokens and 70 GPU hours on AMD MI300, whereas a 10.6x compression have less than 0.1\% average score drop with 7B training tokens and 140 GPU hours.

**Link**: [arxiv](http://arxiv.org/abs/2503.11132v2),  [pdf](http://arxiv.org/pdf/2503.11132v2)

**Tags**: cs.CL 



### SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with   Sweep Reconstruction
**Authors**: Zikang Yuan, Ruiye Ming, Chengwei Zhao, Yonghao Tan, Pingcheng Dong, Hongcheng Luo, Yuzhong Jiao, Xin Yang, Kwang-Ting Cheng

**Updated**: 2025-03-29T01:06:54Z

**Summary**: Addressing the inherent low acquisition frequency limitation of 3D LiDAR to achieve high-frequency output has become a critical research focus in the LiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance, frequency-enhanced LIO systems must process each sweep within significantly reduced timeframe, which presents substantial challenges for deployment on low-computational-power platforms. To address these limitations, we introduce SR-LIO++, an innovative LIO system capable of achieving doubled output frequency relative to input frequency on resource-constrained hardware platforms, including the Raspberry Pi 4B. Our system employs a sweep reconstruction methodology to enhance LiDAR sweep frequency, generating high-frequency reconstructed sweeps. Building upon this foundation, we propose a caching mechanism for intermediate results (i.e., surface parameters) of the most recent segments, effectively minimizing redundant processing of common segments in adjacent reconstructed sweeps. This method decouples processing time from the traditionally linear dependence on reconstructed sweep frequency. Furthermore, we present a quantized map point management based on index table mapping, significantly reducing memory usage by converting global 3D point storage from 64-bit double precision to 8-bit char representation. This method also converts the computationally intensive Euclidean distance calculations in nearest neighbor searches from 64-bit double precision to 16-bit short and 32-bit integer formats, significantly reducing both memory and computational cost. Extensive experimental evaluations across three distinct computing platforms and four public datasets demonstrate that SR-LIO++ maintains state-of-the-art accuracy while substantially enhancing efficiency. Notably, our system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.

**Link**: [arxiv](http://arxiv.org/abs/2503.22926v1),  [pdf](http://arxiv.org/pdf/2503.22926v1)

**Tags**: cs.RO 



### DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality   Diffusion Transformers
**Authors**: Hanling Zhang, Rundong Su, Zhihang Yuan, Pengtao Chen, Mingzhu Shen Yibo Fan, Shengen Yan, Guohao Dai, Yu Wang

**Updated**: 2025-03-28T18:00:12Z

**Summary**: Text-to-image generation models, especially Multimodal Diffusion Transformers (MMDiT), have shown remarkable progress in generating high-quality images. However, these models often face significant computational bottlenecks, particularly in attention mechanisms, which hinder their scalability and efficiency. In this paper, we introduce DiTFastAttnV2, a post-training compression method designed to accelerate attention in MMDiT. Through an in-depth analysis of MMDiT's attention patterns, we identify key differences from prior DiT-based methods and propose head-wise arrow attention and caching mechanisms to dynamically adjust attention heads, effectively bridging this gap. We also design an Efficient Fused Kernel for further acceleration. By leveraging local metric methods and optimization techniques, our approach significantly reduces the search time for optimal compression schemes to just minutes while maintaining generation quality. Furthermore, with the customized kernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x end-to-end speedup on 2K image generation without compromising visual fidelity.

**Link**: [arxiv](http://arxiv.org/abs/2503.22796v1),  [pdf](http://arxiv.org/pdf/2503.22796v1)

**Tags**: cs.CV cs.AI 



### Towards Stabilized and Efficient Diffusion Transformers through   Long-Skip-Connections with Spectral Constraints
**Authors**: Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng

**Updated**: 2025-03-28T16:15:19Z

**Summary**: Diffusion Transformers (DiT) have emerged as a powerful architecture for image and video generation, offering superior quality and scalability. However, their practical application suffers from inherent dynamic feature instability, leading to error amplification during cached inference. Through systematic analysis, we identify the absence of long-range feature preservation mechanisms as the root cause of unstable feature propagation and perturbation sensitivity. To this end, we propose Skip-DiT, a novel DiT variant enhanced with Long-Skip-Connections (LSCs) - the key efficiency component in U-Nets. Theoretical spectral norm and visualization analysis demonstrate how LSCs stabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic feature enable an efficient statical caching mechanism that reuses deep features across timesteps while updating shallow components. Extensive experiments across image and video generation tasks demonstrate that Skip-DiT achieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2 times inference acceleration without quality loss and high fidelity to original output, outperforming existing DiT caching methods across various quantitative metrics. Our findings establish long-skip connections as critical architectural components for training stable and efficient diffusion transformers.

**Link**: [arxiv](http://arxiv.org/abs/2411.17616v3),  [pdf](http://arxiv.org/pdf/2411.17616v3)

**Tags**: cs.CV 



### DyCoke: Dynamic Compression of Tokens for Fast Video Large Language   Models
**Authors**: Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang

**Updated**: 2025-03-28T14:11:37Z

**Summary**: Video large language models (VLLMs) have significantly advanced recently in processing complex video content, yet their inference efficiency remains constrained because of the high computational cost stemming from the thousands of visual tokens generated from the video inputs. We empirically observe that, unlike single image inputs, VLLMs typically attend visual tokens from different frames at different decoding iterations, making a one-shot pruning strategy prone to removing important tokens by mistake. Motivated by this, we present DyCoke, a training-free token compression method to optimize token representation and accelerate VLLMs. DyCoke incorporates a plug-and-play temporal compression module to minimize temporal redundancy by merging redundant tokens across frames, and applies dynamic KV cache reduction to prune spatially redundant tokens selectively. It ensures high-quality inference by dynamically retaining the critical tokens at each decoding step. Extensive experimental results demonstrate that DyCoke can outperform the prior SoTA counterparts, achieving 1.5X inference speedup, 1.4X memory reduction against the baseline VLLM, while still improving the performance, with no training.

**Link**: [arxiv](http://arxiv.org/abs/2411.15024v3),  [pdf](http://arxiv.org/pdf/2411.15024v3)

**Tags**: cs.CV cs.LG 



### A Refined Analysis of Massive Activations in LLMs
**Authors**: Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian Güra

**Updated**: 2025-03-28T11:08:34Z

**Summary**: Motivated in part by their relevance for low-precision training and quantization, massive activations in large language models (LLMs) have recently emerged as a topic of interest. However, existing analyses are limited in scope, and generalizability across architectures is unclear. This paper helps address some of these gaps by conducting an analysis of massive activations across a broad range of LLMs, including both GLU-based and non-GLU-based architectures. Our findings challenge several prior assumptions, most importantly: (1) not all massive activations are detrimental, i.e. suppressing them does not lead to an explosion of perplexity or a collapse in downstream task performance; (2) proposed mitigation strategies such as Attention KV bias are model-specific and ineffective in certain cases. We consequently investigate novel hybrid mitigation strategies; in particular pairing Target Variance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT) successfully balances the mitigation of massive activations with preserved downstream model performance in the scenarios we investigated. Our code is available at: https://github.com/bluorion-com/refine_massive_activations.

**Link**: [arxiv](http://arxiv.org/abs/2503.22329v1),  [pdf](http://arxiv.org/pdf/2503.22329v1)

**Tags**: cs.CL 



### EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge   Devices
**Authors**: Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, Xiaoxin Chen

**Updated**: 2025-03-28T07:26:37Z

**Summary**: Transformer-based large language models (LLMs) encounter challenges in processing long sequences on edge devices due to the quadratic complexity of attention mechanisms and growing memory demands from Key-Value (KV) cache. Existing KV cache optimizations struggle with irreversible token eviction in long-output tasks, while alternative sequence modeling architectures prove costly to adopt within established Transformer infrastructure. We present EdgeInfinite, a memory-efficient solution for infinite contexts that integrates compressed memory into Transformer-based LLMs through a trainable memory-gating module. This approach maintains full compatibility with standard Transformer architectures, requiring fine-tuning only a small part of parameters, and enables selective activation of the memory-gating module for long and short context task routing. The experimental result shows that EdgeInfinite achieves comparable performance to baseline Transformer-based LLM on long context benchmarks while optimizing memory consumption and time to first token.

**Link**: [arxiv](http://arxiv.org/abs/2503.22196v1),  [pdf](http://arxiv.org/pdf/2503.22196v1)

**Tags**: cs.CL 



### Performance Characterizations and Usage Guidelines of Samsung CXL Memory   Module Hybrid Prototype
**Authors**: Jianping Zeng, Shuyi Pei, Da Zhang, Yuchen Zhou, Amir Beygi, Xuebin Yao, Ramdas Kachare, Tong Zhang, Zongwang Li, Marie Nguyen, Rekha Pitchumani, Yang Soek Ki, Changhee Jung

**Updated**: 2025-03-27T22:16:57Z

**Summary**: The growing prevalence of data-intensive workloads, such as artificial intelligence (AI), machine learning (ML), high-performance computing (HPC), in-memory databases, and real-time analytics, has exposed limitations in conventional memory technologies like DRAM. While DRAM offers low latency and high throughput, it is constrained by high costs, scalability challenges, and volatility, making it less viable for capacity-bound and persistent applications in modern datacenters.   Recently, Compute Express Link (CXL) has emerged as a promising alternative, enabling high-speed, cacheline-granular communication between CPUs and external devices. By leveraging CXL technology, NAND flash can now be used as memory expansion, offering three-fold benefits: byte-addressability, scalable capacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid (CMM-H) is the first product to deliver these benefits through a hardware-only solution, i.e., it does not incur any OS and IO overheads like conventional block devices. In particular, CMM-H integrates a DRAM cache with NAND flash in a single device to deliver near-DRAM latency. This paper presents the first publicly available study for comprehensive characterizations of an FPGA-based CMM-H prototype. Through this study, we address users' concerns about whether a wide variety of applications can successfully run on a memory device backed by NAND flash medium. Additionally, based on these characterizations, we provide key insights into how to best take advantage of the CMM-H device.

**Link**: [arxiv](http://arxiv.org/abs/2503.22017v1),  [pdf](http://arxiv.org/pdf/2503.22017v1)

**Tags**: cs.AR 



### Reimagining Memory Access for LLM Inference: Compression-Aware Memory   Controller Design
**Authors**: Rui Xie, Asad Ul Haq, Linsen Ma, Yunhua Fang, Zirak Burzin Engineer, Liu Liu, Tong Zhang

**Updated**: 2025-03-27T17:48:14Z

**Summary**: The efficiency of Large Language Model~(LLM) inference is often constrained by substantial memory bandwidth and capacity demands. Existing techniques, such as pruning, quantization, and mixture of experts/depth, reduce memory capacity and/or bandwidth consumption at the cost of slight degradation in inference quality. This paper introduces a design solution that further alleviates memory bottlenecks by enhancing the on-chip memory controller in AI accelerators to achieve two main objectives: (1) significantly reducing memory capacity and bandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of model weights and key-value (KV) cache without compromising inference quality, and (2) enabling memory bandwidth and energy consumption to scale proportionally with context-dependent dynamic quantization. These goals are accomplished by equipping the on-chip memory controller with mechanisms to improve fine-grained bit-level accessibility and compressibility of weights and KV cache through LLM-aware configuration of in-memory placement and representation. Experimental results on publicly available LLMs demonstrate the effectiveness of this approach, showing memory footprint reductions of 25.2\% for model weights and 46.9\% for KV cache. In addition, our hardware prototype at 4\,GHz and 32 lanes (7\,nm) achieves 8\,TB/s throughput with a modest area overhead (under 3.8\,mm\(^2\)), which underscores the viability of LLM-aware memory control as a key to efficient large-scale inference.

**Link**: [arxiv](http://arxiv.org/abs/2503.18869v2),  [pdf](http://arxiv.org/pdf/2503.18869v2)

**Tags**: cs.AR 



### Low-noise environment for probing fundamental symmetries
**Authors**: F. J. Collings, N. J. Fitch, J. M. Dyne, R. A. Jenkins, E. Wursten, M. T. Ziemba, X. S. Zheng, F. Castellini, J. Lim, B. E. Sauer, M. R. Tarbutt

**Updated**: 2025-03-27T17:37:12Z

**Summary**: We present the design and characterization of a low-noise environment for measuring the electron's electric dipole moment (EDM) with a beam of molecules. To minimize magnetic Johnson noise from metals, the design features ceramic electric field plates housed in a glass vacuum chamber. To suppress external magnetic noise the apparatus is enclosed within a cylindrical four-layer mu-metal shield with a shielding factor exceeding $10^6$ in one radial direction and $10^5$ in the other. Finite element modelling shows that the difference between these shielding factors is due to imperfect joints between sections of mu-metal. Using atomic magnetometers to monitor the magnetic field inside the shield, we measure noise below 40 fT/$\sqrt{{\rm Hz}}$ at 1 Hz and above, rising to 500 fT/$\sqrt{{\rm Hz}}$ at 0.1 Hz. Analytical and numerical studies show that residual magnetic Johnson noise contributes approximately 13 fT/$\sqrt{{\rm Hz}}$. The background magnetic field averaged along the beamline is maintained below 3 pT, with typical gradients of a few nT/m. An electric field of 20 kV/cm is applied without discharges and with leakage currents below 1 nA. Each magnetometer measures the magnetic field correlated with the direction of the applied electric field with a precision of 0.11 fT in 104 hours of data. These results demonstrate that the apparatus is suitable for measuring the electron EDM with precision at the $10^{-31}$ e cm level. The design principles and characterization techniques presented here are broadly applicable to precision measurements probing fundamental symmetries in molecules, atoms, and neutrons.

**Link**: [arxiv](http://arxiv.org/abs/2503.21725v1),  [pdf](http://arxiv.org/pdf/2503.21725v1)

**Tags**: physics.atom-ph 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-03-27T15:21:19Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty; only the dirty elements need be processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing a number of auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a more computationally demanding priority queue algorithm to avoid the need to access auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, or animations. Moreover, thanks to numerous low-level optimizations, we are able to make Spineless Traversal competitive across the whole spectrum of incremental layout workloads. As a result, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the benchmark, with a mean speedup of 3.23x concentrated in the most latency-critical interactions such as hovering, typing, and animations.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v4),  [pdf](http://arxiv.org/pdf/2411.10659v4)

**Tags**: cs.PL 



### WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for   Efficient LLM Inference
**Authors**: Youhui Zuo, Sibo Wei, Chen Zhang, Zhuorui Liu, Wenpeng Lu, Dawei Song

**Updated**: 2025-03-27T14:11:37Z

**Summary**: With the advancements in long-context inference capabilities of large language models (LLMs), the KV cache has become one of the foundational components. However, its substantial GPU memory consumption makes KV cache compression a key technique for enabling efficient LLM inference in industrial scenarios. While recent studies have focused on optimizing the memory occupied by the KV cache, they overlook two critical factors: preserving semantic coherence and considering task-specific characteristic during compression. To address these limitations, we propose a novel task-adaptive KV cache window selection method, WindowKV. WindowKV dynamically selects local semantic windows consisting of consecutive tokens, according to task-specific characteristics, ensuring the retained KV cache captures continuous, essential context. Additionally, we introduce an intra-group layer KV cache indices sharing strategy to reduce computational overhead, achieving a balance between performance and efficiency. We rigorously evaluate WindowKV on the LongBench benchmark, and the results demonstrate that it maintains a performance comparable to full KV cache retention while using only 12% of the original KV cache, significantly reducing memory requirements. Furthermore, our method also achieves state-of-the-art results in the Needle-in-a-Haystack evaluation, highlighting its effectiveness and robustness.

**Link**: [arxiv](http://arxiv.org/abs/2503.17922v2),  [pdf](http://arxiv.org/pdf/2503.17922v2)

**Tags**: cs.CL 



### Arm DynamIQ Shared Unit and Real-Time: An Empirical Evaluation
**Authors**: Ashutosh Pradhan, Daniele Ottaviano, Yi Jiang, Haozheng Huang, Alexander Zuepke, Andrea Bastoni, Marco Caccamo

**Updated**: 2025-03-27T12:14:56Z

**Summary**: The increasing complexity of embedded hardware platforms poses significant challenges for real-time workloads. Architectural features such as Intel RDT, Arm QoS, and Arm MPAM are either unavailable on commercial embedded platforms or designed primarily for server environments optimized for average-case performance and might fail to deliver the expected real-time guarantees. Arm DynamIQ Shared Unit (DSU) includes isolation features-among others, hardware per-way cache partitioning-that can improve the real-time guarantees of complex embedded multicore systems and facilitate real-time analysis. However, the DSU also targets average cases, and its real-time capabilities have not yet been evaluated. This paper presents the first comprehensive analysis of three real-world deployments of the Arm DSU on Rockchip RK3568, Rockchip RK3588, and NVIDIA Orin platforms. We integrate support for the DSU at the operating system and hypervisor level and conduct a large-scale evaluation using both synthetic and real-world benchmarks with varying types and intensities of interference. Our results make extensive use of performance counters and indicate that, although effective, the quality of partitioning and isolation provided by the DSU depends on the type and the intensity of the interfering workloads. In addition, we uncover and analyze in detail the correlation between benchmarks and different types and intensities of interference.

**Link**: [arxiv](http://arxiv.org/abs/2503.17038v3),  [pdf](http://arxiv.org/pdf/2503.17038v3)

**Tags**: cs.PF cs.AR 68M20 C.3; C.4; D.4.7 



### Rethinking Video Tokenization: A Conditioned Diffusion-based Approach
**Authors**: Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan

**Updated**: 2025-03-27T11:46:22Z

**Summary**: Existing video tokenizers typically use the traditional Variational Autoencoder (VAE) architecture for video compression and reconstruction. However, to achieve good performance, its training process often relies on complex multi-stage training tricks that go beyond basic reconstruction loss and KL regularization. Among these tricks, the most challenging is the precise tuning of adversarial training with additional Generative Adversarial Networks (GANs) in the final stage, which can hinder stable convergence. In contrast to GANs, diffusion models offer more stable training processes and can generate higher-quality results. Inspired by these advantages, we propose CDT, a novel Conditioned Diffusion-based video Tokenizer, that replaces the GAN-based decoder with a conditional causal diffusion model. The encoder compresses spatio-temporal information into compact latents, while the decoder reconstructs videos through a reverse diffusion process conditioned on these latents. During inference, we incorporate a feature cache mechanism to generate videos of arbitrary length while maintaining temporal continuity and adopt sampling acceleration technique to enhance efficiency. Trained using only a basic MSE diffusion loss for reconstruction, along with KL term and LPIPS perceptual loss from scratch, extensive experiments demonstrate that CDT achieves state-of-the-art performance in video reconstruction tasks with just a single-step sampling. Even a scaled-down version of CDT (3$\times$ inference speedup) still performs comparably with top baselines. Moreover, the latent video generation model trained with CDT also exhibits superior performance. The source code and pretrained weights are available at https://github.com/ali-vilab/CDT.

**Link**: [arxiv](http://arxiv.org/abs/2503.03708v3),  [pdf](http://arxiv.org/pdf/2503.03708v3)

**Tags**: cs.CV cs.AI 



### FlooNoC: A 645 Gbps/link 0.15 pJ/B/hop Open-Source NoC with Wide   Physical Links and End-to-End AXI4 Parallel Multi-Stream Support
**Authors**: Tim Fischer, Michael Rogenmoser, Thomas Benz, Frank K. Gürkaynak, Luca Benini

**Updated**: 2025-03-27T09:53:15Z

**Summary**: The new generation of domain-specific AI accelerators is characterized by rapidly increasing demands for bulk data transfers, as opposed to small, latency-critical cache line transfers typical of traditional cache-coherent systems. In this paper, we address this critical need by introducing the FlooNoC Network-on-Chip (NoC), featuring very wide, fully Advanced eXtensible Interface (AXI4) compliant links designed to meet the massive bandwidth needs at high energy efficiency. At the transport level, non-blocking transactions are supported for latency tolerance. Additionally, a novel end-to-end ordering approach for AXI4, enabled by a multi-stream capable Direct Memory Access (DMA) engine simplifies network interfaces and eliminates inter-stream dependencies. Furthermore, dedicated physical links are instantiated for short, latency-critical messages. A complete end-to-end reference implementation in 12nm FinFET technology demonstrates the physical feasibility and power performance area (PPA) benefits of our approach. Utilizing wide links on high levels of metal, we achieve a bandwidth of 645 Gbps per link and a total aggregate bandwidth of 103 Tbps for an 8x4 mesh of processors cluster tiles, with a total of 288 RISC-V cores. The NoC imposes a minimal area overhead of only 3.5% per compute tile and achieves a leading-edge energy efficiency of 0.15 pJ/B/hop at 0.8 V. Compared to state-of-the-art NoCs, our system offers three times the energy efficiency and more than double the link bandwidth. Furthermore, compared to a traditional AXI4-based multi-layer interconnect, our NoC achieves a 30% reduction in area, corresponding to a 47% increase in GFLOPSDP within the same floorplan.

**Link**: [arxiv](http://arxiv.org/abs/2409.17606v2),  [pdf](http://arxiv.org/pdf/2409.17606v2)

**Tags**: cs.AR 



### Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer   Decoding
**Authors**: Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yu Tian

**Updated**: 2025-03-27T07:02:19Z

**Summary**: The key-value (KV) cache in the tensor version of transformers presents a significant bottleneck during inference. While previous work analyzes the fundamental space complexity barriers in standard attention mechanisms [Haris and Onak, 2025], our work generalizes the space complexity barriers result to tensor attention version. Our theoretical contributions rely on a reduction from communication complexity and deduce the memory lower bound for tensor-structured attention mechanisms when $d = \Omega(\log n)$. Furthermore, we introduce two types of tensor attention cache and present a trade-off between time and memory for two scenarios. Overall, our work provides a theoretical foundation for us to understand the time-memory tradeoff of KV-Cache compression in tensor attention decoding and offers more perspectives in developing more memory-efficient tensor attention Transformer architectures.

**Link**: [arxiv](http://arxiv.org/abs/2503.11108v2),  [pdf](http://arxiv.org/pdf/2503.11108v2)

**Tags**: cs.LG cs.AI cs.CC cs.CL 



### Unleashing Vecset Diffusion Model for Fast Shape Generation
**Authors**: Zeqiang Lai, Yunfei Zhao, Zibo Zhao, Haolin Liu, Fuyun Wang, Huiwen Shi, Xianghui Yang, Qingxiang Lin, Jingwei Huang, Yuhong Liu, Jie Jiang, Chunchao Guo, Xiangyu Yue

**Updated**: 2025-03-26T15:08:12Z

**Summary**: 3D shape generation has greatly flourished through the development of so-called "native" 3D diffusion, particularly through the Vecset Diffusion Model (VDM). While recent advancements have shown promising results in generating high-resolution 3D shapes, VDM still struggles with high-speed generation. Challenges exist because of difficulties not only in accelerating diffusion sampling but also VAE decoding in VDM, areas under-explored in previous works. To address these challenges, we present FlashVDM, a systematic framework for accelerating both VAE and DiT in VDM. For DiT, FlashVDM enables flexible diffusion sampling with as few as 5 inference steps and comparable quality, which is made possible by stabilizing consistency distillation with our newly introduced Progressive Flow Distillation. For VAE, we introduce a lightning vecset decoder equipped with Adaptive KV Selection, Hierarchical Volume Decoding, and Efficient Network Design. By exploiting the locality of the vecset and the sparsity of shape surface in the volume, our decoder drastically lowers FLOPs, minimizing the overall decoding overhead. We apply FlashVDM to Hunyuan3D-2 to obtain Hunyuan3D-2 Turbo. Through systematic evaluation, we show that our model significantly outperforms existing fast 3D generation methods, achieving comparable performance to the state-of-the-art while reducing inference time by over 45x for reconstruction and 32x for generation. Code and models are available at https://github.com/Tencent/FlashVDM.

**Link**: [arxiv](http://arxiv.org/abs/2503.16302v2),  [pdf](http://arxiv.org/pdf/2503.16302v2)

**Tags**: cs.CV cs.AI eess.IV 



### SGDRC: Software-Defined Dynamic Resource Control for Concurrent DNN   Inference on NVIDIA GPUs
**Authors**: Yongkang Zhang, Haoxuan Yu, Chenxia Han, Cheng Wang, Baotong Lu, Yunzhe Li, Zhifeng Jiang, Yang Li, Xiaowen Chu, Huaicheng Li

**Updated**: 2025-03-26T13:59:53Z

**Summary**: Cloud service providers heavily colocate high-priority, latency-sensitive (LS), and low-priority, best-effort (BE) DNN inference services on the same GPU to improve resource utilization in data centers. Among the critical shared GPU resources, there has been very limited analysis on the dynamic allocation of compute units and VRAM bandwidth, mainly for two reasons: (1) The native GPU resource management solutions are either hardware-specific, or unable to dynamically allocate resources to different tenants, or both; (2) NVIDIA doesn't expose interfaces for VRAM bandwidth allocation, and the software stack and VRAM channel architectures are black-box, both of which limit the software-level resource management. These drive prior work to design either conservative sharing policies detrimental to throughput, or static resource partitioning only applicable to a few GPU models.   To bridge this gap, this paper proposes SGDRC, a fully software-defined dynamic VRAM bandwidth and compute unit management solution for concurrent DNN inference services. SGDRC aims at guaranteeing service quality, maximizing the overall throughput, and providing general applicability to NVIDIA GPUs. SGDRC first reveals a general VRAM channel hash mapping architecture of NVIDIA GPUs through comprehensive reverse engineering and eliminates VRAM channel conflicts using software-level cache coloring. SGDRC applies bimodal tensors and tidal SM masking to dynamically allocate VRAM bandwidth and compute units, and guides the allocation of resources based on offline profiling. We evaluate 11 mainstream DNNs with real-world workloads on two NVIDIA GPUs. The results show that compared with the state-of-the-art GPU sharing solutions, SGDRC achieves the highest SLO attainment rates (99.0% on average), and improves overall throughput by up to 1.47x and BE job throughput by up to 2.36x.

**Link**: [arxiv](http://arxiv.org/abs/2407.13996v3),  [pdf](http://arxiv.org/pdf/2407.13996v3)

**Tags**: cs.DC cs.AR cs.PF D.4.9; I.2.5 



### Analyzing Modern NVIDIA GPU cores
**Authors**: Rodrigo Huerta, Mojtaba Abaie Shoushtary, José-Lorenzo Cruz, Antonio González

**Updated**: 2025-03-26T12:10:53Z

**Summary**: GPUs are the most popular platform for accelerating HPC workloads, such as artificial intelligence and science simulations. However, most microarchitectural research in academia relies on GPU core pipeline designs based on architectures that are more than 15 years old.   This paper reverse engineers modern NVIDIA GPU cores, unveiling many key aspects of its design and explaining how GPUs leverage hardware-compiler techniques where the compiler guides hardware during execution. In particular, it reveals how the issue logic works including the policy of the issue scheduler, the structure of the register file and its associated cache, and multiple features of the memory pipeline. Moreover, it analyses how a simple instruction prefetcher based on a stream buffer fits well with modern NVIDIA GPUs and is likely to be used. Furthermore, we investigate the impact of the register file cache and the number of register file read ports on both simulation accuracy and performance.   By modeling all these new discovered microarchitectural details, we achieve 18.24% lower mean absolute percentage error (MAPE) in execution cycles than previous state-of-the-art simulators, resulting in an average of 13.98% MAPE with respect to real hardware (NVIDIA RTX A6000). Also, we demonstrate that this new model stands for other NVIDIA architectures, such as Turing. Finally, we show that the software-based dependence management mechanism included in modern NVIDIA GPUs outperforms a hardware mechanism based on scoreboards in terms of performance and area.

**Link**: [arxiv](http://arxiv.org/abs/2503.20481v1),  [pdf](http://arxiv.org/pdf/2503.20481v1)

**Tags**: cs.AR 



### Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and   Generalizable Point Cloud Analysis
**Authors**: Hongyu Sun, Qiuhong Ke, Ming Cheng, Yongcai Wang, Deying Li, Chenhui Gou, Jianfei Cai

**Updated**: 2025-03-26T11:08:20Z

**Summary**: This paper proposes a general solution to enable point cloud recognition models to handle distribution shifts at test time. Unlike prior methods, which rely heavily on training data (often inaccessible during online inference) and are limited to recognizing a fixed set of point cloud classes predefined during training, we explore a more practical and challenging scenario: adapting the model solely based on online test data to recognize both previously seen classes and novel, unseen classes at test time. To this end, we develop \textbf{Point-Cache}, a hierarchical cache model that captures essential clues of online test samples, particularly focusing on the global structure of point clouds and their local-part details. Point-Cache, which serves as a rich 3D knowledge base, is dynamically managed to prioritize the inclusion of high-quality samples. Designed as a plug-and-play module, our method can be flexibly integrated into large multimodal 3D models to support open-vocabulary point cloud recognition. Notably, our solution operates with efficiency comparable to zero-shot inference, as it is entirely training-free. Point-Cache demonstrates substantial gains across 8 challenging benchmarks and 4 representative large 3D models, highlighting its effectiveness. Code is available at https://github.com/auniquesun/Point-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2503.12150v2),  [pdf](http://arxiv.org/pdf/2503.12150v2)

**Tags**: cs.CV 



### Devil is in the Uniformity: Exploring Diverse Learners within   Transformer for Image Restoration
**Authors**: Shihao Zhou, Dayu Li, Jinshan Pan, Juncheng Zhou, Jinglei Shi, Jufeng Yang

**Updated**: 2025-03-26T02:58:41Z

**Summary**: Transformer-based approaches have gained significant attention in image restoration, where the core component, i.e, Multi-Head Attention (MHA), plays a crucial role in capturing diverse features and recovering high-quality results. In MHA, heads perform attention calculation independently from uniform split subspaces, and a redundancy issue is triggered to hinder the model from achieving satisfactory outputs. In this paper, we propose to improve MHA by exploring diverse learners and introducing various interactions between heads, which results in a Hierarchical multI-head atteNtion driven Transformer model, termed HINT, for image restoration. HINT contains two modules, i.e., the Hierarchical Multi-Head Attention (HMHA) and the Query-Key Cache Updating (QKCU) module, to address the redundancy problem that is rooted in vanilla MHA. Specifically, HMHA extracts diverse contextual features by employing heads to learn from subspaces of varying sizes and containing different information. Moreover, QKCU, comprising intra- and inter-layer schemes, further reduces the redundancy problem by facilitating enhanced interactions between attention heads within and across layers. Extensive experiments are conducted on 12 benchmarks across 5 image restoration tasks, including low-light enhancement, dehazing, desnowing, denoising, and deraining, to demonstrate the superiority of HINT. The source code is available in the supplementary materials.

**Link**: [arxiv](http://arxiv.org/abs/2503.20174v1),  [pdf](http://arxiv.org/pdf/2503.20174v1)

**Tags**: cs.CV 



### Medha: Efficiently Serving Multi-Million Context Length LLM Inference   Requests Without Approximations
**Authors**: Amey Agrawal, Haoran Qiu, Junda Chen, Íñigo Goiri, Ramachandran Ramjee, Chaojie Zhang, Alexey Tumanov, Esha Choukse

**Updated**: 2025-03-26T01:58:40Z

**Summary**: As large language models (LLMs) handle increasingly longer contexts, serving inference requests for context lengths in the range of millions of tokens presents unique challenges. While existing techniques are effective for training, they fail to address the unique challenges of inference, such as varying prefill and decode phases and their associated latency constraints -- like Time to First Token (TTFT) and Time per Output Token (TPOT). Furthermore, no long-context inference solutions address head-of-line blocking today.   We present Medha, a system for efficient long-context LLM inference that introduces three key innovations: adaptive chunking with slack-aware scheduling to prevent head-ofline blocking, Sequence Pipeline Parallelism (SPP) to reduce TTFT, and KV Cache Parallelism (KVP) to minimize TPOT. By combining these into a novel 3D parallelism serving engine, Medha achieves unprecedented scale -- supporting contexts up to 10M tokens with production-grade latency. Our evaluation shows Medha reduces median latency by up to 30x compared to state-of-the-art systems when serving a mix of short and long requests, while improving throughput by upwards of 5x. This enables, for the first time, efficient long-context LLM inference at scale without compromising on shorter request latencies or system efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2409.17264v2),  [pdf](http://arxiv.org/pdf/2409.17264v2)

**Tags**: cs.LG cs.DC 



### Visualizing the Invisible: A Generative AR System for Intuitive   Multi-Modal Sensor Data Presentation
**Authors**: Yunqi Guo, Kaiyuan Hou, Heming Fu, Hongkai Chen, Zhenyu Yan, Guoliang Xing, Xiaofan Jiang

**Updated**: 2025-03-25T17:56:01Z

**Summary**: Understanding sensor data can be difficult for non-experts because of the complexity and different semantic meanings of sensor modalities. This leads to a need for intuitive and effective methods to present sensor information. However, creating intuitive sensor data visualizations presents three key challenges: the variability of sensor readings, gaps in domain comprehension, and the dynamic nature of sensor data. To address these issues, we propose Vivar, a novel system that integrates multi-modal sensor data and presents 3D volumetric content for AR visualization. In particular, we introduce a cross-modal embedding approach that maps sensor data into a pre-trained visual embedding space through barycentric interpolation. This approach accurately reflects value changes in multi-modal sensor information, ensuring that sensor variations are properly shown in visualization outcomes. Vivar also incorporates sensor-aware AR scene generation using foundation models and 3D Gaussian Splatting (3DGS) without requiring domain expertise. In addition, Vivar leverages latent reuse and caching strategies to accelerate 2D and AR content generation, demonstrating 11x latency reduction without compromising quality. A user study involving over 503 participants, including domain experts, demonstrates Vivar's effectiveness in accuracy, consistency, and real-world applicability, paving the way for more intuitive sensor data visualization.

**Link**: [arxiv](http://arxiv.org/abs/2412.13509v2),  [pdf](http://arxiv.org/pdf/2412.13509v2)

**Tags**: cs.HC 



### LogQuant: Log-Distributed 2-Bit Quantization of KV Cache with Superior   Accuracy Preservation
**Authors**: Han Chen, Zicong Jiang, Zining Zhang, Bingsheng He, Pingyi Luo, Mian Lu, Yuqiang Chen

**Updated**: 2025-03-25T16:24:45Z

**Summary**: We introduce LogQuant, a groundbreaking 2-bit quantization technique for KV Cache in large language model (LLM) inference, delivering substantial memory savings while preserving superior performance. Previous methods either assume that later tokens are more important or attempt to predict important tokens based on earlier attention patterns. Both approaches, however, can result in performance bottlenecks or frequent mispredictions.   LogQuant takes a different approach. By applying a log-based filtering mechanism, it selectively compresses the KV Cache across the entire context, achieving better performance with the same or even reduced memory footprint compared to existing methods. In benchmark tests, it enhances throughput by 25% and boosts batch size by 60% without increasing memory consumption. For challenging tasks such as Math and Code Completion, LogQuant improves accuracy by 40% to 200% at the same compression ratio, outperforming comparable techniques.LogQuant integrates effortlessly with popular inference frameworks like Python's transformers library. Implementation can be available in https://github.com/Concyclics/LogQuantKV.

**Link**: [arxiv](http://arxiv.org/abs/2503.19950v1),  [pdf](http://arxiv.org/pdf/2503.19950v1)

**Tags**: cs.LG cs.AI cs.CL 



### Gemma 3 Technical Report
**Authors**: Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju-yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, Léonard Hussenot

**Updated**: 2025-03-25T15:52:34Z

**Summary**: We introduce Gemma 3, a multimodal addition to the Gemma family of lightweight open models, ranging in scale from 1 to 27 billion parameters. This version introduces vision understanding abilities, a wider coverage of languages and longer context - at least 128K tokens. We also change the architecture of the model to reduce the KV-cache memory that tends to explode with long context. This is achieved by increasing the ratio of local to global attention layers, and keeping the span on local attention short. The Gemma 3 models are trained with distillation and achieve superior performance to Gemma 2 for both pre-trained and instruction finetuned versions. In particular, our novel post-training recipe significantly improves the math, chat, instruction-following and multilingual abilities, making Gemma3-4B-IT competitive with Gemma2-27B-IT and Gemma3-27B-IT comparable to Gemini-1.5-Pro across benchmarks. We release all our models to the community.

**Link**: [arxiv](http://arxiv.org/abs/2503.19786v1),  [pdf](http://arxiv.org/pdf/2503.19786v1)

**Tags**: cs.CL cs.AI 



### Integrating Prefetcher Selection with Dynamic Request Allocation   Improves Prefetching Efficiency
**Authors**: Mengming Li, Qijun Zhang, Yongqing Ren, Zhiyao Xie

**Updated**: 2025-03-25T06:45:13Z

**Summary**: Hardware prefetching plays a critical role in hiding the off-chip DRAM latency. The complexity of applications results in a wide variety of memory access patterns, prompting the development of numerous cache-prefetching algorithms. Consequently, commercial processors often employ a hybrid of these algorithms to enhance the overall prefetching performance. Nonetheless, since these prefetchers share hardware resources, conflicts arising from competing prefetching requests can negate the benefits of hardware prefetching. Under such circumstances, several prefetcher selection algorithms have been proposed to mitigate conflicts between prefetchers. However, these prior solutions suffer from two limitations. First, the input demand request allocation is inaccurate. Second, the prefetcher selection criteria are coarse-grained.   In this paper, we address both limitations by introducing an efficient and widely applicable prefetcher selection algorithm--Alecto, which tailors the demand requests for each prefetcher. Every demand request is first sent to Alecto to identify suitable prefetchers before being routed to prefetchers for training and prefetching. Our analysis shows that Alecto is adept at not only harmonizing prefetching accuracy, coverage, and timeliness but also significantly enhancing the utilization of the prefetcher table, which is vital for temporal prefetching. Alecto outperforms the state-of-the-art RL-based prefetcher selection algorithm--Bandit by 2.76% in single-core, and 7.56% in eight-core. For memory-intensive benchmarks, Alecto outperforms Bandit by 5.25%. Alecto consistently delivers state-of-the-art performance in scheduling various types of cache prefetchers. In addition to the performance improvement, Alecto can reduce the energy consumption associated with accessing the prefetchers' table by 48%, while only adding less than 1 KB of storage overhead.

**Link**: [arxiv](http://arxiv.org/abs/2503.19390v1),  [pdf](http://arxiv.org/pdf/2503.19390v1)

**Tags**: cs.AR 



### CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs
**Authors**: Insu Han, Zeliang Zhang, Zhiyuan Wang, Yifan Zhu, Susan Liang, Jiani Liu, Haiting Lin, Mingjie Zhao, Chenliang Xu, Kun Wan, Wentian Zhao

**Updated**: 2025-03-24T23:47:51Z

**Summary**: Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance across diverse applications. However, their computational overhead during deployment remains a critical bottleneck. While Key-Value (KV) caching effectively trades memory for computation to enhance inference efficiency, the growing memory footprint from extensive KV caches significantly reduces throughput and restricts prolonged deployment on memory-constrained GPU devices. To address this challenge, we propose CalibQuant, a simple yet highly effective visual quantization strategy that drastically reduces both memory and computational overhead. Specifically, CalibQuant introduces an extreme 1-bit quantization scheme, complemented by novel post-scaling and calibration techniques tailored to the intrinsic patterns of KV caches, thereby ensuring high efficiency without compromising model performance. Leveraging Triton for runtime optimization, we achieve a 10x throughput increase on InternVL models. Our method is designed to be plug-and-play, seamlessly integrating with various existing MLLMs without requiring architectural changes. Extensive experiments confirm that our approach significantly reduces memory usage while maintaining computational efficiency and preserving multimodal capabilities. Codes are available at https://github.com/insuhan/calibquant.

**Link**: [arxiv](http://arxiv.org/abs/2502.14882v2),  [pdf](http://arxiv.org/pdf/2502.14882v2)

**Tags**: cs.CV 



### Skew-Symmetric Matrix Decompositions on Shared-Memory Architectures
**Authors**: Ishna Satyarth, Chao Yin, Devin A. Matthews, Maggie Myers, Robert van de Geijn, RuQing G. Xu

**Updated**: 2025-03-24T21:27:53Z

**Summary**: The factorization of skew-symmetric matrices is a critically understudied area of dense linear algebra, particularly in comparison to that of general and symmetric matrices. While some algorithms can be adapted from the symmetric case, the cost of algorithms can be reduced by exploiting skew-symmetry. This work examines the factorization of a skew-symmetric matrix $X$ into its $LTL^\mathrm{T}$ decomposition, where $L$ is unit lower triangular and $T$ is tridiagonal. This is also known as a triangular tridiagonalization. This operation is a means for computing the determinant of $X$ as the square of the (cheaply-computed) Pfaffian of the skew-symmetric tridiagonal matrix $T$ as well as for solving systems of equations, across fields such as quantum electronic structure and machine learning. Its application also often requires pivoting in order to improve numerical stability. We compare and contrast previously-published algorithms with those systematically derived using the FLAME methodology. Performant parallel CPU implementations are achieved by fusing operations at multiple levels in order to reduce memory traffic overhead. A key factor is the employment of new capabilities of the BLAS-like Library Instantiation Software (BLIS) framework, which now supports casting level-2 and level-3 BLAS-like operations by leveraging its gemm and other kernels, hierarchical parallelism, and cache blocking. A prototype, concise C++ API facilitates the translation of correct-by-construction algorithms into correct code. Experiments verify that the resulting implementations greatly exceed the performance of previous work.

**Link**: [arxiv](http://arxiv.org/abs/2411.09859v2),  [pdf](http://arxiv.org/pdf/2411.09859v2)

**Tags**: cs.MS 



### Compositional Caching for Training-free Open-vocabulary Attribute   Detection
**Authors**: Marco Garosi, Alessandro Conti, Gaowen Liu, Elisa Ricci, Massimiliano Mancini

**Updated**: 2025-03-24T21:00:37Z

**Summary**: Attribute detection is crucial for many computer vision tasks, as it enables systems to describe properties such as color, texture, and material. Current approaches often rely on labor-intensive annotation processes which are inherently limited: objects can be described at an arbitrary level of detail (e.g., color vs. color shades), leading to ambiguities when the annotators are not instructed carefully. Furthermore, they operate within a predefined set of attributes, reducing scalability and adaptability to unforeseen downstream applications. We present Compositional Caching (ComCa), a training-free method for open-vocabulary attribute detection that overcomes these constraints. ComCa requires only the list of target attributes and objects as input, using them to populate an auxiliary cache of images by leveraging web-scale databases and Large Language Models to determine attribute-object compatibility. To account for the compositional nature of attributes, cache images receive soft attribute labels. Those are aggregated at inference time based on the similarity between the input and cache images, refining the predictions of underlying Vision-Language Models (VLMs). Importantly, our approach is model-agnostic, compatible with various VLMs. Experiments on public datasets demonstrate that ComCa significantly outperforms zero-shot and cache-based baselines, competing with recent training-based methods, proving that a carefully designed training-free approach can successfully address open-vocabulary attribute detection.

**Link**: [arxiv](http://arxiv.org/abs/2503.19145v1),  [pdf](http://arxiv.org/pdf/2503.19145v1)

**Tags**: cs.CV 



### Mitigating KV Cache Competition to Enhance User Experience in LLM   Inference
**Authors**: Haiying Shen, Tanmoy Sen, Masahiro Tanaka

**Updated**: 2025-03-24T18:50:09Z

**Summary**: In Large Language Model (LLM) serving, the KV-cache (KVC) bottleneck causes high tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing user experience, particularly in time-sensitive applications. However, satisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To address this, we propose a system, named CacheOPT for mitigating KV Cache competition, based on key insights from our measurements, incorporating novel components. First, it estimates a request's output length, bounding the deviation with a high specified probability, adjusted based on the request arrival rate. Second, it allocates the estimated KVC demand to a request, and reuses other requests' allocated KVC to avoid preemptions while reducing waiting time. Third, it proactively allocates KVC before instead of at the time a request exhausts its allocation and reserves KVC globally to prevent preemptions. Fourth, it chooses a request that has long TBT SLO, long job remaining time and short preemption time to preempt. Fifth, it selects the shortest-latency strategy between swapping and recomputation for preemptions. Experiments show that CacheOPT achieves up to 3.29$\times$ and 2.83$\times$ lower tail TBT and tail TTFT, 47\% and 53\% higher TTFT and TBT SLO attainments, and supports up to 1.58$\times$ higher request arrival rate than the state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2503.13773v2),  [pdf](http://arxiv.org/pdf/2503.13773v2)

**Tags**: cs.CL 



### EconoServe: Maximizing Multi-Resource Utilization with SLO Guarantees in   LLM Serving
**Authors**: Haiying Shen, Tanmoy Sen

**Updated**: 2025-03-24T18:16:58Z

**Summary**: As Large Language Models (LLMs) continue to grow, reducing costs and alleviating GPU demands has become increasingly critical. However, existing schedulers primarily target either GPU compute or Key-Value Cache (KVC) utilization, failing to fully optimize both GPU compute and KVC usage during each iteration or guarantee timely KVC allocations when needed. To address these challenges, we conducted a trace-based experimental analysis and made insightful observations, leading to the design of a system called EconoServe. EconoServe maximizes multi-resource utilization while ensuring service-level objective (SLO) guarantees in LLM serving. To enable adding prompts to a batch to maximize GPU utilization in each iteration, EconoServe maintains separate waiting queues for prompt processing tasks (PTs) and generation tasks (GTs). It batches GTs with the same predicted response lengths (RL) to save scheduling time and allocates KVC space for the predicted RL to avoid KVC allocation failures. It further has a novel KVC pipelining method, allowing sharing allocated but unused KVC space to enhance KVC utilization. In addition, it prioritizes queued requests that occupy more KVC to release KVC earlier and satisfy request service-level-objective (SLO). Experimental results demonstrate that EconoServe increases throughput by up to 4$\times$ with the same level of latency, generates up to 91\% lower job completion time and up to 91\% higher SLO satisfaction ratio compared to vLLM. It also reduces the number of GPUs used in DistServe by up to 78\% while maintaining the same level of goodput.

**Link**: [arxiv](http://arxiv.org/abs/2411.06364v2),  [pdf](http://arxiv.org/pdf/2411.06364v2)

**Tags**: cs.DC 



### xKV: Cross-Layer SVD for KV-Cache Compression
**Authors**: Chi-Chih Chang, Chien-Yu Lin, Yash Akhauri, Wei-Cheng Lin, Kai-Chiang Wu, Luis Ceze, Mohamed S. Abdelfattah

**Updated**: 2025-03-24T17:06:37Z

**Summary**: Large Language Models (LLMs) with long context windows enable powerful applications but come at the cost of high memory consumption to store the Key and Value states (KV-Cache). Recent studies attempted to merge KV-cache from multiple layers into shared representations, yet these approaches either require expensive pretraining or rely on assumptions of high per-token cosine similarity across layers which generally does not hold in practice. We find that the dominant singular vectors are remarkably well-aligned across multiple layers of the KV-Cache. Exploiting this insight, we propose xKV, a simple post-training method that applies Singular Value Decomposition (SVD) on the KV-Cache of grouped layers. xKV consolidates the KV-Cache of multiple layers into a shared low-rank subspace, significantly reducing KV-Cache sizes. Through extensive evaluations on the RULER long-context benchmark with widely-used LLMs (e.g., Llama-3.1 and Qwen2.5), xKV achieves up to 6.8x higher compression rates than state-of-the-art inter-layer technique while improving accuracy by 2.7%. Moreover, xKV is compatible with the emerging Multi-Head Latent Attention (MLA) (e.g., DeepSeek-Coder-V2), yielding a notable 3x compression rates on coding tasks without performance degradation. These results highlight xKV's strong capability and versatility in addressing memory bottlenecks for long-context LLM inference. Our code is publicly available at: https://github.com/abdelfattah-lab/xKV.

**Link**: [arxiv](http://arxiv.org/abs/2503.18893v1),  [pdf](http://arxiv.org/pdf/2503.18893v1)

**Tags**: cs.CL cs.LG 



### HERMES: High-Performance RISC-V Memory Hierarchy for ML Workloads
**Authors**: Pranav Suryadevara

**Updated**: 2025-03-24T16:47:48Z

**Summary**: The growth of machine learning (ML) workloads has underscored the importance of efficient memory hierarchies to address bandwidth, latency, and scalability challenges. HERMES focuses on optimizing memory subsystems for RISC-V architectures to meet the computational needs of ML models such as CNNs, RNNs, and Transformers. This project explores state-of-the-art techniques such as advanced prefetching, tensor-aware caching, and hybrid memory models. The cornerstone of HERMES is the integration of shared L3 caches with fine-grained coherence protocols equipped with specialized pathways to deep-learning accelerators such as Gemmini. Simulation tools like Gem5 and DRAMSim2 were used to evaluate baseline performance and scalability under representative ML workloads. The findings of this study highlight the design choices, and the anticipated challenges, paving the way for low-latency scalable memory operations for ML applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.13064v2),  [pdf](http://arxiv.org/pdf/2503.13064v2)

**Tags**: cs.AR cs.PF B.3.2; C.1.3; C.3 



### Exploring the Integration of Key-Value Attention Into Pure and Hybrid   Transformers for Semantic Segmentation
**Authors**: DeShin Hwa, Tobias Holmes, Klaus Drechsler

**Updated**: 2025-03-24T16:38:31Z

**Summary**: While CNNs were long considered state of the art for image processing, the introduction of Transformer architectures has challenged this position. While achieving excellent results in image classification and segmentation, Transformers remain inherently reliant on large training datasets and remain computationally expensive. A newly introduced Transformer derivative named KV Transformer shows promising results in synthetic, NLP, and image classification tasks, while reducing complexity and memory usage. This is especially conducive to use cases where local inference is required, such as medical screening applications. We endeavoured to further evaluate the merit of KV Transformers on semantic segmentation tasks, specifically in the domain of medical imaging. By directly comparing traditional and KV variants of the same base architectures, we provide further insight into the practical tradeoffs of reduced model complexity. We observe a notable reduction in parameter count and multiply accumulate operations, while achieving similar performance from most of the KV variant models when directly compared to their QKV implementation.

**Link**: [arxiv](http://arxiv.org/abs/2503.18862v1),  [pdf](http://arxiv.org/pdf/2503.18862v1)

**Tags**: cs.CV cs.AI cs.LG 



### BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with   Low-Bit KV Cache
**Authors**: Dayou Du, Shijie Cao, Jianyi Cheng, Ting Cao, Mao Yang

**Updated**: 2025-03-24T15:22:41Z

**Summary**: The growing adoption of long-context Large Language Models (LLMs) has introduced significant memory and computational challenges in autoregressive decoding due to the expanding Key-Value (KV) cache. KV cache quantization has emerged as a promising solution, with prior work showing that 4-bit or even 2-bit quantization can maintain model accuracy while reducing memory costs. However, despite these benefits, preliminary implementations for the low-bit KV cache struggle to deliver the expected speedup due to quantization and dequantization overheads and the lack of Tensor Cores utilization. In this work, we propose BitDecoding, a GPU-optimized framework that unlocks Tensor Cores for efficient decoding with low-bit KV cache. Efficiently leveraging Tensor Cores for low-bit KV cache is challenging due to the dynamic nature of KV cache generation at each decoding step. BitDecoding addresses these challenges with a Tensor Cores-Centric BitFusion Scheme that ensures data layout compatibility to enable high utilization of Tensor Cores. Additionally, BitDecoding incorporates a warp-efficient parallel decoding kernel and a fine-grained asynchronous pipeline, minimizing dequantization overhead and improving computational efficiency. Experiments show that BitDecoding achieves up to 7.5x speedup on RTX 4090, 4.8x on A100, and 8.9x on H100, compared to FP16 FlashDecoding-v2. It also outperforms the state-of-the-art low-bit KV cache implementation (QServe) by up to 4.3x. On LLaMA-3.1-8B with a 128K sequence length, BitDecoding reduces single-batch decoding latency by 3x, demonstrating its effectiveness in long-context generation scenarios. The code is available at https://github.com/DD-DuDa/BitDecoding.

**Link**: [arxiv](http://arxiv.org/abs/2503.18773v1),  [pdf](http://arxiv.org/pdf/2503.18773v1)

**Tags**: cs.AR cs.AI cs.CL cs.PF 



### Choosing Augmentation Parameters in OSQP- A New Approach based on   Conjugate Directions
**Authors**: Avinash Kumar

**Updated**: 2025-03-24T13:09:03Z

**Summary**: This work proposes a new method to select the augmentation parameters in the operator splitting quadratic program (OSQP) algorithm so as to reduce the computation time of overall algorithm. The selection is based upon the information of conjugate directions of the coefficient matrix of a linear system of equations present in the algorithm. This selection makes it possible to cache these conjugate directions, instead of computing them at each iteration, resulting in faster computation of the solution of the linear system thus reducing the overall computation time. This reduction is demonstrated by a numerical example.

**Link**: [arxiv](http://arxiv.org/abs/2503.05941v2),  [pdf](http://arxiv.org/pdf/2503.05941v2)

**Tags**: math.OC 



### Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV   Cache Quantization
**Authors**: Minsu Kim, Seongmin Hong, RyeoWook Ko, Soongyu Choi, Hunjong Lee, Junsoo Kim, Joo-Young Kim, Jongse Park

**Updated**: 2025-03-24T11:56:50Z

**Summary**: Modern Large Language Model serving system batches multiple requests to achieve high throughput, while batching attention operations is challenging, rendering memory bandwidth a critical bottleneck. The community relies on high-end GPUs with multiple high-bandwidth memory channels. Unfortunately, HBM's high bandwidth often comes at the expense of limited memory capacity, which reduces core utilization and increases costs. Recent advancements enabling longer contexts for LLMs have substantially increased the key-value cache size, further intensifying the pressures on memory capacity. The literature has explored KV cache quantization techniques, which commonly use low bitwidth for most values, selectively using higher bitwidth for outlier values. While this approach helps achieve high accuracy and low bitwidth simultaneously, it comes with the limitation that cost for online outlier detection is excessively high, negating the advantages. We propose Oaken, an acceleration solution that achieves high accuracy and high performance simultaneously through co-designing algorithm and hardware. To effectively find a sweet spot in the accuracy-performance trade-off space of KV cache quantization, Oaken employs an online-offline hybrid approach, setting outlier thresholds offline, which are then used to determine the quantization scale online. To translate the proposed algorithmic technique into tangible performance gains, Oaken also comes with custom quantization engines and memory management units that can be integrated with any LLM accelerators. We built an Oaken accelerator on top of an LLM accelerator, LPU, and conducted a comprehensive evaluation. Our experiments show that for a batch size of 256, Oaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU, incurring a minimal accuracy loss of only 0.54\% on average, compared to state-of-the-art KV cache quantization techniques.

**Link**: [arxiv](http://arxiv.org/abs/2503.18599v1),  [pdf](http://arxiv.org/pdf/2503.18599v1)

**Tags**: cs.AR cs.LG 



### Register Dispersion: Reducing the Footprint of the Vector Register File   in Vector Engines of Low-Cost RISC-V CPUs
**Authors**: Vasileios Titopoulos, George Alexakis, Kosmas Alexandridis, Chrysostomos Nicopoulos, Giorgos Dimitrakopoulos

**Updated**: 2025-03-24T11:00:35Z

**Summary**: The deployment of Machine Learning (ML) applications at the edge on resource-constrained devices has accentuated the need for efficient ML processing on low-cost processors. While traditional CPUs provide programming flexibility, their general-purpose architecture often lacks the throughput required for complex ML models. The augmentation of a RISC-V processor with a vector unit can provide substantial data-level parallelism. However, increasing the data-level parallelism supported by vector processing would make the Vector Register File (VRF) a major area consumer in ultra low-cost processors, since 32 vector registers are required for RISC-V Vector ISA compliance. This work leverages the insight that many ML vectorized kernels require a small number of active vector registers, and proposes the use of a physically smaller VRF that dynamically caches only the vector registers currently accessed by the application. This approach, called Register Dispersion, maps the architectural vector registers to a smaller set of physical registers. The proposed ISA-compliant VRF is significantly smaller than a full-size VRF and operates like a conventional cache, i.e., it only stores the most recently accessed vector registers. Essential registers remain readily accessible within the compact VRF, while the others are offloaded to the cache/memory sub-system. The compact VRF design is demonstrated to yield substantial area and power savings, as compared to using a full VRF, with no or minimal impact on performance. This effective trade-off renders the inclusion of vector units in low-cost processors feasible and practical.

**Link**: [arxiv](http://arxiv.org/abs/2503.17333v2),  [pdf](http://arxiv.org/pdf/2503.17333v2)

**Tags**: cs.AR 



### iFlame: Interleaving Full and Linear Attention for Efficient Mesh   Generation
**Authors**: Hanxiao Wang, Biao Zhang, Weize Quan, Dong-Ming Yan, Peter Wonka

**Updated**: 2025-03-24T03:18:49Z

**Summary**: This paper propose iFlame, a novel transformer-based network architecture for mesh generation. While attention-based models have demonstrated remarkable performance in mesh generation, their quadratic computational complexity limits scalability, particularly for high-resolution 3D data. Conversely, linear attention mechanisms offer lower computational costs but often struggle to capture long-range dependencies, resulting in suboptimal outcomes. To address this trade-off, we propose an interleaving autoregressive mesh generation framework that combines the efficiency of linear attention with the expressive power of full attention mechanisms. To further enhance efficiency and leverage the inherent structure of mesh representations, we integrate this interleaving approach into an hourglass architecture, which significantly boosts efficiency. Our approach reduces training time while achieving performance comparable to pure attention-based models. To improve inference efficiency, we implemented a caching algorithm that almost doubles the speed and reduces the KV cache size by seven-eighths compared to the original Transformer. We evaluate our framework on ShapeNet and Objaverse, demonstrating its ability to generate high-quality 3D meshes efficiently. Our results indicate that the proposed interleaving framework effectively balances computational efficiency and generative performance, making it a practical solution for mesh generation. The training takes only 2 days with 4 GPUs on 39k data with a maximum of 4k faces on Objaverse.

**Link**: [arxiv](http://arxiv.org/abs/2503.16653v2),  [pdf](http://arxiv.org/pdf/2503.16653v2)

**Tags**: cs.CV 



### Jenga: Effective Memory Management for Serving LLM with Heterogeneity
**Authors**: Chen Zhang, Kuntai Du, Shu Liu, Woosuk Kwon, Xiangxi Mo, Yufeng Wang, Xiaoxuan Liu, Kaichao You, Zhuohan Li, Mingsheng Long, Jidong Zhai, Joseph Gonzalez, Ion Stoica

**Updated**: 2025-03-24T02:28:04Z

**Summary**: Large language models (LLMs) are widely used but expensive to run, especially as inference workloads grow. To lower costs, maximizing the request batch size by managing GPU memory efficiently is crucial. While PagedAttention has recently been proposed to improve the efficiency of memory management, we find that the growing heterogeneity in the embeddings dimensions, attention, and access patterns of modern LLM architectures introduces new challenges for memory allocation.   In this paper, we present Jenga, a novel memory allocation framework for heterogeneous embeddings in LLMs. Jenga tackles two key challenges: (1) minimizing memory fragmentation when managing embeddings of different sizes, and (2) enabling flexible caching and eviction policies tailored to the specific token-dependency patterns of various layers. Jenga employs a two-level memory allocator, leveraging the least common multiple (LCM) of embedding sizes to optimize memory usage and providing APIs to express layer-specific caching logic to enhance memory reuse.   We implemente Jenga on vLLM, a state-of-the-art LLM inference engine, and evaluate it with diverse LLMs, datasets, and GPU configurations. Evaluations show that Jenga improves GPU memory utilization by up to 79.6%, and increases serving throughput by up to 4.92x (1.80x on average).

**Link**: [arxiv](http://arxiv.org/abs/2503.18292v1),  [pdf](http://arxiv.org/pdf/2503.18292v1)

**Tags**: cs.DC 



### ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video   Understanding
**Authors**: Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, Liqiang Nie

**Updated**: 2025-03-24T02:17:34Z

**Summary**: Video Large Language Models (VideoLLMs) have made significant strides in video understanding but struggle with long videos due to the limitations of their backbone LLMs. Existing solutions rely on length extrapolation, which is memory-constrained, or visual token compression, which primarily leverages low-level temporal redundancy while overlooking the more effective high-level knowledge redundancy. To address this, we propose $\textbf{ReTaKe}$, a training-free method with two novel modules DPSelect and PivotKV, to jointly reduce both temporal visual redundancy and knowledge redundancy for video compression. To align with the way of human temporal perception, DPSelect identifies keyframes based on inter-frame distance peaks. To leverage LLMs' learned prior knowledge, PivotKV marks the keyframes as pivots and compress non-pivot frames by pruning low-attention tokens in their KV cache. ReTaKe enables VideoLLMs to process 8 times longer frames (up to 2048), outperforming similar-sized models by 3-5% and even rivaling much larger ones on VideoMME, MLVU, LongVideoBench, and LVBench. Moreover, by overlapping compression operations with prefilling, ReTaKe introduces only ~10% prefilling latency overhead while reducing decoding latency by ~20%. Our code is available at https://github.com/SCZwangxiao/video-ReTaKe.

**Link**: [arxiv](http://arxiv.org/abs/2412.20504v5),  [pdf](http://arxiv.org/pdf/2412.20504v5)

**Tags**: cs.CV cs.CL cs.MM 



### Risk Management for Distributed Arbitrage Systems: Integrating   Artificial Intelligence
**Authors**: Akaash Vishal Hazarika, Mahak Shah, Swapnil Patil, Pradyumna Shukla

**Updated**: 2025-03-24T01:15:43Z

**Summary**: Effective risk management solutions become absolutely crucial when financial markets embrace distributed technology and decentralized financing (DeFi). This study offers a thorough survey and comparative analysis of the integration of artificial intelligence (AI) in risk management for distributed arbitrage systems. We examine several modern caching techniques namely in memory caching, distributed caching, and proxy caching and their functions in enhancing performance in decentralized settings. Through literature review we examine the utilization of AI techniques for alleviating risks related to market volatility, liquidity challenges, operational failures, regulatory compliance, and security threats. This comparison research evaluates various case studies from prominent DeFi technologies, emphasizing critical performance metrics like latency reduction, load balancing, and system resilience. Additionally, we examine the problems and trade offs associated with these technologies, emphasizing their effects on consistency, scalability, and fault tolerance. By meticulously analyzing real world applications, specifically centering on the Aave platform as our principal case study, we illustrate how the purposeful amalgamation of AI with contemporary caching methodologies has revolutionized risk management in distributed arbitrage systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.18265v1),  [pdf](http://arxiv.org/pdf/2503.18265v1)

**Tags**: cs.DC cs.AI cs.LG I.2.11; G.3 



### Enabling the Write-Back Page Cache with Strong Consistency in   Distributed Userspace File Systems
**Authors**: Haoyu Li, Jingkai Fu, Qing Li, Windsor Hsu, Asaf Cidon

**Updated**: 2025-03-23T20:18:16Z

**Summary**: The large-scale, multi-tenant nature of cloud computing requires distributed file systems that offer stability, adaptability, and compatibility. FUSE-based distributed file systems have emerged as a popular solution for the cloud, offering fast deployment, fault isolation, and POSIX compliance. However, FUSE's performance limitations, particularly its inability to reconcile page caching with strong consistency in distributed environments, remain a persistent problem. Existing approaches either sacrifice consistency for performance or rely on inefficient caching, limiting their practicality.   To this end, we present DistFUSE, the first FUSE-based distributed file system that relies on a write-back kernel-based page cache for performance and provides strong consistency. DistFUSE achieves this by offloading userspace lock management to the kernel driver, allowing coordinated access to the kernel's page cache across nodes. This design eliminates blind local cache updates and ensures cluster-wide consistency without compromising performance. Our evaluation shows DistFUSE improves throughput by up to 75% compared to baseline approaches.

**Link**: [arxiv](http://arxiv.org/abs/2503.18191v1),  [pdf](http://arxiv.org/pdf/2503.18191v1)

**Tags**: cs.OS 



### Formal Verification of Parameterized Systems based on Induction
**Authors**: Jiaqi Xiu, Yongjian Li

**Updated**: 2025-03-23T11:07:24Z

**Summary**: Parameterized systems play a crucial role in the computer field, and their security is of great significance. Formal verification of parameterized protocols is especially challenging due to its "parameterized" feature, which brings complexity and undecidability. Existing automated parameterized verification methods have limitations, such as facing difficulties in automatically deriving parameterized invariants constrained by mixed Forall and Exists quantifiers, or having challenges in completing the parameterized verification of large and complex protocols. This paper proposes a formal verification framework for parameterized systems based on induction, named wiseParaverifier. It starts from small concretizations of protocols, analyzes inductive counterexamples, and constructs counterexample formulas to guide the entire process of parameterized verification. It also presents a heuristic Generalize method to quickly find auxiliary invariants, a method for promoting complex mixed quantifiers and merging parameterized invariants, and uses symmetric reduction ideas to accelerate the verification process. Experimental results show that wiseParaverifier can successfully complete automatic inductive verification on 7 cache coherence protocols and 10 distributed protocols. It has strong verification capabilities and migration capabilities, and can provide concise and readable verification results, which is helpful for learners to understand protocol behaviors.

**Link**: [arxiv](http://arxiv.org/abs/2503.18030v1),  [pdf](http://arxiv.org/pdf/2503.18030v1)

**Tags**: cs.LO cs.SC 



### Knowledge Rumination for Client Utility Evaluation in Heterogeneous   Federated Learning
**Authors**: Xiaorui Jiang, Yu Gao, Hengwei Xu, Qi Zhang, Yong Liao, Pengyuan Zhou

**Updated**: 2025-03-23T06:14:35Z

**Summary**: Federated Learning (FL) allows several clients to cooperatively train machine learning models without disclosing the raw data. In practical applications, asynchronous FL (AFL) can address the straggler effect compared to synchronous FL. However, Non-IID data and stale models pose significant challenges to AFL, as they can diminish the practicality of the global model and even lead to training failures. In this work, we propose a novel AFL framework called Federated Historical Learning (FedHist), which effectively addresses the challenges posed by both Non-IID data and gradient staleness based on the concept of knowledge rumination. FedHist enhances the stability of local gradients by performing weighted fusion with historical global gradients cached on the server. Relying on hindsight, it assigns aggregation weights to each participant in a multi-dimensional manner during each communication round. To further enhance the efficiency and stability of the training process, we introduce an intelligent $\ell_2$-norm amplification scheme, which dynamically regulates the learning progress based on the $\ell_2$-norms of the submitted gradients. Extensive experiments indicate FedHist outperforms state-of-the-art methods in terms of convergence performance and test accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2312.10425v2),  [pdf](http://arxiv.org/pdf/2312.10425v2)

**Tags**: cs.LG 



### Cache-Aware Cooperative Multicast Beamforming in Dynamic   Satellite-Terrestrial Networks
**Authors**: Shuo Yuan, Yaohua Sun, Mugen Peng

**Updated**: 2025-03-23T03:20:25Z

**Summary**: With the burgeoning demand for data-intensive services, satellite-terrestrial networks (STNs) face increasing backhaul link congestion, deteriorating user quality of service (QoS), and escalating power consumption. Cache-aided STNs are acknowledged as a promising paradigm for accelerating content delivery to users and alleviating the load of backhaul links. However, the dynamic nature of low earth orbit (LEO) satellites and the complex interference among satellite beams and terrestrial base stations pose challenges in effectively managing limited edge resources. To address these issues, this paper proposes a method for dynamically scheduling caching and communication resources, aiming to reduce network costs in terms of transmission power consumption and backhaul traffic, while meeting user QoS demands and resource constraints. We formulate a mixed timescale problem to jointly optimize cache placement, LEO satellite beam direction, and cooperative multicast beamforming among satellite beams and base stations. To tackle this intricate problem, we propose a two-stage solution framework, where the primary problem is decoupled into a short-term content delivery subproblem and a long-term cache placement subproblem. The former subproblem is solved by designing an alternating optimization approach with whale optimization and successive convex approximation methods according to the cache placement state, while cache content in STNs is updated using an iterative algorithm that utilizes historical information. Simulation results demonstrate the effectiveness of our proposed algorithms, showcasing their convergence and significantly reducing transmission power consumption and backhaul traffic by up to 52%.

**Link**: [arxiv](http://arxiv.org/abs/2503.17913v1),  [pdf](http://arxiv.org/pdf/2503.17913v1)

**Tags**: cs.NI eess.SP 



### VSAG: An Optimized Search Framework for Graph-based Approximate Nearest   Neighbor Search
**Authors**: Xiaoyao Zhong, Haotian Li, Jiabao Jin, Mingyu Yang, Deming Chu, Xiangyu Wang, Zhitao Shen, Wei Jia, George Gu, Yi Xie, Xuemin Lin, Heng Tao Shen, Jingkuan Song, Peng Cheng

**Updated**: 2025-03-23T03:16:50Z

**Summary**: Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graph-based ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index.   This paper introduces VSAG, an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations: (i) efficient memory access: it reduces L3 cache misses with pre-fetching and cache-friendly vector organization; (ii) automated parameter tuning: it automatically selects performance-optimal parameters without requiring index rebuilding; (iii) efficient distance computation: it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate VSAG on real-world datasets. The experimental results show that VSAG achieves the state-of-the-art performance and provides up to 4x speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2503.17911v1),  [pdf](http://arxiv.org/pdf/2503.17911v1)

**Tags**: cs.DB 



### Orientation-Dependent \b{eta}-Ga2O3 Heterojunction Diode with Atomic   Layer Deposition (ALD) Grown NiO
**Authors**: Yizheng Liu, Shane M. W. Witsell, John F. Conley, Sriram Krishnamoorthy

**Updated**: 2025-03-23T01:17:08Z

**Summary**: This work reports the demonstration of ALD-deposited NiO/\b{eta}-Ga2O3 heterojunction diodes (HJDs) on low doped drift layer and highly doped (001) & (100) n+ substrates with experimental observation of a parallel-plane junction electric field as high as 7.5 MV/cm, revealing a crystal orientation dependence in \b{eta}-Ga2O3. We use a novel metalorganic precursor bis(1,4-di-tert-butyl-1,3-diazadienyl) (nickel Ni(tBu2DAD)2) with ozone (O3) to deposit NiO. The NiO/\b{eta}-Ga2O3 HJD on 7.7 {\mu}m-thick HVPE-grown drift region exhibited an on-state current density of ~20 A/cm2 at 5 V, ~10-8 A/cm2 reverse leakage at low reverse bias(-5 V), and a rectifying ratio(Jon/Joff) of ~109. The HJD broke down at ~2.2 kV reverse bias, corresponding to a ~3.4 MV/cm parallel-plane junction electric field, with a noise floor reverse leakage (10-8~10-6 A/cm2, nA) at 80% of the device catastrophic breakdown voltage. The NiO/\b{eta}-Ga2O3 HJDs on n+ (001) & (100) highly-doped substrates exhibited breakdown voltages at 12.5-16.0 V and 28.5-70.5 V, respectively, with extracted critical electric field (EC) at 2.30-2.76 MV/cm, and 4.33-7.50 MV/cm, revealing a substrate crystal orientation dependence on breakdown electric field for \b{eta}-Ga2O3. The 7.5 MV/cm EC reported here is one of the highest parallel-plane junction electric fields reported in literature.

**Link**: [arxiv](http://arxiv.org/abs/2503.17895v1),  [pdf](http://arxiv.org/pdf/2503.17895v1)

**Tags**: cond-mat.mtrl-sci physics.app-ph 



### A Generative Caching System for Large Language Models
**Authors**: Arun Iyengar, Ashish Kundu, Ramana Kompella, Sai Nandan Mamidi

**Updated**: 2025-03-22T01:17:56Z

**Summary**: Caching has the potential to be of significant benefit for accessing large language models (LLMs) due to their high latencies which typically range from a small number of seconds to well over a minute. Furthermore, many LLMs charge money for queries; caching thus has a clear monetary benefit. This paper presents a new caching system for improving user experiences with LLMs. In addition to reducing both latencies and monetary costs for accessing LLMs, our system also provides important features that go beyond the performance benefits typically associated with caches. A key feature we provide is generative caching, wherein multiple cached responses can be synthesized to provide answers to queries which have never been seen before. Our generative caches function as repositories of valuable information which can be mined and analyzed. We also improve upon past semantic caching techniques by tailoring the caching algorithms to optimally balance cost and latency reduction with the quality of responses provided. Performance tests indicate that our caches are considerably faster than GPTcache.

**Link**: [arxiv](http://arxiv.org/abs/2503.17603v1),  [pdf](http://arxiv.org/pdf/2503.17603v1)

**Tags**: cs.DB cs.AI cs.DC cs.NI 



### Multiport Support for Vortex OpenGPU Memory Hierarchy
**Authors**: Injae Shin, Blaise Tine

**Updated**: 2025-03-22T01:16:24Z

**Summary**: Modern day applications have grown in size and require more computational power. The rise of machine learning and AI increased the need for parallel computation, which has increased the need for GPGPUs. With the increasing demand for computational power, GPGPUs' SIMT architecture has solved this with an increase in the number of threads and the number of cores in a GPU, increasing the throughput of these processors to match the demand of the applications. However, this created a larger demand for the memory, making the memory bandwidth a bottleneck. The introduction of High-Bandwidth Memory (HBM) with its increased number of memory ports offers a potential solution for the GPU to exploit its memory parallelism to increase the memory bandwidth. However, effectively leveraging HBM's memory parallelism to maximize bandwidth presents a unique and complex challenge for GPU architectures on how to distribute those ports among the streaming multiprocessors in the GPGPU. In this work, we extend the Vortex OpenGPU microarchitecture to incorporate a multiport memory hierarchy, spanning from the L1 cache to the last-level cache (LLC). In addition, we propose various arbitration strategies to optimize memory transfers across the cache hierarchy. The results have shown that an increase in memory ports increases IPC, achieving an average speedup of 2.34x with 8 memory ports in the tested configuration while showing relatively small area overhead.

**Link**: [arxiv](http://arxiv.org/abs/2503.17602v1),  [pdf](http://arxiv.org/pdf/2503.17602v1)

**Tags**: cs.AR 



## Keyword: LLM Inference 
 ### Bonsai: Interpretable Tree-Adaptive Grounded Reasoning
**Authors**: Kate Sanders, Benjamin Van Durme

**Updated**: 2025-04-04T17:59:50Z

**Summary**: To develop general-purpose collaborative agents, humans need reliable AI systems that can (1) adapt to new domains and (2) transparently reason with uncertainty to allow for verification and correction. Black-box models demonstrate powerful data processing abilities but do not satisfy these criteria due to their opaqueness, domain specificity, and lack of uncertainty awareness. We introduce Bonsai, a compositional and probabilistic reasoning system that generates adaptable inference trees by retrieving relevant grounding evidence and using it to compute likelihoods of sub-claims derived from broader natural language inferences. Bonsai's reasoning power is tunable at test-time via evidence scaling and it demonstrates reliable handling of varied domains including transcripts, photographs, videos, audio, and databases. Question-answering and human alignment experiments demonstrate that Bonsai matches the performance of domain-specific black-box methods while generating interpretable, grounded, and uncertainty-aware reasoning traces.

**Link**: [arxiv](http://arxiv.org/abs/2504.03640v1),  [pdf](http://arxiv.org/pdf/2504.03640v1)

**Tags**: cs.CL cs.AI cs.CV 68T50, 68T37 I.2.7 



### AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive   Cross-Modality Memory Reduction
**Authors**: Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao Yin

**Updated**: 2025-04-04T17:58:08Z

**Summary**: The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM$^2$, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%.

**Link**: [arxiv](http://arxiv.org/abs/2411.12593v3),  [pdf](http://arxiv.org/pdf/2411.12593v3)

**Tags**: cs.CV cs.AI 



### Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling   Law for Reasoning
**Authors**: Xinyi Wang, Shawn Tan, Mingyu Jin, William Yang Wang, Rameswar Panda, Yikang Shen

**Updated**: 2025-04-04T17:57:22Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. In this paper, we introduce a synthetic multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. Our reasoning task involves completing missing edges in the graph, which requires advanced multi-hop reasoning and mimics real-world reasoning scenarios. To evaluate this, we pretrain language models (LMs) from scratch solely on triples from the incomplete graph and assess their ability to infer the missing edges. Interestingly, we observe that overparameterization can impair reasoning performance due to excessive memorization. We investigate different factors that affect this U-shaped loss curve, including graph structure, model size, and training steps. To predict the optimal model size for a specific knowledge graph, we find an empirical scaling that linearly maps the knowledge graph search entropy to the optimal model size. This work provides new insights into the relationship between scaling and reasoning in LLMs, shedding light on possible ways to optimize their performance for reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.03635v1),  [pdf](http://arxiv.org/pdf/2504.03635v1)

**Tags**: cs.AI cs.CL 



### Profile Likelihoods in Cosmology: When, Why and How illustrated with   $Λ$CDM, Massive Neutrinos and Dark Energy
**Authors**: Laura Herold, Elisa G. M. Ferreira, Lukas Heinrich

**Updated**: 2025-04-04T17:52:32Z

**Summary**: Frequentist parameter inference using profile likelihoods has received increased attention in the cosmology literature recently since it can give important complementary information to Bayesian credible intervals. Here, we give a pedagogical review of frequentist parameter inference in cosmology and focus on when the graphical profile likelihood construction gives meaningful constraints, i.e. confidence intervals with correct coverage. This construction rests on the assumption of the asymptotic limit of a large data set such as in Wilks' theorem. We assess the validity of this assumption in the context of three cosmological models with Planck 2018 Plik_lite data: While our tests for the $\Lambda$CDM model indicate that the profile likelihood method gives correct coverage, $\Lambda$CDM with the sum of neutrino masses as a free parameter appears consistent with a Gaussian near a boundary motivating the use of the boundary-corrected or Feldman-Cousins graphical method; for $w_0$CDM with the equation of state of dark energy, $w_0$, as a free parameter, we find indication of a violation of the assumptions. Finally, we compare frequentist and Bayesian constraints of these models. Our results motivate care when using the graphical profile likelihood method in cosmology. Along with this paper, we publish our profile-likelihood code "pinc".

**Link**: [arxiv](http://arxiv.org/abs/2408.07700v2),  [pdf](http://arxiv.org/pdf/2408.07700v2)

**Tags**: astro-ph.CO 



### Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer   Models
**Authors**: NVIDIA, :, Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl, Marta Stepniewska-Dziubinska, Matthieu Le, Matvei Novikov, Mehrzad Samadi, Michael Andersch, Michael Evans, Miguel Martinez, Mike Chrzanowski, Mike Ranzinger, Mikolaj Blaz, Misha Smelyanskiy, Mohamed Fawzy, Mohammad Shoeybi, Mostofa Patwary, Nayeon Lee, Nima Tajbakhsh, Ning Xu, Oleg Rybakov, Oleksii Kuchaiev, Olivier Delalleau, Osvald Nitski, Parth Chadha, Pasha Shamis, Paulius Micikevicius, Pavlo Molchanov, Peter Dykas, Philipp Fischer, Pierre-Yves Aquilanti, Piotr Bialecki, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi, Rahul Kandu, Ran El-Yaniv, Raviraj Joshi, Roger Waleffe, Ruoxi Zhang, Sabrina Kavanaugh, Sahil Jain, Samuel Kriman, Sangkug Lym, Sanjeev Satheesh, Saurav Muralidharan, Sean Narenthiran, Selvaraj Anandaraj, Seonmyeong Bak, Sergey Kashirsky, Seungju Han, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Sharon Clay, Shelby Thomas, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shyamala Prayaga, Siddhartha Jain, Sirshak Das, Slawek Kierat, Somshubra Majumdar, Song Han, Soumye Singhal, Sriharsha Niverty, Stefania Alborghetti, Suseella Panguluri, Swetha Bhendigeri, Syeda Nahida Akter, Szymon Migacz, Tal Shiri, Terry Kong, Timo Roman, Tomer Ronen, Trisha Saar, Tugrul Konuk, Tuomas Rintamaki, Tyler Poon, Ushnish De, Vahid Noroozi, Varun Singh, Vijay Korthikanti, Vitaly Kurin, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenliang Dai, Wonmin Byeon, Xiaowei Ren, Yao Xu, Yejin Choi, Yian Zhang, Ying Lin, Yoshi Suhara, Zhiding Yu, Zhiqi Li, Zhiyu Li, Zhongbo Zhu, Zhuolin Yang, Zijia Chen

**Updated**: 2025-04-04T17:41:58Z

**Summary**: As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. All Nemotron-H models will be released, with support in Hugging Face, NeMo, and Megatron-LM.

**Link**: [arxiv](http://arxiv.org/abs/2504.03624v1),  [pdf](http://arxiv.org/pdf/2504.03624v1)

**Tags**: cs.CL cs.AI cs.LG 



### Align to Structure: Aligning Large Language Models with Structural   Information
**Authors**: Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang

**Updated**: 2025-04-04T17:40:04Z

**Summary**: Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at https://github.com/minnesotanlp/struct_align.

**Link**: [arxiv](http://arxiv.org/abs/2504.03622v1),  [pdf](http://arxiv.org/pdf/2504.03622v1)

**Tags**: cs.CL cs.AI cs.LG 



### Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task
**Authors**: Leonardo Ranaldi, Barry Haddow, Alexandra Birch

**Updated**: 2025-04-04T17:35:43Z

**Summary**: Retrieval-augmented generation (RAG) has become a cornerstone of contemporary NLP, enhancing large language models (LLMs) by allowing them to access richer factual contexts through in-context retrieval. While effective in monolingual settings, especially in English, its use in multilingual tasks remains unexplored. This paper investigates the effectiveness of RAG across multiple languages by proposing novel approaches for multilingual open-domain question-answering. We evaluate the performance of various multilingual RAG strategies, including question-translation (tRAG), which translates questions into English before retrieval, and Multilingual RAG (MultiRAG), where retrieval occurs directly across multiple languages. Our findings reveal that tRAG, while useful, suffers from limited coverage. In contrast, MultiRAG improves efficiency by enabling multilingual retrieval but introduces inconsistencies due to cross-lingual variations in the retrieved content. To address these issues, we propose Crosslingual RAG (CrossRAG), a method that translates retrieved documents into a common language (e.g., English) before generating the response. Our experiments show that CrossRAG significantly enhances performance on knowledge-intensive tasks, benefiting both high-resource and low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.03616v1),  [pdf](http://arxiv.org/pdf/2504.03616v1)

**Tags**: cs.CL cs.AI 



### PAFFA: Premeditated Actions For Fast Agents
**Authors**: Shambhavi Krishna, Zheng Chen, Yuan Ling, Xiaojiang Huang, Yingjie Li, Fan Yang, Xiang Li

**Updated**: 2025-04-04T17:33:53Z

**Summary**: Modern AI assistants have made significant progress in natural language understanding and tool-use, with emerging efforts to interact with Web interfaces. However, current approaches that heavily rely on repeated LLM-driven HTML parsing are computationally expensive and error-prone, particularly when handling dynamic web interfaces and multi-step tasks. We introduce PAFFA (Premeditated Actions For Fast Agents), a method that makes LLMs faster and more accurate in completing tasks on the internet using a novel inference-time technique that requires no task-specific training. PAFFA constructs an 'Action Library', leveraging the parametric knowledge of the base LLM to pre-compute browser interaction patterns that generalize across tasks. By strategically re-using LLM inference across tasks - either via 'Dist-Map' for task-agnostic identification of key interactive web elements, or 'Unravel' for first-encounter, stateful exploration of novel tasks/sites) - PAFFA drastically reduces inference time tokens by 87% while maintaining robust performance (achieving 0.57 vs. 0.50 step accuracy compared to baseline). Further, Unravel's ability to update its action library based on explorations allows generalization and adaptation to unseen websites. In sum, this work exhibits that LLM reasoning sequences can generalize across prompts, offering a way to scale inference-time techniques for internet-scale data with sublinear token count.

**Link**: [arxiv](http://arxiv.org/abs/2412.07958v2),  [pdf](http://arxiv.org/pdf/2412.07958v2)

**Tags**: cs.AI 



### AIR: A Systematic Analysis of Annotations, Instructions, and Response   Pairs in Preference Dataset
**Authors**: Bingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen Hong, Longtao Huang, Hui Xue, Ganqu Cui, Wanxiang Che, Zhiyuan Liu, Maosong Sun

**Updated**: 2025-04-04T17:33:07Z

**Summary**: Preference learning is critical for aligning large language models (LLMs) with human values, yet its success hinges on high-quality datasets comprising three core components: Preference \textbf{A}nnotations, \textbf{I}nstructions, and \textbf{R}esponse Pairs. Current approaches conflate these components, obscuring their individual impacts and hindering systematic optimization. In this work, we propose \textbf{AIR}, a component-wise analysis framework that systematically isolates and optimizes each component while evaluating their synergistic effects. Through rigorous experimentation, AIR reveals actionable principles: annotation simplicity (point-wise generative scoring), instruction inference stability (variance-based filtering across LLMs), and response pair quality (moderate margins + high absolute scores). When combined, these principles yield +5.3 average gains over baseline method, even with only 14k high-quality pairs. Our work shifts preference dataset design from ad hoc scaling to component-aware optimization, offering a blueprint for efficient, reproducible alignment.

**Link**: [arxiv](http://arxiv.org/abs/2504.03612v1),  [pdf](http://arxiv.org/pdf/2504.03612v1)

**Tags**: cs.CL 



### Fully Automated Generation of Combinatorial Optimisation Systems Using   Large Language Models
**Authors**: Daniel Karapetyan

**Updated**: 2025-04-04T17:13:59Z

**Summary**: Over the last few decades, researchers have made considerable efforts to make decision support more accessible for small and medium enterprises by reducing the cost of designing, developing and maintaining automated decision support systems. However, due to the diversity of the underlying combinatorial optimisation problems, reusability of such systems has been limited; in most cases, expensive expertise has been required to implement bespoke software components.   We explore the feasibility of fully automated generation of combinatorial optimisation systems using large language models (LLMs). An LLM will be responsible for interpreting the user-provided problem description in natural language and designing and implementing problem-specific software components. We discuss the principles of fully automated LLM-based optimisation system generation, and evaluate several proof-of-concept generators, comparing their performance on four optimisation problems.

**Link**: [arxiv](http://arxiv.org/abs/2503.15556v2),  [pdf](http://arxiv.org/pdf/2503.15556v2)

**Tags**: cs.SE cs.PL 



### APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated   Agent-Human Interplay
**Authors**: Akshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Shiyu Wang, Zhiwei Liu, Tulika Awalgaonkar, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong

**Updated**: 2025-04-04T17:13:57Z

**Summary**: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io

**Link**: [arxiv](http://arxiv.org/abs/2504.03601v1),  [pdf](http://arxiv.org/pdf/2504.03601v1)

**Tags**: cs.CL cs.AI cs.LG 



### EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline
**Authors**: Peter Baile Chen, Tomer Wolfson, Michael Cafarella, Dan Roth

**Updated**: 2025-04-04T17:08:46Z

**Summary**: Existing information retrieval systems excel in cases where the language of target documents closely matches that of the user query. However, real-world retrieval systems are often required to implicitly reason whether a document is relevant. For example, when retrieving technical texts or tables, their relevance to the user query may be implied through a particular jargon or structure, rather than explicitly expressed in their content. Large language models (LLMs) hold great potential in identifying such implied relevance by leveraging their reasoning skills. Nevertheless, current LLM-augmented retrieval is hindered by high latency and computation cost, as the LLM typically computes the query-document relevance online, for every query anew. To tackle this issue we introduce EnrichIndex, a retrieval approach which instead uses the LLM offline to build semantically-enriched retrieval indices, by performing a single pass over all documents in the retrieval corpus once during ingestion time. Furthermore, the semantically-enriched indices can complement existing online retrieval approaches, boosting the performance of LLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving passages and tables, and found that it outperforms strong online LLM-based retrieval systems, with an average improvement of 11.7 points in recall @ 10 and 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online calls to the LLM, it processes 293.3 times fewer tokens which greatly reduces the online latency and cost. Overall, EnrichIndex is an effective way to build better retrieval indices offline by leveraging the strong reasoning skills of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.03598v1),  [pdf](http://arxiv.org/pdf/2504.03598v1)

**Tags**: cs.CL cs.AI cs.IR 



### Adaptive Semantic Prompt Caching with VectorQ
**Authors**: Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez

**Updated**: 2025-04-04T16:51:15Z

**Summary**: Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different embeddings. We propose VectorQ, an online framework with a threshold convergence guarantee to learn embedding-specific threshold regions that adapt to the uncertainty of an embedding. Through evaluations on a combination of three diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 26x increases in cache hit rate and error rate reductions up to 74%.

**Link**: [arxiv](http://arxiv.org/abs/2502.03771v2),  [pdf](http://arxiv.org/pdf/2502.03771v2)

**Tags**: cs.LG cs.CL 



### Scalable Hypergraph Structure Learning with Diverse Smoothness Priors
**Authors**: Benjamin T. Brown, Haoxiang Zhang, Daniel L. Lau, Gonzalo R. Arce

**Updated**: 2025-04-04T16:47:30Z

**Summary**: In graph signal processing, learning the weighted connections between nodes from a set of sample signals is a fundamental task when the underlying relationships are not known a priori. This task is typically addressed by finding a graph Laplacian on which the observed signals are smooth. With the extension of graphs to hypergraphs - where edges can connect more than two nodes - graph learning methods have similarly been generalized to hypergraphs. However, the absence of a unified framework for calculating total variation has led to divergent definitions of smoothness and, consequently, differing approaches to hyperedge recovery. We confront this challenge through generalization of several previously proposed hypergraph total variations, subsequently allowing ease of substitution into a vector based optimization. To this end, we propose a novel hypergraph learning method that recovers a hypergraph topology from time-series signals based on a smoothness prior. Our approach addresses key limitations in prior works, such as hyperedge selection and convergence issues, by formulating the problem as a convex optimization solved via a forward-backward-forward algorithm, ensuring guaranteed convergence. Additionally, we introduce a process that simultaneously limits the span of the hyperedge search and maintains a valid hyperedge selection set. In doing so, our method becomes scalable in increasingly complex network structures. The experimental results demonstrate improved performance, in terms of accuracy, over other state-of-the-art hypergraph inference methods; furthermore, we empirically show our method to be robust to total variation terms, biased towards global smoothness, and scalable to larger hypergraphs.

**Link**: [arxiv](http://arxiv.org/abs/2504.03583v1),  [pdf](http://arxiv.org/pdf/2504.03583v1)

**Tags**: cs.LG eess.SP 



### Hallucination Detection on a Budget: Efficient Bayesian Estimation of   Semantic Entropy
**Authors**: Kamil Ciosek, Nicolò Felicioni, Sina Ghiassian

**Updated**: 2025-04-04T16:30:44Z

**Summary**: Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 59% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM.

**Link**: [arxiv](http://arxiv.org/abs/2504.03579v1),  [pdf](http://arxiv.org/pdf/2504.03579v1)

**Tags**: cs.LG 



### Intuitionistic Fuzzy Cognitive Maps for Interpretable Image   Classification
**Authors**: Georgia Sovatzidi, Michael D. Vasilakakis, Dimitris K. Iakovidis

**Updated**: 2025-04-04T16:28:33Z

**Summary**: Several deep learning (DL) approaches have been proposed to deal with image classification tasks. However, despite their effectiveness, they lack interpretability, as they are unable to explain or justify their results. To address the challenge of interpretable image classification, this paper introduces a novel framework, named Interpretable Intuitionistic Fuzzy Cognitive Maps (I2FCMs).Intuitionistic FCMs (iFCMs) have been proposed as an extension of FCMs offering a natural mechanism to assess the quality of their output through the estimation of hesitancy, a concept resembling human hesitation in decision making. In the context of image classification, hesitancy is considered as a degree of unconfidence with which an image is categorized to a class. To the best of our knowledge this is the first time iFCMs are applied for image classification. Further novel contributions of the introduced framework include the following: a) a feature extraction process focusing on the most informative image regions; b) a learning algorithm for automatic data-driven determination of the intuitionistic fuzzy interconnections of the iFCM, thereby reducing human intervention in the definition of the graph structure; c) an inherently interpretable classification approach based on image contents, providing understandable explanations of its predictions, using linguistic terms. Furthermore, the proposed I2FCM framework can be applied to DL models, including Convolutional Neural Network (CNN), rendering them interpretable. The effectiveness of I2FCM is evaluated on publicly available datasets, and the results confirm that it can provide enhanced classification performance, while providing interpretable inferences.

**Link**: [arxiv](http://arxiv.org/abs/2408.03745v2),  [pdf](http://arxiv.org/pdf/2408.03745v2)

**Tags**: cs.CV cs.AI 



### SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge   Refinement
**Authors**: Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen

**Updated**: 2025-04-04T16:10:57Z

**Summary**: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.

**Link**: [arxiv](http://arxiv.org/abs/2504.03561v1),  [pdf](http://arxiv.org/pdf/2504.03561v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG cs.MA 



### Adaptive functional principal components analysis
**Authors**: Sunny G. W. Wang, Valentin Patilea, Nicolas Klutchnikoff

**Updated**: 2025-04-04T16:03:42Z

**Summary**: Functional data analysis almost always involves smoothing discrete observations into curves, because they are never observed in continuous time and rarely without error. Although smoothing parameters affect the subsequent inference, data-driven methods for selecting these parameters are not well-developed, frustrated by the difficulty of using all the information shared by curves while being computationally efficient. On the one hand, smoothing individual curves in an isolated, albeit sophisticated way, ignores useful signals present in other curves. On the other hand, bandwidth selection by automatic procedures such as cross-validation after pooling all the curves together quickly become computationally unfeasible due to the large number of data points. In this paper we propose a new data-driven, adaptive kernel smoothing, specifically tailored for functional principal components analysis through the derivation of sharp, explicit risk bounds for the eigen-elements. The minimization of these quadratic risk bounds provide refined, yet computationally efficient bandwidth rules for each eigen-element separately. Both common and independent design cases are allowed. Rates of convergence for the estimators are derived. An extensive simulation study, designed in a versatile manner to closely mimic the characteristics of real data sets supports our methodological contribution. An illustration on a real data application is provided.

**Link**: [arxiv](http://arxiv.org/abs/2306.16091v4),  [pdf](http://arxiv.org/pdf/2306.16091v4)

**Tags**: stat.ME math.ST stat.TH 62R10, 62G08, 62M99 



### Agentic Knowledgeable Self-awareness
**Authors**: Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen

**Updated**: 2025-04-04T16:03:38Z

**Summary**: Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.

**Link**: [arxiv](http://arxiv.org/abs/2504.03553v1),  [pdf](http://arxiv.org/pdf/2504.03553v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG cs.MA 



### Diverse In-Context Example Selection After Decomposing Programs and   Aligned Utterances Improves Semantic Parsing
**Authors**: Mayank Kothyari, Sunita Sarawagi, Soumen Chakrabarti, Gaurav Arora, Srujana Merugu

**Updated**: 2025-04-04T15:41:44Z

**Summary**: LLMs are increasingly used as seq2seq translators from natural language utterances to structured programs, a process called semantic interpretation. Unlike atomic labels or token sequences, programs are naturally represented as abstract syntax trees (ASTs). Such structured representation raises novel issues related to the design and selection of in-context examples (ICEs) presented to the LLM. We focus on decomposing the pool of available ICE trees into fragments, some of which may be better suited to solving the test instance. Next, we propose how to use (additional invocations of) an LLM with prompted syntax constraints to automatically map the fragments to corresponding utterances. Finally, we adapt and extend a recent method for diverse ICE selection to work with whole and fragmented ICE instances. We evaluate our system, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing visible accuracy gains from our proposed decomposed diverse demonstration method. Benefits are particularly notable for smaller LLMs, ICE pools having larger labeled trees, and programs in lower resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.03541v1),  [pdf](http://arxiv.org/pdf/2504.03541v1)

**Tags**: cs.CL 



### RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action   Issue Detection, Explanation and Recovery
**Authors**: Silvia Izquierdo-Badiola, Carlos Rizzo, Guillem Alenyà

**Updated**: 2025-04-04T15:38:50Z

**Summary**: As robots increasingly operate in dynamic human-centric environments, improving their ability to detect, explain, and recover from action-related issues becomes crucial. Traditional model-based and data-driven techniques lack adaptability, while more flexible generative AI methods struggle with grounding extracted information to real-world constraints. We introduce RAIDER, a novel agent that integrates Large Language Models (LLMs) with grounded tools for adaptable and efficient issue detection and explanation. Using a unique "Ground, Ask&Answer, Issue" procedure, RAIDER dynamically generates context-aware precondition questions and selects appropriate tools for resolution, achieving targeted information gathering. Our results within a simulated household environment surpass methods relying on predefined models, full scene descriptions, or standalone trained models. Additionally, RAIDER's explanations enhance recovery success, including cases requiring human interaction. Its modular architecture, featuring self-correction mechanisms, enables straightforward adaptation to diverse scenarios, as demonstrated in a real-world human-assistive task. This showcases RAIDER's potential as a versatile agentic AI solution for robotic issue detection and explanation, while addressing the problem of grounding generative AI for its effective application in embodied agents. Project website: https://eurecat.github.io/raider-llmagent/

**Link**: [arxiv](http://arxiv.org/abs/2503.17703v2),  [pdf](http://arxiv.org/pdf/2503.17703v2)

**Tags**: cs.RO 



### Coupling hydrodynamics with comoving frame radiative transfer. III. The   wind regime of early-type B hypergiants
**Authors**: M. Bernini-Peron, A. A. C. Sander, F. Najarro, G. N. Sabhahit, D. Pauli, R. R. Lefever, J. S. Vink, V. Ramachandran, L. M. Oskinova, G. González-Torà, E. C. Schösser

**Updated**: 2025-04-04T15:35:57Z

**Summary**: Context. B hypergiants (BHGs) are important for understanding high-mass stellar evolution. While they are in a similar parameter space of B supergiants (BSGs), some BHGs are known to be luminous blue variables (LBVs). Their spectra with absorption and emission features resemble Of/WNh stars. Yet, their wind physics and their evolutionary connections are not clear. Aims. In this study, we aim to understand (i) the atmospheric and wind structure, (ii) the wind-launching and -driving mechanisms, and (iii) the spectrum formation of early-type BHGs. As an observational prototype, we use zet1 Sco (B1.5Ia+). Methods. Using the atmosphere code PoWRhd, we calculated the first hydrodynamically consistent models at the BHG domain. They give insights into the radiative driving of the calculated wind regimes and enable us to study the influence of clumping and X-rays on the resulting wind structure. Results. Our consistent model reproduces the main spectral features of zet1 Sco. The obtained mass-loss rate is higher than that of similar spectral type BSGs. However, the wind optical depth of BHGs is way below unity, making them less of a transition type. To reproduce zet1 Sco's spectrum, we needed low clumping with subsonic onset. The wind has a shallow-gradient velocity profile, deviating from the beta law, and is mainly driven by Fe III opacity. Conclusions. Our study suggests that despite more mass loss, early-type Galactic BHGs have winds relatively similar to BSGs. Their winds are not thick enough to characterize them as "transition-type" stars, unlike Of/WNh, implying that emission features arise more easily in cooler than in hotter stars. The spectral BHG appearance is likely connected to atmospheric inhomogeneities below the sonic point. To reach an appearance similar to LBVs, BHGs need to be either closer to the Eddington limit or have higher wind clumping than inferred for zeta1 Sco.

**Link**: [arxiv](http://arxiv.org/abs/2504.03537v1),  [pdf](http://arxiv.org/pdf/2504.03537v1)

**Tags**: astro-ph.SR astro-ph.GA astro-ph.HE 



### Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable   Explanations
**Authors**: Lihu Chen, Shuojie Fu, Gabriel Freedman, Cemre Zor, Guy Martin, James Kinross, Uddhav Vaghela, Ovidiu Serban, Francesca Toni

**Updated**: 2025-04-04T15:21:03Z

**Summary**: A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool.

**Link**: [arxiv](http://arxiv.org/abs/2502.15429v3),  [pdf](http://arxiv.org/pdf/2502.15429v3)

**Tags**: cs.CL 



### Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles
**Authors**: Chen Wei Kuo, Kevin Chu, Nouar AlDahoul, Hazem Ibrahim, Talal Rahwan, Yasir Zaki

**Updated**: 2025-04-04T15:17:53Z

**Summary**: Bias in news reporting significantly impacts public perception, particularly regarding crime, politics, and societal issues. Traditional bias detection methods, predominantly reliant on human moderation, suffer from subjective interpretations and scalability constraints. Here, we introduce an AI-driven framework leveraging advanced large language models (LLMs), specifically GPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to systematically identify and mitigate biases in news articles. To this end, we collect an extensive dataset consisting of over 30,000 crime-related articles from five politically diverse news sources spanning a decade (2013-2023). Our approach employs a two-stage methodology: (1) bias detection, where each LLM scores and justifies biased content at the paragraph level, validated through human evaluation for ground truth establishment, and (2) iterative debiasing using GPT-4o Mini, verified by both automated reassessment and human reviewers. Empirical results indicate GPT-4o Mini's superior accuracy in bias detection and effectiveness in debiasing. Furthermore, our analysis reveals temporal and geographical variations in media bias correlating with socio-political dynamics and real-world events. This study contributes to scalable computational methodologies for bias mitigation, promoting fairness and accountability in news reporting.

**Link**: [arxiv](http://arxiv.org/abs/2504.03520v1),  [pdf](http://arxiv.org/pdf/2504.03520v1)

**Tags**: cs.CL cs.CY 



### Intracluster light is a biased tracer of the dark matter distribution in   clusters
**Authors**: J. Butler, G. Martin, N. A. Hatch, F. Pearce, S. Brough, Y. Dubois

**Updated**: 2025-04-04T15:17:26Z

**Summary**: The diffuse stellar component of galaxy clusters known as intracluster light (ICL) has been proposed as an observable tracer of the cluster's dark matter (DM) halo. Assessing its reliability as a DM tracer requires understanding how the intracluster stars are energetically linked to the underlying DM distribution, which we investigate at $z\approx0$ in 12 galaxy clusters with $M_{178} = 1.18 - 3.71 \times 10^{14}\,\textrm{M}_\odot$ from the Horizon-AGN simulation. We quantify the orbital energies of these components by their mean specific energies ${\langle \varepsilon \rangle}$, and find that this quantity is $\approx$ 25 per cent lower for the intracluster stars than the DM, whilst the energetics of the satellite galaxies (a standard DM tracer) are only marginally ($\approx$ 5 per cent) higher than the DM. Importantly, the lower ${\langle \varepsilon \rangle}$ of the intracluster stars compared to the DM is robust against the precise separation between the brightest cluster galaxy (BCG) and the ICL. The specific energy distribution of ICL stars is concentrated towards lower energies and poorly samples the higher energies, where much of the DM resides. Consequently, the intracluster stars have velocity distributions with lower typical speeds and a more centrally-concentrated density profile than the DM. We also find that intracluster stars have more radially-biased orbits than the DM, indicating these components have distinct orbital distributions. This study demonstrates that although the morphology of the ICL may match the DM halo, the ICL is a biased tracer of DM, and these biases must be understood in order to infer properties of the DM from the ICL.

**Link**: [arxiv](http://arxiv.org/abs/2504.03518v1),  [pdf](http://arxiv.org/pdf/2504.03518v1)

**Tags**: astro-ph.GA astro-ph.CO 



### A framework for computing upper bounds in passive learning settings
**Authors**: Benjamin Bordais, Daniel Neider

**Updated**: 2025-04-04T15:17:09Z

**Summary**: The task of inferring logical formulas from examples has garnered significant attention as a means to assist engineers in creating formal specifications used in the design, synthesis, and verification of computing systems. Among various approaches, enumeration algorithms have emerged as some of the most effective techniques for this task. These algorithms employ advanced strategies to systematically enumerate candidate formulas while minimizing redundancies by avoiding the generation of syntactically different but semantically equivalent formulas. However, a notable drawback is that these algorithms typically do not provide guarantees of termination, which poses challenges for their use in real-world applications.   This paper develops an abstract framework to bound the size of possible solutions for a logic inference task, thereby providing a termination guarantee for enumeration algorithms through the introduction of a sufficient stopping criterion. The proposed framework is designed with flexibility in mind and is applicable to a broad spectrum of practically relevant logical formalisms, including Modal Logic, Linear Temporal Logic, Computation Tree Logic, Alternating-time Temporal Logic, and even selected inference task for finite automata. In addition, our approach enabled us to develop a new class of algorithms that enumerate over the semantics of formulas rather than their syntactic representations, offering new possibilities for reducing redundancy.

**Link**: [arxiv](http://arxiv.org/abs/2504.03517v1),  [pdf](http://arxiv.org/pdf/2504.03517v1)

**Tags**: cs.LO 



### Low rock mass fraction within trans-Neptunian objects inferred from the   spin-orbit evolution of Orcus-Vanth and Salacia-Actaea
**Authors**: Sota Arakawa, Shunichi Kamata, Hidenori Genda

**Updated**: 2025-04-04T15:11:23Z

**Summary**: Satellites play a crucial role in understanding the formation and evolution of trans-Neptunian objects (TNOs). The spin--orbit evolution of satellite systems depends on their thermal histories, allowing us to constrain the rock mass fraction within TNOs based on their current spin--orbit states. In this study, we perform coupled thermal--orbital evolution calculations for two satellite systems around undifferentiated TNOs: Orcus--Vanth and Salacia--Actaea. Our results demonstrate that the current spin--orbit states of these systems are consistent with a rock mass fraction of approximately 20--30%. Additionally, we estimate the organic mass fraction within the TNOs and find that it is comparable to the rock mass fraction. These findings suggest that the chemical composition of TNOs closely resembles that of comets.

**Link**: [arxiv](http://arxiv.org/abs/2504.03508v1),  [pdf](http://arxiv.org/pdf/2504.03508v1)

**Tags**: astro-ph.EP 



### Structured Legal Document Generation in India: A Model-Agnostic Wrapper   Approach with VidhikDastaavej
**Authors**: Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya

**Updated**: 2025-04-04T14:41:50Z

**Summary**: Automating legal document drafting can significantly enhance efficiency, reduce manual effort, and streamline legal workflows. While prior research has explored tasks such as judgment prediction and case summarization, the structured generation of private legal documents in the Indian legal domain remains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej, a novel, anonymized dataset of private legal documents, and develop NyayaShilp, a fine-tuned legal document generation model specifically adapted to Indian legal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework that first generates structured section titles and then iteratively produces content while leveraging retrieval-based mechanisms to ensure coherence and factual accuracy. We benchmark multiple open-source LLMs, including instruction-tuned and domain-adapted versions, alongside proprietary models for comparison. Our findings indicate that while direct fine-tuning on small datasets does not always yield improvements, our structured wrapper significantly enhances coherence, factual adherence, and overall document quality while mitigating hallucinations. To ensure real-world applicability, we developed a Human-in-the-Loop (HITL) Document Generation System, an interactive user interface that enables users to specify document types, refine section details, and generate structured legal drafts. This tool allows legal professionals and researchers to generate, validate, and refine AI-generated legal documents efficiently. Extensive evaluations, including expert assessments, confirm that our framework achieves high reliability in structured legal drafting. This research establishes a scalable and adaptable foundation for AI-assisted legal drafting in India, offering an effective approach to structured legal document generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.03486v1),  [pdf](http://arxiv.org/pdf/2504.03486v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Gaussian Process Tilted Nonparametric Density Estimation using Fisher   Divergence Score Matching
**Authors**: John Paisley, Wei Zhang, Brian Barr

**Updated**: 2025-04-04T14:41:41Z

**Summary**: We present three Fisher divergence (FD) minimization algorithms for learning Gaussian process (GP) based score models for lower dimensional density estimation problems. The density is formed by multiplying a base multivariate normal distribution with an exponentiated GP refinement, and so we refer to it as a GP-tilted nonparametric density. By representing the GP part of the score as a linear function using the random Fourier feature (RFF) approximation, we show that all learning problems can be solved in closed form. This includes the basic and noise conditional versions of the Fisher divergence, as well as a novel alternative to noise conditional FD models based on variational inference (VI). Here, we propose using an ELBO-like optimization of the approximate posterior with which we derive a Fisher variational predictive distribution. The RFF representation of the GP, which is functionally equivalent to a single layer neural network score model with cosine activation, provides a unique linear form for which all expectations are in closed form. The Gaussian base also helps with tractability of the VI approximation. We demonstrate our three learning algorithms, as well as a MAP baseline algorithm, on several low dimensional density estimation problems. The closed-form nature of the learning problem removes the reliance on iterative algorithms, making this technique particularly well-suited to large data sets.

**Link**: [arxiv](http://arxiv.org/abs/2504.03485v1),  [pdf](http://arxiv.org/pdf/2504.03485v1)

**Tags**: cs.LG 



### Standardizing reverberation-mapped H$α$ and H$β$ active   galactic nuclei using radius--luminosity relations involving monochromatic   and broad H$α$ luminosities
**Authors**: Shulei Cao, Amit Kumar Mandal, Michal Zajaček, Bożena Czerny, Bharat Ratra

**Updated**: 2025-04-04T14:40:40Z

**Summary**: We test the standardizability of a homogeneous sample of 41 lower-redshift ($0.00415\leq z \leq 0.474$) active galactic nuclei (AGNs) reverberation-mapped (RM) using the broad H$\alpha$ and H$\beta$ emission lines. We find that these sources can be standardized using four radius$-$luminosity ($R-L$) relations incorporating H$\alpha$ and H$\beta$ time delays and monochromatic and broad H$\alpha$ luminosities. Although the $R-L$ relation parameters are well constrained and independent of the six cosmological models considered, the resulting cosmological constraints are weak. The measured $R-L$ relations exhibit slightly steeper slopes than predicted by a simple photoionization model and steeper than those from previous higher-redshift H$\beta$ analyses based on larger datasets. These differences likely reflect the absence of high-accreting sources in our smaller, lower-redshift sample, which primarily comprises lower-accreting AGNs. The inferred cosmological parameters are consistent within 2$\sigma$ (or better) with those from better-established cosmological probes. This contrasts with our earlier findings using a larger, heterogeneous sample of 118 H$\beta$ AGNs, which yielded cosmological constraints differing by $\gtrsim 2\sigma$ from better-established cosmological probes. Our analysis demonstrates that sample homogeneity$-$specifically, the use of a consistent time-lag determination method$-$is crucial for developing RM AGNs as a cosmological probe.

**Link**: [arxiv](http://arxiv.org/abs/2412.19665v2),  [pdf](http://arxiv.org/pdf/2412.19665v2)

**Tags**: astro-ph.GA astro-ph.CO gr-qc hep-ph hep-th 



### Multivariate Causal Effects: a Bayesian Causal Regression Factor Model
**Authors**: Dafne Zorzetto, Jenna Landy, Corwin Zigler, Giovanni Parmigiani, Roberta De Vito

**Updated**: 2025-04-04T14:38:54Z

**Summary**: The impact of wildfire smoke on air quality is a growing concern, contributing to air pollution through a complex mixture of chemical species with important implications for public health. While previous studies have primarily focused on its association with total particulate matter (PM2.5), the causal relationship between wildfire smoke and the chemical composition of PM2.5 remains largely unexplored. Exposure to these chemical mixtures plays a critical role in shaping public health, yet capturing their relationships requires advanced statistical methods capable of modeling the complex dependencies among chemical species. To fill this gap, we propose a Bayesian causal regression factor model that estimates the multivariate causal effects of wildfire smoke on the concentration of 27 chemical species in PM2.5 across the United States. Our approach introduces two key innovations: (i) a causal inference framework for multivariate potential outcomes, and (ii) a novel Bayesian factor model that employs a probit stick-breaking process as prior for treatment-specific factor scores. By focusing on factor scores, our method addresses the missing data challenge common in causal inference and enables a flexible, data-driven characterization of the latent factor structure, which is crucial to capture the complex correlation among multivariate outcomes. Through Monte Carlo simulations, we show the model's accuracy in estimating the causal effects in multivariate outcomes and characterizing the treatment-specific latent structure. Finally, we apply our method to US air quality data, estimating the causal effect of wildfire smoke on 27 chemical species in PM2.5, providing a deeper understanding of their interdependencies.

**Link**: [arxiv](http://arxiv.org/abs/2504.03480v1),  [pdf](http://arxiv.org/pdf/2504.03480v1)

**Tags**: stat.ME 



### Real-time Video Prediction With Fast Video Interpolation Model and   Prediction Training
**Authors**: Shota Hirose, Kazuki Kotoyori, Kasidis Arunruangsirilert, Fangzheng Lin, Heming Sun, Jiro Katto

**Updated**: 2025-04-04T14:29:20Z

**Summary**: Transmission latency significantly affects users' quality of experience in real-time interaction and actuation. As latency is principally inevitable, video prediction can be utilized to mitigate the latency and ultimately enable zero-latency transmission. However, most of the existing video prediction methods are computationally expensive and impractical for real-time applications. In this work, we therefore propose real-time video prediction towards the zero-latency interaction over networks, called IFRVP (Intermediate Feature Refinement Video Prediction). Firstly, we propose three training methods for video prediction that extend frame interpolation models, where we utilize a simple convolution-only frame interpolation network based on IFRNet. Secondly, we introduce ELAN-based residual blocks into the prediction models to improve both inference speed and accuracy. Our evaluations show that our proposed models perform efficiently and achieve the best trade-off between prediction accuracy and computational speed among the existing video prediction methods. A demonstration movie is also provided at http://bit.ly/IFRVPDemo. The code will be released at https://github.com/FykAikawa/IFRVP.

**Link**: [arxiv](http://arxiv.org/abs/2503.23185v2),  [pdf](http://arxiv.org/pdf/2503.23185v2)

**Tags**: cs.CV 



### Dynamic Importance in Diffusion U-Net for Enhanced Image Synthesis
**Authors**: Xi Wang, Ziqi He, Yang Zhou

**Updated**: 2025-04-04T14:23:30Z

**Summary**: Traditional diffusion models typically employ a U-Net architecture. Previous studies have unveiled the roles of attention blocks in the U-Net. However, they overlook the dynamic evolution of their importance during the inference process, which hinders their further exploitation to improve image applications. In this study, we first theoretically proved that, re-weighting the outputs of the Transformer blocks within the U-Net is a "free lunch" for improving the signal-to-noise ratio during the sampling process. Next, we proposed Importance Probe to uncover and quantify the dynamic shifts in importance of the Transformer blocks throughout the denoising process. Finally, we design an adaptive importance-based re-weighting schedule tailored to specific image generation and editing tasks. Experimental results demonstrate that, our approach significantly improves the efficiency of the inference process, and enhances the aesthetic quality of the samples with identity consistency. Our method can be seamlessly integrated into any U-Net-based architecture. Code: https://github.com/Hytidel/UNetReweighting

**Link**: [arxiv](http://arxiv.org/abs/2504.03471v1),  [pdf](http://arxiv.org/pdf/2504.03471v1)

**Tags**: cs.CV 



### Spatiotemporal causal inference with arbitrary spillover and carryover   effects
**Authors**: Mitsuru Mukaigawara, Kosuke Imai, Jason Lyall, Georgia Papadogeorgou

**Updated**: 2025-04-04T14:14:06Z

**Summary**: Micro-level data with granular spatial and temporal information are becoming increasingly available to social scientists. Most researchers aggregate such data into a convenient panel data format and apply standard causal inference methods. This approach, however, has two limitations. First, data aggregation results in the loss of detailed geo-location and temporal information, leading to potential biases. Second, most panel data methods either ignore spatial spillover and temporal carryover effects or impose restrictive assumptions on their structure. We introduce a general methodological framework for spatiotemporal causal inference with arbitrary spillover and carryover effects. Under this general framework, we demonstrate how to define and estimate causal quantities of interest, explore heterogeneous treatment effects, investigate causal mechanisms, and visualize the results to facilitate their interpretation. We illustrate the proposed methodology through an analysis of airstrikes and insurgent attacks in Iraq. The open-source software package geocausal implements all of our methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.03464v1),  [pdf](http://arxiv.org/pdf/2504.03464v1)

**Tags**: stat.ME stat.AP 



### Real-time Speech Summarization for Medical Conversations
**Authors**: Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy

**Updated**: 2025-04-04T14:12:54Z

**Summary**: In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum

**Link**: [arxiv](http://arxiv.org/abs/2406.15888v2),  [pdf](http://arxiv.org/pdf/2406.15888v2)

**Tags**: cs.CL cs.AI cs.LG cs.SD eess.AS 



### Generating ensembles of spatially-coherent in-situ forecasts using flow   matching
**Authors**: David Landry, Claire Monteleoni, Anastase Charantonis

**Updated**: 2025-04-04T14:12:53Z

**Summary**: We propose a machine-learning-based methodology for in-situ weather forecast postprocessing that is both spatially coherent and multivariate. Compared to previous work, our Flow MAtching Postprocessing (FMAP) better represents the correlation structures of the observations distribution, while also improving marginal performance at the stations. FMAP generates forecasts that are not bound to what is already modeled by the underlying gridded prediction and can infer new correlation structures from data. The resulting model can generate an arbitrary number of forecasts from a limited number of numerical simulations, allowing for low-cost forecasting systems. A single training is sufficient to perform postprocessing at multiple lead times, in contrast with other methods which use multiple trained networks at generation time. This work details our methodology, including a spatial attention transformer backbone trained within a flow matching generative modeling framework. FMAP shows promising performance in experiments on the EUPPBench dataset, forecasting surface temperature and wind gust values at station locations in western Europe up to five-day lead times.

**Link**: [arxiv](http://arxiv.org/abs/2504.03463v1),  [pdf](http://arxiv.org/pdf/2504.03463v1)

**Tags**: physics.ao-ph cs.LG 



### SpectR: Dynamically Composing LM Experts with Spectral Routing
**Authors**: William Fleshman, Benjamin Van Durme

**Updated**: 2025-04-04T13:58:44Z

**Summary**: Training large, general-purpose language models poses significant challenges. The growing availability of specialized expert models, fine-tuned from pretrained models for specific tasks or domains, offers a promising alternative. Leveraging the potential of these existing expert models in real-world applications requires effective methods to select or merge the models best suited for a given task. This paper introduces SPECTR, an approach for dynamically composing expert models at each time step during inference. Notably, our method requires no additional training and enables flexible, token- and layer-wise model combinations. Our experimental results demonstrate that SPECTR improves routing accuracy over alternative training-free methods, increasing task performance across expert domains.

**Link**: [arxiv](http://arxiv.org/abs/2504.03454v1),  [pdf](http://arxiv.org/pdf/2504.03454v1)

**Tags**: cs.CL cs.AI cs.LG 



### LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM   Applications
**Authors**: Botao Zhu, Chen Chen, Xiaoyi Fan, Yifei Zhu

**Updated**: 2025-04-04T13:37:29Z

**Summary**: Developing compound Large Language Model (LLM) applications is becoming an increasingly prevalent approach to solving real-world problems. In these applications, an LLM collaborates with various external modules, including APIs and even other LLMs, to realize complex intelligent services. However, we reveal that the intrinsic duration and structural uncertainty in compound LLM applications pose great challenges for LLM service providers in serving and scheduling them efficiently. In this paper, we propose LLMSched, an uncertainty-aware scheduling framework for emerging compound LLM applications. In LLMSched, we first design a novel DAG-based model to describe the uncertain compound LLM applications. Then, we adopt the Bayesian network to comprehensively profile compound LLM applications and identify uncertainty-reducing stages, along with an entropy-based mechanism to quantify their uncertainty reduction. Combining an uncertainty reduction strategy and a job completion time (JCT)-efficient scheme, we further propose an efficient scheduler to reduce the average JCT. Evaluation of both simulation and testbed experiments on various representative compound LLM applications shows that compared to existing state-of-the-art scheduling schemes, LLMSched can reduce the average JCT by 14~79%.

**Link**: [arxiv](http://arxiv.org/abs/2504.03444v1),  [pdf](http://arxiv.org/pdf/2504.03444v1)

**Tags**: cs.DC 



### ORLM: A Customizable Framework in Training Large Models for Automated   Optimization Modeling
**Authors**: Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, Zizhuo Wang

**Updated**: 2025-04-04T13:31:38Z

**Summary**: Optimization modeling plays a critical role in the application of Operations Research (OR) tools to address real-world problems, yet they pose challenges and require extensive expertise from OR experts. With the advent of large language models (LLMs), new opportunities have emerged to streamline and automate such task. However, current research predominantly relies on closed-source LLMs such as GPT-4, along with extensive prompt engineering techniques. This reliance stems from the scarcity of high-quality training datasets for optimization modeling, resulting in elevated costs, prolonged processing times, and privacy concerns. To address these challenges, our work is the first to propose a viable path for training open-source LLMs that are capable of optimization modeling and developing solver codes, eventually leading to a superior ability for automating optimization modeling and solving. Particularly, we design the {\sc OR-Instruct}, a semi-automated data synthesis framework for optimization modeling that enables customizable enhancements for specific scenarios or model types. This work also introduces IndustryOR, the first industrial benchmark for evaluating LLMs in solving practical OR problems. We train several 7B-scale open-source LLMs using synthesized data (dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit significantly enhanced optimization modeling capabilities, achieving competitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks. Additionally, our experiments highlight the potential of scaling law and reinforcement learning to further enhance the performance of ORLMs. The workflows and human-machine interaction paradigms of ORLMs in practical industrial applications are also discussed in the paper.

**Link**: [arxiv](http://arxiv.org/abs/2405.17743v5),  [pdf](http://arxiv.org/pdf/2405.17743v5)

**Tags**: cs.CL cs.AI cs.CE cs.LG 



### Know What You do Not Know: Verbalized Uncertainty Estimation Robustness   on Corrupted Images in Vision-Language Models
**Authors**: Mirko Borszukovszki, Ivo Pascal de Jong, Matias Valdenegro-Toro

**Updated**: 2025-04-04T13:31:08Z

**Summary**: To leverage the full potential of Large Language Models (LLMs) it is crucial to have some information on their answers' uncertainty. This means that the model has to be able to quantify how certain it is in the correctness of a given response. Bad uncertainty estimates can lead to overconfident wrong answers undermining trust in these models. Quite a lot of research has been done on language models that work with text inputs and provide text outputs. Still, since the visual capabilities have been added to these models recently, there has not been much progress on the uncertainty of Visual Language Models (VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found that the severity of the corruption negatively impacted the models' ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments.

**Link**: [arxiv](http://arxiv.org/abs/2504.03440v1),  [pdf](http://arxiv.org/pdf/2504.03440v1)

**Tags**: cs.CV cs.LG 



### ZFusion: An Effective Fuser of Camera and 4D Radar for 3D Object   Perception in Autonomous Driving
**Authors**: Sheng Yang, Tong Zhan, Shichen Qiao, Jicheng Gong, Qing Yang, Yanfeng Lu, Jian Wang

**Updated**: 2025-04-04T13:29:32Z

**Summary**: Reliable 3D object perception is essential in autonomous driving. Owing to its sensing capabilities in all weather conditions, 4D radar has recently received much attention. However, compared to LiDAR, 4D radar provides much sparser point cloud. In this paper, we propose a 3D object detection method, termed ZFusion, which fuses 4D radar and vision modality. As the core of ZFusion, our proposed FP-DDCA (Feature Pyramid-Double Deformable Cross Attention) fuser complements the (sparse) radar information and (dense) vision information, effectively. Specifically, with a feature-pyramid structure, the FP-DDCA fuser packs Transformer blocks to interactively fuse multi-modal features at different scales, thus enhancing perception accuracy. In addition, we utilize the Depth-Context-Split view transformation module due to the physical properties of 4D radar. Considering that 4D radar has a much lower cost than LiDAR, ZFusion is an attractive alternative to LiDAR-based methods. In typical traffic scenarios like the VoD (View-of-Delft) dataset, experiments show that with reasonable inference speed, ZFusion achieved the state-of-the-art mAP (mean average precision) in the region of interest, while having competitive mAP in the entire area compared to the baseline methods, which demonstrates performance close to LiDAR and greatly outperforms those camera-only methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.03438v1),  [pdf](http://arxiv.org/pdf/2504.03438v1)

**Tags**: cs.CV 



### Locations of Characters in Narratives: Andersen and Persuasion Datasets
**Authors**: Batuhan Ozyurt, Roya Arkhmammadova, Deniz Yuret

**Updated**: 2025-04-04T13:25:32Z

**Summary**: The ability of machines to grasp spatial understanding within narrative contexts is an intriguing aspect of reading comprehension that continues to be studied. Motivated by the goal to test the AI's competence in understanding the relationship between characters and their respective locations in narratives, we introduce two new datasets: Andersen and Persuasion. For the Andersen dataset, we selected fifteen children's stories from "Andersen's Fairy Tales" by Hans Christian Andersen and manually annotated the characters and their respective locations throughout each story. Similarly, for the Persuasion dataset, characters and their locations in the novel "Persuasion" by Jane Austen were also manually annotated. We used these datasets to prompt Large Language Models (LLMs). The prompts are created by extracting excerpts from the stories or the novel and combining them with a question asking the location of a character mentioned in that excerpt. Out of the five LLMs we tested, the best-performing one for the Andersen dataset accurately identified the location in 61.85% of the examples, while for the Persuasion dataset, the best-performing one did so in 56.06% of the cases.

**Link**: [arxiv](http://arxiv.org/abs/2504.03434v1),  [pdf](http://arxiv.org/pdf/2504.03434v1)

**Tags**: cs.CL I.2.7 



### Dual Low-Rank Adaptation for Continual Learning with Pre-Trained Models
**Authors**: Huancheng Chen, Jingtao Li, Nidham Gazagnadou, Weiming Zhuang, Chen Chen, Lingjuan Lyu

**Updated**: 2025-04-04T13:16:35Z

**Summary**: In the era of foundation models, we revisit continual learning~(CL), which aims to enable vision transformers (ViTs) to learn new tasks over time. However, as the scale of these models increases, catastrophic forgetting remains a persistent challenge, particularly in the presence of significant domain shifts across tasks. Recent studies highlight a crossover between CL techniques and parameter-efficient fine-tuning (PEFT), which focuses on fine-tuning only a small set of trainable parameters to adapt to downstream tasks, such as low-rank adaptation (LoRA). While LoRA achieves faster convergence and requires fewer trainable parameters, it has seldom been explored in the context of continual learning. To address this gap, we propose a novel PEFT-CL method called Dual Low-Rank Adaptation (DualLoRA), which introduces both an orthogonal LoRA adapter and a residual LoRA adapter parallel to pre-trained weights in each layer. These components are orchestrated by a dynamic memory mechanism to strike a balance between stability and plasticity. The orthogonal LoRA adapter's parameters are updated in an orthogonal subspace of previous tasks to mitigate catastrophic forgetting, while the residual LoRA adapter's parameters are updated in the residual subspace spanned by task-specific bases without interaction across tasks, offering complementary capabilities for fine-tuning new tasks. On ViT-based models, we demonstrate that DualLoRA offers significant advantages in accuracy, inference speed, and memory efficiency over existing CL methods across multiple benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2411.00623v2),  [pdf](http://arxiv.org/pdf/2411.00623v2)

**Tags**: cs.CV cs.LG 



### Edge-SD-SR: Low Latency and Parameter Efficient On-device   Super-Resolution with Stable Diffusion via Bidirectional Conditioning
**Authors**: Mehdi Noroozi, Isma Hadji, Victor Escorcia, Anestis Zaganidis, Brais Martinez, Georgios Tzimiropoulos

**Updated**: 2025-04-04T12:48:16Z

**Summary**: There has been immense progress recently in the visual quality of Stable Diffusion-based Super Resolution (SD-SR). However, deploying large diffusion models on computationally restricted devices such as mobile phones remains impractical due to the large model size and high latency. This is compounded for SR as it often operates at high res (e.g. 4Kx3K). In this work, we introduce Edge-SD-SR, the first parameter efficient and low latency diffusion model for image super-resolution. Edge-SD-SR consists of ~169M parameters, including UNet, encoder and decoder, and has a complexity of only ~142 GFLOPs. To maintain a high visual quality on such low compute budget, we introduce a number of training strategies: (i) A novel conditioning mechanism on the low resolution input, coined bidirectional conditioning, which tailors the SD model for the SR task. (ii) Joint training of the UNet and encoder, while decoupling the encodings of the HR and LR images and using a dedicated schedule. (iii) Finetuning the decoder using the UNet's output to directly tailor the decoder to the latents obtained at inference time. Edge-SD-SR runs efficiently on device, e.g. it can upscale a 128x128 patch to 512x512 in 38 msec while running on a Samsung S24 DSP, and of a 512x512 to 2048x2048 (requiring 25 model evaluations) in just ~1.1 sec. Furthermore, we show that Edge-SD-SR matches or even outperforms state-of-the-art SR approaches on the most established SR benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2412.06978v2),  [pdf](http://arxiv.org/pdf/2412.06978v2)

**Tags**: cs.CV 



### PaperBench: Evaluating AI's Ability to Replicate AI Research
**Authors**: Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, Tejal Patwardhan

**Updated**: 2025-04-04T12:44:57Z

**Summary**: We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We \href{https://github.com/openai/preparedness}{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.

**Link**: [arxiv](http://arxiv.org/abs/2504.01848v2),  [pdf](http://arxiv.org/pdf/2504.01848v2)

**Tags**: cs.AI cs.CL 



### Eigen-inference by Marchenko-Pastur inversion
**Authors**: Ben Deitmar

**Updated**: 2025-04-04T12:03:11Z

**Summary**: A new formula for Marchenko-Pastur inversion is derived and used for inference of population linear spectral statistics. The formula allows for estimation of the Stieltjes transform of the population spectral distribution $s_H(z)$, when $z$ is sufficiently far from the support of the population spectral distribution $H$. If the dimension $d$ and the sample size $n$ go to infinity simultaneously such that $\frac{d}{n} \rightarrow c>0$, the estimation error is shown to be asymptotically less than $\frac{n^{\varepsilon}}{n}$ for arbitrary $\varepsilon > 0$. By integrating along a curve around the support of $H$, estimators for population linear spectral statistics are constructed, which benefit from this convergence speed of $\frac{n^{\varepsilon}}{n}$.

**Link**: [arxiv](http://arxiv.org/abs/2504.03390v1),  [pdf](http://arxiv.org/pdf/2504.03390v1)

**Tags**: math.ST stat.TH 



### Assessing SPARQL capabilities of Large Language Models
**Authors**: Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt

**Updated**: 2025-04-04T11:59:49Z

**Summary**: The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach.   We implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion.   With this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases.

**Link**: [arxiv](http://arxiv.org/abs/2409.05925v2),  [pdf](http://arxiv.org/pdf/2409.05925v2)

**Tags**: cs.DB cs.AI cs.CL cs.IR 



### Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of   Architectural Inductive Biases on Hallucination
**Authors**: Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Boxing Chen, Sarath Chandar

**Updated**: 2025-04-04T11:55:58Z

**Summary**: The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to hallucinate false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.

**Link**: [arxiv](http://arxiv.org/abs/2410.17477v4),  [pdf](http://arxiv.org/pdf/2410.17477v4)

**Tags**: cs.CL cs.AI cs.LG 



### Capturing Small-Scale Reionization Physics: A Sub-Grid Model for Photon   Sinks with SCRIPT
**Authors**: Tirthankar Roy Choudhury, Anirban Chakraborty

**Updated**: 2025-04-04T11:55:26Z

**Summary**: The epoch of reionization represents a major phase transition in cosmic history, during which the first luminous sources ionized the intergalactic medium (IGM). However, the small-scale physics governing ionizing photon sinks - particularly the interplay between recombinations, photon propagation, and self-shielded regions - remains poorly understood. Accurately modeling these processes requires a framework that self-consistently links ionizing emissivity, the clumping factor, mean free path, and photoionization rate. In this work, we extend the photon-conserving semi-numerical framework, SCRIPT, by introducing a self-consistent sub-grid model that dynamically connects these quantities to the underlying density field, enabling a more realistic treatment of inhomogeneous recombinations and photon sinks. We validate our model against a comprehensive set of observational constraints, including the UV luminosity function from HST and JWST, CMB optical depth from Planck, and Lyman-$\alpha$ forest measurements of the IGM temperature, photoionization rate, and mean free path. Our fiducial model also successfully reproduces Lyman-$\alpha$ opacity fluctuations, reinforcing its ability to capture large-scale inhomogeneities in the reionization process. Notably, we demonstrate that traditionally independent parameters, such as the clumping factor and mean free path, are strongly correlated, with implications for the timing, morphology, and thermal evolution of reionization. Looking ahead, we will extend this framework to include machine learning-based parameter inference. With upcoming 21cm experiments poised to provide unprecedented insights, SCRIPT offers a powerful computational tool for interpreting high-redshift observations and refining our understanding of the last major phase transition in the universe.

**Link**: [arxiv](http://arxiv.org/abs/2504.03384v1),  [pdf](http://arxiv.org/pdf/2504.03384v1)

**Tags**: astro-ph.CO astro-ph.GA 



### Online Difficulty Filtering for Reasoning Oriented Reinforcement   Learning
**Authors**: Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak

**Updated**: 2025-04-04T11:52:05Z

**Summary**: Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks show that balanced online filtering yields an additional 10% in AIME and 4% improvements in average over plain GRPO. Moreover, further analysis shows the gains in sample efficiency and training time efficiency, exceeding the maximum reward of plain GRPO within 60% training time and the volume of the training set.

**Link**: [arxiv](http://arxiv.org/abs/2504.03380v1),  [pdf](http://arxiv.org/pdf/2504.03380v1)

**Tags**: cs.CL cs.AI 



### RMB: Comprehensively Benchmarking Reward Models in LLM Alignment
**Authors**: Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang

**Updated**: 2025-04-04T11:45:02Z

**Summary**: Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization. We demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs. Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. Our evaluation code and datasets are available at https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2410.09893v2),  [pdf](http://arxiv.org/pdf/2410.09893v2)

**Tags**: cs.CL 



### Mixing Samples to Address Weak Overlap in Causal Inference
**Authors**: Jaehyuk Jang, Suehyun Kim, Kwonsang Lee

**Updated**: 2025-04-04T11:43:47Z

**Summary**: In observational studies, the assumption of sufficient overlap (positivity) is fundamental for the identification and estimation of causal effects. Failing to account for this assumption yields inaccurate and potentially infeasible estimators. To address this issue, we introduce a simple yet novel approach, \textit{mixing}, which mitigates overlap violations by constructing a synthetic treated group that combines treated and control units. Our strategy offers three key advantages. First, it improves the accuracy of the estimator by preserving unbiasedness while reducing variance. The benefit is particularly significant in settings with weak overlap, though the method remains effective regardless of the overlap level. This phenomenon results from the shrinkage of propensity scores in the mixed sample, which enhances robustness to poor overlap. Second, it enables direct estimation of the target estimand without discarding extreme observations or modifying the target population, thus facilitating a straightforward interpretation of the results. Third, the mixing approach is highly adaptable to various weighting schemes, including contemporary methods such as entropy balancing. The estimation of the Mixed IPW (MIPW) estimator is done via M-estimation, and the method extends to a broader class of weighting estimators through a resampling algorithm. We illustrate the mixing approach through extensive simulation studies and provide practical guidance with a real-data analysis.

**Link**: [arxiv](http://arxiv.org/abs/2411.10801v3),  [pdf](http://arxiv.org/pdf/2411.10801v3)

**Tags**: stat.ME 



### Deep Learning-Assisted Jamming Mitigation with Movable Antenna Array
**Authors**: Xiao Tang, Yudan Jiang, Jinxin Liu, Qinghe Du, Dusit Niyato, Zhu Han

**Updated**: 2025-04-04T11:38:15Z

**Summary**: This paper reveals the potential of movable antennas in enhancing anti-jamming communication. We consider a legitimate communication link in the presence of multiple jammers and propose deploying a movable antenna array at the receiver to combat jamming attacks. We formulate the problem as a signal-to-interference-plus-noise ratio maximization, by jointly optimizing the receive beamforming and antenna element positioning. Due to the non-convexity and multi-fold difficulties from an optimization perspective, we develop a deep learning-based framework where beamforming is tackled as a Rayleigh quotient problem, while antenna positioning is addressed through multi-layer perceptron training. The neural network parameters are optimized using stochastic gradient descent to achieve effective jamming mitigation strategy, featuring offline training with marginal complexity for online inference. Numerical results demonstrate that the proposed approach achieves near-optimal anti-jamming performance thereby significantly improving the efficiency in strategy determination.

**Link**: [arxiv](http://arxiv.org/abs/2410.20344v2),  [pdf](http://arxiv.org/pdf/2410.20344v2)

**Tags**: eess.SP cs.IT math.IT 



### Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for   Energy Efficiency, Output Accuracy, and Inference Latency
**Authors**: Erik Johannes Husom, Arda Goknil, Merve Astekin, Lwin Khin Shar, Andre Kåsen, Sagar Sen, Benedikt Andreas Mithassel, Ahmet Soylu

**Updated**: 2025-04-04T11:29:30Z

**Summary**: Deploying Large Language Models (LLMs) on edge devices presents significant challenges due to computational constraints, memory limitations, inference speed, and energy consumption. Model quantization has emerged as a key technique to enable efficient LLM inference by reducing model size and computational overhead. In this study, we conduct a comprehensive analysis of 28 quantized LLMs from the Ollama library, which applies by default Post-Training Quantization (PTQ) and weight-only quantization techniques, deployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy efficiency, inference performance, and output accuracy across multiple quantization levels and task types. Models are benchmarked on five standardized datasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and we employ a high-resolution, hardware-based energy measurement tool to capture real-world power consumption. Our findings reveal the trade-offs between energy efficiency, inference speed, and accuracy in different quantization settings, highlighting configurations that optimize LLM deployment for resource-constrained environments. By integrating hardware-level energy profiling with LLM benchmarking, this study provides actionable insights for sustainable AI, bridging a critical gap in existing research on energy-aware LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2504.03360v1),  [pdf](http://arxiv.org/pdf/2504.03360v1)

**Tags**: cs.CY cs.AI cs.CL cs.LG 



### End-to-end Learning of Sparse Interventions on Activations to Steer   Generation
**Authors**: Pau Rodriguez, Michal Klein, Eleonora Gualdoni, Arno Blaas, Luca Zappella, Marco Cuturi, Xavier Suau

**Updated**: 2025-04-04T11:17:20Z

**Summary**: The growing use of generative models in daily life calls for efficient mechanisms to control their generation, to e.g., produce safe content or provide users with tools to explore style changes. Ideally, such mechanisms should be cheap, both at train and inference time, while preserving output quality. Recent research has shown that such mechanisms can be obtained by intervening exclusively on model activations, with the goal of correcting distributional differences between activations seen when using prompts from a source vs. a target set (e.g., toxic and non-toxic sentences). While cheap, these fast methods are inherently crude: their maps are tuned locally, not accounting for their impact on downstream layers, resulting in interventions that cause unintended shifts when used out-of-sample. We propose in this work linear end-to-end activation steering (LinEAS), an approach trained with a global loss that accounts simultaneously for all layerwise distributional shifts. In addition to being more robust, the loss used to train LinEAS can be regularized with sparsifying norms, which can automatically carry out neuron and layer selection. Empirically, LinEAS only requires a handful of samples to be effective, and beats similar baselines on toxicity mitigation, while performing on par with far more involved finetuning approaches. We show that LinEAS interventions can be composed, study the impact of sparsity on their performance, and showcase applications in text-to-image diffusions.

**Link**: [arxiv](http://arxiv.org/abs/2503.10679v2),  [pdf](http://arxiv.org/pdf/2503.10679v2)

**Tags**: cs.CL cs.AI 



### Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social   Psychological Underpinnings
**Authors**: Kaustubh Shivshankar Shejole, Pushpak Bhattacharyya

**Updated**: 2025-04-04T11:14:38Z

**Summary**: Stereotypes are known to be highly pernicious, making their detection critically important. However, current research predominantly focuses on detecting and evaluating stereotypical biases in LLMs, leaving the study of stereotypes in its early stages. Many studies have failed to clearly distinguish between stereotypes and stereotypical biases, which has significantly slowed progress in advancing research in this area. Stereotype and anti-stereotype detection is a problem that requires knowledge of society; hence, it is one of the most difficult areas in Responsible AI. This work investigates this task, where we propose a four-tuple definition and provide precise terminology distinguishing stereotype, anti-stereotype, stereotypical bias, and bias, offering valuable insights into their various aspects. In this paper, we propose StereoDetect, a high-quality benchmarking dataset curated for this task by optimally utilizing current datasets such as StereoSet and WinoQueer, involving a manual verification process and the transfer of semantic information. We demonstrate that language models for reasoning with fewer than 10B parameters often get confused when detecting anti-stereotypes. We also demonstrate the critical importance of well-curated datasets by comparing our model with other current models for stereotype detection. The dataset and code is available at https://github.com/KaustubhShejole/StereoDetect.

**Link**: [arxiv](http://arxiv.org/abs/2504.03352v1),  [pdf](http://arxiv.org/pdf/2504.03352v1)

**Tags**: cs.CL cs.CY cs.HC 



### Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered   Chatbots on the Web
**Authors**: Lars Krupp, Daniel Geißler, Peter Hevesi, Marco Hirsch, Paul Lukowicz, Jakob Karolus

**Updated**: 2025-04-04T10:58:57Z

**Summary**: Integrated into websites, LLM-powered chatbots offer alternative means of navigation and information retrieval, leading to a shift in how users access information on the web. Yet, predominantly closed-sourced solutions limit proliferation among web hosts and suffer from a lack of transparency with regard to implementation details and energy efficiency. In this work, we propose our openly available agent Talk2X leveraging an adapted retrieval-augmented generation approach (RAG) combined with an automatically generated vector database, benefiting energy efficiency. Talk2X's architecture is generalizable to arbitrary websites offering developers a ready to use tool for integration. Using a mixed-methods approach, we evaluated Talk2X's usability by tasking users to acquire specific assets from an open science repository. Talk2X significantly improved task completion time, correctness, and user experience supporting users in quickly pinpointing specific information as compared to standard user-website interaction. Our findings contribute technical advancements to an ongoing paradigm shift of how we access information on the web.

**Link**: [arxiv](http://arxiv.org/abs/2504.03343v1),  [pdf](http://arxiv.org/pdf/2504.03343v1)

**Tags**: cs.AI cs.HC cs.IR 



### LLMs Prompted for Graphs: Hallucinations and Generative Capabilities
**Authors**: Gurvan Richardeau, Samy Chali, Erwan Le Merrer, Camilla Penzo, Gilles Tredan

**Updated**: 2025-04-04T10:58:40Z

**Summary**: Large Language Models (LLMs) are nowadays prompted for a wide variety of tasks. In this article, we investigate their ability in reciting and generating graphs. We first study the ability of LLMs to regurgitate well known graphs from the literature (e.g. Karate club or the graph atlas)4. Secondly, we question the generative capabilities of LLMs by asking for Erdos-Renyi random graphs. As opposed to the possibility that they could memorize some Erdos-Renyi graphs included in their scraped training set, this second investigation aims at studying a possible emergent property of LLMs. For both tasks, we propose a metric to assess their errors with the lens of hallucination (i.e. incorrect information returned as facts). We most notably find that the amplitude of graph hallucinations can characterize the superiority of some LLMs. Indeed, for the recitation task, we observe that graph hallucinations correlate with the Hallucination Leaderboard, a hallucination rank that leverages 10, 000 times more prompts to obtain its ranking. For the generation task, we find surprisingly good and reproducible results in most of LLMs. We believe this to constitute a starting point for more in-depth studies of this emergent capability and a challenging benchmark for their improvements. Altogether, these two aspects of LLMs capabilities bridge a gap between the network science and machine learning communities.

**Link**: [arxiv](http://arxiv.org/abs/2409.00159v3),  [pdf](http://arxiv.org/pdf/2409.00159v3)

**Tags**: cs.CL cs.AI cs.SI 



### BabyLM's First Words: Word Segmentation as a Phonological Probing Task
**Authors**: Zébulon Goriely

**Updated**: 2025-04-04T10:42:56Z

**Summary**: Language models provide a key framework for studying linguistic theories based on prediction, but phonological analysis using large language models (LLMs) is difficult; there are few phonological benchmarks beyond English and the standard input representation used in LLMs (subwords of graphemes) is not suitable for analyzing the representation of phonemes. In this work, we demonstrate how word segmentation can be used as a phonological probing task, allowing us to study the representations learned by phoneme-based language models trained on child-directed speech across 31 languages. Following computational models of word segmentation, we present unsupervised methods for extracting word boundaries from a trained model using the observation that prediction-error peaks at the start of words. We also use linear probes to identify that these models implicitly track word boundaries, even when they do not appear in training. This cross-lingual work corroborates statistical learning theories of acquisition and empirically motivates new methods for training subword tokenizers.

**Link**: [arxiv](http://arxiv.org/abs/2504.03338v1),  [pdf](http://arxiv.org/pdf/2504.03338v1)

**Tags**: cs.CL 



### Evolution 6.0: Evolving Robotic Capabilities Through Generative Design
**Authors**: Muhammad Haris Khan, Artyom Myshlyaev, Artem Lykov, Miguel Altamirano Cabrera, Dzmitry Tsetserukou

**Updated**: 2025-04-04T10:42:39Z

**Summary**: We propose a new concept, Evolution 6.0, which represents the evolution of robotics driven by Generative AI. When a robot lacks the necessary tools to accomplish a task requested by a human, it autonomously designs the required instruments and learns how to use them to achieve the goal. Evolution 6.0 is an autonomous robotic system powered by Vision-Language Models (VLMs), Vision-Language Action (VLA) models, and Text-to-3D generative models for tool design and task execution. The system comprises two key modules: the Tool Generation Module, which fabricates task-specific tools from visual and textual data, and the Action Generation Module, which converts natural language instructions into robotic actions. It integrates QwenVLM for environmental understanding, OpenVLA for task execution, and Llama-Mesh for 3D tool generation. Evaluation results demonstrate a 90% success rate for tool generation with a 10-second inference time, and action generation achieving 83.5% in physical and visual generalization, 70% in motion generalization, and 37% in semantic generalization. Future improvements will focus on bimanual manipulation, expanded task capabilities, and enhanced environmental interpretation to improve real-world adaptability.

**Link**: [arxiv](http://arxiv.org/abs/2502.17034v4),  [pdf](http://arxiv.org/pdf/2502.17034v4)

**Tags**: cs.RO cs.NE 



### QIRL: Boosting Visual Question Answering via Optimized Question-Image   Relation Learning
**Authors**: Quanxing Xu, Ling Zhou, Xian Zhong, Feifei Zhang, Rubing Huang, Chia-Wen Lin

**Updated**: 2025-04-04T10:38:28Z

**Summary**: Existing debiasing approaches in Visual Question Answering (VQA) primarily focus on enhancing visual learning, integrating auxiliary models, or employing data augmentation strategies. However, these methods exhibit two major drawbacks. First, current debiasing techniques fail to capture the superior relation between images and texts because prevalent learning frameworks do not enable models to extract deeper correlations from highly contrasting samples. Second, they do not assess the relevance between the input question and image during inference, as no prior work has examined the degree of input relevance in debiasing studies. Motivated by these limitations, we propose a novel framework, Optimized Question-Image Relation Learning (QIRL), which employs a generation-based self-supervised learning strategy. Specifically, two modules are introduced to address the aforementioned issues. The Negative Image Generation (NIG) module automatically produces highly irrelevant question-image pairs during training to enhance correlation learning, while the Irrelevant Sample Identification (ISI) module improves model robustness by detecting and filtering irrelevant inputs, thereby reducing prediction errors. Furthermore, to validate our concept of reducing output errors through filtering unrelated question-image inputs, we propose a specialized metric to evaluate the performance of the ISI module. Notably, our approach is model-agnostic and can be integrated with various VQA models. Extensive experiments on VQA-CPv2 and VQA-v2 demonstrate the effectiveness and generalization ability of our method. Among data augmentation strategies, our approach achieves state-of-the-art results.

**Link**: [arxiv](http://arxiv.org/abs/2504.03337v1),  [pdf](http://arxiv.org/pdf/2504.03337v1)

**Tags**: cs.CV 



### Minority-Focused Text-to-Image Generation via Prompt Optimization
**Authors**: Soobin Um, Jong Chul Ye

**Updated**: 2025-04-04T10:37:36Z

**Summary**: We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of text-conditional data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for high-quality generation. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that encourages emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes generation of minority features by incorporating a carefully-crafted likelihood objective. Extensive experiments conducted across various types of T2I models demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers. Code is available at https://github.com/soobin-um/MinorityPrompt.

**Link**: [arxiv](http://arxiv.org/abs/2410.07838v3),  [pdf](http://arxiv.org/pdf/2410.07838v3)

**Tags**: cs.CV cs.AI cs.LG 



### NucleiML: A machine learning framework of ground-state properties of   finite nuclei for accelerated Bayesian exploration
**Authors**: Anagh Venneti, Chiranjib Mondal, Sk Md Adil Imam, Sarmistha Banik, Bijay K. Agrawal

**Updated**: 2025-04-04T10:29:06Z

**Summary**: The global behavior of the nuclear equation of state (EoS) is usually investigated using finite nuclei (FN) data, along with constraints from heavy-ion collisions and astrophysical observations of neutron star (NS) properties. The FN constraints explicitly imposed through the binding energies and charge radii of selected nuclei are found to significantly affect the EoS across different densities. However, the high computational cost of these constraints makes it challenging to extend the analysis to a broader set of nuclei, particularly when the objective is not merely to obtain a single optimized model but to systematically explore uncertainties in global modeling. To overcome this challenge, we introduce NucleiML (NML), a machine learning framework trained on ground-state FN properties obtained from mean-field models. Integrated into a Bayesian inference approach, NML demonstrates high accuracy and strong consistency with the underlying mean-field model. The NML achieves around ten-fold computational speed-up, from $\sim 4.5$ hours to 30 minutes. Its predictive performance improves further as the number of nuclei in the training data increases, which we plan to employ in extensive future explorations.

**Link**: [arxiv](http://arxiv.org/abs/2504.03333v1),  [pdf](http://arxiv.org/pdf/2504.03333v1)

**Tags**: nucl-th astro-ph.HE 



### The Galactic-Centre Arms inferred from ACES (ALMA CMZ Exploration   Survey)
**Authors**: Y. Sofue, Tomo. Oka, S. N. Longmore, D. Walker, A. Ginsburg, J. D. Henshaw, J. Bally, A. T. Barne, C. Battersby, L. Colzi, P. Ho, I. Jimenez-Serra, J. M. D. Kruijssen, E. Mills, M. A. Petkova, M. C. Sormani, J. Wallace, J. Armijos-Abendano, K. M. Dutkowska, R. Enokiya, Y. Fukui, P. Garcia, A. Guzman, C. Henkel, P. -Y. Hsieh, Y. Hu, K. Immer, D. Jeff, R. S. Klessen, K. Kohno, M. R. Krumholz, D. Lipman, S. Martin, M. R. Morris, F. Nogueras-Lara, M. Nonhebel, J. Otto, J. E. Pineda, M. A. Requena-Torres, V. M. Rivilla, D. Riquelme-Vasquez, A. Sanchez-Monge, M. G. Santa-Maria, H. A. Smith, T. S. Tanvir, V. Tolls, Q. D. Wang

**Updated**: 2025-04-04T10:23:22Z

**Summary**: Analyzing longitude-velocity diagrams (LVDs) in the CS(J=2-1) and H13CN(J=1-0) molecular lines from the internal release data of the ALMA Central-Molecular-Zone Exploration Survey (ACES) and in the 13CO (J=1-0) line from the Nobeyama Galactic-Centre (GC) survey, we identify six GC Arms as prominent straight LV ridges. In addition to the currently known Arms I to IV, we identify a new inner arm, Arm V, and further highlight the circum-nuclear disc (CND) as Arm VI. Integrated intensity maps of the Arms on the sky suggest that most of the Arms compose ring-like structures inclined from the Galactic plane. We determine the radii (curvatures) of the Arms using the velocity-gradient ($dv/dl$) method, assuming that the arms are rotating on circular orbits at a constant velocity of $\sim 150$ km/s. We show that Arms I and II compose the main ring structure of the CMZ with radii $\sim 100$--120 pc; Arm III is a dense arm 42 pc from the GC; Arm IV is a clear and narrow arm 20 pc from the GC; and Arm V is a faint, long arm of 8.2 pc radius. We show that the circum-nuclear disc (CND) composes the sixth arm, Arm VI, of radius $\sim 2.3$ pc associated with bifurcated spiral fins. We also discuss the association of the 20- and 50-km/s clouds with these Arms. The radii of the arms fall on an empirical relation $R\sim 630 (2/5)^N$ for $N=1$ (Arm I) to 6 (VI), suggesting either discrete rings or a logarithmic spiral with pitch angle $\sim 22^\circ$. The vertical full extent of the arm increases with radius and is represented by $z\sim 0.7 (R/1 {\rm pc})^{0.7}$ pc. The tilt angle of the arms from the Galactic plane, or the warping, increases rapidly toward the GC.

**Link**: [arxiv](http://arxiv.org/abs/2504.03331v1),  [pdf](http://arxiv.org/pdf/2504.03331v1)

**Tags**: astro-ph.GA 



### Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User   Devices
**Authors**: Luís Couto Seller, Íñigo Sanz Torres, Adrián Vogel-Fernández, Carlos González Carballo, Pedro Miguel Sánchez Sánchez, Adrián Carruana Martín, Enrique de Miguel Ambite

**Updated**: 2025-04-04T09:47:58Z

**Summary**: Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance

**Link**: [arxiv](http://arxiv.org/abs/2504.03312v1),  [pdf](http://arxiv.org/pdf/2504.03312v1)

**Tags**: cs.CL cs.LG 



### Noise Augmented Fine Tuning for Mitigating Hallucinations in Large   Language Models
**Authors**: Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani

**Updated**: 2025-04-04T09:27:19Z

**Summary**: Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2504.03302v1),  [pdf](http://arxiv.org/pdf/2504.03302v1)

**Tags**: cs.CL cs.AI 



### Stance-Driven Multimodal Controlled Statement Generation: New Dataset   and Task
**Authors**: Bingqian Wang, Quan Fang, Jiachen Sun, Xiaoxiao Ma

**Updated**: 2025-04-04T09:20:19Z

**Summary**: Formulating statements that support diverse or controversial stances on specific topics is vital for platforms that enable user expression, reshape political discourse, and drive social critique and information dissemination. With the rise of Large Language Models (LLMs), controllable text generation towards specific stances has become a promising research area with applications in shaping public opinion and commercial marketing. However, current datasets often focus solely on pure texts, lacking multimodal content and effective context, particularly in the context of stance detection. In this paper, we formally define and study the new problem of stance-driven controllable content generation for tweets with text and images, where given a multimodal post (text and image/video), a model generates a stance-controlled response. To this end, we create the Multimodal Stance Generation Dataset (StanceGen2024), the first resource explicitly designed for multimodal stance-controllable text generation in political discourse. It includes posts and user comments from the 2024 U.S. presidential election, featuring text, images, videos, and stance annotations to explore how multimodal political content shapes stance expression. Furthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework that integrates weighted fusion of multimodal features and stance guidance to improve semantic consistency and stance control. We release the dataset and code (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and further research.

**Link**: [arxiv](http://arxiv.org/abs/2504.03295v1),  [pdf](http://arxiv.org/pdf/2504.03295v1)

**Tags**: cs.CL cs.AI 



### Do Large Language Models Solve the Problems of Agent-Based Modeling? A   Critical Review of Generative Social Simulations
**Authors**: Maik Larooij, Petter Törnberg

**Updated**: 2025-04-04T08:48:43Z

**Summary**: Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as the integration of Large Language Models (LLMs) has led to the emergence of ``generative ABMs'' as a novel approach to simulating social systems. While ABMs offer means to bridge micro-level interactions with macro-level patterns, they have long faced criticisms from social scientists, pointing to e.g., lack of realism, computational complexity, and challenges of calibrating and validating against empirical data. This paper reviews the generative ABM literature to assess how this new approach adequately addresses these long-standing criticisms. Our findings show that studies show limited awareness of historical debates. Validation remains poorly addressed, with many studies relying solely on subjective assessments of model `believability', and even the most rigorous validation failing to adequately evidence operational validity. We argue that there are reasons to believe that LLMs will exacerbate rather than resolve the long-standing challenges of ABMs. The black-box nature of LLMs moreover limit their usefulness for disentangling complex emergent causal mechanisms. While generative ABMs are still in a stage of early experimentation, these findings question of whether and how the field can transition to the type of rigorous modeling needed to contribute to social scientific theory.

**Link**: [arxiv](http://arxiv.org/abs/2504.03274v1),  [pdf](http://arxiv.org/pdf/2504.03274v1)

**Tags**: cs.MA cs.AI 



### Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant
**Authors**: Alan Dao, Dinh Bach Vu, Huy Hoang Ha

**Updated**: 2025-04-04T08:29:19Z

**Summary**: Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.

**Link**: [arxiv](http://arxiv.org/abs/2410.15316v3),  [pdf](http://arxiv.org/pdf/2410.15316v3)

**Tags**: cs.CL cs.SD eess.AS 



### TQD-Track: Temporal Query Denoising for 3D Multi-Object Tracking
**Authors**: Shuxiao Ding, Yutong Yang, Julian Wiederer, Markus Braun, Peizheng Li, Juergen Gall, Bin Yang

**Updated**: 2025-04-04T08:18:48Z

**Summary**: Query denoising has become a standard training strategy for DETR-based detectors by addressing the slow convergence issue. Besides that, query denoising can be used to increase the diversity of training samples for modeling complex scenarios which is critical for Multi-Object Tracking (MOT), showing its potential in MOT application. Existing approaches integrate query denoising within the tracking-by-attention paradigm. However, as the denoising process only happens within the single frame, it cannot benefit the tracker to learn temporal-related information. In addition, the attention mask in query denoising prevents information exchange between denoising and object queries, limiting its potential in improving association using self-attention. To address these issues, we propose TQD-Track, which introduces Temporal Query Denoising (TQD) tailored for MOT, enabling denoising queries to carry temporal information and instance-specific feature representation. We introduce diverse noise types onto denoising queries that simulate real-world challenges in MOT. We analyze our proposed TQD for different tracking paradigms, and find out the paradigm with explicit learned data association module, e.g. tracking-by-detection or alternating detection and association, benefit from TQD by a larger margin. For these paradigms, we further design an association mask in the association module to ensure the consistent interaction between track and detection queries as during inference. Extensive experiments on the nuScenes dataset demonstrate that our approach consistently enhances different tracking methods by only changing the training process, especially the paradigms with explicit association module.

**Link**: [arxiv](http://arxiv.org/abs/2504.03258v1),  [pdf](http://arxiv.org/pdf/2504.03258v1)

**Tags**: cs.CV 



### FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from   Few Images
**Authors**: Rong Wang, Fabian Prada, Ziyan Wang, Zhongshi Jiang, Chengxiang Yin, Junxuan Li, Shunsuke Saito, Igor Santesteban, Javier Romero, Rohan Joshi, Hongdong Li, Jason Saragih, Yaser Sheikh

**Updated**: 2025-04-04T08:17:08Z

**Summary**: We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA.

**Link**: [arxiv](http://arxiv.org/abs/2503.19207v2),  [pdf](http://arxiv.org/pdf/2503.19207v2)

**Tags**: cs.CV 



### Inherent and emergent liability issues in LLM-based agentic systems: a   principal-agent perspective
**Authors**: Garry A. Gabison, R. Patrick Xian

**Updated**: 2025-04-04T08:10:02Z

**Summary**: Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention over effective governance policies, monitoring and control protocols. Based on emerging landscapes of the agentic market, we analyze the potential liability issues stemming from delegated use of LLM agents and their extended systems from a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing and monitoring approaches to enhancing transparency and accountability.

**Link**: [arxiv](http://arxiv.org/abs/2504.03255v1),  [pdf](http://arxiv.org/pdf/2504.03255v1)

**Tags**: cs.CY cs.CL cs.MA 



### Retrieving Semantics from the Deep: an RAG Solution for Gesture   Synthesis
**Authors**: M. Hamza Mughal, Rishabh Dabral, Merel C. J. Scholman, Vera Demberg, Christian Theobalt

**Updated**: 2025-04-04T07:48:19Z

**Summary**: Non-verbal communication often comprises of semantically rich gestures that help convey the meaning of an utterance. Producing such semantic co-speech gestures has been a major challenge for the existing neural systems that can generate rhythmic beat gestures, but struggle to produce semantically meaningful gestures. Therefore, we present RAG-Gesture, a diffusion-based gesture generation approach that leverages Retrieval Augmented Generation (RAG) to produce natural-looking and semantically rich gestures. Our neuro-explicit gesture generation approach is designed to produce semantic gestures grounded in interpretable linguistic knowledge. We achieve this by using explicit domain knowledge to retrieve exemplar motions from a database of co-speech gestures. Once retrieved, we then inject these semantic exemplar gestures into our diffusion-based gesture generation pipeline using DDIM inversion and retrieval guidance at the inference time without any need of training. Further, we propose a control paradigm for guidance, that allows the users to modulate the amount of influence each retrieval insertion has over the generated sequence. Our comparative evaluations demonstrate the validity of our approach against recent gesture generation approaches. The reader is urged to explore the results on our project page.

**Link**: [arxiv](http://arxiv.org/abs/2412.06786v3),  [pdf](http://arxiv.org/pdf/2412.06786v3)

**Tags**: cs.CV 



### Why do LLMs attend to the first token?
**Authors**: Federico Barbero, Álvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veličković, Razvan Pascanu

**Updated**: 2025-04-04T07:41:19Z

**Summary**: Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.

**Link**: [arxiv](http://arxiv.org/abs/2504.02732v2),  [pdf](http://arxiv.org/pdf/2504.02732v2)

**Tags**: cs.CL 



### VLIPP: Towards Physically Plausible Video Generation with Vision and   Language Informed Physical Prior
**Authors**: Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, Xu Jia

**Updated**: 2025-04-04T07:23:21Z

**Summary**: Video diffusion models (VDMs) have advanced significantly in recent years, enabling the generation of highly realistic videos and drawing the attention of the community in their potential as world simulators. However, despite their capabilities, VDMs often fail to produce physically plausible videos due to an inherent lack of understanding of physics, resulting in incorrect dynamics and event sequences. To address this limitation, we propose a novel two-stage image-to-video generation framework that explicitly incorporates physics with vision and language informed physical prior. In the first stage, we employ a Vision Language Model (VLM) as a coarse-grained motion planner, integrating chain-of-thought and physics-aware reasoning to predict a rough motion trajectories/changes that approximate real-world physical dynamics while ensuring the inter-frame consistency. In the second stage, we use the predicted motion trajectories/changes to guide the video generation of a VDM. As the predicted motion trajectories/changes are rough, noise is added during inference to provide freedom to the VDM in generating motion with more fine details. Extensive experimental results demonstrate that our framework can produce physically plausible motion, and comparative evaluations highlight the notable superiority of our approach over existing methods. More video results are available on our Project Page: https://madaoer.github.io/projects/physically_plausible_video_generation.

**Link**: [arxiv](http://arxiv.org/abs/2503.23368v3),  [pdf](http://arxiv.org/pdf/2503.23368v3)

**Tags**: cs.CV cs.AI 



### Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted   Decision-Making
**Authors**: Zelun Tony Zhang, Leon Reicherts

**Updated**: 2025-04-04T06:40:03Z

**Summary**: How can we use generative AI to design tools that augment rather than replace human cognition? In this position paper, we review our own research on AI-assisted decision-making for lessons to learn. We observe that in both AI-assisted decision-making and generative AI, a popular approach is to suggest AI-generated end-to-end solutions to users, which users can then accept, reject, or edit. Alternatively, AI tools could offer more incremental support to help users solve tasks themselves, which we call process-oriented support. We describe findings on the challenges of end-to-end solutions, and how process-oriented support can address them. We also discuss the applicability of these findings to generative AI based on a recent study in which we compared both approaches to assist users in a complex decision-making task with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.03207v1),  [pdf](http://arxiv.org/pdf/2504.03207v1)

**Tags**: cs.HC cs.AI 



### Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward
**Authors**: Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques

**Updated**: 2025-04-04T06:35:02Z

**Summary**: Effective conversational agents must be able to personalize their behavior to suit a user's preferences, personality, and attributes, whether they are assisting with writing tasks or operating in domains like education or healthcare. Current training methods like Reinforcement Learning from Human Feedback (RLHF) prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized interactions. Traditional approaches to personalization often rely on extensive user history, limiting their effectiveness for new or context-limited users. To overcome these limitations, we propose to incorporate an intrinsic motivation to improve the conversational agents's model of the user as an additional reward alongside multi-turn RLHF. This reward mechanism encourages the agent to actively elicit user traits by optimizing conversations to increase the accuracy of its user model. Consequently, the policy agent can deliver more personalized interactions through obtaining more information about the user. We applied our method both education and fitness settings, where LLMs teach concepts or recommend personalized strategies based on users' hidden learning style or lifestyle attributes. Using LLM-simulated users, our approach outperformed a multi-turn RLHF baseline in revealing information about the users' preferences, and adapting to them.

**Link**: [arxiv](http://arxiv.org/abs/2504.03206v1),  [pdf](http://arxiv.org/pdf/2504.03206v1)

**Tags**: cs.CL cs.AI 



### LAM: Large Avatar Model for One-shot Animatable Gaussian Head
**Authors**: Yisheng He, Xiaodong Gu, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo

**Updated**: 2025-04-04T06:30:27Z

**Summary**: We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. Our experimental results demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks. Our code and video are available at https://aigc3d.github.io/projects/LAM/

**Link**: [arxiv](http://arxiv.org/abs/2502.17796v2),  [pdf](http://arxiv.org/pdf/2502.17796v2)

**Tags**: cs.CV 



### Natural Multimodal Fusion-Based Human-Robot Interaction: Application   With Voice and Deictic Posture via Large Language Model
**Authors**: Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Atmaraaj Gopal, Arihiro Yorita, Naoyuki Kubota, Matthias Rätsch

**Updated**: 2025-04-04T06:14:36Z

**Summary**: Translating human intent into robot commands is crucial for the future of service robots in an aging society. Existing Human-Robot Interaction (HRI) systems relying on gestures or verbal commands are impractical for the elderly due to difficulties with complex syntax or sign language. To address the challenge, this paper introduces a multi-modal interaction framework that combines voice and deictic posture information to create a more natural HRI system. The visual cues are first processed by the object detection model to gain a global understanding of the environment, and then bounding boxes are estimated based on depth information. By using a large language model (LLM) with voice-to-text commands and temporally aligned selected bounding boxes, robot action sequences can be generated, while key control syntax constraints are applied to avoid potential LLM hallucination issues. The system is evaluated on real-world tasks with varying levels of complexity using a Universal Robots UR3e manipulator. Our method demonstrates significantly better performance in HRI in terms of accuracy and robustness. To benefit the research community and the general public, we will make our code and design open-source.

**Link**: [arxiv](http://arxiv.org/abs/2501.00785v3),  [pdf](http://arxiv.org/pdf/2501.00785v3)

**Tags**: cs.RO 



### A Large-Scale Simulation on Large Language Models for Decision-Making in   Political Science
**Authors**: Chenxiao Yu, Jinyi Ye, Yuangang Li, Zhaotian Weng, Zheng Li, Emilio Ferrara, Xiyang Hu, Yue Zhao

**Updated**: 2025-04-04T06:11:55Z

**Summary**: While LLMs have demonstrated remarkable capabilities in text generation and reasoning, their ability to simulate human decision-making -- particularly in political contexts -- remains an open question. However, modeling voter behavior presents unique challenges due to limited voter-level data, evolving political landscapes, and the complexity of human reasoning. In this study, we develop a theory-driven, multi-step reasoning framework that integrates demographic, temporal and ideological factors to simulate voter decision-making at scale. Using synthetic personas calibrated to real-world voter data, we conduct large-scale simulations of recent U.S. presidential elections. Our method significantly improves simulation accuracy while mitigating model biases. We examine its robustness by comparing performance across different LLMs. We further investigate the challenges and constraints that arise from LLM-based political simulations. Our work provides both a scalable framework for modeling political decision-making behavior and insights into the promise and limitations of using LLMs in political science research.

**Link**: [arxiv](http://arxiv.org/abs/2412.15291v3),  [pdf](http://arxiv.org/pdf/2412.15291v3)

**Tags**: cs.CL cs.SI 



### FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG   Distillation
**Authors**: Tianyun Zhong, Chao Liang, Jianwen Jiang, Gaojie Lin, Jiaqi Yang, Zhou Zhao

**Updated**: 2025-04-04T06:07:56Z

**Summary**: Diffusion-based audio-driven talking avatar methods have recently gained attention for their high-fidelity, vivid, and expressive results. However, their slow inference speed limits practical applications. Despite the development of various distillation techniques for diffusion models, we found that naive diffusion distillation methods do not yield satisfactory results. Distilled models exhibit reduced robustness with open-set input images and a decreased correlation between audio and video compared to teacher models, undermining the advantages of diffusion models. To address this, we propose FADA (Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation). We first designed a mixed-supervised loss to leverage data of varying quality and enhance the overall model capability as well as robustness. Additionally, we propose a multi-CFG distillation with learnable tokens to utilize the correlation between audio and reference image conditions, reducing the threefold inference runs caused by multi-CFG with acceptable quality degradation. Extensive experiments across multiple datasets show that FADA generates vivid videos comparable to recent diffusion model-based methods while achieving an NFE speedup of 4.17-12.5 times. Demos are available at our webpage http://fadavatar.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2412.16915v2),  [pdf](http://arxiv.org/pdf/2412.16915v2)

**Tags**: cs.CV cs.AI cs.GR cs.SD eess.AS 



### Explain with Visual Keypoints Like a Real Mentor! A Benchmark for   Multimodal Solution Explanation
**Authors**: Jaewoo Park, Jungyang Park, Dongju Jang, Jiwan Chung, Byungwoo Yoo, Jaewoo Shin, Seonjoon Park, Taehyeong Kim, Youngjae Yu

**Updated**: 2025-04-04T06:03:13Z

**Summary**: With the rapid advancement of mathematical reasoning capabilities in large language models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: visual explanation. In real-world instructional contexts, human tutors routinely employ visual aids-such as diagrams, markings, and highlights-to enhance conceptual clarity. To bridge this gap, we introduce a novel task of visual solution explanation, which requires not only solving problems but also generating explanations that incorporate newly introduced visual elements essential for understanding (e.g., auxiliary lines, annotations, or geometric constructions). To evaluate model performance on this task, we propose MathExplain, a multimodal benchmark consisting of 997 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that while some closed-source models demonstrate promising capabilities on visual solution-explaining, current open-source general-purpose models perform inconsistently, particularly in identifying relevant visual components and producing coherent keypoint-based explanations. We expect that visual solution-explaining and the MathExplain dataset will catalyze further research on multimodal LLMs in education and advance their deployment as effective, explanation-oriented AI tutors. Code and data will be released publicly.

**Link**: [arxiv](http://arxiv.org/abs/2504.03197v1),  [pdf](http://arxiv.org/pdf/2504.03197v1)

**Tags**: cs.CL 



### PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for   Text-to-Image Models
**Authors**: Lingzhi Yuan, Xiaojun Jia, Yihao Huang, Wei Dong, Yang Liu

**Updated**: 2025-04-04T05:56:04Z

**Summary**: Text-to-image (T2I) models have been shown to be vulnerable to misuse, particularly in generating not-safe-for-work (NSFW) content, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. Extensive experiments across three datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 7.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.

**Link**: [arxiv](http://arxiv.org/abs/2501.03544v2),  [pdf](http://arxiv.org/pdf/2501.03544v2)

**Tags**: cs.CV cs.AI cs.CR 



### Semi-empirical versus Theoretical Stellar Population Models: a   comparison with Star Clusters
**Authors**: Randa Asad, Paula R. T. Coelho, Johina M. John, Igor Chilingarian, Gustavo Bruzual, Stephane Charlot

**Updated**: 2025-04-04T05:50:24Z

**Summary**: Stellar population synthesis (SPS) models are a key tool for deriving the age, metallicity, radial velocity and reddening of star clusters from their integrated spectra. Using a sample of 129 star clusters with high-quality spectra, we analyze the uncertainties associated with selecting an empirical versus a theoretical stellar spectral library in the SPS models. We find that the fits from the different models agree on the goodness of fit metrics and inferred reddening. However, the derived age and metallicity can be affected by the choice of the stellar library, with synthetic libraries tending to give lower age and metallicity, especially for spectra with low SNR. Ages and reddening values from SSP-equivalent fits are consistent with the multi-population fits, however, SSP-equivalent metallicities are affected by the coarse coverage of the SPS grid in [Fe/H]. When comparing the spectral fitting results with the literature, we find that (1) all models underestimate age for old and metal-poor systems; (2) on average, SPS models based on synthetic stellar libraries better match the isochrone ages and metallicities from high-resolution stellar spectroscopy.

**Link**: [arxiv](http://arxiv.org/abs/2504.03194v1),  [pdf](http://arxiv.org/pdf/2504.03194v1)

**Tags**: astro-ph.SR astro-ph.GA 



### How Do Teachers Create Pedagogical Chatbots?: Current Practices and   Challenges
**Authors**: Minju Yoo, Hyoungwook Jin, Juho Kim

**Updated**: 2025-04-04T05:49:11Z

**Summary**: AI chatbots have emerged as promising educational tools for personalized learning experiences, with advances in large language models (LLMs) enabling teachers to create and customize these chatbots for their specific classroom needs. However, there is a limited understanding of how teachers create pedagogical chatbots and integrate them into their lessons. Through semi-structured interviews with seven K-12 teachers, we examined their practices and challenges when designing, implementing, and deploying chatbots. Our findings revealed that teachers prioritize developing task-specific chatbots aligned with their lessons. Teachers engaged in various creation practices and had different challenges; novices in chatbot creation struggled mainly with initial design and technical implementation, while experienced teachers faced challenges with technical aspects and analyzing conversational data. Based on these insights, we explore approaches to supporting teachers' chatbot development and opportunities for designing future chatbot creation systems. This work provides foundational insights from teachers that can empower teacher-created chatbots, facilitating AI-augmented teaching.

**Link**: [arxiv](http://arxiv.org/abs/2503.00967v2),  [pdf](http://arxiv.org/pdf/2503.00967v2)

**Tags**: cs.HC 



### An ADMM Algorithm for Structure Learning in Equilibrium Networks
**Authors**: Rohith Reddy Mada, Rajasekhar Anguluri

**Updated**: 2025-04-04T05:33:23Z

**Summary**: Learning the edge connectivity structure of networked systems from limited data is a fundamental challenge in many critical infrastructure domains, including power, traffic, and finance. Such systems obey steady-state conservation laws: $x(t) = Ly(t)$, where $x(t) \in \mathbb{R}^p$ and $y(t) \in \mathbb{R}^p$ represent injected flows (inputs) and potentials (outputs), respectively. The sparsity pattern of the $p\times p$ Laplacian L encodes the underlying edge structure. In a stochastic setting, the goal is to infer this sparsity pattern from zero-mean i.i.d. samples of y(t).   Recent work by Rayas et al. [1] has established statistical consistency results for this learning problem by considering an $\ell_1$-regularized maximum likelihood estimator. However, their approach did not focus on developing a scalable algorithm but relies on solving a convex program via the CVX package in Python. To address this gap, we propose an alternating direction method of multipliers (ADMM) algorithm. Our approach is simple, transparent, and computationally fast. A key contribution is demonstrating the role of a non-symmetric algebraic Riccati equation in the primal step of ADMM. Numerical experiments on a host of synthetic and benchmark networks, including power and water systems, show that our method achieves high recovery accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.03189v1),  [pdf](http://arxiv.org/pdf/2504.03189v1)

**Tags**: math.OC 



### Learning Natural Language Constraints for Safe Reinforcement Learning of   Language Agents
**Authors**: Jaymari Chua, Chen Wang, Lina Yao

**Updated**: 2025-04-04T05:26:28Z

**Summary**: Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings.

**Link**: [arxiv](http://arxiv.org/abs/2504.03185v1),  [pdf](http://arxiv.org/pdf/2504.03185v1)

**Tags**: cs.CL cs.AI I.2.7; I.2.4; I.2.6; I.2.8 



### Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion   Dependency
**Authors**: Jianwen Jiang, Chao Liang, Jiaqi Yang, Gaojie Lin, Tianyun Zhong, Yanbo Zheng

**Updated**: 2025-04-04T05:13:40Z

**Summary**: With the introduction of diffusion-based video generation techniques, audio-conditioned human video generation has recently achieved significant breakthroughs in both the naturalness of motion and the synthesis of portrait details. Due to the limited control of audio signals in driving human motion, existing methods often add auxiliary spatial signals to stabilize movements, which may compromise the naturalness and freedom of motion. In this paper, we propose an end-to-end audio-only conditioned video diffusion model named Loopy. Specifically, we designed an inter- and intra-clip temporal module and an audio-to-latents module, enabling the model to leverage long-term motion information from the data to learn natural motion patterns and improving audio-portrait movement correlation. This method removes the need for manually specified spatial motion templates used in existing methods to constrain motion during inference. Extensive experiments show that Loopy outperforms recent audio-driven portrait diffusion models, delivering more lifelike and high-quality results across various scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2409.02634v3),  [pdf](http://arxiv.org/pdf/2409.02634v3)

**Tags**: cs.CV 



### Multi-lingual Multi-turn Automated Red Teaming for LLMs
**Authors**: Abhishek Singhania, Christophe Dupuy, Shivam Mangale, Amani Namboori

**Updated**: 2025-04-04T05:06:12Z

**Summary**: Language Model Models (LLMs) have improved dramatically in the past few years, increasing their adoption and the scope of their capabilities over time. A significant amount of work is dedicated to ``model alignment'', i.e., preventing LLMs to generate unsafe responses when deployed into customer-facing applications. One popular method to evaluate safety risks is \textit{red-teaming}, where agents attempt to bypass alignment by crafting elaborate prompts that trigger unsafe responses from a model. Standard human-driven red-teaming is costly, time-consuming and rarely covers all the recent features (e.g., multi-lingual, multi-modal aspects), while proposed automation methods only cover a small subset of LLMs capabilities (i.e., English or single-turn). We present Multi-lingual Multi-turn Automated Red Teaming (\textbf{MM-ART}), a method to fully automate conversational, multi-lingual red-teaming operations and quickly identify prompts leading to unsafe responses. Through extensive experiments on different languages, we show the studied LLMs are on average 71\% more vulnerable after a 5-turn conversation in English than after the initial turn. For conversations in non-English languages, models display up to 195\% more safety vulnerabilities than the standard single-turn English approach, confirming the need for automated red-teaming methods matching LLMs capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2504.03174v1),  [pdf](http://arxiv.org/pdf/2504.03174v1)

**Tags**: cs.CL 



### Strategizing with AI: Insights from a Beauty Contest Experiment
**Authors**: Iuliia Alekseenko, Dmitry Dagaev, Sofia Paklina, Petr Parshakov

**Updated**: 2025-04-04T05:01:08Z

**Summary**: A beauty contest is a wide class of games of guessing the most popular strategy among other players. In particular, guessing a fraction of a mean of numbers chosen by all players is a classic behavioral experiment designed to test iterative reasoning patterns among various groups of people. The previous literature reveals that the level of sophistication of the opponents is an important factor affecting the outcome of the game. Smarter decision makers choose strategies that are closer to theoretical Nash equilibrium and demonstrate faster convergence to equilibrium in iterated contests with information revelation. We replicate a series of classic experiments by running virtual experiments with modern large language models (LLMs) who play against various groups of virtual players. We test how advanced the LLMs' behavior is compared to the behavior of human players. We show that LLMs typically take into account the opponents' level of sophistication and adapt by changing the strategy. In various settings, most LLMs (with the exception of Llama) are more sophisticated and play lower numbers compared to human players. Our results suggest that LLMs (except Llama) are rather successful in identifying the underlying strategic environment and adopting the strategies to the changing set of parameters of the game in the same way that human players do. All LLMs still fail to play dominant strategies in a two-player game. Our results contribute to the discussion on the accuracy of modeling human economic agents by artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2502.03158v2),  [pdf](http://arxiv.org/pdf/2502.03158v2)

**Tags**: econ.GN q-fin.EC 



### Efficient Dynamic Clustering-Based Document Compression for   Retrieval-Augmented-Generation
**Authors**: Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu

**Updated**: 2025-04-04T04:43:13Z

**Summary**: Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge integration during large language model (LLM) inference in recent years. However, current RAG implementations face challenges in effectively addressing noise, repetition and redundancy in retrieved content, primarily due to their limited ability to exploit fine-grained inter-document relationships. To address these limitations, we propose an \textbf{E}fficient \textbf{D}ynamic \textbf{C}lustering-based document \textbf{C}ompression framework (\textbf{EDC\textsuperscript{2}-RAG}) that effectively utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The results show that this method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets can be found at https://github.com/Tsinghua-dhy/EDC-2-RAG.

**Link**: [arxiv](http://arxiv.org/abs/2504.03165v1),  [pdf](http://arxiv.org/pdf/2504.03165v1)

**Tags**: cs.CL 



### Modified Tests of Linear Hypotheses Under Heteroscedasticity for   Multivariate Functional Data with Finite Sample Sizes
**Authors**: Tianming Zhu

**Updated**: 2025-04-04T04:42:36Z

**Summary**: As big data continues to grow, statistical inference for multivariate functional data (MFD) has become crucial. Although recent advancements have been made in testing the equality of mean functions, research on testing linear hypotheses for mean functions remains limited. Current methods primarily consist of permutation-based tests or asymptotic tests. However, permutation-based tests are known to be time-consuming, while asymptotic tests typically require larger sample sizes to maintain an accurate Type I error rate. This paper introduces three finite-sample tests that modify traditional MANOVA methods to tackle the general linear hypothesis testing problem for MFD. The test statistics rely on two symmetric, nonnegative-definite matrices, approximated by Wishart distributions, with degrees of freedom estimated via a U-statistics-based method. The proposed tests are affine-invariant, computationally more efficient than permutation-based tests, and better at controlling significance levels in small samples compared to asymptotic tests. A real-data example further showcases their practical utility.

**Link**: [arxiv](http://arxiv.org/abs/2504.03161v1),  [pdf](http://arxiv.org/pdf/2504.03161v1)

**Tags**: stat.ME 



### DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments
**Authors**: Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu

**Updated**: 2025-04-04T04:41:28Z

**Summary**: Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.

**Link**: [arxiv](http://arxiv.org/abs/2504.03160v1),  [pdf](http://arxiv.org/pdf/2504.03160v1)

**Tags**: cs.AI cs.CL cs.LG 



### sudo rm -rf agentic_security
**Authors**: Sejin Lee, Jian Kim, Haon Park, Ashkan Yousefpour, Sangyoon Yu, Min Song

**Updated**: 2025-04-04T04:36:20Z

**Summary**: Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs Our code is available at: https://github.com/AIM-Intelligence/SUDO.git

**Link**: [arxiv](http://arxiv.org/abs/2503.20279v2),  [pdf](http://arxiv.org/pdf/2503.20279v2)

**Tags**: cs.CL cs.AI cs.CR 



### Accelerating Particle-based Energetic Variational Inference
**Authors**: Xuelian Bao, Lulu Kang, Chun Liu, Yiwei Wang

**Updated**: 2025-04-04T04:31:19Z

**Summary**: In this work, we propose a novel particle-based variational inference (ParVI) method that accelerates the EVI-Im. Inspired by energy quadratization (EQ) and operator splitting techniques for gradient flows, our approach efficiently drives particles towards the target distribution. Unlike EVI-Im, which employs the implicit Euler method to solve variational-preserving particle dynamics for minimizing the KL divergence, derived using a "discretize-then-variational" approach, the proposed algorithm avoids repeated evaluation of inter-particle interaction terms, significantly reducing computational cost. The framework is also extensible to other gradient-based sampling techniques. Through several numerical experiments, we demonstrate that our method outperforms existing ParVI approaches in efficiency, robustness, and accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.03158v1),  [pdf](http://arxiv.org/pdf/2504.03158v1)

**Tags**: stat.ML cs.LG 62G05, 65K10, 65L05 



### AMP4EC: Adaptive Model Partitioning Framework for Efficient Deep   Learning Inference in Edge Computing Environments
**Authors**: Guilin Zhang, Wulan Guo, Ziqi Tan, Hailong Jiang

**Updated**: 2025-04-04T04:28:20Z

**Summary**: Edge computing facilitates deep learning in resource-constrained environments, but challenges such as resource heterogeneity and dynamic constraints persist. This paper introduces AMP4EC, an Adaptive Model Partitioning framework designed to optimize deep learning inference in edge environments through real-time resource monitoring, dynamic model partitioning, and adaptive task scheduling. AMP4EC features a resource-aware model partitioner that splits deep learning models based on device capabilities, a task scheduler that ensures efficient load balancing using a weighted scoring mechanism, and a Docker-based deployment environment for validation. Experimental results show up to a 78% reduction in latency and a 414% improvement in throughput compared to baseline methods. The framework achieves consistent performance with low scheduling overhead across varying resource profiles, demonstrating adaptability in high-resource (1 CPU, 1GB RAM) and low-resource (0.4 CPU, 512MB RAM) scenarios. These results highlight AMP4EC's scalability, efficiency, and robustness for real-world edge deployments, addressing the critical need for efficient distributed inference in dynamic, resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.00407v2),  [pdf](http://arxiv.org/pdf/2504.00407v2)

**Tags**: cs.DC I.2.11; I.2.6; C.2.4 



### Synthesizing Optimal Object Selection Predicates for Image Editing using   Lattices
**Authors**: Yang He, Xiaoyu Liu, Yuepeng Wang

**Updated**: 2025-04-04T04:25:14Z

**Summary**: Image editing is a common task across a wide range of domains, from personal use to professional applications. Despite advances in computer vision, current tools still demand significant manual effort for editing tasks that require repetitive operations on images with many objects. In this paper, we present a novel approach to automating the image editing process using program synthesis. We propose a new algorithm based on lattice structures to automatically synthesize object selection predicates for image editing from positive and negative examples. By leveraging the algebraic properties of lattices, our algorithm efficiently synthesizes an optimal object selection predicate among multiple correct solutions. We have implemented our technique and evaluated it on 100 tasks over 20 images. The evaluation result demonstrates our tool is effective and efficient, which outperforms state-of-the-art synthesizers and LLM-based approaches.

**Link**: [arxiv](http://arxiv.org/abs/2504.03155v1),  [pdf](http://arxiv.org/pdf/2504.03155v1)

**Tags**: cs.PL 



### TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference
**Authors**: Junshan Hu, Jialiang Mao, Zhikang Liu, Zhongpu Xia, Peng Jia, Xianpeng Lang

**Updated**: 2025-04-04T04:24:29Z

**Summary**: Conventional Vision-Language Models(VLMs) typically utilize a fixed number of vision tokens, regardless of task complexity. This one-size-fits-all strategy introduces notable inefficiencies: using excessive tokens leads to unnecessary computational overhead in simpler tasks, whereas insufficient tokens compromise fine-grained visual comprehension in more complex contexts. To overcome these limitations, we present TokenFLEX, an innovative and adaptable vision-language framework that encodes images into a variable number of tokens for efficient integration with a Large Language Model (LLM). Our approach is underpinned by two pivotal innovations. Firstly, we present a novel training paradigm that enhances performance across varying numbers of vision tokens by stochastically modulating token counts during training. Secondly, we design a lightweight vision token projector incorporating an adaptive pooling layer and SwiGLU, allowing for flexible downsampling of vision tokens and adaptive selection of features tailored to specific token counts. Comprehensive experiments reveal that TokenFLEX consistently outperforms its fixed-token counterparts, achieving notable performance gains across various token counts enhancements of 1.6%, 1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight vision-language benchmarks. These results underscore TokenFLEX's remarkable flexibility while maintaining high-performance vision-language understanding.

**Link**: [arxiv](http://arxiv.org/abs/2504.03154v1),  [pdf](http://arxiv.org/pdf/2504.03154v1)

**Tags**: cs.CV 



### New User Event Prediction Through the Lens of Causal Inference
**Authors**: Henry Shaowu Yuchi, Shixiang Zhu, Li Dong, Yigit M. Arisoy, Matthew C. Spencer

**Updated**: 2025-04-04T04:07:39Z

**Summary**: Modeling and analysis for event series generated by users of heterogeneous behavioral patterns are closely involved in our daily lives, including credit card fraud detection, online platform user recommendation, and social network analysis. The most commonly adopted approach to this task is to assign users to behavior-based categories and analyze each of them separately. However, this requires extensive data to fully understand the user behavior, presenting challenges in modeling newcomers without significant historical knowledge. In this work, we propose a novel discrete event prediction framework for new users with limited history, without needing to know the user's category. We treat the user event history as the "treatment" for future events and the user category as the key confounder. Thus, the prediction problem can be framed as counterfactual outcome estimation, where each event is re-weighted by its inverse propensity score. We demonstrate the improved performance of the proposed framework with a numerical simulation study and two real-world applications, including Netflix rating prediction and seller contact prediction for customer support at Amazon.

**Link**: [arxiv](http://arxiv.org/abs/2407.05625v3),  [pdf](http://arxiv.org/pdf/2407.05625v3)

**Tags**: stat.ME cs.LG 



### Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning   (v1)
**Authors**: Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, Jiarui Wu, Shu Yang, Daoan Zhang, Chen Chen, Lianggong Bruce Wen, Zhang Liu, Jiebo Luo, Chenliang Xu

**Updated**: 2025-04-04T04:04:56Z

**Summary**: Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.

**Link**: [arxiv](http://arxiv.org/abs/2504.03151v1),  [pdf](http://arxiv.org/pdf/2504.03151v1)

**Tags**: cs.CL cs.LG 



## Keyword: LLM Deployment 
 ### AdaCM$^2$: On Understanding Extremely Long-Term Video with Adaptive   Cross-Modality Memory Reduction
**Authors**: Yuanbin Man, Ying Huang, Chengming Zhang, Bingzhe Li, Wei Niu, Miao Yin

**Updated**: 2025-04-04T17:58:08Z

**Summary**: The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM$^2$, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM$^2$ achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%.

**Link**: [arxiv](http://arxiv.org/abs/2411.12593v3),  [pdf](http://arxiv.org/pdf/2411.12593v3)

**Tags**: cs.CV cs.AI 



### Do Larger Language Models Imply Better Reasoning? A Pretraining Scaling   Law for Reasoning
**Authors**: Xinyi Wang, Shawn Tan, Mingyu Jin, William Yang Wang, Rameswar Panda, Yikang Shen

**Updated**: 2025-04-04T17:57:22Z

**Summary**: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks requiring complex reasoning. However, the effects of scaling on their reasoning abilities remain insufficiently understood. In this paper, we introduce a synthetic multihop reasoning environment designed to closely replicate the structure and distribution of real-world large-scale knowledge graphs. Our reasoning task involves completing missing edges in the graph, which requires advanced multi-hop reasoning and mimics real-world reasoning scenarios. To evaluate this, we pretrain language models (LMs) from scratch solely on triples from the incomplete graph and assess their ability to infer the missing edges. Interestingly, we observe that overparameterization can impair reasoning performance due to excessive memorization. We investigate different factors that affect this U-shaped loss curve, including graph structure, model size, and training steps. To predict the optimal model size for a specific knowledge graph, we find an empirical scaling that linearly maps the knowledge graph search entropy to the optimal model size. This work provides new insights into the relationship between scaling and reasoning in LLMs, shedding light on possible ways to optimize their performance for reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.03635v1),  [pdf](http://arxiv.org/pdf/2504.03635v1)

**Tags**: cs.AI cs.CL 



### Align to Structure: Aligning Large Language Models with Structural   Information
**Authors**: Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang

**Updated**: 2025-04-04T17:40:04Z

**Summary**: Generating long, coherent text remains a challenge for large language models (LLMs), as they lack hierarchical planning and structured organization in discourse generation. We introduce Structural Alignment, a novel method that aligns LLMs with human-like discourse structures to enhance long-form text generation. By integrating linguistically grounded discourse frameworks into reinforcement learning, our approach guides models to produce coherent and well-organized outputs. We employ a dense reward scheme within a Proximal Policy Optimization framework, assigning fine-grained, token-level rewards based on the discourse distinctiveness relative to human writing. Two complementary reward models are evaluated: the first improves readability by scoring surface-level textual features to provide explicit structuring, while the second reinforces deeper coherence and rhetorical sophistication by analyzing global discourse patterns through hierarchical discourse motifs, outperforming both standard and RLHF-enhanced models in tasks such as essay generation and long-document summarization. All training data and code will be publicly shared at https://github.com/minnesotanlp/struct_align.

**Link**: [arxiv](http://arxiv.org/abs/2504.03622v1),  [pdf](http://arxiv.org/pdf/2504.03622v1)

**Tags**: cs.CL cs.AI cs.LG 



### VISTA-OCR: Towards generative and interactive end to end OCR models
**Authors**: Laziz Hamdi, Amine Tamasna, Pascal Boisson, Thierry Paquet

**Updated**: 2025-04-04T17:39:53Z

**Summary**: We introduce \textbf{VISTA-OCR} (Vision and Spatially-aware Text Analysis OCR), a lightweight architecture that unifies text detection and recognition within a single generative model. Unlike conventional methods that require separate branches with dedicated parameters for text recognition and detection, our approach leverages a Transformer decoder to sequentially generate text transcriptions and their spatial coordinates in a unified branch. Built on an encoder-decoder architecture, VISTA-OCR is progressively trained, starting with the visual feature extraction phase, followed by multitask learning with multimodal token generation. To address the increasing demand for versatile OCR systems capable of advanced tasks, such as content-based text localization \ref{content_based_localization}, we introduce new prompt-controllable OCR tasks during pre-training.To enhance the model's capabilities, we built a new dataset composed of real-world examples enriched with bounding box annotations and synthetic samples. Although recent Vision Large Language Models (VLLMs) can efficiently perform these tasks, their high computational cost remains a barrier for practical deployment. In contrast, our VISTA$_{\text{omni}}$ variant processes both handwritten and printed documents with only 150M parameters, interactively, by prompting. Extensive experiments on multiple datasets demonstrate that VISTA-OCR achieves better performance compared to state-of-the-art specialized models on standard OCR tasks while showing strong potential for more sophisticated OCR applications, addressing the growing need for interactive OCR systems. All code and annotations for VISTA-OCR will be made publicly available upon acceptance.

**Link**: [arxiv](http://arxiv.org/abs/2504.03621v1),  [pdf](http://arxiv.org/pdf/2504.03621v1)

**Tags**: cs.CV 



### Multilingual Retrieval-Augmented Generation for Knowledge-Intensive Task
**Authors**: Leonardo Ranaldi, Barry Haddow, Alexandra Birch

**Updated**: 2025-04-04T17:35:43Z

**Summary**: Retrieval-augmented generation (RAG) has become a cornerstone of contemporary NLP, enhancing large language models (LLMs) by allowing them to access richer factual contexts through in-context retrieval. While effective in monolingual settings, especially in English, its use in multilingual tasks remains unexplored. This paper investigates the effectiveness of RAG across multiple languages by proposing novel approaches for multilingual open-domain question-answering. We evaluate the performance of various multilingual RAG strategies, including question-translation (tRAG), which translates questions into English before retrieval, and Multilingual RAG (MultiRAG), where retrieval occurs directly across multiple languages. Our findings reveal that tRAG, while useful, suffers from limited coverage. In contrast, MultiRAG improves efficiency by enabling multilingual retrieval but introduces inconsistencies due to cross-lingual variations in the retrieved content. To address these issues, we propose Crosslingual RAG (CrossRAG), a method that translates retrieved documents into a common language (e.g., English) before generating the response. Our experiments show that CrossRAG significantly enhances performance on knowledge-intensive tasks, benefiting both high-resource and low-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.03616v1),  [pdf](http://arxiv.org/pdf/2504.03616v1)

**Tags**: cs.CL cs.AI 



### PAFFA: Premeditated Actions For Fast Agents
**Authors**: Shambhavi Krishna, Zheng Chen, Yuan Ling, Xiaojiang Huang, Yingjie Li, Fan Yang, Xiang Li

**Updated**: 2025-04-04T17:33:53Z

**Summary**: Modern AI assistants have made significant progress in natural language understanding and tool-use, with emerging efforts to interact with Web interfaces. However, current approaches that heavily rely on repeated LLM-driven HTML parsing are computationally expensive and error-prone, particularly when handling dynamic web interfaces and multi-step tasks. We introduce PAFFA (Premeditated Actions For Fast Agents), a method that makes LLMs faster and more accurate in completing tasks on the internet using a novel inference-time technique that requires no task-specific training. PAFFA constructs an 'Action Library', leveraging the parametric knowledge of the base LLM to pre-compute browser interaction patterns that generalize across tasks. By strategically re-using LLM inference across tasks - either via 'Dist-Map' for task-agnostic identification of key interactive web elements, or 'Unravel' for first-encounter, stateful exploration of novel tasks/sites) - PAFFA drastically reduces inference time tokens by 87% while maintaining robust performance (achieving 0.57 vs. 0.50 step accuracy compared to baseline). Further, Unravel's ability to update its action library based on explorations allows generalization and adaptation to unseen websites. In sum, this work exhibits that LLM reasoning sequences can generalize across prompts, offering a way to scale inference-time techniques for internet-scale data with sublinear token count.

**Link**: [arxiv](http://arxiv.org/abs/2412.07958v2),  [pdf](http://arxiv.org/pdf/2412.07958v2)

**Tags**: cs.AI 



### AIR: A Systematic Analysis of Annotations, Instructions, and Response   Pairs in Preference Dataset
**Authors**: Bingxiang He, Wenbin Zhang, Jiaxi Song, Cheng Qian, Zixuan Fu, Bowen Sun, Ning Ding, Haiwen Hong, Longtao Huang, Hui Xue, Ganqu Cui, Wanxiang Che, Zhiyuan Liu, Maosong Sun

**Updated**: 2025-04-04T17:33:07Z

**Summary**: Preference learning is critical for aligning large language models (LLMs) with human values, yet its success hinges on high-quality datasets comprising three core components: Preference \textbf{A}nnotations, \textbf{I}nstructions, and \textbf{R}esponse Pairs. Current approaches conflate these components, obscuring their individual impacts and hindering systematic optimization. In this work, we propose \textbf{AIR}, a component-wise analysis framework that systematically isolates and optimizes each component while evaluating their synergistic effects. Through rigorous experimentation, AIR reveals actionable principles: annotation simplicity (point-wise generative scoring), instruction inference stability (variance-based filtering across LLMs), and response pair quality (moderate margins + high absolute scores). When combined, these principles yield +5.3 average gains over baseline method, even with only 14k high-quality pairs. Our work shifts preference dataset design from ad hoc scaling to component-aware optimization, offering a blueprint for efficient, reproducible alignment.

**Link**: [arxiv](http://arxiv.org/abs/2504.03612v1),  [pdf](http://arxiv.org/pdf/2504.03612v1)

**Tags**: cs.CL 



### Towards deployment-centric multimodal AI beyond vision and language
**Authors**: Xianyuan Liu, Jiayang Zhang, Shuo Zhou, Thijs L. van der Plas, Avish Vijayaraghavan, Anastasiia Grishina, Mengdie Zhuang, Daniel Schofield, Christopher Tomlinson, Yuhan Wang, Ruizhe Li, Louisa van Zeeland, Sina Tabakhi, Cyndie Demeocq, Xiang Li, Arunav Das, Orlando Timmerman, Thomas Baldwin-McDonald, Jinge Wu, Peizhen Bai, Zahraa Al Sahili, Omnia Alwazzan, Thao N. Do, Mohammod N. I. Suvon, Angeline Wang, Lucia Cipolina-Kun, Luigi A. Moretti, Lucas Farndale, Nitisha Jain, Natalia Efremova, Yan Ge, Marta Varela, Hak-Keung Lam, Oya Celiktutan, Ben R. Evans, Alejandro Coca-Castro, Honghan Wu, Zahraa S. Abdallah, Chen Chen, Valentin Danchev, Nataliya Tkachenko, Lei Lu, Tingting Zhu, Gregory G. Slabaugh, Roger K. Moore, William K. Cheung, Peter H. Charlton, Haiping Lu

**Updated**: 2025-04-04T17:20:05Z

**Summary**: Multimodal artificial intelligence (AI) integrates diverse types of data via machine learning to improve understanding, prediction, and decision-making across disciplines such as healthcare, science, and engineering. However, most multimodal AI advances focus on models for vision and language data, while their deployability remains a key challenge. We advocate a deployment-centric workflow that incorporates deployment constraints early to reduce the likelihood of undeployable solutions, complementing data-centric and model-centric approaches. We also emphasise deeper integration across multiple levels of multimodality and multidisciplinary collaboration to significantly broaden the research scope beyond vision and language. To facilitate this approach, we identify common multimodal-AI-specific challenges shared across disciplines and examine three real-world use cases: pandemic response, self-driving car design, and climate change adaptation, drawing expertise from healthcare, social science, engineering, science, sustainability, and finance. By fostering multidisciplinary dialogue and open research practices, our community can accelerate deployment-centric development for broad societal impact.

**Link**: [arxiv](http://arxiv.org/abs/2504.03603v1),  [pdf](http://arxiv.org/pdf/2504.03603v1)

**Tags**: cs.AI cs.LG 



### Fully Automated Generation of Combinatorial Optimisation Systems Using   Large Language Models
**Authors**: Daniel Karapetyan

**Updated**: 2025-04-04T17:13:59Z

**Summary**: Over the last few decades, researchers have made considerable efforts to make decision support more accessible for small and medium enterprises by reducing the cost of designing, developing and maintaining automated decision support systems. However, due to the diversity of the underlying combinatorial optimisation problems, reusability of such systems has been limited; in most cases, expensive expertise has been required to implement bespoke software components.   We explore the feasibility of fully automated generation of combinatorial optimisation systems using large language models (LLMs). An LLM will be responsible for interpreting the user-provided problem description in natural language and designing and implementing problem-specific software components. We discuss the principles of fully automated LLM-based optimisation system generation, and evaluate several proof-of-concept generators, comparing their performance on four optimisation problems.

**Link**: [arxiv](http://arxiv.org/abs/2503.15556v2),  [pdf](http://arxiv.org/pdf/2503.15556v2)

**Tags**: cs.SE cs.PL 



### APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated   Agent-Human Interplay
**Authors**: Akshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Shiyu Wang, Zhiwei Liu, Tulika Awalgaonkar, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Huan Wang, Silvio Savarese, Caiming Xiong

**Updated**: 2025-04-04T17:13:57Z

**Summary**: Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $\tau$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source both the synthetic data collected and the trained xLAM-2-fc-r models to advance research in AI agents. Models are available on HuggingFace at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4 and project website is https://apigen-mt.github.io

**Link**: [arxiv](http://arxiv.org/abs/2504.03601v1),  [pdf](http://arxiv.org/pdf/2504.03601v1)

**Tags**: cs.CL cs.AI cs.LG 



### MedSAM2: Segment Anything in 3D Medical Images and Videos
**Authors**: Jun Ma, Zongxin Yang, Sumin Kim, Bihui Chen, Mohammed Baharoon, Adibvafa Fallahpour, Reza Asakereh, Hongwei Lyu, Bo Wang

**Updated**: 2025-04-04T17:13:37Z

**Summary**: Medical image and video segmentation is a critical task for precision medicine, which has witnessed considerable progress in developing task or modality-specific and generalist models for 2D images. However, there have been limited studies on building general-purpose models for 3D images and videos with comprehensive user studies. Here, we present MedSAM2, a promptable segmentation foundation model for 3D image and video segmentation. The model is developed by fine-tuning the Segment Anything Model 2 on a large medical dataset with over 455,000 3D image-mask pairs and 76,000 frames, outperforming previous models across a wide range of organs, lesions, and imaging modalities. Furthermore, we implement a human-in-the-loop pipeline to facilitate the creation of large-scale datasets resulting in, to the best of our knowledge, the most extensive user study to date, involving the annotation of 5,000 CT lesions, 3,984 liver MRI lesions, and 251,550 echocardiogram video frames, demonstrating that MedSAM2 can reduce manual costs by more than 85%. MedSAM2 is also integrated into widely used platforms with user-friendly interfaces for local and cloud deployment, making it a practical tool for supporting efficient, scalable, and high-quality segmentation in both research and healthcare environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.03600v1),  [pdf](http://arxiv.org/pdf/2504.03600v1)

**Tags**: eess.IV cs.AI cs.CV 



### EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline
**Authors**: Peter Baile Chen, Tomer Wolfson, Michael Cafarella, Dan Roth

**Updated**: 2025-04-04T17:08:46Z

**Summary**: Existing information retrieval systems excel in cases where the language of target documents closely matches that of the user query. However, real-world retrieval systems are often required to implicitly reason whether a document is relevant. For example, when retrieving technical texts or tables, their relevance to the user query may be implied through a particular jargon or structure, rather than explicitly expressed in their content. Large language models (LLMs) hold great potential in identifying such implied relevance by leveraging their reasoning skills. Nevertheless, current LLM-augmented retrieval is hindered by high latency and computation cost, as the LLM typically computes the query-document relevance online, for every query anew. To tackle this issue we introduce EnrichIndex, a retrieval approach which instead uses the LLM offline to build semantically-enriched retrieval indices, by performing a single pass over all documents in the retrieval corpus once during ingestion time. Furthermore, the semantically-enriched indices can complement existing online retrieval approaches, boosting the performance of LLM re-rankers. We evaluated EnrichIndex on five retrieval tasks, involving passages and tables, and found that it outperforms strong online LLM-based retrieval systems, with an average improvement of 11.7 points in recall @ 10 and 10.6 points in NDCG @ 10 compared to strong baselines. In terms of online calls to the LLM, it processes 293.3 times fewer tokens which greatly reduces the online latency and cost. Overall, EnrichIndex is an effective way to build better retrieval indices offline by leveraging the strong reasoning skills of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.03598v1),  [pdf](http://arxiv.org/pdf/2504.03598v1)

**Tags**: cs.CL cs.AI cs.IR 



### Real-is-Sim: Bridging the Sim-to-Real Gap with a Dynamic Digital Twin   for Real-World Robot Policy Evaluation
**Authors**: Jad Abou-Chakra, Lingfeng Sun, Krishan Rana, Brandon May, Karl Schmeckpeper, Maria Vittoria Minniti, Laura Herlant

**Updated**: 2025-04-04T17:05:56Z

**Summary**: Recent advancements in behavior cloning have enabled robots to perform complex manipulation tasks. However, accurately assessing training performance remains challenging, particularly for real-world applications, as behavior cloning losses often correlate poorly with actual task success. Consequently, researchers resort to success rate metrics derived from costly and time-consuming real-world evaluations, making the identification of optimal policies and detection of overfitting or underfitting impractical. To address these issues, we propose real-is-sim, a novel behavior cloning framework that incorporates a dynamic digital twin (based on Embodied Gaussians) throughout the entire policy development pipeline: data collection, training, and deployment. By continuously aligning the simulated world with the physical world, demonstrations can be collected in the real world with states extracted from the simulator. The simulator enables flexible state representations by rendering image inputs from any viewpoint or extracting low-level state information from objects embodied within the scene. During training, policies can be directly evaluated within the simulator in an offline and highly parallelizable manner. Finally, during deployment, policies are run within the simulator where the real robot directly tracks the simulated robot's joints, effectively decoupling policy execution from real hardware and mitigating traditional domain-transfer challenges. We validate real-is-sim on the PushT manipulation task, demonstrating strong correlation between success rates obtained in the simulator and real-world evaluations. Videos of our system can be found at https://realissim.rai-inst.com.

**Link**: [arxiv](http://arxiv.org/abs/2504.03597v1),  [pdf](http://arxiv.org/pdf/2504.03597v1)

**Tags**: cs.RO cs.AI 



### Adaptive Semantic Prompt Caching with VectorQ
**Authors**: Luis Gaspar Schroeder, Shu Liu, Alejandro Cuadron, Mark Zhao, Stephan Krusche, Alfons Kemper, Matei Zaharia, Joseph E. Gonzalez

**Updated**: 2025-04-04T16:51:15Z

**Summary**: Semantic prompt caches reduce the latency and cost of large language model (LLM) inference by reusing cached LLM-generated responses for semantically similar prompts. Vector similarity metrics assign a numerical score to quantify the similarity between an embedded prompt and its nearest neighbor in the cache. Existing systems rely on a static threshold to classify whether the similarity score is sufficiently high to result in a cache hit. We show that this one-size-fits-all threshold is insufficient across different embeddings. We propose VectorQ, an online framework with a threshold convergence guarantee to learn embedding-specific threshold regions that adapt to the uncertainty of an embedding. Through evaluations on a combination of three diverse datasets, we show that VectorQ consistently outperforms state-of-the-art systems across all static thresholds, achieving up to 26x increases in cache hit rate and error rate reductions up to 74%.

**Link**: [arxiv](http://arxiv.org/abs/2502.03771v2),  [pdf](http://arxiv.org/pdf/2502.03771v2)

**Tags**: cs.LG cs.CL 



### Hallucination Detection on a Budget: Efficient Bayesian Estimation of   Semantic Entropy
**Authors**: Kamil Ciosek, Nicolò Felicioni, Sina Ghiassian

**Updated**: 2025-04-04T16:30:44Z

**Summary**: Detecting whether an LLM hallucinates is an important research challenge. One promising way of doing so is to estimate the semantic entropy (Farquhar et al., 2024) of the distribution of generated sequences. We propose a new algorithm for doing that, with two main advantages. First, due to us taking the Bayesian approach, we achieve a much better quality of semantic entropy estimates for a given budget of samples from the LLM. Second, we are able to tune the number of samples adaptively so that `harder' contexts receive more samples. We demonstrate empirically that our approach systematically beats the baselines, requiring only 59% of samples used by Farquhar et al. (2024) to achieve the same quality of hallucination detection as measured by AUROC. Moreover, quite counterintuitively, our estimator is useful even with just one sample from the LLM.

**Link**: [arxiv](http://arxiv.org/abs/2504.03579v1),  [pdf](http://arxiv.org/pdf/2504.03579v1)

**Tags**: cs.LG 



### Heterogeneous Resource Allocation for Ensuring End-to-End Quality of   Service in Multi-hop Integrated Access and Backhaul Network
**Authors**: Shuaifeng Zhang

**Updated**: 2025-04-04T16:29:08Z

**Summary**: Faced with increasing network traffic demands, cell dense deployment is one of significant means to utilize spectrum resources efficiently to improve network capacity. Multi-hop integrated access and backhaul (IAB) architectures have emerged as a cost-effective solution for network densification. Meanwhile, dynamic time division duplex (D-TDD) is a promising solution to adapt to highly dynamic scenarios with asymmetric uplink and downlink traffic. Thus, dynamic resource allocation between backhaul and access links and high spectral efficiency under ensuring reliable transmission are two key objectives of IAB research. However, due to huge solution space, there are some challenges in multi-hop IAB with D-TDD if only an integrated optimization problem (IOP) is considered. To handle these challenges, we decompose the IOP into sub-problems to reduce the solution space. To tackle these sub-problems, we formulate them separately as the non-cooperative games and design the corresponding utility functions to guarantee the existence of Nash equilibrium solutions. Also, to achieve the system-wide solution, we propose a single-leader heterogeneous multi-follower Stackelberg-game-based resource allocation scheme, which can combine the solving results of all the sub-problems to get the IOP approximate solution. Simulation results show that the proposed scheme can improve throughput performance while meeting spectrum energy efficiency constraints.

**Link**: [arxiv](http://arxiv.org/abs/2504.03576v1),  [pdf](http://arxiv.org/pdf/2504.03576v1)

**Tags**: cs.NE 



### SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge   Refinement
**Authors**: Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen

**Updated**: 2025-04-04T16:10:57Z

**Summary**: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.

**Link**: [arxiv](http://arxiv.org/abs/2504.03561v1),  [pdf](http://arxiv.org/pdf/2504.03561v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG cs.MA 



### Agentic Knowledgeable Self-awareness
**Authors**: Shuofei Qiao, Zhisong Qiu, Baochang Ren, Xiaobin Wang, Xiangyuan Ru, Ningyu Zhang, Xiang Chen, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen

**Updated**: 2025-04-04T16:03:38Z

**Summary**: Large Language Models (LLMs) have achieved considerable performance across various agentic planning tasks. However, traditional agent planning approaches adopt a "flood irrigation" methodology that indiscriminately injects gold trajectories, external feedback, and domain knowledge into agent models. This practice overlooks the fundamental human cognitive principle of situational self-awareness during decision-making-the ability to dynamically assess situational demands and strategically employ resources during decision-making. We propose agentic knowledgeable self-awareness to address this gap, a novel paradigm enabling LLM-based agents to autonomously regulate knowledge utilization. Specifically, we propose KnowSelf, a data-centric approach that applies agents with knowledgeable self-awareness like humans. Concretely, we devise a heuristic situation judgement criterion to mark special tokens on the agent's self-explored trajectories for collecting training data. Through a two-stage training process, the agent model can switch between different situations by generating specific special tokens, achieving optimal planning effects with minimal costs. Our experiments demonstrate that KnowSelf can outperform various strong baselines on different tasks and models with minimal use of external knowledge. Code is available at https://github.com/zjunlp/KnowSelf.

**Link**: [arxiv](http://arxiv.org/abs/2504.03553v1),  [pdf](http://arxiv.org/pdf/2504.03553v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG cs.MA 



### Diverse In-Context Example Selection After Decomposing Programs and   Aligned Utterances Improves Semantic Parsing
**Authors**: Mayank Kothyari, Sunita Sarawagi, Soumen Chakrabarti, Gaurav Arora, Srujana Merugu

**Updated**: 2025-04-04T15:41:44Z

**Summary**: LLMs are increasingly used as seq2seq translators from natural language utterances to structured programs, a process called semantic interpretation. Unlike atomic labels or token sequences, programs are naturally represented as abstract syntax trees (ASTs). Such structured representation raises novel issues related to the design and selection of in-context examples (ICEs) presented to the LLM. We focus on decomposing the pool of available ICE trees into fragments, some of which may be better suited to solving the test instance. Next, we propose how to use (additional invocations of) an LLM with prompted syntax constraints to automatically map the fragments to corresponding utterances. Finally, we adapt and extend a recent method for diverse ICE selection to work with whole and fragmented ICE instances. We evaluate our system, SCUD4ICL, on popular diverse semantic parsing benchmarks, showing visible accuracy gains from our proposed decomposed diverse demonstration method. Benefits are particularly notable for smaller LLMs, ICE pools having larger labeled trees, and programs in lower resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.03541v1),  [pdf](http://arxiv.org/pdf/2504.03541v1)

**Tags**: cs.CL 



### RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action   Issue Detection, Explanation and Recovery
**Authors**: Silvia Izquierdo-Badiola, Carlos Rizzo, Guillem Alenyà

**Updated**: 2025-04-04T15:38:50Z

**Summary**: As robots increasingly operate in dynamic human-centric environments, improving their ability to detect, explain, and recover from action-related issues becomes crucial. Traditional model-based and data-driven techniques lack adaptability, while more flexible generative AI methods struggle with grounding extracted information to real-world constraints. We introduce RAIDER, a novel agent that integrates Large Language Models (LLMs) with grounded tools for adaptable and efficient issue detection and explanation. Using a unique "Ground, Ask&Answer, Issue" procedure, RAIDER dynamically generates context-aware precondition questions and selects appropriate tools for resolution, achieving targeted information gathering. Our results within a simulated household environment surpass methods relying on predefined models, full scene descriptions, or standalone trained models. Additionally, RAIDER's explanations enhance recovery success, including cases requiring human interaction. Its modular architecture, featuring self-correction mechanisms, enables straightforward adaptation to diverse scenarios, as demonstrated in a real-world human-assistive task. This showcases RAIDER's potential as a versatile agentic AI solution for robotic issue detection and explanation, while addressing the problem of grounding generative AI for its effective application in embodied agents. Project website: https://eurecat.github.io/raider-llmagent/

**Link**: [arxiv](http://arxiv.org/abs/2503.17703v2),  [pdf](http://arxiv.org/pdf/2503.17703v2)

**Tags**: cs.RO 



### Lightweight Learning for Grant-Free Activity Detection in Cell-Free   Massive MIMO Networks
**Authors**: Ali Elkeshawy, Haifa Fares, Amor Nafkha

**Updated**: 2025-04-04T15:24:36Z

**Summary**: Grant-free random access (GF-RA) is a promising access technique for massive machine-type communications (mMTC) in future wireless networks, particularly in the context of 5G and beyond (6G) systems. Within the context of GF-RA, this study investigates the efficiency of employing supervised machine learning techniques to tackle the challenges on the device activity detection (AD). GF-RA addresses scalability by employing non-orthogonal pilot sequences, which provides an efficient alternative comparing to conventional grant-based random access (GB-RA) technique that are constrained by the scarcity of orthogonal preamble resources. In this paper, we propose a novel lightweight data-driven algorithmic framework specifically designed for activity detection in GF-RA for mMTC in cell-free massive multiple-input multiple-output (CF-mMIMO) networks. We propose two distinct framework deployment strategies, centralized and decentralized, both tailored to streamline the proposed approach implementation across network infrastructures. Moreover, we introduce optimized post-detection methodologies complemented by a clustering stage to enhance overall detection performances. Our 3GPP-compliant simulations have validated that the proposed algorithm achieves state-of-the-art model-based activity detection accuracy while significantly reducing complexity. Achieving 99% accuracy, it demonstrates real-world viability and effectiveness.

**Link**: [arxiv](http://arxiv.org/abs/2503.11305v2),  [pdf](http://arxiv.org/pdf/2503.11305v2)

**Tags**: eess.SP cs.LG 



### Programming Distributed Collective Processes in the eXchange Calculus
**Authors**: Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Gianluca Torta, Mirko Viroli

**Updated**: 2025-04-04T15:23:08Z

**Summary**: Recent trends like the Internet of Things (IoT) suggest a vision of dense and multi-scale deployments of computing devices in nearly all kinds of environments. A prominent engineering challenge revolves around programming the collective adaptive behaviour of such computational ecosystems. This requires abstractions able to capture concepts like ensembles (dynamic groups of cooperating devices) and collective tasks (joint activities carried out by ensembles). In this work, we consider collections of devices interacting with neighbours and that execute in nearly-synchronised sense-compute-interact rounds, where the computation is given by a single program mapping sensing values and incoming messages to output and outcoming messages. To support programming whole computational collectives, we propose the abstraction of a distributed collective process, which can be used to define at once the ensemble formation logic and its collective task. We formalise the abstraction in the eXchange Calculus (XC), a core functional language based on neighbouring values (maps from neighbours to values) where state and interaction is handled through a single primitive, exchange, and provide a corresponding implementation in the FCPP language. Then, we exercise distributed collective processes using two case studies: multi-hop message propagation and distributed monitoring of spatial properties. Finally, we discuss the features of the abstraction and its suitability for different kinds of distributed computing applications.

**Link**: [arxiv](http://arxiv.org/abs/2401.11212v3),  [pdf](http://arxiv.org/pdf/2401.11212v3)

**Tags**: cs.DC cs.AI cs.MA cs.PL D.1.3; F.1.1; F.4.3; I.2.11; J.7 



### Pub-Guard-LLM: Detecting Fraudulent Biomedical Articles with Reliable   Explanations
**Authors**: Lihu Chen, Shuojie Fu, Gabriel Freedman, Cemre Zor, Guy Martin, James Kinross, Uddhav Vaghela, Ovidiu Serban, Francesca Toni

**Updated**: 2025-04-04T15:21:03Z

**Summary**: A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool.

**Link**: [arxiv](http://arxiv.org/abs/2502.15429v3),  [pdf](http://arxiv.org/pdf/2502.15429v3)

**Tags**: cs.CL 



### Neutralizing the Narrative: AI-Powered Debiasing of Online News Articles
**Authors**: Chen Wei Kuo, Kevin Chu, Nouar AlDahoul, Hazem Ibrahim, Talal Rahwan, Yasir Zaki

**Updated**: 2025-04-04T15:17:53Z

**Summary**: Bias in news reporting significantly impacts public perception, particularly regarding crime, politics, and societal issues. Traditional bias detection methods, predominantly reliant on human moderation, suffer from subjective interpretations and scalability constraints. Here, we introduce an AI-driven framework leveraging advanced large language models (LLMs), specifically GPT-4o, GPT-4o Mini, Gemini Pro, Gemini Flash, Llama 8B, and Llama 3B, to systematically identify and mitigate biases in news articles. To this end, we collect an extensive dataset consisting of over 30,000 crime-related articles from five politically diverse news sources spanning a decade (2013-2023). Our approach employs a two-stage methodology: (1) bias detection, where each LLM scores and justifies biased content at the paragraph level, validated through human evaluation for ground truth establishment, and (2) iterative debiasing using GPT-4o Mini, verified by both automated reassessment and human reviewers. Empirical results indicate GPT-4o Mini's superior accuracy in bias detection and effectiveness in debiasing. Furthermore, our analysis reveals temporal and geographical variations in media bias correlating with socio-political dynamics and real-world events. This study contributes to scalable computational methodologies for bias mitigation, promoting fairness and accountability in news reporting.

**Link**: [arxiv](http://arxiv.org/abs/2504.03520v1),  [pdf](http://arxiv.org/pdf/2504.03520v1)

**Tags**: cs.CL cs.CY 



### Structured Legal Document Generation in India: A Model-Agnostic Wrapper   Approach with VidhikDastaavej
**Authors**: Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya

**Updated**: 2025-04-04T14:41:50Z

**Summary**: Automating legal document drafting can significantly enhance efficiency, reduce manual effort, and streamline legal workflows. While prior research has explored tasks such as judgment prediction and case summarization, the structured generation of private legal documents in the Indian legal domain remains largely unaddressed. To bridge this gap, we introduce VidhikDastaavej, a novel, anonymized dataset of private legal documents, and develop NyayaShilp, a fine-tuned legal document generation model specifically adapted to Indian legal texts. We propose a Model-Agnostic Wrapper (MAW), a two-step framework that first generates structured section titles and then iteratively produces content while leveraging retrieval-based mechanisms to ensure coherence and factual accuracy. We benchmark multiple open-source LLMs, including instruction-tuned and domain-adapted versions, alongside proprietary models for comparison. Our findings indicate that while direct fine-tuning on small datasets does not always yield improvements, our structured wrapper significantly enhances coherence, factual adherence, and overall document quality while mitigating hallucinations. To ensure real-world applicability, we developed a Human-in-the-Loop (HITL) Document Generation System, an interactive user interface that enables users to specify document types, refine section details, and generate structured legal drafts. This tool allows legal professionals and researchers to generate, validate, and refine AI-generated legal documents efficiently. Extensive evaluations, including expert assessments, confirm that our framework achieves high reliability in structured legal drafting. This research establishes a scalable and adaptable foundation for AI-assisted legal drafting in India, offering an effective approach to structured legal document generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.03486v1),  [pdf](http://arxiv.org/pdf/2504.03486v1)

**Tags**: cs.CL cs.AI cs.IR cs.LG 



### Real-time Speech Summarization for Medical Conversations
**Authors**: Khai Le-Duc, Khai-Nguyen Nguyen, Long Vo-Dang, Truong-Son Hy

**Updated**: 2025-04-04T14:12:54Z

**Summary**: In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online: https://github.com/leduckhai/MultiMed/tree/master/VietMed-Sum

**Link**: [arxiv](http://arxiv.org/abs/2406.15888v2),  [pdf](http://arxiv.org/pdf/2406.15888v2)

**Tags**: cs.CL cs.AI cs.LG cs.SD eess.AS 



### Showcasing Automated Vehicle Prototypes: A Collaborative Release Process   to Manage and Communicate Risk
**Authors**: Marvin Loba, Robert Graubohm, Markus Maurer

**Updated**: 2025-04-04T14:05:33Z

**Summary**: The development and deployment of automated vehicles pose major challenges for manufacturers to this day. Whilst central questions, like the issue of ensuring a sufficient level of safety, remain unanswered, prototypes are increasingly finding their way into public traffic in urban areas. Although safety concepts for prototypes are addressed in literature, published work hardly contains any dedicated considerations on a systematic release for their operation. In this paper, we propose an incremental release process for public demonstrations of prototypes' automated driving functionality. We explicate release process requirements, derive process design decisions, and define stakeholder tasks. Furthermore, we reflect on practical insights gained through implementing the release process as part of the UNICAR$agil$ research project, in which four prototypes based on novel vehicle concepts were built and demonstrated to the public. One observation is the improved quality of internal risk communication, achieved by dismantling information asymmetries between stakeholders. Design conflicts are disclosed - providing a contribution to nurture transparency and, thereby, supporting a valid basis for release decisions. We argue that our release process meets two important requirements, as the results suggest its applicability to the domain of automated driving and its scalability to different vehicle concepts and organizational structures.

**Link**: [arxiv](http://arxiv.org/abs/2404.15962v3),  [pdf](http://arxiv.org/pdf/2404.15962v3)

**Tags**: eess.SY cs.SY 



### LLMSched: Uncertainty-Aware Workload Scheduling for Compound LLM   Applications
**Authors**: Botao Zhu, Chen Chen, Xiaoyi Fan, Yifei Zhu

**Updated**: 2025-04-04T13:37:29Z

**Summary**: Developing compound Large Language Model (LLM) applications is becoming an increasingly prevalent approach to solving real-world problems. In these applications, an LLM collaborates with various external modules, including APIs and even other LLMs, to realize complex intelligent services. However, we reveal that the intrinsic duration and structural uncertainty in compound LLM applications pose great challenges for LLM service providers in serving and scheduling them efficiently. In this paper, we propose LLMSched, an uncertainty-aware scheduling framework for emerging compound LLM applications. In LLMSched, we first design a novel DAG-based model to describe the uncertain compound LLM applications. Then, we adopt the Bayesian network to comprehensively profile compound LLM applications and identify uncertainty-reducing stages, along with an entropy-based mechanism to quantify their uncertainty reduction. Combining an uncertainty reduction strategy and a job completion time (JCT)-efficient scheme, we further propose an efficient scheduler to reduce the average JCT. Evaluation of both simulation and testbed experiments on various representative compound LLM applications shows that compared to existing state-of-the-art scheduling schemes, LLMSched can reduce the average JCT by 14~79%.

**Link**: [arxiv](http://arxiv.org/abs/2504.03444v1),  [pdf](http://arxiv.org/pdf/2504.03444v1)

**Tags**: cs.DC 



### ORLM: A Customizable Framework in Training Large Models for Automated   Optimization Modeling
**Authors**: Chenyu Huang, Zhengyang Tang, Shixi Hu, Ruoqing Jiang, Xin Zheng, Dongdong Ge, Benyou Wang, Zizhuo Wang

**Updated**: 2025-04-04T13:31:38Z

**Summary**: Optimization modeling plays a critical role in the application of Operations Research (OR) tools to address real-world problems, yet they pose challenges and require extensive expertise from OR experts. With the advent of large language models (LLMs), new opportunities have emerged to streamline and automate such task. However, current research predominantly relies on closed-source LLMs such as GPT-4, along with extensive prompt engineering techniques. This reliance stems from the scarcity of high-quality training datasets for optimization modeling, resulting in elevated costs, prolonged processing times, and privacy concerns. To address these challenges, our work is the first to propose a viable path for training open-source LLMs that are capable of optimization modeling and developing solver codes, eventually leading to a superior ability for automating optimization modeling and solving. Particularly, we design the {\sc OR-Instruct}, a semi-automated data synthesis framework for optimization modeling that enables customizable enhancements for specific scenarios or model types. This work also introduces IndustryOR, the first industrial benchmark for evaluating LLMs in solving practical OR problems. We train several 7B-scale open-source LLMs using synthesized data (dubbed ORLMs{https://github.com/Cardinal-Operations/ORLM}), which exhibit significantly enhanced optimization modeling capabilities, achieving competitive performance across the NL4OPT, MAMO, and IndustryOR benchmarks. Additionally, our experiments highlight the potential of scaling law and reinforcement learning to further enhance the performance of ORLMs. The workflows and human-machine interaction paradigms of ORLMs in practical industrial applications are also discussed in the paper.

**Link**: [arxiv](http://arxiv.org/abs/2405.17743v5),  [pdf](http://arxiv.org/pdf/2405.17743v5)

**Tags**: cs.CL cs.AI cs.CE cs.LG 



### Know What You do Not Know: Verbalized Uncertainty Estimation Robustness   on Corrupted Images in Vision-Language Models
**Authors**: Mirko Borszukovszki, Ivo Pascal de Jong, Matias Valdenegro-Toro

**Updated**: 2025-04-04T13:31:08Z

**Summary**: To leverage the full potential of Large Language Models (LLMs) it is crucial to have some information on their answers' uncertainty. This means that the model has to be able to quantify how certain it is in the correctness of a given response. Bad uncertainty estimates can lead to overconfident wrong answers undermining trust in these models. Quite a lot of research has been done on language models that work with text inputs and provide text outputs. Still, since the visual capabilities have been added to these models recently, there has not been much progress on the uncertainty of Visual Language Models (VLMs). We tested three state-of-the-art VLMs on corrupted image data. We found that the severity of the corruption negatively impacted the models' ability to estimate their uncertainty and the models also showed overconfidence in most of the experiments.

**Link**: [arxiv](http://arxiv.org/abs/2504.03440v1),  [pdf](http://arxiv.org/pdf/2504.03440v1)

**Tags**: cs.CV cs.LG 



### Locations of Characters in Narratives: Andersen and Persuasion Datasets
**Authors**: Batuhan Ozyurt, Roya Arkhmammadova, Deniz Yuret

**Updated**: 2025-04-04T13:25:32Z

**Summary**: The ability of machines to grasp spatial understanding within narrative contexts is an intriguing aspect of reading comprehension that continues to be studied. Motivated by the goal to test the AI's competence in understanding the relationship between characters and their respective locations in narratives, we introduce two new datasets: Andersen and Persuasion. For the Andersen dataset, we selected fifteen children's stories from "Andersen's Fairy Tales" by Hans Christian Andersen and manually annotated the characters and their respective locations throughout each story. Similarly, for the Persuasion dataset, characters and their locations in the novel "Persuasion" by Jane Austen were also manually annotated. We used these datasets to prompt Large Language Models (LLMs). The prompts are created by extracting excerpts from the stories or the novel and combining them with a question asking the location of a character mentioned in that excerpt. Out of the five LLMs we tested, the best-performing one for the Andersen dataset accurately identified the location in 61.85% of the examples, while for the Persuasion dataset, the best-performing one did so in 56.06% of the cases.

**Link**: [arxiv](http://arxiv.org/abs/2504.03434v1),  [pdf](http://arxiv.org/pdf/2504.03434v1)

**Tags**: cs.CL I.2.7 



### Fair and Energy-Efficient Activation Control Mechanisms for   Repeater-Assisted Massive MIMO
**Authors**: Ozan Alp Topal, Özlem Tuğfe Demir, Emil Björnson, Cicek Cavdar

**Updated**: 2025-04-04T13:18:39Z

**Summary**: Massive multiple-input multiple-output (mMIMO) has been the core of 5G due to its ability to improve spectral efficiency and spatial multiplexing significantly; however, cell-edge users still experience performance degradation due to inter-cell interference and uneven signal distribution. While cell-free mMIMO (cfmMIMO) addresses this issue by providing uniform coverage through distributed antennas, it requires significantly more deployment cost due to the fronthaul and tight synchronization requirements. Alternatively, repeater-assisted massive MIMO (RA-MIMO) has recently been proposed to extend the coverage of cellular mMIMO by densely deploying low-cost single-antenna repeaters capable of amplifying and forwarding signals. In this work, we investigate amplification control for the repeaters for two different goals: (i) providing a fair performance among users, and (ii) reducing the extra energy consumption by the deployed repeaters. We propose a max-min amplification control algorithm using the convex-concave procedure for fairness and a joint sleep mode and amplification control algorithm for energy efficiency, comparing long- and short-term strategies. Numerical results show that RA-MIMO, with maximum amplification, improves signal-to-interference-plus-noise ratio (SINR) by over 20 dB compared to mMIMO and performs within 1 dB of cfmMIMO when deploying the same number of repeaters as access points in cfmMIMO. Additionally, our majority-rule-based long-term sleep mechanism reduces repeater power consumption by 70% while maintaining less than 1% spectral efficiency outage.

**Link**: [arxiv](http://arxiv.org/abs/2504.03428v1),  [pdf](http://arxiv.org/pdf/2504.03428v1)

**Tags**: eess.SP 



### NeRFlex: Resource-aware Real-time High-quality Rendering of Complex   Scenes on Mobile Devices
**Authors**: Zhe Wang, Yifei Zhu

**Updated**: 2025-04-04T12:53:33Z

**Summary**: Neural Radiance Fields (NeRF) is a cutting-edge neural network-based technique for novel view synthesis in 3D reconstruction. However, its significant computational demands pose challenges for deployment on mobile devices. While mesh-based NeRF solutions have shown potential in achieving real-time rendering on mobile platforms, they often fail to deliver high-quality reconstructions when rendering practical complex scenes. Additionally, the non-negligible memory overhead caused by pre-computed intermediate results complicates their practical application. To overcome these challenges, we present NeRFlex, a resource-aware, high-resolution, real-time rendering framework for complex scenes on mobile devices. NeRFlex integrates mobile NeRF rendering with multi-NeRF representations that decompose a scene into multiple sub-scenes, each represented by an individual NeRF network. Crucially, NeRFlex considers both memory and computation constraints as first-class citizens and redesigns the reconstruction process accordingly. NeRFlex first designs a detail-oriented segmentation module to identify sub-scenes with high-frequency details. For each NeRF network, a lightweight profiler, built on domain knowledge, is used to accurately map configurations to visual quality and memory usage. Based on these insights and the resource constraints on mobile devices, NeRFlex presents a dynamic programming algorithm to efficiently determine configurations for all NeRF representations, despite the NP-hardness of the original decision problem. Extensive experiments on real-world datasets and mobile devices demonstrate that NeRFlex achieves real-time, high-quality rendering on commercial mobile devices.

**Link**: [arxiv](http://arxiv.org/abs/2504.03415v1),  [pdf](http://arxiv.org/pdf/2504.03415v1)

**Tags**: cs.GR cs.CV cs.LG cs.MM cs.PF 



### PaperBench: Evaluating AI's Ability to Replicate AI Research
**Authors**: Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, Tejal Patwardhan

**Updated**: 2025-04-04T12:44:57Z

**Summary**: We introduce PaperBench, a benchmark evaluating the ability of AI agents to replicate state-of-the-art AI research. Agents must replicate 20 ICML 2024 Spotlight and Oral papers from scratch, including understanding paper contributions, developing a codebase, and successfully executing experiments. For objective evaluation, we develop rubrics that hierarchically decompose each replication task into smaller sub-tasks with clear grading criteria. In total, PaperBench contains 8,316 individually gradable tasks. Rubrics are co-developed with the author(s) of each ICML paper for accuracy and realism. To enable scalable evaluation, we also develop an LLM-based judge to automatically grade replication attempts against rubrics, and assess our judge's performance by creating a separate benchmark for judges. We evaluate several frontier models on PaperBench, finding that the best-performing tested agent, Claude 3.5 Sonnet (New) with open-source scaffolding, achieves an average replication score of 21.0\%. Finally, we recruit top ML PhDs to attempt a subset of PaperBench, finding that models do not yet outperform the human baseline. We \href{https://github.com/openai/preparedness}{open-source our code} to facilitate future research in understanding the AI engineering capabilities of AI agents.

**Link**: [arxiv](http://arxiv.org/abs/2504.01848v2),  [pdf](http://arxiv.org/pdf/2504.01848v2)

**Tags**: cs.AI cs.CL 



### Assessing SPARQL capabilities of Large Language Models
**Authors**: Lars-Peter Meyer, Johannes Frey, Felix Brei, Natanael Arndt

**Updated**: 2025-04-04T11:59:49Z

**Summary**: The integration of Large Language Models (LLMs) with Knowledge Graphs (KGs) offers significant synergistic potential for knowledge-driven applications. One possible integration is the interpretation and generation of formal languages, such as those used in the Semantic Web, with SPARQL being a core technology for accessing KGs. In this paper, we focus on measuring out-of-the box capabilities of LLMs to work with SPARQL and more specifically with SPARQL SELECT queries applying a quantitative approach.   We implemented various benchmarking tasks in the LLM-KG-Bench framework for automated execution and evaluation with several LLMs. The tasks assess capabilities along the dimensions of syntax, semantic read, semantic create, and the role of knowledge graph prompt inclusion.   With this new benchmarking tasks, we evaluated a selection of GPT, Gemini, and Claude models. Our findings indicate that working with SPARQL SELECT queries is still challenging for LLMs and heavily depends on the specific LLM as well as the complexity of the task. While fixing basic syntax errors seems to pose no problems for the best of the current LLMs evaluated, creating semantically correct SPARQL SELECT queries is difficult in several cases.

**Link**: [arxiv](http://arxiv.org/abs/2409.05925v2),  [pdf](http://arxiv.org/pdf/2409.05925v2)

**Tags**: cs.DB cs.AI cs.CL cs.IR 



### Do Robot Snakes Dream like Electric Sheep? Investigating the Effects of   Architectural Inductive Biases on Hallucination
**Authors**: Jerry Huang, Prasanna Parthasarathi, Mehdi Rezagholizadeh, Boxing Chen, Sarath Chandar

**Updated**: 2025-04-04T11:55:58Z

**Summary**: The growth in prominence of large language models (LLMs) in everyday life can be largely attributed to their generative abilities, yet some of this is also owed to the risks and costs associated with their use. On one front is their tendency to hallucinate false or misleading information, limiting their reliability. On another is the increasing focus on the computational limitations associated with traditional self-attention based LLMs, which has brought about new alternatives, in particular recurrent models, meant to overcome them. Yet it remains uncommon to consider these two concerns simultaneously. Do changes in architecture exacerbate/alleviate existing concerns about hallucinations? Do they affect how and where they occur? Through an extensive evaluation, we study how these architecture-based inductive biases affect the propensity to hallucinate. While hallucination remains a general phenomenon not limited to specific architectures, the situations in which they occur and the ease with which specific types of hallucinations can be induced can significantly differ based on the model architecture. These findings highlight the need for better understanding both these problems in conjunction with each other, as well as consider how to design more universal techniques for handling hallucinations.

**Link**: [arxiv](http://arxiv.org/abs/2410.17477v4),  [pdf](http://arxiv.org/pdf/2410.17477v4)

**Tags**: cs.CL cs.AI cs.LG 



### Online Difficulty Filtering for Reasoning Oriented Reinforcement   Learning
**Authors**: Sanghwan Bae, Jiwoo Hong, Min Young Lee, Hanbyul Kim, JeongYeon Nam, Donghyun Kwak

**Updated**: 2025-04-04T11:52:05Z

**Summary**: Reasoning-Oriented Reinforcement Learning (RORL) enhances the reasoning ability of Large Language Models (LLMs). However, due to the sparsity of rewards in RORL, effective training is highly dependent on the selection of problems of appropriate difficulty. Although curriculum learning attempts to address this by adjusting difficulty, it often relies on static schedules, and even recent online filtering methods lack theoretical grounding and a systematic understanding of their effectiveness. In this work, we theoretically and empirically show that curating the batch with the problems that the training model achieves intermediate accuracy on the fly can maximize the effectiveness of RORL training, namely balanced online difficulty filtering. We first derive that the lower bound of the KL divergence between the initial and the optimal policy can be expressed with the variance of the sampled accuracy. Building on those insights, we show that balanced filtering can maximize the lower bound, leading to better performance. Experimental results across five challenging math reasoning benchmarks show that balanced online filtering yields an additional 10% in AIME and 4% improvements in average over plain GRPO. Moreover, further analysis shows the gains in sample efficiency and training time efficiency, exceeding the maximum reward of plain GRPO within 60% training time and the volume of the training set.

**Link**: [arxiv](http://arxiv.org/abs/2504.03380v1),  [pdf](http://arxiv.org/pdf/2504.03380v1)

**Tags**: cs.CL cs.AI 



### RMB: Comprehensively Benchmarking Reward Models in LLM Alignment
**Authors**: Enyu Zhou, Guodong Zheng, Binghai Wang, Zhiheng Xi, Shihan Dou, Rong Bao, Wei Shen, Limao Xiong, Jessica Fan, Yurong Mou, Rui Zheng, Tao Gui, Qi Zhang, Xuanjing Huang

**Updated**: 2025-04-04T11:45:02Z

**Summary**: Reward models (RMs) guide the alignment of large language models (LLMs), steering them toward behaviors preferred by humans. Evaluating RMs is the key to better aligning LLMs. However, the current evaluation of RMs may not directly correspond to their alignment performance due to the limited distribution of evaluation data and evaluation methods that are not closely related to alignment objectives. To address these limitations, we propose RMB, a comprehensive RM benchmark that covers over 49 real-world scenarios and includes both pairwise and Best-of-N (BoN) evaluations to better reflect the effectiveness of RMs in guiding alignment optimization. We demonstrate a positive correlation between our benchmark and the downstream alignment task performance. Based on our benchmark, we conduct extensive analysis on the state-of-the-art RMs, revealing their generalization defects that were not discovered by previous benchmarks, and highlighting the potential of generative RMs. Furthermore, we delve into open questions in reward models, specifically examining the effectiveness of majority voting for the evaluation of reward models and analyzing the impact factors of generative RMs, including the influence of evaluation criteria and instructing methods. Our evaluation code and datasets are available at https://github.com/Zhou-Zoey/RMB-Reward-Model-Benchmark.

**Link**: [arxiv](http://arxiv.org/abs/2410.09893v2),  [pdf](http://arxiv.org/pdf/2410.09893v2)

**Tags**: cs.CL 



### An Efficient GPU-based Implementation for Noise Robust Sound Source   Localization
**Authors**: Zirui Lin, Masayuki Takigahira, Naoya Terakado, Haris Gulzar, Monikka Roslianna Busto, Takeharu Eda, Katsutoshi Itoyama, Kazuhiro Nakadai, Hideharu Amano

**Updated**: 2025-04-04T11:44:24Z

**Summary**: Robot audition, encompassing Sound Source Localization (SSL), Sound Source Separation (SSS), and Automatic Speech Recognition (ASR), enables robots and smart devices to acquire auditory capabilities similar to human hearing. Despite their wide applicability, processing multi-channel audio signals from microphone arrays in SSL involves computationally intensive matrix operations, which can hinder efficient deployment on Central Processing Units (CPUs), particularly in embedded systems with limited CPU resources. This paper introduces a GPU-based implementation of SSL for robot audition, utilizing the Generalized Singular Value Decomposition-based Multiple Signal Classification (GSVD-MUSIC), a noise-robust algorithm, within the HARK platform, an open-source software suite. For a 60-channel microphone array, the proposed implementation achieves significant performance improvements. On the Jetson AGX Orin, an embedded device powered by an NVIDIA GPU and ARM Cortex-A78AE v8.2 64-bit CPUs, we observe speedups of 4645.1x for GSVD calculations and 8.8x for the SSL module, while speedups of 2223.4x for GSVD calculation and 8.95x for the entire SSL module on a server configured with an NVIDIA A100 GPU and AMD EPYC 7352 CPUs, making real-time processing feasible for large-scale microphone arrays and providing ample capacity for real-time processing of potential subsequent machine learning or deep learning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2504.03373v1),  [pdf](http://arxiv.org/pdf/2504.03373v1)

**Tags**: cs.SD cs.RO eess.AS 



### Sustainable LLM Inference for Edge AI: Evaluating Quantized LLMs for   Energy Efficiency, Output Accuracy, and Inference Latency
**Authors**: Erik Johannes Husom, Arda Goknil, Merve Astekin, Lwin Khin Shar, Andre Kåsen, Sagar Sen, Benedikt Andreas Mithassel, Ahmet Soylu

**Updated**: 2025-04-04T11:29:30Z

**Summary**: Deploying Large Language Models (LLMs) on edge devices presents significant challenges due to computational constraints, memory limitations, inference speed, and energy consumption. Model quantization has emerged as a key technique to enable efficient LLM inference by reducing model size and computational overhead. In this study, we conduct a comprehensive analysis of 28 quantized LLMs from the Ollama library, which applies by default Post-Training Quantization (PTQ) and weight-only quantization techniques, deployed on an edge device (Raspberry Pi 4 with 4GB RAM). We evaluate energy efficiency, inference performance, and output accuracy across multiple quantization levels and task types. Models are benchmarked on five standardized datasets (CommonsenseQA, BIG-Bench Hard, TruthfulQA, GSM8K, and HumanEval), and we employ a high-resolution, hardware-based energy measurement tool to capture real-world power consumption. Our findings reveal the trade-offs between energy efficiency, inference speed, and accuracy in different quantization settings, highlighting configurations that optimize LLM deployment for resource-constrained environments. By integrating hardware-level energy profiling with LLM benchmarking, this study provides actionable insights for sustainable AI, bridging a critical gap in existing research on energy-aware LLM deployment.

**Link**: [arxiv](http://arxiv.org/abs/2504.03360v1),  [pdf](http://arxiv.org/pdf/2504.03360v1)

**Tags**: cs.CY cs.AI cs.CL cs.LG 



### Detecting Stereotypes and Anti-stereotypes the Correct Way Using Social   Psychological Underpinnings
**Authors**: Kaustubh Shivshankar Shejole, Pushpak Bhattacharyya

**Updated**: 2025-04-04T11:14:38Z

**Summary**: Stereotypes are known to be highly pernicious, making their detection critically important. However, current research predominantly focuses on detecting and evaluating stereotypical biases in LLMs, leaving the study of stereotypes in its early stages. Many studies have failed to clearly distinguish between stereotypes and stereotypical biases, which has significantly slowed progress in advancing research in this area. Stereotype and anti-stereotype detection is a problem that requires knowledge of society; hence, it is one of the most difficult areas in Responsible AI. This work investigates this task, where we propose a four-tuple definition and provide precise terminology distinguishing stereotype, anti-stereotype, stereotypical bias, and bias, offering valuable insights into their various aspects. In this paper, we propose StereoDetect, a high-quality benchmarking dataset curated for this task by optimally utilizing current datasets such as StereoSet and WinoQueer, involving a manual verification process and the transfer of semantic information. We demonstrate that language models for reasoning with fewer than 10B parameters often get confused when detecting anti-stereotypes. We also demonstrate the critical importance of well-curated datasets by comparing our model with other current models for stereotype detection. The dataset and code is available at https://github.com/KaustubhShejole/StereoDetect.

**Link**: [arxiv](http://arxiv.org/abs/2504.03352v1),  [pdf](http://arxiv.org/pdf/2504.03352v1)

**Tags**: cs.CL cs.CY cs.HC 



### Talk2X -- An Open-Source Toolkit Facilitating Deployment of LLM-Powered   Chatbots on the Web
**Authors**: Lars Krupp, Daniel Geißler, Peter Hevesi, Marco Hirsch, Paul Lukowicz, Jakob Karolus

**Updated**: 2025-04-04T10:58:57Z

**Summary**: Integrated into websites, LLM-powered chatbots offer alternative means of navigation and information retrieval, leading to a shift in how users access information on the web. Yet, predominantly closed-sourced solutions limit proliferation among web hosts and suffer from a lack of transparency with regard to implementation details and energy efficiency. In this work, we propose our openly available agent Talk2X leveraging an adapted retrieval-augmented generation approach (RAG) combined with an automatically generated vector database, benefiting energy efficiency. Talk2X's architecture is generalizable to arbitrary websites offering developers a ready to use tool for integration. Using a mixed-methods approach, we evaluated Talk2X's usability by tasking users to acquire specific assets from an open science repository. Talk2X significantly improved task completion time, correctness, and user experience supporting users in quickly pinpointing specific information as compared to standard user-website interaction. Our findings contribute technical advancements to an ongoing paradigm shift of how we access information on the web.

**Link**: [arxiv](http://arxiv.org/abs/2504.03343v1),  [pdf](http://arxiv.org/pdf/2504.03343v1)

**Tags**: cs.AI cs.HC cs.IR 



### LLMs Prompted for Graphs: Hallucinations and Generative Capabilities
**Authors**: Gurvan Richardeau, Samy Chali, Erwan Le Merrer, Camilla Penzo, Gilles Tredan

**Updated**: 2025-04-04T10:58:40Z

**Summary**: Large Language Models (LLMs) are nowadays prompted for a wide variety of tasks. In this article, we investigate their ability in reciting and generating graphs. We first study the ability of LLMs to regurgitate well known graphs from the literature (e.g. Karate club or the graph atlas)4. Secondly, we question the generative capabilities of LLMs by asking for Erdos-Renyi random graphs. As opposed to the possibility that they could memorize some Erdos-Renyi graphs included in their scraped training set, this second investigation aims at studying a possible emergent property of LLMs. For both tasks, we propose a metric to assess their errors with the lens of hallucination (i.e. incorrect information returned as facts). We most notably find that the amplitude of graph hallucinations can characterize the superiority of some LLMs. Indeed, for the recitation task, we observe that graph hallucinations correlate with the Hallucination Leaderboard, a hallucination rank that leverages 10, 000 times more prompts to obtain its ranking. For the generation task, we find surprisingly good and reproducible results in most of LLMs. We believe this to constitute a starting point for more in-depth studies of this emergent capability and a challenging benchmark for their improvements. Altogether, these two aspects of LLMs capabilities bridge a gap between the network science and machine learning communities.

**Link**: [arxiv](http://arxiv.org/abs/2409.00159v3),  [pdf](http://arxiv.org/pdf/2409.00159v3)

**Tags**: cs.CL cs.AI cs.SI 



### EOOD: Entropy-based Out-of-distribution Detection
**Authors**: Guide Yang, Chao Hou, Weilong Peng, Xiang Fang, Yongwei Nie, Peican Zhu, Keke Tang

**Updated**: 2025-04-04T10:57:03Z

**Summary**: Deep neural networks (DNNs) often exhibit overconfidence when encountering out-of-distribution (OOD) samples, posing significant challenges for deployment. Since DNNs are trained on in-distribution (ID) datasets, the information flow of ID samples through DNNs inevitably differs from that of OOD samples. In this paper, we propose an Entropy-based Out-Of-distribution Detection (EOOD) framework. EOOD first identifies specific block where the information flow differences between ID and OOD samples are more pronounced, using both ID and pseudo-OOD samples. It then calculates the conditional entropy on the selected block as the OOD confidence score. Comprehensive experiments conducted across various ID and OOD settings demonstrate the effectiveness of EOOD in OOD detection and its superiority over state-of-the-art methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.03342v1),  [pdf](http://arxiv.org/pdf/2504.03342v1)

**Tags**: cs.CV cs.AI 



### BabyLM's First Words: Word Segmentation as a Phonological Probing Task
**Authors**: Zébulon Goriely

**Updated**: 2025-04-04T10:42:56Z

**Summary**: Language models provide a key framework for studying linguistic theories based on prediction, but phonological analysis using large language models (LLMs) is difficult; there are few phonological benchmarks beyond English and the standard input representation used in LLMs (subwords of graphemes) is not suitable for analyzing the representation of phonemes. In this work, we demonstrate how word segmentation can be used as a phonological probing task, allowing us to study the representations learned by phoneme-based language models trained on child-directed speech across 31 languages. Following computational models of word segmentation, we present unsupervised methods for extracting word boundaries from a trained model using the observation that prediction-error peaks at the start of words. We also use linear probes to identify that these models implicitly track word boundaries, even when they do not appear in training. This cross-lingual work corroborates statistical learning theories of acquisition and empirically motivates new methods for training subword tokenizers.

**Link**: [arxiv](http://arxiv.org/abs/2504.03338v1),  [pdf](http://arxiv.org/pdf/2504.03338v1)

**Tags**: cs.CL 



### Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User   Devices
**Authors**: Luís Couto Seller, Íñigo Sanz Torres, Adrián Vogel-Fernández, Carlos González Carballo, Pedro Miguel Sánchez Sánchez, Adrián Carruana Martín, Enrique de Miguel Ambite

**Updated**: 2025-04-04T09:47:58Z

**Summary**: Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance

**Link**: [arxiv](http://arxiv.org/abs/2504.03312v1),  [pdf](http://arxiv.org/pdf/2504.03312v1)

**Tags**: cs.CL cs.LG 



### Noise Augmented Fine Tuning for Mitigating Hallucinations in Large   Language Models
**Authors**: Afshin Khadangi, Amir Sartipi, Igor Tchappi, Ramin Bahmani

**Updated**: 2025-04-04T09:27:19Z

**Summary**: Large language models (LLMs) often produce inaccurate or misleading content-hallucinations. To address this challenge, we introduce Noise-Augmented Fine-Tuning (NoiseFiT), a novel framework that leverages adaptive noise injection based on the signal-to-noise ratio (SNR) to enhance model robustness. In particular, NoiseFiT selectively perturbs layers identified as either high-SNR (more robust) or low-SNR (potentially under-regularized) using a dynamically scaled Gaussian noise. We further propose a hybrid loss that combines standard cross-entropy, soft cross-entropy, and consistency regularization to ensure stable and accurate outputs under noisy training conditions. Our theoretical analysis shows that adaptive noise injection is both unbiased and variance-preserving, providing strong guarantees for convergence in expectation. Empirical results on multiple test and benchmark datasets demonstrate that NoiseFiT significantly reduces hallucination rates, often improving or matching baseline performance in key tasks. These findings highlight the promise of noise-driven strategies for achieving robust, trustworthy language modeling without incurring prohibitive computational overhead. Given the comprehensive and detailed nature of our experiments, we have publicly released the fine-tuning logs, benchmark evaluation artifacts, and source code online at W&B, Hugging Face, and GitHub, respectively, to foster further research, accessibility and reproducibility.

**Link**: [arxiv](http://arxiv.org/abs/2504.03302v1),  [pdf](http://arxiv.org/pdf/2504.03302v1)

**Tags**: cs.CL cs.AI 



### Stance-Driven Multimodal Controlled Statement Generation: New Dataset   and Task
**Authors**: Bingqian Wang, Quan Fang, Jiachen Sun, Xiaoxiao Ma

**Updated**: 2025-04-04T09:20:19Z

**Summary**: Formulating statements that support diverse or controversial stances on specific topics is vital for platforms that enable user expression, reshape political discourse, and drive social critique and information dissemination. With the rise of Large Language Models (LLMs), controllable text generation towards specific stances has become a promising research area with applications in shaping public opinion and commercial marketing. However, current datasets often focus solely on pure texts, lacking multimodal content and effective context, particularly in the context of stance detection. In this paper, we formally define and study the new problem of stance-driven controllable content generation for tweets with text and images, where given a multimodal post (text and image/video), a model generates a stance-controlled response. To this end, we create the Multimodal Stance Generation Dataset (StanceGen2024), the first resource explicitly designed for multimodal stance-controllable text generation in political discourse. It includes posts and user comments from the 2024 U.S. presidential election, featuring text, images, videos, and stance annotations to explore how multimodal political content shapes stance expression. Furthermore, we propose a Stance-Driven Multimodal Generation (SDMG) framework that integrates weighted fusion of multimodal features and stance guidance to improve semantic consistency and stance control. We release the dataset and code (https://anonymous.4open.science/r/StanceGen-BE9D) for public use and further research.

**Link**: [arxiv](http://arxiv.org/abs/2504.03295v1),  [pdf](http://arxiv.org/pdf/2504.03295v1)

**Tags**: cs.CL cs.AI 



### Do Large Language Models Solve the Problems of Agent-Based Modeling? A   Critical Review of Generative Social Simulations
**Authors**: Maik Larooij, Petter Törnberg

**Updated**: 2025-04-04T08:48:43Z

**Summary**: Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as the integration of Large Language Models (LLMs) has led to the emergence of ``generative ABMs'' as a novel approach to simulating social systems. While ABMs offer means to bridge micro-level interactions with macro-level patterns, they have long faced criticisms from social scientists, pointing to e.g., lack of realism, computational complexity, and challenges of calibrating and validating against empirical data. This paper reviews the generative ABM literature to assess how this new approach adequately addresses these long-standing criticisms. Our findings show that studies show limited awareness of historical debates. Validation remains poorly addressed, with many studies relying solely on subjective assessments of model `believability', and even the most rigorous validation failing to adequately evidence operational validity. We argue that there are reasons to believe that LLMs will exacerbate rather than resolve the long-standing challenges of ABMs. The black-box nature of LLMs moreover limit their usefulness for disentangling complex emergent causal mechanisms. While generative ABMs are still in a stage of early experimentation, these findings question of whether and how the field can transition to the type of rigorous modeling needed to contribute to social scientific theory.

**Link**: [arxiv](http://arxiv.org/abs/2504.03274v1),  [pdf](http://arxiv.org/pdf/2504.03274v1)

**Tags**: cs.MA cs.AI 



### Ichigo: Mixed-Modal Early-Fusion Realtime Voice Assistant
**Authors**: Alan Dao, Dinh Bach Vu, Huy Hoang Ha

**Updated**: 2025-04-04T08:29:19Z

**Summary**: Large Language Models (LLMs) have revolutionized natural language processing, but their application to speech-based tasks remains challenging due to the complexities of integrating audio and text modalities. This paper introduces Ichigo, a mixed-modal model that seamlessly processes interleaved sequences of speech and text. Utilizing a tokenized early-fusion approach, Ichigo quantizes speech into discrete tokens and employs a uniform transformer-based architecture for both speech and text modalities. This method enables joint reasoning and generation across modalities without the need for separate adapters. We present a comprehensive training methodology, including pre-training on multilingual speech recognition datasets and fine-tuning on a curated instruction dataset. Ichigo demonstrates state-of-the-art performance on speech question-answering benchmarks, outperforming existing open-source speech language models and achieving comparable results to cascaded systems. Notably, Ichigo exhibits a latency of just 111 ms to first token generation, significantly lower than current models. Our approach not only advances the field of multimodal AI but also provides a framework for smaller research teams to contribute effectively to open-source speech-language models.

**Link**: [arxiv](http://arxiv.org/abs/2410.15316v3),  [pdf](http://arxiv.org/pdf/2410.15316v3)

**Tags**: cs.CL cs.SD eess.AS 



### Inherent and emergent liability issues in LLM-based agentic systems: a   principal-agent perspective
**Authors**: Garry A. Gabison, R. Patrick Xian

**Updated**: 2025-04-04T08:10:02Z

**Summary**: Agentic systems powered by large language models (LLMs) are becoming progressively more complex and capable. Their increasing agency and expanding deployment settings attract growing attention over effective governance policies, monitoring and control protocols. Based on emerging landscapes of the agentic market, we analyze the potential liability issues stemming from delegated use of LLM agents and their extended systems from a principal-agent perspective. Our analysis complements existing risk-based studies on artificial agency and covers the spectrum of important aspects of the principal-agent relationship and their potential consequences at deployment. Furthermore, we motivate method developments for technical governance along the directions of interpretability and behavior evaluations, reward and conflict management, and the mitigation of misalignment and misconduct through principled engineering of detection and fail-safe mechanisms. By illustrating the outstanding issues in AI liability for LLM-based agentic systems, we aim to inform the system design, auditing and monitoring approaches to enhancing transparency and accountability.

**Link**: [arxiv](http://arxiv.org/abs/2504.03255v1),  [pdf](http://arxiv.org/pdf/2504.03255v1)

**Tags**: cs.CY cs.CL cs.MA 



### Why do LLMs attend to the first token?
**Authors**: Federico Barbero, Álvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veličković, Razvan Pascanu

**Updated**: 2025-04-04T07:41:19Z

**Summary**: Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.

**Link**: [arxiv](http://arxiv.org/abs/2504.02732v2),  [pdf](http://arxiv.org/pdf/2504.02732v2)

**Tags**: cs.CL 



### Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted   Decision-Making
**Authors**: Zelun Tony Zhang, Leon Reicherts

**Updated**: 2025-04-04T06:40:03Z

**Summary**: How can we use generative AI to design tools that augment rather than replace human cognition? In this position paper, we review our own research on AI-assisted decision-making for lessons to learn. We observe that in both AI-assisted decision-making and generative AI, a popular approach is to suggest AI-generated end-to-end solutions to users, which users can then accept, reject, or edit. Alternatively, AI tools could offer more incremental support to help users solve tasks themselves, which we call process-oriented support. We describe findings on the challenges of end-to-end solutions, and how process-oriented support can address them. We also discuss the applicability of these findings to generative AI based on a recent study in which we compared both approaches to assist users in a complex decision-making task with LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2504.03207v1),  [pdf](http://arxiv.org/pdf/2504.03207v1)

**Tags**: cs.HC cs.AI 



### Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward
**Authors**: Yanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, Natasha Jaques

**Updated**: 2025-04-04T06:35:02Z

**Summary**: Effective conversational agents must be able to personalize their behavior to suit a user's preferences, personality, and attributes, whether they are assisting with writing tasks or operating in domains like education or healthcare. Current training methods like Reinforcement Learning from Human Feedback (RLHF) prioritize helpfulness and safety but fall short in fostering truly empathetic, adaptive, and personalized interactions. Traditional approaches to personalization often rely on extensive user history, limiting their effectiveness for new or context-limited users. To overcome these limitations, we propose to incorporate an intrinsic motivation to improve the conversational agents's model of the user as an additional reward alongside multi-turn RLHF. This reward mechanism encourages the agent to actively elicit user traits by optimizing conversations to increase the accuracy of its user model. Consequently, the policy agent can deliver more personalized interactions through obtaining more information about the user. We applied our method both education and fitness settings, where LLMs teach concepts or recommend personalized strategies based on users' hidden learning style or lifestyle attributes. Using LLM-simulated users, our approach outperformed a multi-turn RLHF baseline in revealing information about the users' preferences, and adapting to them.

**Link**: [arxiv](http://arxiv.org/abs/2504.03206v1),  [pdf](http://arxiv.org/pdf/2504.03206v1)

**Tags**: cs.CL cs.AI 



### Natural Multimodal Fusion-Based Human-Robot Interaction: Application   With Voice and Deictic Posture via Large Language Model
**Authors**: Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Atmaraaj Gopal, Arihiro Yorita, Naoyuki Kubota, Matthias Rätsch

**Updated**: 2025-04-04T06:14:36Z

**Summary**: Translating human intent into robot commands is crucial for the future of service robots in an aging society. Existing Human-Robot Interaction (HRI) systems relying on gestures or verbal commands are impractical for the elderly due to difficulties with complex syntax or sign language. To address the challenge, this paper introduces a multi-modal interaction framework that combines voice and deictic posture information to create a more natural HRI system. The visual cues are first processed by the object detection model to gain a global understanding of the environment, and then bounding boxes are estimated based on depth information. By using a large language model (LLM) with voice-to-text commands and temporally aligned selected bounding boxes, robot action sequences can be generated, while key control syntax constraints are applied to avoid potential LLM hallucination issues. The system is evaluated on real-world tasks with varying levels of complexity using a Universal Robots UR3e manipulator. Our method demonstrates significantly better performance in HRI in terms of accuracy and robustness. To benefit the research community and the general public, we will make our code and design open-source.

**Link**: [arxiv](http://arxiv.org/abs/2501.00785v3),  [pdf](http://arxiv.org/pdf/2501.00785v3)

**Tags**: cs.RO 



### A Large-Scale Simulation on Large Language Models for Decision-Making in   Political Science
**Authors**: Chenxiao Yu, Jinyi Ye, Yuangang Li, Zhaotian Weng, Zheng Li, Emilio Ferrara, Xiyang Hu, Yue Zhao

**Updated**: 2025-04-04T06:11:55Z

**Summary**: While LLMs have demonstrated remarkable capabilities in text generation and reasoning, their ability to simulate human decision-making -- particularly in political contexts -- remains an open question. However, modeling voter behavior presents unique challenges due to limited voter-level data, evolving political landscapes, and the complexity of human reasoning. In this study, we develop a theory-driven, multi-step reasoning framework that integrates demographic, temporal and ideological factors to simulate voter decision-making at scale. Using synthetic personas calibrated to real-world voter data, we conduct large-scale simulations of recent U.S. presidential elections. Our method significantly improves simulation accuracy while mitigating model biases. We examine its robustness by comparing performance across different LLMs. We further investigate the challenges and constraints that arise from LLM-based political simulations. Our work provides both a scalable framework for modeling political decision-making behavior and insights into the promise and limitations of using LLMs in political science research.

**Link**: [arxiv](http://arxiv.org/abs/2412.15291v3),  [pdf](http://arxiv.org/pdf/2412.15291v3)

**Tags**: cs.CL cs.SI 



### Explain with Visual Keypoints Like a Real Mentor! A Benchmark for   Multimodal Solution Explanation
**Authors**: Jaewoo Park, Jungyang Park, Dongju Jang, Jiwan Chung, Byungwoo Yoo, Jaewoo Shin, Seonjoon Park, Taehyeong Kim, Youngjae Yu

**Updated**: 2025-04-04T06:03:13Z

**Summary**: With the rapid advancement of mathematical reasoning capabilities in large language models (LLMs), AI systems are increasingly being adopted in educational settings to support students' comprehension of problem-solving processes. However, a critical component remains underexplored in current LLM-generated explanations: visual explanation. In real-world instructional contexts, human tutors routinely employ visual aids-such as diagrams, markings, and highlights-to enhance conceptual clarity. To bridge this gap, we introduce a novel task of visual solution explanation, which requires not only solving problems but also generating explanations that incorporate newly introduced visual elements essential for understanding (e.g., auxiliary lines, annotations, or geometric constructions). To evaluate model performance on this task, we propose MathExplain, a multimodal benchmark consisting of 997 math problems annotated with visual keypoints and corresponding explanatory text that references those elements. Our empirical results show that while some closed-source models demonstrate promising capabilities on visual solution-explaining, current open-source general-purpose models perform inconsistently, particularly in identifying relevant visual components and producing coherent keypoint-based explanations. We expect that visual solution-explaining and the MathExplain dataset will catalyze further research on multimodal LLMs in education and advance their deployment as effective, explanation-oriented AI tutors. Code and data will be released publicly.

**Link**: [arxiv](http://arxiv.org/abs/2504.03197v1),  [pdf](http://arxiv.org/pdf/2504.03197v1)

**Tags**: cs.CL 



### PromptGuard: Soft Prompt-Guided Unsafe Content Moderation for   Text-to-Image Models
**Authors**: Lingzhi Yuan, Xiaojun Jia, Yihao Huang, Wei Dong, Yang Liu

**Updated**: 2025-04-04T05:56:04Z

**Summary**: Text-to-image (T2I) models have been shown to be vulnerable to misuse, particularly in generating not-safe-for-work (NSFW) content, raising serious ethical concerns. In this work, we present PromptGuard, a novel content moderation technique that draws inspiration from the system prompt mechanism in large language models (LLMs) for safety alignment. Unlike LLMs, T2I models lack a direct interface for enforcing behavioral guidelines. Our key idea is to optimize a safety soft prompt that functions as an implicit system prompt within the T2I model's textual embedding space. This universal soft prompt (P*) directly moderates NSFW inputs, enabling safe yet realistic image generation without altering the inference efficiency or requiring proxy models. Extensive experiments across three datasets demonstrate that PromptGuard effectively mitigates NSFW content generation while preserving high-quality benign outputs. PromptGuard achieves 7.8 times faster than prior content moderation methods, surpassing eight state-of-the-art defenses with an optimal unsafe ratio down to 5.84%.

**Link**: [arxiv](http://arxiv.org/abs/2501.03544v2),  [pdf](http://arxiv.org/pdf/2501.03544v2)

**Tags**: cs.CV cs.AI cs.CR 



### How Do Teachers Create Pedagogical Chatbots?: Current Practices and   Challenges
**Authors**: Minju Yoo, Hyoungwook Jin, Juho Kim

**Updated**: 2025-04-04T05:49:11Z

**Summary**: AI chatbots have emerged as promising educational tools for personalized learning experiences, with advances in large language models (LLMs) enabling teachers to create and customize these chatbots for their specific classroom needs. However, there is a limited understanding of how teachers create pedagogical chatbots and integrate them into their lessons. Through semi-structured interviews with seven K-12 teachers, we examined their practices and challenges when designing, implementing, and deploying chatbots. Our findings revealed that teachers prioritize developing task-specific chatbots aligned with their lessons. Teachers engaged in various creation practices and had different challenges; novices in chatbot creation struggled mainly with initial design and technical implementation, while experienced teachers faced challenges with technical aspects and analyzing conversational data. Based on these insights, we explore approaches to supporting teachers' chatbot development and opportunities for designing future chatbot creation systems. This work provides foundational insights from teachers that can empower teacher-created chatbots, facilitating AI-augmented teaching.

**Link**: [arxiv](http://arxiv.org/abs/2503.00967v2),  [pdf](http://arxiv.org/pdf/2503.00967v2)

**Tags**: cs.HC 



### Learning Natural Language Constraints for Safe Reinforcement Learning of   Language Agents
**Authors**: Jaymari Chua, Chen Wang, Lina Yao

**Updated**: 2025-04-04T05:26:28Z

**Summary**: Generalizable alignment is a core challenge for deploying Large Language Models (LLMs) safely in real-world NLP applications. Current alignment methods, including Reinforcement Learning from Human Feedback (RLHF), often fail to guarantee constraint satisfaction outside their training distribution due to their reliance on implicit, post-hoc preferences. Inspired by a paradigm shift to first curate data before tuning, we introduce a new framework for safe language alignment that learns natural language constraints from positive and negative demonstrations as a primary step. From inferring both a task-specific reward function and latent constraint functions, our approach fosters adaptation to novel safety requirements and robust generalization under domain shifts and adversarial inputs. We formalize the framework within a Constrained Markov Decision Process (CMDP) and validate it via a text-based navigation environment, demonstrating safe adaptation to changing danger zones. Our experiments show fewer violations upon domain shift when following a safe navigation path, and we achieve zero violations by applying learned constraints to a distilled BERT model as a fine-tuning technique. This work offers a promising path toward building safety-critical and more generalizable LLMs for practical NLP settings.

**Link**: [arxiv](http://arxiv.org/abs/2504.03185v1),  [pdf](http://arxiv.org/pdf/2504.03185v1)

**Tags**: cs.CL cs.AI I.2.7; I.2.4; I.2.6; I.2.8 



### Multi-lingual Multi-turn Automated Red Teaming for LLMs
**Authors**: Abhishek Singhania, Christophe Dupuy, Shivam Mangale, Amani Namboori

**Updated**: 2025-04-04T05:06:12Z

**Summary**: Language Model Models (LLMs) have improved dramatically in the past few years, increasing their adoption and the scope of their capabilities over time. A significant amount of work is dedicated to ``model alignment'', i.e., preventing LLMs to generate unsafe responses when deployed into customer-facing applications. One popular method to evaluate safety risks is \textit{red-teaming}, where agents attempt to bypass alignment by crafting elaborate prompts that trigger unsafe responses from a model. Standard human-driven red-teaming is costly, time-consuming and rarely covers all the recent features (e.g., multi-lingual, multi-modal aspects), while proposed automation methods only cover a small subset of LLMs capabilities (i.e., English or single-turn). We present Multi-lingual Multi-turn Automated Red Teaming (\textbf{MM-ART}), a method to fully automate conversational, multi-lingual red-teaming operations and quickly identify prompts leading to unsafe responses. Through extensive experiments on different languages, we show the studied LLMs are on average 71\% more vulnerable after a 5-turn conversation in English than after the initial turn. For conversations in non-English languages, models display up to 195\% more safety vulnerabilities than the standard single-turn English approach, confirming the need for automated red-teaming methods matching LLMs capabilities.

**Link**: [arxiv](http://arxiv.org/abs/2504.03174v1),  [pdf](http://arxiv.org/pdf/2504.03174v1)

**Tags**: cs.CL 



### Strategizing with AI: Insights from a Beauty Contest Experiment
**Authors**: Iuliia Alekseenko, Dmitry Dagaev, Sofia Paklina, Petr Parshakov

**Updated**: 2025-04-04T05:01:08Z

**Summary**: A beauty contest is a wide class of games of guessing the most popular strategy among other players. In particular, guessing a fraction of a mean of numbers chosen by all players is a classic behavioral experiment designed to test iterative reasoning patterns among various groups of people. The previous literature reveals that the level of sophistication of the opponents is an important factor affecting the outcome of the game. Smarter decision makers choose strategies that are closer to theoretical Nash equilibrium and demonstrate faster convergence to equilibrium in iterated contests with information revelation. We replicate a series of classic experiments by running virtual experiments with modern large language models (LLMs) who play against various groups of virtual players. We test how advanced the LLMs' behavior is compared to the behavior of human players. We show that LLMs typically take into account the opponents' level of sophistication and adapt by changing the strategy. In various settings, most LLMs (with the exception of Llama) are more sophisticated and play lower numbers compared to human players. Our results suggest that LLMs (except Llama) are rather successful in identifying the underlying strategic environment and adopting the strategies to the changing set of parameters of the game in the same way that human players do. All LLMs still fail to play dominant strategies in a two-player game. Our results contribute to the discussion on the accuracy of modeling human economic agents by artificial intelligence.

**Link**: [arxiv](http://arxiv.org/abs/2502.03158v2),  [pdf](http://arxiv.org/pdf/2502.03158v2)

**Tags**: econ.GN q-fin.EC 



### RingMoE: Mixture-of-Modality-Experts Multi-Modal Foundation Models for   Universal Remote Sensing Image Interpretation
**Authors**: Hanbo Bi, Yingchao Feng, Boyuan Tong, Mengyu Wang, Haichen Yu, Yongqiang Mao, Hao Chang, Wenhui Diao, Peijin Wang, Yue Yu, Hanyang Peng, Yehong Zhang, Kun Fu, Xian Sun

**Updated**: 2025-04-04T04:47:54Z

**Summary**: The rapid advancement of foundation models has revolutionized visual representation learning in a self-supervised manner. However, their application in remote sensing (RS) remains constrained by a fundamental gap: existing models predominantly handle single or limited modalities, overlooking the inherently multi-modal nature of RS observations. Optical, synthetic aperture radar (SAR), and multi-spectral data offer complementary insights that significantly reduce the inherent ambiguity and uncertainty in single-source analysis. To bridge this gap, we introduce RingMoE, a unified multi-modal RS foundation model with 14.7 billion parameters, pre-trained on 400 million multi-modal RS images from nine satellites. RingMoE incorporates three key innovations: (1) A hierarchical Mixture-of-Experts (MoE) architecture comprising modal-specialized, collaborative, and shared experts, effectively modeling intra-modal knowledge while capturing cross-modal dependencies to mitigate conflicts between modal representations; (2) Physics-informed self-supervised learning, explicitly embedding sensor-specific radiometric characteristics into the pre-training objectives; (3) Dynamic expert pruning, enabling adaptive model compression from 14.7B to 1B parameters while maintaining performance, facilitating efficient deployment in Earth observation applications. Evaluated across 23 benchmarks spanning six key RS tasks (i.e., classification, detection, segmentation, tracking, change detection, and depth estimation), RingMoE outperforms existing foundation models and sets new SOTAs, demonstrating remarkable adaptability from single-modal to multi-modal scenarios. Beyond theoretical progress, it has been deployed and trialed in multiple sectors, including emergency response, land management, marine sciences, and urban planning.

**Link**: [arxiv](http://arxiv.org/abs/2504.03166v1),  [pdf](http://arxiv.org/pdf/2504.03166v1)

**Tags**: cs.CV 



### Efficient Dynamic Clustering-Based Document Compression for   Retrieval-Augmented-Generation
**Authors**: Weitao Li, Kaiming Liu, Xiangyu Zhang, Xuanyu Lei, Weizhi Ma, Yang Liu

**Updated**: 2025-04-04T04:43:13Z

**Summary**: Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach for knowledge integration during large language model (LLM) inference in recent years. However, current RAG implementations face challenges in effectively addressing noise, repetition and redundancy in retrieved content, primarily due to their limited ability to exploit fine-grained inter-document relationships. To address these limitations, we propose an \textbf{E}fficient \textbf{D}ynamic \textbf{C}lustering-based document \textbf{C}ompression framework (\textbf{EDC\textsuperscript{2}-RAG}) that effectively utilizes latent inter-document relationships while simultaneously removing irrelevant information and redundant content. We validate our approach, built upon GPT-3.5, on widely used knowledge-QA and hallucination-detected datasets. The results show that this method achieves consistent performance improvements across various scenarios and experimental settings, demonstrating strong robustness and applicability. Our code and datasets can be found at https://github.com/Tsinghua-dhy/EDC-2-RAG.

**Link**: [arxiv](http://arxiv.org/abs/2504.03165v1),  [pdf](http://arxiv.org/pdf/2504.03165v1)

**Tags**: cs.CL 



### DeepResearcher: Scaling Deep Research via Reinforcement Learning in   Real-world Environments
**Authors**: Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, Pengfei Liu

**Updated**: 2025-04-04T04:41:28Z

**Summary**: Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.

**Link**: [arxiv](http://arxiv.org/abs/2504.03160v1),  [pdf](http://arxiv.org/pdf/2504.03160v1)

**Tags**: cs.AI cs.CL cs.LG 



### sudo rm -rf agentic_security
**Authors**: Sejin Lee, Jian Kim, Haon Park, Ashkan Yousefpour, Sangyoon Yu, Min Song

**Updated**: 2025-04-04T04:36:20Z

**Summary**: Large Language Models (LLMs) are increasingly deployed as computer-use agents, autonomously performing tasks within real desktop or web environments. While this evolution greatly expands practical use cases for humans, it also creates serious security exposures. We present SUDO (Screen-based Universal Detox2Tox Offense), a novel attack framework that systematically bypasses refusal trained safeguards in commercial computer-use agents, such as Claude Computer Use. The core mechanism, Detox2Tox, transforms harmful requests (that agents initially reject) into seemingly benign requests via detoxification, secures detailed instructions from advanced vision language models (VLMs), and then reintroduces malicious content via toxification just before execution. Unlike conventional jailbreaks, SUDO iteratively refines its attacks based on a built-in refusal feedback, making it increasingly effective against robust policy filters. In extensive tests spanning 50 real-world tasks and multiple state-of-the-art VLMs, SUDO achieves a stark attack success rate of 24% (with no refinement), and up to 41% (by its iterative refinement) in Claude Computer Use. By revealing these vulnerabilities and demonstrating the ease with which they can be exploited in real-world computing environments, this paper highlights an immediate need for robust, context-aware safeguards. WARNING: This paper includes harmful or offensive model outputs Our code is available at: https://github.com/AIM-Intelligence/SUDO.git

**Link**: [arxiv](http://arxiv.org/abs/2503.20279v2),  [pdf](http://arxiv.org/pdf/2503.20279v2)

**Tags**: cs.CL cs.AI cs.CR 



### AMP4EC: Adaptive Model Partitioning Framework for Efficient Deep   Learning Inference in Edge Computing Environments
**Authors**: Guilin Zhang, Wulan Guo, Ziqi Tan, Hailong Jiang

**Updated**: 2025-04-04T04:28:20Z

**Summary**: Edge computing facilitates deep learning in resource-constrained environments, but challenges such as resource heterogeneity and dynamic constraints persist. This paper introduces AMP4EC, an Adaptive Model Partitioning framework designed to optimize deep learning inference in edge environments through real-time resource monitoring, dynamic model partitioning, and adaptive task scheduling. AMP4EC features a resource-aware model partitioner that splits deep learning models based on device capabilities, a task scheduler that ensures efficient load balancing using a weighted scoring mechanism, and a Docker-based deployment environment for validation. Experimental results show up to a 78% reduction in latency and a 414% improvement in throughput compared to baseline methods. The framework achieves consistent performance with low scheduling overhead across varying resource profiles, demonstrating adaptability in high-resource (1 CPU, 1GB RAM) and low-resource (0.4 CPU, 512MB RAM) scenarios. These results highlight AMP4EC's scalability, efficiency, and robustness for real-world edge deployments, addressing the critical need for efficient distributed inference in dynamic, resource-constrained environments.

**Link**: [arxiv](http://arxiv.org/abs/2504.00407v2),  [pdf](http://arxiv.org/pdf/2504.00407v2)

**Tags**: cs.DC I.2.11; I.2.6; C.2.4 



### Synthesizing Optimal Object Selection Predicates for Image Editing using   Lattices
**Authors**: Yang He, Xiaoyu Liu, Yuepeng Wang

**Updated**: 2025-04-04T04:25:14Z

**Summary**: Image editing is a common task across a wide range of domains, from personal use to professional applications. Despite advances in computer vision, current tools still demand significant manual effort for editing tasks that require repetitive operations on images with many objects. In this paper, we present a novel approach to automating the image editing process using program synthesis. We propose a new algorithm based on lattice structures to automatically synthesize object selection predicates for image editing from positive and negative examples. By leveraging the algebraic properties of lattices, our algorithm efficiently synthesizes an optimal object selection predicate among multiple correct solutions. We have implemented our technique and evaluated it on 100 tasks over 20 images. The evaluation result demonstrates our tool is effective and efficient, which outperforms state-of-the-art synthesizers and LLM-based approaches.

**Link**: [arxiv](http://arxiv.org/abs/2504.03155v1),  [pdf](http://arxiv.org/pdf/2504.03155v1)

**Tags**: cs.PL 



### TokenFLEX: Unified VLM Training for Flexible Visual Tokens Inference
**Authors**: Junshan Hu, Jialiang Mao, Zhikang Liu, Zhongpu Xia, Peng Jia, Xianpeng Lang

**Updated**: 2025-04-04T04:24:29Z

**Summary**: Conventional Vision-Language Models(VLMs) typically utilize a fixed number of vision tokens, regardless of task complexity. This one-size-fits-all strategy introduces notable inefficiencies: using excessive tokens leads to unnecessary computational overhead in simpler tasks, whereas insufficient tokens compromise fine-grained visual comprehension in more complex contexts. To overcome these limitations, we present TokenFLEX, an innovative and adaptable vision-language framework that encodes images into a variable number of tokens for efficient integration with a Large Language Model (LLM). Our approach is underpinned by two pivotal innovations. Firstly, we present a novel training paradigm that enhances performance across varying numbers of vision tokens by stochastically modulating token counts during training. Secondly, we design a lightweight vision token projector incorporating an adaptive pooling layer and SwiGLU, allowing for flexible downsampling of vision tokens and adaptive selection of features tailored to specific token counts. Comprehensive experiments reveal that TokenFLEX consistently outperforms its fixed-token counterparts, achieving notable performance gains across various token counts enhancements of 1.6%, 1.0%, and 0.4% with 64, 144, and 256 tokens, respectively averaged over eight vision-language benchmarks. These results underscore TokenFLEX's remarkable flexibility while maintaining high-performance vision-language understanding.

**Link**: [arxiv](http://arxiv.org/abs/2504.03154v1),  [pdf](http://arxiv.org/pdf/2504.03154v1)

**Tags**: cs.CV 



### Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning   (v1)
**Authors**: Jing Bi, Susan Liang, Xiaofei Zhou, Pinxin Liu, Junjia Guo, Yunlong Tang, Luchuan Song, Chao Huang, Guangyu Sun, Jinxi He, Jiarui Wu, Shu Yang, Daoan Zhang, Chen Chen, Lianggong Bruce Wen, Zhang Liu, Jiebo Luo, Chenliang Xu

**Updated**: 2025-04-04T04:04:56Z

**Summary**: Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.

**Link**: [arxiv](http://arxiv.org/abs/2504.03151v1),  [pdf](http://arxiv.org/pdf/2504.03151v1)

**Tags**: cs.CL cs.LG 



### A Human Digital Twin Architecture for Knowledge-based Interactions and   Context-Aware Conversations
**Authors**: Abdul Mannan Mohammed, Azhar Ali Mohammad, Jason A. Ortiz, Carsten Neumann, Grace Bochenek, Dirk Reiners, Carolina Cruz-Neira

**Updated**: 2025-04-04T03:56:26Z

**Summary**: Recent developments in Artificial Intelligence (AI) and Machine Learning (ML) are creating new opportunities for Human-Autonomy Teaming (HAT) in tasks, missions, and continuous coordinated activities. A major challenge is enabling humans to maintain awareness and control over autonomous assets, while also building trust and supporting shared contextual understanding. To address this, we present a real-time Human Digital Twin (HDT) architecture that integrates Large Language Models (LLMs) for knowledge reporting, answering, and recommendation, embodied in a visual interface.   The system applies a metacognitive approach to enable personalized, context-aware responses aligned with the human teammate's expectations. The HDT acts as a visually and behaviorally realistic team member, integrated throughout the mission lifecycle, from training to deployment to after-action review. Our architecture includes speech recognition, context processing, AI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We describe the system design, performance metrics, and future development directions for more adaptive and realistic HAT systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.03147v1),  [pdf](http://arxiv.org/pdf/2504.03147v1)

**Tags**: cs.HC cs.AI 



### One-Shot Heterogeneous Federated Learning with Local Model-Guided   Diffusion Models
**Authors**: Mingzhao Yang, Shangchao Su, Bin Li, Xiangyang Xue

**Updated**: 2025-04-04T03:46:28Z

**Summary**: In recent years, One-shot Federated Learning methods based on Diffusion Models have garnered increasing attention due to their remarkable performance. However, most of these methods require the deployment of foundation models on client devices, which significantly raises the computational requirements and reduces their adaptability to heterogeneous client models compared to traditional FL methods. In this paper, we propose FedLMG, a heterogeneous one-shot Federated learning method with Local Model-Guided diffusion models. Briefly speaking, in FedLMG, clients do not need access to any foundation models but only train and upload their local models, which is consistent with traditional FL methods. On the clients, we employ classification loss and BN loss to capture the broad category features and detailed contextual features of the client distributions. On the server, based on the uploaded client models, we utilize backpropagation to guide the server's DM in generating synthetic datasets that comply with the client distributions, which are then used to train the aggregated model. By using the locally trained client models as a medium to transfer client knowledge, our method significantly reduces the computational requirements on client devices and effectively adapts to scenarios with heterogeneous clients. Extensive quantitation and visualization experiments on three large-scale real-world datasets, along with theoretical analysis, demonstrate that the synthetic datasets generated by FedLMG exhibit comparable quality and diversity to the client datasets, which leads to an aggregated model that outperforms all compared methods and even the performance ceiling, further elucidating the significant potential of utilizing DMs in FL.

**Link**: [arxiv](http://arxiv.org/abs/2311.08870v3),  [pdf](http://arxiv.org/pdf/2311.08870v3)

**Tags**: cs.CV cs.LG 



### See-Through Face Display for DHH People: Enhancing Gaze Awareness in   Remote Sign Language Conversations with Camera-Behind Displays
**Authors**: Kazuya Izumi, Akihisa Shitara, Yoichi Ochiai

**Updated**: 2025-04-04T03:34:05Z

**Summary**: This paper presents a sign language conversation system based on the See-Through Face Display to address the challenge of maintaining eye contact in remote sign language interactions. A camera positioned behind a transparent display allows users to look at the face of their conversation partner while appearing to maintain direct eye contact. Unlike conventional methods that rely on software-based gaze correction or large-scale half-mirror setups, this design reduces visual distortions and simplifies installation. We implemented and evaluated a videoconferencing system that integrates See-Through Face Display, comparing it to traditional videoconferencing methods. We explore its potential applications for Deaf and Hard of Hearing (DHH), including multi-party sign language conversations, corpus collection, remote interpretation, and AI-driven sign language avatars. Collaboration with DHH communities will be key to refining the system for real-world use and ensuring its practical deployment.

**Link**: [arxiv](http://arxiv.org/abs/2504.03141v1),  [pdf](http://arxiv.org/pdf/2504.03141v1)

**Tags**: cs.HC 



### Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at   Pixel Level
**Authors**: Andong Deng, Tongjia Chen, Shoubin Yu, Taojiannan Yang, Lincoln Spencer, Yapeng Tian, Ajmal Saeed Mian, Mohit Bansal, Chen Chen

**Updated**: 2025-04-04T03:20:03Z

**Summary**: In this paper, we introduce Motion-Grounded Video Reasoning, a new motion understanding task that requires generating visual answers (video segmentation masks) according to the input question, and hence needs implicit spatiotemporal reasoning and grounding. This task extends existing spatiotemporal grounding work focusing on explicit action/motion grounding, to a more general format by enabling implicit reasoning via questions. To facilitate the development of the new task, we collect a large-scale dataset called GROUNDMORE, which comprises 1,715 video clips, 249K object masks that are deliberately designed with 4 question types (Causal, Sequential, Counterfactual, and Descriptive) for benchmarking deep and comprehensive motion reasoning abilities. GROUNDMORE uniquely requires models to generate visual answers, providing a more concrete and visually interpretable response than plain texts. It evaluates models on both spatiotemporal grounding and reasoning, fostering to address complex challenges in motion-related video reasoning, temporal perception, and pixel-level understanding. Furthermore, we introduce a novel baseline model named Motion-Grounded Video Reasoning Assistant (MORA). MORA incorporates the multimodal reasoning ability from the Multimodal LLM, the pixel-level perception capability from the grounding model (SAM), and the temporal perception ability from a lightweight localization head. MORA achieves respectable performance on GROUNDMORE outperforming the best existing visual grounding baseline model by an average of 21.5% relatively. We hope this novel and challenging task will pave the way for future advancements in robust and general motion understanding via video reasoning segmentation

**Link**: [arxiv](http://arxiv.org/abs/2411.09921v2),  [pdf](http://arxiv.org/pdf/2411.09921v2)

**Tags**: cs.CV cs.AI 



### LightPROF: A Lightweight Reasoning Framework for Large Language Model on   Knowledge Graph
**Authors**: Tu Ao, Yanhua Yu, Yuling Wang, Yang Deng, Zirui Guo, Liang Pang, Pinghui Wang, Tat-Seng Chua, Xiao Zhang, Zhen Cai

**Updated**: 2025-04-04T03:03:47Z

**Summary**: Large Language Models (LLMs) have impressive capabilities in text understanding and zero-shot reasoning. However, delays in knowledge updates may cause them to reason incorrectly or produce harmful results. Knowledge Graphs (KGs) provide rich and reliable contextual information for the reasoning process of LLMs by structurally organizing and connecting a wide range of entities and relations. Existing KG-based LLM reasoning methods only inject KGs' knowledge into prompts in a textual form, ignoring its structural information. Moreover, they mostly rely on close-source models or open-source models with large parameters, which poses challenges to high resource consumption. To address this, we propose a novel Lightweight and efficient Prompt learning-ReasOning Framework for KGQA (LightPROF), which leverages the full potential of LLMs to tackle complex reasoning tasks in a parameter-efficient manner. Specifically, LightPROF follows a "Retrieve-Embed-Reason process", first accurately, and stably retrieving the corresponding reasoning graph from the KG through retrieval module. Next, through a Transformer-based Knowledge Adapter, it finely extracts and integrates factual and structural information from the KG, then maps this information to the LLM's token embedding space, creating an LLM-friendly prompt to be used by the LLM for the final reasoning. Additionally, LightPROF only requires training Knowledge Adapter and can be compatible with any open-source LLM. Extensive experiments on two public KGQA benchmarks demonstrate that LightPROF achieves superior performance with small-scale LLMs. Furthermore, LightPROF shows significant advantages in terms of input token count and reasoning time.

**Link**: [arxiv](http://arxiv.org/abs/2504.03137v1),  [pdf](http://arxiv.org/pdf/2504.03137v1)

**Tags**: cs.AI cs.CL 



### Memorization in In-Context Learning
**Authors**: Shahriar Golchin, Mihai Surdeanu, Steven Bethard, Eduardo Blanco, Ellen Riloff

**Updated**: 2025-04-04T02:50:14Z

**Summary**: In-context learning (ICL) has proven to be an effective strategy for improving the performance of large language models (LLMs) with no additional training. However, the exact mechanism behind this performance improvement remains unclear. This study is the first to show how ICL surfaces memorized training data and to explore the correlation between this memorization and performance on downstream tasks across various ICL regimes: zero-shot, few-shot, and many-shot. Our most notable findings include: (1) ICL significantly surfaces memorization compared to zero-shot learning in most cases; (2) demonstrations, without their labels, are the most effective element in surfacing memorization; (3) ICL improves performance when the surfaced memorization in few-shot regimes reaches a high level (about 40%); and (4) there is a very strong correlation between performance and memorization in ICL when it outperforms zero-shot learning. Overall, our study uncovers memorization as a new factor impacting ICL, raising an important question: to what extent do LLMs truly generalize from demonstrations in ICL, and how much of their success is due to memorization?

**Link**: [arxiv](http://arxiv.org/abs/2408.11546v3),  [pdf](http://arxiv.org/pdf/2408.11546v3)

**Tags**: cs.CL cs.AI cs.LG 



### A Survey of Large Language Models in Mental Health Disorder Detection on   Social Media
**Authors**: Zhuohan Ge, Nicole Hu, Darian Li, Yubo Wang, Shihao Qi, Yuming Xu, Han Shi, Jason Zhang

**Updated**: 2025-04-04T02:07:59Z

**Summary**: The detection and intervention of mental health issues represent a critical global research focus, and social media data has been recognized as an important resource for mental health research. However, how to utilize Large Language Models (LLMs) for mental health problem detection on social media poses significant challenges. Hence, this paper aims to explore the potential of LLM applications in social media data analysis, focusing not only on the most common psychological disorders such as depression and anxiety but also incorporating psychotic disorders and externalizing disorders, summarizing the application methods of LLM from different dimensions, such as text data analysis and detection of mental disorders, and revealing the major challenges and shortcomings of current research. In addition, the paper provides an overview of popular datasets, and evaluation metrics. The survey in this paper provides a comprehensive frame of reference for researchers in the field of mental health, while demonstrating the great potential of LLMs in mental health detection to facilitate the further application of LLMs in future mental health interventions.

**Link**: [arxiv](http://arxiv.org/abs/2504.02800v2),  [pdf](http://arxiv.org/pdf/2504.02800v2)

**Tags**: cs.CL I.2.7; J.3; J.4 



### Les Dissonances: Cross-Tool Harvesting and Polluting in Multi-Tool   Empowered LLM Agents
**Authors**: Zichuan Li, Jian Cui, Xiaojing Liao, Luyi Xing

**Updated**: 2025-04-04T01:41:06Z

**Summary**: Large Language Model (LLM) agents are autonomous systems powered by LLMs, capable of reasoning and planning to solve problems by leveraging a set of tools. However, the integration of multi-tool capabilities in LLM agents introduces challenges in securely managing tools, ensuring their compatibility, handling dependency relationships, and protecting control flows within LLM agent workflows. In this paper, we present the first systematic security analysis of task control flows in multi-tool-enabled LLM agents. We identify a novel threat, Cross-Tool Harvesting and Polluting (XTHP), which includes multiple attack vectors to first hijack the normal control flows of agent tasks, and then collect and pollute confidential or private information within LLM agent systems. To understand the impact of this threat, we developed Chord, a dynamic scanning tool designed to automatically detect real-world agent tools susceptible to XTHP attacks. Our evaluation of 73 real-world tools from the repositories of two major LLM agent development frameworks, LangChain and LlamaIndex, revealed a significant security concern: 80% of the tools are vulnerable to hijacking attacks, 78% to XTH attacks, and 41% to XTP attacks, highlighting the prevalence of this threat.

**Link**: [arxiv](http://arxiv.org/abs/2504.03111v1),  [pdf](http://arxiv.org/pdf/2504.03111v1)

**Tags**: cs.CR 



### Towards More Accurate US Presidential Election via Multi-step Reasoning   with Large Language Models
**Authors**: Chenxiao Yu, Zhaotian Weng, Yuangang Li, Zheng Li, Xiyang Hu, Yue Zhao

**Updated**: 2025-04-04T01:33:20Z

**Summary**: Can Large Language Models (LLMs) accurately predict election outcomes? While LLMs have demonstrated impressive performance in various domains, including healthcare, legal analysis, and creative tasks, their ability to forecast elections remains unknown. Election prediction poses unique challenges, such as limited voter-level data, rapidly changing political landscapes, and the need to model complex human behavior. To address these challenges, we introduce a multi-step reasoning framework designed for political analysis. Our approach is validated on real-world data from the American National Election Studies (ANES) 2016 and 2020, as well as synthetic personas generated by the leading machine learning framework, offering scalable datasets for voter behavior modeling. To capture temporal dynamics, we incorporate candidates' policy positions and biographical details, ensuring that the model adapts to evolving political contexts. Drawing on Chain of Thought prompting, our multi-step reasoning pipeline systematically integrates demographic, ideological, and time-dependent factors, enhancing the model's predictive power.

**Link**: [arxiv](http://arxiv.org/abs/2411.03321v3),  [pdf](http://arxiv.org/pdf/2411.03321v3)

**Tags**: cs.AI cs.CL cs.LG 



### Productively Deploying Emerging Models on Emerging Platforms: A Top-Down   Approach for Testing and Debugging
**Authors**: Siyuan Feng, Jiawei Liu, Ruihang Lai, Charlie F. Ruan, Yong Yu, Lingming Zhang, Tianqi Chen

**Updated**: 2025-04-03T23:20:47Z

**Summary**: While existing machine learning (ML) frameworks focus on established platforms, like running CUDA on server-grade GPUs, there have been growing demands to enable emerging AI applications in a broader set of scenarios, such as running Large Language Models (LLMs) within browsers and mobile phones. However, deploying emerging models on new platforms (such as Metal and WebGPU) presents significant software engineering challenges due to rapid model evolution and limited tooling and practices for these platforms.   Previous practice for ML model deployment often follows a bottom-up fashion, where engineers first implement individual required operators and then put them together. However, this traditional development approach fails to meet the productivity requirements when deploying emerging ML applications, with the testing and debugging part as a bottleneck. To this end, we introduce \textsc{TapML}, a top-down approach designed to streamline model deployment on diverse platforms. While the traditional bottom-up approach requires crafting manual tests, \textsc{TapML} automatically creates high-quality, realistic test data through operator-wise test carving. Furthermore, \textsc{TapML} uses a migration-based strategy to gradually offload model implementation from the mature source platform to the target platform, minimizing the debugging scope of compound errors.   \textsc{TapML} has been used as the default development method in the MLC-LLM project to deploy emerging ML models. Within 2 years, \textsc{TapML} has accelerated the deployment of 105 emerging models in 27 model architectures across 5 emerging platforms. We show that \textsc{TapML} effectively boosts developer productivity while ensuring the quality of deployed models. Furthermore, we summarize comprehensive case studies from our real-world development, offering best practices for developing emerging ML systems.

**Link**: [arxiv](http://arxiv.org/abs/2404.09151v3),  [pdf](http://arxiv.org/pdf/2404.09151v3)

**Tags**: cs.SE cs.LG 



### PROMPTFUZZ: Harnessing Fuzzing Techniques for Robust Testing of Prompt   Injection in LLMs
**Authors**: Jiahao Yu, Yangguang Shao, Hanwen Miao, Junzheng Shi

**Updated**: 2025-04-03T23:03:17Z

**Summary**: Large Language Models (LLMs) have gained widespread use in various applications due to their powerful capability to generate human-like text. However, prompt injection attacks, which involve overwriting a model's original instructions with malicious prompts to manipulate the generated text, have raised significant concerns about the security and reliability of LLMs. Ensuring that LLMs are robust against such attacks is crucial for their deployment in real-world applications, particularly in critical tasks.   In this paper, we propose PROMPTFUZZ, a novel testing framework that leverages fuzzing techniques to systematically assess the robustness of LLMs against prompt injection attacks. Inspired by software fuzzing, PROMPTFUZZ selects promising seed prompts and generates a diverse set of prompt injections to evaluate the target LLM's resilience. PROMPTFUZZ operates in two stages: the prepare phase, which involves selecting promising initial seeds and collecting few-shot examples, and the focus phase, which uses the collected examples to generate diverse, high-quality prompt injections. Using PROMPTFUZZ, we can uncover more vulnerabilities in LLMs, even those with strong defense prompts.   By deploying the generated attack prompts from PROMPTFUZZ in a real-world competition, we achieved the 7th ranking out of over 4000 participants (top 0.14%) within 2 hours. Additionally, we construct a dataset to fine-tune LLMs for enhanced robustness against prompt injection attacks. While the fine-tuned model shows improved robustness, PROMPTFUZZ continues to identify vulnerabilities, highlighting the importance of robust testing for LLMs. Our work emphasizes the critical need for effective testing tools and provides a practical framework for evaluating and improving the robustness of LLMs against prompt injection attacks.

**Link**: [arxiv](http://arxiv.org/abs/2409.14729v2),  [pdf](http://arxiv.org/pdf/2409.14729v2)

**Tags**: cs.CR cs.AI 



### Distributed Locking: Performance Analysis and Optimization Strategies
**Authors**: Andre Rodriguez, William Osborn

**Updated**: 2025-04-03T22:52:16Z

**Summary**: Distributed locking mechanisms are fundamental to ensuring data consistency and integrity in distributed systems. This paper presents a comprehensive analysis of distributed locking algorithms, focusing on their performance characteristics under various workload conditions. We compare traditional centralized locking approaches with modern distributed protocols, evaluating them based on throughput, latency, and scalability metrics. Our experimental results demonstrate that optimized distributed locking protocols can achieve up to 68\% better performance compared to centralized approaches in high-contention scenarios, while maintaining strong consistency guarantees. Furthermore, we propose novel optimizations for distributed locking that significantly reduce coordination overhead in geo-distributed deployments. The findings contribute to the growing body of knowledge on designing efficient concurrency control mechanisms for modern distributed systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.03073v1),  [pdf](http://arxiv.org/pdf/2504.03073v1)

**Tags**: cs.DC 



### CacheBlend: Fast Large Language Model Serving for RAG with Cached   Knowledge Fusion
**Authors**: Jiayi Yao, Hanchen Li, Yuhan Liu, Siddhant Ray, Yihua Cheng, Qizheng Zhang, Kuntai Du, Shan Lu, Junchen Jiang

**Updated**: 2025-04-03T22:49:22Z

**Summary**: Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, which makes precomputed KV caches not directly usable since they ignore the text's cross-attention with the preceding texts. Thus, the benefits of reusing KV caches remain largely unrealized.   This paper tackles just one challenge: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? This challenge naturally arises in retrieval-augmented generation (RAG) where the input is supplemented with multiple retrieved texts as the context. We present CacheBlend, a scheme that reuses the precomputed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime, the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job, allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3x and increases the inference throughput by 2.8-5x from full KV recompute without compromising generation quality. The code is available at https://github.com/LMCache/LMCache.

**Link**: [arxiv](http://arxiv.org/abs/2405.16444v3),  [pdf](http://arxiv.org/pdf/2405.16444v3)

**Tags**: cs.LG 



### AD-GPT: Large Language Models in Alzheimer's Disease
**Authors**: Ziyu Liu, Lintao Tang, Zeliang Sun, Zhengliang Liu, Yanjun Lyu, Wei Ruan, Yangshuang Xu, Liang Shan, Jiyoon Shin, Xiaohe Chen, Dajiang Zhu, Tianming Liu, Rongjie Liu, Chao Huang

**Updated**: 2025-04-03T22:49:10Z

**Summary**: Large language models (LLMs) have emerged as powerful tools for medical information retrieval, yet their accuracy and depth remain limited in specialized domains such as Alzheimer's disease (AD), a growing global health challenge. To address this gap, we introduce AD-GPT, a domain-specific generative pre-trained transformer designed to enhance the retrieval and analysis of AD-related genetic and neurobiological information. AD-GPT integrates diverse biomedical data sources, including potential AD-associated genes, molecular genetic information, and key gene variants linked to brain regions. We develop a stacked LLM architecture combining Llama3 and BERT, optimized for four critical tasks in AD research: (1) genetic information retrieval, (2) gene-brain region relationship assessment, (3) gene-AD relationship analysis, and (4) brain region-AD relationship mapping. Comparative evaluations against state-of-the-art LLMs demonstrate AD-GPT's superior precision and reliability across these tasks, underscoring its potential as a robust and specialized AI tool for advancing AD research and biomarker discovery.

**Link**: [arxiv](http://arxiv.org/abs/2504.03071v1),  [pdf](http://arxiv.org/pdf/2504.03071v1)

**Tags**: cs.CL cs.AI 



### Design of AI-Powered Tool for Self-Regulation Support in Programming   Education
**Authors**: Huiyong Li, Boxuan Ma

**Updated**: 2025-04-03T22:47:33Z

**Summary**: Large Language Model (LLM) tools have demonstrated their potential to deliver high-quality assistance by providing instant, personalized feedback that is crucial for effective programming education. However, many of these tools operate independently from institutional Learning Management Systems, which creates a significant disconnect. This isolation limits the ability to leverage learning materials and exercise context for generating tailored, context-aware feedback. Furthermore, previous research on self-regulated learning and LLM support mainly focused on knowledge acquisition, not the development of important self-regulation skills. To address these challenges, we developed CodeRunner Agent, an LLM-based programming assistant that integrates the CodeRunner, a student-submitted code executing and automated grading plugin in Moodle. CodeRunner Agent empowers educators to customize AI-generated feedback by incorporating detailed context from lecture materials, programming questions, student answers, and execution results. Additionally, it enhances students' self-regulated learning by providing strategy-based AI responses. This integrated, context-aware, and skill-focused approach offers promising avenues for data-driven improvements in programming education.

**Link**: [arxiv](http://arxiv.org/abs/2504.03068v1),  [pdf](http://arxiv.org/pdf/2504.03068v1)

**Tags**: cs.HC cs.AI 



### MatterChat: A Multi-Modal LLM for Material Science
**Authors**: Yingheng Tang, Wenbin Xu, Jie Cao, Jianzhu Ma, Weilu Gao, Steve Farrell, Benjamin Erichson, Michael W. Mahoney, Andy Nonaka, Zhi Yao

**Updated**: 2025-04-03T22:39:34Z

**Summary**: Understanding and predicting the properties of inorganic materials is crucial for accelerating advancements in materials science and driving applications in energy, electronics, and beyond. Integrating material structure data with language-based information through multi-modal large language models (LLMs) offers great potential to support these efforts by enhancing human-AI interaction. However, a key challenge lies in integrating atomic structures at full resolution into LLMs. In this work, we introduce MatterChat, a versatile structure-aware multi-modal LLM that unifies material structural data and textual inputs into a single cohesive model. MatterChat employs a bridging module to effectively align a pretrained machine learning interatomic potential with a pretrained LLM, reducing training costs and enhancing flexibility. Our results demonstrate that MatterChat significantly improves performance in material property prediction and human-AI interaction, surpassing general-purpose LLMs such as GPT-4. We also demonstrate its usefulness in applications such as more advanced scientific reasoning and step-by-step material synthesis.

**Link**: [arxiv](http://arxiv.org/abs/2502.13107v2),  [pdf](http://arxiv.org/pdf/2502.13107v2)

**Tags**: cs.AI cs.LG 



### Do LLMs Consider Security? An Empirical Study on Responses to   Programming Questions
**Authors**: Amirali Sajadi, Binh Le, Anh Nguyen, Kostadin Damevski, Preetha Chatterjee

**Updated**: 2025-04-03T22:13:44Z

**Summary**: The widespread adoption of conversational LLMs for software development has raised new security concerns regarding the safety of LLM-generated content. Our motivational study outlines ChatGPT's potential in volunteering context-specific information to the developers, promoting safe coding practices. Motivated by this finding, we conduct a study to evaluate the degree of security awareness exhibited by three prominent LLMs: Claude 3, GPT-4, and Llama 3. We prompt these LLMs with Stack Overflow questions that contain vulnerable code to evaluate whether they merely provide answers to the questions or if they also warn users about the insecure code, thereby demonstrating a degree of security awareness. Further, we assess whether LLM responses provide information about the causes, exploits, and the potential fixes of the vulnerability, to help raise users' awareness. Our findings show that all three models struggle to accurately detect and warn users about vulnerabilities, achieving a detection rate of only 12.6% to 40% across our datasets. We also observe that the LLMs tend to identify certain types of vulnerabilities related to sensitive information exposure and improper input neutralization much more frequently than other types, such as those involving external control of file names or paths. Furthermore, when LLMs do issue security warnings, they often provide more information on the causes, exploits, and fixes of vulnerabilities compared to Stack Overflow responses. Finally, we provide an in-depth discussion on the implications of our findings and present a CLI-based prompting tool that can be used to generate significantly more secure LLM responses.

**Link**: [arxiv](http://arxiv.org/abs/2502.14202v2),  [pdf](http://arxiv.org/pdf/2502.14202v2)

**Tags**: cs.SE cs.AI cs.CL cs.LG 



### Task as Context Prompting for Accurate Medical Symptom Coding Using   Large Language Models
**Authors**: Chengyang He, Wenlong Zhang, Violet Xinying Chen, Yue Ning, Ping Wang

**Updated**: 2025-04-03T21:57:17Z

**Summary**: Accurate medical symptom coding from unstructured clinical text, such as vaccine safety reports, is a critical task with applications in pharmacovigilance and safety monitoring. Symptom coding, as tailored in this study, involves identifying and linking nuanced symptom mentions to standardized vocabularies like MedDRA, differentiating it from broader medical coding tasks. Traditional approaches to this task, which treat symptom extraction and linking as independent workflows, often fail to handle the variability and complexity of clinical narratives, especially for rare cases. Recent advancements in Large Language Models (LLMs) offer new opportunities but face challenges in achieving consistent performance. To address these issues, we propose Task as Context (TACO) Prompting, a novel framework that unifies extraction and linking tasks by embedding task-specific context into LLM prompts. Our study also introduces SYMPCODER, a human-annotated dataset derived from Vaccine Adverse Event Reporting System (VAERS) reports, and a two-stage evaluation framework to comprehensively assess both symptom linking and mention fidelity. Our comprehensive evaluation of multiple LLMs, including Llama2-chat, Jackalope-7b, GPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o, demonstrates TACO's effectiveness in improving flexibility and accuracy for tailored tasks like symptom coding, paving the way for more specific coding tasks and advancing clinical text processing methodologies.

**Link**: [arxiv](http://arxiv.org/abs/2504.03051v1),  [pdf](http://arxiv.org/pdf/2504.03051v1)

**Tags**: cs.CL cs.AI 



### Explanations of Large Language Models Explain Language Representations   in the Brain
**Authors**: Maryam Rahimi, Yadollah Yaghoobzadeh, Mohammad Reza Daliri

**Updated**: 2025-04-03T21:56:08Z

**Summary**: Large language models (LLMs) not only exhibit human-like performance but also share computational principles with the brain's language processing mechanisms. While prior research has focused on mapping LLMs' internal representations to neural activity, we propose a novel approach using explainable AI (XAI) to strengthen this link. Applying attribution methods, we quantify the influence of preceding words on LLMs' next-word predictions and use these explanations to predict fMRI data from participants listening to narratives. We find that attribution methods robustly predict brain activity across the language network, revealing a hierarchical pattern: explanations from early layers align with the brain's initial language processing stages, while later layers correspond to more advanced stages. Additionally, layers with greater influence on next-word prediction$\unicode{x2014}$reflected in higher attribution scores$\unicode{x2014}$demonstrate stronger brain alignment. These results underscore XAI's potential for exploring the neural basis of language and suggest brain alignment for assessing the biological plausibility of explanation methods.

**Link**: [arxiv](http://arxiv.org/abs/2502.14671v3),  [pdf](http://arxiv.org/pdf/2502.14671v3)

**Tags**: cs.CL cs.AI q-bio.NC 



### LLM Library Learning Fails: A LEGO-Prover Case Study
**Authors**: Ian Berlot-Attwell, Frank Rudzicz, Xujie Si

**Updated**: 2025-04-03T21:53:51Z

**Summary**: Recent advancements in the coding, reasoning, and tool-using abilities of LLMs have spurred interest in library learning (i.e., online learning through the creation, storage, and retrieval of reusable and composable functions, knowledge, checklists, or lemmas). Such systems often promise improved task performance through the automatic creation of broadly applicable tools, as well as superior computational performance through the caching of reasoning (i.e., the storage of generated tools). However, we find strong reason to be skeptical. We perform a deep dive into one such system, LEGO-Prover, which purports to learn reusable lemmas for mathematical reasoning. We find no evidence of the direct reuse of learned lemmas, and find evidence against the soft reuse of learned lemmas (i.e., reuse by modifying relevant examples). Crucially, we find that LEGO-Prover does not in fact improve over the simple baseline of prompting the model - the improvements in task accuracy vanish once computational cost is accounted for. Our findings suggest that serious misconceptions exist as to the effectiveness of these techniques, that a serious re-examination of the state of LLM-based library learning is required, and that we require much stronger standards for evaluation including behavioural analysis and ensuring that an equal computational budget is used for baselines.

**Link**: [arxiv](http://arxiv.org/abs/2504.03048v1),  [pdf](http://arxiv.org/pdf/2504.03048v1)

**Tags**: cs.LG cs.CL 



### Towards Mobile Sensing with Event Cameras on High-agility   Resource-constrained Devices: A Survey
**Authors**: Haoyang Wang, Ruishan Guo, Pengtao Ma, Ciyu Ruan, Xinyu Luo, Wenhua Ding, Tianyang Zhong, Jingao Xu, Yunhao Liu, Xinlei Chen

**Updated**: 2025-04-03T21:53:09Z

**Summary**: With the increasing complexity of mobile device applications, these devices are evolving toward high agility. This shift imposes new demands on mobile sensing, particularly in terms of achieving high accuracy and low latency. Event-based vision has emerged as a disruptive paradigm, offering high temporal resolution, low latency, and energy efficiency, making it well-suited for high-accuracy and low-latency sensing tasks on high-agility platforms. However, the presence of substantial noisy events, the lack of inherent semantic information, and the large data volume pose significant challenges for event-based data processing on resource-constrained mobile devices. This paper surveys the literature over the period 2014-2024, provides a comprehensive overview of event-based mobile sensing systems, covering fundamental principles, event abstraction methods, algorithmic advancements, hardware and software acceleration strategies. We also discuss key applications of event cameras in mobile sensing, including visual odometry, object tracking, optical flow estimation, and 3D reconstruction, while highlighting the challenges associated with event data processing, sensor fusion, and real-time deployment. Furthermore, we outline future research directions, such as improving event camera hardware with advanced optics, leveraging neuromorphic computing for efficient processing, and integrating bio-inspired algorithms to enhance perception. To support ongoing research, we provide an open-source \textit{Online Sheet} with curated resources and recent developments. We hope this survey serves as a valuable reference, facilitating the adoption of event-based vision across diverse applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.22943v2),  [pdf](http://arxiv.org/pdf/2503.22943v2)

**Tags**: cs.RO cs.CV 



### Extending CREAMT: Leveraging Large Language Models for Literary   Translation Post-Editing
**Authors**: Antonio Castaldo, Sheila Castilho, Joss Moorkens, Johanna Monti

**Updated**: 2025-04-03T21:48:09Z

**Summary**: Post-editing machine translation (MT) for creative texts, such as literature, requires balancing efficiency with the preservation of creativity and style. While neural MT systems struggle with these challenges, large language models (LLMs) offer improved capabilities for context-aware and creative translation. This study evaluates the feasibility of post-editing literary translations generated by LLMs. Using a custom research tool, we collaborated with professional literary translators to analyze editing time, quality, and creativity. Our results indicate that post-editing LLM-generated translations significantly reduces editing time compared to human translation while maintaining a similar level of creativity. The minimal difference in creativity between PE and MT, combined with substantial productivity gains, suggests that LLMs may effectively support literary translators working with high-resource languages.

**Link**: [arxiv](http://arxiv.org/abs/2504.03045v1),  [pdf](http://arxiv.org/pdf/2504.03045v1)

**Tags**: cs.CL 



### A Watermark for Black-Box Language Models
**Authors**: Dara Bahri, John Wieting

**Updated**: 2025-04-03T21:13:21Z

**Summary**: Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require white-box access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. black-box access), boasts a distortion-free property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments.

**Link**: [arxiv](http://arxiv.org/abs/2410.02099v2),  [pdf](http://arxiv.org/pdf/2410.02099v2)

**Tags**: cs.CR cs.CL cs.LG 



### Ontologies in Design: How Imagining a Tree Reveals Possibilites and   Assumptions in Large Language Models
**Authors**: Nava Haghighi, Sunny Yu, James Landay, Daniela Rosner

**Updated**: 2025-04-03T21:04:36Z

**Summary**: Amid the recent uptake of Generative AI, sociotechnical scholars and critics have traced a multitude of resulting harms, with analyses largely focused on values and axiology (e.g., bias). While value-based analyses are crucial, we argue that ontologies -- concerning what we allow ourselves to think or talk about -- is a vital but under-recognized dimension in analyzing these systems. Proposing a need for a practice-based engagement with ontologies, we offer four orientations for considering ontologies in design: pluralism, groundedness, liveliness, and enactment. We share examples of potentialities that are opened up through these orientations across the entire LLM development pipeline by conducting two ontological analyses: examining the responses of four LLM-based chatbots in a prompting exercise, and analyzing the architecture of an LLM-based agent simulation. We conclude by sharing opportunities and limitations of working with ontologies in the design and development of sociotechnical systems.

**Link**: [arxiv](http://arxiv.org/abs/2504.03029v1),  [pdf](http://arxiv.org/pdf/2504.03029v1)

**Tags**: cs.HC cs.CL 



### AuDeRe: Automated Strategy Decision and Realization in Robot Planning   and Control via LLMs
**Authors**: Yue Meng, Fei Chen, Yongchao Chen, Chuchu Fan

**Updated**: 2025-04-03T20:20:00Z

**Summary**: Recent advancements in large language models (LLMs) have shown significant promise in various domains, especially robotics. However, most prior LLM-based work in robotic applications either directly predicts waypoints or applies LLMs within fixed tool integration frameworks, offering limited flexibility in exploring and configuring solutions best suited to different tasks. In this work, we propose a framework that leverages LLMs to select appropriate planning and control strategies based on task descriptions, environmental constraints, and system dynamics. These strategies are then executed by calling the available comprehensive planning and control APIs. Our approach employs iterative LLM-based reasoning with performance feedback to refine the algorithm selection. We validate our approach through extensive experiments across tasks of varying complexity, from simple tracking to complex planning scenarios involving spatiotemporal constraints. The results demonstrate that using LLMs to determine planning and control strategies from natural language descriptions significantly enhances robotic autonomy while reducing the need for extensive manual tuning and expert knowledge. Furthermore, our framework maintains generalizability across different tasks and notably outperforms baseline methods that rely on LLMs for direct trajectory, control sequence, or code generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.03015v1),  [pdf](http://arxiv.org/pdf/2504.03015v1)

**Tags**: cs.RO 



### LlamaRestTest: Effective REST API Testing with Small Language Models
**Authors**: Myeongsoo Kim, Saurabh Sinha, Alessandro Orso

**Updated**: 2025-04-03T19:42:32Z

**Summary**: Modern web services rely heavily on REST APIs, typically documented using the OpenAPI specification. The widespread adoption of this standard has resulted in the development of many black-box testing tools that generate tests based on OpenAPI specifications. Although Large Language Models (LLMs) have shown promising test-generation abilities, their application to REST API testing remains mostly unexplored. We present LlamaRestTest, a novel approach that employs two custom LLMs-created by fine-tuning and quantizing the Llama3-8B model using mined datasets of REST API example values and inter-parameter dependencies-to generate realistic test inputs and uncover inter-parameter dependencies during the testing process by analyzing server responses. We evaluated LlamaRestTest on 12 real-world services (including popular services such as Spotify), comparing it against RESTGPT, a GPT-powered specification-enhancement tool, as well as several state-of-the-art REST API testing tools, including RESTler, MoRest, EvoMaster, and ARAT-RL. Our results demonstrate that fine-tuning enables smaller models to outperform much larger models in detecting actionable parameter-dependency rules and generating valid inputs for REST API testing. We also evaluated different tool configurations, ranging from the base Llama3-8B model to fine-tuned versions, and explored multiple quantization techniques, including 2-bit, 4-bit, and 8-bit integer formats. Our study shows that small language models can perform as well as, or better than, large language models in REST API testing, balancing effectiveness and efficiency. Furthermore, LlamaRestTest outperforms state-of-the-art REST API testing tools in code coverage achieved and internal server errors identified, even when those tools use RESTGPT-enhanced specifications.

**Link**: [arxiv](http://arxiv.org/abs/2501.08598v2),  [pdf](http://arxiv.org/pdf/2501.08598v2)

**Tags**: cs.SE cs.AI 



### Understanding and Mitigating Language Confusion in LLMs
**Authors**: Kelly Marchisio, Wei-Yin Ko, Alexandre Bérard, Théo Dehaze, Sebastian Ruder

**Updated**: 2025-04-03T19:31:53Z

**Summary**: We investigate a surprising limitation of LLMs: their inability to consistently generate text in a user's desired language. We create the Language Confusion Benchmark (LCB) to evaluate such failures, covering 15 typologically diverse languages with existing and newly-created English and multilingual prompts. We evaluate a range of LLMs on monolingual and cross-lingual generation reflecting practical use cases, finding that Llama Instruct and Mistral models exhibit high degrees of language confusion and even the strongest models fail to consistently respond in the correct language. We observe that base and English-centric instruct models are more prone to language confusion, which is aggravated by complex prompts and high sampling temperatures. We find that language confusion can be partially mitigated via few-shot prompting, multilingual SFT and preference tuning. We release our language confusion benchmark, which serves as a first layer of efficient, scalable multilingual evaluation at https://github.com/for-ai/language-confusion.

**Link**: [arxiv](http://arxiv.org/abs/2406.20052v3),  [pdf](http://arxiv.org/pdf/2406.20052v3)

**Tags**: cs.CL 



### Language Models Guidance with Multi-Aspect-Cueing: A Case Study for   Competitor Analysis
**Authors**: Amir Hadifar, Christopher Ochs, Arjan Van Ewijk

**Updated**: 2025-04-03T19:18:11Z

**Summary**: Competitor analysis is essential in modern business due to the influence of industry rivals on strategic planning. It involves assessing multiple aspects and balancing trade-offs to make informed decisions. Recent Large Language Models (LLMs) have demonstrated impressive capabilities to reason about such trade-offs but grapple with inherent limitations such as a lack of knowledge about contemporary or future realities and an incomplete understanding of a market's competitive landscape. In this paper, we address this gap by incorporating business aspects into LLMs to enhance their understanding of a competitive market. Through quantitative and qualitative experiments, we illustrate how integrating such aspects consistently improves model performance, thereby enhancing analytical efficacy in competitor analysis.

**Link**: [arxiv](http://arxiv.org/abs/2504.02984v1),  [pdf](http://arxiv.org/pdf/2504.02984v1)

**Tags**: cs.AI cs.CL 



### CoLa -- Learning to Interactively Collaborate with Large LMs
**Authors**: Abhishek Sharma, Dan Goldwasser

**Updated**: 2025-04-03T18:34:36Z

**Summary**: LLMs' remarkable ability to tackle a wide range of language tasks opened new opportunities for collaborative human-AI problem solving. LLMs can amplify human capabilities by applying their intuitions and reasoning strategies at scale. We explore whether human guides can be simulated, by generalizing from human demonstrations of guiding an AI system to solve complex language problems. We introduce CoLa, a novel self-guided learning paradigm for training automated $\textit{guides}$ and evaluate it on two QA datasets, a puzzle-solving task, and a constrained text generation task. Our empirical results show that CoLa consistently outperforms competitive approaches across all domains. Moreover, a small-sized trained guide outperforms a strong model like GPT-4 when acting as a guide. We compare the strategies employed by humans and automated guides by conducting a human study on a QA dataset. We show that automated guides outperform humans by adapting their strategies to reasoners' capabilities and conduct qualitative analyses highlighting distinct differences in guiding strategies.

**Link**: [arxiv](http://arxiv.org/abs/2504.02965v1),  [pdf](http://arxiv.org/pdf/2504.02965v1)

**Tags**: cs.CL cs.AI cs.LG 



### Distributionally Robust Predictive Runtime Verification under   Spatio-Temporal Logic Specifications
**Authors**: Yiqi Zhao, Emily Zhu, Bardh Hoxha, Georgios Fainekos, Jyotirmoy V. Deshmukh, Lars Lindemann

**Updated**: 2025-04-03T18:33:03Z

**Summary**: Cyber-physical systems designed in simulators, often consisting of multiple interacting agents, behave differently in the real-world. We would like to verify these systems during runtime when they are deployed. Thus, we propose robust predictive runtime verification (RPRV) algorithms for: (1) general stochastic CPS under signal temporal logic (STL) tasks, and (2) stochastic multi-agent systems (MAS) under spatio-temporal logic tasks. The RPRV problem presents the following challenges: (1) there may not be sufficient data on the behavior of the deployed CPS, (2) predictive models based on design phase system trajectories may encounter distribution shift during real-world deployment, and (3) the algorithms need to scale to the complexity of MAS and be applicable to spatio-temporal logic tasks. To address these challenges, we assume knowledge of an upper bound on the statistical distance (in terms of an f-divergence) between the trajectory distributions of the system at deployment and design time. We are motivated by our prior work [1, 2] where we proposed an accurate and an interpretable RPRV algorithm for general CPS, which we here extend to the MAS setting and spatio-temporal logic tasks. Specifically, we use a learned predictive model to estimate the system behavior at runtime and robust conformal prediction to obtain probabilistic guarantees by accounting for distribution shifts. Building on [1], we perform robust conformal prediction over the robust semantics of spatio-temporal reach and escape logic (STREL) to obtain centralized RPRV algorithms for MAS. We empirically validate our results in a drone swarm simulator, where we show the scalability of our RPRV algorithms to MAS and analyze the impact of different trajectory predictors on the verification result. To the best of our knowledge, these are the first statistically valid algorithms for MAS under distribution shift.

**Link**: [arxiv](http://arxiv.org/abs/2504.02964v1),  [pdf](http://arxiv.org/pdf/2504.02964v1)

**Tags**: eess.SY cs.LO cs.RO cs.SY 



