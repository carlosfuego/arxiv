# Arxiv Results
## Keyword: kv cache 
 ### Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction
**Authors**: Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

**Updated**: 2025-11-05T14:29:12Z

**Summary**: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.

**Link**: [arxiv](http://arxiv.org/abs/2508.02558v2),  [pdf](http://arxiv.org/pdf/2508.02558v2)

**Tags**: cs.CL 



### RAGBoost: Efficient Retrieval-Augmented Generation with   Accuracy-Preserving Context Reuse
**Authors**: Yinsicheng Jiang, Yeqi Huang, Liang Cheng, Cheng Deng, Xuan Sun, Luo Mai

**Updated**: 2025-11-05T13:59:01Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.

**Link**: [arxiv](http://arxiv.org/abs/2511.03475v1),  [pdf](http://arxiv.org/pdf/2511.03475v1)

**Tags**: cs.LG 



### MagCache: Fast Video Generation with Magnitude-Aware Cache
**Authors**: Zehong Ma, Longhui Wei, Feng Wang, Shiliang Zhang, Qi Tian

**Updated**: 2025-11-05T09:18:48Z

**Summary**: Existing acceleration techniques for video diffusion models often rely on uniform heuristics or time-embedding variants to skip timesteps and reuse cached features. These approaches typically require extensive calibration with curated prompts and risk inconsistent outputs due to prompt-specific overfitting. In this paper, we introduce a novel and robust discovery: a unified magnitude law observed across different models and prompts. Specifically, the magnitude ratio of successive residual outputs decreases monotonically, steadily in most timesteps while rapidly in the last several steps. Leveraging this insight, we introduce a Magnitude-aware Cache (MagCache) that adaptively skips unimportant timesteps using an error modeling mechanism and adaptive caching strategy. Unlike existing methods requiring dozens of curated samples for calibration, MagCache only requires a single sample for calibration. Experimental results show that MagCache achieves 2.10x-2.68x speedups on Open-Sora, CogVideoX, Wan 2.1, and HunyuanVideo, while preserving superior visual fidelity. It significantly outperforms existing methods in LPIPS, SSIM, and PSNR, under similar computational budgets.

**Link**: [arxiv](http://arxiv.org/abs/2506.09045v2),  [pdf](http://arxiv.org/pdf/2506.09045v2)

**Tags**: cs.CV 



### Joint Optimization of DNN Model Caching and Request Routing in Mobile   Edge Computing
**Authors**: Shuting Qiu, Fang Dong, Siyu Tan, Ruiting Zhou, Dian Shen, Patrick P. C. Lee, Qilin Fan

**Updated**: 2025-11-05T03:40:44Z

**Summary**: Mobile edge computing (MEC) can pre-cache deep neural networks (DNNs) near end-users, providing low-latency services and improving users' quality of experience (QoE). However, caching all DNN models at edge servers with limited capacity is difficult, and the impact of model loading time on QoE remains underexplored. Hence, we introduce dynamic DNNs in edge scenarios, disassembling a complete DNN model into interrelated submodels for more fine-grained and flexible model caching and request routing solutions. This raises the pressing issue of jointly deciding request routing and submodel caching for dynamic DNNs to balance model inference precision and loading latency for QoE optimization. In this paper, we study the joint dynamic model caching and request routing problem in MEC networks, aiming to maximize user request inference precision under constraints of server resources, latency, and model loading time. To tackle this problem, we propose CoCaR, an offline algorithm based on linear programming and random rounding that leverages dynamic DNNs to optimize caching and routing schemes, achieving near-optimal performance. Furthermore, we develop an online variant of CoCaR, named CoCaR-OL, enabling effective adaptation to dynamic and unpredictable online request patterns. The simulation results demonstrate that the proposed CoCaR improves the average inference precision of user requests by 46\% compared to state-of-the-art baselines. In addition, in online scenarios, CoCaR-OL achieves an improvement of no less than 32.3\% in user QoE over competitive baselines.

**Link**: [arxiv](http://arxiv.org/abs/2511.03159v1),  [pdf](http://arxiv.org/pdf/2511.03159v1)

**Tags**: cs.NI 



### SnapStream: Efficient Long Sequence Decoding on Dataflow Accelerators
**Authors**: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar

**Updated**: 2025-11-05T00:38:31Z

**Summary**: The proliferation of 100B+ parameter Large Language Models (LLMs) with 100k+ context length support have resulted in increasing demands for on-chip memory to support large KV caches. Techniques such as StreamingLLM and SnapKV demonstrate how to control KV cache size while maintaining model accuracy. Yet, these techniques are not commonly used within industrial deployments using frameworks like vLLM or SGLang. The reason is twofold: on one hand, the static graphs and continuous batching methodology employed by these frameworks make it difficult to admit modifications to the standard multi-head attention algorithm, while on the other hand, the accuracy implications of such techniques on modern instruction-following and reasoning models are not well understood, obfuscating the need for implementing these techniques. In this paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and DeepSeek-R1, and develop SnapStream, a KV cache compression method that can be deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators running at 128k context length and up to 1832 tokens per second in a real production setting. SnapStream enables $4\times$ improved on-chip memory usage and introduces minimal accuracy degradation on LongBench-v2, AIME24 and LiveCodeBench. To the best of our knowledge, this is the first implementation of sparse KV attention techniques deployed in a production inference system with static graphs and continuous batching.

**Link**: [arxiv](http://arxiv.org/abs/2511.03092v1),  [pdf](http://arxiv.org/pdf/2511.03092v1)

**Tags**: cs.AI cs.AR cs.DC 



### Cache Mechanism for Agent RAG Systems
**Authors**: Shuhang Lin, Zhencan Peng, Lingyao Li, Xiao Lin, Xi Zhu, Yongfeng Zhang

**Updated**: 2025-11-04T19:02:29Z

**Summary**: Recent advances in Large Language Model (LLM)-based agents have been propelled by Retrieval-Augmented Generation (RAG), which grants the models access to vast external knowledge bases. Despite RAG's success in improving agent performance, agent-level cache management, particularly constructing, maintaining, and updating a compact, relevant corpus dynamically tailored to each agent's need, remains underexplored. Therefore, we introduce ARC (Agent RAG Cache Mechanism), a novel, annotation-free caching framework that dynamically manages small, high-value corpora for each agent. By synthesizing historical query distribution patterns with the intrinsic geometry of cached items in the embedding space, ARC automatically maintains a high-relevance cache. With comprehensive experiments on three retrieval datasets, our experimental results demonstrate that ARC reduces storage requirements to 0.015% of the original corpus while offering up to 79.8% has-answer rate and reducing average retrieval latency by 80%. Our results demonstrate that ARC can drastically enhance efficiency and effectiveness in RAG-powered LLM agents.

**Link**: [arxiv](http://arxiv.org/abs/2511.02919v1),  [pdf](http://arxiv.org/pdf/2511.02919v1)

**Tags**: cs.CL 



### Non-Contact Manipulation of Induced Magnetic Dipoles
**Authors**: Seth Stewart, Joseph Pawelski, Steve Ward, Andrew J. Petruska

**Updated**: 2025-11-04T17:40:31Z

**Summary**: Extending the field of magnetic manipulation to conductive, non-magnetic objects opens the door for a wide array of applications previously limited to hard or soft magnetic materials. Of particular interest is the recycling of space debris through the use of oscillating magnetic fields, which represent a cache of raw materials in an environment particularly suited to the low forces generated from inductive magnetic manipulation. Building upon previous work that demonstrated 3D open-loop position control by leveraging the opposing dipole moment created from induced eddy currents, this work demonstrates closed-loop position control of a semi-buoyant aluminum sphere in lab tests, and the efficacy of varying methods for force inversion is explored. The closed-loop methods represent a critical first step towards wider applications for 3-DOF position control of induced magnetic dipoles.

**Link**: [arxiv](http://arxiv.org/abs/2511.02761v1),  [pdf](http://arxiv.org/pdf/2511.02761v1)

**Tags**: cs.RO 



### Using Span Queries to Optimize for Cache and Attention Locality
**Authors**: Paul Castro, Nick Mitchell, Nathan Ordonez, Thomas Parnell, Mudhakar Srivatsa, Antoni Viros i Martin

**Updated**: 2025-11-04T17:22:49Z

**Summary**: Clients are evolving beyond chat completion, and now include a variety of innovative inference-time scaling and deep reasoning techniques. At the same time, inference servers remain heavily optimized for chat completion. Prior work has shown that large improvements to KV cache hit rate are possible if inference servers evolve towards these non-chat use cases. However, they offer solutions that are also optimized for a single use case, RAG. In this paper, we introduce the span query to generalize the interface to the inference server. We demonstrate that chat, RAG, inference-time scaling, and agentic workloads can all be expressed as span queries. We show how the critical distinction that had been assumed by prior work lies in whether the order of the inputs matter -- do they commute? In chat, they do not. In RAG, they often do. This paper introduces span queries, which are expression trees of inference calls, linked together with commutativity constraints. We describe span query syntax and semantics. We show how they can be automatically optimized to improve KV cache locality. We show how a small change to vLLM (affecting only 492 lines) can enable high-performance execution of span queries. Using this stack, we demonstrate that span queries can achieve 10-20x reductions in TTFT for two distinct non-chat use cases. Finally, we show that span queries can also be optimized to improve attention locality, so as to avoid the so-called lost-in-the-middle problem. We demonstrate that an attention-optimized span query on a 2b parameter model vastly outperforms the accuracy of a stock inference server using an 8b model.

**Link**: [arxiv](http://arxiv.org/abs/2511.02749v1),  [pdf](http://arxiv.org/pdf/2511.02749v1)

**Tags**: cs.AI 



### Apriel-H1: Towards Efficient Enterprise Reasoning Models
**Authors**: Oleksiy Ostapenko, Luke Kumar, Raymond Li, Denis Kocetkov, Joel Lamy-Poirier, Shruthan Radhakrishna, Soham Parikh, Shambhavi Mishra, Sebastien Paquet, Srinivas Sunkara, Valérie Bécaert, Sathwik Tejaswi Madhusudhan, Torsten Scholak

**Updated**: 2025-11-04T15:17:43Z

**Summary**: Large Language Models (LLMs) achieve remarkable reasoning capabilities through transformer architectures with attention mechanisms. However, transformers suffer from quadratic time and memory complexity in the attention module (MHA) and require caching key-value states during inference, which severely limits throughput and scalability. High inference throughput is critical for agentic tasks, long-context reasoning, efficient deployment under high request loads, and more efficient test-time compute scaling.   State Space Models (SSMs) such as Mamba offer a promising alternative with linear inference complexity and a constant memory footprint via recurrent computation with fixed-size hidden states. In this technical report we introduce the Apriel-H1 family of hybrid LLMs that combine transformer attention and SSM sequence mixers for efficient reasoning at 15B model size. These models are obtained through incremental distillation from a pretrained reasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacing less critical attention layers with linear Mamba blocks.   We release multiple post-distillation variants of Apriel-H1-15B-Thinker with different SSM-to-MHA ratios and analyse how reasoning performance degrades as more Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variant of Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces, achieving over 2x higher inference throughput when deployed in the production-ready vLLM environment, with minimal degradation in reasoning performance. This shows that distilled hybrid SSM-Transformer architectures can deliver substantial efficiency gains over the pretrained transformer equivalent without substantially compromising the reasoning quality.

**Link**: [arxiv](http://arxiv.org/abs/2511.02651v1),  [pdf](http://arxiv.org/pdf/2511.02651v1)

**Tags**: cs.LG cs.AI 



### Federated Attention: A Distributed Paradigm for Collaborative LLM   Inference over Edge Networks
**Authors**: Xiumei Deng, Zehui Xiong, Binbin Chen, Dong In Kim, Merouane Debbah, H. Vincent Poor

**Updated**: 2025-11-04T15:14:58Z

**Summary**: Large language models (LLMs) are proliferating rapidly at the edge, delivering intelligent capabilities across diverse application scenarios. However, their practical deployment in collaborative scenarios confronts fundamental challenges: privacy vulnerabilities, communication overhead, and computational bottlenecks. To address these, we propose Federated Attention (FedAttn), which integrates the federated paradigm into the self-attention mechanism, creating a new distributed LLM inference framework that simultaneously achieves privacy protection, communication efficiency, and computational efficiency. FedAttn enables participants to perform local self-attention over their own token representations while periodically exchanging and aggregating Key-Value (KV) matrices across multiple Transformer blocks, collaboratively generating LLM responses without exposing private prompts. Further, we identify a structural duality between contextual representation refinement in FedAttn and parameter optimization in FL across private data, local computation, and global aggregation. This key insight provides a principled foundation for systematically porting federated optimization techniques to collaborative LLM inference. Building on this framework, we theoretically analyze how local self-attention computation within participants and heterogeneous token relevance among participants shape error propagation dynamics across Transformer blocks. Moreover, we characterize the fundamental trade-off between response quality and communication/computation efficiency, which is governed by the synchronization interval and the number of participants. Experimental results validate our theoretical analysis, and reveal significant optimization opportunities through sparse attention and adaptive KV aggregation, highlighting FedAttn's potential to deliver scalability and efficiency in real-world edge deployments.

**Link**: [arxiv](http://arxiv.org/abs/2511.02647v1),  [pdf](http://arxiv.org/pdf/2511.02647v1)

**Tags**: cs.DC cs.AI cs.LG 



### Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning
**Authors**: Chaofan Lin, Jiaming Tang, Shuo Yang, Hanshuo Wang, Tian Tang, Boyu Tian, Ion Stoica, Song Han, Mingyu Gao

**Updated**: 2025-11-04T12:04:06Z

**Summary**: Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been a hot research topic. However, current algorithms such as sparse attention or key-value (KV) cache compression tend to use a fixed budget, which presents a significant challenge during deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse attention can surprisingly achieve adaptive budgeting. Based on this, we propose Twilight, a framework to bring adaptive sparsity to any existing sparse attention algorithm without sacrificing their accuracy. Empirical results show that Twilight can adaptively prune at most 98% of redundant tokens, leading to $15.4\times$ acceleration in self-attention operations and $3.9\times$ acceleration in end-to-end per token latency in long context LLM decoding.

**Link**: [arxiv](http://arxiv.org/abs/2502.02770v5),  [pdf](http://arxiv.org/pdf/2502.02770v5)

**Tags**: cs.LG cs.CL 



### Laser diagnostics for negative ion source optimization: insights from   SPIDER at the ITER Neutral Beam Test Facility
**Authors**: R. Agnello, M. Barbisan, R. Pasqualotto, B. Pouradier-Duteil, E. Sartori, A. Tiso, B. Zaniol

**Updated**: 2025-11-04T09:03:09Z

**Summary**: The ITER Heating Neutral Beams (HNBs) require large, high-energy H/D atom beams (285/330 A/m^2 extracted current density, and 1/0.87 MeV acceleration energy, respectively for H and D). To address the associated challenges, the SPIDER negative ion RF beam source at the Neutral Beam Test Facility (NBTF) in Padova (Italy) serves as a full-scale source prototype with a 100 kV triode accelerator, for design validation and performance verification. SPIDER is equipped with two advanced laser diagnostics to monitor key plasma parameters; Cavity Ring-Down Spectroscopy (CRDS) is used to measure H$^-$\slash D$^-$ ion densities, while Laser Absorption Spectroscopy (LAS) tracks caesium neutral density in the source. These measurements are essential for optimizing negative ion production and meeting ITER source targets. We present diagnostic upgrade details, recent experimental results, and correlations with other machine parameters. Since CRDS relies on a single 4.637-meter-long optical cavity, the longest used in such sources, it has demonstrated sensitivity to alignment. Based on recent experimental experience, structural improvements are being implemented to enhance both stability and measurement reliability. LAS has mainly been employed as a tool to monitor the caesium conditioning status of SPIDER. Additionally, due to a distributed measurement over four lines of sight, LAS has proven effective in monitoring the caesium distribution within the source. This work demonstrates the essential role of laser diagnostics in developing ITER-relevant plasma sources and informs ongoing efforts to improve measurement accuracy in challenging environments.

**Link**: [arxiv](http://arxiv.org/abs/2511.02381v1),  [pdf](http://arxiv.org/pdf/2511.02381v1)

**Tags**: physics.plasm-ph 



### Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV   Cache Time-to-Live
**Authors**: Hanchen Li, Qiuyang Mang, Runyuan He, Qizheng Zhang, Huanzhi Mao, Xiaokun Chen, Alvin Cheung, Joseph Gonzalez, Ion Stoica

**Updated**: 2025-11-04T03:43:05Z

**Summary**: Agentic LLM applications interleave LLM generation requests with tool calls. These tool calls break the continuity of the workflow by creating pauses between LLM requests, bringing many challenges for the serving system, especially under multi-turn scenarios. Each pause potentially causes KV cache eviction and extra waiting time before entering the continuous batch for the following LLM request. Since these pauses happen for each call, this problem becomes increasingly severe as turn number grow for agentic programs. Previous works either fail to incorporate information from the tool call, evicting KV cache that leads to repetitive prefill or loading, or ignore the continuity of a multi-turn program, creating waiting time between turns that increases per-request latency.   We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by combining tool-aware KV cache timeout with program-level scheduling. By predicting tool call durations in agentic workflows, Continuum selectively pins the KV cache in GPU memory with a time-to-live value based on total turn number. When combined with program-level first-come-first-serve, Continuum prevents scheduling bubbles, preserves multi-turn continuity, and optimizes for throughput for complex agentic workflows. By modeling the variability of tool call and agent program continuity, Continuum outperforms state-of-the-art baselines. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B models shows that Continuum significantly improves the average job completion times, and remains performant across different hardware setups and DRAM offloading schemes. Preview code is available at: https://github.com/Hanchenli/vllm-continuum

**Link**: [arxiv](http://arxiv.org/abs/2511.02230v1),  [pdf](http://arxiv.org/pdf/2511.02230v1)

**Tags**: cs.OS cs.AI cs.NI 



### Optimizing Attention on GPUs by Exploiting GPU Architectural NUMA   Effects
**Authors**: Mansi Choudhary, Karthik Sangaiah, Sonali Singh, Muhammad Osama, Lisa Wu Wills, Ganesh Dasika

**Updated**: 2025-11-03T23:48:39Z

**Summary**: The rise of disaggregated AI GPUs has exposed a critical bottleneck in large-scale attention workloads: non-uniform memory access (NUMA). As multi-chiplet designs become the norm for scaling compute capabilities, memory latency and bandwidth vary sharply across compute regions, undermining the performance of traditional GPU kernel scheduling strategies that assume uniform memory access. We identify how these NUMA effects distort locality in multi-head attention (MHA) and present Swizzled Head-first Mapping, a spatially-aware scheduling strategy that aligns attention heads with GPU NUMA domains to exploit intra-chiplet cache reuse. On AMD's MI300X architecture, our method achieves up to 50% higher performance over state-of-the-art attention algorithms using conventional scheduling techniques and sustains consistently high L2 cache hit rates of 80-97%. These results demonstrate that NUMA-aware scheduling is now fundamental to achieving full efficiency on next-generation disaggregated GPUs, offering a path forward for scalable AI training and inference.

**Link**: [arxiv](http://arxiv.org/abs/2511.02132v1),  [pdf](http://arxiv.org/pdf/2511.02132v1)

**Tags**: cs.AR cs.DC cs.LG cs.PF 



### KV Cache Transform Coding for Compact Storage in LLM Inference
**Authors**: Konrad Staniszewski, Adrian Łańcucki

**Updated**: 2025-11-03T18:20:35Z

**Summary**: Serving large language models (LLMs) at scale necessitates efficient key-value (KV) cache management. KV caches can be reused across conversation turns via shared-prefix prompts that are common in iterative code editing and chat. However, stale caches consume scarce GPU memory, require offloading, or force recomputation. We present KVTC, a lightweight transform coder that compresses KV caches for compact on-GPU and off-GPU storage. Drawing on classical media compression, KVTC combines PCA-based feature decorrelation, adaptive quantization, and entropy coding. It requires only a brief initial calibration and leaves model parameters unchanged. By exploiting redundancies in KV caches, KVTC achieves up to 20$\times$ compression while maintaining reasoning and long-context accuracy, and 40$\times$ or higher for specific use cases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across benchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and MATH-500. It consistently outperforms inference-time baselines such as token eviction, quantization, and SVD-based methods, while achieving higher compression ratios. These results support KVTC as a practical building block for memory-efficient LLM serving with reusable KV caches.

**Link**: [arxiv](http://arxiv.org/abs/2511.01815v1),  [pdf](http://arxiv.org/pdf/2511.01815v1)

**Tags**: cs.CL cs.AI cs.LG 



### Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with   Efficient LLM Serving
**Authors**: Chengying Huan, Ziheng Meng, Yongchao Liu, Zhengyi Yang, Yun Zhu, Yue Yun, Shipeng Li, Rong Gu, Xiabao Wu, Haitao Zhang, Chuntao Hong, Shaonan Ma, Guihai Chen, Chen Tian

**Updated**: 2025-11-03T14:42:53Z

**Summary**: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to perform step-by-step reasoning over graph-structured knowledge, but existing pipelines suffer from low accuracy, excessive token usage, high latency, and low throughput due to single-agent monolithic prompts, repeated context re-encoding, and inefficient serving execution. We present GLM, the first multi-agent Graph-CoT system co-designed with an optimized LLM serving architecture. GLM decomposes reasoning into specialized agents for classification, reasoning, action generation, and graph retrieval, enabling branching and selective context sharing to reduce prompt length and reasoning iterations while preserving reasoning quality, thereby improving accuracy and reducing overall token consumption. To scale inference, we introduce a Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache management, priority-based eviction, and pipelined execution to improve serving efficiency. Experiments demonstrate that GLM improves answer accuracy by up to 38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT baselines, enabling efficient adoption for complex real-world reasoning at scale.

**Link**: [arxiv](http://arxiv.org/abs/2511.01633v1),  [pdf](http://arxiv.org/pdf/2511.01633v1)

**Tags**: cs.LG cs.AI 



### Representation Consistency for Accurate and Coherent LLM Answer   Aggregation
**Authors**: Junqi Jiang, Tom Bewley, Salim I. Amoukou, Francesco Leofante, Antonio Rago, Saumitra Mishra, Francesca Toni

**Updated**: 2025-11-03T11:32:13Z

**Summary**: Test-time scaling improves large language models' (LLMs) performance by allocating more compute budget during inference. To achieve this, existing methods often require intricate modifications to prompting and sampling strategies. In this work, we introduce representation consistency (RC), a test-time scaling method for aggregating answers drawn from multiple candidate responses of an LLM regardless of how they were generated, including variations in prompt phrasing and sampling strategy. RC enhances answer aggregation by not only considering the number of occurrences of each answer in the candidate response set, but also the consistency of the model's internal activations while generating the set of responses leading to each answer. These activations can be either dense (raw model activations) or sparse (encoded via pretrained sparse autoencoders). Our rationale is that if the model's representations of multiple responses converging on the same answer are highly variable, this answer is more likely to be the result of incoherent reasoning and should be down-weighted during aggregation. Importantly, our method only uses cached activations and lightweight similarity computations and requires no additional model queries. Through experiments with four open-source LLMs and four reasoning datasets, we validate the effectiveness of RC for improving task performance during inference, with consistent accuracy improvements (up to 4%) over strong test-time scaling baselines. We also show that consistency in the sparse activation signals aligns well with the common notion of coherent reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2506.21590v2),  [pdf](http://arxiv.org/pdf/2506.21590v2)

**Tags**: cs.CL cs.LG 



### Memory-Efficient Training with In-Place FFT Implementation
**Authors**: Xinyu Ding, Bangtian Liu, Siyu Liao, Zhongfeng Wang

**Updated**: 2025-11-03T09:36:11Z

**Summary**: Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.

**Link**: [arxiv](http://arxiv.org/abs/2511.01385v1),  [pdf](http://arxiv.org/pdf/2511.01385v1)

**Tags**: cs.LG I.2.6; G.1.2; D.1.3 



### MotionStream: Real-Time Video Generation with Interactive Motion   Controls
**Authors**: Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Schechtman, Xun Huang

**Updated**: 2025-11-03T06:37:53Z

**Summary**: Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons: (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.

**Link**: [arxiv](http://arxiv.org/abs/2511.01266v1),  [pdf](http://arxiv.org/pdf/2511.01266v1)

**Tags**: cs.CV cs.LG 



### Multi-head Temporal Latent Attention
**Authors**: Keqi Deng, Philip C. Woodland

**Updated**: 2025-11-02T20:27:27Z

**Summary**: While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3x speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality.

**Link**: [arxiv](http://arxiv.org/abs/2505.13544v3),  [pdf](http://arxiv.org/pdf/2505.13544v3)

**Tags**: cs.LG cs.AI 



### FlexiCache: Leveraging Temporal Stability of Attention Heads for   Efficient KV Cache Management
**Authors**: Nazmul Takbir, Hamidreza Alikhani, Nikil Dutt, Sangeetha Abdu Jyothi

**Updated**: 2025-11-02T09:33:12Z

**Summary**: Large Language Model (LLM) serving is increasingly constrained by the growing size of the key-value (KV) cache, which scales with both context length and generation length. Prior work shows that attention is dominated by a small subset of critical tokens, yet existing systems struggle to exploit this efficiently without degrading accuracy, especially in long generation. We make a key observation: the temporal stability of these critical tokens varies significantly across KV heads: some heads consistently focus on the same tokens, while others shift frequently. Building on this insight, we introduce FlexiCache, a hierarchical KV-cache management system that leverages the temporal stability of KV heads to reduce GPU memory usage and computation overhead, while preserving model accuracy. FlexiCache classifies KV heads as stable or unstable: it retains all KV-cache pages from unstable heads in GPU memory, whereas for stable heads, it keeps only the top-K pages on the GPU and offloads the rest to host memory. By exploiting temporal stability, FlexiCache performs periodic reranking for stable heads to fetch newly promoted top pages. Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and lowers online token latency by 1.6-2.1x, all while maintaining accuracy in long-context, long-generation scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2511.00868v1),  [pdf](http://arxiv.org/pdf/2511.00868v1)

**Tags**: cs.LG 



### Optimizing Native Sparse Attention with Latent Attention and Local   Global Alternating Strategies
**Authors**: Yuxuan Hu, Jianchao Tan, Jiaqi Zhang, Wen Zan, Pingwei Sun, Yifan Lu, Yerui Sun, Yuchen Xie, Xunliang Cai, Jing Zhang

**Updated**: 2025-11-02T06:15:14Z

**Summary**: In this work, we conduct a systematic analysis of Native Sparse Attention (NSA) and propose targeted improvements that enhance long-context modeling. A key insight is that alternating between local (sliding-window) and global (compression, selective) attention across layers, rather than using fixed patterns, enables more effective propagation of long-range dependencies and substantially boosts performance on long-sequence tasks. Meanwhile, we further refine NSA's branches with Latent Attention that the sliding-window branch is enhanced with Multi-head Latent Attention (MLA) while compression and selective branches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache memory by 50\% versus NSA while improving the model's common-sense reasoning and long-text understanding capabilities. Experiments on models from 340M to 1.3B parameters (trained on 15B and 100B tokens) show our method matches or exceeds full attention and native sparse attention in both common-sense reasoning and long-context understanding tasks.

**Link**: [arxiv](http://arxiv.org/abs/2511.00819v1),  [pdf](http://arxiv.org/pdf/2511.00819v1)

**Tags**: cs.CL 



### High-Power Dual-Channel Field Chamber for High-Frequency Magnetic   Neuromodulation
**Authors**: Xiaoyang Tian, Hui Wang, Boshuo Wang, Jinshui Zhang, Dong Yan, Jeannette Ingabire, Samantha Coffler, Guillaume Duret, Quoc-Khanh Pham, Gang Bao, Jacob T. Robinson, Stefan M. Goetz, Angel V. Peterchev

**Updated**: 2025-11-02T00:04:54Z

**Summary**: Several novel methods, including magnetogenetics and magnetoelectric stimulation, use high frequency alternating magnetic fields to precisely manipulate neural activity. To quantify the behavioral effects of such interventions in a freely moving mouse, we developed a dual-channel magnetic chamber, specifically designed for rate-sensitive magnetothermal-genetic stimulation, and adaptable for other uses of alternating magnetic fields. Through an optimized coil design, the system allows independent control of two spatially orthogonal uniform magnetic fields delivered at different frequencies within a 10 cm x 10 cm x 6 cm chamber. The two channels have nominal frequencies of 50 and 550 kHz with peak magnetic field strengths of 88 and 12.5 mT, achieved with resonant coil drives having peak voltages of 1.6 and 1.8 kV and currents of 1.0 and 0.26 kA, respectively. Additionally, a liquid cooling system enables magnetic field generation for second-level duration, and an observation port and camera allow video capture of the animal's behavior within the chamber. The system generates high-amplitude magnetic fields across two widely separated frequency channels with negligible interference (< 1%). Relatively uniform magnetic field distribution (+/-10% across 94% of the chamber volume) is maintained throughout the chamber, and temperature increase of the inner side of the coil enclosure during the operation is limited to < 0.35 {\deg}C/s to ensure in vivo safety. Using cobalt-doped and undoped iron oxide nanoparticles, we demonstrate channel-specific heating rates of 3.5 {\deg}C/s and 1.5 {\deg}C/s, respectively, validating frequency-selectivity. Both channels can run continuously for four seconds stably.

**Link**: [arxiv](http://arxiv.org/abs/2511.00745v1),  [pdf](http://arxiv.org/pdf/2511.00745v1)

**Tags**: eess.SY cs.SY physics.med-ph q-bio.NC 



### Kimi Linear: An Expressive, Efficient Attention Architecture
**Authors**: Kimi Team, Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T. Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du

**Updated**: 2025-11-01T12:05:18Z

**Summary**: We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.   We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.   To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.

**Link**: [arxiv](http://arxiv.org/abs/2510.26692v2),  [pdf](http://arxiv.org/pdf/2510.26692v2)

**Tags**: cs.CL cs.LG 



### Drinfeld associators and Kashiwara-Vergne associators in higher genera
**Authors**: Toyo Taniguchi

**Updated**: 2025-11-01T09:53:47Z

**Summary**: For $g\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction.

**Link**: [arxiv](http://arxiv.org/abs/2511.00473v1),  [pdf](http://arxiv.org/pdf/2511.00473v1)

**Tags**: math.QA math.AT 57K20(primary), 16T05, 16W70, 18M75, 20F36, 20F40, 57M05 (secondary) 



### A Survey on Cache Methods in Diffusion Models: Toward Efficient   Multi-Modal Generation
**Authors**: Jiacheng Liu, Xinyu Wang, Yuqi Lin, Zhikai Wang, Peiru Wang, Peiliang Cai, Qinming Zhou, Zhengan Yan, Zexuan Yan, Zhengyi Shi, Chang Zou, Yue Ma, Linfeng Zhang

**Updated**: 2025-11-01T08:49:20Z

**Summary**: Diffusion Models have become a cornerstone of modern generative AI for their exceptional generation quality and controllability. However, their inherent \textit{multi-step iterations} and \textit{complex backbone networks} lead to prohibitive computational overhead and generation latency, forming a major bottleneck for real-time applications. Although existing acceleration techniques have made progress, they still face challenges such as limited applicability, high training costs, or quality degradation.   Against this backdrop, \textbf{Diffusion Caching} offers a promising training-free, architecture-agnostic, and efficient inference paradigm. Its core mechanism identifies and reuses intrinsic computational redundancies in the diffusion process. By enabling feature-level cross-step reuse and inter-layer scheduling, it reduces computation without modifying model parameters. This paper systematically reviews the theoretical foundations and evolution of Diffusion Caching and proposes a unified framework for its classification and analysis.   Through comparative analysis of representative methods, we show that Diffusion Caching evolves from \textit{static reuse} to \textit{dynamic prediction}. This trend enhances caching flexibility across diverse tasks and enables integration with other acceleration techniques such as sampling optimization and model distillation, paving the way for a unified, efficient inference framework for future multimodal and interactive applications. We argue that this paradigm will become a key enabler of real-time and efficient generative AI, injecting new vitality into both theory and practice of \textit{Efficient Generative Intelligence}.

**Link**: [arxiv](http://arxiv.org/abs/2510.19755v3),  [pdf](http://arxiv.org/pdf/2510.19755v3)

**Tags**: cs.LG cs.AI cs.CV 



### KVCOMM: Online Cross-context KV-cache Communication for Efficient   LLM-based Multi-agent Systems
**Authors**: Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, Yiran Chen

**Updated**: 2025-11-01T08:26:24Z

**Summary**: Multi-agent large language model (LLM) systems are increasingly adopted for complex language processing tasks that require communication and coordination among agents. However, these systems often suffer substantial overhead from repeated reprocessing of overlapping contexts across agents. In typical pipelines, once an agent receives a message from its predecessor, the full context-including prior turns-must be reprocessed from scratch, leading to inefficient processing. While key-value (KV) caching is an effective solution for avoiding redundant computation in single-agent settings where prefixes remain unchanged, it cannot be directly reused in multi-agent scenarios due to diverging prefixes introduced by agent-specific context extensions. We identify that the core challenge lies in the offset variance of KV-caches across agents. To address this, we propose KVCOMM, a training-free framework that enables efficient prefilling in multi-agent inference by reusing KV-caches and aligning cache offsets of overlapping contexts under diverse prefix contexts. KVCOMM estimates and adjusts KV-caches for shared content by referencing a pool of cached examples-termed anchors-that store observed cache deviations under varying prefixes. The anchor pool is maintained and updated online, allowing dynamic adaptation to distinct user requests and context structures. KVCOMM achieves over 70% reuse rate across diverse multi-agent workloads, including retrieval-augmented generation, math reasoning, and collaborative coding tasks, all without quality degradation. Particularly, when each fully-connected agent receives 1K input tokens with 512 prefix tokens and 512 output tokens under a five-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard prefill pipeline, reducing TTFT from ~430 ms to ~55 ms.

**Link**: [arxiv](http://arxiv.org/abs/2510.12872v2),  [pdf](http://arxiv.org/pdf/2510.12872v2)

**Tags**: cs.MA cs.AI stat.ML 



### Jarvis: Towards Personalized AI Assistant via Personal KV-Cache   Retrieval
**Authors**: Binxiao Xu, Junyu Feng, Shaolin Lu, Yulin Luo, Shilin Yan, Hao Liang, Ming Lu, Wentao Zhang

**Updated**: 2025-11-01T07:01:00Z

**Summary**: The rapid development of Vision-language models (VLMs) enables open-ended perception and reasoning. Recent works have started to investigate how to adapt general-purpose VLMs into personalized assistants. Even commercial models such as ChatGPT now support model personalization by incorporating user-specific information. However, existing methods either learn a set of concept tokens or train a VLM to utilize user-specific information. However, both pipelines struggle to generate accurate answers as personalized assistants. We introduce Jarvis, an innovative framework for a personalized AI assistant through personal KV-Cache retrieval, which stores user-specific information in the KV-Caches of both textual and visual tokens. The textual tokens are created by summarizing user information into metadata, while the visual tokens are produced by extracting distinct image patches from the user's images. When answering a question, Jarvis first retrieves related KV-Caches from personal storage and uses them to ensure accuracy in responses. We also introduce a fine-grained benchmark built with the same distinct image patch mining pipeline, emphasizing accurate question answering based on fine-grained user-specific information. Jarvis is capable of providing more accurate responses, particularly when they depend on specific local details. Jarvis achieves state-of-the-art results in both visual question answering and text-only tasks across multiple datasets, indicating a practical path toward personalized AI assistants. The code and dataset will be released.

**Link**: [arxiv](http://arxiv.org/abs/2510.22765v2),  [pdf](http://arxiv.org/pdf/2510.22765v2)

**Tags**: cs.AI 



### Robustifying Learning-Augmented Caching Efficiently without Compromising   1-Consistency
**Authors**: Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng

**Updated**: 2025-11-01T04:26:03Z

**Summary**: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce excessive computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_{k-1} + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only O(1) additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.16242v7),  [pdf](http://arxiv.org/pdf/2507.16242v7)

**Tags**: cs.DS cs.LG 



### Scalable Processing-Near-Memory for 1M-Token LLM Inference: CXL-Enabled   KV-Cache Management Beyond GPU Limits
**Authors**: Dowon Kim, MinJae Lee, Janghyeon Kim, HyuckSung Kwon, Hyeonggyu Jeong, Sang-Soo Park, Minyong Yoon, Si-Dong Roh, Yongsuk Kwon, Jinin So, Jungwook Choi

**Updated**: 2025-10-31T23:50:44Z

**Summary**: The expansion of context windows in large language models (LLMs) to multi-million tokens introduces severe memory and compute bottlenecks, particularly in managing the growing Key-Value (KV) cache. While Compute Express Link (CXL) enables non-eviction frameworks that offload the full KV-cache to scalable external memory, these frameworks still suffer from costly data transfers when recalling non-resident KV tokens to limited GPU memory as context lengths increase. This work proposes scalable Processing-Near-Memory (PNM) for 1M-Token LLM Inference, a CXL-enabled KV-cache management system that coordinates memory and computation beyond GPU limits. Our design offloads token page selection to a PNM accelerator within CXL memory, eliminating costly recalls and enabling larger GPU batch sizes. We further introduce a hybrid parallelization strategy and a steady-token selection mechanism to enhance compute efficiency and scalability. Implemented atop a state-of-the-art CXL-PNM system, our solution delivers consistent performance gains for LLMs with up to 405B parameters and 1M-token contexts. Our PNM-only offloading scheme (PNM-KV) and GPU-PNM hybrid with steady-token execution (PnG-KV) achieve up to 21.9x throughput improvement, up to 60x lower energy per token, and up to 7.3x better total cost efficiency than the baseline, demonstrating that CXL-enabled multi-PNM architectures can serve as a scalable backbone for future long-context LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2511.00321v1),  [pdf](http://arxiv.org/pdf/2511.00321v1)

**Tags**: cs.AR cs.AI 



### AttnCache: Accelerating Self-Attention Inference for LLM Prefill via   Attention Cache
**Authors**: Dinghong Song, Yuan Feng, Yiwei Wang, Shangye Chen, Cyril Guyot, Filip Blagojevic, Hyeran Jeon, Pengfei Su, Dong Li

**Updated**: 2025-10-31T18:19:55Z

**Summary**: Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25979v2),  [pdf](http://arxiv.org/pdf/2510.25979v2)

**Tags**: cs.CL cs.LG 



### SpecAttn: Speculating Sparse Attention
**Authors**: Harsh Shah

**Updated**: 2025-10-31T17:12:34Z

**Summary**: Large Language Models (LLMs) face significant computational bottlenecks during inference due to the quadratic complexity of self-attention mechanisms, particularly as context lengths increase. We introduce SpecAttn, a novel training-free approach that seamlessly integrates with existing speculative decoding techniques to enable efficient sparse attention in pre-trained transformers. Our key insight is to exploit the attention weights already computed by the draft model during speculative decoding to identify important tokens for the target model, eliminating redundant computation while maintaining output quality. SpecAttn employs three core techniques: KL divergence-based layer alignment between draft and target models, a GPU-optimized sorting-free algorithm for top-p token selection from draft attention patterns, and dynamic key-value cache pruning guided by these predictions. By leveraging the computational work already performed in standard speculative decoding pipelines, SpecAttn achieves over 75% reduction in key-value cache accesses with a mere 15.29% increase in perplexity on the PG-19 dataset, significantly outperforming existing sparse attention methods. Our approach demonstrates that speculative execution can be enhanced to provide approximate verification without significant performance degradation.

**Link**: [arxiv](http://arxiv.org/abs/2510.27641v1),  [pdf](http://arxiv.org/pdf/2510.27641v1)

**Tags**: cs.CL cs.LG cs.SY eess.SY 



### VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation
**Authors**: Heng Ping, Arijit Bhattacharjee, Peiyu Zhang, Shixuan Li, Wei Yang, Anzhe Cheng, Xiaole Zhang, Jesse Thomason, Ali Jannesari, Nesreen Ahmed, Paul Bogdan

**Updated**: 2025-10-31T16:40:58Z

**Summary**: Automation of Register Transfer Level (RTL) design can help developers meet increasing computational demands. Large Language Models (LLMs) show promise for Hardware Description Language (HDL) generation, but face challenges due to limited parametric knowledge and domain-specific constraints. While prompt engineering and fine-tuning have limitations in knowledge coverage and training costs, multi-agent architectures offer a training-free paradigm to enhance reasoning through collaborative generation. However, current multi-agent approaches suffer from two critical deficiencies: susceptibility to noise propagation and constrained reasoning space exploration. We propose VeriMoA, a training-free mixture-of-agents (MoA) framework with two synergistic innovations. First, a quality-guided caching mechanism to maintain all intermediate HDL outputs and enables quality-based ranking and selection across the entire generation process, encouraging knowledge accumulation over layers of reasoning. Second, a multi-path generation strategy that leverages C++ and Python as intermediate representations, decomposing specification-to-HDL translation into two-stage processes that exploit LLM fluency in high-resource languages while promoting solution diversity. Comprehensive experiments on VerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves 15--30% improvements in Pass@1 across diverse LLM backbones, especially enabling smaller models to match larger models and fine-tuned alternatives without requiring costly training.

**Link**: [arxiv](http://arxiv.org/abs/2510.27617v1),  [pdf](http://arxiv.org/pdf/2510.27617v1)

**Tags**: cs.AI 



### Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in   Masked Diffusion
**Authors**: Satoshi Hayakawa, Yuhta Takida, Masaaki Imaizumi, Hiromi Wakaki, Yuki Mitsufuji

**Updated**: 2025-10-31T05:31:58Z

**Summary**: Masked diffusion models have shown promising performance in generating high-quality samples in a wide range of domains, but accelerating their sampling process remains relatively underexplored. To investigate efficient samplers for masked diffusion, this paper theoretically analyzes the MaskGIT sampler for image modeling, revealing its implicit temperature sampling mechanism. Through this analysis, we introduce the "moment sampler," an asymptotically equivalent but more tractable and interpretable alternative to MaskGIT, which employs a "choose-then-sample" approach by selecting unmasking positions before sampling tokens. In addition, we improve the efficiency of choose-then-sample algorithms through two key innovations: a partial caching technique for transformers that approximates longer sampling trajectories without proportional computational cost, and a hybrid approach formalizing the exploration-exploitation trade-off in adaptive unmasking. Experiments in image and text domains demonstrate our theory as well as the efficiency of our proposed methods, advancing both theoretical understanding and practical implementation of masked diffusion samplers.

**Link**: [arxiv](http://arxiv.org/abs/2510.04525v2),  [pdf](http://arxiv.org/pdf/2510.04525v2)

**Tags**: cs.LG math.PR stat.ML 



### H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance   Acceleration of Generative Diffusion Models
**Authors**: Mingyu Sung, Il-Min Kim, Sangseok Yun, Jae-Mo Kang

**Updated**: 2025-10-31T04:47:14Z

**Summary**: Diffusion models have emerged as state-of-the-art in image generation, but their practical deployment is hindered by the significant computational cost of their iterative denoising process. While existing caching techniques can accelerate inference, they often create a challenging trade-off between speed and fidelity, suffering from quality degradation and high computational overhead. To address these limitations, we introduce H2-Cache, a novel hierarchical caching mechanism designed for modern generative diffusion model architectures. Our method is founded on the key insight that the denoising process can be functionally separated into a structure-defining stage and a detail-refining stage. H2-cache leverages this by employing a dual-threshold system, using independent thresholds to selectively cache each stage. To ensure the efficiency of our dual-check approach, we introduce pooled feature summarization (PFS), a lightweight technique for robust and fast similarity estimation. Extensive experiments on the Flux architecture demonstrate that H2-cache achieves significant acceleration (up to 5.08x) while maintaining image quality nearly identical to the baseline, quantitatively and qualitatively outperforming existing caching methods. Our work presents a robust and practical solution that effectively resolves the speed-quality dilemma, significantly lowering the barrier for the real-world application of high-fidelity diffusion models. Source code is available at https://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2510.27171v1),  [pdf](http://arxiv.org/pdf/2510.27171v1)

**Tags**: cs.CV cs.AI 



### Tokencake: A KV-Cache-centric Serving Framework for LLM-based   Multi-Agent Applications
**Authors**: Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo

**Updated**: 2025-10-31T04:17:05Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2510.18586v2),  [pdf](http://arxiv.org/pdf/2510.18586v2)

**Tags**: cs.DC 



### NeuronMM: High-Performance Matrix Multiplication for LLM Inference on   AWS Trainium
**Authors**: Dinghong Song, Jierui Xu, Weichu Yang, Pengfei Su, Dong Li

**Updated**: 2025-10-31T01:52:13Z

**Summary**: AI accelerators, customized to AI workloads, provide cost-effective and high-performance solutions for training and inference. Trainium, an AI accelerator recently developed by Amazon Web Services (AWS), provides an attractive option for LLM training and inference through its heterogeneous architecture. However, leveraging Trainium architecture for high performance can be challenging because of its systolic array architecture and special requirement on data layout. In this paper, we design high-performance matrix multiplication (matmul), a critical compute kernel, for LLM inference on Trainium. We introduce a series of techniques customized to Trainium based on kernel fusion and novel caching strategies to reduce data movement across the software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive matrix transpose. Evaluating with nine datasets and four recent LLMs, we show that our system largely outperforms the state-of-the-art matmul implemented by AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x speedup (up to 2.22x), which translates to an average 1.66x speedup (up to 2.49x) for end-to-end LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.25977v2),  [pdf](http://arxiv.org/pdf/2510.25977v2)

**Tags**: cs.CL 



### Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review
**Authors**: Dong Tong

**Updated**: 2025-10-31T00:39:27Z

**Summary**: The security and efficiency of modern computing systems are fundamentally undermined by the absence of a native architectural mechanism to propagate high-level program semantics, such as object identity, bounds, and lifetime, across the hardware/software interface. This paper presents a comprehensive survey of the architectural paradigm designed to bridge this semantic gap: descriptor-based, object-aware memory systems. By elevating the descriptor to a first-class architectural abstraction, this paradigm enables hardware to dynamically acquire and enforce the rich semantics of software-defined objects. This survey systematically charts the evolution and current landscape of this approach. We establish the foundational concepts of memory objects and descriptors and introduce a novel taxonomy of descriptor addressing modes, providing a structured framework for analyzing and comparing diverse implementations. Our unified analysis reveals how this paradigm holistically addresses the intertwined challenges of memory protection, management, and processing. As a culminating case study, we re-examine the CentroID model, demonstrating how its hybrid tagged-pointer encoding and descriptor processing mechanisms embody the path toward practical and efficient object-aware designs. Finally, we outline how the explicit cross-layer communication of object semantics provides a foundational research direction for next-generation cache hierarchies, unified virtual memory, and even 128-bit architectures.

**Link**: [arxiv](http://arxiv.org/abs/2510.27070v1),  [pdf](http://arxiv.org/pdf/2510.27070v1)

**Tags**: cs.AR cs.CR 



### Accelerating Diffusion LLMs via Adaptive Parallel Decoding
**Authors**: Daniel Israel, Guy Van den Broeck, Aditya Grover

**Updated**: 2025-10-30T21:11:33Z

**Summary**: The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2506.00413v2),  [pdf](http://arxiv.org/pdf/2506.00413v2)

**Tags**: cs.CL cs.AI cs.LG cs.PF 



### Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache   Hierarchies
**Authors**: Hoa Nguyen, Pongstorn Maidee, Jason Lowe-Power, Alireza Kaviani

**Updated**: 2025-10-30T18:58:02Z

**Summary**: In this paper, we introduce Choreographer, a simulation framework that enables a holistic system-level evaluation of fine-grained accelerators designed for latency-sensitive tasks. Unlike existing frameworks, Choreographer captures all hardware and software overheads in core-accelerator and cache-accelerator interactions, integrating a detailed gem5-based hardware stack featuring an AMBA coherent hub interface (CHI) mesh network and a complete Linux-based software stack. To facilitate rapid prototyping, it offers a C++ application programming interface and modular configuration options. Our detailed cache model provides accurate insights into performance variations caused by cache configurations, which are not captured by other frameworks. The framework is demonstrated through two case studies: a data-aware prefetcher for graph analytics workloads, and a quicksort accelerator. Our evaluation shows that the prefetcher achieves speedups between 1.08x and 1.88x by reducing memory access latency, while the quicksort accelerator delivers more than 2x speedup with minimal address translation overhead. These findings underscore the ability of Choreographer to model complex hardware-software interactions and optimize performance in small task offloading scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2510.26944v1),  [pdf](http://arxiv.org/pdf/2510.26944v1)

**Tags**: cs.AR 



### ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for   Efficient MoE Inference
**Authors**: Zixu Shen, Kexin Chu, Yifan Zhang, Dawei Xiang, Runxin Wu, Wei Zhang

**Updated**: 2025-10-30T17:29:27Z

**Summary**: The expansion of large language models is increasingly limited by the constrained memory capacity of modern GPUs. To mitigate this, Mixture-of-Experts (MoE) architectures activate only a small portion of parameters during inference, significantly lowering both memory demand and computational overhead. However, conventional MoE inference approaches, which select active experts independently at each layer, often introduce considerable latency because of frequent parameter transfers between host and GPU memory. In addition, current cross-layer prediction strategies, which are typically based on fixed steps, lack adaptability across different hardware platforms and workloads, thereby reducing their robustness and effectiveness.   To address these challenges, we present ExpertFlow, a runtime system for MoE inference that combines adaptive expert prefetching and cache-aware routing. ExpertFlow continuously adjusts its prediction horizon for expert activation by leveraging runtime statistics such as transfer bandwidth, parameter dimensionality, and model feedback signals. Furthermore, it incorporates a hybrid cross-layer prediction scheme that fuses pregating information with intermediate computational states to anticipate future expert needs. By adaptively refining prefetching decisions and aligning them with actual usage behavior, ExpertFlow effectively decreases cache misses and removes latency caused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces model stall time to less than 0.1% of the baseline, highlighting its capability to optimize MoE inference under stringent memory constraints.

**Link**: [arxiv](http://arxiv.org/abs/2510.26730v1),  [pdf](http://arxiv.org/pdf/2510.26730v1)

**Tags**: cs.DC cs.AI cs.PF 



### GPU-Accelerated Primal Heuristics for Mixed Integer Programming
**Authors**: Akif Çördük, Piotr Sielski, Alice Boucher, Kumar Aatish

**Updated**: 2025-10-30T13:43:31Z

**Summary**: We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer Programming. Leveraging GPU acceleration enables exploration of larger search regions and faster iterations. A GPU-accelerated PDLP serves as an approximate LP solver, while a new probing cache facilitates rapid roundings and early infeasibility detection. Several state-of-the-art heuristics, including Feasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further accelerated and enhanced. The combined approach of these GPU-driven algorithms yields significant improvements over existing methods, both in the number of feasible solutions and the quality of objectives by achieving 221 feasible solutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved dataset.

**Link**: [arxiv](http://arxiv.org/abs/2510.20499v2),  [pdf](http://arxiv.org/pdf/2510.20499v2)

**Tags**: math.OC cs.DC 



### LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human   Smuggling Networks
**Authors**: Dipak Meher, Carlotta Domeniconi, Guadalupe Correa-Cabrera

**Updated**: 2025-10-30T13:39:08Z

**Summary**: Human smuggling networks are complex and constantly evolving, making them difficult to analyze comprehensively. Legal case documents offer rich factual and procedural insights into these networks but are often long, unstructured, and filled with ambiguous or shifting references, posing significant challenges for automated knowledge graph (KG) construction. Existing methods either overlook coreference resolution or fail to scale beyond short text spans, leading to fragmented graphs and inconsistent entity linking. We propose LINK-KG, a modular framework that integrates a three-stage, LLM-guided coreference resolution pipeline with downstream KG extraction. At the core of our approach is a type-specific Prompt Cache, which consistently tracks and resolves references across document chunks, enabling clean and disambiguated narratives for structured knowledge graph construction from both short and long legal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes by 32.22% compared to baseline methods, resulting in cleaner and more coherent graph structures. These improvements establish LINK-KG as a strong foundation for analyzing complex criminal networks.

**Link**: [arxiv](http://arxiv.org/abs/2510.26486v1),  [pdf](http://arxiv.org/pdf/2510.26486v1)

**Tags**: cs.AI cs.IR cs.LG 



### Model-Document Protocol for AI Search
**Authors**: Hongjin Qian, Zheng Liu

**Updated**: 2025-10-30T08:52:17Z

**Summary**: AI search depends on linking large language models (LLMs) with vast external knowledge sources. Yet web pages, PDF files, and other raw documents are not inherently LLM-ready: they are long, noisy, and unstructured. Conventional retrieval methods treat these documents as verbatim text and return raw passages, leaving the burden of fragment assembly and contextual reasoning to the LLM. This gap underscores the need for a new retrieval paradigm that redefines how models interact with documents.   We introduce the Model-Document Protocol (MDP), a general framework that formalizes how raw text is bridged to LLMs through consumable knowledge representations. Rather than treating retrieval as passage fetching, MDP defines multiple pathways that transform unstructured documents into task-specific, LLM-ready inputs. These include agentic reasoning, which curates raw evidence into coherent context; memory grounding, which accumulates reusable notes to enrich reasoning; and structured leveraging, which encodes documents into formal representations such as graphs or key-value caches. All three pathways share the same goal: ensuring that what reaches the LLM is not raw fragments but compact, structured knowledge directly consumable for reasoning.   As an instantiation, we present MDP-Agent, which realizes the protocol through an agentic process: constructing document-level gist memories for global coverage, performing diffusion-based exploration with vertical exploitation to uncover layered dependencies, and applying map-reduce style synthesis to integrate large-scale evidence into compact yet sufficient context. Experiments on information-seeking benchmarks demonstrate that MDP-Agent outperforms baselines, validating both the soundness of the MDP framework and the effectiveness of its agentic instantiation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25160v2),  [pdf](http://arxiv.org/pdf/2510.25160v2)

**Tags**: cs.CL cs.AI cs.IR 



### How Efficient Are Diffusion Language Models? A Critical Examination of   Efficiency Evaluation Practices
**Authors**: Han Peng, Peiyu Liu, Zican Dong, Daixuan Cheng, Junyi Li, Yiru Tang, Shuo Wang, Wayne Xin Zhao

**Updated**: 2025-10-30T08:46:37Z

**Summary**: Diffusion language models (DLMs) have emerged as a promising alternative to the long-dominant autoregressive (AR) paradigm, offering a parallelable decoding process that could yield greater efficiency. Yet, in practice, current open-source DLMs often underperform their AR counterparts in speed, limiting their real-world utility. This work presents a systematic study of DLM efficiency, identifying key issues in prior evaluation methods. Through empirical benchmarking and a roofline-based theoretical analysis, we demonstrate that AR models generally achieve higher throughput, while DLMs consistently lag. We also investigate acceleration strategies, finding that techniques like dual cache and parallel decoding mainly offer gains at small batch sizes, with their benefits diminishing upon scaling. Our findings underscore the necessity of robust evaluation methods and improved acceleration strategies to advance research on DLMs.

**Link**: [arxiv](http://arxiv.org/abs/2510.18480v2),  [pdf](http://arxiv.org/pdf/2510.18480v2)

**Tags**: cs.CL 



### From req/res to pub/sub: Exploring Media over QUIC Transport for DNS
**Authors**: Mathis Engelbart, Mike Kosek, Lars Eggert, Jörg Ott

**Updated**: 2025-10-30T08:12:53Z

**Summary**: The DNS is a key component of the Internet. Originally designed to facilitate the resolution of host names to IP addresses, its scope has continuously expanded over the years, today covering use cases such as load balancing or service discovery. While DNS was initially conceived as a rather static directory service in which resource records (RR) only change rarely, we have seen a number of use cases over the years where a DNS flavor that isn't purely based upon requesting and caching RRs, but rather on an active distribution of updates for all resolvers that showed interest in the respective records in the past, would be preferable. In this paper, we thus explore a publish-subscribe variant of DNS based on the Media-over-QUIC architecture, where we devise a strawman system and protocol proposal to enable pushing RR updates. We provide a prototype implementation, finding that DNS can benefit from a publish-subscribe variant: next to limiting update traffic, it can considerably reduce the time it takes for a resolver to receive the latest version of a record, thereby supporting use cases such as load balancing in content distribution networks. The publish-subscribe architecture also brings new challenges to the DNS, including a higher overhead for endpoints due to additional state management, and increased query latencies on first lookup, due to session establishment latencies.

**Link**: [arxiv](http://arxiv.org/abs/2510.26234v1),  [pdf](http://arxiv.org/pdf/2510.26234v1)

**Tags**: cs.NI 



### LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based   Video Generation
**Authors**: Huanlin Gao, Ping Chen, Fuyuan Shi, Chao Tan, Zhaoxiang Liu, Fang Zhao, Kai Wang, Shiguo Lian

**Updated**: 2025-10-30T04:57:26Z

**Summary**: We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9x speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis. Our code is available at :https://github.com/UnicomAI/LeMiCa

**Link**: [arxiv](http://arxiv.org/abs/2511.00090v1),  [pdf](http://arxiv.org/pdf/2511.00090v1)

**Tags**: cs.CV cs.AI 



### PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse   Attention for Vision-Language Large Models
**Authors**: Zhonghua Jiang, Kunxi Li, Yiyun Zhou, Sihao Liu, Zhaode Wang, Chengfei lv, Shengyu Zhang

**Updated**: 2025-10-30T03:43:02Z

**Summary**: Vision-Language Large Models (VLLMs) face significant efficiency challenges when processing high-resolution inputs. The quadratic complexity in attention and autoregressive generation, as well as the constantly growing key value (KV) cache size, severely hinder the prefilling and decoding stages. Recent efforts have attempted to compress KV cache by identifying and pruning KV cache of less important tokens, but these methods typically rely on attention scores to estimate token importance, making them incompatible with efficient attention mechanisms such as FlashAttention and Sparse Attention, which do not explicitly compute attention matrices. Moreover, existing methods overlook how sparse attention, while accelerating the prefilling stage, alters the information structure of the KV cache, thereby compromising the effectiveness of downstream KV cache compression strategies. To address this issue, we propose PureKV, a plug-and-play framework for joint optimization of sparse attention and KV cache compression. We first introduce a KV cache compression strategy that is fully compatible with efficient attention accelerators. Our method utilizes lower layer attention scores to estimate the importance of high layers' KV cache, enabling active pruning without compromising accuracy. In addition, we have designed a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically tailored for video KV cache compression algorithms. This module combines spatial and temporal attention sparsity to improve the compression efficiency of KV cache optimization algorithms by purifying spatial noise and temporal redundancy in KV cache. At the same time, ST-SpAttn also accelerated the prefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2, Qwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and 3.16 times prefill acceleration, with negligible quality degradation.

**Link**: [arxiv](http://arxiv.org/abs/2510.25600v2),  [pdf](http://arxiv.org/pdf/2510.25600v2)

**Tags**: cs.MM 



### OneTrans: Unified Feature Interaction and Sequence Modeling with One   Transformer in Industrial Recommender
**Authors**: Zhaoqi Zhang, Haolei Pei, Jun Guo, Tianyu Wang, Yufei Feng, Hui Sun, Shaowei Liu, Aixin Sun

**Updated**: 2025-10-30T03:30:12Z

**Summary**: In recommendation systems, scaling up feature-interaction modules (e.g., Wukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has achieved notable success. However, these efforts typically proceed on separate tracks, which not only hinders bidirectional information exchange but also prevents unified optimization and scaling. In this paper, we propose OneTrans, a unified Transformer backbone that simultaneously performs user-behavior sequence modeling and feature interaction. OneTrans employs a unified tokenizer to convert both sequential and non-sequential attributes into a single token sequence. The stacked OneTrans blocks share parameters across similar sequential tokens while assigning token-specific parameters to non-sequential tokens. Through causal attention and cross-request KV caching, OneTrans enables precomputation and caching of intermediate representations, significantly reducing computational costs during both training and inference. Experimental results on industrial-scale datasets demonstrate that OneTrans scales efficiently with increasing parameters, consistently outperforms strong baselines, and yields a 5.68% lift in per-user GMV in online A/B tests.

**Link**: [arxiv](http://arxiv.org/abs/2510.26104v1),  [pdf](http://arxiv.org/pdf/2510.26104v1)

**Tags**: cs.IR 



### Oneiros: KV Cache Optimization through Parameter Remapping for   Multi-tenant LLM Serving
**Authors**: Ruihao Li, Shagnik Pal, Vineeth Narayan Pullu, Prasoon Sinha, Jeeho Ryoo, Lizy K. John, Neeraja J. Yadwadkar

**Updated**: 2025-10-29T21:56:19Z

**Summary**: KV cache accelerates LLM inference by avoiding redundant computation, at the expense of memory. To support larger KV caches, prior work extends GPU memory with CPU memory via CPU-offloading. This involves swapping KV cache between GPU and CPU memory. However, because the cache updates dynamically, such swapping incurs high CPU memory traffic. We make a key observation that model parameters remain constant during runtime, unlike the dynamically updated KV cache. Building on this, we introduce Oneiros, which avoids KV cache swapping by remapping, and thereby repurposing, the memory allocated to model parameters for KV cache. This parameter remapping is especially beneficial in multi-tenant environments, where the memory used for the parameters of the inactive models can be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth offered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we show that Oneiros significantly outperforms state-of-the-art solutions, achieving a reduction of 44.8%-82.5% in tail time-between-token latency, 20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher throughput compared to vLLM. Source code of Oneiros is available at https://github.com/UT-SysML/Oneiros/.

**Link**: [arxiv](http://arxiv.org/abs/2507.11507v2),  [pdf](http://arxiv.org/pdf/2507.11507v2)

**Tags**: cs.OS 



### Category-Aware Semantic Caching for Heterogeneous LLM Workloads
**Authors**: Chen Wang, Xunzhuo Liu, Yue Zhu, Alaa Youssef, Priya Nagpurkar, Huamin Chen

**Updated**: 2025-10-29T19:59:45Z

**Summary**: LLM serving systems process heterogeneous query workloads where different categories exhibit different characteristics. Code queries cluster densely in embedding space while conversational queries distribute sparsely. Content staleness varies from minutes (stock data) to months (code patterns). Query repetition patterns range from power-law (code) to uniform (conversation), producing long tail cache hit rate distributions: high-repetition categories achieve 40-60% hit rates while low-repetition or volatile categories achieve 5-15% hit rates. Vector databases must exclude the long tail because remote search costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of production traffic uncached. Uniform cache policies compound this problem: fixed thresholds cause false positives in dense spaces and miss valid paraphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This paper presents category-aware semantic caching where similarity thresholds, TTLs, and quotas vary by query category. We present a hybrid architecture separating in-memory HNSW search from external document storage, reducing miss cost from 30ms to 2ms. This reduction makes low-hit-rate categories economically viable (break-even at 3-5% versus 15-20%), enabling cache coverage across the entire workload distribution. Adaptive load-based policies extend this framework to respond to downstream model load, dynamically adjusting thresholds and TTLs to reduce traffic to overloaded models by 9-17% in theoretical projections.

**Link**: [arxiv](http://arxiv.org/abs/2510.26835v1),  [pdf](http://arxiv.org/pdf/2510.26835v1)

**Tags**: cs.DB cs.AI cs.LG 



### Over 3 kV and Ultra-Low leakage Vertical (011) \b{eta}-Ga2O3 Power   Diodes with Engineered Schottky Contact and High-permittivity Dielectric   Field Plate
**Authors**: Emerson J. Hollar, Esmat Farzana

**Updated**: 2025-10-29T17:00:16Z

**Summary**: We report over 3 kV breakdown voltage and ultra-low leakage (011) \b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and high-permittivity (\k{appa}) dielectric (ZrO2) field plate. The (011) orientation of \b{eta}-Ga2O3 enabled low background doping and thick drift layers which are promising to support kV-class vertical \b{eta}-Ga2O3 power switches. The Schottky barrier engineering was performed with a composite Pt cap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse blocking capabilities enabled by PtOx while allowing low turn-on voltage by the interfacing thin Pt layer. We also performed a systematic study using a co-processed Pt/(011) \b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same wafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the field-plate Pt/(011) \b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage of 2.75 kV owing to the edge field management. Further enhancement of the breakdown voltage was achieved by tunneling leakage management using composite Pt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown voltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt (1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011) \b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management by composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage, edge field reduction by high-\k{appa} dielectric ZrO2 field plate, as well as the advantageous material properties offered by (011) \b{eta}-Ga2O3 demonstrate a promising strategy for developing ultra-low leakage and multi-kV class vertical (011) \b{eta}-Ga2O3 power devices.

**Link**: [arxiv](http://arxiv.org/abs/2510.25695v1),  [pdf](http://arxiv.org/pdf/2510.25695v1)

**Tags**: eess.SY cond-mat.mtrl-sci cs.SY 



### Quickest Change Point Detection with Measurements over a Lossy Link
**Authors**: Krishna Chaythanya KV, Saqib Abbas Baba, Anurag Kumar, Arpan Chattopadhyay, Rajesh Sundaresan

**Updated**: 2025-10-29T15:12:35Z

**Summary**: Motivated by Industry 4.0 applications, we consider quickest change detection (QCD) of an abrupt change in a process when its measurements are transmitted by a sensor over a lossy wireless link to a decision maker (DM). The sensor node samples measurements using a Bernoulli sampling process, and places the measurement samples in the transmit queue of its transmitter. The transmitter uses a retransmit-until-success transmission strategy to deliver packets to the DM over the lossy link, in which the packet losses are modeled as a Bernoulli process, with different loss probabilities before and after the change. We pose the QCD problem in the non-Bayesian setting under Lorden's framework, and propose a CUSUM algorithm. By defining a suitable Markov process, involving the DM measurements and the queue length process, we show that the problem reduces to QCD in a Markov process. Characterizing the information measure per measurement sample at the DM, we establish the asymptotic optimality of our algorithm when the false alarm rate tends to zero. Further, when the DM receives incomplete data due to channel loss, we present asymptotically optimal QCD algorithms by suitably modifying the CUSUM algorithm. We then explore the last-come-first-served (LCFS) queuing discipline at the sensor transmit queue to lower detection delay in the non-asymptotic case. Next, we consider the case of multiple sensors, each with its own wireless transmitter queue, and show that our analysis extends to the case of multiple homogeneous sensors. When the sensors are heterogeneous, we present a sensor scheduling algorithm that minimizes detection delay by balancing the trade-off between the age of the observations and their information content. Numerical analysis demonstrate trade-offs that can be used to optimize system design parameters in the non-asymptotic regime.

**Link**: [arxiv](http://arxiv.org/abs/2510.25604v1),  [pdf](http://arxiv.org/pdf/2510.25604v1)

**Tags**: eess.SP 



### RegionE: Adaptive Region-Aware Generation for Efficient Image Editing
**Authors**: Pengtao Chen, Xianfang Zeng, Maosen Zhao, Mingzhu Shen, Peng Ye, Bangyin Xiang, Zhibo Wang, Wei Cheng, Gang Yu, Tao Chen

**Updated**: 2025-10-29T14:58:37Z

**Summary**: Recently, instruction-based image editing (IIE) has received widespread attention. In practice, IIE often modifies only specific regions of an image, while the remaining areas largely remain unchanged. Although these two types of regions differ significantly in generation difficulty and computational redundancy, existing IIE models do not account for this distinction, instead applying a uniform generation process across the entire image. This motivates us to propose RegionE, an adaptive, region-aware generation framework that accelerates IIE tasks without additional training. Specifically, the RegionE framework consists of three main components: 1) Adaptive Region Partition. We observed that the trajectory of unedited regions is straight, allowing for multi-step denoised predictions to be inferred in a single step. Therefore, in the early denoising stages, we partition the image into edited and unedited regions based on the difference between the final estimated result and the reference image. 2) Region-Aware Generation. After distinguishing the regions, we replace multi-step denoising with one-step prediction for unedited areas. For edited regions, the trajectory is curved, requiring local iterative denoising. To improve the efficiency and quality of local iterative generation, we propose the Region-Instruction KV Cache, which reduces computational cost while incorporating global information. 3) Adaptive Velocity Decay Cache. Observing that adjacent timesteps in edited regions exhibit strong velocity similarity, we further propose an adaptive velocity decay cache to accelerate the local denoising process. We applied RegionE to state-of-the-art IIE base models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o confirmed that semantic and perceptual fidelity were well preserved.

**Link**: [arxiv](http://arxiv.org/abs/2510.25590v1),  [pdf](http://arxiv.org/pdf/2510.25590v1)

**Tags**: cs.CV cs.AI 



### FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual   Question Answering
**Authors**: Liangyu Zhong, Fabio Rosenthal, Joachim Sicking, Fabian Hüger, Thorsten Bagdonat, Hanno Gottschalk, Leo Schwinn

**Updated**: 2025-10-29T14:46:17Z

**Summary**: While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and three types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 - 6.5 x less compute.

**Link**: [arxiv](http://arxiv.org/abs/2506.21710v2),  [pdf](http://arxiv.org/pdf/2506.21710v2)

**Tags**: cs.CV 



### Serve Programs, Not Prompts
**Authors**: In Gim, Lin Zhong

**Updated**: 2025-10-29T11:29:03Z

**Summary**: Current large language model (LLM) serving systems, primarily designed for text completion, are neither efficient nor adaptable for increasingly complex LLM applications due to their inflexible design. We propose a new LLM serving system architecture that serves programs instead of prompts to address this problem. These programs, called LLM Inference Programs (LIPs), allow users to customize token prediction and KV cache management at runtime and to offload parts of their application logic, such as tool execution, to the server. We describe an example of this architecture through a system named Symphony, which functions as an operating system for LIPs. Symphony exposes LLM model computations via system calls and virtualizes KV cache with a dedicated file system, while ensuring GPU efficiency with a two-level process scheduling scheme. Symphony has the potential to open the door to a more efficient and extensible ecosystem for LLM applications.

**Link**: [arxiv](http://arxiv.org/abs/2510.25412v1),  [pdf](http://arxiv.org/pdf/2510.25412v1)

**Tags**: cs.CL 



### Off-Centered WoS-Type Solvers with Statistical Weighting
**Authors**: Anchang Bao, Jie Xu, Enya Shen, Jianmin Wang

**Updated**: 2025-10-29T04:09:50Z

**Summary**: Stochastic PDE solvers have emerged as a powerful alternative to traditional discretization-based methods for solving partial differential equations (PDEs), especially in geometry processing and graphics. While off-centered estimators enhance sample reuse in WoS-type Monte Carlo solvers, they introduce correlation artifacts and bias when Green's functions are approximated. In this paper, we propose a statistically weighted off-centered WoS-type estimator that leverages local similarity filtering to selectively combine samples across neighboring evaluation points. Our method balances bias and variance through a principled weighting strategy that suppresses unreliable estimators. We demonstrate our approach's effectiveness on various PDEs,including screened Poisson equations and boundary conditions, achieving consistent improvements over existing solvers such as vanilla Walk on Spheres, mean value caching, and boundary value caching. Our method also naturally extends to gradient field estimation and mixed boundary problems.

**Link**: [arxiv](http://arxiv.org/abs/2510.25152v1),  [pdf](http://arxiv.org/pdf/2510.25152v1)

**Tags**: cs.GR 



### NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized   Generalist Robotic Policies
**Authors**: Jiahong Chen, Jing Wang, Long Chen, Chuwei Cai, Jinghui Lu

**Updated**: 2025-10-29T03:00:36Z

**Summary**: Vision-language-action (VLA) models have significantly advanced robotic manipulation by integrating vision-language models (VLMs), and action decoders into a unified architecture. However, their deployment on resource-constrained edge devices, such as mobile robots or embedded systems (e.g., Jetson Orin Nano), remains challenging due to high computational demands, especially in real-world scenarios where power, latency, and computational resources are critical. To close this gap, we introduce Nano-scale Vision-Language Action (NanoVLA), a family of lightweight VLA architectures that achieve high performance with minimal resources. Our core innovations include: (1) vision-language decoupling that moves conventional early vision and language inputs fusion in VLM to late stage, achieving better performance while enabling caching and reduce inference overhead and latency; (2) long-short action chunking to ensure smooth, coherent multi-step planning without sacrificing real-time responsiveness; (3) dynamic routing that adaptively assigns lightweight or heavy backbones based on task complexity, further optimizing inference efficiency. Experimental results on several benchmarks, as well as real-world deployments, demonstrate that NanoVLA achieves up to 52x faster inference on edge devices compared to previous state-of-the-art VLA models, with 98% less parameters while maintaining or surpassing their task accuracy and generalization. Ablation studies confirm that our decoupling strategy preserves cross-task transferability, and the routing module enhances cost-performance trade-offs, enabling practical, high-precision robotic manipulation on resource-constrained hardware.

**Link**: [arxiv](http://arxiv.org/abs/2510.25122v1),  [pdf](http://arxiv.org/pdf/2510.25122v1)

**Tags**: cs.RO 



### Parallel Loop Transformer for Efficient Test-Time Computation Scaling
**Authors**: Bohong Wu, Mengzhao Chen, Xiang Luo, Shen Yan, Qifan Yu, Fan Xia, Tianqi Zhang, Hongrui Zhan, Zheng Zhong, Xun Zhou, Siyuan Qiao, Xingyan Bin

**Updated**: 2025-10-28T15:35:50Z

**Summary**: Large Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or "loops." However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.

**Link**: [arxiv](http://arxiv.org/abs/2510.24824v1),  [pdf](http://arxiv.org/pdf/2510.24824v1)

**Tags**: cs.CL 



### An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine
**Authors**: Pedram Fard, Alaleh Azhir, Neguine Rezaii, Jiazi Tian, Hossein Estiri

**Updated**: 2025-10-28T12:28:02Z

**Summary**: Artificial intelligence in medicine is built to serve the average patient. By minimizing error across large datasets, most systems deliver strong aggregate accuracy yet falter at the margins: patients with rare variants, multimorbidity, or underrepresented demographics. This average patient fallacy erodes both equity and trust. We propose a different design: a multi-agent ecosystem for N-of-1 decision support. In this environment, agents clustered by organ systems, patient populations, and analytic modalities draw on a shared library of models and evidence synthesis tools. Their results converge in a coordination layer that weighs reliability, uncertainty, and data density before presenting the clinician with a decision-support packet: risk estimates bounded by confidence ranges, outlier flags, and linked evidence. Validation shifts from population averages to individual reliability, measured by error in low-density regions, calibration in the small, and risk--coverage trade-offs. Anticipated challenges include computational demands, automation bias, and regulatory fit, addressed through caching strategies, consensus checks, and adaptive trial frameworks. By moving from monolithic models to orchestrated intelligence, this approach seeks to align medical AI with the first principle of medicine: care that is transparent, equitable, and centered on the individual.

**Link**: [arxiv](http://arxiv.org/abs/2510.24359v1),  [pdf](http://arxiv.org/pdf/2510.24359v1)

**Tags**: cs.AI cs.SY eess.SY q-bio.QM stat.AP 



### SALS: Sparse Attention in Latent Space for KV cache Compression
**Authors**: Junlin Mu, Hantao Huang, Jihang Zhang, Minghui Yu, Tao Wang, Yidong Li

**Updated**: 2025-10-28T10:32:52Z

**Summary**: Large Language Models capable of handling extended contexts are in high demand, yet their inference remains challenging due to substantial Key-Value cache size and high memory bandwidth requirements. Previous research has demonstrated that KV cache exhibits low-rank characteristics within the hidden dimension, suggesting the potential for effective compression. However, due to the widely adopted Rotary Position Embedding mechanism in modern LLMs, naive low-rank compression suffers severe accuracy degradation or creates a new speed bottleneck, as the low-rank cache must first be reconstructed in order to apply RoPE. In this paper, we introduce two key insights: first, the application of RoPE to the key vectors increases their variance, which in turn results in a higher rank; second, after the key vectors are transformed into the latent space, they largely maintain their representation across most layers. Based on these insights, we propose the Sparse Attention in Latent Space framework. SALS projects the KV cache into a compact latent space via low-rank projection, and performs sparse token selection using RoPE-free query-key interactions in this space. By reconstructing only a small subset of important tokens, it avoids the overhead of full KV cache reconstruction. We comprehensively evaluate SALS on various tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and additionally verify its scalability on the RULER-128k benchmark with LLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA performance by maintaining competitive accuracy. Under different settings, SALS achieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention operator compared to FlashAttention2 on the 4K sequence. For the end-to-end throughput performance, we achieves 1.4-fold and 4.5-fold improvement compared to GPT-fast on 4k and 32K sequences, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2510.24273v1),  [pdf](http://arxiv.org/pdf/2510.24273v1)

**Tags**: cs.LG 



### Pie: A Programmable Serving System for Emerging LLM Applications
**Authors**: In Gim, Zhiyao Ma, Seung-seob Lee, Lin Zhong

**Updated**: 2025-10-28T04:17:55Z

**Summary**: Emerging large language model (LLM) applications involve diverse reasoning strategies and agentic workflows, straining the capabilities of existing serving systems built on a monolithic token generation loop. This paper introduces Pie, a programmable LLM serving system designed for flexibility and efficiency. Pie decomposes the traditional generation loop into fine-grained service handlers exposed via an API and delegates control of the generation process to user-provided programs, called inferlets. This enables applications to implement new KV cache strategies, bespoke generation logic, and seamlessly integrate computation and I/O-entirely within the application, without requiring modifications to the serving system. Pie executes inferlets using WebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows Pie matches state-of-the-art performance on standard tasks (3-12% latency overhead) while significantly improving latency and throughput (1.3x-3.4x higher) on agentic workflows by enabling application-specific optimizations.

**Link**: [arxiv](http://arxiv.org/abs/2510.24051v1),  [pdf](http://arxiv.org/pdf/2510.24051v1)

**Tags**: cs.CL 



### FastKV: KV Cache Compression for Fast Long-Context Processing with   Token-Selective Propagation
**Authors**: Dongwon Jo, Jiwon Song, Yulhwa Kim, Jae-Joon Kim

**Updated**: 2025-10-28T04:00:18Z

**Summary**: While large language models (LLMs) excel at handling long-context sequences, they require substantial prefill computation and key-value (KV) cache, which can heavily burden computational efficiency and memory usage in both prefill and decoding stages. Recent works that compress KV caches with prefill acceleration reduce this cost but inadvertently tie the prefill compute reduction to the decoding KV budget. This coupling arises from overlooking the layer-dependent variation of critical context, often leading to accuracy degradation. To address this issue, we introduce FastKV, a KV cache compression framework designed to reduce latency in both prefill and decoding by leveraging the stabilization of token importance in later layers. FastKV performs full-context computation until a Token-Selective Propagation (TSP) layer, which forwards only the most informative tokens to subsequent layers. From these propagated tokens, FastKV independently selects salient KV entries for caching, thereby decoupling KV budget from the prefill compute reduction based on the TSP decision. This independent control of the TSP rate and KV retention rate enables flexible optimization of efficiency and accuracy. Experimental results show that FastKV achieves speedups of up to 1.82$\times$ in prefill and 2.87$\times$ in decoding compared to the full-context baseline, while matching the accuracy of the baselines that only accelerate the decoding stage. Our code is available at https://github.com/dongwonjo/FastKV.

**Link**: [arxiv](http://arxiv.org/abs/2502.01068v4),  [pdf](http://arxiv.org/pdf/2502.01068v4)

**Tags**: cs.LG cs.CL 



### STree: Speculative Tree Decoding for Hybrid State-Space Models
**Authors**: Yangchao Wu, Zongyue Qin, Alex Wong, Stefano Soatto

**Updated**: 2025-10-27T21:48:48Z

**Summary**: Speculative decoding is a technique to leverage hardware concurrency in order to enable multiple steps of token generation in a single forward pass, thus improving the efficiency of large-scale autoregressive (AR) Transformer models. State-space models (SSMs) are already more efficient than AR Transformers, since their state summarizes all past data with no need to cache or re-process tokens in the sliding window context. However, their state can also comprise thousands of tokens; so, speculative decoding has recently been extended to SSMs. Existing approaches, however, do not leverage the tree-based verification methods, since current SSMs lack the means to compute a token tree efficiently. We propose the first scalable algorithm to perform tree-based speculative decoding in state-space models (SSMs) and hybrid architectures of SSMs and Transformer layers. We exploit the structure of accumulated state transition matrices to facilitate tree-based speculative decoding with minimal overhead relative to current SSM implementations. Along with the algorithm, we describe a hardware-aware implementation that improves naive application of AR Transformer tree-based speculative decoding methods to SSMs. Furthermore, we outperform vanilla speculative decoding with SSMs even with a baseline drafting model and tree structure on three different benchmarks, opening up opportunities for further speed up with SSM and hybrid model inference. Code can be found at: https://github.com/wyc1997/stree.

**Link**: [arxiv](http://arxiv.org/abs/2505.14969v2),  [pdf](http://arxiv.org/pdf/2505.14969v2)

**Tags**: cs.LG cs.AI 



### KV-weights are all you need for skipless transformers
**Authors**: Nils Graef

**Updated**: 2025-10-27T17:31:15Z

**Summary**: He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the V and P (post-attention projection) linear layers, which reduces the total number of weights. However, this scheme is only applicable to MHA (multi-head attention), but not for MQA (multi-query attention) and GQA (grouped-query attention). The latter schemes are used by many popular LLMs such as Llama 2, Mistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes mathematically equivalent versions that are suitable for MQA and GQA. For example, removing Q and P from a skipless version of Mistral-7B would remove 15% of its weights (and thus reduce its compute and memory complexity). Watch our explainer video https://youtu.be/Tx_lMpphd2g and see https://github.com/OpenMachine-ai/transformer-tricks for code and more transformer tricks.

**Link**: [arxiv](http://arxiv.org/abs/2404.12362v2),  [pdf](http://arxiv.org/pdf/2404.12362v2)

**Tags**: cs.LG 



### Leveraging Approximate Caching for Faster Retrieval-Augmented Generation
**Authors**: Shai Bergman, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis Randl, Martijn de Vos, Ji Zhang

**Updated**: 2025-10-27T16:20:28Z

**Summary**: Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing the reliance on expensive vector database lookups. To efficiently scale, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question-answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically-skewed MedRAG workload reduces database calls by 77.2% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our results demonstrate that approximate caching is a practical and effective strategy for optimizing RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2503.05530v3),  [pdf](http://arxiv.org/pdf/2503.05530v3)

**Tags**: cs.DB cs.LG cs.PF 



### A Data-driven ML Approach for Maximizing Performance in LLM-Adapter   Serving
**Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral

**Updated**: 2025-10-27T14:59:46Z

**Summary**: With the rapid adoption of Large Language Models (LLMs), LLM-adapters have become increasingly common, providing lightweight specialization of large-scale models. Serving hundreds or thousands of these adapters on a single GPU allows request aggregation, increasing throughput, but may also cause request starvation if GPU memory limits are exceeded. To address this issue, this study focuses on determining the joint configuration of concurrent and parallel adapters that maximizes GPU throughput without inducing starvation, given heterogeneous adapter and traffic properties. We propose a data-driven ML approach leveraging interpretable models to tackle this caching problem and introduce the first Digital Twin capable of reproducing an LLM-adapter serving system, enabling efficient training data generation. Experiments with the vLLM framework and LoRA adapters show that the Digital Twin reproduces throughput within 5.1% of real results, while the ML approach predicts optimal numbers of concurrent and parallel adapters with an error of at most 7.2% under heterogeneous, real-world workloads.

**Link**: [arxiv](http://arxiv.org/abs/2508.08343v2),  [pdf](http://arxiv.org/pdf/2508.08343v2)

**Tags**: cs.PF cs.AI cs.CL 



### PESTO: Real-Time Pitch Estimation with Self-supervised   Transposition-equivariant Objective
**Authors**: Alain Riou, Bernardo Torres, Ben Hayes, Stefan Lattner, Gaëtan Hadjeres, Gaël Richard, Geoffroy Peeters

**Updated**: 2025-10-27T11:55:07Z

**Summary**: In this paper, we introduce PESTO, a self-supervised learning approach for single-pitch estimation using a Siamese architecture. Our model processes individual frames of a Variable-$Q$ Transform (VQT) and predicts pitch distributions. The neural network is designed to be equivariant to translations, notably thanks to a Toeplitz fully-connected layer. In addition, we construct pitch-shifted pairs by translating and cropping the VQT frames and train our model with a novel class-based transposition-equivariant objective, eliminating the need for annotated data. Thanks to this architecture and training objective, our model achieves remarkable performances while being very lightweight ($130$k parameters). Evaluations on music and speech datasets (MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms self-supervised baselines but also competes with supervised methods, exhibiting superior cross-dataset generalization. Finally, we enhance PESTO's practical utility by developing a streamable VQT implementation using cached convolutions. Combined with our model's low latency (less than 10 ms) and minimal parameter count, this makes PESTO particularly suitable for real-time applications.

**Link**: [arxiv](http://arxiv.org/abs/2508.01488v2),  [pdf](http://arxiv.org/pdf/2508.01488v2)

**Tags**: cs.SD cs.AI cs.LG eess.AS 



### Batch Speculative Decoding Done Right
**Authors**: Ranran Haoran Zhang, Soumik Dey, Ashirbad Mishra, Hansi Wu, Binbin Li, Rui Zhang

**Updated**: 2025-10-26T23:59:23Z

**Summary**: Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3$\times$ throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec.

**Link**: [arxiv](http://arxiv.org/abs/2510.22876v1),  [pdf](http://arxiv.org/pdf/2510.22876v1)

**Tags**: cs.CL cs.AI 



### FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline
**Authors**: Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen

**Updated**: 2025-10-26T13:31:41Z

**Summary**: Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year and has been open-sourced.

**Link**: [arxiv](http://arxiv.org/abs/2507.10367v3),  [pdf](http://arxiv.org/pdf/2507.10367v3)

**Tags**: cs.DC cs.PF 



### SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression   Block Size
**Authors**: Jinhan Chen, Jianchun Liu, Hongli Xu, Xianjun Gao, Shilong Wang

**Updated**: 2025-10-26T07:17:10Z

**Summary**: The growing memory footprint of the Key-Value (KV) cache poses a severe scalability bottleneck for long-context Large Language Model (LLM) inference. While KV cache eviction has emerged as an effective solution by discarding less critical tokens, existing token-, block-, and sentence-level compression methods struggle to balance semantic coherence and memory efficiency. To this end, we introduce SABlock, a \underline{s}emantic-aware KV cache eviction framework with \underline{a}daptive \underline{block} sizes. Specifically, SABlock first performs semantic segmentation to align compression boundaries with linguistic structures, then applies segment-guided token scoring to refine token importance estimation. Finally, for each segment, a budget-driven search strategy adaptively determines the optimal block size that preserves semantic integrity while improving compression efficiency under a given cache budget. Extensive experiments on long-context benchmarks demonstrate that SABlock consistently outperforms state-of-the-art baselines under the same memory budgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9% retrieval accuracy with only 96 KV entries, nearly matching the performance of the full-cache baseline that retains up to 8K entries. Under a fixed cache budget of 1,024, SABlock further reduces peak memory usage by 46.28% and achieves up to 9.5x faster decoding on a 128K context length.

**Link**: [arxiv](http://arxiv.org/abs/2510.22556v1),  [pdf](http://arxiv.org/pdf/2510.22556v1)

**Tags**: cs.CL 



### AttentionPredictor: Temporal Patterns Matter for KV Cache Compression
**Authors**: Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, Bin Li

**Updated**: 2025-10-26T04:25:10Z

**Summary**: With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through static modeling of attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the temporal patterns in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose AttentionPredictor, which is the first learning-based method to directly predict attention patterns for KV cache compression and critical token identification. Specifically, AttentionPredictor learns a lightweight, unified convolution model to dynamically capture spatiotemporal patterns and predict the next-token attention scores. An appealing feature of AttentionPredictor is that it accurately predicts the attention score and shares the unified prediction model, which consumes negligible memory, among all transformer layers. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves 13$\times$ KV cache compression and 5.6$\times$ speedup in a cache offloading scenario with comparable LLM performance, significantly outperforming the state-of-the-arts. The code is available at https://github.com/MIRALab-USTC/LLM-AttentionPredictor.

**Link**: [arxiv](http://arxiv.org/abs/2502.04077v3),  [pdf](http://arxiv.org/pdf/2502.04077v3)

**Tags**: cs.CL cs.LG 



### A machine learning framework integrating seed traits and plasma   parameters for predicting germination uplift in crops
**Authors**: Saklain Niam, Tashfiqur Rahman, Md. Amjad Patwary, Mukarram Hossain

**Updated**: 2025-10-26T01:25:24Z

**Summary**: Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet outcomes remain difficult to predict due to complex seed--plasma--environment interactions. This study introduces the first machine learning framework to forecast germination uplift in soybean, barley, sunflower, radish, and tomato under dielectric barrier discharge (DBD) plasma. Among the models tested (GB, XGB, ET, and hybrids), Extra Trees (ET) performed best (R\textsuperscript{2} = 0.919; RMSE = 3.21; MAE = 2.62), improving to R\textsuperscript{2} = 0.925 after feature reduction. Engineering analysis revealed a hormetic response: negligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for 200--500 s, and reduced germination beyond 20 kV or prolonged exposures. Discharge power was also a dominant factor, with germination rate maximizing at $\geq$100 W with low exposure time. Species and cultivar-level predictions showed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high consistency, while sunflower remained slightly higher variable (MAE = 3.80). Among cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted, while Arian (2.86) and Ny\'{\i}rs\'{e}gi fekete (3.74) were comparatively poorly captured. This framework was also embedded into MLflow, providing a decision-support tool for optimizing CP seed germination in precision agriculture.

**Link**: [arxiv](http://arxiv.org/abs/2510.23657v1),  [pdf](http://arxiv.org/pdf/2510.23657v1)

**Tags**: cs.LG 



### Backward-Friendly Optimization: Training Large Language Models with   Approximate Gradients under Memory Constraints
**Authors**: Jing Yang, Kaitong Cai, Yijia Fan, Yufeng Yang, Keze Wang

**Updated**: 2025-10-26T00:50:12Z

**Summary**: Full fine-tuning of Large Language Models (LLMs) is notoriously memory-intensive, primarily because conventional optimizers such as SGD or Adam assume access to exact gradients derived from cached activations. Existing solutions either alter the model architecture (e.g., reversible networks) or trade memory for computation (e.g., activation checkpointing), but the optimizer itself remains untouched. In this work, we introduce GradLite, a backward-friendly optimizer that relaxes the requirement of exact gradients, enabling efficient training even when intermediate activations are aggressively discarded or approximated. GradLite leverages two key techniques: (i) low-rank Jacobian approximation, which reduces the dimensionality of backpropagated error signals, and (ii) error-feedback correction, which accumulates and compensates approximation errors across iterations to preserve convergence guarantees. We provide a theoretical analysis showing that GradLite maintains unbiased gradient estimates with bounded variance, ensuring convergence rates comparable to Adam. Empirically, GradLite reduces optimizer-state and activation memory consumption by up to 50\% without architectural changes, and achieves on-par or superior downstream performance on reasoning (MMLU, GSM8K), multilingual, and dialogue benchmarks compared to checkpointing and optimizer-centric baselines (LoMo, GaLore).

**Link**: [arxiv](http://arxiv.org/abs/2510.22467v1),  [pdf](http://arxiv.org/pdf/2510.22467v1)

**Tags**: cs.LG cs.AI 



### Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation
**Authors**: Sangmin Bae, Yujin Kim, Reza Bayat, Sungnyun Kim, Jiyoun Ha, Tal Schuster, Adam Fisch, Hrayr Harutyunyan, Ziwei Ji, Aaron Courville, Se-Young Yun

**Updated**: 2025-10-25T14:12:56Z

**Summary**: Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to further decrease memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines. These gains demonstrate that MoR is an effective path towards large-model quality without incurring large-model cost.

**Link**: [arxiv](http://arxiv.org/abs/2507.10524v3),  [pdf](http://arxiv.org/pdf/2507.10524v3)

**Tags**: cs.CL cs.LG 



### Efficient Low Rank Attention for Long-Context Inference in Large   Language Models
**Authors**: Tenghui Li, Guoxu Zhou, Xuyang Zhao, Yuning Qiu, Qibin Zhao

**Updated**: 2025-10-25T11:43:27Z

**Summary**: As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two-stage framework that jointly decomposes the full-precision query and key matrices into compact rank-\(r\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \(\mathcal{O}(lr)\) time at each decode step. By selecting only the top-\(k\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at https://github.com/tenghuilee/LRQK.

**Link**: [arxiv](http://arxiv.org/abs/2510.23649v1),  [pdf](http://arxiv.org/pdf/2510.23649v1)

**Tags**: cs.LG cs.AI 



### Fundamental Limits of Coded Caching with Fixed Subpacketization
**Authors**: Minquan Cheng, Yifei Huang, Youlong Wu, Jinyan Wang

**Updated**: 2025-10-25T03:34:34Z

**Summary**: Coded caching is a promising technique to create coded multicast opportunities for cache-aided networks. By splitting each file into $F$ equal packets (i.e., the subpacketization level $F$) and letting each user cache a set of packets, the transmission load can be significantly reduced via coded multicasting. It has been shown that a higher subpacketization level could potentially lead to a lower transmission load, as more packets can be combined for efficient transmission. On the other hand, a larger $F$ indicates a higher coding complexity and is problematic from a practical perspective when $F$ is extremely large. Despite many works attempting to design coded caching schemes with low subpacketization levels, a fundamental problem remains open: What is the minimum transmission load given any fixed subpacketization level? In this paper, we consider the classical cache-aided networks with identically uncoded placement and one-shot delivery strategy, and investigate the fundamental trade-off between the transmission load and the subpacketization level. We propose a \emph{general} lower bound on the transmission load for any fixed subpacketization by reformulating the centralized coded caching schemes via the combinatorial structure of the corresponding placement delivery array. The lower bound also recovers existing optimality results for the bipartite graph scheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the conjugate MN scheme) as well as the grouping bipartite graph scheme. Furthermore, by carefully exploiting the combinatorial structure and computing the union size of sorted sets, we establish a new optimality result, i.e., the partition scheme can achieve the optimal rate-subpacketization trade-off.

**Link**: [arxiv](http://arxiv.org/abs/2510.22145v1),  [pdf](http://arxiv.org/pdf/2510.22145v1)

**Tags**: cs.IT math.IT 



### EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient   Image Editing
**Authors**: Zexuan Yan, Yue Ma, Chang Zou, Wenteng Chen, Qifeng Chen, Linfeng Zhang

**Updated**: 2025-10-25T02:29:47Z

**Summary**: Inversion-based image editing is rapidly gaining momentum while suffering from significant computation overhead, hindering its application in real-time interactive scenarios. In this paper, we rethink that the redundancy in inversion-based image editing exists in both the spatial and temporal dimensions, such as the unnecessary computation in unedited regions and the redundancy in the inversion progress. To tackle these challenges, we propose a practical framework, named EEdit, to achieve efficient image editing. Specifically, we introduce three techniques to solve them one by one. For spatial redundancy, spatial locality caching is introduced to compute the edited region and its neighboring regions while skipping the unedited regions, and token indexing preprocessing is designed to further accelerate the caching. For temporal redundancy, inversion step skipping is proposed to reuse the latent for efficient editing. Our experiments demonstrate an average of 2.46 $\times$ acceleration without performance drop in a wide range of editing tasks including prompt-guided image editing, dragging and image composition. Our codes are available at https://github.com/yuriYanZeXuan/EEdit

**Link**: [arxiv](http://arxiv.org/abs/2503.10270v3),  [pdf](http://arxiv.org/pdf/2503.10270v3)

**Tags**: cs.CV 



### Massive Memorization with Hundreds of Trillions of Parameters for   Sequential Transducer Generative Recommenders
**Authors**: Zhimin Chen, Chenyu Zhao, Ka Chun Mo, Yunjiang Jiang, Jane H. Lee, Shouwei Chen, Khushhall Chandra Mahajan, Ning Jiang, Kai Ren, Jinhui Li, Wen-Yun Yang

**Updated**: 2025-10-24T22:17:49Z

**Summary**: Modern large-scale recommendation systems rely heavily on user interaction history sequences to enhance the model performance. The advent of large language models and sequential modeling techniques, particularly transformer-like architectures, has led to significant advancements recently (e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories (10k to 100k items) generally improves model performance, it also creates significant challenges on latency, queries per second (QPS) and GPU cost in industry-scale recommendation systems. Existing models do not adequately address these industrial scalability issues. In this paper, we propose a novel two-stage modeling framework, namely VIrtual Sequential Target Attention (VISTA), which decomposes traditional target attention from a candidate item to user history items into two distinct stages: (1) user history summarization into a few hundred tokens; followed by (2) candidate item attention to those tokens. These summarization token embeddings are then cached in storage system and then utilized as sequence features for downstream model training and inference. This novel design for scalability enables VISTA to scale to lifelong user histories (up to one million items) while keeping downstream training and inference costs fixed, which is essential in industry. Our approach achieves significant improvements in offline and online metrics and has been successfully deployed on an industry leading recommendation platform serving billions of users.

**Link**: [arxiv](http://arxiv.org/abs/2510.22049v1),  [pdf](http://arxiv.org/pdf/2510.22049v1)

**Tags**: cs.IR cs.LG 



### BachVid: Training-Free Video Generation with Consistent Background and   Character
**Authors**: Han Yan, Xibin Song, Yifu Wang, Hongdong Li, Pan Ji, Chao Ma

**Updated**: 2025-10-24T17:56:37Z

**Summary**: Diffusion Transformers (DiTs) have recently driven significant progress in text-to-video (T2V) generation. However, generating multiple videos with consistent characters and backgrounds remains a significant challenge. Existing methods typically rely on reference images or extensive training, and often only address character consistency, leaving background consistency to image-to-video models. We introduce BachVid, the first training-free method that achieves consistent video generation without needing any reference images. Our approach is based on a systematic analysis of DiT's attention mechanism and intermediate features, revealing its ability to extract foreground masks and identify matching points during the denoising process. Our method leverages this finding by first generating an identity video and caching the intermediate variables, and then inject these cached variables into corresponding positions in newly generated videos, ensuring both foreground and background consistency across multiple videos. Experimental results demonstrate that BachVid achieves robust consistency in generated videos without requiring additional training, offering a novel and efficient solution for consistent video generation without relying on reference images or additional training.

**Link**: [arxiv](http://arxiv.org/abs/2510.21696v1),  [pdf](http://arxiv.org/pdf/2510.21696v1)

**Tags**: cs.CV 



### Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention   and Contextualized Learnable Token Eviction
**Authors**: Mutian He, Philip N. Garner

**Updated**: 2025-10-24T16:56:22Z

**Summary**: Linear-attention models that compress the entire input sequence into a fixed-size recurrent state offer an efficient alternative to Transformers, but their finite memory induces forgetfulness that harms retrieval-intensive tasks. To mitigate the issue, we explore a series of hybrid models that restore direct access to past tokens. We interleave token mixers with intermediate time and space complexity between linear and full attention, including sparse attention with token eviction, and the query-aware native sparse attention. Particularly, we propose a novel learnable token eviction approach. Combined with sliding-window attention, an end-to-end trainable lightweight CNN aggregates information from both past and future adjacent tokens to adaptively retain a limited set of critical KV-pairs per head, maintaining linear attention's constant time and space complexity. Efficient Triton kernels for the sparse attention mechanisms are provided. Empirical evaluations on retrieval-intensive benchmarks support the effectiveness of our approaches.

**Link**: [arxiv](http://arxiv.org/abs/2510.20787v2),  [pdf](http://arxiv.org/pdf/2510.20787v2)

**Tags**: cs.CL cs.LG 



### Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory
**Authors**: Manel Lurbe, Miguel Avargues, Salvador Petit, Maria E. Gomez, Rui Yang, Guanhao Wang, Julio Sahuquillo

**Updated**: 2025-10-24T14:55:42Z

**Summary**: Emerging applications, such as big data analytics and machine learning, require increasingly large amounts of main memory, often exceeding the capacity of current commodity processors built on DRAM technology. To address this, recent research has focused on off-chip memory controllers that facilitate access to diverse memory media, each with unique density and latency characteristics. While these solutions improve memory system performance, they also exacerbate the already significant memory latency. As a result, multi-level prefetching techniques are essential to mitigate these extended latencies.   This paper investigates the advantages of prefetching across both sides of the memory system: the off-chip memory and the on-chip cache hierarchy. Our primary objective is to assess the impact of a multi-level prefetching engine on overall system performance. Additionally, we analyze the individual contribution of each prefetching level to system efficiency. To achieve this, the study evaluates two key prefetching approaches: HMC (Hybrid Memory Controller) and HMC+L1, both of which employ prefetching mechanisms commonly used by processor vendors. The HMC approach integrates a prefetcher within the off-chip hybrid memory controller, while the HMC+L1 approach combines this with additional L1 on-chip prefetchers.   Experimental results on an out-of-order execution processor show that on-chip cache prefetchers are crucial for maximizing the benefits of off-chip prefetching, which in turn further enhances performance. Specifically, the off-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and up to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher coverage to as much as 92%. Consequently, overall performance increases from 9% with the HMC approach to 12% when L1 prefetching is also employed.

**Link**: [arxiv](http://arxiv.org/abs/2509.17388v2),  [pdf](http://arxiv.org/pdf/2509.17388v2)

**Tags**: cs.DC 



### Global Predecessor Indexing: Avoiding Binary Search in Weighted Job   Scheduling
**Authors**: Amit Joshi

**Updated**: 2025-10-24T11:53:34Z

**Summary**: We present an improved solution to the Weighted Job Scheduling (WJS) problem. While the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n \log(n))$ time due to comparison-based sorting and per-job binary search, we eliminate the binary search bottleneck. In its place, we introduce a novel multi-phase preprocessing technique called \emph{Global Predecessor Indexing (GPI)}, which computes the latest non-overlapping job (i.e., the predecessor) for all jobs via a two-pointer linear-time pass after sorting. This yields a time complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI enables direct use in the classical DP recurrence. When combined with linear-time sorting, GPI yields a complete $O(n)$ solution. Even with comparison-based sorting, GPI significantly outperforms the classical solution in practice by avoiding repeated binary searches in favor of the more cache-efficient extra sort and two-pointer pass.

**Link**: [arxiv](http://arxiv.org/abs/2506.22922v2),  [pdf](http://arxiv.org/pdf/2506.22922v2)

**Tags**: cs.DS 



### Compositional Monte Carlo Tree Diffusion for Extendable Planning
**Authors**: Jaesik Yoon, Hyeonseo Cho, Sungjin Ahn

**Updated**: 2025-10-24T11:42:38Z

**Summary**: Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs.

**Link**: [arxiv](http://arxiv.org/abs/2510.21361v1),  [pdf](http://arxiv.org/pdf/2510.21361v1)

**Tags**: cs.LG 



### A General Solution for the Implementation of CI/CD in Embedded Linux   Development
**Authors**: Behnam Agahi, Hamed Farbeh

**Updated**: 2025-10-24T08:35:21Z

**Summary**: With the growing use of embedded systems in various industries, the need for automated platforms for the development and deployment of customized Linux-based operating systems has become more important. This research was conducted with the aim of designing and implementing an integrated and reproducible infrastructure for the development, building, and testing of a Linux-based operating system using the Yocto Project. The proposed structure was implemented based on a three-layer architecture consisting of the main Yocto repositories, a custom layer (meta-custom), and a coordinating manifest layer to ensure version synchronization, scalability, and reproducibility. Three sample projects, including libhelloworld, helloworld, and the kernel module hello mod, were developed and integrated into the build process. Continuous Integration and Continuous Deployment pipelines were implemented with GitLab CI and combined with an isolated Docker environment to automate and streamline the build and testing workflows. Using a local cache server containing hashserv, downloads and sstate cache significantly reduced the build time. The functionality and stability of the system were verified through six boot test scenarios in the QEMU simulator. The results show that the proposed design not only ensures reproducibility but also can be extended to advanced applications such as continuous deployment of real-time Linux versions. Future recommendations include expanding automated tests, implementing system monitoring with Prometheus and Grafana, using distributed builds, optimizing with Docker multi-stage builds, and enabling continuous deployment of real-time Linux changes to provide a stable and scalable model for industrial and research projects in embedded systems with a rapid and reliable development cycle.

**Link**: [arxiv](http://arxiv.org/abs/2510.19240v2),  [pdf](http://arxiv.org/pdf/2510.19240v2)

**Tags**: cs.SE 



### InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video   Understanding
**Authors**: Minsoo Kim, Kyuhong Shim, Jungwook Choi, Simyung Chang

**Updated**: 2025-10-24T05:39:03Z

**Summary**: Modern multimodal large language models (MLLMs) can reason over hour-long video, yet their key-value (KV) cache grows linearly with time-quickly exceeding the fixed memory of phones, AR glasses, and edge robots. Prior compression schemes either assume the whole video and user query are available offline or must first build the full cache, so memory still scales with stream length. InfiniPot-V is the first training-free, query-agnostic framework that enforces a hard, length-independent memory cap for streaming video understanding. During video encoding it monitors the cache and, once a user-set threshold is reached, runs a lightweight compression pass that (i) removes temporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii) keeps semantically significant tokens via Value-Norm (VaN) ranking. Across four open-source MLLMs and four long-video and streaming-video benchmarks, InfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation, and matches or surpasses full-cache accuracy-even in multi-turn dialogues. By dissolving the KV cache bottleneck without retraining or query knowledge, InfiniPot-V closes the gap for on-device streaming video assistants.

**Link**: [arxiv](http://arxiv.org/abs/2506.15745v2),  [pdf](http://arxiv.org/pdf/2506.15745v2)

**Tags**: eess.IV cs.LG 



### Reasoning Path Compression: Compressing Generation Trajectories for   Efficient LLM Reasoning
**Authors**: Jiwon Song, Dongwon Jo, Yulhwa Kim, Jae-Joon Kim

**Updated**: 2025-10-24T04:48:06Z

**Summary**: Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and reduce throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining cache entries that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full KV cache, with an accuracy drop of 1.2\% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.

**Link**: [arxiv](http://arxiv.org/abs/2505.13866v2),  [pdf](http://arxiv.org/pdf/2505.13866v2)

**Tags**: cs.CL 



### Tensor Product Attention Is All You Need
**Authors**: Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Zhen Qin, Yang Yuan, Quanquan Gu, Andrew Chi-Chih Yao

**Updated**: 2025-10-23T23:35:32Z

**Summary**: Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA.

**Link**: [arxiv](http://arxiv.org/abs/2501.06425v5),  [pdf](http://arxiv.org/pdf/2501.06425v5)

**Tags**: cs.CL cs.AI cs.LG 



### T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic   Planning
**Authors**: Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang, Sambit Sahu, Milind Naphade, Genta Indra Winata

**Updated**: 2025-10-23T21:31:35Z

**Summary**: Large Language Models (LLMs) have demonstrated impressive capabilities as intelligent agents capable of solving complex problems. However, effective planning in scenarios involving dependencies between API or tool calls-particularly in multi-turn conversations-remains a significant challenge. To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn conversational dataset specifically designed to capture and manage inter-tool dependencies across diverse domains. T1 enables rigorous evaluation of agents' ability to coordinate tool use across nine distinct domains (4 single domain and 5 multi-domain) with the help of an integrated caching mechanism for both short- and long-term memory, while supporting dynamic replanning-such as deciding whether to recompute or reuse cached results. Beyond facilitating research on tool use and planning, T1 also serves as a benchmark for evaluating the performance of open-weight and proprietary large language models. We present results powered by T1-Agent, highlighting their ability to plan and reason in complex, tool-dependent scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.16986v2),  [pdf](http://arxiv.org/pdf/2505.16986v2)

**Tags**: cs.CL cs.AI 



### Panorama: Fast-Track Nearest Neighbors
**Authors**: Vansh Ramani, Alexis Schlomer, Akash Nayar, Sayan Ranu, Jignesh M. Patel, Panagiotis Karras

**Updated**: 2025-10-23T19:45:39Z

**Summary**: Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose embeddings are close to that of a given query in a high-dimensional space, aiming to balance accuracy with speed. Used in recommendation systems, image and video retrieval, natural language processing, and retrieval-augmented generation (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT utilize graph, tree, clustering, and quantization techniques to navigate large vector spaces. Despite this progress, ANNS systems spend up to 99% of query time to compute distances in their final refinement phase. In this paper, we present PANORAMA, a machine learning-driven approach that tackles the ANNS verification bottleneck through data-adaptive learned orthogonal transforms that facilitate the accretive refinement of distance bounds. Such transforms compact over 90% of signal energy into the first half of dimensions, enabling early candidate pruning with partial distance computations. We integrate PANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and Annoy, without index modification, using level-major memory layouts, SIMD-vectorized partial distance computations, and cache-aware access patterns. Experiments across diverse datasets -- from image-based CIFAR-10 and GIST to modern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate that PANORAMA affords a 2--30$\times$ end-to-end speedup with no recall loss.

**Link**: [arxiv](http://arxiv.org/abs/2510.00566v3),  [pdf](http://arxiv.org/pdf/2510.00566v3)

**Tags**: cs.LG cs.AI cs.DB 



### A Stable Whitening Optimizer for Efficient Neural Network Training
**Authors**: Kevin Frans, Sergey Levine, Pieter Abbeel

**Updated**: 2025-10-23T18:52:25Z

**Summary**: In this work, we take an experimentally grounded look at neural network optimization. Building on the Shampoo family of algorithms, we identify and alleviate three key issues, resulting in the proposed SPlus method. First, we find that naive Shampoo is prone to divergence when matrix-inverses are cached for long periods. We introduce an alternate bounded update combining a historical eigenbasis with instantaneous normalization, resulting in across-the-board stability and significantly lower computational requirements. Second, we adapt a shape-aware scaling to enable learning rate transfer across network width. Third, we find that high learning rates result in large parameter noise, and propose a simple iterate-averaging scheme which unblocks faster learning. To properly confirm these findings, we introduce a pointed Transformer training benchmark, considering three objectives (language modelling, image classification, and diffusion modelling) across different stages of training. On average, SPlus is able to reach the validation performance of Adam within 44-58% of the gradient steps and 62-83% of the wallclock time.

**Link**: [arxiv](http://arxiv.org/abs/2506.07254v3),  [pdf](http://arxiv.org/pdf/2506.07254v3)

**Tags**: cs.LG 



### Mixing Importance with Diversity: Joint Optimization for KV Cache   Compression in Large Vision-Language Models
**Authors**: Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang

**Updated**: 2025-10-23T16:17:47Z

**Summary**: Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose \texttt{MixKV}, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. \texttt{MixKV} adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that \texttt{MixKV} consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), \texttt{MixKV} improves baseline methods by an average of \textbf{5.1\%} across five multi-modal understanding benchmarks and achieves remarkable gains of \textbf{8.0\%} and \textbf{9.0\%} for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, \texttt{MixKV} extends seamlessly to LLMs with comparable performance gains. Our code is available at \href{https://github.com/xuyang-liu16/MixKV}{\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.

**Link**: [arxiv](http://arxiv.org/abs/2510.20707v1),  [pdf](http://arxiv.org/pdf/2510.20707v1)

**Tags**: cs.CV 



### WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables
**Authors**: Hunter McCoy, Prashant Pandey

**Updated**: 2025-10-23T15:26:38Z

**Summary**: GPU hash tables are increasingly used to accelerate data processing, but their limited functionality restricts adoption in large-scale data processing applications. Current limitations include incomplete concurrency support and missing compound operations such as upserts.   This paper presents WarpSpeed, a library of high-performance concurrent GPU hash tables with a unified benchmarking framework for performance analysis. WarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and provides a rich API designed for modern GPU applications. Our evaluation uses diverse benchmarks to assess both correctness and scalability, and we demonstrate real-world impact by integrating these hash tables into three downstream applications.   We propose several optimization techniques to reduce concurrency overhead, including fingerprint-based metadata to minimize cache line probes and specialized Nvidia GPU instructions for lock-free queries. Our findings provide new insights into concurrent GPU hash table design and offer practical guidance for developing efficient, scalable data structures on modern GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2509.16407v2),  [pdf](http://arxiv.org/pdf/2509.16407v2)

**Tags**: cs.DC cs.DS 



### Neural Attention Search
**Authors**: Difan Deng, Marius Lindauer

**Updated**: 2025-10-23T14:23:24Z

**Summary**: We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.

**Link**: [arxiv](http://arxiv.org/abs/2502.13251v4),  [pdf](http://arxiv.org/pdf/2502.13251v4)

**Tags**: cs.CL cs.AI 



### HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data   Placement
**Authors**: Danying Ge, Jianhua Gao, Yixue Yang, Weixing Ji

**Updated**: 2025-10-23T12:28:58Z

**Summary**: Retrieval-Augmented Generation (RAG) improves model output accuracy by leveraging external knowledge bases, serving as an effective solution to address hallucination issues and knowledge-update delays in Large Language Models (LLMs). However, the introduction of external knowledge bases presents RAG with challenges in long-context processing, significantly increasing memory consumption and inference latency. Existing research accelerates inference by precomputing Key and Value (KV) of the knowledge base and loading them on-demand during inference. Based on the access frequency of different KV chunks within the external knowledge base, this paper proposes a hotness-aware RAG (HA-RAG) inference optimization system. First, leveraging the numerical distribution of KV chunks, we introduce a hotness-aware mixed-precision compressing and loading method to reduce disk I/O and memory access overhead. Second, we design a hotness-aware data placement strategy that prioritizes storing frequently accessed KV chunks in high-speed memory to improve data access efficiency. Experimental results demonstrate that, compared with TurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum speedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.

**Link**: [arxiv](http://arxiv.org/abs/2510.20878v1),  [pdf](http://arxiv.org/pdf/2510.20878v1)

**Tags**: cs.LG cs.AI C.4; E.4; I.2 



### Prefetching Cache Optimization Using Graph Neural Networks: A Modular   Framework and Conceptual Analysis
**Authors**: F. I. Qowy

**Updated**: 2025-10-23T10:35:35Z

**Summary**: Caching and prefetching techniques are fundamental to modern computing, serving to bridge the growing performance gap between processors and memory. Traditional prefetching strategies are often limited by their reliance on predefined heuristics or simplified statistical models, which fail to capture the complex, non-linear dependencies in modern data access patterns. This paper introduces a modular framework leveraging Graph Neural Networks (GNNs) to model and predict access patterns within graph-structured data, focusing on web navigation and hierarchical file systems. The toolchain consists of: a route mapper for extracting structural information, a graph constructor for creating graph representations, a walk session generator for simulating user behaviors, and a gnn prefetch module for training and inference. We provide a detailed conceptual analysis showing how GNN-based approaches can outperform conventional methods by learning intricate dependencies. This work offers both theoretical foundations and a practical, replicable pipeline for future research in graph-driven systems optimization.

**Link**: [arxiv](http://arxiv.org/abs/2510.21865v1),  [pdf](http://arxiv.org/pdf/2510.21865v1)

**Tags**: cs.PF cs.LG cs.SE 



### Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism   on Dependency-Bound Kernels
**Authors**: Rubén Langarita, Jesús Alastruey-Benedé, Pablo Ibáñez-Marín, Santiago Marco-Sola, Miquel Moretó, Adrià Armejach

**Updated**: 2025-10-23T10:06:48Z

**Summary**: Multiple HPC applications are often bottlenecked by compute-intensive kernels implementing complex dependency patterns (data-dependency bound). Traditional general-purpose accelerators struggle to effectively exploit fine-grain parallelism due to limitations in implementing convoluted data-dependency patterns (like SIMD) and overheads due to synchronization and data transfers (like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved performance and energy efficiency at a high cost in hardware design and programming complexity and often lack the flexibility to process different workloads. We propose Squire, a general-purpose accelerator designed to exploit fine-grain parallelism effectively on dependency-bound kernels. Each Squire accelerator has a set of general-purpose low-power in-order cores that can rapidly communicate among themselves and directly access data from the L2 cache. Our proposal integrates one Squire accelerator per core in a typical multicore system, allowing the acceleration of dependency-bound kernels within parallel tasks with minimal software changes. As a case study, we evaluate Squire's effectiveness by accelerating five kernels that implement complex dependency patterns. We use three of these kernels to build an end-to-end read-mapping tool that will be used to evaluate Squire. Squire obtains speedups up to 7.64$\times$ in dynamic programming kernels. Overall, Squire provides an acceleration for an end-to-end application of 3.66$\times$. In addition, Squire reduces energy consumption by up to 56% with a minimal area overhead of 10.5% compared to a Neoverse-N1 baseline.

**Link**: [arxiv](http://arxiv.org/abs/2510.20400v1),  [pdf](http://arxiv.org/pdf/2510.20400v1)

**Tags**: cs.AR 



### Bi-Mamba: Towards Accurate 1-Bit State Space Models
**Authors**: Shengkun Tang, Liqun Ma, Haonan Li, Mingjie Sun, Zhiqiang Shen

**Updated**: 2025-10-23T09:55:50Z

**Summary**: The typical Selective State-Space Model (SSM) used in Mamba addresses several limitations of Transformers, such as the quadratic computational complexity with respect to sequence length and the significant memory requirements during inference due to the key-value (KV) cache. However, the increasing size of Mamba models continues to pose challenges for training and deployment, particularly due to their substantial computational demands during both training and inference. In this work, we introduce $\texttt{Bi-Mamba}$, a scalable and powerful 1-bit Mamba architecture designed to enable more efficient large language models (LLMs), with model sizes of 780M, 1.3B, and 2.7B parameters. $\texttt{Bi-Mamba}$ models are trained from scratch on a standard LLM-scale dataset using an autoregressive distillation loss. Extensive experiments on language modeling benchmarks demonstrate that $\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16 or BF16) counterparts, while outperforming post-training binarization (PTB) Mamba and binarization-aware training (BAT) Transformer baselines. Moreover, $\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost compared to the original Mamba. Our work pioneers a new line of linear-complexity LLMs under low-bit representation and provides the way for the design of specialized hardware optimized for efficient 1-bit Mamba-based models. Code and the pre-trained weights are available at https://github.com/Tangshengku/Bi-Mamba.

**Link**: [arxiv](http://arxiv.org/abs/2411.11843v2),  [pdf](http://arxiv.org/pdf/2411.11843v2)

**Tags**: cs.CL cs.AI 



### Improving Model Representation and Reducing KV Cache via Skip   Connections with First Value Heads
**Authors**: Zhoutong Wu, Yuan Zhang, Yiming Dong, Chenheng Zhang, Cong Fang, Kun Yuan, Zhouchen Lin

**Updated**: 2025-10-23T08:29:11Z

**Summary**: Transformer models have driven breakthroughs across various language tasks by their strong capability to learn rich contextual representations. Scaling them to improve representation, however, often demands substantial memory and compute costs, such as the Key-Value (KV) cache used during auto-regressive decoding. Skip connections offer a promising way to improve representation without bloating resource usage, yet most prior works either improve expressivity while leaving KV costs unchanged, or reduce memory at the cost of weaker representation. In this work, we propose SkipV1Former, a Transformer variant that uses skip connections from the first layer's Value heads to strengthen model representation and reduce KV cache. Specifically, from the second block onward, each layer reuses half of its Value heads from the very first layer, while computing the other half as usual-cutting Value projections and V cache by nearly 50 \%. Theoretically, we show that routing uncompressed first-layer Values into deeper layers restores information lost to compression and accelerates the model's implicit mesa-optimization-a key pattern of Transformer in auto-regressive tasks. Empirically, across different model scales, SkipV1Former delivers consistent reductions of approximately 25 \% in KV cache while improving perplexity relative to standard Multi-Head Attention (MHA) Transformers and some advanced variants. Moreover, we propose a recipe for uptraining existing MHA Transformer checkpoints to SkipV1Former with only 10-15\% additional compute. Finally, SkipV1Former can seamlessly combine advanced methods like Group-Query Attention and Multi-Latent Attention to achieve further KV cache savings and performance improvement. When combined with YOCO, it cuts KV cache size by nearly 50 \% while still improving performance.

**Link**: [arxiv](http://arxiv.org/abs/2510.16807v2),  [pdf](http://arxiv.org/pdf/2510.16807v2)

**Tags**: cs.LG cs.AI 



### Soft Phonon Charge-Density Wave Formation in the Kagome Metal   KV$_3$Sb$_5$
**Authors**: Yifan Wang, Chenchao Xu, Zhimian Wu, Huachen Rao, Zhaoyang Shan, Yi Liu, Guanghan Cao, Michael Smidman, Ming Shi, Huiqiu Yuan, Tao Wu, Xianhui Chen, Chao Cao, Yu Song

**Updated**: 2025-10-23T05:22:09Z

**Summary**: A range of of unusual emergent behaviors have been reported in the charge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome metals, including a CDW formation process without soft phonons, which points to an unconventional CDW mechanism. Here, we use inelastic x-ray scattering to show that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy at the CDW ordering vector ($L$-point) around $T_{\rm CDW}=78$~K. These soft phonons exhibit a remarkable in-plane anisotropy, extending over a much larger momentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse scattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we find that the momentum-dependent electron-phonon coupling (EPC) is peaked at $L$ and exhibits the same in-plane anisotropy as the phonon softening. Conversely, the electronic susceptibility is not peaked at $L$ and shows the opposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the driving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process similar to that of transition metal dichalcogenides.

**Link**: [arxiv](http://arxiv.org/abs/2510.20230v1),  [pdf](http://arxiv.org/pdf/2510.20230v1)

**Tags**: cond-mat.supr-con cond-mat.str-el 



## Keyword: LLM Inference 
 ### Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via   Self-Play and Reinforcement Learning
**Authors**: Richard Dewey, Janos Botyanszki, Ciamac C. Moallemi, Andrew T. Zheng

**Updated**: 2025-11-05T18:58:18Z

**Summary**: AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.

**Link**: [arxiv](http://arxiv.org/abs/2511.03724v1),  [pdf](http://arxiv.org/pdf/2511.03724v1)

**Tags**: cs.AI cs.MA 



### Observation of phase memory and dynamical phase transitions in spinor   gases
**Authors**: J. O. Austin-Harris, P. Sigdel, C. Binegar, S. E. Begg, T. Bilitewski, Y. Liu

**Updated**: 2025-11-05T18:53:57Z

**Summary**: Utilizing ultracold spinor gases as large-scale, many-body quantum simulation platforms, we establish a toolbox for the precise control, characterization, and detection of nonequilibrium dynamics via internal spinor phases. We develop a method to extract the phase evolution from the observed spin population dynamics, allowing us to define an order parameter that sharply identifies dynamical phase transitions over a wide range of conditions. This work also demonstrates a technique for inferring spin-dependent interactions from a single experimental time trace, in contrast to the standard approach that requires mapping a cross section of the phase diagram, with immediate applications to systems experiencing complex time-dependent interactions. Additionally, we demonstrate experimental access to and control over non-ergodic relaxation dynamics, where states in the (nominally) thermal region of the energy spectrum retain memory of the initial state, via the manipulation of spinor phases, enabling the study of non-ergodic thermalization dynamics connected to quantum scarring.

**Link**: [arxiv](http://arxiv.org/abs/2511.03720v1),  [pdf](http://arxiv.org/pdf/2511.03720v1)

**Tags**: cond-mat.quant-gas 



### Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist   Annotation Scheme for MapTask
**Authors**: Nan Li, Albert Gatt, Massimo Poesio

**Updated**: 2025-11-05T18:52:28Z

**Summary**: Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.

**Link**: [arxiv](http://arxiv.org/abs/2511.03718v1),  [pdf](http://arxiv.org/pdf/2511.03718v1)

**Tags**: cs.CL cs.AI 



### GDS Agent for Graph Algorithmic Reasoning
**Authors**: Borun Shi, Ioannis Panagiotas

**Updated**: 2025-11-05T18:39:38Z

**Summary**: Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.

**Link**: [arxiv](http://arxiv.org/abs/2508.20637v2),  [pdf](http://arxiv.org/pdf/2508.20637v2)

**Tags**: cs.LG cs.AI cs.CL 



### LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol
**Authors**: Yu-Erh Pan, Ayesha Siddika Nipu

**Updated**: 2025-11-05T18:38:02Z

**Summary**: Air quality monitoring is central to environmental sustainability and public health, yet traditional systems remain difficult for non-expert users to interpret due to complex visualizations, limited interactivity, and high deployment costs. Recent advances in Large Language Models (LLMs) offer new opportunities to make sensor data more accessible, but their tendency to produce hallucinations limits reliability in safety-critical domains. To address these challenges, we present an LLM-enhanced Air Monitoring Interface (AMI) that integrates real-time sensor data with a conversational interface via the Model Context Protocol (MCP). Our system grounds LLM outputs in live environmental data, enabling accurate, context-aware responses while reducing hallucination risk. The architecture combines a Django-based backend, a responsive user dashboard, and a secure MCP server that exposes system functions as discoverable tools, allowing the LLM to act as an active operator rather than a passive responder. Expert evaluation demonstrated high factual accuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a scale of 5, supported by inter-rater reliability analysis. These results highlight the potential of combining LLMs with standardized tool protocols to create reliable, secure, and user-friendly interfaces for real-time environmental monitoring.

**Link**: [arxiv](http://arxiv.org/abs/2511.03706v1),  [pdf](http://arxiv.org/pdf/2511.03706v1)

**Tags**: cs.ET 



### Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset   in Large Language Models
**Authors**: Francesco Corso, Francesco Pierri, Gianmarco De Francisci Morales

**Updated**: 2025-11-05T18:28:28Z

**Summary**: In this paper, we investigate whether Large Language Models (LLMs) exhibit conspiratorial tendencies, whether they display sociodemographic biases in this domain, and how easily they can be conditioned into adopting conspiratorial perspectives. Conspiracy beliefs play a central role in the spread of misinformation and in shaping distrust toward institutions, making them a critical testbed for evaluating the social fidelity of LLMs. LLMs are increasingly used as proxies for studying human behavior, yet little is known about whether they reproduce higher-order psychological constructs such as a conspiratorial mindset. To bridge this research gap, we administer validated psychometric surveys measuring conspiracy mindset to multiple models under different prompting and conditioning strategies. Our findings reveal that LLMs show partial agreement with elements of conspiracy belief, and conditioning with socio-demographic attributes produces uneven effects, exposing latent demographic biases. Moreover, targeted prompts can easily shift model responses toward conspiratorial directions, underscoring both the susceptibility of LLMs to manipulation and the potential risks of their deployment in sensitive contexts. These results highlight the importance of critically evaluating the psychological dimensions embedded in LLMs, both to advance computational social science and to inform possible mitigation strategies against harmful uses.

**Link**: [arxiv](http://arxiv.org/abs/2511.03699v1),  [pdf](http://arxiv.org/pdf/2511.03699v1)

**Tags**: cs.CL cs.CY 



### AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and   Sample-Efficient Analog Circuit Sizing
**Authors**: Mohsen Ahmadzadeh, Kaichang Chen, Georges Gielen

**Updated**: 2025-11-05T18:24:01Z

**Summary**: Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit's design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.

**Link**: [arxiv](http://arxiv.org/abs/2511.03697v1),  [pdf](http://arxiv.org/pdf/2511.03697v1)

**Tags**: cs.LG cs.AI cs.AR 



### Voost: A Unified and Scalable Diffusion Transformer for Bidirectional   Virtual Try-On and Try-Off
**Authors**: Seungyong Lee, Jeong-gi Kwak

**Updated**: 2025-11-05T18:23:44Z

**Summary**: Virtual try-on aims to synthesize a realistic image of a person wearing a target garment, but accurately modeling garment-body correspondence remains a persistent challenge, especially under pose and appearance variation. In this paper, we propose Voost - a unified and scalable framework that jointly learns virtual try-on and try-off with a single diffusion transformer. By modeling both tasks jointly, Voost enables each garment-person pair to supervise both directions and supports flexible conditioning over generation direction and garment category, enhancing garment-body relational reasoning without task-specific networks, auxiliary losses, or additional labels. In addition, we introduce two inference-time techniques: attention temperature scaling for robustness to resolution or mask variation, and self-corrective sampling that leverages bidirectional consistency between tasks. Extensive experiments demonstrate that Voost achieves state-of-the-art results on both try-on and try-off benchmarks, consistently outperforming strong baselines in alignment accuracy, visual fidelity, and generalization.

**Link**: [arxiv](http://arxiv.org/abs/2508.04825v2),  [pdf](http://arxiv.org/pdf/2508.04825v2)

**Tags**: cs.GR cs.AI cs.CV cs.LG 



### Improving Gene Trees without more data
**Authors**: Ashu Gupta

**Updated**: 2025-11-05T18:18:06Z

**Summary**: Estimating species and gene trees from sequence data is challenging. Gene tree estimation is often hampered by low phylogenetic signal in alignments, leading to inaccurate trees. Species tree estimation is complicated by incomplete lineage sorting (ILS), where gene histories differ from the species' history. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees from gene trees but suffer when gene tree accuracy is low. To address this, the Statistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were developed to improve gene tree estimation. However, previous studies only tested these pipelines using multi-locus bootstrapping (MLBS), not the BestML approach.   This thesis proposes a novel pipeline, WSB+WQMC, which shares design features with the existing WSB+CAML pipeline but has other desirable properties and is statistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC against WSB+CAML using BestML analysis on various simulated datasets. The results confirmed many trends seen in prior MLBS analyses. WSB+WQMC substantially improved gene tree and species tree accuracy (using ASTRAL2 and ASTRID) on most datasets with low, medium, and moderately high ILS levels. In a direct comparison, WSB+WQMC computed less accurate trees than WSB+CAML under certain low and medium ILS conditions. However, WSB+WQMC performed better or at least as accurately as WSB+CAML on all datasets with moderately high and high ILS. It also proved better for estimating gene trees on some medium and low ILS datasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for phylogenetic estimation, especially in the presence of low phylogenetic signal.

**Link**: [arxiv](http://arxiv.org/abs/2511.03692v1),  [pdf](http://arxiv.org/pdf/2511.03692v1)

**Tags**: q-bio.PE cs.CE 



### The OpenHands Software Agent SDK: A Composable and Extensible Foundation   for Production Agents
**Authors**: Xingyao Wang, Simon Rosenberg, Juan Michelini, Calvin Smith, Hoang Tran, Engel Nyst, Rohit Malhotra, Xuhui Zhou, Valerie Chen, Robert Brennan, Graham Neubig

**Updated**: 2025-11-05T18:16:44Z

**Summary**: Agents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. In this paper, we present the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, we design a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation for prototyping, unlocking new classes of custom applications, and reliably deploying agents at scale.

**Link**: [arxiv](http://arxiv.org/abs/2511.03690v1),  [pdf](http://arxiv.org/pdf/2511.03690v1)

**Tags**: cs.SE cs.AI 



### FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving   on Heterogeneous GPUs
**Authors**: Xuan He, Zequan Fang, Jinzhao Lian, Danny H. K. Tsang, Baosen Zhang, Yize Chen

**Updated**: 2025-11-05T18:15:57Z

**Summary**: The ever-increasing computation and energy demand for LLM and AI agents call for holistic and efficient optimization of LLM serving systems. In practice, heterogeneous GPU clusters can be deployed in a geographically distributed manner, while LLM load also observes diversity in terms of both query traffic and serving patterns. LLM queries running on advanced GPUs during a high-emission hour at one location can lead to significantly higher carbon footprints versus same queries running on mid-level GPUs at a low-emission time and location. By observing LLM serving requirements and leveraging spatiotemporal computation flexibility, we consider the joint routing and scheduling problem, and propose FREESH to cooperatively run a group of data centers while minimizing user-specified carbon or energy objectives. FREESH identifies the optimal configurations of balanced load serving by matching distinct GPU instance's power-throughput characteristics with predictable LLM query length and workloads. To ensure both latency and fairness requirements, FREESH identifies optimized parallelism and query routing schedules together with dynamic GPU frequency scaling for power saving, and Least-Laxity-First (LLF) serving strategy for query scheduling. During the 1-hour serving on production workloads, FREESH reduces energy by 28.6% and emissions by 45.45% together with improvements in SLO attainment and fairness.

**Link**: [arxiv](http://arxiv.org/abs/2511.00807v2),  [pdf](http://arxiv.org/pdf/2511.00807v2)

**Tags**: cs.DC 



### LLM Query Scheduling with Prefix Reuse and Latency Constraints
**Authors**: Gregory Dexter, Shao Tang, Ata Fatahi Baarzi, Qingquan Song, Tejas Dharamsi, Aman Gupta

**Updated**: 2025-11-05T18:12:33Z

**Summary**: The efficient deployment of large language models (LLMs) in online settings requires optimizing inference performance under stringent latency constraints, particularly the time-to-first-token (TTFT) and time-per-output-token (TPOT). This paper focuses on the query scheduling problem for LLM inference with prefix reuse, a technique that leverages shared prefixes across queries to reduce computational overhead. Our work reveals previously unknown limitations of the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM) scheduling strategies with respect to satisfying latency constraints. We present a formal theoretical framework for LLM query scheduling under RadixAttention, a prefix reuse mechanism that stores and reuses intermediate representations in a radix tree structure. Our analysis establishes the NP-hardness of the scheduling problem with prefix reuse under TTFT constraints and proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing methods by balancing prefix reuse and fairness in query processing. Theoretical guarantees demonstrate that $k$-LPM achieves improved TTFT performance under realistic traffic patterns captured by a data generative model. Empirical evaluations in a realistic serving setting validates our findings, showing significant reductions in P99 TTFT compared to baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2502.04677v2),  [pdf](http://arxiv.org/pdf/2502.04677v2)

**Tags**: cs.DS 



### PDE-SHARP: PDE Solver Hybrids through Analysis and Refinement Passes
**Authors**: Shaghayegh Fazliani, Madeleine Udell

**Updated**: 2025-11-05T17:58:32Z

**Summary**: Current LLM-driven approaches using test-time computing to generate PDE solvers execute a large number of solver samples to identify high-accuracy solvers. These paradigms are especially costly for complex PDEs requiring substantial computational resources for numerical evaluation. We introduce PDE-SHARP, a framework to reduce computational costs by replacing expensive scientific computation by cheaper LLM inference that achieves superior solver accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three stages: (1) Analysis: mathematical chain-of-thought analysis including PDE classification, solution type detection, and stability analysis; (2) Genesis: solver generation based on mathematical insights from the previous stage; and (3) Synthesis: collaborative selection-hybridization tournaments in which LLM judges iteratively refine implementations through flexible performance feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13 solver evaluations on average compared to 30+ for baseline methods, improving accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates robust performance across LLM architectures, from general-purpose to specialized reasoning models.

**Link**: [arxiv](http://arxiv.org/abs/2511.00183v2),  [pdf](http://arxiv.org/pdf/2511.00183v2)

**Tags**: cs.LG 



### Using latent representations to link disjoint longitudinal data for   mixed-effects regression
**Authors**: Clemens Schächter, Maren Hackenberg, Michelle Pfaffenlehner, Félix B. Tambe-Ndonfack, Thorsten Schmidt, Astrid Pechmann, Janbernd Kirschner, Jan Hasenauer, Harald Binder

**Updated**: 2025-11-05T17:49:58Z

**Summary**: Many rare diseases offer limited established treatment options, leading patients to switch therapies when new medications emerge. To analyze the impact of such treatment switches within the low sample size limitations of rare disease trials, it is important to use all available data sources. This, however, is complicated when usage of measurement instruments change during the observation period, for example when instruments are adapted to specific age ranges. The resulting disjoint longitudinal data trajectories, complicate the application of traditional modeling approaches like mixed-effects regression. We tackle this by mapping observations of each instrument to a aligned low-dimensional temporal trajectory, enabling longitudinal modeling across instruments. Specifically, we employ a set of variational autoencoder architectures to embed item values into a shared latent space for each time point. Temporal disease dynamics and treatment switch effects are then captured through a mixed-effects regression model applied to latent representations. To enable statistical inference, we present a novel statistical testing approach that accounts for the joint parameter estimation of mixed-effects regression and variational autoencoders. The methodology is applied to quantify the impact of treatment switches for patients with spinal muscular atrophy. Here, our approach aligns motor performance items from different measurement instruments for mixed-effects regression and maps estimated effects back to the observed item level to quantify the treatment switch effect. Our approach allows for model selection as well as for assessing effects of treatment switching. The results highlight the potential of modeling in joint latent representations for addressing small data challenges.

**Link**: [arxiv](http://arxiv.org/abs/2510.25531v2),  [pdf](http://arxiv.org/pdf/2510.25531v2)

**Tags**: stat.ML cs.AI cs.LG 68T07 G.3; I.2.6; J.3 



### Whisper Leak: a side-channel attack on Large Language Models
**Authors**: Geoff McDonald, Jonathan Bar Or

**Updated**: 2025-11-05T17:47:46Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in sensitive domains including healthcare, legal services, and confidential communications, where privacy is paramount. This paper introduces Whisper Leak, a side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming responses. Despite TLS encryption protecting content, these metadata patterns leak sufficient information to enable topic classification. We demonstrate the attack across 28 popular LLMs from major providers, achieving near-perfect classification (often >98% AUPRC) and high precision even at extreme class imbalance (10,000:1 noise-to-target ratio). For many models, we achieve 100% precision in identifying sensitive topics like "money laundering" while recovering 5-20% of target conversations. This industry-wide vulnerability poses significant risks for users under network surveillance by ISPs, governments, or local adversaries. We evaluate three mitigation strategies - random padding, token batching, and packet injection - finding that while each reduces attack effectiveness, none provides complete protection. Through responsible disclosure, we have collaborated with providers to implement initial countermeasures. Our findings underscore the need for LLM providers to address metadata leakage as AI systems handle increasingly sensitive information.

**Link**: [arxiv](http://arxiv.org/abs/2511.03675v1),  [pdf](http://arxiv.org/pdf/2511.03675v1)

**Tags**: cs.CR cs.AI K.4.1; C.2.0; K.6.5; I.2.7 



### Do Automatic Factuality Metrics Measure Factuality? A Critical   Evaluation
**Authors**: Sanjana Ramprasad, Byron C. Wallace

**Updated**: 2025-11-05T17:42:36Z

**Summary**: Modern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. Measuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. This in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. But are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? In this work, we stress test a range of automatic factuality metrics, including specialized models and LLM-based prompting methods, to probe what they actually capture. Using a shallow classifier to separate ``easy'' examples for factual evaluation where surface features suffice from ``hard'' cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter. Furthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed, i.e., their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the prompt based ChatGPT-DA approach is the most robust and reliable. However, this comes with a notable caveat: Prompting LLMs to assess factuality may overly rely on their parametric knowledge rather than the provided reference when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring.

**Link**: [arxiv](http://arxiv.org/abs/2411.16638v4),  [pdf](http://arxiv.org/pdf/2411.16638v4)

**Tags**: cs.CL cs.AI 



### TabTune: A Unified Library for Inference and Fine-Tuning Tabular   Foundation Models
**Authors**: Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu

**Updated**: 2025-11-05T17:36:30Z

**Summary**: Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.

**Link**: [arxiv](http://arxiv.org/abs/2511.02802v2),  [pdf](http://arxiv.org/pdf/2511.02802v2)

**Tags**: cs.LG cs.AI 



### Addressing prior dependence in hierarchical Bayesian modeling for PTA   data analysis I: Methodology and implementation
**Authors**: Luigi D'amico, Eleonora Villa, Fatima Modica Bittordo, Aldo Barca, Francesco Alì, Massimo Meneghetti, Luca Naso

**Updated**: 2025-11-05T17:33:44Z

**Summary**: Complex inference tasks, such as those encountered in Pulsar Timing Array (PTA) data analysis, rely on Bayesian frameworks. The high-dimensional parameter space and the strong interdependencies among astrophysical, pulsar noise, and nuisance parameters introduce significant challenges for efficient learning and robust inference. These challenges are emblematic of broader issues in decision science, where model over-parameterization and prior sensitivity can compromise both computational tractability and the reliability of the results. We address these issues in the framework of hierarchical Bayesian modeling by introducing a reparameterization strategy. Our approach employs Normalizing Flows (NFs) to decorrelate the parameters governing hierarchical priors from those of astrophysical interest. The use of NF-based mappings provides both the flexibility to realize the reparametrization and the tractability to preserve proper probability densities. We further adopt i-nessai, a flow-guided nested sampler, to accelerate exploration of complex posteriors. This unified use of NFs improves statistical robustness and computational efficiency, providing a principled methodology for addressing hierarchical Bayesian inference in PTA analysis.

**Link**: [arxiv](http://arxiv.org/abs/2511.03667v1),  [pdf](http://arxiv.org/pdf/2511.03667v1)

**Tags**: astro-ph.IM astro-ph.CO astro-ph.HE stat.ML 



### Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs
**Authors**: Changhao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, Bo Dai

**Updated**: 2025-11-05T17:33:06Z

**Summary**: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.

**Link**: [arxiv](http://arxiv.org/abs/2410.20749v3),  [pdf](http://arxiv.org/pdf/2410.20749v3)

**Tags**: cs.LG cs.AI cs.CL 



### Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction   Detection
**Authors**: Dongkeun Kim, Minsu Cho, Suha Kwak

**Updated**: 2025-11-05T17:33:03Z

**Summary**: Social interactions often emerge from subtle, fine-grained cues such as facial expressions, gaze, and gestures. However, existing methods for social interaction detection overlook such nuanced cues and primarily rely on holistic representations of individuals. Moreover, they directly detect social groups without explicitly modeling the underlying interactions between individuals. These drawbacks limit their ability to capture localized social signals and introduce ambiguity when group configurations should be inferred from social interactions grounded in nuanced cues. In this work, we propose a part-aware bottom-up group reasoning framework for fine-grained social interaction detection. The proposed method infers social groups and their interactions using body part features and their interpersonal relations. Our model first detects individuals and enhances their features using part-aware cues, and then infers group configuration by associating individuals via similarity-based reasoning, which considers not only spatial relations but also subtle social cues that signal interactions, leading to more accurate group inference. Experiments on the NVI dataset demonstrate that our method outperforms prior methods, achieving the new state of the art.

**Link**: [arxiv](http://arxiv.org/abs/2511.03666v1),  [pdf](http://arxiv.org/pdf/2511.03666v1)

**Tags**: cs.CV 



### 3D Full Spectrum Fitting: Algorithm Comparison
**Authors**: Prashin Jethwa, Simon Hubmer, Ronny Ramlau, Glenn Van de Ven

**Updated**: 2025-11-05T17:26:05Z

**Summary**: Full spectrum fitting is the prevailing method for extracting stellar kinematic and population measurements from 1D galaxy spectra. 3D methods refer to analysis of Integral Field Spectroscopy (IFS) data where spatial and spectral dimensions are modelled simultaneously. While several 3D methods exist for modelling gas structures there has been less investigation into the more computationally demanding problem of 3D full spectrum fitting for stellar recoveries. This work introduces and compares two algorithms for this task: the Projected Nesterov Kaczmarz Reconstruction method (PNKR) and a version of the Bayes-LOSVD software which has been modified to account for spatial correlations. We aim to understand strengths and weaknesses of both algorithms and assess the impact of 3D methods for stellar inferences. We apply both recovery algorithms to a mock IFS data over a signal-to-noise ratio (SNR) range from 20-200 and evaluate the quality of the recoveries compared to the known ground truth. Accounting for spatial correlations in Bayes-LOSVD significantly improved the accuracy and precision of kinematic recoveries. 3D modelling with PNKR did not provide any significant improvement over 1D fits however, for SNR>40, PNKR did recover the most accurate kinematics overall. Additionally, by modelling the joint distribution over kinematics and populations, PNKR could successfully infer trends between these quantities e.g. inferring local metallicity-velocity trends, albeit with a significant bias on the absolute metallicity. Having demonstrated advantages of (i) 3D modelling with Bayes-LOSVD, and (ii) joint kinematic-population analyses with PNKR, we conclude that both methodological advances will prove useful for detecting and characterising stellar structures from IFS data.

**Link**: [arxiv](http://arxiv.org/abs/2511.03663v1),  [pdf](http://arxiv.org/pdf/2511.03663v1)

**Tags**: astro-ph.GA astro-ph.IM 



### RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based   Autonomous Navigation
**Authors**: Matteo El-Hariry, Antoine Richard, Ricard M. Castan, Luis F. W. Batista, Matthieu Geist, Cedric Pradalier, Miguel Olivares-Mendez

**Updated**: 2025-11-05T17:12:59Z

**Summary**: Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present a multi-domain framework for training, evaluating and deploying RL-based navigation policies across diverse robotic platforms and operational environments. Our work presents four key contributions: (1) a scalable and modular framework, facilitating seamless robot-task interchangeability and reproducible training pipelines; (2) sim-to-real transfer demonstrated through real-world experiments with multiple robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle; (3) the release of the first open-source API for deploying Isaac Lab-trained policies to real robots, enabling lightweight inference and rapid field validation; and (4) uniform tasks and metrics for cross-medium evaluation, through a unified evaluation testbed to assess performance of navigation tasks in diverse operational conditions (aquatic, terrestrial and space). By ensuring consistency between simulation and real-world deployment, RoboRAN lowers the barrier to developing adaptable RL-based navigation strategies. Its modular design enables straightforward integration of new robots and tasks through predefined templates, fostering reproducibility and extension to diverse domains. To support the community, we release RoboRAN as open-source.

**Link**: [arxiv](http://arxiv.org/abs/2505.14526v2),  [pdf](http://arxiv.org/pdf/2505.14526v2)

**Tags**: cs.RO cs.AI 



### Post Persona Alignment for Multi-Session Dialogue Generation
**Authors**: Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto

**Updated**: 2025-11-05T17:05:02Z

**Summary**: Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speaker's persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.11857v2),  [pdf](http://arxiv.org/pdf/2506.11857v2)

**Tags**: cs.CL 



### Watermarking Large Language Models in Europe: Interpreting the AI Act in   Light of Technology
**Authors**: Thomas Souverain

**Updated**: 2025-11-05T17:00:39Z

**Summary**: To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ''sufficiently reliable, interoperable, effective and robust''. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2511.03641v1),  [pdf](http://arxiv.org/pdf/2511.03641v1)

**Tags**: cs.CR cs.AI cs.CL cs.CY 68T01, 68727, 68T30, 68T35, 68T37, 68T50 



### Quantifying Weighted Morphological Content of Large-Scale Structures via   Simulation-Based Inference
**Authors**: M. H. Jalali Kanafi, S. M. S. Movahed

**Updated**: 2025-11-05T16:54:17Z

**Summary**: In this work, we perform a simulation-based forecasting analysis to compare the constraining power of two higher-order summary statistics of the large-scale structure (LSS), the Minkowski Functionals (MFs) and the Conditional Moments of Derivative (CMD), with a particular focus on their sensitivity to nonlinear and anisotropic features in redshift-space. Our analysis relies on halo catalogs from the Big Sobol Sequence(BSQ) simulations at redshift $z=0.5$, employing a likelihood-free inference framework implemented via neural posterior estimation. At the fiducial cosmology of the Quijote simulations $(\Omega_{m}=0.3175,\,\sigma_{8}=0.834)$, and for the smoothing scale $R=15\,h^{-1}$Mpc, we find that the CMD yields tighter forecasts for $(\Omega_{m}},\,\sigma_{8})$ than the zeroth- to third-order MFs components, improving the constraint precision by ${\sim}(44\%,\,52\%)$, ${\sim}(30\%,\,45\%)$, ${\sim}(27\%,\,17\%)$, and ${\sim}(26\%,\,17\%)$, respectively. A joint configuration combining the MFs and CMD further enhances the precision by approximately ${\sim}27\%$ compared to the standard MFs alone, highlighting the complementary anisotropy-sensitive information captured by the CMD in contrast to the scalar morphological content encapsulated by the MFs. We further extend the forecasting analysis to a continuous range of cosmological parameter values and multiple smoothing scales. Our results show that, although the absolute forecast uncertainty for each component of summary statistics depends on the underlying parameter values and the adopted smoothing scale, the relative constraining power among the summary statistics remains nearly constant throughout.

**Link**: [arxiv](http://arxiv.org/abs/2511.03636v1),  [pdf](http://arxiv.org/pdf/2511.03636v1)

**Tags**: astro-ph.CO cs.LG physics.comp-ph 



### Towards Transparent Stance Detection: A Zero-Shot Approach Using   Implicit and Explicit Interpretability
**Authors**: Apoorva Upadhyaya, Wolfgang Nejdl, Marco Fisichella

**Updated**: 2025-11-05T16:54:10Z

**Summary**: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward unseen targets. Existing research using contrastive, meta-learning, or data augmentation suffers from generalizability issues or lack of coherence between text and target. Recent works leveraging large language models (LLMs) for ZSSD focus either on improving unseen target-specific knowledge or generating explanations for stance analysis. However, most of these works are limited by their over-reliance on explicit reasoning, provide coarse explanations that lack nuance, and do not explicitly model the reasoning process, making it difficult to interpret the model's predictions. To address these issues, in our study, we develop a novel interpretable ZSSD framework, IRIS. We provide an interpretable understanding of the attitude of the input towards the target implicitly based on sequences within the text (implicit rationales) and explicitly based on linguistic measures (explicit rationales). IRIS considers stance detection as an information retrieval ranking task, understanding the relevance of implicit rationales for different stances to guide the model towards correct predictions without requiring the ground-truth of rationales, thus providing inherent interpretability. In addition, explicit rationales based on communicative features help decode the emotional and cognitive dimensions of stance, offering an interpretable understanding of the author's attitude towards the given target. Extensive experiments on the benchmark datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10% training data prove the generalizability of our model, benefiting from the proposed architecture and interpretable design.

**Link**: [arxiv](http://arxiv.org/abs/2511.03635v1),  [pdf](http://arxiv.org/pdf/2511.03635v1)

**Tags**: cs.CL cs.LG 



### Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in   LLMs
**Authors**: Jakub Podolak, Rajeev Verma

**Updated**: 2025-11-05T16:53:09Z

**Summary**: We study the source of uncertainty in DeepSeek R1-32B by analyzing its self-reported verbal confidence on question answering (QA) tasks. In the default answer-then-confidence setting, the model is regularly over-confident, whereas semantic entropy - obtained by sampling many responses - remains reliable. We hypothesize that this is because of semantic entropy's larger test-time compute, which lets us explore the model's predictive distribution. We show that granting DeepSeek the budget to explore its distribution by forcing a long chain-of-thought before the final answer greatly improves its verbal score effectiveness, even on simple fact-retrieval questions that normally require no reasoning. Furthermore, a separate reader model that sees only the chain can reconstruct very similar confidences, indicating the verbal score might be merely a statistic of the alternatives surfaced during reasoning. Our analysis concludes that reliable uncertainty estimation requires explicit exploration of the generative space, and self-reported confidence is trustworthy only after such exploration.

**Link**: [arxiv](http://arxiv.org/abs/2505.23845v2),  [pdf](http://arxiv.org/pdf/2505.23845v2)

**Tags**: cs.CL 



### LiveTradeBench: Seeking Real-World Alpha with Large Language Models
**Authors**: Haofei Yu, Fenghai Li, Jiaxuan You

**Updated**: 2025-11-05T16:47:26Z

**Summary**: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.

**Link**: [arxiv](http://arxiv.org/abs/2511.03628v1),  [pdf](http://arxiv.org/pdf/2511.03628v1)

**Tags**: q-fin.TR cs.AI cs.CE cs.CL 



### SecRepoBench: Benchmarking Code Agents for Secure Code Completion in   Real-World Repositories
**Authors**: Chihao Shen, Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, Yizheng Chen

**Updated**: 2025-11-05T16:42:11Z

**Summary**: This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28 standalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.

**Link**: [arxiv](http://arxiv.org/abs/2504.21205v2),  [pdf](http://arxiv.org/pdf/2504.21205v2)

**Tags**: cs.CR cs.AI 



### R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large   Model Token Routing
**Authors**: Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang

**Updated**: 2025-11-05T16:39:11Z

**Summary**: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.

**Link**: [arxiv](http://arxiv.org/abs/2505.21600v2),  [pdf](http://arxiv.org/pdf/2505.21600v2)

**Tags**: cs.CL cs.AI cs.LG cs.PF I.2.7 



### Uncovering flow and deformation regimes in the coupled fluid-solid   vestibular system
**Authors**: Javier Chico-Vázquez, Derek E. Moulton, Dominic Vella

**Updated**: 2025-11-05T16:36:55Z

**Summary**: In this paper, we showcase how flow obstruction by a deformable object can lead to symmetry breaking in curved domains subject to angular acceleration. Our analysis is motivated by the deflection of the cupula, a soft tissue located in the inner ear that is used to perceive rotational motion as part of the vestibular system. The cupula is understood to block the rotation-induced flow in a toroidal region with the flow-induced deformation of the cupula used by the brain to infer motion. By asymptotically solving the governing equations for this flow, we characterise regimes for which the sensory system is sensitive to either angular velocity or angular acceleration. Moreover, we show the fluid flow is not symmetric in the latter case. Finally, we extend our analysis of symmetry breaking to understand the formation of vortical flow in cavernous regions within channels. We discuss the implications of our results for the sensing of rotation by mammals.

**Link**: [arxiv](http://arxiv.org/abs/2504.06394v2),  [pdf](http://arxiv.org/pdf/2504.06394v2)

**Tags**: physics.flu-dyn physics.bio-ph 



### Going Beyond Expert Performance via Deep Implicit Imitation   Reinforcement Learning
**Authors**: Iason Chrysomallis, Georgios Chalkiadakis

**Updated**: 2025-11-05T16:33:39Z

**Summary**: Imitation learning traditionally requires complete state-action demonstrations from optimal or near-optimal experts. These requirements severely limit practical applicability, as many real-world scenarios provide only state observations without corresponding actions and expert performance is often suboptimal. In this paper we introduce a deep implicit imitation reinforcement learning framework that addresses both limitations by combining deep reinforcement learning with implicit imitation learning from observation-only datasets. Our main algorithm, Deep Implicit Imitation Q-Network (DIIQN), employs an action inference mechanism that reconstructs expert actions through online exploration and integrates a dynamic confidence mechanism that adaptively balances expert-guided and self-directed learning. This enables the agent to leverage expert guidance for accelerated training while maintaining capacity to surpass suboptimal expert performance. We further extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to tackle scenarios where expert and agent possess different action sets, a challenge previously unaddressed in the implicit imitation learning literature. HA-DIIQN introduces an infeasibility detection mechanism and a bridging procedure identifying alternative pathways connecting agent capabilities to expert guidance when direct action replication is impossible. Our experimental results demonstrate that DIIQN achieves up to 130% higher episodic returns compared to standard DQN, while consistently outperforming existing implicit imitation methods that cannot exceed expert performance. In heterogeneous action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging expert datasets unusable by conventional approaches. Extensive parameter sensitivity analysis reveals the framework's robustness across varying dataset sizes and hyperparameter configurations.

**Link**: [arxiv](http://arxiv.org/abs/2511.03616v1),  [pdf](http://arxiv.org/pdf/2511.03616v1)

**Tags**: cs.LG 



### A local eigenvector centrality
**Authors**: Ruaridh A. Clark, Francesca Arrigo, Agathe Bouis, Malcolm Macdonald

**Updated**: 2025-11-05T16:27:20Z

**Summary**: Eigenvector centrality is an established measure of global connectivity, from which the importance and influence of nodes can be inferred. We introduce a local eigenvector centrality that incorporates both local and global connectivity. This new measure references prominent eigengaps and combines their associated eigenspectrum, via the Euclidean norm, to detect centrality that reflects the influence of prominent community structures. In contact networks, with clearly defined community structures, local eigenvector centrality is shown to identify similar but distinct distributions to eigenvector centrality applied on each community in isolation and PageRank. Discrepancies between the two eigenvector measures highlight nodes and communities that do not conform to their defined local structures, e.g. nodes with more connections outside of their defined community than within it. While reference to PageRank's centrality assessment enables a mitigation strategy for localisation effects inherent in eigenvector-based measures. In networks without clearly defined communities, such as city road networks, local eigenvector centrality is shown to identify both locally prominent and globally connected hubs.

**Link**: [arxiv](http://arxiv.org/abs/2511.03608v1),  [pdf](http://arxiv.org/pdf/2511.03608v1)

**Tags**: cs.SI 



### Bayesian Topological Analysis of Functional Brain Networks
**Authors**: Xukun Zhu, Michael W Lutz, Tananun Songdechakraiwut

**Updated**: 2025-11-05T16:25:22Z

**Summary**: Subtle alterations in brain network topology often evade detection by traditional statistical methods. To address this limitation, we introduce a Bayesian inference framework for topological comparison of brain networks that probabilistically models within- and between-group dissimilarities. The framework employs Markov chain Monte Carlo sampling to estimate posterior distributions of test statistics and Bayes factors, enabling graded evidence assessment beyond binary significance testing. Simulations confirmed statistical consistency to permutation testing. Applied to fMRI data from the Duke-UNC Alzheimer's Disease Research Center, the framework detected topology-based network differences that conventional permutation tests failed to reveal, highlighting its enhanced sensitivity to early or subtle brain network alterations in clinical neuroimaging.

**Link**: [arxiv](http://arxiv.org/abs/2511.03605v1),  [pdf](http://arxiv.org/pdf/2511.03605v1)

**Tags**: stat.ME 



### The first year of LISA Galactic foreground
**Authors**: Riccardo Buscicchio, Federico Pozzoli, Daniele Chirico, Alberto Sesana

**Updated**: 2025-11-05T16:24:53Z

**Summary**: Galactic white-dwarf binaries play a central role in the inference model for the Laser Interferometer Space Antenna. In this manuscript, we employ the $\texttt{bahamas}$ codebase to characterize, in a global-fit fashion, the reconstruction of the Galactic foreground during the first year of observation. To account for its statistical properties, we represent the data in time--frequency domain, and characterize the effectiveness of multiple approaches, e.g. statistically viable likelihoods, sampling schemes, segmentation widths, and gaps density. Our analysis yields consistent results across, with overwhelming evidence in favor of a non-stationary model in less than a month of data. Moreover, we show robustness against the presence of additional extragalactic foregrounds, and test the suitability of our approximations on the more complex simulated data in the $\textit{Yorsh}$ data challenge.

**Link**: [arxiv](http://arxiv.org/abs/2511.03604v1),  [pdf](http://arxiv.org/pdf/2511.03604v1)

**Tags**: astro-ph.IM astro-ph.HE gr-qc 



### Step-Audio-EditX Technical Report
**Authors**: Chao Yan, Boyong Wu, Peng Yang, Pengfei Tan, Guoqiang Hu, Yuxin Zhang, Xiangyu, Zhang, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu

**Updated**: 2025-11-05T16:22:19Z

**Summary**: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

**Link**: [arxiv](http://arxiv.org/abs/2511.03601v1),  [pdf](http://arxiv.org/pdf/2511.03601v1)

**Tags**: cs.CL cs.AI cs.HC cs.SD eess.AS 



### Parameter estimation in interacting particle systems on dynamic random   networks
**Authors**: Simone Baldassarri, Jiesen Wang

**Updated**: 2025-11-05T16:19:04Z

**Summary**: In this paper we consider a class of interacting particle systems on dynamic random networks, in which the joint dynamics of vertices and edges acts as one-way feedback, i.e., edges appear and disappear over time depending on the state of the two connected vertices, while the vertex dynamics does not depend on the edge process. Our goal is to estimate the underlying dynamics from partial information of the process, specifically from snapshots of the total number of edges present. We showcase the effectiveness of our inference method through various numerical results.

**Link**: [arxiv](http://arxiv.org/abs/2507.06633v2),  [pdf](http://arxiv.org/pdf/2507.06633v2)

**Tags**: math.PR 



### SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of   AI and LLMs in SMEs
**Authors**: Iqbal H. Sarker, Helge Janicke, Ahmad Mohsin, Leandros Maglaras

**Updated**: 2025-11-05T16:07:58Z

**Summary**: Artificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing today's business practices; however, their adoption within small and medium-sized enterprises (SMEs) raises serious trust, ethical, and technical issues. In this perspective paper, we introduce a structured, multi-phased framework, "SME-TEAM" for the secure and responsible use of these technologies in SMEs. Based on a conceptual structure of four key pillars, i.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM bridges theoretical ethical principles with operational practice, enhancing AI capabilities across a wide range of applications in SMEs. Ultimately, this paper provides a structured roadmap for the adoption of these emerging technologies, positioning trust and ethics as a driving force for resilience, competitiveness, and sustainable innovation within the area of business analytics and SMEs.

**Link**: [arxiv](http://arxiv.org/abs/2509.10594v2),  [pdf](http://arxiv.org/pdf/2509.10594v2)

**Tags**: cs.LG cs.AI cs.CR 



### PerfDojo: Automated ML Library Generation for Heterogeneous   Architectures
**Authors**: Andrei Ivanov, Siyuan Shen, Gioele Gottardo, Marcin Chrapek, Afif Boudaoud, Timo Schneider, Luca Benini, Torsten Hoefler

**Updated**: 2025-11-05T16:05:26Z

**Summary**: The increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model features (e.g., sparsity, quantization), and architecture-specific optimizations complicate performance tuning. Manual optimization is resource-intensive, while existing automatic approaches often rely on complex hardware-specific heuristics and uninterpretable intermediate representations, hindering performance portability. We introduce PerfLLM, a novel automatic optimization methodology leveraging Large Language Models (LLMs) and Reinforcement Learning (RL). Central to this is PerfDojo, an environment framing optimization as an RL game using a human-readable, mathematically-inspired code representation that guarantees semantic validity through transformations. This allows effective optimization without prior hardware knowledge, facilitating both human analysis and RL agent training. We demonstrate PerfLLM's ability to achieve significant performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.

**Link**: [arxiv](http://arxiv.org/abs/2511.03586v1),  [pdf](http://arxiv.org/pdf/2511.03586v1)

**Tags**: cs.PF cs.AI 



### First Detection of CH3OD in Prestellar Cores
**Authors**: Beatrice M. Kulterer, Asunción Fuente, Maria N. Drozdovskaya, Silvia Spezzano, Gisela Esplugues, David Navarro-Almaida, Marina Rodríguez Baras, Angèle Taillard, Karin Öberg

**Updated**: 2025-11-05T16:02:40Z

**Summary**: The isotopic ratios of deuterated methanol derived around protostars are commonly used to infer the physical conditions under which they formed in the earlier prestellar stage. However, there is a discrepancy in the ratio of the singly deuterated methanol isotopologues, CH2DOH/CH3OD, between low- and high-mass protostars, which puts into question whether prestellar isotopic ratios are generally preserved during the star- and planet-forming process. Resolving this puzzle is only made harder by the complete lack of data on this ratio in the prestellar stage. This work presents observations with the IRAM 30m telescope that securely detect CH3OD in the prestellar core L1448 in Perseus and tentatively in B213-C6 in Taurus. This work constrains the ratio of CH2DOH/CH3OD and the D/H ratios for both singly deuterated methanol isotopologues for the first time at the prestellar stage. Column densities calculated under the assumption of local thermal equilibrium lead to a CH2DOH/CH3OD ratio of 2.8-8.5 in L1448 and $\leq$ 5.7 in B213-C6. The values are marginally consistent with the statistically expected ratio of 3, but most assumptions put the values in an elevated range in line with values found around low-mass protostars. The D/H ratio in CH2DOH is between 3.6% and 6.8% in L1448 and in the range of 2.4-5.8% in B213-C6. The D/H ratio derived for CH3OD is lower, namely 1.4-4.4% in L1448 and $\leq$ 3.8% in B213-C6.

**Link**: [arxiv](http://arxiv.org/abs/2511.03581v1),  [pdf](http://arxiv.org/pdf/2511.03581v1)

**Tags**: astro-ph.GA astro-ph.EP astro-ph.SR 



### Leniency Designs: An Operator's Manual
**Authors**: Paul Goldsmith-Pinkham, Peter Hull, Michal Kolesár

**Updated**: 2025-11-05T15:53:19Z

**Summary**: We develop a step-by-step guide to leniency (a.k.a. judge or examiner instrument) designs, drawing on recent econometric literatures. The unbiased jackknife instrumental variables estimator (UJIVE) is purpose-built for leveraging exogenous leniency variation, avoiding subtle biases even in the presence of many decision-makers or controls. We show how UJIVE can also be used to assess key assumptions underlying leniency designs, including quasi-random assignment and average first-stage monotonicity, and to probe the external validity of treatment effect estimates. We further discuss statistical inference, arguing that non-clustered standard errors are often appropriate. A reanalysis of Farre-Mensa et al. (2020), using quasi-random examiner assignment to estimate the value of patents to startups, illustrates our checklist.

**Link**: [arxiv](http://arxiv.org/abs/2511.03572v1),  [pdf](http://arxiv.org/pdf/2511.03572v1)

**Tags**: econ.EM 



### TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and   Retrieval
**Authors**: Günther Schindler, Maximilian Schambach, Michael Medek, Sam Thelin

**Updated**: 2025-11-05T15:51:03Z

**Summary**: We study LLMs for tabular prediction with mixed text, numeric, and categorical fields. We introduce TabGemma, a schema-agnostic in-context learner that treats rows as sequences and tackles two practical hurdles when adapting pretrained LLMs for tabular predictions: unstable numeric tokenization and limited context size. We propose to canonicalize numbers via signed scientific notation and continue pretraining of a 12B Gemma 3 model with a target imputation objective using a large-scale real world dataset. For inference, we use a compact n-gram-based retrieval to select informative exemplars that fit within a 128k-token window.   On semantically rich benchmarks, TabGemma establishes a new state of the art on classification across low- and high-data regimes and improves monotonically with more context rows. For regression, it is competitive at small sample sizes but trails conventional approaches as data grows. Our results show that LLMs can be effective tabular in-context learners on highly semantic tasks when paired with dedicated numeric handling and context retrieval, while motivating further advances in numeric modeling and long-context scaling.

**Link**: [arxiv](http://arxiv.org/abs/2511.03570v1),  [pdf](http://arxiv.org/pdf/2511.03570v1)

**Tags**: cs.LG 



### ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for   Enhanced Legal Regulation
**Authors**: One Octadion, Bondan Sapta Prakoso, Nanang Yudi Setiawan, Novanto Yudistira

**Updated**: 2025-11-05T15:45:52Z

**Summary**: In this study, we explore the fine-tuning of Large Language Models (LLMs) to better support policymakers in their crucial work of understanding, analyzing, and crafting legal regulations. To equip the model with a deep understanding of legal texts, we curated a supervised dataset tailored to the specific needs of the legal domain. Additionally, we integrated the Retrieval-Augmented Generation (RAG) method, enabling the LLM to access and incorporate up-to-date legal knowledge from external sources. This combination of fine-tuning and RAG-based augmentation results in a tool that not only processes legal information but actively assists policymakers in interpreting regulations and drafting new ones that align with current needs. The results demonstrate that this approach can significantly enhance the effectiveness of legal research and regulation development, offering a valuable resource in the ever-evolving field of law.

**Link**: [arxiv](http://arxiv.org/abs/2511.03563v1),  [pdf](http://arxiv.org/pdf/2511.03563v1)

**Tags**: cs.CL 



### Post-2024 U.S. Presidential Election Analysis of Election and Poll Data:   Real-life Validation of Prediction via Small Area Estimation and Uncertainty   Quantification
**Authors**: Zheshi Zheng, Yuanyuan Li, Peter X. K. Song, Jiming Jiang

**Updated**: 2025-11-05T15:36:19Z

**Summary**: We carry out a post-election analysis of the 2024 U.S. Presidential Election (USPE) using a prediction model derived from the Small Area Estimation (SAE) methodology. With pollster data obtained one week prior to the election day, retrospectively, our SAE-based prediction model can perfectly predict the Electoral College election results in all 44 states where polling data were available. In addition to such desirable prediction accuracy, we introduce the probability of incorrect prediction (PoIP) to rigorously analyze prediction uncertainty. Since the standard bootstrap method appears inadequate for estimating PoIP, we propose a conformal inference method that yields reliable uncertainty quantification. We further investigate potential pollster biases by the means of sensitivity analyses and conclude that swing states are particularly vulnerable to polling bias in the prediction of the 2024 USPE.

**Link**: [arxiv](http://arxiv.org/abs/2511.03555v1),  [pdf](http://arxiv.org/pdf/2511.03555v1)

**Tags**: stat.AP 



### Trustworthy Representation Learning via Information Funnels and   Bottlenecks
**Authors**: João Machado de Freitas, Bernhard C. Geiger

**Updated**: 2025-11-05T15:35:30Z

**Summary**: Ensuring trustworthiness in machine learning -- by balancing utility, fairness, and privacy -- remains a critical challenge, particularly in representation learning. In this work, we investigate a family of closely related information-theoretic objectives, including information funnels and bottlenecks, designed to extract invariant representations from data. We introduce the Conditional Privacy Funnel with Side-information (CPFSI), a novel formulation within this family, applicable in both fully and semi-supervised settings. Given the intractability of these objectives, we derive neural-network-based approximations via amortized variational inference. We systematically analyze the trade-offs between utility, invariance, and representation fidelity, offering new insights into the Pareto frontiers of these methods. Our results demonstrate that CPFSI effectively balances these competing objectives and frequently outperforms existing approaches. Furthermore, we show that by intervening on sensitive attributes in CPFSI's predictive posterior enhances fairness while maintaining predictive performance. Finally, we focus on the real-world applicability of these approaches, particularly for learning robust and fair representations from tabular datasets in data scarce-environments -- a modality where these methods are often especially relevant.

**Link**: [arxiv](http://arxiv.org/abs/2211.01446v2),  [pdf](http://arxiv.org/pdf/2211.01446v2)

**Tags**: cs.LG 



### Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning   Large Language Models
**Authors**: Nghia Bui, Guergana Savova, Lijing Wang

**Updated**: 2025-11-05T15:35:21Z

**Summary**: The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance.In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2503.07329v2),  [pdf](http://arxiv.org/pdf/2503.07329v2)

**Tags**: cs.CL cs.AI cs.LG 



### Intelligent Computing Social Modeling and Methodological Innovations in   Political Science in the Era of Large Language Models
**Authors**: Zhenyu Wang, Dequan Wang, Yi Xu, Lingfeng Zhou, Yiqi Zhou

**Updated**: 2025-11-05T15:35:06Z

**Summary**: The recent wave of artificial intelligence, epitomized by large language models (LLMs),has presented opportunities and challenges for methodological innovation in political science,sparking discussions on a potential paradigm shift in the social sciences. However, how can weunderstand the impact of LLMs on knowledge production and paradigm transformation in thesocial sciences from a comprehensive perspective that integrates technology and methodology? What are LLMs' specific applications and representative innovative methods in political scienceresearch? These questions, particularly from a practical methodological standpoint, remainunderexplored. This paper proposes the "Intelligent Computing Social Modeling" (ICSM) methodto address these issues by clarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs in idea synthesis and action simulation, advancing intellectual exploration inpolitical science through "simulated social construction" and "simulation validation." Bysimulating the U.S. presidential election, this study empirically demonstrates the operationalpathways and methodological advantages of ICSM. By integrating traditional social scienceparadigms, ICSM not only enhances the quantitative paradigm's capability to apply big data toassess the impact of factors but also provides qualitative paradigms with evidence for socialmechanism discovery at the individual level, offering a powerful tool that balances interpretabilityand predictability in social science research. The findings suggest that LLMs will drivemethodological innovation in political science through integration and improvement rather thandirect substitution.

**Link**: [arxiv](http://arxiv.org/abs/2410.16301v2),  [pdf](http://arxiv.org/pdf/2410.16301v2)

**Tags**: cs.CY cs.AI 



### MultiZebraLogic: A Multilingual Logical Reasoning Benchmark
**Authors**: Sofie Helene Bruun, Dan Saattrup Smart

**Updated**: 2025-11-05T15:34:48Z

**Summary**: Measuring the full abilities of large language models (LLMs) requires benchmarks representing multiple tasks. We aim to create large, high-quality datasets for comparison of logical reasoning skills across several languages and of suitable difficulty for LLMs of various reasoning ability. We explore multiple ways of increasing difficulty. We generate zebra puzzles in multiple languages, themes, sizes and including 14 different clue types and 8 red herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a reasoning model), respectively. Including 5 red herrings decreases o3-mini puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5 puzzles are not significantly affected by use of English vs. Danish or the common houses theme vs. the country-specific smoerrebroed theme. We find no correlation between difficulty and the selected clue types. Datasets of 128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic languages for sizes 2x3 and 4x5. We publish code for puzzle generation, designed for adaptablity into more languages and themes.

**Link**: [arxiv](http://arxiv.org/abs/2511.03553v1),  [pdf](http://arxiv.org/pdf/2511.03553v1)

**Tags**: cs.CL cs.AI 



### Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code   Understanding
**Authors**: Ziv Nevo, Orna Raz, Karen Yorav

**Updated**: 2025-11-05T15:31:42Z

**Summary**: Understanding the purpose of source code is a critical task in software maintenance, onboarding, and modernization. While large language models (LLMs) have shown promise in generating code explanations, they often lack grounding in the broader software engineering context. We propose a novel approach that leverages natural language artifacts from GitHub -- such as pull request descriptions, issue descriptions and discussions, and commit messages -- to enhance LLM-based code understanding. Our system consists of three components: one that extracts and structures relevant GitHub context, another that uses this context to generate high-level explanations of the code's purpose, and a third that validates the explanation. We implemented this as a standalone tool, as well as a server within the Model Context Protocol (MCP), enabling integration with other AI-assisted development tools. Our main use case is that of enhancing a standard LLM-based code explanation with code insights that our system generates. To evaluate explanations' quality, we conducted a small scale user study, with developers of several open projects, as well as developers of proprietary projects. Our user study indicates that when insights are generated they often are helpful and non trivial, and are free from hallucinations.

**Link**: [arxiv](http://arxiv.org/abs/2511.03549v1),  [pdf](http://arxiv.org/pdf/2511.03549v1)

**Tags**: cs.SE cs.AI 



### EP250108a/SN2025kg: A Magnetar-powered Gamma-Ray Burst Supernova   Originating from a Close Helium-star Binary via Isolated Binary Evolution
**Authors**: Jin-Ping Zhu, Jian-He Zheng, Bing Zhang

**Updated**: 2025-11-05T15:28:30Z

**Summary**: SN\,2025kg, linked to EP250108a, is among the brightest broad-lined Type Ic supernova (SN Ic-BL) known, showing unique helium absorptions, a late-time broad H$\alpha$, and an early bump. In this {\em{Letter}}, we propose a jet-cocoon origin to explain EP250108a as off-axis cooling emission from a mildly relativistic inner cocoon viewed at $\sim45^\circ$ and the early bump of SN\,2025kg as the outer cocoon cooling emission, both constraining an energy of $\sim(1-2)\times10^{52}{\rm{erg}}$ and a progenitor radius of $\sim5\,R_\odot$. To explain SN\,2025kg's exceptionally luminous peak, potential energy injection into the $\sim2.5\,M_\odot$ ejecta from a magnetar with initial period $\sim1.7\,{\rm{ms}}$ and magnetic field $\sim2\times10^{15}{\rm{G}}$ may be required, implying a rapidly rotating $\sim4\,M_\odot$ progenitor. Thus, the progenitor may be a low-mass helium star with an extended helium envelope, supported by helium absorption lines and an inferred weak pre-SN wind. Hydrogen-rich material may reside in the inner ejecta layers, as suggested by the late-time broad H$\alpha$, possibly originating from main-sequence companion material evaporated by the magnetar wind. Since the observed near-solar metallicity challenges the popular quasi-chemically homogeneous evolution channel, the rapidly rotating helium-star progenitor of EP250108a/SN\,2025kg might attain angular momentum by being tidally spun up by a main-sequence companion in a close binary formed through isolated binary evolution.

**Link**: [arxiv](http://arxiv.org/abs/2507.18544v2),  [pdf](http://arxiv.org/pdf/2507.18544v2)

**Tags**: astro-ph.HE astro-ph.SR 



### One pathogen does not an epidemic make: A review of interacting   contagions, diseases, beliefs, and stories
**Authors**: Laurent Hébert-Dufresne, Yong-Yeol Ahn, Antoine Allard, Vittoria Colizza, Jessica W. Crothers, Peter Sheridan Dodds, Mirta Galesic, Fakhteh Ghanbarnejad, Dominique Gravel, Ross A. Hammond, Kristina Lerman, Juniper Lovato, John J. Openshaw, S. Redner, Samuel V. Scarpino, Guillaume St-Onge, Timothy R. Tangherlini, Jean-Gabriel Young

**Updated**: 2025-11-05T15:21:15Z

**Summary**: From pathogens and computer viruses to genes and memes, contagion models have found widespread utility across the natural and social sciences. Despite their success and breadth of adoption, the approach and structure of these models remain surprisingly siloed by field. Given the siloed nature of their development and widespread use, one persistent assumption is that a given contagion can be studied in isolation, independently from what else might be spreading in the population. In reality, countless contagions of biological and social nature interact within hosts (interacting with existing beliefs, or the immune system) and across hosts (interacting in the environment, or affecting transmission mechanisms). Additionally, from a modeling perspective, we know that relaxing these assumptions has profound effects on the physics and translational implications of the models. Here, we review mechanisms for interactions in social and biological contagions, as well as the models and frameworks developed to include these interactions in the study of the contagions. We highlight existing problems related to the inference of interactions and to the scalability of mathematical models and identify promising avenues of future inquiries. In doing so, we highlight the need for interdisciplinary efforts under a unified science of contagions and for removing a common dichotomy between social and biological contagions.

**Link**: [arxiv](http://arxiv.org/abs/2504.15053v2),  [pdf](http://arxiv.org/pdf/2504.15053v2)

**Tags**: physics.soc-ph q-bio.PE 



### Security Audit of intel ICE Driver for e810 Network Interface Card
**Authors**: Oisin O Sullivan

**Updated**: 2025-11-05T15:15:41Z

**Summary**: The security of enterprise-grade networking hardware and software is critical to ensuring the integrity, availability, and confidentiality of data in modern cloud and data center environments. Network interface controllers (NICs) play a pivotal role in high-performance computing and virtualization, but their privileged access to system resources makes them a prime target for security vulnerabilities. This study presents a security analysis of the Intel ICE driver using the E810 Ethernet Controller, employing static analysis, fuzz testing, and timing-based side-channel evaluation to assess robustness against exploitation. The objective is to evaluate the drivers resilience to malformed inputs, identify implementation weaknesses, and determine whether timing discrepancies can be exploited for unauthorized inference of system states. Static code analysis reveals that insufficient bounds checking and unsafe string operations may introduce security flaws. Fuzz testing targets the Admin Queue, debugfs interface, and virtual function (VF) management. Interface-aware fuzzing and command mutation confirm strong input validation that prevents memory corruption and privilege escalation under normal conditions. However, using principles from KernelSnitch, the driver is found to be susceptible to timing-based side-channel attacks. Execution time discrepancies in hash table lookups allow an unprivileged attacker to infer VF occupancy states, enabling potential network mapping in multi-tenant environments. Further analysis shows inefficiencies in Read-Copy-Update (RCU) synchronization, where missing synchronization leads to stale data persistence, memory leaks, and out-of-memory conditions. Kernel instrumentation confirms that occupied VF lookups complete faster than unoccupied queries, exposing timing-based information leakage.

**Link**: [arxiv](http://arxiv.org/abs/2511.01910v2),  [pdf](http://arxiv.org/pdf/2511.01910v2)

**Tags**: cs.CR 



### ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS
**Authors**: Weijie Wang, Donny Y. Chen, Zeyu Zhang, Duochao Shi, Akide Liu, Bohan Zhuang

**Updated**: 2025-11-05T15:14:01Z

**Summary**: Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a promising solution for novel view synthesis, enabling one-pass inference without the need for per-scene 3DGS optimization. However, their scalability is fundamentally constrained by the limited capacity of their models, leading to degraded performance or excessive memory consumption as the number of input views increases. In this work, we analyze feed-forward 3DGS frameworks through the lens of the Information Bottleneck principle and introduce ZPressor, a lightweight architecture-agnostic module that enables efficient compression of multi-view inputs into a compact latent state $Z$ that retains essential scene information while discarding redundancy. Concretely, ZPressor enables existing feed-forward 3DGS models to scale to over 100 input views at 480P resolution on an 80GB GPU, by partitioning the views into anchor and support sets and using cross attention to compress the information from the support views into anchor views, forming the compressed latent state $Z$. We show that integrating ZPressor into several state-of-the-art feed-forward 3DGS models consistently improves performance under moderate input views and enhances robustness under dense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K. The video results, code and trained models are available on our project page: https://lhmd.top/zpressor.

**Link**: [arxiv](http://arxiv.org/abs/2505.23734v3),  [pdf](http://arxiv.org/pdf/2505.23734v3)

**Tags**: cs.CV 



### CASteer: Steering Diffusion Models for Controllable Generation
**Authors**: Tatiana Gaintseva, Andreea-Maria Oncescu, Chengcheng Ma, Ziquan Liu, Martin Benning, Gregory Slabaugh, Jiankang Deng, Ismail Elezi

**Updated**: 2025-11-05T15:13:40Z

**Summary**: Diffusion models have transformed image generation, yet controlling their outputs to reliably erase undesired concepts remains challenging. Existing approaches usually require task-specific training and struggle to generalize across both concrete (e.g., objects) and abstract (e.g., styles) concepts. We propose CASteer (Cross-Attention Steering), a training-free framework for concept erasure in diffusion models using steering vectors to influence hidden representations dynamically. CASteer precomputes concept-specific steering vectors by averaging neural activations from images generated for each target concept. During inference, it dynamically applies these vectors to suppress undesired concepts only when they appear, ensuring that unrelated regions remain unaffected. This selective activation enables precise, context-aware erasure without degrading overall image quality. This approach achieves effective removal of harmful or unwanted content across a wide range of visual concepts, all without model retraining. CASteer outperforms state-of-the-art concept erasure techniques while preserving unrelated content and minimizing unintended effects. Pseudocode is provided in the supplementary.

**Link**: [arxiv](http://arxiv.org/abs/2503.09630v2),  [pdf](http://arxiv.org/pdf/2503.09630v2)

**Tags**: cs.GR 



### The GECKOS Survey: revealing the formation history of a barred galaxy   via structural decomposition and resolved spectroscopy
**Authors**: A. Fraser-McKelvie, D. A. Gadotti, F. Fragkoudi, C. de Sá-Freitas, M. Martig, M. Bureau, T. Davis, R. Elliott, E. Emsellem, D. Fisher, M. R. Hayden, J. van de Sande, A. B. Watts

**Updated**: 2025-11-05T15:07:38Z

**Summary**: Disentangling the (co-)evolution of individual galaxy structural components remains a difficult task, owing to the inability to cleanly isolate light from spatially overlapping components. In this pilot study of PGC\,044931, observed as part of the GECKOS survey, we utilise a VIRCAM $H$-band image to decompose the galaxy into five photometric components, three of which dominate by contributing $>50\%$ of light in specific regions: a main disc, a boxy/peanut bulge, and a nuclear disc. When the photometric decompositions are mapped onto MUSE observations, we find remarkably good separation in stellar kinematic space. All three structures occupy unique locations in the parameter space of the ratio of the light-weighted stellar line-of-sight mean velocity and velocity dispersion ($\rm{V}_{\star}/\sigma_{\star}$), and the high-order stellar skew ($h_{3}$). These clear and distinct kinematic behaviours allow us to make inferences about the formation histories of the individual components from observations of the mean stellar ages and metallicities of the three components. A clear story emerges: the main disc built over a sustained and extended star formation phase, possibly partly fuelled by gas from a low-metallicity reservoir. Early on, that disc formed a bar that buckled and subsequently formed a nuclear disc in multiple and enriched star-formation episodes. This result is an example of how careful photometric decompositions, combined with spatially well-resolved stellar kinematic information, can help separate out age-metallicity relations of different components and therefore disentangle the formation history of a galaxy. The results of this pilot study can be extended to a differential study of all GECKOS survey galaxies to assert the true diversity of Milky Way-like galaxies.

**Link**: [arxiv](http://arxiv.org/abs/2509.15976v2),  [pdf](http://arxiv.org/pdf/2509.15976v2)

**Tags**: astro-ph.GA 



### HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs
**Authors**: Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto L. Castro, Torsten Hoefler, Dan Alistarh

**Updated**: 2025-11-05T15:07:16Z

**Summary**: Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight and activation outlier values that make lower-precision optimization difficult. To address this, we present HALO, a novel quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, which mitigate outliers, 2) high-performance kernel support, and 3) FSDP integration for low-precision communication. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLAMA-family models, HALO achieves near-full-precision-equivalent results during fine-tuning on various tasks, while delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. HALO efficiently supports both standard and parameterefficient fine-tuning (PEFT). Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in 8-bit precision, while delivering performance benefits. Code is available at https://github.com/IST-DASLab/HALO.

**Link**: [arxiv](http://arxiv.org/abs/2501.02625v3),  [pdf](http://arxiv.org/pdf/2501.02625v3)

**Tags**: cs.LG 



### PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband   Signals
**Authors**: Zhaoxin Chang, Fusang Zhang, Jie Xiong, Ziyu Li, Badii Jouaber, Daqing Zhang

**Updated**: 2025-11-05T15:06:39Z

**Summary**: In recent years, the number of Internet of Things (IoT) devices in smart homes has rapidly increased. A key challenge affecting user experience is how to enable users to efficiently and intuitively select the devices they wish to control. This paper proposes PnPSelect, a plug-and-play IoT device selection solution utilizing Ultra-wideband (UWB) technology on commercial devices. Unlike previous works, PnPSelect does not require the installation of dedicated hardware on each IoT device, thereby reducing deployment costs and complexities, and achieving true plug-and-play functionality. To enable intuitive device selection, we introduce a pointing direction estimation method that utilizes UWB readings from a single anchor to infer the user pointing direction. Additionally, we propose a lightweight device localization method that allows users to register new IoT devices by simply pointing at them from two distinct positions, eliminating the need for manual measurements. We implement PnPSelect on commercial smartphones and smartwatches and conduct extensive evaluations in both controlled laboratory settings and real-world environments. Our results demonstrate high accuracy, robustness, and adaptability, making PnPSelect a practical and scalable solution for next-generation smart home interactions.

**Link**: [arxiv](http://arxiv.org/abs/2511.03534v1),  [pdf](http://arxiv.org/pdf/2511.03534v1)

**Tags**: cs.HC 



### Counterexamples to statements on isometric graph coverings
**Authors**: Paul Bastide, Julien Duron, Jędrzej Hodor, Weichan Liu, Xiangxiang Nie

**Updated**: 2025-11-05T14:57:03Z

**Summary**: A connected subgraph of a graph is isometric if it preserves distances. In this short note, we provide counterexamples to several variants of the following general question: When a graph $G$ is edge covered by connected isometric subgraphs $H_1,\dots,H_k$, which properties of $G$ can we infer from properties of $H_1,\dots,H_k$? For example, Dumas, Foucaud, Perez and Todinca (SIDMA, 2024) proved that when $H_1,\dots,H_k$ are paths, then the pathwidth of $G$ is bounded in terms of $k$. Among others, we show that there are graphs of arbitrarily large treewidth that can be isometrically edge covered by four trees.

**Link**: [arxiv](http://arxiv.org/abs/2511.03524v1),  [pdf](http://arxiv.org/pdf/2511.03524v1)

**Tags**: math.CO 



### Inv-Entropy: A Fully Probabilistic Framework for Uncertainty   Quantification in Language Models
**Authors**: Haoyi Song, Ruihan Ji, Naichen Shi, Fan Lai, Raed Al Kontar

**Updated**: 2025-11-05T14:51:07Z

**Summary**: Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a probabilistic interpretation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input-output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.09684v2),  [pdf](http://arxiv.org/pdf/2506.09684v2)

**Tags**: cs.CL 



### U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility
**Authors**: Wencheng Ye, Yan Liu

**Updated**: 2025-11-05T14:46:58Z

**Summary**: Large language models (LLMs) have shown strong capabilities in software engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle well-defined problems using conventional methods, often overlooking alternative or innovative solutions beyond their predefined frameworks. This limitation is evident in open-world software environments, where emerging challenges transcend established paradigms.   We propose U2F (Unknown Unknowns to Functional solutions), a cognitive-inspired, uncertainty-embracing multi-agent framework that systematically surfaces "Unknown Unknowns" - novel solution pathways absent from initial formulations but holding innovative potential. U2F consists of two key components: (1) a Discovery-Exploration-Integration agent system for uncovering and synthesizing potential solutions, and (2) cognitive enhancement mechanisms across three dimensions: cross-domain analogical reasoning, reverse thinking, and external validation, which strategically reframe and extend conventional solution boundaries.   Applied to 218 real-world software enabler stories curated from authentic engineering tasks, U2F achieved notable improvements: human experts reported a 14 percent increase in overall novelty, 51 percent improvement in semantic novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based evaluator. These results highlight the potential of embracing uncertainty as a catalyst for innovation in software engineering.

**Link**: [arxiv](http://arxiv.org/abs/2511.03517v1),  [pdf](http://arxiv.org/pdf/2511.03517v1)

**Tags**: cs.SE 



### One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction   Following with a Benchmark Evolving Framework
**Authors**: Qi Jia, Kaiwei Zhang, Xiujie Song, Ye Shen, Xiangyang Zhu, Guangtao Zhai

**Updated**: 2025-11-05T14:39:59Z

**Summary**: Understanding how well large language models can follow users' instructions throughout a dialogue spanning multiple topics is of great importance for data-intensive conversational applications. Existing benchmarks are often limited to a fixed number of turns, making them susceptible to saturation and failing to account for the user's interactive experience. In this work, we propose an extensible framework for assessing multi-turn instruction-following ability. At its core, our framework decouples linguistic surface forms from user intent simulation through a three-layer mechanism that tracks constraints, instructions, and topics. This framework mimics User-LLM interaction by enabling the dynamic construction of benchmarks with state changes and tracebacks, terminating a conversation only when the model exhausts a simulated user's patience. We define a suite of metrics capturing the quality of the interaction process. Using this framework, we construct EvolIF, an evolving instruction-following benchmark incorporating nine distinct constraint types. Our results indicate that GPT-5 exhibits superior instruction-following performance. It sustains an average of 18.54 conversational turns and demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant margin of 11.41%, while other models lag far behind. All of the data and code will be made publicly available online.

**Link**: [arxiv](http://arxiv.org/abs/2511.03508v1),  [pdf](http://arxiv.org/pdf/2511.03508v1)

**Tags**: cs.CL 



### HaluMem: Evaluating Hallucinations in Memory Systems of Agents
**Authors**: Ding Chen, Simin Niu, Kehang Li, Peng Liu, Xiangping Zheng, Bo Tang, Xinchi Li, Feiyu Xiong, Zhiyu Li

**Updated**: 2025-11-05T14:37:34Z

**Summary**: Memory systems are key components that enable AI systems such as LLMs and AI agents to achieve long-term learning and sustained interaction. However, during memory storage and retrieval, these systems frequently exhibit memory hallucinations, including fabrication, errors, conflicts, and omissions. Existing evaluations of memory hallucinations are primarily end-to-end question answering, which makes it difficult to localize the operational stage within the memory system where hallucinations arise. To address this, we introduce the Hallucination in Memory Benchmark (HaluMem), the first operation level hallucination evaluation benchmark tailored to memory systems. HaluMem defines three evaluation tasks (memory extraction, memory updating, and memory question answering) to comprehensively reveal hallucination behaviors across different operational stages of interaction. To support evaluation, we construct user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and HaluMem-Long. Both include about 15k memory points and 3.5k multi-type questions. The average dialogue length per user reaches 1.5k and 2.6k turns, with context lengths exceeding 1M tokens, enabling evaluation of hallucinations across different context scales and task complexities. Empirical studies based on HaluMem show that existing memory systems tend to generate and accumulate hallucinations during the extraction and updating stages, which subsequently propagate errors to the question answering stage. Future research should focus on developing interpretable and constrained memory operation mechanisms that systematically suppress hallucinations and improve memory reliability.

**Link**: [arxiv](http://arxiv.org/abs/2511.03506v1),  [pdf](http://arxiv.org/pdf/2511.03506v1)

**Tags**: cs.CL 



### Emergent tuning heterogeneity in cortical circuits is sensitive to   cellular neuronal dynamics
**Authors**: Mohammadreza Soltanipour, Stefan Treue, Fred Wolf

**Updated**: 2025-11-05T14:33:04Z

**Summary**: Cortical circuits exhibit high levels of response diversity, even across apparently uniform neuronal populations. While emerging data-driven approaches exploit this heterogeneity to infer effective models of cortical circuit computation (e.g. Genkin et al. Nature 2025), the power of response diversity to enable inference of mechanistic circuit models is largely unexplored. Within the landscape of cortical circuit models, spiking neuron networks in the balanced state naturally exhibit high levels of response and tuning diversity emerging from their internal dynamics. A statistical theory for this emergent tuning heterogeneity, however, has only been formulated for binary spin models (Vreeswijk & Sompolinsky, 2005). Here we present a formulation of feature-tuned balanced state networks that allows for arbitrary and diverse dynamics of postsynaptic currents and variable levels of heterogeneity in cellular excitability but nevertheless is analytically exactly tractable with respect to the emergent tuning curve heterogeneity. Using this framework, we present a case study demonstrating that, for a wide range of parameters even the population mean response is non-universal and sensitive to mechanistic circuit details. As our theory enables exactly and analytically obtaining the likelihood-function of tuning heterogeneity given circuit parameters, we argue that it forms a powerful and rigorous basis for neural circuit inference.

**Link**: [arxiv](http://arxiv.org/abs/2511.03502v1),  [pdf](http://arxiv.org/pdf/2511.03502v1)

**Tags**: q-bio.NC physics.bio-ph 



### Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction
**Authors**: Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

**Updated**: 2025-11-05T14:29:12Z

**Summary**: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.

**Link**: [arxiv](http://arxiv.org/abs/2508.02558v2),  [pdf](http://arxiv.org/pdf/2508.02558v2)

**Tags**: cs.CL 



### ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied   AI Applications
**Authors**: Lei Fu, Sahar Salimpour, Leonardo Militano, Harry Edelman, Jorge Peña Queralta, Giovanni Toffetti

**Updated**: 2025-11-05T14:27:58Z

**Summary**: Agentic AI systems and Physical or Embodied AI systems have been two key research verticals at the forefront of Artificial Intelligence and Robotics, with Model Context Protocol (MCP) increasingly becoming a key component and enabler of agentic applications. However, the literature at the intersection of these verticals, i.e., Agentic Embodied AI, remains scarce. This paper introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for analyzing, visualizing and processing robot data with natural language through LLMs and VLMs. We describe specific tooling built with robotics domain knowledge, with our initial release focused on mobile robotics and supporting natively the analysis of trajectories, laser scan data, transforms, or time series data. This is in addition to providing an interface to standard ROS 2 CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to filter bags with a subset of topics or trimmed in time. Coupled with the MCP server, we provide a lightweight UI that allows the benchmarking of the tooling with different LLMs, both proprietary (Anthropic, OpenAI) and open-source (through Groq). Our experimental results include the analysis of tool calling capabilities of eight different state-of-the-art LLM/VLM models, both proprietary and open-source, large and small. Our experiments indicate that there is a large divide in tool calling capabilities, with Kimi K2 and Claude Sonnet 4 demonstrating clearly superior performance. We also conclude that there are multiple factors affecting the success rates, from the tool description schema to the number of arguments, as well as the number of tools available to the models. The code is available with a permissive license at https://github.com/binabik-ai/mcp-rosbags.

**Link**: [arxiv](http://arxiv.org/abs/2511.03497v1),  [pdf](http://arxiv.org/pdf/2511.03497v1)

**Tags**: cs.RO cs.AI cs.SE 



### Revisiting Multimodal Positional Encoding in Vision-Language Models
**Authors**: Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, Shuai Bai

**Updated**: 2025-11-05T14:25:38Z

**Summary**: Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.

**Link**: [arxiv](http://arxiv.org/abs/2510.23095v2),  [pdf](http://arxiv.org/pdf/2510.23095v2)

**Tags**: cs.CV 



### Why Less is More (Sometimes): A Theory of Data Curation
**Authors**: Elvis Dohmatob, Mohammad Pezeshki, Reyhane Askari-Hemmat

**Updated**: 2025-11-05T14:21:18Z

**Summary**: This paper introduces a theoretical framework to resolve a central paradox in modern machine learning: When is it better to use less data? This question has become critical as classical scaling laws suggesting ``more is more'' (Sun et al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et al., 2025; Muenighoff et al., 2025), which achieve superior performance with small, aggressively curated datasets. Here, we study data curation strategies where an imperfect oracle selects the training examples according to their difficulty and correctness. Our results provide exact scaling law curves for test error under both label-agnostic and label-aware curation rules, revealing when and why keeping only a subset of data can improve generalization. In contrast to classical scaling laws, we show that under certain conditions, small curated datasets can outperform full datasets, and we provide analytical conditions for this by deriving precise phase transition curves tied to data size and quality. We validate these theoretical claims with empirical results on ImageNet, confirming our predictions about when curation improves accuracy and can even mitigate model collapse. Furthermore, our framework provides a principled explanation for the contradictory curation strategies recently observed in LLM mathematical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2511.03492v1),  [pdf](http://arxiv.org/pdf/2511.03492v1)

**Tags**: cs.LG stat.ML 



### From Haystack to Needle: Label Space Reduction for Zero-shot   Classification
**Authors**: Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke

**Updated**: 2025-11-05T14:16:25Z

**Summary**: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.

**Link**: [arxiv](http://arxiv.org/abs/2502.08436v2),  [pdf](http://arxiv.org/pdf/2502.08436v2)

**Tags**: cs.CL cs.AI cs.LG 



### Probing $J/ψ$ Production Mechanisms in Proton-Proton Collisions at   SPD/NICA Energies
**Authors**: Shubham Sharma, Alexey Aparin

**Updated**: 2025-11-05T14:02:11Z

**Summary**: We investigate inclusive $J/\psi$ production in proton-proton collisions at tens of GeV $\sqrt{s}$ energy, relevant for forthcoming measurements with the Spin Physics Detector (SPD) at NICA. Simulations are performed using the PEGASUS event generator with transverse-momentum-dependent (TMD) gluon densities, comparing the recent KMR-based KL$'2025$ and CCFM-based LLM$'2024$ parametrizations. Differential cross sections in rapidity and transverse momentum exhibit smooth, stable behavior under renormalization-scale variation, while factorization-scale dependence exposes limitations of the LLM$'2024$ set at low scales in contrast to KL$'2025$. Normalized $p_T$ spectra reveal distinct hardening patterns linked to the underlying gluon $k_T$ broadening in each model. The relative contributions of color-singlet and color-octet channels are also quantified, demonstrating the dominance of color-octet mechanisms in the SPD energy regime. These results provide the first detailed assessment of quarkonium production sensitivity to gluon TMDs near threshold, offering timely theoretical guidance for upcoming $J/\psi$ measurements at SPD/NICA.

**Link**: [arxiv](http://arxiv.org/abs/2511.03477v1),  [pdf](http://arxiv.org/pdf/2511.03477v1)

**Tags**: hep-ph hep-ex 



### Traversal Verification for Speculative Tree Decoding
**Authors**: Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi

**Updated**: 2025-11-05T13:59:39Z

**Summary**: Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2505.12398v2),  [pdf](http://arxiv.org/pdf/2505.12398v2)

**Tags**: cs.CL cs.AI cs.LG 



### RAGBoost: Efficient Retrieval-Augmented Generation with   Accuracy-Preserving Context Reuse
**Authors**: Yinsicheng Jiang, Yeqi Huang, Liang Cheng, Cheng Deng, Xuan Sun, Luo Mai

**Updated**: 2025-11-05T13:59:01Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.

**Link**: [arxiv](http://arxiv.org/abs/2511.03475v1),  [pdf](http://arxiv.org/pdf/2511.03475v1)

**Tags**: cs.LG 



### RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial   Analysis
**Authors**: Van-Duc Le, Hai-Thien To

**Updated**: 2025-11-05T13:53:51Z

**Summary**: Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyses demand significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analyses through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a comprehensive financial instruction dataset derived from extensive financial documents and earnings reports to guide the LLM's adaptation to specialized financial reasoning. Experimental results demonstrate that RAG-IT outperforms general-purpose open-source models and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.

**Link**: [arxiv](http://arxiv.org/abs/2412.08179v2),  [pdf](http://arxiv.org/pdf/2412.08179v2)

**Tags**: q-fin.ST cs.AI 



### First Associated Neutrino Search for a Failed Supernova Candidate with   Super-Kamiokande
**Authors**: F. Nakanishi, K. Abe, S. Abe, C. Bronner, M. Harada, Y. Hayato, K. Hiraide, K. Hosokawa, T. H. Hung, K. Ieki, M. Ikeda, J. Kameda, Y. Kanemura, Y. Kataoka, S. Miki, S. Mine, M. Miura, S. Moriyama, M. Nakahata, S. Nakayama, Y. Noguchi, G. Pronost, K. Sato, H. Sekiya, K. Shimizu, M. Shiozawa, Y. Suzuki, A. Takeda, Y. Takemoto, H. Tanaka, T. Yano, Y. Itow, T. Kajita, K. Okumura, T. Tashiro, T. Tomiya, X. Wang, P. Fernandez, L. Labarga, B. Zaldivar, B. W. Pointon, C. Yanagisawa, E. Kearns, L. Wan, T. Wester, J. Bian, B. Cortez, N. J. Griskevich, Y. Jiang, M. B. Smy, H. W. Sobel, V. Takhistov, A. Yankelevich, J. Hill, M. C. Jang, S. H. Lee, D. H. Moon, R. G. Park, B. S. Yang, B. Bodur, K. Scholberg, C. W. Walter, A. Beauchêne, Le Blévec, O. Drapier, A. Ershova, M. Ferey, Th. A. Mueller, A. D. Santos, P. Paganini, C. Quach, R. Rogly, T. Nakamura, J. S. Jang, R. P. Litchfield, L. N. Machado, F. J. . P Soler, J. G. Learned, K. Choi, S. Cao, L. H. V. Anthony, N. W. Prouse, M. Scott, Y. Uchida, V. Berardi, N. F. Calabria, M. G. Catanesi, N. Ospina, E. Radicioni, A. Langella, G. De Rosa, G. Collazuol, M. Feltre, M. Mattiazzi, L. Ludovici, M. Gonin, L. L. Périssé, B. Quilain, S. Horiuchi, A. Kawabata, M. Kobayashi, Y. M. Liu, Y. Maekawa, Y. Nishimura, R. Akutsu, M. Friend, T. Hasegawa, Y. Hino, T. Ishida, T. Kobayashi, M. Jakkapu, T. Matsubara, T. Nakadaira, Y. Oyama, A. Portocarrero Yrey, K. Sakashita, T. Sekiguchi, T. Tsukamoto, N. Bhuiyan, G. T. Burton, F. Di Lodovico, J. Gao, T. Katori, R. Kralik, N. Latham, R. M. Ramsden, H. Ito, T. Sone, A. T. Suzuki, Y. Takeuchi, S. Wada, H. Zhong, J. Feng, L. Feng, S. Han, J. Hikida, J. R. Hu, Z. Hu, M. Kawaue, T. Kikawa, T. V. Ngoc, T. Nakaya, R. A. Wendell, S. J. Jenkins, N. McCauley, A. Tarrant, M. Fan`ı, M. J. Wilking, Z. Xie, Y. Fukuda, H. Menjo, Y. Yoshioka, J. Lagoda, M. Mandal, Y. S. Prabhu, J. Zalipska, M. Mori, J. Jiang, K. Hamaguchi, H. Ishino, Y. Koshio, T. Tada, T. Ishizuka, G. Barr, D. Barrow, L. Cook, S. Samani, D. Wark, A. Holin, F. Nova, S. Jung, J. Yoo, J. E. P. Fannon, L. Kneale, M. Malek, J. M. McElwee, T. Peacock, P. Stowell, M. D. Thiesse, L. F. Thompson, H. Okazawa, S. M. Lakshmi, E. Kwon, M. W. Lee, J. W. Seo, I. Yu, Y. Ashida, A. K. Ichikawa, K. D. Nakamura, S. Goto, H. Hayasaki, S. Kodama, Y. Kong, Y. Masaki, Y. Mizuno, T. Muro, K. Nakagiri, Y. Nakajima, N. Taniuchi, M. Yokoyama, P. de Perio, S. Fujita, C. Jesús-Valls, K. Martens, Ll. Marti, K. M. Tsui, M. R. Vagins, J. Xia, M. Kuze, S. Izumiyama, R. Matsumoto, R. Asaka, M. Ishitsuka, M. Sugo, M. Wako, K. Yamauchi, Y. Nakano, F. Cormier, R. Gaur, M. Hartz, A. Konaka, X. Li, B. R. Smithers, S. Chen, Y. Wu, B. D. Xu, A. Q. Zhang, B. Zhang, H. Adhikary, M. Girgus, P. Govindaraj, M. Posiadala-Zezula, Y. S. Prabhu, S. B. Boyd, R. Edwards, D. Hadley, M. Nicholson, M. O'Flaherty, B. Richards, A. Ali, B. Jamieson, C. Bronner, D. Horiguchi, A. Minamino, Y. Sasaki, R. Shibayama, R. Shimamura

**Updated**: 2025-11-05T13:47:17Z

**Summary**: In 2024, a failed supernova candidate, M31-2014-DS1, was reported in the Andromeda galaxy (M31), located at a distance of approximately 770 kpc. In this paper, we search for neutrinos from this failed supernova using data from Super-Kamiokande (SK). Based on the estimated time of black hole formation inferred from optical and infrared observations, we define a search window for neutrino events in the SK data. Using this window, we develop a dedicated analysis method for failed supernovae and apply it to M31-2014-DS1, by conducting a cluster search using the timing and energy information of candidate events. No significant neutrino excess is observed within the search region. Consequently, we place an upper limit on the electron antineutrino luminosity from M31-2014-DS1 and discuss its implications for various failed SN models and their neutrino emission characteristics. Despite the 18 MeV threshold adopted to suppress backgrounds, the search remains sufficiently sensitive to constrain the Shen-TM1 EOS, yielding a 90% confidence level upper limit of 1.76 \times 10^{53} erg on the electron antineutrino luminosity, slightly above the expected value of 1.35 \times 10^{53} erg.

**Link**: [arxiv](http://arxiv.org/abs/2511.03470v1),  [pdf](http://arxiv.org/pdf/2511.03470v1)

**Tags**: astro-ph.HE 



### Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing
**Authors**: Zhihui Chen, Mengling Feng

**Updated**: 2025-11-05T13:45:24Z

**Summary**: Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [https://github.com/richardChenzhihui/med-banana-50k].

**Link**: [arxiv](http://arxiv.org/abs/2511.00801v2),  [pdf](http://arxiv.org/pdf/2511.00801v2)

**Tags**: cs.CV cs.MM 



### A general polynomial emulator for cosmology via moment projection
**Authors**: Zheng Zhang

**Updated**: 2025-11-05T13:25:10Z

**Summary**: We present MomentEmu, a general-purpose polynomial emulator for fast and interpretable mappings between theoretical parameters and observational features. The method constructs moment matrices to project simulation data onto polynomial bases, yielding symbolic expressions that approximate the target mapping. Compared to neural-network-based emulators, MomentEmu offers negligible training cost, millisecond-level evaluation, and transparent functional forms. As a proof-of-concept demonstration, we develop two emulators: PolyCAMB-$D_\ell$, which maps six cosmological parameters to the CMB power spectra (TT, EE, BB, TE), and PolyCAMB-peak, which enables a bidirectional mapping between the cosmological parameters and the acoustic peak features of $D_\ell^{\rm TT}$. PolyCAMB-$D_\ell$ achieves sub-percent accuracy over multipoles $\ell \leq 4050$, while PolyCAMB-peak also attains comparable precision and produces symbolic forms consistent with known analytical approximations. The method is well suited for forward modelling, parameter inference, and uncertainty propagation, particularly when the parameter space is moderate in dimensionality and the mapping is smooth. MomentEmu offers a lightweight and portable alternative to regression-based or black-box emulators in cosmological analysis.

**Link**: [arxiv](http://arxiv.org/abs/2507.02179v3),  [pdf](http://arxiv.org/pdf/2507.02179v3)

**Tags**: astro-ph.CO astro-ph.IM physics.comp-ph 



### HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono-   and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models
**Authors**: Stephan Oepen, Nikolay Arefev, Mikko Aulamo, Marta Bañón, Maja Buljan, Laurie Burchell, Lucas Charpentier, Pinzhen Chen, Mariya Fedorova, Ona de Gibert, Barry Haddow, Jan Hajič, Jindřich Helcl, Andrey Kutuzov, Veronika Laippala, Zihao Li, Risto Luukkonen, Bhavitvya Malik, Vladislav Mikhailov, Amanda Myntti, Dayyán O'Brien, Lucie Poláková, Sampo Pyysalo, Gema Ramírez Sánchez, Janine Siewert, Pavel Stepachev, Jörg Tiedemann, Teemu Vahtola, Dušan Variš, Fedor Vitiugin, Tea Vojtěchová, Jaume Zaragoza

**Updated**: 2025-11-05T13:19:47Z

**Summary**: We present an ongoing initiative to provide open, very large, high-quality, and richly annotated textual datasets for almost 200 languages. At 30 trillion tokens, this is likely the largest generally available multilingual collection of LLM pre-training data. These datasets are derived from web crawls from different sources and accompanied with a complete, open-source pipeline for document selection from web archives, text extraction from HTML, language identification for noisy texts, exact and near-deduplication, annotation with, among others, register labels, text quality estimates, and personally identifiable information; and final selection and filtering. We report on data quality probes through contrastive and analytical statistics, through manual inspection of samples for 24 languages, and through end-to-end evaluation of various language model architectures trained on this data. For multilingual LLM evaluation, we provide a comprehensive collection of benchmarks for nine European languages, with special emphasis on natively created tasks, mechanisms to mitigate prompt sensitivity, and refined normalization and aggregation of scores. Additionally, we train and evaluate a family of 57 monolingual encoder-decoder models, as well as a handful of monolingual GPT-like reference models. Besides the monolingual data and models, we also present a very large collection of parallel texts automatically mined from this data, together with a novel parallel corpus synthesized via machine translation.

**Link**: [arxiv](http://arxiv.org/abs/2511.01066v2),  [pdf](http://arxiv.org/pdf/2511.01066v2)

**Tags**: cs.CL 



### (Approximate) Matrix Multiplication via Convolutions
**Authors**: Yahel Uffenheimer, Omri Weinstein

**Updated**: 2025-11-05T13:05:52Z

**Summary**: We study the capability of the Fast Fourier Transform (FFT) to accelerate exact and approximate matrix multiplication without using Strassen-like divide-and-conquer. We present a simple exact algorithm running in $O(n^{2.89})$ time, which only sums a few convolutions (FFTs) in $\mathbb{Z}_{m}^{k}$, building on the work of Cohn, Kleinberg, Szegedy and Umans (2005). As a corollary, combining this algorithm with linear sketching breaks the longstanding linear speed-accuracy tradeoff for "combinatorial" approximate matrix multiplication (AMM, Pagh'13, Sarlos'06, Clarkson-Woodruff'13), achieving error $\frac{1}{r^{1.1}}\left\lVert \mathbf{A} \right\rVert_{F}^{2}\left\lVert \mathbf{B}\right\rVert_{F}^{2}$ in $O(rn^{2})$ time, using nothing but FFTs.   Motivated by the rich literature for approximating polynomials, our main contribution in this paper is extending the group-theoretic framework of Cohn and Umans (2003) to approximate matrix multiplication (AMM). Specifically, we introduce and study an approximate notion of the Triple Product Property, which in the abelian case is equivalent to finding a Sumset which minimizes (multi-)intersections with an arithmetic progression. We prove tight bounds on this quantity for abelian groups (yielding a simple and practical AMM algorithm via polynomial multiplication), and establish a weaker lower bound for non-abelian groups, extending a lemma of Gowers. Finally, we propose a concrete approach that uses low-degree approximation of multi-variate polynomials for AMM, which we believe will lead to practical, non-asymptotic AMM algorithms in real-world applications, most notably LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.22193v2),  [pdf](http://arxiv.org/pdf/2510.22193v2)

**Tags**: cs.DS cs.DM 



### CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field
**Authors**: Doria Bonzi, Alexandre Guiggi, Frédéric Béchet, Carlos Ramisch, Benoit Favre

**Updated**: 2025-11-05T13:02:06Z

**Summary**: Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.

**Link**: [arxiv](http://arxiv.org/abs/2511.03441v1),  [pdf](http://arxiv.org/pdf/2511.03441v1)

**Tags**: cs.CL cs.AI 



### Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof,   Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2,   ERC-8004, and Beyond
**Authors**: Botao 'Amber' Hu, Helena Rong

**Updated**: 2025-11-05T12:50:06Z

**Summary**: As the "agentic web" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 "Trustless Agents," yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.

**Link**: [arxiv](http://arxiv.org/abs/2511.03434v1),  [pdf](http://arxiv.org/pdf/2511.03434v1)

**Tags**: cs.HC cs.AI cs.MA cs.NI cs.SI 



### The moment is here: a generalised class of estimators for fuzzy   regression discontinuity designs
**Authors**: Stuart Lane

**Updated**: 2025-11-05T12:40:34Z

**Summary**: The standard fuzzy regression discontinuity (FRD) estimator is a ratio of differences of local polynomial estimators. I show that this estimator does not have finite moments of any order in finite samples, regardless of the choice of kernel function, bandwidth, or order of polynomial. This leads to an imprecise estimator with a heavy-tailed sampling distribution, and inaccurate inference with small sample sizes or when the discontinuity in the probability of treatment assignment at the cutoff is small. I present a generalised class of computationally simple FRD estimators, which contains a continuum of estimators with finite moments of all orders in finite samples, and nests both the standard FRD and sharp (SRD) estimators. The class is indexed by a single tuning parameter, and I provide simple values that lead to substantial improvements in median bias, median absolute deviation and root mean squared error. These new estimators remain very stable in small samples, or when the discontinuity in the probability of treatment assignment at the cutoff is small. Simple confidence intervals that have strong coverage and length properties in small samples are also developed. The improvements are seen across a wide range of models and using common bandwidth selection algorithms in extensive Monte Carlo simulations. The improved stability and performance of the estimators and confidence intervals is also demonstrated using data on class size effects on educational attainment.

**Link**: [arxiv](http://arxiv.org/abs/2511.03424v1),  [pdf](http://arxiv.org/pdf/2511.03424v1)

**Tags**: econ.EM 



### Light over Heavy: Automated Performance Requirements Quantification with   Linguistic Inducement
**Authors**: Shihai Wang, Tao Chen

**Updated**: 2025-11-05T12:38:11Z

**Summary**: Elicited performance requirements need to be quantified for compliance in different engineering tasks, e.g., configuration tuning and performance testing. Much existing work has relied on manual quantification, which is expensive and error-prone due to the imprecision. In this paper, we present LQPR, a highly efficient automatic approach for performance requirements quantification.LQPR relies on a new theoretical framework that converts quantification as a classification problem. Despite the prevalent applications of Large Language Models (LLMs) for requirement analytics, LQPR takes a different perspective to address the classification: we observed that performance requirements can exhibit strong patterns and are often short/concise, therefore we design a lightweight linguistically induced matching mechanism. We compare LQPR against nine state-of-the-art learning-based approaches over diverse datasets, demonstrating that it is ranked as the sole best for 75% or more cases with two orders less cost. Our work proves that, at least for performance requirement quantification, specialized methods can be more suitable than the general LLM-driven approaches.

**Link**: [arxiv](http://arxiv.org/abs/2511.03421v1),  [pdf](http://arxiv.org/pdf/2511.03421v1)

**Tags**: cs.SE cs.AI 



### Multi-layer dissolution exponential-family models for weighted signed   networks
**Authors**: Alberto Caimo, Isabella Gollini

**Updated**: 2025-11-05T12:36:16Z

**Summary**: Understanding the structure of weighted signed networks is essential for analysing social systems in which relationships vary both in sign and strength. Despite significant advances in statistical network analysis, there is still a lack of statistical models that can jointly and rigorously account for both the sign and strength of relationships in networks. We introduce a multi-layer dissolution exponential random graph modelling framework that jointly captures the signed and weighted processes, conditional on the observed interaction structure. The framework enables rigorous assessment of structural balance effects while fully accounting for edge weights. To enhance inference, we adopt a fully-probabilistic Bayesian hierarchical approach that partially pools information across layers, with parameters estimated via an adaptive approximate exchange algorithm. We demonstrate the flexibility and explanatory power of the proposed methodology by applying it to bill sponsorship data from the 108th US Senate, revealing complex patterns of signed and weighted interactions and structural balance effects that traditional approaches are unable to capture.

**Link**: [arxiv](http://arxiv.org/abs/2511.03420v1),  [pdf](http://arxiv.org/pdf/2511.03420v1)

**Tags**: stat.ME 



### Knowledge-Augmented Question Error Correction for Chinese Question   Answer System with QuestionRAG
**Authors**: Longpeng Qiu, Ting Li, Shuai Mao, Nan Yang, Xiaohui Yan

**Updated**: 2025-11-05T12:24:20Z

**Summary**: Input errors in question-answering (QA) systems often lead to incorrect responses. Large language models (LLMs) struggle with this task, frequently failing to interpret user intent (misinterpretation) or unnecessarily altering the original question's structure (over-correction). We propose QuestionRAG, a framework that tackles these problems. To address misinterpretation, it enriches the input with external knowledge (e.g., search results, related entities). To prevent over-correction, it uses reinforcement learning (RL) to align the model's objective with precise correction, not just paraphrasing. Our results demonstrate that knowledge augmentation is critical for understanding faulty questions. Furthermore, RL-based alignment proves significantly more effective than traditional supervised fine-tuning (SFT), boosting the model's ability to follow instructions and generalize. By integrating these two strategies, QuestionRAG unlocks the full potential of LLMs for the question correction task.

**Link**: [arxiv](http://arxiv.org/abs/2511.03410v1),  [pdf](http://arxiv.org/pdf/2511.03410v1)

**Tags**: cs.CL 



### Efficient Reasoning via Thought-Training and Thought-Free Inference
**Authors**: Canhui Wu, Qiong Cao, Chao Xue, Wei Xi, Xiaodong He

**Updated**: 2025-11-05T12:20:45Z

**Summary**: Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily compress verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but still rely on explicit reasoning during inference. In this work, we introduce \textbf{3TF} (\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree inference), a framework for efficient reasoning that takes a Short-to-Long perspective. We first train a hybrid model that can operate in both reasoning and non-reasoning modes, and then further train it on CoT-annotated data to internalize structured reasoning, while enforcing concise, thought-free outputs at inference time using the no-reasoning mode. Unlike compression-based approaches, 3TF improves the reasoning quality of non-reasoning outputs, enabling models to perform rich internal reasoning implicitly while keeping external outputs short. Empirically, 3TF-trained models obtain large improvements on reasoning benchmarks under thought-free inference, demonstrating that high quality reasoning can be learned and executed implicitly without explicit step-by-step generation.

**Link**: [arxiv](http://arxiv.org/abs/2511.03408v1),  [pdf](http://arxiv.org/pdf/2511.03408v1)

**Tags**: cs.CL I.2.7 



### Towards Realistic Project-Level Code Generation via Multi-Agent   Collaboration and Semantic Architecture Modeling
**Authors**: Qianhui Zhao, Li Zhang, Fang Liu, Junhang Cheng, Chengru Wu, Junchen Ai, Qiaoyuanhe Meng, Lichen Zhang, Xiaoli Lian, Shubin Song, Yuanping Guo

**Updated**: 2025-11-05T12:12:35Z

**Summary**: In recent years, Large Language Models (LLMs) have achieved remarkable progress in automated code generation. In real-world software engineering, the growing demand for rapid iteration and continuous delivery underscores the importance of project-level code generation, where LLMs are expected to generate complete software projects directly from complex user requirements. Although existing studies have made initial explorations, they still face key limitations, including unrealistic datasets and unreliable evaluation metrics that fail to reflect real-world complexity, the semantic gap between human-written requirements and machine-interpretable structures, and difficulties in managing hierarchical dependencies and maintaining quality throughout the generation process. To address these limitations, we first introduce CodeProjectEval, a project-level code generation dataset built from 18 real-world repositories with 12.7 files and 2,388.6 lines of code per task on average, supplemented with documentation and executable test cases for automatic evaluation. We further propose ProjectGen, a multi-agent framework that decomposes projects into architecture design, skeleton generation, and code filling stages with iterative refinement and memory-based context management. Within this framework, we introduce the Semantic Software Architecture Tree (SSAT), a structured and semantically rich representation that effectively bridges user requirements and source code implementation. Experiments show that ProjectGen achieves state-of-the-art performance, passing 52/124 test cases on the small-scale project-level code generation dataset DevBench, a 57% improvement over the baseline approaches, and 310 test cases on CodeProjectEval, representing an improvement of roughly tenfold compared to the baselines.

**Link**: [arxiv](http://arxiv.org/abs/2511.03404v1),  [pdf](http://arxiv.org/pdf/2511.03404v1)

**Tags**: cs.SE 



### GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained   Robot Policy Enhancement
**Authors**: Minquan Gao, Xinyi Li, Qing Yan, Xiaojian Sun, Xiaopan Zhang, Chien-Ming Huang, Jiachen Li

**Updated**: 2025-11-05T12:08:05Z

**Summary**: Pre-trained robot policies serve as the foundation of many validated robotic systems, which encapsulate extensive embodied knowledge. However, they often lack the semantic awareness characteristic of foundation models, and replacing them entirely is impractical in many situations due to high costs and the loss of accumulated knowledge. To address this gap, we introduce GUIDES, a lightweight framework that augments pre-trained policies with semantic guidance from foundation models without requiring architectural redesign. GUIDES employs a fine-tuned vision-language model (Instructor) to generate contextual instructions, which are encoded by an auxiliary module into guidance embeddings. These embeddings are injected into the policy's latent space, allowing the legacy model to adapt to this new semantic input through brief, targeted fine-tuning. For inference-time robustness, a large language model-based Reflector monitors the Instructor's confidence and, when confidence is low, initiates a reasoning loop that analyzes execution history, retrieves relevant examples, and augments the VLM's context to refine subsequent actions. Extensive validation in the RoboCasa simulation environment across diverse policy architectures shows consistent and substantial improvements in task success rates. Real-world deployment on a UR5 robot further demonstrates that GUIDES enhances motion precision for critical sub-tasks such as grasping. Overall, GUIDES offers a practical and resource-efficient pathway to upgrade, rather than replace, validated robot policies.

**Link**: [arxiv](http://arxiv.org/abs/2511.03400v1),  [pdf](http://arxiv.org/pdf/2511.03400v1)

**Tags**: cs.RO 



### Bayesian Causal Effect Estimation for Categorical Data using Staged Tree   Models
**Authors**: Andrea Cremaschi, Manuele Leonelli, Gherardo Varando

**Updated**: 2025-11-05T12:07:55Z

**Summary**: We propose a fully Bayesian approach for causal inference with multivariate categorical data based on staged tree models, a class of probabilistic graphical models capable of representing asymmetric and context-specific dependencies. To account for uncertainty in both structure and parameters, we introduce a flexible family of prior distributions over staged trees. These include product partition models to encourage parsimony, a novel distance-based prior to promote interpretable dependence patterns, and an extension that incorporates continuous covariates into the learning process. Posterior inference is achieved via a tailored Markov Chain Monte Carlo algorithm with split-and-merge moves, yielding posterior samples of staged trees from which average treatment effects and uncertainty measures are derived. Posterior summaries and uncertainty measures are obtained via techniques from the Bayesian nonparametrics literature. Two case studies on electronic fetal monitoring and cesarean delivery and on anthracycline therapy and cardiac dysfunction in breast cancer illustrate the methods.

**Link**: [arxiv](http://arxiv.org/abs/2511.03399v1),  [pdf](http://arxiv.org/pdf/2511.03399v1)

**Tags**: stat.ME 



### The subtle statistics of the distance ladder: On the distance prior and   selection effects
**Authors**: Harry Desmond, Richard Stiskalek, Jose Antonio Najera, Indranil Banik

**Updated**: 2025-11-05T11:55:20Z

**Summary**: Statistical methodology is rarely considered significant in distance ladder studies or a potential contributor to the Hubble tension. We suggest it should be, highlighting two appreciable issues. First, astronomical distances are inferred latent parameters, requiring a prior. We show that the common assumption of (perhaps implicit) uniform priors on distance moduli biases distances low due to objects being intrinsically uniformly distributed in volume. At fixed measured redshifts, this biases the Hubble constant high. Second, selection effects introduce additional factors in the posterior. These typically counteract the effect of the volume prior to some extent, but depend significantly on the nature of the selection. Typical assumptions place $H_0$ at the top of the plausible range, corresponding to a redshift-selected sample. After a detailed analytic and mock-based study of these effects, we apply them to the CosmicFlows-4 sample, where introducing the distance prior causes an approximately 12~per cent increase in distances and $>8$ km/s/Mpc (55 statistical $\sigma$) decrease in the Hubble constant for the case of volume or magnitude selection. Redshift selection would fully undo this shift and is the more likely scenario, as a phenomenological model shows. We also investigate the SH0ES sample, where the volume-prior effect is modest ($1.6\sigma$) and is likely already accounted for within the SH0ES pipeline. Our work highlights the crucial need to model both the distance prior and selection accurately for robust distance ladders and derived parameters. The latter requires samples with known, homogeneous selection criteria, which should be prioritised in future surveys.

**Link**: [arxiv](http://arxiv.org/abs/2511.03394v1),  [pdf](http://arxiv.org/pdf/2511.03394v1)

**Tags**: astro-ph.CO astro-ph.GA 



### Distilling LLM Agent into Small Models with Retrieval and Code Tools
**Authors**: Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang

**Updated**: 2025-11-05T11:42:56Z

**Summary**: Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.

**Link**: [arxiv](http://arxiv.org/abs/2505.17612v2),  [pdf](http://arxiv.org/pdf/2505.17612v2)

**Tags**: cs.CL cs.AI 



### An Empirical Bayes approach to ARX Estimation
**Authors**: Timofei Leahu, Giorgio Picci

**Updated**: 2025-11-05T11:40:58Z

**Summary**: Empirical Bayes inference is based on estimation of the parameters of an a priori distribution from the observed data. The estimation technique of the parameters of the prior, called hyperparameters, is based on the marginal distribution obtained by integrating the joint density of the model with respect to the prior. This is a key step which needs to be properly adapted to the problem at hand. In this paper we study Empirical Bayes inference of linear autoregressive models with inputs (ARX models) for time series and compare the performance of the marginal parametric estimator with that a full Empirical Bayesian analysis based on the estimated prior. Such a comparison, can only make sense for a (realistic) finite data length. In this setting, we propose a new estimation technique of the hyperparameters by a sequential Bayes procedure which is essentially a backward Kalman filter. It turns out that for finite data length the marginal Bayes tends to behave slightly better than the full Empirical Bayesian parameter estimator and so also in the case of slowly varying random parameters.

**Link**: [arxiv](http://arxiv.org/abs/2505.13384v2),  [pdf](http://arxiv.org/pdf/2505.13384v2)

**Tags**: eess.SY cs.SY 



### Learning noisy tissue dynamics across time scales
**Authors**: Ming Han, John Devany, Michel Fruchart, Margaret L. Gardel, Vincenzo Vitelli

**Updated**: 2025-11-05T11:39:05Z

**Summary**: Tissue dynamics play a crucial role in biological processes ranging from inflammation to morphogenesis. However, these noisy multicellular dynamics are notoriously hard to predict. Here, we introduce a biomimetic machine learning framework capable of inferring noisy multicellular dynamics directly from experimental movies. This generative model combines graph neural networks, normalizing flows and WaveNet algorithms to represent tissues as neural stochastic differential equations where cells are edges of an evolving graph. Cell interactions are encoded in a dual signaling graph capable of handling signaling cascades. The dual graph architecture of our neural networks reflects the architecture of the underlying biological tissues, substantially reducing the amount of data needed for training, compared to convolutional or fully-connected neural networks. Taking epithelial tissue experiments as a case study, we show that our model not only captures stochastic cell motion but also predicts the evolution of cell states in their division cycle. Finally, we demonstrate that our method can accurately generate the experimental dynamics of developmental systems, such as the fly wing, and cell signaling processes mediated by stochastic ERK waves, paving the way for its use as a digital twin in bioengineering and clinical contexts.

**Link**: [arxiv](http://arxiv.org/abs/2510.19090v2),  [pdf](http://arxiv.org/pdf/2510.19090v2)

**Tags**: cond-mat.soft cs.LG physics.bio-ph q-bio.QM 



### A Digital Twin of Evaporative Thermo-Fluidic Process in Fixation Unit of   DoD Inkjet Printers
**Authors**: Samarth Toolhally, Joeri Roelofs, Siep Weiland, Amritam Das

**Updated**: 2025-11-05T11:35:00Z

**Summary**: In inkjet printing, optimal paper moisture is crucial for print quality, achieved through hot-air impingement in the fixation unit. This paper presents a modular digital twin of the fixation unit, modeling the thermo-fluidic drying process and monitoring its spatio-temporal performance. The novel approach formulates the digital twin as an infinite-dimensional state estimator that infers fixation states from limited sensor data, while remaining robust to disturbances. Modularity is achieved through a graph-theoretic model, where each node represents thermo-fluidic dynamics in different sections of the fixation unit. Evaporation is modeled as a nonlinear boundary effect coupled with node dynamics via Linear Fractional Representation. Using the Partial Integral Equation (PIE) framework, we develop a unified approach for stability, input-output analysis, simulation, and rapid prototyping, validated with operational data from a commercial printer. An $\mathcal{H}_{\infty}$-optimal Luenberger state estimator is then synthesized to estimate thermal states from available sensor data, enabling real-time monitoring of spatio-temporal thermal effects on paper sheets.

**Link**: [arxiv](http://arxiv.org/abs/2511.03379v1),  [pdf](http://arxiv.org/pdf/2511.03379v1)

**Tags**: eess.SY cs.SY 



### SpatialLM: Training Large Language Models for Structured Indoor Modeling
**Authors**: Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, Zihan Zhou

**Updated**: 2025-11-05T11:34:39Z

**Summary**: SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.

**Link**: [arxiv](http://arxiv.org/abs/2506.07491v2),  [pdf](http://arxiv.org/pdf/2506.07491v2)

**Tags**: cs.CV 



### Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in   Brain Gliomas
**Authors**: Syed Muqeem Mahmood, Hassan Mohy-ud-Din

**Updated**: 2025-11-05T11:31:08Z

**Summary**: We present a framework that combines Large Language Models with computational image analytics for non-invasive, zero-shot prediction of IDH mutation status in brain gliomas. For each subject, coregistered multi-parametric MRI scans and multi-class tumor segmentation maps were processed to extract interpretable semantic (visual) attributes and quantitative features, serialized in a standardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning. We evaluated this framework on six publicly available datasets (N = 1427) and results showcased high accuracy and balanced classification performance across heterogeneous cohorts, even in the absence of manual annotations. GPT 5 outperformed GPT 4o in context-driven phenotype interpretation. Volumetric features emerged as the most important predictors, supplemented by subtype-specific imaging markers and clinical information. Our results demonstrate the potential of integrating LLM-based reasoning with computational image analytics for precise, non-invasive tumor genotyping, advancing diagnostic strategies in neuro-oncology. The code is available at https://github.com/ATPLab-LUMS/CIM-LLM.

**Link**: [arxiv](http://arxiv.org/abs/2511.03376v1),  [pdf](http://arxiv.org/pdf/2511.03376v1)

**Tags**: eess.IV cs.AI q-bio.QM 



### Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven   Graph Partitioning
**Authors**: Ruiyi Yang, Hao Xue, Imran Razzak, Shirui Pan, Hakim Hacid, Flora D. Salim

**Updated**: 2025-11-05T11:26:59Z

**Summary**: Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.13994v2),  [pdf](http://arxiv.org/pdf/2505.13994v2)

**Tags**: cs.AI cs.IR cs.MA 



### LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced   Logical Reasoning
**Authors**: Shenghao Li

**Updated**: 2025-11-05T11:26:38Z

**Summary**: For complex logical data augmentation, heavy reliance on human annotation is costly, whereas direct generation with large language models yields uninterpretable and logically homogeneous examples. To address this, we present LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to propositional expressions, a compact rule library is compiled, and a bounded state-space search systematically discovers valid formulas that are then verbalized back into natural-language questions, ensuring both diversity and logical rigor under propositional logic. Experiments on ReClor and LogiQA show significant improvements in the logical-reasoning accuracy of pretrained models, confirming the effectiveness of LFC-DA for LLM-guided logical data augmentation.

**Link**: [arxiv](http://arxiv.org/abs/2511.03372v1),  [pdf](http://arxiv.org/pdf/2511.03372v1)

**Tags**: cs.CL I.2.7; I.2.6; F.4.1 



### Sundial: A Family of Highly Capable Time Series Foundation Models
**Authors**: Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, Mingsheng Long

**Updated**: 2025-11-05T11:26:10Z

**Summary**: We introduce Sundial, a family of native, flexible, and scalable time series foundation models. To predict the next-patch's distribution, we propose a TimeFlow Loss based on flow-matching, which facilitates native pre-training of Transformers on continuous-valued time series without discrete tokenization. Conditioned on arbitrary-length time series, our models are pre-trained without specifying any prior distribution and can generate multiple probable predictions, achieving more flexibility in representation learning than using parametric densities. Towards time series foundation models, we leverage minimal but crucial adaptations of Transformers and curate TimeBench with one trillion time points, comprising mostly real-world datasets and synthetic data. By mitigating mode collapse via TimeFlow Loss, we pre-train a family of Sundial models on TimeBench, which achieve unprecedented model capacity and generalization performance. In addition to excellent scalability, Sundial achieves state-of-the-art results on both point and probabilistic forecasting benchmarks with a just-in-time inference speed, i.e., making zero-shot predictions within a few milliseconds. We believe that Sundial's pioneering generative forecasting capability can improve model reliability in real-world decision-making. Code is available at: https://github.com/thuml/Sundial.

**Link**: [arxiv](http://arxiv.org/abs/2502.00816v3),  [pdf](http://arxiv.org/pdf/2502.00816v3)

**Tags**: cs.LG 



### DiCoFlex: Model-agnostic diverse counterfactuals with flexible control
**Authors**: Oleksii Furman, Ulvi Movsum-zada, Patryk Marszalek, Maciej Zięba, Marek Śmieja

**Updated**: 2025-11-05T11:26:02Z

**Summary**: Counterfactual explanations play a pivotal role in explainable artificial intelligence (XAI) by offering intuitive, human-understandable alternatives that elucidate machine learning model decisions. Despite their significance, existing methods for generating counterfactuals often require constant access to the predictive model, involve computationally intensive optimization for each instance and lack the flexibility to adapt to new user-defined constraints without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic, conditional generative framework that produces multiple diverse counterfactuals in a single forward pass. Leveraging conditional normalizing flows trained solely on labeled data, DiCoFlex addresses key limitations by enabling real-time user-driven customization of constraints such as sparsity and actionability at inference time. Extensive experiments on standard benchmark datasets show that DiCoFlex outperforms existing methods in terms of validity, diversity, proximity, and constraint adherence, making it a practical and scalable solution for counterfactual generation in sensitive decision-making domains.

**Link**: [arxiv](http://arxiv.org/abs/2505.23700v2),  [pdf](http://arxiv.org/pdf/2505.23700v2)

**Tags**: cs.LG 



### EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models   for Edge-Deployable Credit Negotiation
**Authors**: Yunbo Long, Yuhan Liu, Alexandra Brintrup

**Updated**: 2025-11-05T11:25:07Z

**Summary**: The deployment of large language models (LLMs) in automated negotiation has set a high performance benchmark, but their computational cost and data privacy requirements render them unsuitable for many privacy-sensitive, on-device applications such as mobile assistants, embodied AI agents or private client interactions. While small language models (SLMs) offer a practical alternative, they suffer from a significant performance gap compared to LLMs in playing emotionally charged complex personas, especially for credit negotiation. This paper introduces EQ-Negotiator, a novel framework that bridges this capability gap using emotional personas. Its core is a reasoning system that integrates game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional states online, without pre-training. This allows EQ-Negotiator to equip SLMs with the strategic intelligence to counter manipulation while de-escalating conflict and upholding ethical standards. Through extensive agent-to-agent simulations across diverse credit negotiation scenarios, including adversarial debtor strategies like cheating, threatening, and playing the victim, we show that a 7B parameter language model with EQ-Negotiator achieves better debt recovery and negotiation efficiency than baseline LLMs more than 10 times its size. This work advances persona modeling from descriptive character profiles to dynamic emotional architectures that operate within privacy constraints. Besides, this paper establishes that strategic emotional intelligence, not raw model scale, is the critical factor for success in automated negotiation, paving the way for effective, ethical, and privacy-preserving AI negotiators that can operate on the edge.

**Link**: [arxiv](http://arxiv.org/abs/2511.03370v1),  [pdf](http://arxiv.org/pdf/2511.03370v1)

**Tags**: cs.CL 



### Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative   Knowledge Retrieval
**Authors**: Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim

**Updated**: 2025-11-05T11:24:50Z

**Summary**: Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.14234v4),  [pdf](http://arxiv.org/pdf/2503.14234v4)

**Tags**: cs.AI cs.MA 



## Keyword: LLM Deployment 
 ### Outbidding and Outbluffing Elite Humans: Mastering Liar's Poker via   Self-Play and Reinforcement Learning
**Authors**: Richard Dewey, Janos Botyanszki, Ciamac C. Moallemi, Andrew T. Zheng

**Updated**: 2025-11-05T18:58:18Z

**Summary**: AI researchers have long focused on poker-like games as a testbed for environments characterized by multi-player dynamics, imperfect information, and reasoning under uncertainty. While recent breakthroughs have matched elite human play at no-limit Texas hold'em, the multi-player dynamics are subdued: most hands converge quickly with only two players engaged through multiple rounds of bidding. In this paper, we present Solly, the first AI agent to achieve elite human play in reduced-format Liar's Poker, a game characterized by extensive multi-player engagement. We trained Solly using self-play with a model-free, actor-critic, deep reinforcement learning algorithm. Solly played at an elite human level as measured by win rate (won over 50% of hands) and equity (money won) in heads-up and multi-player Liar's Poker. Solly also outperformed large language models (LLMs), including those with reasoning abilities, on the same metrics. Solly developed novel bidding strategies, randomized play effectively, and was not easily exploitable by world-class human players.

**Link**: [arxiv](http://arxiv.org/abs/2511.03724v1),  [pdf](http://arxiv.org/pdf/2511.03724v1)

**Tags**: cs.AI cs.MA 



### Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist   Annotation Scheme for MapTask
**Authors**: Nan Li, Albert Gatt, Massimo Poesio

**Updated**: 2025-11-05T18:52:28Z

**Summary**: Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue.

**Link**: [arxiv](http://arxiv.org/abs/2511.03718v1),  [pdf](http://arxiv.org/pdf/2511.03718v1)

**Tags**: cs.CL cs.AI 



### GDS Agent for Graph Algorithmic Reasoning
**Authors**: Borun Shi, Ioannis Panagiotas

**Updated**: 2025-11-05T18:39:38Z

**Summary**: Large language models (LLMs) have shown remarkable multimodal information processing and reasoning ability. When equipped with tools through function calling and enhanced with retrieval-augmented techniques, compound LLM-based systems can access closed data sources and answer questions about them. However, they still struggle to process and reason over large-scale graph-structure data. We introduce the GDS (Graph Data Science) agent in this technical report. The GDS agent introduces a comprehensive set of graph algorithms as tools, together with preprocessing (retrieval) and postprocessing of algorithm results, in a model context protocol (MCP) server. The server can be used with any modern LLM out-of-the-box. GDS agent allows users to ask any question that implicitly and intrinsically requires graph algorithmic reasoning about their data, and quickly obtain accurate and grounded answers. We introduce new benchmarks that evaluate intermediate tool calls as well as final responses. The results indicate that GDS agent is able to solve a wide spectrum of graph tasks. We also provide detailed case studies for more open-ended tasks and study scenarios where the agent struggles. Finally, we discuss the remaining challenges and the future roadmap.

**Link**: [arxiv](http://arxiv.org/abs/2508.20637v2),  [pdf](http://arxiv.org/pdf/2508.20637v2)

**Tags**: cs.LG cs.AI cs.CL 



### LLM-enhanced Air Quality Monitoring Interface via Model Context Protocol
**Authors**: Yu-Erh Pan, Ayesha Siddika Nipu

**Updated**: 2025-11-05T18:38:02Z

**Summary**: Air quality monitoring is central to environmental sustainability and public health, yet traditional systems remain difficult for non-expert users to interpret due to complex visualizations, limited interactivity, and high deployment costs. Recent advances in Large Language Models (LLMs) offer new opportunities to make sensor data more accessible, but their tendency to produce hallucinations limits reliability in safety-critical domains. To address these challenges, we present an LLM-enhanced Air Monitoring Interface (AMI) that integrates real-time sensor data with a conversational interface via the Model Context Protocol (MCP). Our system grounds LLM outputs in live environmental data, enabling accurate, context-aware responses while reducing hallucination risk. The architecture combines a Django-based backend, a responsive user dashboard, and a secure MCP server that exposes system functions as discoverable tools, allowing the LLM to act as an active operator rather than a passive responder. Expert evaluation demonstrated high factual accuracy (4.78), completeness (4.82), and minimal hallucinations (4.84), on a scale of 5, supported by inter-rater reliability analysis. These results highlight the potential of combining LLMs with standardized tool protocols to create reliable, secure, and user-friendly interfaces for real-time environmental monitoring.

**Link**: [arxiv](http://arxiv.org/abs/2511.03706v1),  [pdf](http://arxiv.org/pdf/2511.03706v1)

**Tags**: cs.ET 



### Do Androids Dream of Unseen Puppeteers? Probing for a Conspiracy Mindset   in Large Language Models
**Authors**: Francesco Corso, Francesco Pierri, Gianmarco De Francisci Morales

**Updated**: 2025-11-05T18:28:28Z

**Summary**: In this paper, we investigate whether Large Language Models (LLMs) exhibit conspiratorial tendencies, whether they display sociodemographic biases in this domain, and how easily they can be conditioned into adopting conspiratorial perspectives. Conspiracy beliefs play a central role in the spread of misinformation and in shaping distrust toward institutions, making them a critical testbed for evaluating the social fidelity of LLMs. LLMs are increasingly used as proxies for studying human behavior, yet little is known about whether they reproduce higher-order psychological constructs such as a conspiratorial mindset. To bridge this research gap, we administer validated psychometric surveys measuring conspiracy mindset to multiple models under different prompting and conditioning strategies. Our findings reveal that LLMs show partial agreement with elements of conspiracy belief, and conditioning with socio-demographic attributes produces uneven effects, exposing latent demographic biases. Moreover, targeted prompts can easily shift model responses toward conspiratorial directions, underscoring both the susceptibility of LLMs to manipulation and the potential risks of their deployment in sensitive contexts. These results highlight the importance of critically evaluating the psychological dimensions embedded in LLMs, both to advance computational social science and to inform possible mitigation strategies against harmful uses.

**Link**: [arxiv](http://arxiv.org/abs/2511.03699v1),  [pdf](http://arxiv.org/pdf/2511.03699v1)

**Tags**: cs.CL cs.CY 



### AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and   Sample-Efficient Analog Circuit Sizing
**Authors**: Mohsen Ahmadzadeh, Kaichang Chen, Georges Gielen

**Updated**: 2025-11-05T18:24:01Z

**Summary**: Analog/mixed-signal circuits are key for interfacing electronics with the physical world. Their design, however, remains a largely handcrafted process, resulting in long and error-prone design cycles. While the recent rise of AI-based reinforcement learning and generative AI has created new techniques to automate this task, the need for many time-consuming simulations is a critical bottleneck hindering the overall efficiency. Furthermore, the lack of explainability of the resulting design solutions hampers widespread adoption of the tools. To address these issues, a novel agentic AI framework for sample-efficient and explainable analog circuit sizing is presented. It employs a multi-agent workflow where specialized Large Language Model (LLM)-based agents collaborate to interpret the circuit topology, to understand the design goals, and to iteratively refine the circuit's design parameters towards the target goals with human-interpretable reasoning. The adaptive simulation strategy creates an intelligent control that yields a high sample efficiency. The AnaFlow framework is demonstrated for two circuits of varying complexity and is able to complete the sizing task fully automatically, differently from pure Bayesian optimization and reinforcement learning approaches. The system learns from its optimization history to avoid past mistakes and to accelerate convergence. The inherent explainability makes this a powerful tool for analog design space exploration and a new paradigm in analog EDA, where AI agents serve as transparent design assistants.

**Link**: [arxiv](http://arxiv.org/abs/2511.03697v1),  [pdf](http://arxiv.org/pdf/2511.03697v1)

**Tags**: cs.LG cs.AI cs.AR 



### Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online   RL
**Authors**: Lipeng Zu, Hansong Zhou, Xiaonan Zhang

**Updated**: 2025-11-05T18:20:23Z

**Summary**: Offline reinforcement learning (RL) enables training from fixed data without online interaction, but policies learned offline often struggle when deployed in dynamic environments due to distributional shift and unreliable value estimates on unseen state-action pairs. We introduce Behavior-Adaptive Q-Learning (BAQ), a framework designed to enable a smooth and reliable transition from offline to online RL. The key idea is to leverage an implicit behavioral model derived from offline data to provide a behavior-consistency signal during online fine-tuning. BAQ incorporates a dual-objective loss that (i) aligns the online policy toward the offline behavior when uncertainty is high, and (ii) gradually relaxes this constraint as more confident online experience is accumulated. This adaptive mechanism reduces error propagation from out-of-distribution estimates, stabilizes early online updates, and accelerates adaptation to new scenarios. Across standard benchmarks, BAQ consistently outperforms prior offline-to-online RL approaches, achieving faster recovery, improved robustness, and higher overall performance. Our results demonstrate that implicit behavior adaptation is a principled and practical solution for reliable real-world policy deployment.

**Link**: [arxiv](http://arxiv.org/abs/2511.03695v1),  [pdf](http://arxiv.org/pdf/2511.03695v1)

**Tags**: cs.LG 



### Colorectal Cancer Histopathological Grading using Multi-Scale Federated   Learning
**Authors**: Md Ahasanul Arafath, Abhijit Kumar Ghosh, Md Rony Ahmed, Sabrin Afroz, Minhazul Hosen, Md Hasan Moon, Md Tanzim Reza, Md Ashad Alam

**Updated**: 2025-11-05T18:18:09Z

**Summary**: Colorectal cancer (CRC) grading is a critical prognostic factor but remains hampered by inter-observer variability and the privacy constraints of multi-institutional data sharing. While deep learning offers a path to automation, centralized training models conflict with data governance regulations and neglect the diagnostic importance of multi-scale analysis. In this work, we propose a scalable, privacy-preserving federated learning (FL) framework for CRC histopathological grading that integrates multi-scale feature learning within a distributed training paradigm. Our approach employs a dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear detail and broader tissue-level context. This architecture is integrated into a robust FL system stabilized using FedProx to mitigate client drift across heterogeneous data distributions from multiple hospitals. Extensive evaluation on the CRC-HGD dataset demonstrates that our framework achieves an overall accuracy of 83.5%, outperforming a comparable centralized model (81.6%). Crucially, the system excels in identifying the most aggressive Grade III tumors with a high recall of 87.5%, a key clinical priority to prevent dangerous false negatives. Performance further improves with higher magnification, reaching 88.0% accuracy at 40x. These results validate that our federated multi-scale approach not only preserves patient privacy but also enhances model performance and generalization. The proposed modular pipeline, with built-in preprocessing, checkpointing, and error handling, establishes a foundational step toward deployable, privacy-aware clinical AI for digital pathology.

**Link**: [arxiv](http://arxiv.org/abs/2511.03693v1),  [pdf](http://arxiv.org/pdf/2511.03693v1)

**Tags**: stat.ML cs.LG 



### The OpenHands Software Agent SDK: A Composable and Extensible Foundation   for Production Agents
**Authors**: Xingyao Wang, Simon Rosenberg, Juan Michelini, Calvin Smith, Hoang Tran, Engel Nyst, Rohit Malhotra, Xuhui Zhou, Valerie Chen, Robert Brennan, Graham Neubig

**Updated**: 2025-11-05T18:16:44Z

**Summary**: Agents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. In this paper, we present the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, we design a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation for prototyping, unlocking new classes of custom applications, and reliably deploying agents at scale.

**Link**: [arxiv](http://arxiv.org/abs/2511.03690v1),  [pdf](http://arxiv.org/pdf/2511.03690v1)

**Tags**: cs.SE cs.AI 



### FREESH: Fair, Resource- and Energy-Efficient Scheduling for LLM Serving   on Heterogeneous GPUs
**Authors**: Xuan He, Zequan Fang, Jinzhao Lian, Danny H. K. Tsang, Baosen Zhang, Yize Chen

**Updated**: 2025-11-05T18:15:57Z

**Summary**: The ever-increasing computation and energy demand for LLM and AI agents call for holistic and efficient optimization of LLM serving systems. In practice, heterogeneous GPU clusters can be deployed in a geographically distributed manner, while LLM load also observes diversity in terms of both query traffic and serving patterns. LLM queries running on advanced GPUs during a high-emission hour at one location can lead to significantly higher carbon footprints versus same queries running on mid-level GPUs at a low-emission time and location. By observing LLM serving requirements and leveraging spatiotemporal computation flexibility, we consider the joint routing and scheduling problem, and propose FREESH to cooperatively run a group of data centers while minimizing user-specified carbon or energy objectives. FREESH identifies the optimal configurations of balanced load serving by matching distinct GPU instance's power-throughput characteristics with predictable LLM query length and workloads. To ensure both latency and fairness requirements, FREESH identifies optimized parallelism and query routing schedules together with dynamic GPU frequency scaling for power saving, and Least-Laxity-First (LLF) serving strategy for query scheduling. During the 1-hour serving on production workloads, FREESH reduces energy by 28.6% and emissions by 45.45% together with improvements in SLO attainment and fairness.

**Link**: [arxiv](http://arxiv.org/abs/2511.00807v2),  [pdf](http://arxiv.org/pdf/2511.00807v2)

**Tags**: cs.DC 



### LLM Query Scheduling with Prefix Reuse and Latency Constraints
**Authors**: Gregory Dexter, Shao Tang, Ata Fatahi Baarzi, Qingquan Song, Tejas Dharamsi, Aman Gupta

**Updated**: 2025-11-05T18:12:33Z

**Summary**: The efficient deployment of large language models (LLMs) in online settings requires optimizing inference performance under stringent latency constraints, particularly the time-to-first-token (TTFT) and time-per-output-token (TPOT). This paper focuses on the query scheduling problem for LLM inference with prefix reuse, a technique that leverages shared prefixes across queries to reduce computational overhead. Our work reveals previously unknown limitations of the existing first-come-first-serve (FCFS) and longest-prefix-match (LPM) scheduling strategies with respect to satisfying latency constraints. We present a formal theoretical framework for LLM query scheduling under RadixAttention, a prefix reuse mechanism that stores and reuses intermediate representations in a radix tree structure. Our analysis establishes the NP-hardness of the scheduling problem with prefix reuse under TTFT constraints and proposes a novel scheduling algorithm, $k$-LPM, which generalizes existing methods by balancing prefix reuse and fairness in query processing. Theoretical guarantees demonstrate that $k$-LPM achieves improved TTFT performance under realistic traffic patterns captured by a data generative model. Empirical evaluations in a realistic serving setting validates our findings, showing significant reductions in P99 TTFT compared to baseline methods.

**Link**: [arxiv](http://arxiv.org/abs/2502.04677v2),  [pdf](http://arxiv.org/pdf/2502.04677v2)

**Tags**: cs.DS 



### PDE-SHARP: PDE Solver Hybrids through Analysis and Refinement Passes
**Authors**: Shaghayegh Fazliani, Madeleine Udell

**Updated**: 2025-11-05T17:58:32Z

**Summary**: Current LLM-driven approaches using test-time computing to generate PDE solvers execute a large number of solver samples to identify high-accuracy solvers. These paradigms are especially costly for complex PDEs requiring substantial computational resources for numerical evaluation. We introduce PDE-SHARP, a framework to reduce computational costs by replacing expensive scientific computation by cheaper LLM inference that achieves superior solver accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three stages: (1) Analysis: mathematical chain-of-thought analysis including PDE classification, solution type detection, and stability analysis; (2) Genesis: solver generation based on mathematical insights from the previous stage; and (3) Synthesis: collaborative selection-hybridization tournaments in which LLM judges iteratively refine implementations through flexible performance feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13 solver evaluations on average compared to 30+ for baseline methods, improving accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates robust performance across LLM architectures, from general-purpose to specialized reasoning models.

**Link**: [arxiv](http://arxiv.org/abs/2511.00183v2),  [pdf](http://arxiv.org/pdf/2511.00183v2)

**Tags**: cs.LG 



### Whisper Leak: a side-channel attack on Large Language Models
**Authors**: Geoff McDonald, Jonathan Bar Or

**Updated**: 2025-11-05T17:47:46Z

**Summary**: Large Language Models (LLMs) are increasingly deployed in sensitive domains including healthcare, legal services, and confidential communications, where privacy is paramount. This paper introduces Whisper Leak, a side-channel attack that infers user prompt topics from encrypted LLM traffic by analyzing packet size and timing patterns in streaming responses. Despite TLS encryption protecting content, these metadata patterns leak sufficient information to enable topic classification. We demonstrate the attack across 28 popular LLMs from major providers, achieving near-perfect classification (often >98% AUPRC) and high precision even at extreme class imbalance (10,000:1 noise-to-target ratio). For many models, we achieve 100% precision in identifying sensitive topics like "money laundering" while recovering 5-20% of target conversations. This industry-wide vulnerability poses significant risks for users under network surveillance by ISPs, governments, or local adversaries. We evaluate three mitigation strategies - random padding, token batching, and packet injection - finding that while each reduces attack effectiveness, none provides complete protection. Through responsible disclosure, we have collaborated with providers to implement initial countermeasures. Our findings underscore the need for LLM providers to address metadata leakage as AI systems handle increasingly sensitive information.

**Link**: [arxiv](http://arxiv.org/abs/2511.03675v1),  [pdf](http://arxiv.org/pdf/2511.03675v1)

**Tags**: cs.CR cs.AI K.4.1; C.2.0; K.6.5; I.2.7 



### Do Automatic Factuality Metrics Measure Factuality? A Critical   Evaluation
**Authors**: Sanjana Ramprasad, Byron C. Wallace

**Updated**: 2025-11-05T17:42:36Z

**Summary**: Modern LLMs can now produce highly readable abstractive summaries, to the point that traditional automated metrics for evaluating summary quality, such as ROUGE, have saturated. However, LLMs still sometimes introduce inaccuracies into summaries, i.e., information inconsistent with or unsupported by the corresponding source. Measuring the occurrence of these often subtle factual inconsistencies automatically has proved challenging. This in turn has motivated development of metrics intended to measure the factual consistency of generated summaries against sources. But are these approaches measuring what they purport to? Or are they mostly exploiting artifacts? In this work, we stress test a range of automatic factuality metrics, including specialized models and LLM-based prompting methods, to probe what they actually capture. Using a shallow classifier to separate ``easy'' examples for factual evaluation where surface features suffice from ``hard'' cases requiring deeper reasoning, we find that all metrics show substantial performance drops on the latter. Furthermore, some metrics are more sensitive to benign, fact-preserving edits than to factual corrections. Building on this observation, we demonstrate that most automatic factuality metrics can be gamed, i.e., their scores can be artificially inflated by appending innocuous, content-free sentences to summaries. Among the metrics tested, the prompt based ChatGPT-DA approach is the most robust and reliable. However, this comes with a notable caveat: Prompting LLMs to assess factuality may overly rely on their parametric knowledge rather than the provided reference when making judgments. Taken together, our findings call into question the reliability of current factuality metrics and prompt a broader reflection on what these metrics are truly measuring.

**Link**: [arxiv](http://arxiv.org/abs/2411.16638v4),  [pdf](http://arxiv.org/pdf/2411.16638v4)

**Tags**: cs.CL cs.AI 



### TabTune: A Unified Library for Inference and Fine-Tuning Tabular   Foundation Models
**Authors**: Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu

**Updated**: 2025-11-05T17:36:30Z

**Summary**: Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.

**Link**: [arxiv](http://arxiv.org/abs/2511.02802v2),  [pdf](http://arxiv.org/pdf/2511.02802v2)

**Tags**: cs.LG cs.AI 



### Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs
**Authors**: Changhao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, Bo Dai

**Updated**: 2025-11-05T17:33:06Z

**Summary**: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.

**Link**: [arxiv](http://arxiv.org/abs/2410.20749v3),  [pdf](http://arxiv.org/pdf/2410.20749v3)

**Tags**: cs.LG cs.AI cs.CL 



### A Lightweight 3D-CNN for Event-Based Human Action Recognition with   Privacy-Preserving Potential
**Authors**: Mehdi Sefidgar Dilmaghani, Francis Fowley, Peter Corcoran

**Updated**: 2025-11-05T17:30:31Z

**Summary**: This paper presents a lightweight three-dimensional convolutional neural network (3DCNN) for human activity recognition (HAR) using event-based vision data. Privacy preservation is a key challenge in human monitoring systems, as conventional frame-based cameras capture identifiable personal information. In contrast, event cameras record only changes in pixel intensity, providing an inherently privacy-preserving sensing modality. The proposed network effectively models both spatial and temporal dynamics while maintaining a compact design suitable for edge deployment. To address class imbalance and enhance generalization, focal loss with class reweighting and targeted data augmentation strategies are employed. The model is trained and evaluated on a composite dataset derived from the Toyota Smart Home and ETRI datasets. Experimental results demonstrate an F1-score of 0.9415 and an overall accuracy of 94.17%, outperforming benchmark 3D-CNN architectures such as C3D, ResNet3D, and MC3_18 by up to 3%. These results highlight the potential of event-based deep learning for developing accurate, efficient, and privacy-aware human action recognition systems suitable for real-world edge applications.

**Link**: [arxiv](http://arxiv.org/abs/2511.03665v1),  [pdf](http://arxiv.org/pdf/2511.03665v1)

**Tags**: cs.CV 



### PLUTO-4: Frontier Pathology Foundation Models
**Authors**: Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed

**Updated**: 2025-11-05T17:25:40Z

**Summary**: Foundation models trained on large-scale pathology image corpora have demonstrated strong transfer capabilities across diverse histopathology tasks. Building on this progress, we introduce PLUTO-4, our next generation of pathology foundation models that extend the Pathology-Universal Transformer (PLUTO) to frontier scale. We share two complementary Vision Transformer architectures in the PLUTO-4 family: a compact and efficient PLUTO-4S model optimized for multi-scale deployment using a FlexiViT setup with 2D-RoPE embeddings, and a frontier-scale PLUTO-4G model trained with a single patch size to maximize representation capacity and stability. Both models are pretrained using a self-supervised objective derived from DINOv2 on a large multi-institutional corpus containing 551,164 WSIs from 137,144 patients across over 50 institutions, spanning over 60 disease types and over 100 stains. Comprehensive evaluation across public and internal benchmarks demonstrates that PLUTO-4 achieves state-of-the-art performance on tasks requiring varying spatial and biological context, including patch-level classification, segmentation, and slide-level diagnosis. The compact PLUTO-4S provides high-throughput and robust performance for practical deployment, while PLUTO-4G establishes new performance frontiers across multiple pathology benchmarks, including an 11% improvement in dermatopathology diagnosis. These diverse improvements underscore PLUTO-4's potential to transform real-world applications as a backbone for translational research and diagnostic use cases.

**Link**: [arxiv](http://arxiv.org/abs/2511.02826v2),  [pdf](http://arxiv.org/pdf/2511.02826v2)

**Tags**: cs.CV 



### RoboRAN: A Unified Robotics Framework for Reinforcement Learning-Based   Autonomous Navigation
**Authors**: Matteo El-Hariry, Antoine Richard, Ricard M. Castan, Luis F. W. Batista, Matthieu Geist, Cedric Pradalier, Miguel Olivares-Mendez

**Updated**: 2025-11-05T17:12:59Z

**Summary**: Autonomous robots must navigate and operate in diverse environments, from terrestrial and aquatic settings to aerial and space domains. While Reinforcement Learning (RL) has shown promise in training policies for specific autonomous robots, existing frameworks and benchmarks are often constrained to unique platforms, limiting generalization and fair comparisons across different mobility systems. In this paper, we present a multi-domain framework for training, evaluating and deploying RL-based navigation policies across diverse robotic platforms and operational environments. Our work presents four key contributions: (1) a scalable and modular framework, facilitating seamless robot-task interchangeability and reproducible training pipelines; (2) sim-to-real transfer demonstrated through real-world experiments with multiple robots, including a satellite robotic simulator, an unmanned surface vessel, and a wheeled ground vehicle; (3) the release of the first open-source API for deploying Isaac Lab-trained policies to real robots, enabling lightweight inference and rapid field validation; and (4) uniform tasks and metrics for cross-medium evaluation, through a unified evaluation testbed to assess performance of navigation tasks in diverse operational conditions (aquatic, terrestrial and space). By ensuring consistency between simulation and real-world deployment, RoboRAN lowers the barrier to developing adaptable RL-based navigation strategies. Its modular design enables straightforward integration of new robots and tasks through predefined templates, fostering reproducibility and extension to diverse domains. To support the community, we release RoboRAN as open-source.

**Link**: [arxiv](http://arxiv.org/abs/2505.14526v2),  [pdf](http://arxiv.org/pdf/2505.14526v2)

**Tags**: cs.RO cs.AI 



### Flying Robotics Art: ROS-based Drone Draws the Record-Breaking Mural
**Authors**: Andrei A. Korigodskii, Oleg D. Kalachev, Artem E. Vasiunik, Matvei V. Urvantsev, Georgii E. Bondar

**Updated**: 2025-11-05T17:09:16Z

**Summary**: This paper presents the innovative design and successful deployment of a pioneering autonomous unmanned aerial system developed for executing the world's largest mural painted by a drone. Addressing the dual challenges of maintaining artistic precision and operational reliability under adverse outdoor conditions such as wind and direct sunlight, our work introduces a robust system capable of navigating and painting outdoors with unprecedented accuracy. Key to our approach is a novel navigation system that combines an infrared (IR) motion capture camera and LiDAR technology, enabling precise location tracking tailored specifically for largescale artistic applications. We employ a unique control architecture that uses different regulation in tangential and normal directions relative to the planned path, enabling precise trajectory tracking and stable line rendering. We also present algorithms for trajectory planning and path optimization, allowing for complex curve drawing and area filling. The system includes a custom-designed paint spraying mechanism, specifically engineered to function effectively amidst the turbulent airflow generated by the drone's propellers, which also protects the drone's critical components from paint-related damage, ensuring longevity and consistent performance. Experimental results demonstrate the system's robustness and precision in varied conditions, showcasing its potential for autonomous large-scale art creation and expanding the functional applications of robotics in creative fields.

**Link**: [arxiv](http://arxiv.org/abs/2511.03651v1),  [pdf](http://arxiv.org/pdf/2511.03651v1)

**Tags**: cs.RO cs.CV cs.SY eess.SY I.2.9; J.5 



### Post Persona Alignment for Multi-Session Dialogue Generation
**Authors**: Yi-Pei Chen, Noriki Nishida, Hideki Nakayama, Yuji Matsumoto

**Updated**: 2025-11-05T17:05:02Z

**Summary**: Multi-session persona-based dialogue generation presents challenges in maintaining long-term consistency and generating diverse, personalized responses. While large language models (LLMs) excel in single-session dialogues, they struggle to preserve persona fidelity and conversational coherence across extended interactions. Existing methods typically retrieve persona information before response generation, which can constrain diversity and result in generic outputs. We propose Post Persona Alignment (PPA), a novel two-stage framework that reverses this process. PPA first generates a general response based solely on dialogue context, then retrieves relevant persona memories using the response as a query, and finally refines the response to align with the speaker's persona. This post-hoc alignment strategy promotes naturalness and diversity while preserving consistency and personalization. Experiments on multi-session LLM-generated dialogue data demonstrate that PPA significantly outperforms prior approaches in consistency, diversity, and persona relevance, offering a more flexible and effective paradigm for long-term personalized dialogue generation.

**Link**: [arxiv](http://arxiv.org/abs/2506.11857v2),  [pdf](http://arxiv.org/pdf/2506.11857v2)

**Tags**: cs.CL 



### Watermarking Large Language Models in Europe: Interpreting the AI Act in   Light of Technology
**Authors**: Thomas Souverain

**Updated**: 2025-11-05T17:00:39Z

**Summary**: To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ''sufficiently reliable, interoperable, effective and robust''. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2511.03641v1),  [pdf](http://arxiv.org/pdf/2511.03641v1)

**Tags**: cs.CR cs.AI cs.CL cs.CY 68T01, 68727, 68T30, 68T35, 68T37, 68T50 



### Towards Transparent Stance Detection: A Zero-Shot Approach Using   Implicit and Explicit Interpretability
**Authors**: Apoorva Upadhyaya, Wolfgang Nejdl, Marco Fisichella

**Updated**: 2025-11-05T16:54:10Z

**Summary**: Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward unseen targets. Existing research using contrastive, meta-learning, or data augmentation suffers from generalizability issues or lack of coherence between text and target. Recent works leveraging large language models (LLMs) for ZSSD focus either on improving unseen target-specific knowledge or generating explanations for stance analysis. However, most of these works are limited by their over-reliance on explicit reasoning, provide coarse explanations that lack nuance, and do not explicitly model the reasoning process, making it difficult to interpret the model's predictions. To address these issues, in our study, we develop a novel interpretable ZSSD framework, IRIS. We provide an interpretable understanding of the attitude of the input towards the target implicitly based on sequences within the text (implicit rationales) and explicitly based on linguistic measures (explicit rationales). IRIS considers stance detection as an information retrieval ranking task, understanding the relevance of implicit rationales for different stances to guide the model towards correct predictions without requiring the ground-truth of rationales, thus providing inherent interpretability. In addition, explicit rationales based on communicative features help decode the emotional and cognitive dimensions of stance, offering an interpretable understanding of the author's attitude towards the given target. Extensive experiments on the benchmark datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10% training data prove the generalizability of our model, benefiting from the proposed architecture and interpretable design.

**Link**: [arxiv](http://arxiv.org/abs/2511.03635v1),  [pdf](http://arxiv.org/pdf/2511.03635v1)

**Tags**: cs.CL cs.LG 



### Read Your Own Mind: Reasoning Helps Surface Self-Confidence Signals in   LLMs
**Authors**: Jakub Podolak, Rajeev Verma

**Updated**: 2025-11-05T16:53:09Z

**Summary**: We study the source of uncertainty in DeepSeek R1-32B by analyzing its self-reported verbal confidence on question answering (QA) tasks. In the default answer-then-confidence setting, the model is regularly over-confident, whereas semantic entropy - obtained by sampling many responses - remains reliable. We hypothesize that this is because of semantic entropy's larger test-time compute, which lets us explore the model's predictive distribution. We show that granting DeepSeek the budget to explore its distribution by forcing a long chain-of-thought before the final answer greatly improves its verbal score effectiveness, even on simple fact-retrieval questions that normally require no reasoning. Furthermore, a separate reader model that sees only the chain can reconstruct very similar confidences, indicating the verbal score might be merely a statistic of the alternatives surfaced during reasoning. Our analysis concludes that reliable uncertainty estimation requires explicit exploration of the generative space, and self-reported confidence is trustworthy only after such exploration.

**Link**: [arxiv](http://arxiv.org/abs/2505.23845v2),  [pdf](http://arxiv.org/pdf/2505.23845v2)

**Tags**: cs.CL 



### Financial Management System for SMEs: Real-World Deployment of Accounts   Receivable and Cash Flow Prediction
**Authors**: Bartłomiej Małkus, Szymon Bobek, Grzegorz J. Nalepa

**Updated**: 2025-11-05T16:49:50Z

**Summary**: Small and Medium Enterprises (SMEs), particularly freelancers and early-stage businesses, face unique financial management challenges due to limited resources, small customer bases, and constrained data availability. This paper presents the development and deployment of an integrated financial prediction system that combines accounts receivable prediction and cash flow forecasting specifically designed for SME operational constraints. Our system addresses the gap between enterprise-focused financial tools and the practical needs of freelancers and small businesses. The solution integrates two key components: a binary classification model for predicting invoice payment delays, and a multi-module cash flow forecasting model that handles incomplete and limited historical data. A prototype system has been implemented and deployed as a web application with integration into Cluee's platform, a startup providing financial management tools for freelancers, demonstrating practical feasibility for real-world SME financial management.

**Link**: [arxiv](http://arxiv.org/abs/2511.03631v1),  [pdf](http://arxiv.org/pdf/2511.03631v1)

**Tags**: cs.LG 



### LiveTradeBench: Seeking Real-World Alpha with Large Language Models
**Authors**: Haofei Yu, Fenghai Li, Jiaxuan You

**Updated**: 2025-11-05T16:47:26Z

**Summary**: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.

**Link**: [arxiv](http://arxiv.org/abs/2511.03628v1),  [pdf](http://arxiv.org/pdf/2511.03628v1)

**Tags**: q-fin.TR cs.AI cs.CE cs.CL 



### SecRepoBench: Benchmarking Code Agents for Secure Code Completion in   Real-World Repositories
**Authors**: Chihao Shen, Connor Dilgren, Purva Chiniya, Luke Griffith, Yu Ding, Yizheng Chen

**Updated**: 2025-11-05T16:42:11Z

**Summary**: This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28 standalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.

**Link**: [arxiv](http://arxiv.org/abs/2504.21205v2),  [pdf](http://arxiv.org/pdf/2504.21205v2)

**Tags**: cs.CR cs.AI 



### R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large   Model Token Routing
**Authors**: Tianyu Fu, Yi Ge, Yichen You, Enshu Liu, Zhihang Yuan, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang

**Updated**: 2025-11-05T16:39:11Z

**Summary**: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.

**Link**: [arxiv](http://arxiv.org/abs/2505.21600v2),  [pdf](http://arxiv.org/pdf/2505.21600v2)

**Tags**: cs.CL cs.AI cs.LG cs.PF I.2.7 



### 3D Cooperative User Tracking for Distributed Integrated Sensing and   Communication
**Authors**: Yingjie Xu, Xuesong Cai, Michiel Sandra, Sara Willhammar, Fredrik Tufvesson

**Updated**: 2025-11-05T16:29:20Z

**Summary**: As integrated sensing and communication (ISAC) becomes an integral part of 6G networks, distributed ISAC (DISAC) is expected to enhance both sensing and communication performance through its decentralized architecture. This paper presents a complete framework to address the challenge of cooperative user tracking in DISAC systems. By incorporating a global probability hypothesis density (PHD) filter and a field-of-view-aware access point (AP) management strategy, the framework enables accurate user tracking using radio signals while optimizing AP scheduling. In addition, a real-world distributed MIMO channel measurement campaign is performed to evaluate the effectiveness of the framework. The results demonstrate that a centimeter-level root mean-square trajectory error can be achieved. Furthermore, the results show that it is not necessary to keep APs active at all times to maintain high tracking accuracy, indicating the need for robust and efficient AP management. These findings provide valuable insight into practical deployments and further development of cooperative user tracking techniques in DISAC systems.

**Link**: [arxiv](http://arxiv.org/abs/2511.03612v1),  [pdf](http://arxiv.org/pdf/2511.03612v1)

**Tags**: eess.SP 



### Step-Audio-EditX Technical Report
**Authors**: Chao Yan, Boyong Wu, Peng Yang, Pengfei Tan, Guoqiang Hu, Yuxin Zhang, Xiangyu, Zhang, Fei Tian, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu

**Updated**: 2025-11-05T16:22:19Z

**Summary**: We present Step-Audio-EditX, the first open-source LLM-based audio model excelling at expressive and iterative audio editing encompassing emotion, speaking style, and paralinguistics alongside robust zero-shot text-to-speech (TTS) capabilities.Our core innovation lies in leveraging only large-margin synthetic data, which circumvents the need for embedding-based priors or auxiliary modules. This large-margin learning approach enables both iterative control and high expressivity across voices, and represents a fundamental pivot from the conventional focus on representation-level disentanglement. Evaluation results demonstrate that Step-Audio-EditX surpasses both MiniMax-2.6-hd and Doubao-Seed-TTS-2.0 in emotion editing and other fine-grained control tasks.

**Link**: [arxiv](http://arxiv.org/abs/2511.03601v1),  [pdf](http://arxiv.org/pdf/2511.03601v1)

**Tags**: cs.CL cs.AI cs.HC cs.SD eess.AS 



### SME-TEAM: Leveraging Trust and Ethics for Secure and Responsible Use of   AI and LLMs in SMEs
**Authors**: Iqbal H. Sarker, Helge Janicke, Ahmad Mohsin, Leandros Maglaras

**Updated**: 2025-11-05T16:07:58Z

**Summary**: Artificial Intelligence (AI) and Large Language Models (LLMs) are revolutionizing today's business practices; however, their adoption within small and medium-sized enterprises (SMEs) raises serious trust, ethical, and technical issues. In this perspective paper, we introduce a structured, multi-phased framework, "SME-TEAM" for the secure and responsible use of these technologies in SMEs. Based on a conceptual structure of four key pillars, i.e., Data, Algorithms, Human Oversight, and Model Architecture, SME-TEAM bridges theoretical ethical principles with operational practice, enhancing AI capabilities across a wide range of applications in SMEs. Ultimately, this paper provides a structured roadmap for the adoption of these emerging technologies, positioning trust and ethics as a driving force for resilience, competitiveness, and sustainable innovation within the area of business analytics and SMEs.

**Link**: [arxiv](http://arxiv.org/abs/2509.10594v2),  [pdf](http://arxiv.org/pdf/2509.10594v2)

**Tags**: cs.LG cs.AI cs.CR 



### PerfDojo: Automated ML Library Generation for Heterogeneous   Architectures
**Authors**: Andrei Ivanov, Siyuan Shen, Gioele Gottardo, Marcin Chrapek, Afif Boudaoud, Timo Schneider, Luca Benini, Torsten Hoefler

**Updated**: 2025-11-05T16:05:26Z

**Summary**: The increasing complexity of machine learning models and the proliferation of diverse hardware architectures (CPUs, GPUs, accelerators) make achieving optimal performance a significant challenge. Heterogeneity in instruction sets, specialized kernel requirements for different data types and model features (e.g., sparsity, quantization), and architecture-specific optimizations complicate performance tuning. Manual optimization is resource-intensive, while existing automatic approaches often rely on complex hardware-specific heuristics and uninterpretable intermediate representations, hindering performance portability. We introduce PerfLLM, a novel automatic optimization methodology leveraging Large Language Models (LLMs) and Reinforcement Learning (RL). Central to this is PerfDojo, an environment framing optimization as an RL game using a human-readable, mathematically-inspired code representation that guarantees semantic validity through transformations. This allows effective optimization without prior hardware knowledge, facilitating both human analysis and RL agent training. We demonstrate PerfLLM's ability to achieve significant performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.

**Link**: [arxiv](http://arxiv.org/abs/2511.03586v1),  [pdf](http://arxiv.org/pdf/2511.03586v1)

**Tags**: cs.PF cs.AI 



### OneOcc: Semantic Occupancy Prediction for Legged Robots with a Single   Panoramic Camera
**Authors**: Hao Shi, Ze Wang, Shangwei Guo, Mengfei Duan, Song Wang, Teng Chen, Kailun Yang, Lin Wang, Kaiwei Wang

**Updated**: 2025-11-05T15:51:42Z

**Summary**: Robust 3D semantic occupancy is crucial for legged/humanoid robots, yet most semantic scene completion (SSC) systems target wheeled platforms with forward-facing sensors. We present OneOcc, a vision-only panoramic SSC framework designed for gait-introduced body jitter and 360{\deg} continuity. OneOcc combines: (i) Dual-Projection fusion (DP-ER) to exploit the annular panorama and its equirectangular unfolding, preserving 360{\deg} continuity and grid alignment; (ii) Bi-Grid Voxelization (BGV) to reason in Cartesian and cylindrical-polar spaces, reducing discretization bias and sharpening free/occupied boundaries; (iii) a lightweight decoder with Hierarchical AMoE-3D for dynamic multi-scale fusion and better long-range/occlusion reasoning; and (iv) plug-and-play Gait Displacement Compensation (GDC) learning feature-level motion correction without extra sensors. We also release two panoramic occupancy benchmarks: QuadOcc (real quadruped, first-person 360{\deg}) and Human360Occ (H3O) (CARLA human-ego 360{\deg} with RGB, Depth, semantic occupancy; standardized within-/cross-city splits). OneOcc sets new state-of-the-art (SOTA): on QuadOcc it beats strong vision baselines and popular LiDAR ones; on H3O it gains +3.83 mIoU (within-city) and +8.08 (cross-city). Modules are lightweight, enabling deployable full-surround perception for legged/humanoid robots. Datasets and code will be publicly available at https://github.com/MasterHow/OneOcc.

**Link**: [arxiv](http://arxiv.org/abs/2511.03571v1),  [pdf](http://arxiv.org/pdf/2511.03571v1)

**Tags**: cs.RO cs.CV eess.IV 



### TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and   Retrieval
**Authors**: Günther Schindler, Maximilian Schambach, Michael Medek, Sam Thelin

**Updated**: 2025-11-05T15:51:03Z

**Summary**: We study LLMs for tabular prediction with mixed text, numeric, and categorical fields. We introduce TabGemma, a schema-agnostic in-context learner that treats rows as sequences and tackles two practical hurdles when adapting pretrained LLMs for tabular predictions: unstable numeric tokenization and limited context size. We propose to canonicalize numbers via signed scientific notation and continue pretraining of a 12B Gemma 3 model with a target imputation objective using a large-scale real world dataset. For inference, we use a compact n-gram-based retrieval to select informative exemplars that fit within a 128k-token window.   On semantically rich benchmarks, TabGemma establishes a new state of the art on classification across low- and high-data regimes and improves monotonically with more context rows. For regression, it is competitive at small sample sizes but trails conventional approaches as data grows. Our results show that LLMs can be effective tabular in-context learners on highly semantic tasks when paired with dedicated numeric handling and context retrieval, while motivating further advances in numeric modeling and long-context scaling.

**Link**: [arxiv](http://arxiv.org/abs/2511.03570v1),  [pdf](http://arxiv.org/pdf/2511.03570v1)

**Tags**: cs.LG 



### ENDF/B-VIII.1: Updated Nuclear Reaction Data Library for Science and   Applications
**Authors**: G. P. A. Nobre, R. Capote, M. T. Pigni, A. Trkov, C. M. Mattoon, D. Neudecker, D. A. Brown, M. B. Chadwick, A. C. Kahler, N. A. Kleedtke, M. Zerkle, A. I. Hawari, C. W. Chapman, N. C. Fleming, J. L. Wormald, K. Ramić, Y. Danon, N. A. Gibson, P. Brain, M. W. Paris, G. M. Hale, I. J. Thompson, D. P. Barry, I. Stetcu, W. Haeck, A. E. Lovell, M. R. Mumpower, G. Potel, K. Kravvaris, G. Noguere, J. D. McDonnell, A. D. Carlson, M. Dunn, T. Kawano, D. Wiarda, I. Al-Qasir, G. Arbanas, R. Arcilla, B. Beck, D. Bernard, R. Beyer, J. M. Brown, O. Cabellos, R. J. Casperson, Y. Cheng, E. V. Chimanski, R. Coles, M. Cornock, J. Cotchen, J. P. W. Crozier, D. E. Cullen, A. Daskalakis, M. -A. Descalle, D. D. DiJulio, P. Dimitriou, A. C. Dreyfuss, I. Durán, R. Ferrer, T. Gaines, V. Gillette, G. Gert, K. H. Guber, J. D. Haverkamp, M. W. Herman, J. Holmes, M. Hursin, N. Jisrawi, A. R. Junghans, K. J. Kelly, H. I. Kim, K. S. Kim, A. J. Koning, M. Koštál, B. K. Laramee, A. Lauer-Coles, L. Leal, H. Y. Lee, A. M. Lewis, J. Malec, J. I. Márquez Damián, W. J. Marshall, A. Mattera, G. Muhrer, A. Ney, W. E. Ormand, D. K. Parsons, C. M. Percher, V. G. Pronyaev, A. Qteish, S. Quaglioni, M. Rapp, J. J. Ressler, M. Rising, D. Rochman, P. K. Romano, D. Roubtsov, G. Schnabel, M. Schulc, G. J. Siemers, A. A. Sonzogni, P. Talou, J. Thompson, T. H. Trumbull, S. C. van der Marck, M. Vorabbi, C. Wemple, K. A. Wendt, M. White, R. Q. Wright

**Updated**: 2025-11-05T15:45:55Z

**Summary**: The ENDF/B-VIII.1 library is the newest recommended evaluated nuclear data file by the Cross Section Evaluation Working Group (CSEWG) for use in nuclear science and technology applications, and incorporates advances made in the six years since the release of ENDF/B-VIII.0. Among key advances made are that the $^{239}$Pu file was reevaluated by a joint international effort and that updated $^{16,18}$O, $^{19}$F, $^{28-30}$Si, $^{50-54}$Cr, $^{55}$Mn, $^{54,56,57}$Fe, $^{63,65}$Cu, $^{139}$La, $^{233,235,238}$U, and $^{240,241}$Pu neutron nuclear data from the IAEA coordinated INDEN collaboration were adopted. Over 60 neutron dosimetry cross sections were adopted from the IAEA's IRDFF-II library. In addition, the new library includes significant changes for $^3$He, $^6$Li,$^9$Be, $^{51}$V, $^{88}$Sr, $^{103}$Rh, $^{140,142}$Ce, Dy, $^{181}$Ta, Pt, $^{206-208}$Pb, and $^{234,236}$U neutron data, and new nuclear data for the photonuclear, charged-particle and atomic sublibraries. Numerous thermal neutron scattering kernels were reevaluated or provided for the very first time. On the covariance side, work was undertaken to introduce better uncertainty quantification standards and testing for nuclear data covariances. The significant effort to reevaluate important nuclides has reduced bias in the simulations of many integral experiments with particular progress noted for fluorine, copper, and stainless steel containing benchmarks. Data issues hindered the successful deployment of the previous ENDF/B-VIII.0 for commercial nuclear power applications in high burnup situations. These issues were addressed by improving the $^{238}$U and $^{239,240,241}$Pu evaluated data in the resonance region. The new library performance as a function of burnup is similar to the reference ENDF/B-VII.1 library. The ENDF/B-VIII.1 data are available in ENDF-6 and GNDS format at https://doi.org/10.11578/endf/2571019.

**Link**: [arxiv](http://arxiv.org/abs/2511.03564v1),  [pdf](http://arxiv.org/pdf/2511.03564v1)

**Tags**: physics.app-ph nucl-ex nucl-th 



### ASVRI-Legal: Fine-Tuning LLMs with Retrieval Augmented Generation for   Enhanced Legal Regulation
**Authors**: One Octadion, Bondan Sapta Prakoso, Nanang Yudi Setiawan, Novanto Yudistira

**Updated**: 2025-11-05T15:45:52Z

**Summary**: In this study, we explore the fine-tuning of Large Language Models (LLMs) to better support policymakers in their crucial work of understanding, analyzing, and crafting legal regulations. To equip the model with a deep understanding of legal texts, we curated a supervised dataset tailored to the specific needs of the legal domain. Additionally, we integrated the Retrieval-Augmented Generation (RAG) method, enabling the LLM to access and incorporate up-to-date legal knowledge from external sources. This combination of fine-tuning and RAG-based augmentation results in a tool that not only processes legal information but actively assists policymakers in interpreting regulations and drafting new ones that align with current needs. The results demonstrate that this approach can significantly enhance the effectiveness of legal research and regulation development, offering a valuable resource in the ever-evolving field of law.

**Link**: [arxiv](http://arxiv.org/abs/2511.03563v1),  [pdf](http://arxiv.org/pdf/2511.03563v1)

**Tags**: cs.CL 



### Assessing the Macro and Micro Effects of Random Seeds on Fine-Tuning   Large Language Models
**Authors**: Nghia Bui, Guergana Savova, Lijing Wang

**Updated**: 2025-11-05T15:35:21Z

**Summary**: The impact of random seeds in fine-tuning large language models (LLMs) has been largely overlooked despite its potential influence on model performance.In this study, we systematically evaluate the effects of random seeds on LLMs using the GLUE and SuperGLUE benchmarks. We analyze the macro-level impact through traditional metrics like accuracy and F1, calculating their mean and variance to quantify performance fluctuations. To capture the micro-level effects, we introduce a novel metric, consistency, measuring the stability of individual predictions across runs. Our experiments reveal significant variance at both macro and micro levels, underscoring the need for careful consideration of random seeds in fine-tuning and evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2503.07329v2),  [pdf](http://arxiv.org/pdf/2503.07329v2)

**Tags**: cs.CL cs.AI cs.LG 



### Intelligent Computing Social Modeling and Methodological Innovations in   Political Science in the Era of Large Language Models
**Authors**: Zhenyu Wang, Dequan Wang, Yi Xu, Lingfeng Zhou, Yiqi Zhou

**Updated**: 2025-11-05T15:35:06Z

**Summary**: The recent wave of artificial intelligence, epitomized by large language models (LLMs),has presented opportunities and challenges for methodological innovation in political science,sparking discussions on a potential paradigm shift in the social sciences. However, how can weunderstand the impact of LLMs on knowledge production and paradigm transformation in thesocial sciences from a comprehensive perspective that integrates technology and methodology? What are LLMs' specific applications and representative innovative methods in political scienceresearch? These questions, particularly from a practical methodological standpoint, remainunderexplored. This paper proposes the "Intelligent Computing Social Modeling" (ICSM) methodto address these issues by clarifying the critical mechanisms of LLMs. ICSM leverages thestrengths of LLMs in idea synthesis and action simulation, advancing intellectual exploration inpolitical science through "simulated social construction" and "simulation validation." Bysimulating the U.S. presidential election, this study empirically demonstrates the operationalpathways and methodological advantages of ICSM. By integrating traditional social scienceparadigms, ICSM not only enhances the quantitative paradigm's capability to apply big data toassess the impact of factors but also provides qualitative paradigms with evidence for socialmechanism discovery at the individual level, offering a powerful tool that balances interpretabilityand predictability in social science research. The findings suggest that LLMs will drivemethodological innovation in political science through integration and improvement rather thandirect substitution.

**Link**: [arxiv](http://arxiv.org/abs/2410.16301v2),  [pdf](http://arxiv.org/pdf/2410.16301v2)

**Tags**: cs.CY cs.AI 



### MultiZebraLogic: A Multilingual Logical Reasoning Benchmark
**Authors**: Sofie Helene Bruun, Dan Saattrup Smart

**Updated**: 2025-11-05T15:34:48Z

**Summary**: Measuring the full abilities of large language models (LLMs) requires benchmarks representing multiple tasks. We aim to create large, high-quality datasets for comparison of logical reasoning skills across several languages and of suitable difficulty for LLMs of various reasoning ability. We explore multiple ways of increasing difficulty. We generate zebra puzzles in multiple languages, themes, sizes and including 14 different clue types and 8 red herring types (uninformative clues). We find puzzle sizes 2x3 and 4x5 are sufficiently challenging for GPT-4o mini (a non-reasoning model) and o3-mini (a reasoning model), respectively. Including 5 red herrings decreases o3-mini puzzle-level accuracy on 4x5 puzzles by 15$\pm$7 %. Scores of o3-mini on 4x5 puzzles are not significantly affected by use of English vs. Danish or the common houses theme vs. the country-specific smoerrebroed theme. We find no correlation between difficulty and the selected clue types. Datasets of 128+1024 puzzles are published as MultiZebraLogic in each of nine Germanic languages for sizes 2x3 and 4x5. We publish code for puzzle generation, designed for adaptablity into more languages and themes.

**Link**: [arxiv](http://arxiv.org/abs/2511.03553v1),  [pdf](http://arxiv.org/pdf/2511.03553v1)

**Tags**: cs.CL cs.AI 



### Uncovering Code Insights: Leveraging GitHub Artifacts for Deeper Code   Understanding
**Authors**: Ziv Nevo, Orna Raz, Karen Yorav

**Updated**: 2025-11-05T15:31:42Z

**Summary**: Understanding the purpose of source code is a critical task in software maintenance, onboarding, and modernization. While large language models (LLMs) have shown promise in generating code explanations, they often lack grounding in the broader software engineering context. We propose a novel approach that leverages natural language artifacts from GitHub -- such as pull request descriptions, issue descriptions and discussions, and commit messages -- to enhance LLM-based code understanding. Our system consists of three components: one that extracts and structures relevant GitHub context, another that uses this context to generate high-level explanations of the code's purpose, and a third that validates the explanation. We implemented this as a standalone tool, as well as a server within the Model Context Protocol (MCP), enabling integration with other AI-assisted development tools. Our main use case is that of enhancing a standard LLM-based code explanation with code insights that our system generates. To evaluate explanations' quality, we conducted a small scale user study, with developers of several open projects, as well as developers of proprietary projects. Our user study indicates that when insights are generated they often are helpful and non trivial, and are free from hallucinations.

**Link**: [arxiv](http://arxiv.org/abs/2511.03549v1),  [pdf](http://arxiv.org/pdf/2511.03549v1)

**Tags**: cs.SE cs.AI 



### SOLVE-Med: Specialized Orchestration for Leading Vertical Experts across   Medical Specialties
**Authors**: Roberta Di Marino, Giovanni Dioguardi, Antonio Romano, Giuseppe Riccio, Mariano Barone, Marco Postiglione, Flora Amato, Vincenzo Moscato

**Updated**: 2025-11-05T15:15:35Z

**Summary**: Medical question answering systems face deployment challenges including hallucinations, bias, computational demands, privacy concerns, and the need for specialized expertise across diverse domains. Here, we present SOLVE-Med, a multi-agent architecture combining domain-specialized small language models for complex medical queries. The system employs a Router Agent for dynamic specialist selection, ten specialized models (1B parameters each) fine-tuned on specific medical domains, and an Orchestrator Agent that synthesizes responses. Evaluated on Italian medical forum data across ten specialties, SOLVE-Med achieves superior performance with ROUGE-1 of 0.301 and BERTScore F1 of 0.697, outperforming standalone models up to 14B parameters while enabling local deployment. Our code is publicly available on GitHub: https://github.com/PRAISELab-PicusLab/SOLVE-Med.

**Link**: [arxiv](http://arxiv.org/abs/2511.03542v1),  [pdf](http://arxiv.org/pdf/2511.03542v1)

**Tags**: cs.CL cs.AI 



### Reinforcement Learning Foundations for Deep Research Systems: A Survey
**Authors**: Wenjun Li, Zhi Chen, Jingru Lin, Hannan Cao, Wei Han, Sheng Liang, Zhi Zhang, Kuicai Dong, Dexun Li, Chen Zhang, Yong Liu

**Updated**: 2025-11-05T15:14:29Z

**Summary**: Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.   This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes recent work along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.

**Link**: [arxiv](http://arxiv.org/abs/2509.06733v2),  [pdf](http://arxiv.org/pdf/2509.06733v2)

**Tags**: cs.AI cs.CL cs.IR 



### Security and Privacy Management of IoT Using Quantum Computing
**Authors**: Jaydip Sen

**Updated**: 2025-11-05T15:08:55Z

**Summary**: The convergence of the Internet of Things (IoT) and quantum computing is redefining the security paradigm of interconnected digital systems. Classical cryptographic algorithms such as RSA, Elliptic Curve Cryptography (ECC), and Advanced Encryption Standard (AES) have long provided the foundation for securing IoT communication. However, the emergence of quantum algorithms such as Shor's and Grover's threatens to render these techniques vulnerable, necessitating the development of quantum-resilient alternatives. This chapter examines the implications of quantum computing for IoT security and explores strategies for building cryptographically robust systems in the post-quantum era. It presents an overview of Post-Quantum Cryptographic (PQC) families, including lattice-based, code-based, hash-based, and multivariate approaches, analyzing their potential for deployment in resource-constrained IoT environments. In addition, quantum-based methods such as Quantum Key Distribution (QKD) and Quantum Random Number Generators (QRNGs) are discussed for their ability to enhance confidentiality and privacy through physics-based security guarantees. The chapter also highlights issues of privacy management, regulatory compliance, and standardization, emphasizing the need for collaborative efforts across academia, industry, and governance. Overall, it provides a comprehensive perspective on security IoT ecosystems against quantum threats and ensures resilience in the next generation of intelligent networks.

**Link**: [arxiv](http://arxiv.org/abs/2511.03538v1),  [pdf](http://arxiv.org/pdf/2511.03538v1)

**Tags**: cs.CR 



### HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs
**Authors**: Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto L. Castro, Torsten Hoefler, Dan Alistarh

**Updated**: 2025-11-05T15:07:16Z

**Summary**: Quantized training of Large Language Models (LLMs) remains an open challenge, as maintaining accuracy while performing all matrix multiplications in low precision has proven difficult. This is particularly the case when fine-tuning pre-trained models, which can have large weight and activation outlier values that make lower-precision optimization difficult. To address this, we present HALO, a novel quantization-aware training approach for Transformers that enables accurate and efficient low-precision training by combining 1) strategic placement of Hadamard rotations in both forward and backward passes, which mitigate outliers, 2) high-performance kernel support, and 3) FSDP integration for low-precision communication. Our approach ensures that all large matrix multiplications during the forward and backward passes are executed in lower precision. Applied to LLAMA-family models, HALO achieves near-full-precision-equivalent results during fine-tuning on various tasks, while delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX 4090 GPUs. HALO efficiently supports both standard and parameterefficient fine-tuning (PEFT). Our results demonstrate the first practical approach to fully quantized LLM fine-tuning that maintains accuracy in 8-bit precision, while delivering performance benefits. Code is available at https://github.com/IST-DASLab/HALO.

**Link**: [arxiv](http://arxiv.org/abs/2501.02625v3),  [pdf](http://arxiv.org/pdf/2501.02625v3)

**Tags**: cs.LG 



### PnPSelect: Plug-and-play IoT Device Selection Using Ultra-wideband   Signals
**Authors**: Zhaoxin Chang, Fusang Zhang, Jie Xiong, Ziyu Li, Badii Jouaber, Daqing Zhang

**Updated**: 2025-11-05T15:06:39Z

**Summary**: In recent years, the number of Internet of Things (IoT) devices in smart homes has rapidly increased. A key challenge affecting user experience is how to enable users to efficiently and intuitively select the devices they wish to control. This paper proposes PnPSelect, a plug-and-play IoT device selection solution utilizing Ultra-wideband (UWB) technology on commercial devices. Unlike previous works, PnPSelect does not require the installation of dedicated hardware on each IoT device, thereby reducing deployment costs and complexities, and achieving true plug-and-play functionality. To enable intuitive device selection, we introduce a pointing direction estimation method that utilizes UWB readings from a single anchor to infer the user pointing direction. Additionally, we propose a lightweight device localization method that allows users to register new IoT devices by simply pointing at them from two distinct positions, eliminating the need for manual measurements. We implement PnPSelect on commercial smartphones and smartwatches and conduct extensive evaluations in both controlled laboratory settings and real-world environments. Our results demonstrate high accuracy, robustness, and adaptability, making PnPSelect a practical and scalable solution for next-generation smart home interactions.

**Link**: [arxiv](http://arxiv.org/abs/2511.03534v1),  [pdf](http://arxiv.org/pdf/2511.03534v1)

**Tags**: cs.HC 



### Toward Humanoid Brain-Body Co-design: Joint Optimization of Control and   Morphology for Fall Recovery
**Authors**: Bo Yue, Sheng Xu, Kui Jia, Guiliang Liu

**Updated**: 2025-11-05T15:01:29Z

**Summary**: Humanoid robots represent a central frontier in embodied intelligence, as their anthropomorphic form enables natural deployment in humans' workspace. Brain-body co-design for humanoids presents a promising approach to realizing this potential by jointly optimizing control policies and physical morphology. Within this context, fall recovery emerges as a critical capability. It not only enhances safety and resilience but also integrates naturally with locomotion systems, thereby advancing the autonomy of humanoids. In this paper, we propose RoboCraft, a scalable humanoid co-design framework for fall recovery that iteratively improves performance through the coupled updates of control policy and morphology. A shared policy pretrained across multiple designs is progressively finetuned on high-performing morphologies, enabling efficient adaptation without retraining from scratch. Concurrently, morphology search is guided by human-inspired priors and optimization algorithms, supported by a priority buffer that balances reevaluation of promising candidates with the exploration of novel designs. Experiments show that RoboCraft achieves an average performance gain of 44.55% on seven public humanoid robots, with morphology optimization drives at least 40% of improvements in co-designing four humanoid robots, underscoring the critical role of humanoid co-design.

**Link**: [arxiv](http://arxiv.org/abs/2510.22336v2),  [pdf](http://arxiv.org/pdf/2510.22336v2)

**Tags**: cs.RO cs.AI 



### Inv-Entropy: A Fully Probabilistic Framework for Uncertainty   Quantification in Language Models
**Authors**: Haoyi Song, Ruihan Ji, Naichen Shi, Fan Lai, Raed Al Kontar

**Updated**: 2025-11-05T14:51:07Z

**Summary**: Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a probabilistic interpretation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input-output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code to reproduce the results can be found at https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2506.09684v2),  [pdf](http://arxiv.org/pdf/2506.09684v2)

**Tags**: cs.CL 



### U2F: Encouraging SWE-Agent to Seize Novelty without Losing Feasibility
**Authors**: Wencheng Ye, Yan Liu

**Updated**: 2025-11-05T14:46:58Z

**Summary**: Large language models (LLMs) have shown strong capabilities in software engineering tasks, yet most existing LLM-based SWE-Agents mainly tackle well-defined problems using conventional methods, often overlooking alternative or innovative solutions beyond their predefined frameworks. This limitation is evident in open-world software environments, where emerging challenges transcend established paradigms.   We propose U2F (Unknown Unknowns to Functional solutions), a cognitive-inspired, uncertainty-embracing multi-agent framework that systematically surfaces "Unknown Unknowns" - novel solution pathways absent from initial formulations but holding innovative potential. U2F consists of two key components: (1) a Discovery-Exploration-Integration agent system for uncovering and synthesizing potential solutions, and (2) cognitive enhancement mechanisms across three dimensions: cross-domain analogical reasoning, reverse thinking, and external validation, which strategically reframe and extend conventional solution boundaries.   Applied to 218 real-world software enabler stories curated from authentic engineering tasks, U2F achieved notable improvements: human experts reported a 14 percent increase in overall novelty, 51 percent improvement in semantic novelty, and stable feasibility (4.02/5.0), corroborated by an LLM-based evaluator. These results highlight the potential of embracing uncertainty as a catalyst for innovation in software engineering.

**Link**: [arxiv](http://arxiv.org/abs/2511.03517v1),  [pdf](http://arxiv.org/pdf/2511.03517v1)

**Tags**: cs.SE 



### One Battle After Another: Probing LLMs' Limits on Multi-Turn Instruction   Following with a Benchmark Evolving Framework
**Authors**: Qi Jia, Kaiwei Zhang, Xiujie Song, Ye Shen, Xiangyang Zhu, Guangtao Zhai

**Updated**: 2025-11-05T14:39:59Z

**Summary**: Understanding how well large language models can follow users' instructions throughout a dialogue spanning multiple topics is of great importance for data-intensive conversational applications. Existing benchmarks are often limited to a fixed number of turns, making them susceptible to saturation and failing to account for the user's interactive experience. In this work, we propose an extensible framework for assessing multi-turn instruction-following ability. At its core, our framework decouples linguistic surface forms from user intent simulation through a three-layer mechanism that tracks constraints, instructions, and topics. This framework mimics User-LLM interaction by enabling the dynamic construction of benchmarks with state changes and tracebacks, terminating a conversation only when the model exhausts a simulated user's patience. We define a suite of metrics capturing the quality of the interaction process. Using this framework, we construct EvolIF, an evolving instruction-following benchmark incorporating nine distinct constraint types. Our results indicate that GPT-5 exhibits superior instruction-following performance. It sustains an average of 18.54 conversational turns and demonstrates 70.31% robustness, outperforming Gemini-2.5-Pro by a significant margin of 11.41%, while other models lag far behind. All of the data and code will be made publicly available online.

**Link**: [arxiv](http://arxiv.org/abs/2511.03508v1),  [pdf](http://arxiv.org/pdf/2511.03508v1)

**Tags**: cs.CL 



### HaluMem: Evaluating Hallucinations in Memory Systems of Agents
**Authors**: Ding Chen, Simin Niu, Kehang Li, Peng Liu, Xiangping Zheng, Bo Tang, Xinchi Li, Feiyu Xiong, Zhiyu Li

**Updated**: 2025-11-05T14:37:34Z

**Summary**: Memory systems are key components that enable AI systems such as LLMs and AI agents to achieve long-term learning and sustained interaction. However, during memory storage and retrieval, these systems frequently exhibit memory hallucinations, including fabrication, errors, conflicts, and omissions. Existing evaluations of memory hallucinations are primarily end-to-end question answering, which makes it difficult to localize the operational stage within the memory system where hallucinations arise. To address this, we introduce the Hallucination in Memory Benchmark (HaluMem), the first operation level hallucination evaluation benchmark tailored to memory systems. HaluMem defines three evaluation tasks (memory extraction, memory updating, and memory question answering) to comprehensively reveal hallucination behaviors across different operational stages of interaction. To support evaluation, we construct user-centric, multi-turn human-AI interaction datasets, HaluMem-Medium and HaluMem-Long. Both include about 15k memory points and 3.5k multi-type questions. The average dialogue length per user reaches 1.5k and 2.6k turns, with context lengths exceeding 1M tokens, enabling evaluation of hallucinations across different context scales and task complexities. Empirical studies based on HaluMem show that existing memory systems tend to generate and accumulate hallucinations during the extraction and updating stages, which subsequently propagate errors to the question answering stage. Future research should focus on developing interpretable and constrained memory operation mechanisms that systematically suppress hallucinations and improve memory reliability.

**Link**: [arxiv](http://arxiv.org/abs/2511.03506v1),  [pdf](http://arxiv.org/pdf/2511.03506v1)

**Tags**: cs.CL 



### Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction
**Authors**: Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu

**Updated**: 2025-11-05T14:29:12Z

**Summary**: Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and parallel decoding but suffer from prohibitive quadratic computational complexity and memory overhead during inference. Current caching techniques accelerate decoding by storing full-layer states, yet impose substantial memory usage that limit long-context applications. Our analysis of attention patterns in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining salient across decoding steps and low-relevance tokens staying unimportant, motivating selective cache eviction. We propose Sparse-dLLM, the first training-free framework integrating dynamic cache eviction with sparse attention via delayed bidirectional sparse caching. By leveraging the stability of token saliency over steps, it retains critical tokens and dynamically evicts unimportant prefix/suffix entries using an attention-guided strategy. Extensive experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to 10$\times$ higher throughput than vanilla dLLMs, with comparable performance and similar peak memory costs, outperforming previous methods in efficiency and effectiveness. The code is available at https://github.com/OpenMOSS/Sparse-dLLM.

**Link**: [arxiv](http://arxiv.org/abs/2508.02558v2),  [pdf](http://arxiv.org/pdf/2508.02558v2)

**Tags**: cs.CL 



### ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied   AI Applications
**Authors**: Lei Fu, Sahar Salimpour, Leonardo Militano, Harry Edelman, Jorge Peña Queralta, Giovanni Toffetti

**Updated**: 2025-11-05T14:27:58Z

**Summary**: Agentic AI systems and Physical or Embodied AI systems have been two key research verticals at the forefront of Artificial Intelligence and Robotics, with Model Context Protocol (MCP) increasingly becoming a key component and enabler of agentic applications. However, the literature at the intersection of these verticals, i.e., Agentic Embodied AI, remains scarce. This paper introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for analyzing, visualizing and processing robot data with natural language through LLMs and VLMs. We describe specific tooling built with robotics domain knowledge, with our initial release focused on mobile robotics and supporting natively the analysis of trajectories, laser scan data, transforms, or time series data. This is in addition to providing an interface to standard ROS 2 CLI tools ("ros2 bag list" or "ros2 bag info"), as well as the ability to filter bags with a subset of topics or trimmed in time. Coupled with the MCP server, we provide a lightweight UI that allows the benchmarking of the tooling with different LLMs, both proprietary (Anthropic, OpenAI) and open-source (through Groq). Our experimental results include the analysis of tool calling capabilities of eight different state-of-the-art LLM/VLM models, both proprietary and open-source, large and small. Our experiments indicate that there is a large divide in tool calling capabilities, with Kimi K2 and Claude Sonnet 4 demonstrating clearly superior performance. We also conclude that there are multiple factors affecting the success rates, from the tool description schema to the number of arguments, as well as the number of tools available to the models. The code is available with a permissive license at https://github.com/binabik-ai/mcp-rosbags.

**Link**: [arxiv](http://arxiv.org/abs/2511.03497v1),  [pdf](http://arxiv.org/pdf/2511.03497v1)

**Tags**: cs.RO cs.AI cs.SE 



### Revisiting Multimodal Positional Encoding in Vision-Language Models
**Authors**: Jie Huang, Xuejing Liu, Sibo Song, Ruibing Hou, Hong Chang, Junyang Lin, Shuai Bai

**Updated**: 2025-11-05T14:25:38Z

**Summary**: Multimodal position encoding is essential for vision-language models, yet there has been little systematic investigation into multimodal position encoding. We conduct a comprehensive analysis of multimodal Rotary Positional Embedding (RoPE) by examining its two core components: position design and frequency allocation. Through extensive experiments, we identify three key guidelines: positional coherence, full frequency utilization, and preservation of textual priors-ensuring unambiguous layout, rich representation, and faithful transfer from the pre-trained LLM. Based on these insights, we propose Multi-Head RoPE (MHRoPE) and MRoPE-Interleave (MRoPE-I), two simple and plug-and-play variants that require no architectural changes. Our methods consistently outperform existing approaches across diverse benchmarks, with significant improvements in both general and fine-grained multimodal understanding. Code will be avaliable at https://github.com/JJJYmmm/Multimodal-RoPEs.

**Link**: [arxiv](http://arxiv.org/abs/2510.23095v2),  [pdf](http://arxiv.org/pdf/2510.23095v2)

**Tags**: cs.CV 



### Why Less is More (Sometimes): A Theory of Data Curation
**Authors**: Elvis Dohmatob, Mohammad Pezeshki, Reyhane Askari-Hemmat

**Updated**: 2025-11-05T14:21:18Z

**Summary**: This paper introduces a theoretical framework to resolve a central paradox in modern machine learning: When is it better to use less data? This question has become critical as classical scaling laws suggesting ``more is more'' (Sun et al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et al., 2025; Muenighoff et al., 2025), which achieve superior performance with small, aggressively curated datasets. Here, we study data curation strategies where an imperfect oracle selects the training examples according to their difficulty and correctness. Our results provide exact scaling law curves for test error under both label-agnostic and label-aware curation rules, revealing when and why keeping only a subset of data can improve generalization. In contrast to classical scaling laws, we show that under certain conditions, small curated datasets can outperform full datasets, and we provide analytical conditions for this by deriving precise phase transition curves tied to data size and quality. We validate these theoretical claims with empirical results on ImageNet, confirming our predictions about when curation improves accuracy and can even mitigate model collapse. Furthermore, our framework provides a principled explanation for the contradictory curation strategies recently observed in LLM mathematical reasoning.

**Link**: [arxiv](http://arxiv.org/abs/2511.03492v1),  [pdf](http://arxiv.org/pdf/2511.03492v1)

**Tags**: cs.LG stat.ML 



### From Haystack to Needle: Label Space Reduction for Zero-shot   Classification
**Authors**: Nathan Vandemoortele, Bram Steenwinckel, Femke Ongenae, Sofie Van Hoecke

**Updated**: 2025-11-05T14:16:25Z

**Summary**: We present Label Space Reduction (LSR), a novel method for improving zero-shot classification performance of Large Language Models (LLMs). LSR iteratively refines the classification label space by systematically ranking and reducing candidate classes, enabling the model to concentrate on the most relevant options. By leveraging unlabeled data with the statistical learning capabilities of data-driven models, LSR dynamically optimizes the label space representation at test time. Our experiments across seven benchmarks demonstrate that LSR improves macro-F1 scores by an average of 7.0% (up to 14.2%) with Llama-3.1-70B and 3.3% (up to 11.1%) with Claude-3.5-Sonnet compared to standard zero-shot classification baselines. To reduce the computational overhead of LSR, which requires an additional LLM call at each iteration, we propose distilling the model into a probabilistic classifier, allowing for efficient inference.

**Link**: [arxiv](http://arxiv.org/abs/2502.08436v2),  [pdf](http://arxiv.org/pdf/2502.08436v2)

**Tags**: cs.CL cs.AI cs.LG 



### Probing $J/ψ$ Production Mechanisms in Proton-Proton Collisions at   SPD/NICA Energies
**Authors**: Shubham Sharma, Alexey Aparin

**Updated**: 2025-11-05T14:02:11Z

**Summary**: We investigate inclusive $J/\psi$ production in proton-proton collisions at tens of GeV $\sqrt{s}$ energy, relevant for forthcoming measurements with the Spin Physics Detector (SPD) at NICA. Simulations are performed using the PEGASUS event generator with transverse-momentum-dependent (TMD) gluon densities, comparing the recent KMR-based KL$'2025$ and CCFM-based LLM$'2024$ parametrizations. Differential cross sections in rapidity and transverse momentum exhibit smooth, stable behavior under renormalization-scale variation, while factorization-scale dependence exposes limitations of the LLM$'2024$ set at low scales in contrast to KL$'2025$. Normalized $p_T$ spectra reveal distinct hardening patterns linked to the underlying gluon $k_T$ broadening in each model. The relative contributions of color-singlet and color-octet channels are also quantified, demonstrating the dominance of color-octet mechanisms in the SPD energy regime. These results provide the first detailed assessment of quarkonium production sensitivity to gluon TMDs near threshold, offering timely theoretical guidance for upcoming $J/\psi$ measurements at SPD/NICA.

**Link**: [arxiv](http://arxiv.org/abs/2511.03477v1),  [pdf](http://arxiv.org/pdf/2511.03477v1)

**Tags**: hep-ph hep-ex 



### RAGBoost: Efficient Retrieval-Augmented Generation with   Accuracy-Preserving Context Reuse
**Authors**: Yinsicheng Jiang, Yeqi Huang, Liang Cheng, Cheng Deng, Xuan Sun, Luo Mai

**Updated**: 2025-11-05T13:59:01Z

**Summary**: Retrieval-augmented generation (RAG) enhances large language models (LLMs) with retrieved context but often suffers from downgraded prefill performance as modern applications demand longer and more complex inputs. Existing caching techniques either preserve accuracy with low cache reuse or improve reuse at the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG system that achieves high cache reuse without sacrificing accuracy through accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items across concurrent sessions and multi-turn interactions, using efficient context indexing, ordering, and de-duplication to maximize reuse, while lightweight contextual hints maintain reasoning fidelity. It integrates seamlessly with existing LLM inference engines and improves their prefill performance by 1.5-3X over state-of-the-art methods, while preserving or even enhancing reasoning accuracy across diverse RAG and agentic AI workloads. Our code is released at: https://github.com/Edinburgh-AgenticAI/RAGBoost.

**Link**: [arxiv](http://arxiv.org/abs/2511.03475v1),  [pdf](http://arxiv.org/pdf/2511.03475v1)

**Tags**: cs.LG 



### RAG-IT: Retrieval-Augmented Instruction Tuning for Automated Financial   Analysis
**Authors**: Van-Duc Le, Hai-Thien To

**Updated**: 2025-11-05T13:53:51Z

**Summary**: Financial analysis relies heavily on the interpretation of earnings reports to assess company performance and guide decision-making. Traditional methods for generating such analyses demand significant financial expertise and are often time-consuming. With the rapid advancement of Large Language Models (LLMs), domain-specific adaptations have emerged for financial tasks such as sentiment analysis and entity recognition. This paper introduces RAG-IT (Retrieval-Augmented Instruction Tuning), a novel framework designed to automate the generation of earnings report analyses through an LLM fine-tuned specifically for the financial domain. Our approach integrates retrieval augmentation with instruction-based fine-tuning to enhance factual accuracy, contextual relevance, and domain adaptability. We construct a comprehensive financial instruction dataset derived from extensive financial documents and earnings reports to guide the LLM's adaptation to specialized financial reasoning. Experimental results demonstrate that RAG-IT outperforms general-purpose open-source models and achieves performance comparable to commercial systems like GPT-3.5 on financial report generation tasks. This research highlights the potential of retrieval-augmented instruction tuning to streamline and elevate financial analysis automation, advancing the broader field of intelligent financial reporting.

**Link**: [arxiv](http://arxiv.org/abs/2412.08179v2),  [pdf](http://arxiv.org/pdf/2412.08179v2)

**Tags**: q-fin.ST cs.AI 



### Med-Banana-50K: A Cross-modality Large-Scale Dataset for Text-guided   Medical Image Editing
**Authors**: Zhihui Chen, Mengling Feng

**Updated**: 2025-11-05T13:45:24Z

**Summary**: Recent advances in multimodal large language models have enabled remarkable medical image editing capabilities. However, the research community's progress remains constrained by the absence of large-scale, high-quality, and openly accessible datasets built specifically for medical image editing with strict anatomical and clinical constraints. We introduce Med-Banana-50K, a comprehensive 50K-image dataset for instruction-based medical image editing spanning three modalities (chest X-ray, brain MRI, fundus photography) and 23 disease types. Our dataset is constructed by leveraging Gemini-2.5-Flash-Image to generate bidirectional edits (lesion addition and removal) from real medical images. What distinguishes Med-Banana-50K from general-domain editing datasets is our systematic approach to medical quality control: we employ LLM-as-Judge with a medically grounded rubric (instruction compliance, structural plausibility, realism, and fidelity preservation) and history-aware iterative refinement up to five rounds. Beyond single-turn editing, Med-Banana-50K includes 37K failed attempts with full conversation logs for preference learning and alignment research. By providing this large-scale, medically validated, and fully documented resource, Med-Banana-50K establishes a foundation for training and evaluating the next generation of medical image editing models.Our dataset and code are publicly available at [https://github.com/richardChenzhihui/med-banana-50k].

**Link**: [arxiv](http://arxiv.org/abs/2511.00801v2),  [pdf](http://arxiv.org/pdf/2511.00801v2)

**Tags**: cs.CV cs.MM 



### HPLT 3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono-   and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models
**Authors**: Stephan Oepen, Nikolay Arefev, Mikko Aulamo, Marta Bañón, Maja Buljan, Laurie Burchell, Lucas Charpentier, Pinzhen Chen, Mariya Fedorova, Ona de Gibert, Barry Haddow, Jan Hajič, Jindřich Helcl, Andrey Kutuzov, Veronika Laippala, Zihao Li, Risto Luukkonen, Bhavitvya Malik, Vladislav Mikhailov, Amanda Myntti, Dayyán O'Brien, Lucie Poláková, Sampo Pyysalo, Gema Ramírez Sánchez, Janine Siewert, Pavel Stepachev, Jörg Tiedemann, Teemu Vahtola, Dušan Variš, Fedor Vitiugin, Tea Vojtěchová, Jaume Zaragoza

**Updated**: 2025-11-05T13:19:47Z

**Summary**: We present an ongoing initiative to provide open, very large, high-quality, and richly annotated textual datasets for almost 200 languages. At 30 trillion tokens, this is likely the largest generally available multilingual collection of LLM pre-training data. These datasets are derived from web crawls from different sources and accompanied with a complete, open-source pipeline for document selection from web archives, text extraction from HTML, language identification for noisy texts, exact and near-deduplication, annotation with, among others, register labels, text quality estimates, and personally identifiable information; and final selection and filtering. We report on data quality probes through contrastive and analytical statistics, through manual inspection of samples for 24 languages, and through end-to-end evaluation of various language model architectures trained on this data. For multilingual LLM evaluation, we provide a comprehensive collection of benchmarks for nine European languages, with special emphasis on natively created tasks, mechanisms to mitigate prompt sensitivity, and refined normalization and aggregation of scores. Additionally, we train and evaluate a family of 57 monolingual encoder-decoder models, as well as a handful of monolingual GPT-like reference models. Besides the monolingual data and models, we also present a very large collection of parallel texts automatically mined from this data, together with a novel parallel corpus synthesized via machine translation.

**Link**: [arxiv](http://arxiv.org/abs/2511.01066v2),  [pdf](http://arxiv.org/pdf/2511.01066v2)

**Tags**: cs.CL 



### (Approximate) Matrix Multiplication via Convolutions
**Authors**: Yahel Uffenheimer, Omri Weinstein

**Updated**: 2025-11-05T13:05:52Z

**Summary**: We study the capability of the Fast Fourier Transform (FFT) to accelerate exact and approximate matrix multiplication without using Strassen-like divide-and-conquer. We present a simple exact algorithm running in $O(n^{2.89})$ time, which only sums a few convolutions (FFTs) in $\mathbb{Z}_{m}^{k}$, building on the work of Cohn, Kleinberg, Szegedy and Umans (2005). As a corollary, combining this algorithm with linear sketching breaks the longstanding linear speed-accuracy tradeoff for "combinatorial" approximate matrix multiplication (AMM, Pagh'13, Sarlos'06, Clarkson-Woodruff'13), achieving error $\frac{1}{r^{1.1}}\left\lVert \mathbf{A} \right\rVert_{F}^{2}\left\lVert \mathbf{B}\right\rVert_{F}^{2}$ in $O(rn^{2})$ time, using nothing but FFTs.   Motivated by the rich literature for approximating polynomials, our main contribution in this paper is extending the group-theoretic framework of Cohn and Umans (2003) to approximate matrix multiplication (AMM). Specifically, we introduce and study an approximate notion of the Triple Product Property, which in the abelian case is equivalent to finding a Sumset which minimizes (multi-)intersections with an arithmetic progression. We prove tight bounds on this quantity for abelian groups (yielding a simple and practical AMM algorithm via polynomial multiplication), and establish a weaker lower bound for non-abelian groups, extending a lemma of Gowers. Finally, we propose a concrete approach that uses low-degree approximation of multi-variate polynomials for AMM, which we believe will lead to practical, non-asymptotic AMM algorithms in real-world applications, most notably LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2510.22193v2),  [pdf](http://arxiv.org/pdf/2510.22193v2)

**Tags**: cs.DS cs.DM 



### CareMedEval dataset: Evaluating Critical Appraisal and Reasoning in the   Biomedical Field
**Authors**: Doria Bonzi, Alexandre Guiggi, Frédéric Béchet, Carlos Ramisch, Benoit Favre

**Updated**: 2025-11-05T13:02:06Z

**Summary**: Critical appraisal of scientific literature is an essential skill in the biomedical field. While large language models (LLMs) can offer promising support in this task, their reliability remains limited, particularly for critical reasoning in specialized domains. We introduce CareMedEval, an original dataset designed to evaluate LLMs on biomedical critical appraisal and reasoning tasks. Derived from authentic exams taken by French medical students, the dataset contains 534 questions based on 37 scientific articles. Unlike existing benchmarks, CareMedEval explicitly evaluates critical reading and reasoning grounded in scientific papers. Benchmarking state-of-the-art generalist and biomedical-specialized LLMs under various context conditions reveals the difficulty of the task: open and commercial models fail to exceed an Exact Match Rate of 0.5 even though generating intermediate reasoning tokens considerably improves the results. Yet, models remain challenged especially on questions about study limitations and statistical analysis. CareMedEval provides a challenging benchmark for grounded reasoning, exposing current LLM limitations and paving the way for future development of automated support for critical appraisal.

**Link**: [arxiv](http://arxiv.org/abs/2511.03441v1),  [pdf](http://arxiv.org/pdf/2511.03441v1)

**Tags**: cs.CL cs.AI 



### Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof,   Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2,   ERC-8004, and Beyond
**Authors**: Botao 'Amber' Hu, Helena Rong

**Updated**: 2025-11-05T12:50:06Z

**Summary**: As the "agentic web" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 "Trustless Agents," yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.

**Link**: [arxiv](http://arxiv.org/abs/2511.03434v1),  [pdf](http://arxiv.org/pdf/2511.03434v1)

**Tags**: cs.HC cs.AI cs.MA cs.NI cs.SI 



### Light over Heavy: Automated Performance Requirements Quantification with   Linguistic Inducement
**Authors**: Shihai Wang, Tao Chen

**Updated**: 2025-11-05T12:38:11Z

**Summary**: Elicited performance requirements need to be quantified for compliance in different engineering tasks, e.g., configuration tuning and performance testing. Much existing work has relied on manual quantification, which is expensive and error-prone due to the imprecision. In this paper, we present LQPR, a highly efficient automatic approach for performance requirements quantification.LQPR relies on a new theoretical framework that converts quantification as a classification problem. Despite the prevalent applications of Large Language Models (LLMs) for requirement analytics, LQPR takes a different perspective to address the classification: we observed that performance requirements can exhibit strong patterns and are often short/concise, therefore we design a lightweight linguistically induced matching mechanism. We compare LQPR against nine state-of-the-art learning-based approaches over diverse datasets, demonstrating that it is ranked as the sole best for 75% or more cases with two orders less cost. Our work proves that, at least for performance requirement quantification, specialized methods can be more suitable than the general LLM-driven approaches.

**Link**: [arxiv](http://arxiv.org/abs/2511.03421v1),  [pdf](http://arxiv.org/pdf/2511.03421v1)

**Tags**: cs.SE cs.AI 



### Knowledge-Augmented Question Error Correction for Chinese Question   Answer System with QuestionRAG
**Authors**: Longpeng Qiu, Ting Li, Shuai Mao, Nan Yang, Xiaohui Yan

**Updated**: 2025-11-05T12:24:20Z

**Summary**: Input errors in question-answering (QA) systems often lead to incorrect responses. Large language models (LLMs) struggle with this task, frequently failing to interpret user intent (misinterpretation) or unnecessarily altering the original question's structure (over-correction). We propose QuestionRAG, a framework that tackles these problems. To address misinterpretation, it enriches the input with external knowledge (e.g., search results, related entities). To prevent over-correction, it uses reinforcement learning (RL) to align the model's objective with precise correction, not just paraphrasing. Our results demonstrate that knowledge augmentation is critical for understanding faulty questions. Furthermore, RL-based alignment proves significantly more effective than traditional supervised fine-tuning (SFT), boosting the model's ability to follow instructions and generalize. By integrating these two strategies, QuestionRAG unlocks the full potential of LLMs for the question correction task.

**Link**: [arxiv](http://arxiv.org/abs/2511.03410v1),  [pdf](http://arxiv.org/pdf/2511.03410v1)

**Tags**: cs.CL 



### Efficient Reasoning via Thought-Training and Thought-Free Inference
**Authors**: Canhui Wu, Qiong Cao, Chao Xue, Wei Xi, Xiaodong He

**Updated**: 2025-11-05T12:20:45Z

**Summary**: Recent advances in large language models (LLMs) have leveraged explicit Chain-of-Thought (CoT) prompting to improve reasoning accuracy. However, most existing methods primarily compress verbose reasoning outputs. These Long-to-Short transformations aim to improve efficiency, but still rely on explicit reasoning during inference. In this work, we introduce \textbf{3TF} (\textbf{T}hought-\textbf{T}raining and \textbf{T}hought-\textbf{F}ree inference), a framework for efficient reasoning that takes a Short-to-Long perspective. We first train a hybrid model that can operate in both reasoning and non-reasoning modes, and then further train it on CoT-annotated data to internalize structured reasoning, while enforcing concise, thought-free outputs at inference time using the no-reasoning mode. Unlike compression-based approaches, 3TF improves the reasoning quality of non-reasoning outputs, enabling models to perform rich internal reasoning implicitly while keeping external outputs short. Empirically, 3TF-trained models obtain large improvements on reasoning benchmarks under thought-free inference, demonstrating that high quality reasoning can be learned and executed implicitly without explicit step-by-step generation.

**Link**: [arxiv](http://arxiv.org/abs/2511.03408v1),  [pdf](http://arxiv.org/pdf/2511.03408v1)

**Tags**: cs.CL I.2.7 



### Towards Realistic Project-Level Code Generation via Multi-Agent   Collaboration and Semantic Architecture Modeling
**Authors**: Qianhui Zhao, Li Zhang, Fang Liu, Junhang Cheng, Chengru Wu, Junchen Ai, Qiaoyuanhe Meng, Lichen Zhang, Xiaoli Lian, Shubin Song, Yuanping Guo

**Updated**: 2025-11-05T12:12:35Z

**Summary**: In recent years, Large Language Models (LLMs) have achieved remarkable progress in automated code generation. In real-world software engineering, the growing demand for rapid iteration and continuous delivery underscores the importance of project-level code generation, where LLMs are expected to generate complete software projects directly from complex user requirements. Although existing studies have made initial explorations, they still face key limitations, including unrealistic datasets and unreliable evaluation metrics that fail to reflect real-world complexity, the semantic gap between human-written requirements and machine-interpretable structures, and difficulties in managing hierarchical dependencies and maintaining quality throughout the generation process. To address these limitations, we first introduce CodeProjectEval, a project-level code generation dataset built from 18 real-world repositories with 12.7 files and 2,388.6 lines of code per task on average, supplemented with documentation and executable test cases for automatic evaluation. We further propose ProjectGen, a multi-agent framework that decomposes projects into architecture design, skeleton generation, and code filling stages with iterative refinement and memory-based context management. Within this framework, we introduce the Semantic Software Architecture Tree (SSAT), a structured and semantically rich representation that effectively bridges user requirements and source code implementation. Experiments show that ProjectGen achieves state-of-the-art performance, passing 52/124 test cases on the small-scale project-level code generation dataset DevBench, a 57% improvement over the baseline approaches, and 310 test cases on CodeProjectEval, representing an improvement of roughly tenfold compared to the baselines.

**Link**: [arxiv](http://arxiv.org/abs/2511.03404v1),  [pdf](http://arxiv.org/pdf/2511.03404v1)

**Tags**: cs.SE 



### Performance Analysis of Wireless-Powered Pinching Antenna Systems
**Authors**: Kunrui Cao, Jingyu Chen, Panagiotis D. Diamantoulakis, Lei Zhou, Xingwang Li, Yuanwei Liu, George K. Karagiannidis

**Updated**: 2025-11-05T12:08:08Z

**Summary**: Pinching antenna system (PAS) serves as a groundbreaking paradigm that enhances wireless communications by flexibly adjusting the position of pinching antenna (PA) and establishing a strong line-of-sight (LoS) link, thereby reducing the free-space path loss. This paper introduces the concept of wireless-powered PAS, and investigates the reliability of wireless-powered PAS to explore the advantages of PA in improving the performance of wireless-powered communication (WPC) system. In addition, we derive the closed-form expressions of outage probability and ergodic rate for the practical lossy waveguide case and ideal lossless waveguide case, respectively, and analyze the optimal deployment of waveguides and user to provide valuable insights for guiding their deployments. The results show that an increase in the absorption coefficient and in the dimensions of the user area leads to higher in-waveguide and free-space propagation losses, respectively, which in turn increase the outage probability and reduce the ergodic rate of the wireless-powered PAS. However, the performance of wireless-powered PAS is severely affected by the absorption coefficient and the waveguide length, e.g., under conditions of high absorption coefficient and long waveguide, the outage probability of wireless-powered PAS is even worse than that of traditional WPC system. While the ergodic rate of wireless-powered PAS is better than that of traditional WPC system under conditions of high absorption coefficient and long waveguide. Interestingly, the wireless-powered PAS has the optimal time allocation factor and optimal distance between power station (PS) and access point (AP) to minimize the outage probability or maximize the ergodic rate. Moreover, the system performance of PS and AP separated at the optimal distance between PS and AP is superior to that of PS and AP integrated into a hybrid access point.

**Link**: [arxiv](http://arxiv.org/abs/2511.03401v1),  [pdf](http://arxiv.org/pdf/2511.03401v1)

**Tags**: eess.SP H.1 



### GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained   Robot Policy Enhancement
**Authors**: Minquan Gao, Xinyi Li, Qing Yan, Xiaojian Sun, Xiaopan Zhang, Chien-Ming Huang, Jiachen Li

**Updated**: 2025-11-05T12:08:05Z

**Summary**: Pre-trained robot policies serve as the foundation of many validated robotic systems, which encapsulate extensive embodied knowledge. However, they often lack the semantic awareness characteristic of foundation models, and replacing them entirely is impractical in many situations due to high costs and the loss of accumulated knowledge. To address this gap, we introduce GUIDES, a lightweight framework that augments pre-trained policies with semantic guidance from foundation models without requiring architectural redesign. GUIDES employs a fine-tuned vision-language model (Instructor) to generate contextual instructions, which are encoded by an auxiliary module into guidance embeddings. These embeddings are injected into the policy's latent space, allowing the legacy model to adapt to this new semantic input through brief, targeted fine-tuning. For inference-time robustness, a large language model-based Reflector monitors the Instructor's confidence and, when confidence is low, initiates a reasoning loop that analyzes execution history, retrieves relevant examples, and augments the VLM's context to refine subsequent actions. Extensive validation in the RoboCasa simulation environment across diverse policy architectures shows consistent and substantial improvements in task success rates. Real-world deployment on a UR5 robot further demonstrates that GUIDES enhances motion precision for critical sub-tasks such as grasping. Overall, GUIDES offers a practical and resource-efficient pathway to upgrade, rather than replace, validated robot policies.

**Link**: [arxiv](http://arxiv.org/abs/2511.03400v1),  [pdf](http://arxiv.org/pdf/2511.03400v1)

**Tags**: cs.RO 



### Distilling LLM Agent into Small Models with Retrieval and Code Tools
**Authors**: Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang

**Updated**: 2025-11-05T11:42:56Z

**Summary**: Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents. Our code is available at https://github.com/Nardien/agent-distillation.

**Link**: [arxiv](http://arxiv.org/abs/2505.17612v2),  [pdf](http://arxiv.org/pdf/2505.17612v2)

**Tags**: cs.CL cs.AI 



### SpatialLM: Training Large Language Models for Structured Indoor Modeling
**Authors**: Yongsen Mao, Junhao Zhong, Chuan Fang, Jia Zheng, Rui Tang, Hao Zhu, Ping Tan, Zihan Zhou

**Updated**: 2025-11-05T11:34:39Z

**Summary**: SpatialLM is a large language model designed to process 3D point cloud data and generate structured 3D scene understanding outputs. These outputs include architectural elements like walls, doors, windows, and oriented object boxes with their semantic categories. Unlike previous methods which exploit task-specific network designs, our model adheres to the standard multimodal LLM architecture and is fine-tuned directly from open-source LLMs.   To train SpatialLM, we collect a large-scale, high-quality synthetic dataset consisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with ground-truth 3D annotations, and conduct a careful study on various modeling and training decisions. On public benchmarks, our model gives state-of-the-art performance in layout estimation and competitive results in 3D object detection. With that, we show a feasible path for enhancing the spatial understanding capabilities of modern LLMs for applications in augmented reality, embodied robotics, and more.

**Link**: [arxiv](http://arxiv.org/abs/2506.07491v2),  [pdf](http://arxiv.org/pdf/2506.07491v2)

**Tags**: cs.CV 



### Computational Imaging Meets LLMs: Zero-Shot IDH Mutation Prediction in   Brain Gliomas
**Authors**: Syed Muqeem Mahmood, Hassan Mohy-ud-Din

**Updated**: 2025-11-05T11:31:08Z

**Summary**: We present a framework that combines Large Language Models with computational image analytics for non-invasive, zero-shot prediction of IDH mutation status in brain gliomas. For each subject, coregistered multi-parametric MRI scans and multi-class tumor segmentation maps were processed to extract interpretable semantic (visual) attributes and quantitative features, serialized in a standardized JSON file, and used to query GPT 4o and GPT 5 without fine-tuning. We evaluated this framework on six publicly available datasets (N = 1427) and results showcased high accuracy and balanced classification performance across heterogeneous cohorts, even in the absence of manual annotations. GPT 5 outperformed GPT 4o in context-driven phenotype interpretation. Volumetric features emerged as the most important predictors, supplemented by subtype-specific imaging markers and clinical information. Our results demonstrate the potential of integrating LLM-based reasoning with computational image analytics for precise, non-invasive tumor genotyping, advancing diagnostic strategies in neuro-oncology. The code is available at https://github.com/ATPLab-LUMS/CIM-LLM.

**Link**: [arxiv](http://arxiv.org/abs/2511.03376v1),  [pdf](http://arxiv.org/pdf/2511.03376v1)

**Tags**: eess.IV cs.AI q-bio.QM 



### Divide by Question, Conquer by Agent: SPLIT-RAG with Question-Driven   Graph Partitioning
**Authors**: Ruiyi Yang, Hao Xue, Imran Razzak, Shirui Pan, Hakim Hacid, Flora D. Salim

**Updated**: 2025-11-05T11:26:59Z

**Summary**: Retrieval-Augmented Generation (RAG) systems empower large language models (LLMs) with external knowledge, yet struggle with efficiency-accuracy trade-offs when scaling to large knowledge graphs. Existing approaches often rely on monolithic graph retrieval, incurring unnecessary latency for simple queries and fragmented reasoning for complex multi-hop questions. To address these challenges, this paper propose SPLIT-RAG, a multi-agent RAG framework that addresses these limitations with question-driven semantic graph partitioning and collaborative subgraph retrieval. The innovative framework first create Semantic Partitioning of Linked Information, then use the Type-Specialized knowledge base to achieve Multi-Agent RAG. The attribute-aware graph segmentation manages to divide knowledge graphs into semantically coherent subgraphs, ensuring subgraphs align with different query types, while lightweight LLM agents are assigned to partitioned subgraphs, and only relevant partitions are activated during retrieval, thus reduce search space while enhancing efficiency. Finally, a hierarchical merging module resolves inconsistencies across subgraph-derived answers through logical verifications. Extensive experimental validation demonstrates considerable improvements compared to existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2505.13994v2),  [pdf](http://arxiv.org/pdf/2505.13994v2)

**Tags**: cs.AI cs.IR cs.MA 



### LFC-DA: Logical Formula-Controlled Data Augmentation for Enhanced   Logical Reasoning
**Authors**: Shenghao Li

**Updated**: 2025-11-05T11:26:38Z

**Summary**: For complex logical data augmentation, heavy reliance on human annotation is costly, whereas direct generation with large language models yields uninterpretable and logically homogeneous examples. To address this, we present LFC-DA, a symbolic-logic-controlled pipeline: logical text is first mapped to propositional expressions, a compact rule library is compiled, and a bounded state-space search systematically discovers valid formulas that are then verbalized back into natural-language questions, ensuring both diversity and logical rigor under propositional logic. Experiments on ReClor and LogiQA show significant improvements in the logical-reasoning accuracy of pretrained models, confirming the effectiveness of LFC-DA for LLM-guided logical data augmentation.

**Link**: [arxiv](http://arxiv.org/abs/2511.03372v1),  [pdf](http://arxiv.org/pdf/2511.03372v1)

**Tags**: cs.CL I.2.7; I.2.6; F.4.1 



### EQ-Negotiator: Dynamic Emotional Personas Empower Small Language Models   for Edge-Deployable Credit Negotiation
**Authors**: Yunbo Long, Yuhan Liu, Alexandra Brintrup

**Updated**: 2025-11-05T11:25:07Z

**Summary**: The deployment of large language models (LLMs) in automated negotiation has set a high performance benchmark, but their computational cost and data privacy requirements render them unsuitable for many privacy-sensitive, on-device applications such as mobile assistants, embodied AI agents or private client interactions. While small language models (SLMs) offer a practical alternative, they suffer from a significant performance gap compared to LLMs in playing emotionally charged complex personas, especially for credit negotiation. This paper introduces EQ-Negotiator, a novel framework that bridges this capability gap using emotional personas. Its core is a reasoning system that integrates game theory with a Hidden Markov Model(HMM) to learn and track debtor emotional states online, without pre-training. This allows EQ-Negotiator to equip SLMs with the strategic intelligence to counter manipulation while de-escalating conflict and upholding ethical standards. Through extensive agent-to-agent simulations across diverse credit negotiation scenarios, including adversarial debtor strategies like cheating, threatening, and playing the victim, we show that a 7B parameter language model with EQ-Negotiator achieves better debt recovery and negotiation efficiency than baseline LLMs more than 10 times its size. This work advances persona modeling from descriptive character profiles to dynamic emotional architectures that operate within privacy constraints. Besides, this paper establishes that strategic emotional intelligence, not raw model scale, is the critical factor for success in automated negotiation, paving the way for effective, ethical, and privacy-preserving AI negotiators that can operate on the edge.

**Link**: [arxiv](http://arxiv.org/abs/2511.03370v1),  [pdf](http://arxiv.org/pdf/2511.03370v1)

**Tags**: cs.CL 



### Beyond Single Pass, Looping Through Time: KG-IRAG with Iterative   Knowledge Retrieval
**Authors**: Ruiyi Yang, Hao Xue, Imran Razzak, Hakim Hacid, Flora D. Salim

**Updated**: 2025-11-05T11:24:50Z

**Summary**: Graph Retrieval-Augmented Generation (GraphRAG) has proven highly effective in enhancing the performance of Large Language Models (LLMs) on tasks that require external knowledge. By leveraging Knowledge Graphs (KGs), GraphRAG improves information retrieval for complex reasoning tasks, providing more precise and comprehensive retrieval and generating more accurate responses to QAs. However, most RAG methods fall short in addressing multi-step reasoning, particularly when both information extraction and inference are necessary. To address this limitation, this paper presents Knowledge Graph-Based Iterative Retrieval-Augmented Generation (KG-IRAG), a novel framework that integrates KGs with iterative reasoning to improve LLMs' ability to handle queries involving temporal and logical dependencies. Through iterative retrieval steps, KG-IRAG incrementally gathers relevant data from external KGs, enabling step-by-step reasoning. The proposed approach is particularly suited for scenarios where reasoning is required alongside dynamic temporal data extraction, such as determining optimal travel times based on weather conditions or traffic patterns. Experimental results show that KG-IRAG improves accuracy in complex reasoning tasks by effectively integrating external knowledge with iterative, logic-based retrieval. Additionally, three new datasets: weatherQA-Irish, weatherQA-Sydney, and trafficQA-TFNSW, are formed to evaluate KG-IRAG's performance, demonstrating its potential beyond traditional RAG applications.

**Link**: [arxiv](http://arxiv.org/abs/2503.14234v4),  [pdf](http://arxiv.org/pdf/2503.14234v4)

**Tags**: cs.AI cs.MA 



### Silenced Biases: The Dark Side LLMs Learned to Refuse
**Authors**: Rom Himelstein, Amit LeVi, Brit Youngmann, Yaniv Nemcovsky, Avi Mendelson

**Updated**: 2025-11-05T11:24:50Z

**Summary**: Safety-aligned large language models (LLMs) are becoming increasingly widespread, especially in sensitive applications where fairness is essential and biased outputs can cause significant harm. However, evaluating the fairness of models is a complex challenge, and approaches that do so typically utilize standard question-answer (QA) styled schemes. Such methods often overlook deeper issues by interpreting the model's refusal responses as positive fairness measurements, which creates a false sense of fairness. In this work, we introduce the concept of silenced biases, which are unfair preferences encoded within models' latent space and are effectively concealed by safety-alignment. Previous approaches that considered similar indirect biases often relied on prompt manipulation or handcrafted implicit queries, which present limited scalability and risk contaminating the evaluation process with additional biases. We propose the Silenced Bias Benchmark (SBB), which aims to uncover these biases by employing activation steering to reduce model refusals during QA. SBB supports easy expansion to new demographic groups and subjects, presenting a fairness evaluation framework that encourages the future development of fair models and tools beyond the masking effects of alignment training. We demonstrate our approach over multiple LLMs, where our findings expose an alarming distinction between models' direct responses and their underlying fairness issues.

**Link**: [arxiv](http://arxiv.org/abs/2511.03369v1),  [pdf](http://arxiv.org/pdf/2511.03369v1)

**Tags**: cs.CL stat.ML 



### Lightwave Power Transfer-Enabled Underwater Optical ISAC Systems under   Ship Attitude Variation
**Authors**: Kapila W. S. Palitharathna, Constantinos Psomas, Ioannis Krikidis

**Updated**: 2025-11-05T11:10:13Z

**Summary**: In this paper, we propose a lightwave power transfer-enabled underwater optical integrated sensing and communication (O-ISAC) system, where an access point (AP) mounted on a seasurface ship transmits lightwave signals to two nodes, namely ($i$) a seabed sensor that harvests energy and transmits uplink information to the AP, and ($ii$) a sensing target whose position is estimated by the AP using an array of pinhole cameras. To capture practical deployment conditions, the ship attitude variation is modeled through its roll, pitch, and yaw angles, each following a Gaussian distribution under low-to-moderate sea states. Closed-form approximations are derived for the mean squared error (MSE) of target localization and the achievable uplink data rate. Analytical and simulation results demonstrate excellent agreement, validating the proposed models and derived expressions, while revealing the fundamental communication-sensing tradeoff in the O-ISAC system. The results further provide valuable design insights, including the optimal camera placement on the ship to minimize localization error, achieving a minimum MSE of $10^{-2}$ $\text{m}^2$ with multiple cameras under roll, pitch, and yaw angle variation of $10^{\circ}$, and the optimal harvest-use ratio of $0.55$ for the considered setup.

**Link**: [arxiv](http://arxiv.org/abs/2511.03366v1),  [pdf](http://arxiv.org/pdf/2511.03366v1)

**Tags**: eess.SY cs.SY eess.SP 



### A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in   Transportation Agentic AI Applications
**Authors**: Xiaocai Zhang, Hur Lim, Ke Wang, Zhe Xiao, Jing Wang, Kelvin Lee, Xiuju Fu, Zheng Qin

**Updated**: 2025-11-05T11:08:08Z

**Summary**: In this study, a modular, data-free pipeline for multi-label intention recognition is proposed for agentic AI applications in transportation. Unlike traditional intent recognition systems that depend on large, annotated corpora and often struggle with fine-grained, multi-label discrimination, our approach eliminates the need for costly data collection while enhancing the accuracy of multi-label intention understanding. Specifically, the overall pipeline, named DMTC, consists of three steps: 1) using prompt engineering to guide large language models (LLMs) to generate diverse synthetic queries in different transport scenarios; 2) encoding each textual query with a Sentence-T5 model to obtain compact semantic embeddings; 3) training a lightweight classifier using a novel online focal-contrastive (OFC) loss that emphasizes hard samples and maximizes inter-class separability. The applicability of the proposed pipeline is demonstrated in an agentic AI application in the maritime transportation context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35% and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that Sentence-T5 embeddings improve subset accuracy by at least 3.29% over alternative encoders, and integrating the OFC loss yields an additional 0.98% gain compared to standard contrastive objectives. In conclusion, our system seamlessly routes user queries to task-specific modules (e.g., ETA information, traffic risk evaluation, and other typical scenarios in the transportation domain), laying the groundwork for fully autonomous, intention-aware agents without costly manual labelling.

**Link**: [arxiv](http://arxiv.org/abs/2511.03363v1),  [pdf](http://arxiv.org/pdf/2511.03363v1)

**Tags**: cs.LG 



### Open Source State-Of-the-Art Solution for Romanian Speech Recognition
**Authors**: Gabriel Pirlogeanu, Alexandru-Lucian Georgescu, Horia Cucu

**Updated**: 2025-11-05T11:02:16Z

**Summary**: In this work, we present a new state-of-the-art Romanian Automatic Speech Recognition (ASR) system based on NVIDIA's FastConformer architecture--explored here for the first time in the context of Romanian. We train our model on a large corpus of, mostly, weakly supervised transcriptions, totaling over 2,600 hours of speech. Leveraging a hybrid decoder with both Connectionist Temporal Classification (CTC) and Token-Duration Transducer (TDT) branches, we evaluate a range of decoding strategies including greedy, ALSD, and CTC beam search with a 6-gram token-level language model. Our system achieves state-of-the-art performance across all Romanian evaluation benchmarks, including read, spontaneous, and domain-specific speech, with up to 27% relative WER reduction compared to previous best-performing systems. In addition to improved transcription accuracy, our approach demonstrates practical decoding efficiency, making it suitable for both research and deployment in low-latency ASR applications.

**Link**: [arxiv](http://arxiv.org/abs/2511.03361v1),  [pdf](http://arxiv.org/pdf/2511.03361v1)

**Tags**: eess.AS cs.AI 



### A Survey on Collaborating Small and Large Language Models for   Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness
**Authors**: Fali Wang, Jihai Chen, Shuhua Yang, Ali Al-Lawati, Linli Tang, Hui Liu, Suhang Wang

**Updated**: 2025-11-05T10:30:09Z

**Summary**: Large language models (LLMs) have achieved remarkable progress across domains and applications but face challenges such as high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), with compact, efficient, and adaptable features, offer promising solutions. Building on this potential, recent research explores collaborative frameworks that integrate their complementary strengths, leveraging SLMs' specialization and efficiency with LLMs' generalization and reasoning to address diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration from the perspective of collaboration objectives. We propose a taxonomy covering four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Under this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient and secure SLM-LLM collaboration. The collected papers are available at https://github.com/FairyFali/SLMs-Survey.

**Link**: [arxiv](http://arxiv.org/abs/2510.13890v2),  [pdf](http://arxiv.org/pdf/2510.13890v2)

**Tags**: cs.CL cs.AI 68T50 (Primary) 68T07 (Secondary) I.2.7 



### Decentralized Aerial Manipulation of a Cable-Suspended Load using   Multi-Agent Reinforcement Learning
**Authors**: Jack Zeng, Andreu Matoses Gimenez, Eugene Vinitsky, Javier Alonso-Mora, Sihao Sun

**Updated**: 2025-11-05T10:20:43Z

**Summary**: This paper presents the first decentralized method to enable real-world 6-DoF manipulation of a cable-suspended load using a team of Micro-Aerial Vehicles (MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train an outer-loop control policy for each MAV. Unlike state-of-the-art controllers that utilize a centralized scheme, our policy does not require global states, inter-MAV communications, nor neighboring MAV information. Instead, agents communicate implicitly through load pose observations alone, which enables high scalability and flexibility. It also significantly reduces computing costs during inference time, enabling onboard deployment of the policy. In addition, we introduce a new action space design for the MAVs using linear acceleration and body rates. This choice, combined with a robust low-level controller, enables reliable sim-to-real transfer despite significant uncertainties caused by cable tension during dynamic 3D motion. We validate our method in various real-world experiments, including full-pose control under load model uncertainties, showing setpoint tracking performance comparable to the state-of-the-art centralized method. We also demonstrate cooperation amongst agents with heterogeneous control policies, and robustness to the complete in-flight loss of one MAV. Videos of experiments: https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl

**Link**: [arxiv](http://arxiv.org/abs/2508.01522v3),  [pdf](http://arxiv.org/pdf/2508.01522v3)

**Tags**: cs.RO cs.AI cs.MA I.2.9; I.2.11; I.2.6 



### The ORCA Benchmark: Evaluating Real-World Calculation Accuracy in Large   Language Models
**Authors**: Claudia Herambourg, Dawid Siuda, Julia Kopczyńska, Joao R. L. Santos, Wojciech Sas, Joanna Śmietańska-Nowak

**Updated**: 2025-11-05T10:13:35Z

**Summary**: We present ORCA (Omni Research on Calculation in AI) Benchmark - a novel benchmark that evaluates large language models (LLMs) on multi-domain, real-life quantitative reasoning using verified outputs from Omni's calculator engine. In 500 natural-language tasks across domains such as finance, physics, health, and statistics, the five state-of-the-art systems (ChatGPT-5, Gemini~2.5~Flash, Claude~Sonnet~4.5, Grok~4, and DeepSeek~V3.2) achieved only $45\text{--}63\,\%$ accuracy, with errors mainly related to rounding ($35\,\%$) and calculation mistakes ($33\,\%$). Results in specific domains indicate strengths in mathematics and engineering, but weaknesses in physics and natural sciences. Correlation analysis ($r \approx 0.40\text{--}0.65$) shows that the models often fail together but differ in the types of errors they make, highlighting their partial complementarity rather than redundancy. Unlike standard math datasets, ORCA evaluates step-by-step reasoning, numerical precision, and domain generalization across real problems from finance, physics, health, and statistics.

**Link**: [arxiv](http://arxiv.org/abs/2511.02589v2),  [pdf](http://arxiv.org/pdf/2511.02589v2)

**Tags**: cs.AI 



### SurgViVQA: Temporally-Grounded Video Question Answering for Surgical   Scene Understanding
**Authors**: Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque

**Updated**: 2025-11-05T09:40:16Z

**Summary**: Video Question Answering (VideoQA) in the surgical domain aims to enhance intraoperative understanding by enabling AI models to reason over temporally coherent events rather than isolated frames. Current approaches are limited to static image features, and available datasets often lack temporal annotations, ignoring the dynamics critical for accurate procedural interpretation. We propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder to fuse video and question features, capturing temporal cues such as motion and tool--tissue interactions, which a fine-tuned large language model (LLM) then decodes into coherent answers. To evaluate its performance, we curated REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related questions and diagnostic attributes, as well as out-of-template questions with rephrased or semantically altered formulations to assess model robustness. Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset shows that SurgViVQA outperforms existing image-based VQA benchmark models, particularly in keyword accuracy, improving over PitVQA by +11\% on REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions further confirms improved generalizability and robustness to variations in question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework for temporally-aware understanding in surgical VideoQA, enabling AI models to interpret dynamic procedural contexts more effectively. Code and dataset available at https://github.com/madratak/SurgViVQA.

**Link**: [arxiv](http://arxiv.org/abs/2511.03325v1),  [pdf](http://arxiv.org/pdf/2511.03325v1)

**Tags**: cs.CV 



### Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for   Prognosis Prediction in Medical Imaging
**Authors**: Filippo Ruffini, Elena Mulero Ayllon, Linlin Shen, Paolo Soda, Valerio Guarrasi

**Updated**: 2025-11-05T09:33:35Z

**Summary**: Despite the significant potential of Foundation Models (FMs) in medical imaging, their application to prognosis prediction remains challenging due to data scarcity, class imbalance, and task complexity, which limit their clinical adoption. This study introduces the first structured benchmark to assess the robustness and efficiency of transfer learning strategies for FMs compared with convolutional neural networks (CNNs) in predicting COVID-19 patient outcomes from chest X-rays. The goal is to systematically compare finetuning strategies, both classical and parameter efficient, under realistic clinical constraints related to data scarcity and class imbalance, offering empirical guidance for AI deployment in clinical workflows. Four publicly available COVID-19 chest X-ray datasets were used, covering mortality, severity, and ICU admission, with varying sample sizes and class imbalances. CNNs pretrained on ImageNet and FMs pretrained on general or biomedical datasets were adapted using full finetuning, linear probing, and parameter-efficient methods. Models were evaluated under full data and few shot regimes using the Matthews Correlation Coefficient (MCC) and Precision Recall AUC (PR-AUC), with cross validation and class weighted losses. CNNs with full fine-tuning performed robustly on small, imbalanced datasets, while FMs with Parameter-Efficient Fine-Tuning (PEFT), particularly LoRA and BitFit, achieved competitive results on larger datasets. Severe class imbalance degraded PEFT performance, whereas balanced data mitigated this effect. In few-shot settings, FMs showed limited generalization, with linear probing yielding the most stable results. No single fine-tuning strategy proved universally optimal: CNNs remain dependable for low-resource scenarios, whereas FMs benefit from parameter-efficient methods when data are sufficient.

**Link**: [arxiv](http://arxiv.org/abs/2506.18434v2),  [pdf](http://arxiv.org/pdf/2506.18434v2)

**Tags**: cs.CV cs.AI 



### Gaussian Copula-Based Outage Performance Analysis of Fluid Antenna   Systems: Channel Coefficient- or Envelope-Level Correlation Matrix?
**Authors**: Rui Xu, Yinghui Ye, Xiaoli Chu, Guangyue Lu, Farshad Rostami Ghadi, Kai-Kit Wong

**Updated**: 2025-11-05T09:29:17Z

**Summary**: Gaussian copula has been employed to evaluate the outage performance of Fluid Antenna Systems (FAS), with the covariance matrix reflecting the dependence among multivariate normal random variables (RVs). While prior studies approximate this matrix using the channel coefficient correlation matrix from Jake's model, this work instead employs the channel envelope correlation matrix, motivated by the fact that the multivariate normal RVs are generated by transforming correlated channel envelopes. This raises an open question of whether using the coefficient- or envelope-level correlation matrix yields better accuracy in accessing FAS performance. Toward this end, this paper explores the benefits of using the envelope-level correlation matrix under fully correlated Nakagami-m fading, and develops a method for generating such fading channels for Monte Carlo simulations, which serve as a benchmark for validating the theoretical results. Simulation results confirm the effectiveness of the proposed channel modeling approach and demonstrate the superior accuracy of using the envelope-level correlation matrix, particularly in sparse port deployment and low-outage regime.

**Link**: [arxiv](http://arxiv.org/abs/2509.09411v3),  [pdf](http://arxiv.org/pdf/2509.09411v3)

**Tags**: cs.IT math.IT 



### TASU: Text-Only Alignment for Speech Understanding
**Authors**: Jing Peng, Yi Yang, Xu Li, Yu Xi, Quanwei Tang, Yangui Fang, Junjie Li, Kai Yu

**Updated**: 2025-11-05T09:24:48Z

**Summary**: Recent advances in Speech Large Language Models (Speech LLMs) have paved the way for unified architectures across diverse speech understanding tasks. However, prevailing alignment paradigms rely heavily on large-scale audio-text paired data and computationally intensive training, yet often exhibit limited generalization to unseen domains or tasks. To address these limitations, we propose TASU (Text-only Alignment for Speech Understanding), a novel alignment paradigm that can leverage only unpaired text data to guide cross-modal alignment. Experiments show that TASU achieves competitive zero-shot speech recognition. Leveraging this property, it can further function as a pre-training stage in curriculum learning, enhancing domain generalization in speech recognition. Ultimately, TASU can extend its zero-shot generalization to a wide range of speech understanding tasks and notably outperforms prominent Speech LLMs including GLM-4-Voice and Step-Audio on the MMSU benchmark, establishing TASU as an efficient and scalable alignment paradigm for Speech LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2511.03310v1),  [pdf](http://arxiv.org/pdf/2511.03310v1)

**Tags**: eess.AS 



### CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and   Prediction
**Authors**: Jueon Park, Yein Park, Minju Song, Soyon Park, Donghyeon Lee, Seungheun Baek, Jaewoo Kang

**Updated**: 2025-11-05T09:19:17Z

**Summary**: Drug toxicity remains a major challenge in pharmaceutical development. Recent machine learning models have improved in silico toxicity prediction, but their reliance on annotated data and lack of interpretability limit their applicability. This limits their ability to capture organ-specific toxicities driven by complex biological mechanisms. Large language models (LLMs) offer a promising alternative through step-by-step reasoning and integration of textual data, yet prior approaches lack biological context and transparent rationale. To address this issue, we propose CoTox, a novel framework that integrates LLM with chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox combines chemical structure data, biological pathways, and gene ontology (GO) terms to generate interpretable toxicity predictions through step-by-step reasoning. Using GPT-4o, we show that CoTox outperforms both traditional machine learning and deep learning model. We further examine its performance across various LLMs to identify where CoTox is most effective. Additionally, we find that representing chemical structures with IUPAC names, which are easier for LLMs to understand than SMILES, enhances the model's reasoning ability and improves predictive performance. To demonstrate its practical utility in drug development, we simulate the treatment of relevant cell types with drug and incorporated the resulting biological context into the CoTox framework. This approach allow CoTox to generate toxicity predictions aligned with physiological responses, as shown in case study. This result highlights the potential of LLM-based frameworks to improve interpretability and support early-stage drug safety assessment. The code and prompt used in this work are available at https://github.com/dmis-lab/CoTox.

**Link**: [arxiv](http://arxiv.org/abs/2508.03159v2),  [pdf](http://arxiv.org/pdf/2508.03159v2)

**Tags**: cs.LG cs.AI 



### VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for   Scalable and Robust Quantum Machine Learning
**Authors**: Jun Qi, Chao-Han Yang, Pin-Yu Chen, Min-Hsiu Hsieh

**Updated**: 2025-11-05T09:00:09Z

**Summary**: Variational quantum circuits (VQCs) hold promise for quantum machine learning but face challenges in expressivity, trainability, and noise resilience. We propose VQC-MLPNet, a hybrid architecture where a VQC generates the first-layer weights of a classical multilayer perceptron during training, while inference is performed entirely classically. This design preserves scalability, reduces quantum resource demands, and enables practical deployment. We provide a theoretical analysis based on statistical learning and neural tangent kernel theory, establishing explicit risk bounds and demonstrating improved expressivity and trainability compared to purely quantum or existing hybrid approaches. These theoretical insights demonstrate exponential improvements in representation capacity relative to quantum circuit depth and the number of qubits, providing clear computational advantages over standalone quantum circuits and existing hybrid quantum architectures. Empirical results on diverse datasets, including quantum-dot classification and genomic sequence analysis, show that VQC-MLPNet achieves high accuracy and robustness under realistic noise models, outperforming classical and quantum baselines while using significantly fewer trainable parameters.

**Link**: [arxiv](http://arxiv.org/abs/2506.10275v2),  [pdf](http://arxiv.org/pdf/2506.10275v2)

**Tags**: quant-ph cs.LG stat.ML 



### Large Language Models Miss the Multi-Agent Mark
**Authors**: Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie M. Zhang, Elizabeth Black, Michael Luck, Philip Torr, Michael Wooldridge

**Updated**: 2025-11-05T08:55:08Z

**Summary**: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.

**Link**: [arxiv](http://arxiv.org/abs/2505.21298v3),  [pdf](http://arxiv.org/pdf/2505.21298v3)

**Tags**: cs.MA cs.AI cs.LG 



### LexTime: A Benchmark for Temporal Ordering of Legal Events
**Authors**: Claire Barale, Leslie Barrett, Vikram Sunil Bajaj, Michael Rovatsos

**Updated**: 2025-11-05T08:52:45Z

**Summary**: Understanding temporal relationships and accurately reconstructing the event timeline is important for case law analysis, compliance monitoring, and legal summarization. However, existing benchmarks lack specialized language evaluation, leaving a gap in understanding how LLMs handle event ordering in legal contexts. We introduce LexTime, a dataset designed to evaluate LLMs' event ordering capabilities in legal language, consisting of 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations. Our findings show that (1) LLMs are more accurate on legal event ordering than on narrative texts (up to +10.5%); (2) longer input contexts and implicit events boost accuracy, reaching 80.8% for implicit-explicit event pairs; (3) legal linguistic complexities and nested clauses remain a challenge. While performance is promising, specific features of legal texts remain a bottleneck for legal temporal event reasoning, and we propose concrete modeling directions to better address them.

**Link**: [arxiv](http://arxiv.org/abs/2506.04041v2),  [pdf](http://arxiv.org/pdf/2506.04041v2)

**Tags**: cs.CL 



### UMDAM: A Unified Data Layout and DRAM Address Mapping for Heterogenous   NPU-PIM
**Authors**: Hai Huang, Xuhong Qiang, Weisheng Zhao, Chenchen Liu

**Updated**: 2025-11-05T08:44:19Z

**Summary**: Large Language Models (LLMs) are increasingly deployed on edge devices with Neural Processing Units (NPUs), yet the decode phase remains memory-intensive, limiting performance. Processing-in-Memory (PIM) offers a promising solution, but co-executing NPU-PIM systems face challenges such as data layout mismatches, bandwidth loss, and redundant storage. To address these issues, we propose UMDAM, a unified memory-affinity data layout and DRAM address mapping scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major, tile-based layout and a configurable DRAM mapping strategy to ensure compatibility with NPU computation while maximizing PIM efficiency -- without introducing extra memory overhead or bandwidth loss. Comprehensive evaluations on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving end-to-end LLM inference efficiency on edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2511.03293v1),  [pdf](http://arxiv.org/pdf/2511.03293v1)

**Tags**: cs.DC 



### Decentralized Federated Learning with Distributed Aggregation Weight   Optimization
**Authors**: Zhiyuan Zhai, Xiaojun Yuan, Xin Wang, Geoffrey Ye Li

**Updated**: 2025-11-05T08:26:13Z

**Summary**: Decentralized federated learning (DFL) is an emerging paradigm to enable edge devices collaboratively training a learning model using a device-to-device (D2D) communication manner without the coordination of a parameter server (PS). Aggregation weights, also known as mixing weights, are crucial in DFL process, and impact the learning efficiency and accuracy. Conventional design relies on a so-called central entity to collect all local information and conduct system optimization to obtain appropriate weights. In this paper, we develop a distributed aggregation weight optimization algorithm to align with the decentralized nature of DFL. We analyze convergence by quantitatively capturing the impact of the aggregation weights over decentralized communication networks. Based on the analysis, we then formulate a learning performance optimization problem by designing the aggregation weights to minimize the derived convergence bound. The optimization problem is further transformed as an eigenvalue optimization problem and solved by our proposed subgradient-based algorithm in a distributed fashion. In our algorithm, edge devices only need local information to obtain the optimal aggregation weights through local (D2D) communications, just like the learning itself. Therefore, the optimization, communication, and learning process can be all conducted in a distributed fashion, which leads to a genuinely distributed DFL system. Numerical results demonstrate the superiority of the proposed algorithm in practical DFL deployment.

**Link**: [arxiv](http://arxiv.org/abs/2511.03284v1),  [pdf](http://arxiv.org/pdf/2511.03284v1)

**Tags**: eess.SP 



### Multi-Objective Adaptive Rate Limiting in Microservices Using Deep   Reinforcement Learning
**Authors**: Ning Lyu, Yuxi Wang, Ziyu Cheng, Qingyuan Zhang, Feng Chen

**Updated**: 2025-11-05T08:22:42Z

**Summary**: As cloud computing and microservice architectures become increasingly prevalent, API rate limiting has emerged as a critical mechanism for ensuring system stability and service quality. Traditional rate limiting algorithms, such as token bucket and sliding window, while widely adopted, struggle to adapt to dynamic traffic patterns and varying system loads. This paper proposes an adaptive rate limiting strategy based on deep reinforcement learning that dynamically balances system throughput and service latency. We design a hybrid architecture combining Deep Q-Network (DQN) and Asynchronous Advantage Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a Markov Decision Process. The system continuously monitors microservice states and learns optimal rate limiting policies through environmental interaction. Extensive experiments conducted in a Kubernetes cluster environment demonstrate that our approach achieves 23.7% throughput improvement and 31.4% P99 latency reduction compared to traditional fixed-threshold strategies under high-load scenarios. Results from a 90-day production deployment handling 500 million daily requests validate the practical effectiveness of the proposed method, with 82% reduction in service degradation incidents and 68% decrease in manual interventions.

**Link**: [arxiv](http://arxiv.org/abs/2511.03279v1),  [pdf](http://arxiv.org/pdf/2511.03279v1)

**Tags**: cs.LG 



### The Rise of AI Companions: How Human-Chatbot Relationships Influence   Well-Being
**Authors**: Yutong Zhang, Dora Zhao, Jeffrey T. Hancock, Robert Kraut, Diyi Yang

**Updated**: 2025-11-05T08:17:16Z

**Summary**: As large language models (LLMs)-enhanced chatbots grow increasingly expressive and socially responsive, many users are beginning to form companionship-like bonds with them, particularly with simulated AI partners designed to mimic emotionally attuned interlocutors. These emerging AI companions raise critical questions: Can such systems fulfill social needs typically met by human relationships? How do they shape psychological well-being? And what new risks arise as users develop emotional ties to non-human agents? This study investigates how people interact with AI companions, especially simulated partners on CharacterAI, and how this use is associated with users' psychological well-being. We analyzed survey data from 1,131 users and 4,363 chat sessions (413,509 messages) donated by 244 participants, focusing on three dimensions of use: nature of the interaction, interaction intensity, and self-disclosure. By triangulating self-reports primary motivation, open-ended relationship descriptions, and annotated chat transcripts, we identify patterns in how users engage with AI companions and its associations with well-being. Findings suggest that people with smaller social networks are more likely to turn to chatbots for companionship, but that companionship-oriented chatbot usage is consistently associated with lower well-being, particularly when people use the chatbots more intensively, engage in higher levels of self-disclosure, and lack strong human social support. Even though some people turn to chatbots to fulfill social needs, these uses of chatbots do not fully substitute for human connection. As a result, the psychological benefits may be limited, and the relationship could pose risks for more socially isolated or emotionally vulnerable users.

**Link**: [arxiv](http://arxiv.org/abs/2506.12605v3),  [pdf](http://arxiv.org/pdf/2506.12605v3)

**Tags**: cs.HC 



### Let the Bees Find the Weak Spots: A Path Planning Perspective on   Multi-Turn Jailbreak Attacks against LLMs
**Authors**: Yize Liu, Yunyun Hou, Aina Sui

**Updated**: 2025-11-05T08:05:58Z

**Summary**: Large Language Models (LLMs) have been widely deployed across various applications, yet their potential security and ethical risks have raised increasing concerns. Existing research employs red teaming evaluations, utilizing multi-turn jailbreaks to identify potential vulnerabilities in LLMs. However, these approaches often lack exploration of successful dialogue trajectories within the attack space, and they tend to overlook the considerable overhead associated with the attack process. To address these limitations, this paper first introduces a theoretical model based on dynamically weighted graph topology, abstracting the multi-turn attack process as a path planning problem. Based on this framework, we propose ABC, an enhanced Artificial Bee Colony algorithm for multi-turn jailbreaks, featuring a collaborative search mechanism with employed, onlooker, and scout bees. This algorithm significantly improves the efficiency of optimal attack path search while substantially reducing the average number of queries required. Empirical evaluations on three open-source and two proprietary language models demonstrate the effectiveness of our approach, achieving attack success rates above 90\% across the board, with a peak of 98\% on GPT-3.5-Turbo, and outperforming existing baselines. Furthermore, it achieves comparable success with only 26 queries on average, significantly reducing red teaming overhead and highlighting its superior efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2511.03271v1),  [pdf](http://arxiv.org/pdf/2511.03271v1)

**Tags**: cs.CR cs.CL 



### AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs
**Authors**: Di He, Songjun Tu, Ajay Jaiswal, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin

**Updated**: 2025-11-05T08:04:59Z

**Summary**: Weight decay is a standard regularization technique for training large language models (LLMs). While it is common to assign a uniform decay rate to every layer, this approach overlooks the structural diversity of LLMs and the varying spectral properties across modules. In this paper, we introduce AlphaDecay, a simple yet effective method that adaptively assigns different weight decay strengths to each module of an LLM. Our approach is guided by Heavy-Tailed Self-Regularization (HT-SR) theory, which analyzes the empirical spectral density (ESD) of weight correlation matrices to quantify "heavy-tailedness." Modules exhibiting more pronounced heavy-tailed ESDs, reflecting stronger feature learning, are assigned weaker decay, while modules with lighter-tailed spectra receive stronger decay. Our method leverages tailored weight decay assignments to balance the module-wise differences in spectral properties, leading to improved performance. Extensive pre-training tasks with various model sizes from 60M to 1B demonstrate that AlphaDecay achieves better perplexity and generalization than conventional uniform decay and other adaptive decay baselines. Our code is available at https://github.com/hed-ucas/AlphaDecay.

**Link**: [arxiv](http://arxiv.org/abs/2506.14562v3),  [pdf](http://arxiv.org/pdf/2506.14562v3)

**Tags**: cs.CL cs.AI cs.LG 



### PhysicsEval: Inference-Time Techniques to Improve the Reasoning   Proficiency of Large Language Models on Physics Problems
**Authors**: Oshayer Siddique, J. M Areeb Uzair Alam, Md Jobayer Rahman Rafy, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan

**Updated**: 2025-11-05T07:50:45Z

**Summary**: The discipline of physics stands as a cornerstone of human intellect, driving the evolution of technology and deepening our understanding of the fundamental principles of the cosmos. Contemporary literature includes some works centered on the task of solving physics problems - a crucial domain of natural language reasoning. In this paper, we evaluate the performance of frontier LLMs in solving physics problems, both mathematical and descriptive. We also employ a plethora of inference-time techniques and agentic frameworks to improve the performance of the models. This includes the verification of proposed solutions in a cumulative fashion by other, smaller LLM agents, and we perform a comparative analysis of the performance that the techniques entail. There are significant improvements when the multi-agent framework is applied to problems that the models initially perform poorly on. Furthermore, we introduce a new evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small VAL}}$, consisting of 19,609 problems sourced from various physics textbooks and their corresponding correct solutions scraped from physics forums and educational websites. Our code and data are publicly available at https://github.com/areebuzair/PhysicsEval.

**Link**: [arxiv](http://arxiv.org/abs/2508.00079v2),  [pdf](http://arxiv.org/pdf/2508.00079v2)

**Tags**: cs.CL cs.AI 



### Comparing the Performance of LLMs in RAG-based Question-Answering: A   Case Study in Computer Science Literature
**Authors**: Ranul Dayarathne, Uvini Ranaweera, Upeksha Ganegoda

**Updated**: 2025-11-05T07:45:53Z

**Summary**: Retrieval Augmented Generation (RAG) is emerging as a powerful technique to enhance the capabilities of Generative AI models by reducing hallucination. Thus, the increasing prominence of RAG alongside Large Language Models (LLMs) has sparked interest in comparing the performance of different LLMs in question-answering (QA) in diverse domains. This study compares the performance of four open-source LLMs, Mistral-7b-instruct, LLaMa2-7b-chat, Falcon-7b-instruct and Orca-mini-v3-7b, and OpenAI's trending GPT-3.5 over QA tasks within the computer science literature leveraging RAG support. Evaluation metrics employed in the study include accuracy and precision for binary questions and ranking by a human expert, ranking by Google's AI model Gemini, alongside cosine similarity for long-answer questions. GPT-3.5, when paired with RAG, effectively answers binary and long-answer questions, reaffirming its status as an advanced LLM. Regarding open-source LLMs, Mistral AI's Mistral-7b-instruct paired with RAG surpasses the rest in answering both binary and long-answer questions. However, among the open-source LLMs, Orca-mini-v3-7b reports the shortest average latency in generating responses, whereas LLaMa2-7b-chat by Meta reports the highest average latency. This research underscores the fact that open-source LLMs, too, can go hand in hand with proprietary models like GPT-3.5 with better infrastructure.

**Link**: [arxiv](http://arxiv.org/abs/2511.03261v1),  [pdf](http://arxiv.org/pdf/2511.03261v1)

**Tags**: cs.CL cs.AI I.2.1; I.2.7 



### Auditing M-LLMs for Privacy Risks: A Synthetic Benchmark and Evaluation   Framework
**Authors**: Junhao Li, Jiahao Chen, Zhou Feng, Chunyi Zhou

**Updated**: 2025-11-05T07:23:21Z

**Summary**: Recent advances in multi-modal Large Language Models (M-LLMs) have demonstrated a powerful ability to synthesize implicit information from disparate sources, including images and text. These resourceful data from social media also introduce a significant and underexplored privacy risk: the inference of sensitive personal attributes from seemingly daily media content. However, the lack of benchmarks and comprehensive evaluations of state-of-the-art M-LLM capabilities hinders the research of private attribute profiling on social media. Accordingly, we propose (1) PRISM, the first multi-modal, multi-dimensional and fine-grained synthesized dataset incorporating a comprehensive privacy landscape and dynamic user history; (2) an Efficient evaluation framework that measures the cross-modal privacy inference capabilities of advanced M-LLM. Specifically, PRISM is a large-scale synthetic benchmark designed to evaluate cross-modal privacy risks. Its key feature is 12 sensitive attribute labels across a diverse set of multi-modal profiles, which enables targeted privacy analysis. These profiles are generated via a sophisticated LLM agentic workflow, governed by a prior distribution to ensure they realistically mimic social media users. Additionally, we propose a Multi-Agent Inference Framework that leverages a pipeline of specialized LLMs to enhance evaluation capabilities. We evaluate the inference capabilities of six leading M-LLMs (Qwen, Gemini, GPT-4o, GLM, Doubao, and Grok) on PRISM. The comparison with human performance reveals that these MLLMs significantly outperform in accuracy and efficiency, highlighting the threat of potential privacy risks and the urgent need for robust defenses.

**Link**: [arxiv](http://arxiv.org/abs/2511.03248v1),  [pdf](http://arxiv.org/pdf/2511.03248v1)

**Tags**: cs.CR 



