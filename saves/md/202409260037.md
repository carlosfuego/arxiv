# Arxiv Results
## Keyword: kv cache 
 ### SWARM: Replicating Shared Disaggregated-Memory Data in No Time
**Authors**: Antoine Murat, Cl√©ment Burgelin, Athanasios Xygkis, Igor Zablotchi, Marcos K. Aguilera, Rachid Guerraoui

**Updated**: 2024-09-24T17:28:47Z

**Summary**: Memory disaggregation is an emerging data center architecture that improves resource utilization and scalability. Replication is key to ensure the fault tolerance of applications, but replicating shared data in disaggregated memory is hard. We propose SWARM (Swift WAit-free Replication in disaggregated Memory), the first replication scheme for in-disaggregated-memory shared objects to provide (1) single-roundtrip reads and writes in the common case, (2) strong consistency (linearizability), and (3) strong liveness (wait-freedom). SWARM makes two independent contributions. The first is Safe-Guess, a novel wait-free replication protocol with single-roundtrip operations. The second is In-n-Out, a novel technique to provide conditional atomic update and atomic retrieval of large buffers in disaggregated memory in one roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly consistent and highly available disaggregated key-value store. We evaluate SWARM-KV and find that it has marginal latency overhead compared to an unreplicated key-value store, and that it offers much lower latency and better availability than FUSEE, a state-of-the-art replicated disaggregated key-value store.

**Link**: [arxiv](http://arxiv.org/abs/2409.16258v1),  [pdf](http://arxiv.org/pdf/2409.16258v1)

**Tags**: cs.DC 



### Wind lulls and slews; consequences for the stability of future UK   electricity systems
**Authors**: Anthony D Stephens, David R Walwyn

**Updated**: 2024-09-24T14:16:26Z

**Summary**: As the United Kingdom wind fleet increases in size, wind lulls and slews will increasingly challenge the stability of its electricity system. The paper describes the use of models based on real time records and including solar slews, to investigate the most extreme wind variations likely to be encountered in future, enabling strategies to be devised to mitigate them. Wind lulls are surprisingly frequent, occasionally lasting a week or more, and are always likely to be beyond the capabilities of stored or imported electrical energy to mitigate them. The models indicate that there will be a continuing need for gas powered generation to mitigate wind lulls. Currently, Combined Cycle Gas Turbines (CCGTs) provide most of the dispatchable generation. However, CCGTs are not sufficiently fast acting to cope with the wind and solar slews anticipated in future. The paper suggests that a range of already proven fast-acting sources of dispatchable generation, including Open Cycle Gas Turbines (OCGTs), Internal Combustion Gas-Fired Reciprocating engines (ICGRs) and stored electrical energy systems, should be capable of coping with the largest wind and solar slews likely to be encountered up to the year 2035. Examples are given of the recent introduction of these fast-acting sources of generation which, it is suggested, will progressively replace CCGTs as the wind and solar fleets increase in size. Moreover, we see the pattern of recent investments, summarised in the paper, as a good indication of likely future investments, with OCGT investments mainly serving the 440 kV grid, and ICGRs and stored electrical energy more local networks.

**Link**: [arxiv](http://arxiv.org/abs/2409.16110v1),  [pdf](http://arxiv.org/pdf/2409.16110v1)

**Tags**: eess.SY cs.SY 



### The Bicameral Cache: a split cache for vector architectures
**Authors**: Susana Rebolledo, Borja Perez, Jose Luis Bosque, Peter Hsu

**Updated**: 2024-09-24T11:37:43Z

**Summary**: The Bicameral Cache is a cache organization proposal for a vector architecture that segregates data according to their access type, distinguishing scalar from vector references. Its aim is to avoid both types of references from interfering in each other's data locality, with a special focus on prioritizing the performance on vector references. The proposed system incorporates an additional, non-polluting prefetching mechanism to help populate the long vector cache lines in advance to increase the hit rate by further exploiting the spatial locality on vector data. Its evaluation was conducted on the Cavatools simulator, comparing the performance to a standard conventional cache, over different typical vector benchmarks for several vector lengths. The results proved the proposed cache speeds up performance on stride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition, the prefetching feature consistently provided an additional value.

**Link**: [arxiv](http://arxiv.org/abs/2407.15440v3),  [pdf](http://arxiv.org/pdf/2407.15440v3)

**Tags**: cs.AR cs.PF 



### SEAL: Suite for Evaluating API-use of LLMs
**Authors**: Woojeong Kim, Ashish Jagmohan, Aditya Vempaty

**Updated**: 2024-09-23T20:16:49Z

**Summary**: Large language models (LLMs) have limitations in handling tasks that require real-time access to external APIs. While several benchmarks like ToolBench and APIGen have been developed to assess LLMs' API-use capabilities, they often suffer from issues such as lack of generalizability, limited multi-step reasoning coverage, and instability due to real-time API fluctuations. In this paper, we introduce SEAL, an end-to-end testbed designed to evaluate LLMs in real-world API usage. SEAL standardizes existing benchmarks, integrates an agent system for testing API retrieval and planning, and addresses the instability of real-time APIs by introducing a GPT-4-powered API simulator with caching for deterministic evaluations. Our testbed provides a comprehensive evaluation pipeline that covers API retrieval, API calls, and final responses, offering a reliable framework for structured performance comparison in diverse real-world scenarios. SEAL is publicly available, with ongoing updates for new benchmarks.

**Link**: [arxiv](http://arxiv.org/abs/2409.15523v1),  [pdf](http://arxiv.org/pdf/2409.15523v1)

**Tags**: cs.AI 



### BlockLLM: Multi-tenant Finer-grained Serving for Large Language Models
**Authors**: Bodun Hu, Jiamin Li, Le Xu, Myungjin Lee, Akshay Jajoo, Geon-Woo Kim, Hong Xu, Aditya Akella

**Updated**: 2024-09-23T20:09:28Z

**Summary**: The increasing demand for Large Language Models (LLMs) across various applications has led to a significant shift in the design of deep learning serving systems. Deploying LLMs, particularly in multi-tenant environments, poses substantial challenges due to their high computational and memory demands. We introduce BlockLLM, a serving system that leverages component sharing among fine-tuned LLM models to provide an efficient and flexible solution for LLM workloads. BlockLLM partitions models into finer-grained blocks, enabling the reuse of model components and independent provisioning to improve computation efficiency. BlockLLM comprises an offline block zoo for storing blocks and an online system to serve requests through chains of blocks. It offers multi-fold flexibilities: (1) Adaptive assembly of blocks on-the-fly through equivalence evaluation among blocks in the zoo; (2) Per-block batch size configuration and best-effort KV cache coordination at the individual block level; (3) Speculative execution and locality-aware block placement to reduce communication costs from dynamic block resource allocation. Our evaluation shows that BlockLLM reduces memory and storage footprints and improves computational efficiency, outperforming existing serving approach in 95%ile latency and GPU utilization by 33.5% and 20.1%, respectively, with minimal impact on accuracy

**Link**: [arxiv](http://arxiv.org/abs/2404.18322v2),  [pdf](http://arxiv.org/pdf/2404.18322v2)

**Tags**: cs.DC 



### RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal   Reinforcement and Retrieval-Augmented Generation
**Authors**: Jicheng Wang, Yifeng He, Hao Chen

**Updated**: 2024-09-23T19:53:37Z

**Summary**: In real-world software engineering tasks, solving a problem often requires understanding and modifying multiple functions, classes, and files across a large codebase. Therefore, on the repository level, it is crucial to extract the relevant information to achieve accurate code completion effectively. Existing code completion tools have achieved some success, but they struggle to optimize the retrieval and generation process dynamically. In this paper, we propose RepoGenReflex, a generic, dynamic, effective framework to address this challenge. By leveraging the Retrieval-Augmented Generation (RAG) enhanced with Verbal Reinforcement Learning (VRL), it can dynamically choose the optimal results for repository-level code completion. RepoGenReflex uses Reflector to give directional feedback to the next loop. RepoGenReflex chooses the optimal results stored in the Experience cache based on the RAG-VRL loop. To validate the framework's generalization ability, we propose a new benchmark RepoGenEval, which consists of the latest, high-quality real-world repositories in line completion scenarios. Our experiments demonstrate that RepoGenReflex achieves significant improvements after optimizing the Reflector component, resulting in enhanced accuracy and relevance of code completions. Additionally, RepoGenReflex consistently demonstrates superior performance and effectiveness across standard code completion tasks, highlighting the robustness and adaptability of our framework.

**Link**: [arxiv](http://arxiv.org/abs/2409.13122v2),  [pdf](http://arxiv.org/pdf/2409.13122v2)

**Tags**: cs.SE 



### Steward: Natural Language Web Automation
**Authors**: Brian Tang, Kang G. Shin

**Updated**: 2024-09-23T18:06:32Z

**Summary**: Recently, large language models (LLMs) have demonstrated exceptional capabilities in serving as the foundation for AI assistants. One emerging application of LLMs, navigating through websites and interacting with UI elements across various web pages, remains somewhat underexplored. We introduce Steward, a novel LLM-powered web automation tool designed to serve as a cost-effective, scalable, end-to-end solution for automating web interactions. Traditional browser automation frameworks like Selenium, Puppeteer, and Playwright are not scalable for extensive web interaction tasks, such as studying recommendation algorithms on platforms like YouTube and Twitter. These frameworks require manual coding of interactions, limiting their utility in large-scale or dynamic contexts. Steward addresses these limitations by integrating LLM capabilities with browser automation, allowing for natural language-driven interaction with websites. Steward operates by receiving natural language instructions and reactively planning and executing a sequence of actions on websites, looping until completion, making it a practical tool for developers and researchers to use. It achieves high efficiency, completing actions in 8.52 to 10.14 seconds at a cost of $0.028 per action or an average of $0.18 per task, which is further reduced to 4.8 seconds and $0.022 through a caching mechanism. It runs tasks on real websites with a 40% completion success rate. We discuss various design and implementation challenges, including state representation, action sequence selection, system responsiveness, detecting task completion, and caching implementation.

**Link**: [arxiv](http://arxiv.org/abs/2409.15441v1),  [pdf](http://arxiv.org/pdf/2409.15441v1)

**Tags**: cs.AI 



### CSPS: A Communication-Efficient Sequence-Parallelism based Serving   System for Transformer based Models with Long Prompts
**Authors**: Zeyu Zhang, Haiying Shen

**Updated**: 2024-09-23T15:16:29Z

**Summary**: Long-sequence generative large-language model (LLM) applications have become increasingly popular. In this paper, through trace-based experiments, we found that the existing method for long sequences results in a high Time-To-First-Token (TTFT) due to sequential chunk processing, long Time-Between-Tokens (TBT) from batching long-sequence prefills and decodes, and low throughput due to constrained key-value cache (KVC) for long sequences. To address these issues, we propose two Sequence-Parallelism (SP) architectures for both tensor parallelism (TP) and non-TP. However, SP introduces two challenges: 1) network communication and computation become performance bottlenecks; 2) the latter two issues above are mitigated but not resolved, and SP's resultant KV value distribution across GPUs still requires communication for decode, increasing TBT. Hence, we propose a Communication-efficient Sparse Attention (CSA) and communication-computation-communication three-phase pipelining. We also propose SP-based decode that processes decode separately from prefill, distributes KV values of a request across different GPUs, and novelly moves Query (Q) values instead of KV values to reduce communication overhead. These methods constitute a communication-efficient Sequence-Parallelism based LLM Serving System (SPS2). Our trace-driven evaluation demonstrates that SPS2 improves the average TTFT, TBT, and response time by up to 7.5x, 1.92x, and 9.8x and improves the prefill and decode throughput by 8.2x and 5.2x while maintaining the accuracy compared to Sarathi-Serve. We distributed our source code.

**Link**: [arxiv](http://arxiv.org/abs/2409.15104v1),  [pdf](http://arxiv.org/pdf/2409.15104v1)

**Tags**: cs.DC cs.LG 



### Inference-Friendly Models With MixAttention
**Authors**: Shashank Rajput, Ying Sheng, Sean Owen, Vitaliy Chiley

**Updated**: 2024-09-23T13:37:25Z

**Summary**: The size of the key-value (KV) cache plays a critical role in determining both the maximum context length and the number of concurrent requests supported during inference in modern language models. The KV cache size grows proportionally with the number of attention heads and the tokens processed, leading to increased memory consumption and slower inference for long inputs. In this work, we explore the use of MixAttention, a model architecture modification closely related to a blog published by Character.AI. MixAttention combines sliding window attention, where only a small subset of recent tokens is stored in the KV cache, with KV cache sharing across layers. Our experiments demonstrate that MixAttention significantly reduces memory usage and improves inference speed without sacrificing model performance in both short and long-context tasks. We also explore various configurations of this architecture, identifying those that maintain quality across evaluation metrics while optimizing resource efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2409.15012v1),  [pdf](http://arxiv.org/pdf/2409.15012v1)

**Tags**: cs.CL cs.AI 



### Mutation-Based Deep Learning Framework Testing Method in JavaScript   Environment
**Authors**: Yinglong Zou, Juan Zhai, Chunrong Fang, Jiawei Liu, Tao Zheng, Zhenyu Chen

**Updated**: 2024-09-23T12:37:56Z

**Summary**: In recent years, Deep Learning (DL) applications in JavaScript environment have become increasingly popular. As the infrastructure for DL applications, JavaScript DL frameworks play a crucial role in the development and deployment. It is essential to ensure the quality of JavaScript DL frameworks. However, the bottleneck of limited computational resources in the JavaScript environment brings new challenges to framework testing. Specifically, JavaScript DL frameworks are equipped with various optimization mechanisms (e.g., cache reuse, inference acceleration) to overcome the bottleneck of limited computational resources. These optimization mechanisms are overlooked by existing methods, resulting in many bugs in JavaScript DL frameworks being missed. To address the above challenges, we propose a mutation-based JavaScript DL framework testing method named DLJSFuzzer. DLJSFuzzer designs 13 tensor mutation rules targeting the cache reuse mechanism to generate test input tensors. Besides, DLJSFuzzer designs eight model mutation rules targeting the inference acceleration mechanism to generate test input models. To evaluate the effectiveness of DLJSFuzzer, we conduct experiments on the most widely-used JavaScript DL framework, TensorFlow.js. The experimental results show that DLJSFuzzer outperforms state-of-the-art methods in both effectiveness and efficiency. DLJSFuzzer successfully detects 21 unique crashes and 126 unique NaN & Inconsistency bugs. All detected crashes have been reported to the open-source community, with 12 of them already confirmed by developers. Additionally, DLJSFuzzer has improved by over 47% in model generation efficiency and over 91% in bug detection efficiency compared to all baselines.

**Link**: [arxiv](http://arxiv.org/abs/2409.14968v1),  [pdf](http://arxiv.org/pdf/2409.14968v1)

**Tags**: cs.SE 



### A-VL: Adaptive Attention for Large Vision-Language Models
**Authors**: Junyang Zhang, Mu Yuan, Ruiguang Zhong, Puhan Luo, Huiyou Zhan, Ningkang Zhang, Chengchen Hu, Xiangyang Li

**Updated**: 2024-09-23T09:22:59Z

**Summary**: The Large Vision-Language Model (LVLM) integrates computer vision and natural language processing techniques, offering substantial application potential. However, these models demand extensive resources during inference. Adaptive attention techniques can dynamically reduce computational redundancy and thus improve efficiency. Although current adaptive attention methods significantly reduce the memory requirements of Transformer-based language models, they are not tailored for LVLMs. We observe that LVLMs generate responses from both remote image tokens and local text tokens, and different modalities have different attention patterns. This observation inspires us to manage the attention for each modality separately. Specifically, for visual input, we store the cache of potentially useful information but only compute the most critical parts. For language input, we care more about local information. Based on our observation and analysis of vision-language attention patterns, we develop A-VL, a plug-and-play adaptive attention tailored for LVLM inference. Extensive evaluations on three vision-language tasks and five datasets show the effectiveness of our designs. Our approach A-VL outperforms existing adaptive attention methods in reducing memory usage and computational load without compromising performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.14846v1),  [pdf](http://arxiv.org/pdf/2409.14846v1)

**Tags**: cs.AI cs.CV 



### CritiPrefill: A Segment-wise Criticality-based Approach for Prefilling   Acceleration in LLMs
**Authors**: Junlin Lv, Yuan Feng, Xike Xie, Xin Jia, Qirong Peng, Guiming Xie

**Updated**: 2024-09-23T02:24:33Z

**Summary**: Large language models have achieved notable success across various domains, yet efficient inference is still limited by the quadratic computation complexity of the attention mechanism. The inference consists of prefilling and decoding phases. Although several attempts have been made to accelerate decoding, the inefficiency of the prefilling phase, especially for long-context tasks, remains a challenge. In this paper, we observe a locality in query criticality during the prefilling phase of long-context processing: adjacent query tokens tend to focus on similar subsets of the past Key-Value (KV) cache. Based on this observation, we propose CritiPrefill, a criticality-based segment-wise prefilling method. This method partitions the input sequence's queries and KV cache into segments and blocks, utilizing a segment-wise algorithm to estimate the query criticality. By pruning non-critical computations between query segments and cache blocks in the self-attention mechanism, the prefilling process can be significantly accelerated. Extensive evaluations on multiple long-context datasets show up to 2.7x speedup on Llama3-8B and 3.0x speedup on Yi-9B for 128K context length on a single A100 GPU, with minimal quality degradation.

**Link**: [arxiv](http://arxiv.org/abs/2409.12490v2),  [pdf](http://arxiv.org/pdf/2409.12490v2)

**Tags**: cs.CL cs.AI cs.LG 



### In-place Switch: Reprogramming based SLC Cache Design for Hybrid 3D SSDs
**Authors**: Xufeng Yang, Zhengjian Cong, Congming Gao

**Updated**: 2024-09-22T08:30:43Z

**Summary**: Recently, 3D SSDs are widely adopted in PCs, data centers, and cloud storage systems. To increase capacity, high bit-density cells, such as Triple-Level Cell (TLC), are utilized within 3D SSDs. However, due to the inferior performance of TLC, a portion of TLCs is configured to operate as Single-Level Cell (SLC) to provide high performance, with host data initially directed to the SLCs. In SLC/TLC hybrid 3D SSDs, a portion of the TLC space is designated as an SLC cache to achieve high SSD performance by writing host data at the SLC speed. Given the limited size of the SLC cache, block reclamation is necessary to free up the SLC cache during idle periods. However, our preliminary studies indicate that the SLC cache can lead to a performance cliff if filled rapidly and cause significant write amplification when data migration occurs during idle times.   In this work, we propose leveraging a reprogram operation to address these challenges. Specifically, when the SLC cache is full or during idle periods, a reprogram operation is performed to switch used SLC pages to TLC pages in place (termed In-place Switch, IPS). Subsequently, other free TLC space is allocated as the new SLC cache. IPS can continuously provide sufficient SLC cache within SSDs, significantly improving write performance and reducing write amplification. Experimental results demonstrate that IPS can reduce write latency and write amplification by up to 0.75 times and 0.53 times, respectively, compared to state-of-the-art SLC cache technologies.

**Link**: [arxiv](http://arxiv.org/abs/2409.14360v1),  [pdf](http://arxiv.org/pdf/2409.14360v1)

**Tags**: cs.AR 



### D2D Coded Caching from Two Classes of Optimal DPDAs using Cross   Resolvable Designs
**Authors**: Rashid Ummer N. T., B. Sundar Rajan

**Updated**: 2024-09-22T07:24:02Z

**Summary**: Coded caching in a wireless device-to-device (D2D) network was first studied by Ji \textit{et al.} in [4] (referred to as the JCM scheme). In a D2D network, a central server first places the data in the user cache memories and all the user's demands are served by inter-user coded multicast transmissions. Low subpacketization level D2D coded caching schemes are desirable for practical implementations. Wang \textit{et al.} in [7] proposed an array called D2D placement delivery array (DPDA) which characterizes the placement phase and the delivery phase in a D2D network. A lower bound on the transmission load of a DPDA is derived and only the JCM scheme achieves this lower bound, but requires a subpacketization level that grows exponentially with the number of users. Low subpacketization level D2D schemes can be obtained by constructing appropriate DPDAs. In this paper, we propose two new classes of DPDA constructions that give low subpacketization level D2D schemes using cross resolvable designs. The first class of constructed DPDA achieves the known lower bound on the transmission load of DPDA while requiring a subpacketization level lesser than that of the JCM scheme. We propose another lower bound on the transmission load of a DPDA and show that the second class of constructed DPDA achieves this lower bound.

**Link**: [arxiv](http://arxiv.org/abs/2409.14350v1),  [pdf](http://arxiv.org/pdf/2409.14350v1)

**Tags**: cs.IT math.IT 



### Sub-millisecond electric field sensing with an individual rare-earth   doped ferroelectric nanocrystal
**Authors**: Athulya Muraleedharan, Jingye Zou, Maxime Vallet, Abdelali Zaki, Christine Bogicevic, Charles Paillard, Karen Perronet, Fran√ßois Treussart

**Updated**: 2024-09-21T20:45:41Z

**Summary**: Understanding the dynamics of electrical signals within neuronal assemblies is crucial to unraveling complex brain function. Despite recent advances in employing optically active nanostructures in transmembrane potential sensing, there remains room for improvement in terms of response time and sensitivity. Here, we report the development of such a nanosensor capable of detecting electric fields with a submillisecond response time at the single particle level. We achieve this by using ferroelectric nanocrystals doped with rare earth ions producing upconversion (UC). When such a nanocrystal experiences a variation of surrounding electric potential, its surface charge density changes, inducing electric polarization modifications that vary, via converse piezoelectric effect, the crystal field around the ions. The latter variation is finally converted into UC spectral changes, enabling optical detection of electric potential. To develop such a sensor, we synthesized erbium and ytterbium-doped barium titanate crystals of size $\approx160$~nm. We observed distinct changes in the UC spectrum when individual nanocrystals were subjected to an external field via a conductive AFM tip, with a response time of 100~$\mu$s. Furthermore, our sensor exhibits a remarkable sensitivity of 4.8~kV/cm/$\sqrt{\rm Hz}$, enabling time-resolved detection of fast changing electric field of amplitude comparable to that generated during a neuron action potential.

**Link**: [arxiv](http://arxiv.org/abs/2407.02000v2),  [pdf](http://arxiv.org/pdf/2407.02000v2)

**Tags**: physics.optics cond-mat.other 



### CSKV: Training-Efficient Channel Shrinking for KV Cache in Long-Context   Scenarios
**Authors**: Luning Wang, Shiyao Li, Xuefei Ning, Zhihang Yuan, Shengen Yan, Guohao Dai, Yu Wang

**Updated**: 2024-09-21T13:01:43Z

**Summary**: Large Language Models (LLMs) have been widely adopted to process long-context tasks. However, the large memory overhead of the key-value (KV) cache poses significant challenges in long-context scenarios. Existing training-free KV cache compression methods typically focus on quantization and token pruning, which have compression limits, and excessive sparsity can lead to severe performance degradation. Other methods design new architectures with less KV overhead but require significant training overhead. To address the above two drawbacks, we further explore the redundancy in the channel dimension and apply an architecture-level design with minor training costs. Therefore, we introduce CSKV, a training-efficient Channel Shrinking technique for KV cache compression: (1) We first analyze the singular value distribution of the KV cache, revealing significant redundancy and compression potential along the channel dimension. Based on this observation, we propose using low-rank decomposition for key and value layers and storing the low-dimension features. (2) To preserve model performance, we introduce a bi-branch KV cache, including a window-based full-precision KV cache and a low-precision compressed KV cache. (3) To reduce the training costs, we minimize the layer-wise reconstruction loss for the compressed KV cache instead of retraining the entire LLMs. Extensive experiments show that CSKV can reduce the memory overhead of the KV cache by 80% while maintaining the model's long-context capability. Moreover, we show that our method can be seamlessly combined with quantization to further reduce the memory overhead, achieving a compression ratio of up to 95%.

**Link**: [arxiv](http://arxiv.org/abs/2409.10593v2),  [pdf](http://arxiv.org/pdf/2409.10593v2)

**Tags**: cs.LG cs.AI cs.CL 



### A Simple and Effective $L_2$ Norm-Based Strategy for KV Cache   Compression
**Authors**: Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini

**Updated**: 2024-09-21T12:33:00Z

**Summary**: The deployment of large language models (LLMs) is often hindered by the extensive memory requirements of the Key-Value (KV) cache, especially as context lengths increase. Existing approaches to reduce the KV cache size involve either fine-tuning the model to learn a compression strategy or leveraging attention scores to reduce the sequence length. We analyse the attention distributions in decoder-only Transformers-based models and observe that attention allocation patterns stay consistent across most layers. Surprisingly, we find a clear correlation between the $L_2$ and the attention scores over cached KV pairs, where a low $L_2$ of a key embedding usually leads to a high attention score during decoding. This finding indicates that the influence of a KV pair is potentially determined by the key embedding itself before being queried. Based on this observation, we compress the KV cache based on the $L_2$ of key embeddings. Our experimental results show that this simple strategy can reduce the KV cache size by 50% on language modelling and needle-in-a-haystack tasks and 90% on passkey retrieval tasks without losing accuracy. Moreover, without relying on the attention scores, this approach remains compatible with FlashAttention, enabling broader applicability.

**Link**: [arxiv](http://arxiv.org/abs/2406.11430v2),  [pdf](http://arxiv.org/pdf/2406.11430v2)

**Tags**: cs.CL cs.AI 



### LLM-dCache: Improving Tool-Augmented LLMs with GPT-Driven Localized Data   Caching
**Authors**: Simranjit Singh, Michael Fore, Andreas Karatzas, Chaehong Lee, Yanan Jian, Longfei Shangguan, Fuxun Yu, Iraklis Anagnostopoulos, Dimitrios Stamoulis

**Updated**: 2024-09-21T09:10:02Z

**Summary**: As Large Language Models (LLMs) broaden their capabilities to manage thousands of API calls, they are confronted with complex data operations across vast datasets with significant overhead to the underlying system. In this work, we introduce LLM-dCache to optimize data accesses by treating cache operations as callable API functions exposed to the tool-augmented agent. We grant LLMs the autonomy to manage cache decisions via prompting, seamlessly integrating with existing function-calling mechanisms. Tested on an industry-scale massively parallel platform that spans hundreds of GPT endpoints and terabytes of imagery, our method improves Copilot times by an average of 1.24x across various LLMs and prompting techniques.

**Link**: [arxiv](http://arxiv.org/abs/2406.06799v2),  [pdf](http://arxiv.org/pdf/2406.06799v2)

**Tags**: cs.DC cs.CL 



### ARCANE: Adaptive Routing with Caching and Network Exploration
**Authors**: Tommaso Bonato, Abdul Kabbani, Ahmad Ghalayini, Mohammad Dohadwala, Michael Papamichael, Daniele De Sensi, Torsten Hoefler

**Updated**: 2024-09-20T16:59:29Z

**Summary**: Most datacenter transport protocols traditionally depend on in-order packet delivery, a legacy design choice that prioritizes simplicity. However, technological advancements, such as RDMA, now enable the relaxation of this requirement, allowing for more efficient utilization of modern datacenter topologies like FatTree and Dragonfly. With the growing prevalence of AI/ML workloads, the demand for improved link utilization has intensified, creating challenges for single-path load balancers due to problems like ECMP collisions. In this paper, we present ARCANE, a novel, adaptive per-packet traffic load-balancing algorithm designed to work seamlessly with existing congestion control mechanisms. ARCANE dynamically routes packets to bypass congested areas and network failures, all while maintaining a lightweight footprint with minimal state requirements. Our evaluation shows that ARCANE delivers significant performance gains over traditional load-balancing methods, including packet spraying and other advanced solutions, substantially enhancing both performance and link utilization in modern datacenter networks.

**Link**: [arxiv](http://arxiv.org/abs/2407.21625v2),  [pdf](http://arxiv.org/pdf/2407.21625v2)

**Tags**: cs.NI 



### ConfusedPilot: Confused Deputy Risks in RAG-based LLMs
**Authors**: Ayush RoyChowdhury, Mulong Luo, Prateek Sahu, Sarbartha Banerjee, Mohit Tiwari

**Updated**: 2024-09-20T15:51:17Z

**Summary**: Retrieval augmented generation (RAG) is a process where a large language model (LLM) retrieves useful information from a database and then generates the responses. It is becoming popular in enterprise settings for daily business operations. For example, Copilot for Microsoft 365 has accumulated millions of businesses. However, the security implications of adopting such RAG-based systems are unclear.   In this paper, we introduce ConfusedPilot, a class of security vulnerabilities of RAG systems that confuse Copilot and cause integrity and confidentiality violations in its responses. First, we investigate a vulnerability that embeds malicious text in the modified prompt in RAG, corrupting the responses generated by the LLM. Second, we demonstrate a vulnerability that leaks secret data, which leverages the caching mechanism during retrieval. Third, we investigate how both vulnerabilities can be exploited to propagate misinformation within the enterprise and ultimately impact its operations, such as sales and manufacturing. We also discuss the root cause of these attacks by investigating the architecture of a RAG-based system. This study highlights the security vulnerabilities in today's RAG-based systems and proposes design guidelines to secure future RAG-based systems.

**Link**: [arxiv](http://arxiv.org/abs/2408.04870v4),  [pdf](http://arxiv.org/pdf/2408.04870v4)

**Tags**: cs.CR cs.AI 



### RPAF: A Reinforcement Prediction-Allocation Framework for Cache   Allocation in Large-Scale Recommender Systems
**Authors**: Shuo Su, Xiaoshuang Chen, Yao Wang, Yulin Wu, Ziqiang Zhang, Kaiqiao Zhan, Ben Wang, Kun Gai

**Updated**: 2024-09-20T03:02:42Z

**Summary**: Modern recommender systems are built upon computation-intensive infrastructure, and it is challenging to perform real-time computation for each request, especially in peak periods, due to the limited computational resources. Recommending by user-wise result caches is widely used when the system cannot afford a real-time recommendation. However, it is challenging to allocate real-time and cached recommendations to maximize the users' overall engagement. This paper shows two key challenges to cache allocation, i.e., the value-strategy dependency and the streaming allocation. Then, we propose a reinforcement prediction-allocation framework (RPAF) to address these issues. RPAF is a reinforcement-learning-based two-stage framework containing prediction and allocation stages. The prediction stage estimates the values of the cache choices considering the value-strategy dependency, and the allocation stage determines the cache choices for each individual request while satisfying the global budget constraint. We show that the challenge of training RPAF includes globality and the strictness of budget constraints, and a relaxed local allocator (RLA) is proposed to address this issue. Moreover, a PoolRank algorithm is used in the allocation stage to deal with the streaming allocation problem. Experiments show that RPAF significantly improves users' engagement under computational budget constraints.

**Link**: [arxiv](http://arxiv.org/abs/2409.13175v1),  [pdf](http://arxiv.org/pdf/2409.13175v1)

**Tags**: cs.LG cs.IR 



### 3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt
**Authors**: Lukas H√∂llein, Alja≈æ Bo≈æiƒç, Michael Zollh√∂fer, Matthias Nie√üner

**Updated**: 2024-09-19T16:31:44Z

**Summary**: We present 3DGS-LM, a new method that accelerates the reconstruction of 3D Gaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored Levenberg-Marquardt (LM). Existing methods reduce the optimization time by decreasing the number of Gaussians or by improving the implementation of the differentiable rasterizer. However, they still rely on the ADAM optimizer to fit Gaussian parameters of a scene in thousands of iterations, which can take up to an hour. To this end, we change the optimizer to LM that runs in conjunction with the 3DGS differentiable rasterizer. For efficient GPU parallization, we propose a caching data structure for intermediate gradients that allows us to efficiently calculate Jacobian-vector products in custom CUDA kernels. In every LM iteration, we calculate update directions from multiple image subsets using these kernels and combine them in a weighted mean. Overall, our method is 30% faster than the original 3DGS while obtaining the same reconstruction quality. Our optimization is also agnostic to other methods that acclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.

**Link**: [arxiv](http://arxiv.org/abs/2409.12892v1),  [pdf](http://arxiv.org/pdf/2409.12892v1)

**Tags**: cs.CV 



### Learning Harmonized Representations for Speculative Sampling
**Authors**: Lefan Zhang, Xiaodan Wang, Yanhua Huang, Ruiwen Xu

**Updated**: 2024-09-19T15:46:57Z

**Summary**: Speculative sampling is a promising approach to accelerate the decoding stage for Large Language Models (LLMs). Recent advancements that leverage target LLM's contextual information, such as hidden states and KV cache, have shown significant practical improvements. However, these approaches suffer from inconsistent context between training and decoding. We also observe another discrepancy between the training and decoding objectives in existing speculative sampling methods. In this work, we propose a solution named HArmonized Speculative Sampling (HASS) that learns harmonized representations to address these issues. HASS accelerates the decoding stage without adding inference overhead through harmonized objective distillation and harmonized context alignment. Experiments on four LLaMA models demonstrate that HASS achieves 2.81x-4.05x wall-clock time speedup ratio averaging across three datasets, surpassing EAGLE-2 by 8%-20%.

**Link**: [arxiv](http://arxiv.org/abs/2408.15766v2),  [pdf](http://arxiv.org/pdf/2408.15766v2)

**Tags**: cs.LG cs.CL 



### On the Regret of Coded Caching with Adversarial Requests
**Authors**: Anupam Nayak, Kota Srinivas Reddy, Nikhil Karamchandani

**Updated**: 2024-09-19T01:13:03Z

**Summary**: We study the well-known coded caching problem in an online learning framework, wherein requests arrive sequentially, and an online policy can update the cache contents based on the history of requests seen thus far. We introduce a caching policy based on the Follow-The-Perturbed-Leader principle and show that for any time horizon T and any request sequence, it achieves a sub-linear regret of \mathcal{O}(\sqrt(T) ) with respect to an oracle that knows the request sequence beforehand. Our study marks the first examination of adversarial regret in the coded caching setup. Furthermore, we also address the issue of switching cost by establishing an upper bound on the expected number of cache updates made by our algorithm under unrestricted switching and also provide an upper bound on the regret under restricted switching when cache updates can only happen in a pre-specified subset of timeslots. Finally, we validate our theoretical insights with numerical results using a real-world dataset

**Link**: [arxiv](http://arxiv.org/abs/2409.12387v1),  [pdf](http://arxiv.org/pdf/2409.12387v1)

**Tags**: cs.IT cs.LG math.IT 



### Trajectory Anomaly Detection with Language Models
**Authors**: Jonathan Mbuya, Dieter Pfoser, Antonios Anastasopoulos

**Updated**: 2024-09-18T17:33:31Z

**Summary**: This paper presents a novel approach for trajectory anomaly detection using an autoregressive causal-attention model, termed LM-TAD. This method leverages the similarities between language statements and trajectories, both of which consist of ordered elements requiring coherence through external rules and contextual variations. By treating trajectories as sequences of tokens, our model learns the probability distributions over trajectories, enabling the identification of anomalous locations with high precision. We incorporate user-specific tokens to account for individual behavior patterns, enhancing anomaly detection tailored to user context. Our experiments demonstrate the effectiveness of LM-TAD on both synthetic and real-world datasets. In particular, the model outperforms existing methods on the Pattern of Life (PoL) dataset by detecting user-contextual anomalies and achieves competitive results on the Porto taxi dataset, highlighting its adaptability and robustness. Additionally, we introduce the use of perplexity and surprisal rate metrics for detecting outliers and pinpointing specific anomalous locations within trajectories. The LM-TAD framework supports various trajectory representations, including GPS coordinates, staypoints, and activity types, proving its versatility in handling diverse trajectory data. Moreover, our approach is well-suited for online trajectory anomaly detection, significantly reducing computational latency by caching key-value states of the attention mechanism, thereby avoiding repeated computations.

**Link**: [arxiv](http://arxiv.org/abs/2409.15366v1),  [pdf](http://arxiv.org/pdf/2409.15366v1)

**Tags**: cs.LG cs.AI 



### Autonomous Navigation in Ice-Covered Waters with Learned Predictions on   Ship-Ice Interactions
**Authors**: Ninghan Zhong, Alessandro Potenza, Stephen L. Smith

**Updated**: 2024-09-18T17:09:42Z

**Summary**: Autonomous navigation in ice-covered waters poses significant challenges due to the frequent lack of viable collision-free trajectories. When complete obstacle avoidance is infeasible, it becomes imperative for the navigation strategy to minimize collisions. Additionally, the dynamic nature of ice, which moves in response to ship maneuvers, complicates the path planning process. To address these challenges, we propose a novel deep learning model to estimate the coarse dynamics of ice movements triggered by ship actions through occupancy estimation. To ensure real-time applicability, we propose a novel approach that caches intermediate prediction results and seamlessly integrates the predictive model into a graph search planner. We evaluate the proposed planner both in simulation and in a physical testbed against existing approaches and show that our planner significantly reduces collisions with ice when compared to the state-of-the-art. Codes and demos of this work are available at https://github.com/IvanIZ/predictive-asv-planner.

**Link**: [arxiv](http://arxiv.org/abs/2409.11326v2),  [pdf](http://arxiv.org/pdf/2409.11326v2)

**Tags**: cs.RO 



### Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority   Queues
**Authors**: Thore Thie√üen, Jan Vahrenhold

**Updated**: 2024-09-18T14:31:33Z

**Summary**: Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access pattern of a RAM computation; it has a variety of applications in trusted computing, outsourced storage, and multiparty computation. In this paper, we study the so-called offline ORAM in which the sequence of memory access locations to be hidden is known in advance. Apart from their theoretical significance, offline ORAMs can be used to construct efficient oblivious algorithms.   We obtain the first optimal offline ORAM with perfect security from oblivious priority queues via time-forward processing. For this, we present a simple construction of an oblivious priority queue with perfect security. Our construction achieves an asymptotically optimal (amortized) runtime of $\Theta(\log N)$ per operation for a capacity of $N$ elements and is of independent interest.   Building on our construction, we additionally present efficient external-memory instantiations of our oblivious, perfectly-secure construction: For the cache-aware setting, we match the optimal I/O complexity of $\Theta(\frac{1}{B} \log \frac{N}{M})$ per operation (amortized), and for the cache-oblivious setting we achieve a near-optimal I/O complexity of $O(\frac{1}{B} \log \frac{N}{M} \log\log_M N)$ per operation (amortized).

**Link**: [arxiv](http://arxiv.org/abs/2409.12021v1),  [pdf](http://arxiv.org/pdf/2409.12021v1)

**Tags**: cs.DS cs.CR 



### RetrievalAttention: Accelerating Long-Context LLM Inference via Vector   Retrieval
**Authors**: Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu

**Updated**: 2024-09-18T13:11:13Z

**Summary**: Transformer-based Large Language Models (LLMs) have become increasingly important. However, due to the quadratic time complexity of attention computation, scaling LLMs to longer contexts incurs extremely slow inference latency and high GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to both accelerate attention computation and reduce GPU memory consumption. By leveraging the dynamic sparsity of attention mechanism, RetrievalAttention proposes to use approximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory and retrieves the most relevant ones with vector search during generation. Unfortunately, we observe that the off-the-shelf ANNS indexes are often ineffective for such retrieval tasks due to the out-of-distribution (OOD) between query vectors and key vectors in attention mechanism. RetrievalAttention addresses the OOD challenge by designing an attention-aware vector search algorithm that can adapt to the distribution of query vectors. Our evaluation shows that RetrievalAttention only needs to access 1--3% of data while maintaining high model accuracy. This leads to significant reduction in the inference cost of long-context LLMs with much lower GPU memory footprint. In particular, RetrievalAttention only needs a single NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds.

**Link**: [arxiv](http://arxiv.org/abs/2409.10516v2),  [pdf](http://arxiv.org/pdf/2409.10516v2)

**Tags**: cs.LG cs.CL 



### Proportional scintillation in liquid xenon: demonstration in a   single-phase liquid-only time projection chamber
**Authors**: Florian T√∂nnies, Adam Brown, Baris Kiyim, Fabian Kuger, Sebastian Lindemann, Patrick Meinhardt, Marc Schumann, Andrew Stevens

**Updated**: 2024-09-18T08:22:23Z

**Summary**: The largest direct dark matter search experiments to date employ dual-phase time projection chambers (TPCs) with liquid noble gas targets. These detect both the primary photons generated by particle interactions in the liquid target, as well as proportional secondary scintillation light created by the ionization electrons in a strong electric field in the gas phase between the liquid-gas interface and the anode. In this work, we describe the detection of charge signals in a small-scale single-phase liquid-xenon-only TPC, that features the well-established TPC geometry with light readout above and below a cylindrical target. In the single-phase TPC, the proportional scintillation light (S2) is generated in liquid xenon in close proximity to 10 {\mu}m diameter anode wires. The detector was characterized and the proportional scintillation process was studied using the 32.1 keV and 9.4 keV signals from 83mKr decays. A charge gain factor g2 of up to (1.9 $\pm$ 0.3) PE/electron was reached at an anode voltage 4.4 kV higher than the gate electrode 5 mm below it, corresponding to (29 $\pm$ 6) photons emitted per ionization electron. The duration of S2 signals is dominated by electron diffusion and approaches the xenon de-excitation timescale for very short electron drift times. The electron drift velocity and the longitudinal diffusion constant were measured at a drift field of 470 V/cm. The results agree with the literature and demonstrate that a single-phase TPC can be operated successfully.

**Link**: [arxiv](http://arxiv.org/abs/2405.10687v2),  [pdf](http://arxiv.org/pdf/2405.10687v2)

**Tags**: physics.ins-det 



### ASVD: Activation-aware Singular Value Decomposition for Compressing   Large Language Models
**Authors**: Zhihang Yuan, Yuzhang Shang, Yue Song, Qiang Wu, Yan Yan, Guangyu Sun

**Updated**: 2024-09-18T04:53:46Z

**Summary**: In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank factorization, and find that the challenges of this task stem from the outlier phenomenon in the LLM activations and the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by scaling the weight matrix based on the activation distribution, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. ASVD can compress a network by 10-20%, without compromising the performance of LLMs. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. Thanks to the 50-75% reduction in the rank of the KV projection matrices, ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner.

**Link**: [arxiv](http://arxiv.org/abs/2312.05821v3),  [pdf](http://arxiv.org/pdf/2312.05821v3)

**Tags**: cs.CL 



### No Saved Kaleidosope: an 100% Jitted Neural Network Coding Language with   Pythonic Syntax
**Authors**: Augusto Seben da Rosa, Marlon Daniel Angeli, Jorge Aikes Junior, Alef Iury Ferreira, Lucas Rafael Gris, Anderson da Silva Soares, Arnaldo Candido Junior, Frederico Santos de Oliveira, Gabriel Trevisan Damke, Rafael Teixeira Sousa

**Updated**: 2024-09-17T23:15:39Z

**Summary**: We developed a jitted compiler for training Artificial Neural Networks using C++, LLVM and Cuda. It features object-oriented characteristics, strong typing, parallel workers for data pre-processing, pythonic syntax for expressions, PyTorch like model declaration and Automatic Differentiation. We implement the mechanisms of cache and pooling in order to manage VRAM, cuBLAS for high performance matrix multiplication and cuDNN for convolutional layers. Our experiments with Residual Convolutional Neural Networks on ImageNet, we reach similar speed but degraded performance. Also, the GRU network experiments show similar accuracy, but our compiler have degraded speed in that task. However, our compiler demonstrates promising results at the CIFAR-10 benchmark, in which we reach the same performance and about the same speed as PyTorch. We make the code publicly available at: https://github.com/NoSavedDATA/NoSavedKaleidoscope

**Link**: [arxiv](http://arxiv.org/abs/2409.11600v1),  [pdf](http://arxiv.org/pdf/2409.11600v1)

**Tags**: cs.PL cs.AI cs.LG 68T07 D.3; I.2; I.4; I.7 



### Attacking Slicing Network via Side-channel Reinforcement Learning Attack
**Authors**: Wei Shao, Chandra Thapa, Rayne Holland, Sarah Ali Siddiqui, Seyit Camtepe

**Updated**: 2024-09-17T15:07:05Z

**Summary**: Network slicing in 5G and the future 6G networks will enable the creation of multiple virtualized networks on a shared physical infrastructure. This innovative approach enables the provision of tailored networks to accommodate specific business types or industry users, thus delivering more customized and efficient services. However, the shared memory and cache in network slicing introduce security vulnerabilities that have yet to be fully addressed. In this paper, we introduce a reinforcement learning-based side-channel cache attack framework specifically designed for network slicing environments. Unlike traditional cache attack methods, our framework leverages reinforcement learning to dynamically identify and exploit cache locations storing sensitive information, such as authentication keys and user registration data. We assume that one slice network is compromised and demonstrate how the attacker can induce another shared slice to send registration requests, thereby estimating the cache locations of critical data. By formulating the cache timing channel attack as a reinforcement learning-driven guessing game between the attack slice and the victim slice, our model efficiently explores possible actions to pinpoint memory blocks containing sensitive information. Experimental results showcase the superiority of our approach, achieving a success rate of approximately 95\% to 98\% in accurately identifying the storage locations of sensitive data. This high level of accuracy underscores the potential risks in shared network slicing environments and highlights the need for robust security measures to safeguard against such advanced side-channel attacks.

**Link**: [arxiv](http://arxiv.org/abs/2409.11258v1),  [pdf](http://arxiv.org/pdf/2409.11258v1)

**Tags**: cs.CR cs.AI 



### Electron-beam-induced adatom-vacancy-complexes in mono- and bilayer   phosphorene
**Authors**: Carsten Speckmann, Andrea Angeletti, Luk√°≈° K√Ωvala, David Lamprecht, Felix Herterich, Clemens Mangler, Lado Filipovic, Christoph Dellago, Cesare Franchini, Jani Kotakoski

**Updated**: 2024-09-17T11:54:24Z

**Summary**: Phosphorene, a puckered two-dimensional allotrope of phosphorus, has sparked considerable interest in recent years due to its potential especially for optoelectronic applications with its layer-number-dependant direct band gap and strongly bound excitons. However, detailed experimental characterization of its intrinsic defects as well as its defect creation characteristics under electron irradiation are scarce. Here, we report on the creation and stability of a variety of defect configurations under 60 kV electron irradiation in mono- and bilayer phosphorene including the first experimental reports of stable adatom-vacancy-complexes. Displacement cross section measurements in bilayer phosphorene yield a value of 7.7 +- 1.4 barn with an estimated lifetime of adatom-vacancy-complexes of 19.9 +- 0.7 s, while some are stable for up to 68 s under continuous electron irradiation. Surprisingly, ab initio-based simulations indicate that the complexes should readily recombine, even in structures strained by up to 3 %. The presented results will help to improve the understanding of the wide variety of defects in phosphorene, their creation, and their stability, which may enable new pathways for defect engineered phosphorene devices.

**Link**: [arxiv](http://arxiv.org/abs/2409.11102v1),  [pdf](http://arxiv.org/pdf/2409.11102v1)

**Tags**: cond-mat.mes-hall cond-mat.mtrl-sci 



### KVPruner: Structural Pruning for Faster and Memory-Efficient Large   Language Models
**Authors**: Bo Lv, Quan Zhou, Xuanang Ding, Yan Wang, Zeming Ma

**Updated**: 2024-09-17T10:35:30Z

**Summary**: The bottleneck associated with the key-value(KV) cache presents a significant challenge during the inference processes of large language models. While depth pruning accelerates inference, it requires extensive recovery training, which can take up to two weeks. On the other hand, width pruning retains much of the performance but offers slight speed gains. To tackle these challenges, we propose KVPruner to improve model efficiency while maintaining performance. Our method uses global perplexity-based analysis to determine the importance ratio for each block and provides multiple strategies to prune non-essential KV channels within blocks. Compared to the original model, KVPruner reduces runtime memory usage by 50% and boosts throughput by over 35%. Additionally, our method requires only two hours of LoRA fine-tuning on small datasets to recover most of the performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.11057v1),  [pdf](http://arxiv.org/pdf/2409.11057v1)

**Tags**: cs.CL 



### Skip TLB flushes for reused pages within mmap's
**Authors**: Frederic Schimmelpfennig, Andr√© Brinkmann, Hossein Asadi, Reza Salkhordeh

**Updated**: 2024-09-17T07:28:56Z

**Summary**: Memory access efficiency is significantly enhanced by caching recent address translations in the CPUs' Translation Lookaside Buffers (TLBs). However, since the operating system is not aware of which core is using a particular mapping, it flushes TLB entries across all cores where the application runs whenever addresses are unmapped, ensuring security and consistency. These TLB flushes, known as TLB shootdowns, are costly and create a performance and scalability bottleneck. A key contributor to TLB shootdowns is memory-mapped I/O, particularly during mmap-munmap cycles and page cache evictions. Often, the same physical pages are reassigned to the same process post-eviction, presenting an opportunity for the operating system to reduce the frequency of TLB shootdowns. We demonstrate, that by slightly extending the mmap function, TLB shootdowns for these "recycled pages" can be avoided.   Therefore we introduce and implement the "fast page recycling" (FPR) feature within the mmap system call. FPR-mmaps maintain security by only triggering TLB shootdowns when a page exits its recycling cycle and is allocated to a different process. To ensure consistency when FPR-mmap pointers are used, we made minor adjustments to virtual memory management to avoid the ABA problem. Unlike previous methods to mitigate shootdown effects, our approach does not require any hardware modifications and operates transparently within the existing Linux virtual memory framework.   Our evaluations across a variety of CPU, memory, and storage setups, including persistent memory and Optane SSDs, demonstrate that FPR delivers notable performance gains, with improvements of up to 28% in real-world applications and 92% in micro-benchmarks. Additionally, we show that TLB shootdowns are a significant source of bottlenecks, previously misattributed to other components of the Linux kernel.

**Link**: [arxiv](http://arxiv.org/abs/2409.10946v1),  [pdf](http://arxiv.org/pdf/2409.10946v1)

**Tags**: cs.OS cs.DC 



### Resources on the Move for Smart City: A Disruptive Perspective on the   Grand Convergence of Sensing, Communications, Computing, Storage, and   Intelligence
**Authors**: Yuguang Fang, Yiqin Deng, Xianhao Chen

**Updated**: 2024-09-17T04:39:04Z

**Summary**: The most commonly seen things on streets in any city are vehicles. However, most of them are used to transport people or goods. What if they also carry resources and capabilities for sensing, communications, computing, storage, and intelligence (SCCSI)? We will have a web of sensors to monitor the city, a network of powerful communicators to transport data around, a grid of computing power to conduct data analytics and machine learning (ML), a network of distributed storage to buffer/cache data/job for optimization, and a set of movable AI/ML toolboxes made available for specialized smart applications. This perspective article presents how to leverage SCCSI-empowered vehicles to design such a service network, simply called SCCSI network, to help build a smart city with a cost-effective and sustainable solution. It showcases how multi-dimensional technologies, namely, sensing, communications, computing, storage, and intelligence, converge to a unifying technology to solve grand challenges for resource demands from emerging large-scale applications. Thus, with SCCSI-empowered vehicles on the ground, over the air, and on the sea, SCCSI network can make resources and capabilities on the move, practically pushing SCCSI services to the edge! We hope this article serves as a spark to stimulate more disruptive thinking to address grand challenges of paramount importance.

**Link**: [arxiv](http://arxiv.org/abs/2409.09417v2),  [pdf](http://arxiv.org/pdf/2409.09417v2)

**Tags**: cs.NI 



### Do Large Language Models Need a Content Delivery Network?
**Authors**: Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang

**Updated**: 2024-09-16T18:46:24Z

**Summary**: As the use of large language models (LLMs) expands rapidly, so does the range of knowledge needed to supplement various LLM queries. Thus, enabling flexible and efficient injection of new knowledge in LLM inference is critical. Three high-level options exist: (i) embedding the knowledge in LLM's weights (i.e., fine-tuning), (ii) including the knowledge as a part of LLM's text input (i.e., in-context learning), or (iii) injecting the KV caches of the new knowledge to LLM during prefill. This paper argues that, although fine-tuning and in-context learning are popular, using KV caches as the medium of knowledge could simultaneously enable more modular management of knowledge injection and more efficient LLM serving with low cost and fast response. To realize these benefits, we envision a Knowledge Delivery Network (KDN), a new system component in LLM services that dynamically optimizes the storage, transfer, and composition of KV cache across LLM engines and other compute and storage resources. We believe that, just like content delivery networks (CDNs), such as Akamai, enabled the success of the Internet ecosystem through their efficient data delivery, KDNs will be critical to the success of LLM applications through their efficient knowledge delivery. We have open-sourced a KDN prototype at https://github.com/LMCache/LMCache.

**Link**: [arxiv](http://arxiv.org/abs/2409.13761v1),  [pdf](http://arxiv.org/pdf/2409.13761v1)

**Tags**: cs.CL cs.AI 



### Ejected Particles after Impact Splash on Mars: Electrification
**Authors**: T. Becker, F. C. Onyeagusi, J. Teiser, T. Jardiel, M. Peiteado, O. Munoz, J. Martikainen, J. C. Gomez Martin, J. Merrison, G. Wurm

**Updated**: 2024-09-16T13:52:46Z

**Summary**: Within the RoadMap project we investigated the microphysical aspects of particle collisions during saltation on the Martian surface in laboratory experiments. Following the size distribution of ejected particles, their aerodynamic properties and aggregation status upon ejection, we now focus on the electrification and charge distribution of ejected particles. We analyzed rebound and ejection trajectories of grains in a vacuum setup with a strong electric field of 100 kV/m and deduced particle charges from their acceleration. The ejected particles have sizes of about 10 to 100 microns. They carry charges up to $10^5$ e or charge densities up to $> 10^7$ e/mm$^2$. Within the given size range, we find a small bias towards positive charges.

**Link**: [arxiv](http://arxiv.org/abs/2409.10287v1),  [pdf](http://arxiv.org/pdf/2409.10287v1)

**Tags**: astro-ph.EP astro-ph.IM 



### Decoupling DNS Update Timing from TTL Values
**Authors**: Yehuda Afek, Ariel Litmanovich

**Updated**: 2024-09-16T11:56:09Z

**Summary**: A relatively simple safety-belt mechanism for improving DNS system availability and efficiency is proposed here. While it may seem ambitious, a careful examination shows it is both feasible and beneficial for the DNS system. The mechanism called "DNS Real-time Update" (DNSRU), a service that facilitates real-time and secure updates of cached domain records in DNS resolvers worldwide, even before the expiration of the corresponding Time To Live (TTL) values. This service allows Internet domain owners to quickly rectify any erroneous global IP address distribution, even if a long TTL value is associated with it. By addressing this critical DNS high availability issue, DNSRU eliminates the need for short TTL values and their associated drawbacks. Therefore, DNSRU DNSRU reduces the traffic load on authoritative servers while enhancing the system's fault tolerance. In this paper we show that our DNSRU design is backward compatible, supports gradual deployment, secure, efficient, and feasible.

**Link**: [arxiv](http://arxiv.org/abs/2409.10207v1),  [pdf](http://arxiv.org/pdf/2409.10207v1)

**Tags**: cs.NI 



### DARDA: Domain-Aware Real-Time Dynamic Neural Network Adaptation
**Authors**: Shahriar Rifat, Jonathan Ashdown, Francesco Restuccia

**Updated**: 2024-09-15T14:49:30Z

**Summary**: Test Time Adaptation (TTA) has emerged as a practical solution to mitigate the performance degradation of Deep Neural Networks (DNNs) in the presence of corruption/ noise affecting inputs. Existing approaches in TTA continuously adapt the DNN, leading to excessive resource consumption and performance degradation due to accumulation of error stemming from lack of supervision. In this work, we propose Domain-Aware Real-Time Dynamic Adaptation (DARDA) to address such issues. Our key approach is to proactively learn latent representations of some corruption types, each one associated with a sub-network state tailored to correctly classify inputs affected by that corruption. After deployment, DARDA adapts the DNN to previously unseen corruptions in an unsupervised fashion by (i) estimating the latent representation of the ongoing corruption; (ii) selecting the sub-network whose associated corruption is the closest in the latent space to the ongoing corruption; and (iii) adapting DNN state, so that its representation matches the ongoing corruption. This way, DARDA is more resource efficient and can swiftly adapt to new distributions caused by different corruptions without requiring a large variety of input data. Through experiments with two popular mobile edge devices - Raspberry Pi and NVIDIA Jetson Nano - we show that DARDA reduces energy consumption and average cache memory footprint respectively by 1.74x and 2.64x with respect to the state of the art, while increasing the performance by 10.4%, 5.7% and 4.4% on CIFAR-10, CIFAR-100 and TinyImagenet.

**Link**: [arxiv](http://arxiv.org/abs/2409.09753v1),  [pdf](http://arxiv.org/pdf/2409.09753v1)

**Tags**: cs.CV cs.AI cs.LG 



### Language-Queried Target Sound Extraction Without Parallel Training Data
**Authors**: Hao Ma, Zhiyuan Peng, Xu Li, Yukai Li, Mingjie Shao, Qiuqiang Kong, Ju Liu

**Updated**: 2024-09-14T10:15:37Z

**Summary**: Language-queried target sound extraction (TSE) aims to extract specific sounds from mixtures based on language queries. Traditional fully-supervised training schemes require extensively annotated parallel audio-text data, which are labor-intensive. We introduce a language-free training scheme, requiring only unlabelled audio clips for TSE model training by utilizing the multi-modal representation alignment nature of the contrastive language-audio pre-trained model (CLAP). In a vanilla language-free training stage, target audio is encoded using the pre-trained CLAP audio encoder to form a condition embedding for the TSE model, while during inference, user language queries are encoded by CLAP text encoder. This straightforward approach faces challenges due to the modality gap between training and inference queries and information leakage from direct exposure to target audio during training. To address this, we propose a retrieval-augmented strategy. Specifically, we create an embedding cache using audio captions generated by a large language model (LLM). During training, target audio embeddings retrieve text embeddings from this cache to use as condition embeddings, ensuring consistent modalities between training and inference and eliminating information leakage. Extensive experiment results show that our retrieval-augmented approach achieves consistent and notable performance improvements over existing state-of-the-art with better generalizability.

**Link**: [arxiv](http://arxiv.org/abs/2409.09398v1),  [pdf](http://arxiv.org/pdf/2409.09398v1)

**Tags**: eess.AS cs.SD 



### A Compressive Memory-based Retrieval Approach for Event Argument   Extraction
**Authors**: Wanlong Liu, Enqi Zhang, Li Zhou, Dingyi Zeng, Shaohuan Cheng, Chen Zhang, Malu Zhang, Wenyu Chen

**Updated**: 2024-09-14T05:51:50Z

**Summary**: Recent works have demonstrated the effectiveness of retrieval augmentation in the Event Argument Extraction (EAE) task. However, existing retrieval-based EAE methods have two main limitations: (1) input length constraints and (2) the gap between the retriever and the inference model. These issues limit the diversity and quality of the retrieved information. In this paper, we propose a Compressive Memory-based Retrieval (CMR) mechanism for EAE, which addresses the two limitations mentioned above. Our compressive memory, designed as a dynamic matrix that effectively caches retrieved information and supports continuous updates, overcomes the limitations of the input length. Additionally, after pre-loading all candidate demonstrations into the compressive memory, the model further retrieves and filters relevant information from memory based on the input query, bridging the gap between the retriever and the inference model. Extensive experiments show that our method achieves new state-of-the-art performance on three public datasets (RAMS, WikiEvents, ACE05), significantly outperforming existing retrieval-based EAE methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.09322v1),  [pdf](http://arxiv.org/pdf/2409.09322v1)

**Tags**: cs.CL 



### Block-Attention for Efficient RAG
**Authors**: East Sun, Yan Wang, Lan Tian

**Updated**: 2024-09-25T06:46:42Z

**Summary**: We introduce Block-Attention, an attention mechanism designed to address the increased inference latency and cost in Retrieval-Augmented Generation (RAG) scenarios. Unlike existing works that encodes the whole context, its main idea lies in dividing the retrieved documents into blocks, where each block calculates key-value (KV) states independently except for the final block. In RAG scenarios, by defining each passage as a block, Block-Attention enables us to pre-compute the KV states for all passages and cache them in memory, significantly reducing the latency and the computation cost during inference. The implementation involves block segmentation, positional encoding calculation, and fine-tuning the LLM to adapt to the Block-Attention mechanism. Experiments on four RAG benchmarks demonstrate that after block fine-tuning, the Block Attention model can achieve performance comparable to (68.4\% vs 67.9\% on Llama3) or even better (62.8\% vs 59.6\% on Mistral) than self-attention models. Notably, Block-Attention reduces the TTFT (the time to first token) and FLOPs (floating point operations) to a very low level. It only takes 45 ms to output the first token for an input sequence with a total length of 32K. Compared with the self-attention model, the time consumption and corresponding FLOPs are reduced by 98.7\% and 99.8\%, respectively.

**Link**: [arxiv](http://arxiv.org/abs/2409.15355v2),  [pdf](http://arxiv.org/pdf/2409.15355v2)

**Tags**: cs.LG cs.AI cs.CL 



### WarmSwap: Sharing Dependencies for Accelerating Cold Starts in   Serverless Functions
**Authors**: Rui Li, Devesh Tiwari, Gene Cooperman

**Updated**: 2024-09-13T21:31:45Z

**Summary**: This work presents WarmSwap, a novel provider-side cold-start optimization for serverless computing. This optimization reduces cold-start time when booting and loading dependencies at runtime inside a function container. Previous approaches to the optimization of cold starts tend to fall into two categories: optimizing the infrastructure of serverless computing to benefit all serverless functions; or function-specific tuning for individual serverless functions. In contrast, WarmSwap offers a broad middle ground, which optimizes entire categories of serverless functions. WarmSwap eliminates the need to initialize middleware or software dependencies when launching a new serverless container, by migrating a pre-initialized live dependency image to the new function instance. WarmSwap respects the provider's cache constraints, as a single pre-warmed dependency image in the cache is shared among all serverless functions requiring that software dependency image. WarmSwap has been tested on seven representative functions from FunctionBench. The functions are chosen to compare with previous work. In those tests, WarmSwap accelerates cold-start executions for those serverless functions with large dependency requirements by a factor ranging from 1.2 to 2.2.

**Link**: [arxiv](http://arxiv.org/abs/2409.09202v1),  [pdf](http://arxiv.org/pdf/2409.09202v1)

**Tags**: cs.DC 



### Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent   Interconnects
**Authors**: Anastasiia Ruzhanskaia, Pengcheng Xu, David Cock, Timothy Roscoe

**Updated**: 2024-09-12T15:34:23Z

**Summary**: Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism. In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers. We argue that the assumptions that led to this model are obsolete, and in many use-cases use of programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system. We quantitatively demonstrate these advantages using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions. Moreover, we show that while these advantages are significant over a modern PCIe peripheral bus, a truly cache-coherent interconnect offers significant additional efficiency gains.

**Link**: [arxiv](http://arxiv.org/abs/2409.08141v1),  [pdf](http://arxiv.org/pdf/2409.08141v1)

**Tags**: cs.AR cs.OS 



### Light Induced Orbital Magnetism in Metals via Inverse Faraday Effect
**Authors**: Priya Sharma, Alexander V. Balatsky

**Updated**: 2024-09-12T10:35:15Z

**Summary**: We present a microscopic calculation of the inverse Faraday effect in metals. We derive a static local magnetic moment induced on the application of high-frequency light, using the Eilenberger formulation of quasiclassical theory. We include the effect of disorder and formulate a theory applicable across the entire temperature range, in the absence of external applied fields. For light-induced electric fields of amplitude $\sim 100 kV/cm$, the induced fields are large, $\sim 0.1 T$ for metallic Nb! The predictions of our theory agree with recent experimental and theoretical results [1]. An extension of this approach to superconductors would open a new route of inducing orbital magnetic field and potentially vortices in superconductors.

**Link**: [arxiv](http://arxiv.org/abs/2303.01699v5),  [pdf](http://arxiv.org/pdf/2303.01699v5)

**Tags**: cond-mat.supr-con 



### Super Monotonic Alignment Search
**Authors**: Junhyeok Lee, Hyeongju Kim

**Updated**: 2024-09-12T02:13:57Z

**Summary**: Monotonic alignment search (MAS), introduced by Glow-TTS, is one of the most popular algorithm in TTS to estimate unknown alignments between text and speech. Since this algorithm needs to search for the most probable alignment with dynamic programming by caching all paths, the time complexity of the algorithm is $O(T \times S)$. The authors of Glow-TTS run this algorithm on CPU, and while they mentioned it is difficult to parallelize, we found that MAS can be parallelized in text-length dimension and CPU execution consumes an inordinate amount of time for inter-device copy. Therefore, we implemented a Triton kernel and PyTorch JIT script to accelerate MAS on GPU without inter-device copy. As a result, Super-MAS Triton kernel is up to 72 times faster in the extreme-length case. The code is available at \url{https://github.com/supertone-inc/super-monotonic-align}.

**Link**: [arxiv](http://arxiv.org/abs/2409.07704v1),  [pdf](http://arxiv.org/pdf/2409.07704v1)

**Tags**: eess.AS cs.AI 



### Learning to Compress Contexts for Efficient Knowledge-based Visual   Question Answering
**Authors**: Weixi Weng, Jieming Zhu, Hao Zhang, Xiaojun Meng, Rui Zhang, Chun Yuan

**Updated**: 2024-09-11T15:11:39Z

**Summary**: Multimodal Large Language Models (MLLMs) have demonstrated great zero-shot performance on visual question answering (VQA). However, when it comes to knowledge-based VQA (KB-VQA), MLLMs may lack human commonsense or specialized domain knowledge to answer such questions and require obtaining necessary information from external knowledge sources. Previous works like Retrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input information, such as image-based textual descriptions and retrieved knowledge, as possible to improve performance, but they all overlook the issue that with the number of input tokens increasing, inference efficiency significantly decreases, which contradicts the demands of practical applications. To address this issue, we propose Retrieval-Augmented MLLM with Compressed Contexts (RACC). RACC learns to compress and aggregate retrieved contexts, from which it generates a compact modulation in the form of Key-Value (KV) cache. This modulation is then used to adapt the downstream frozen MLLM, thereby achieving effective and efficient inference. RACC achieves a state-of-the-art (SOTA) performance of 62.9% on OK-VQA. Moreover, it significantly reduces inference latency by 22.0%-59.7% compared to the prominent RAVQA-v2. Abundant experiments show RACC's broad applicability. It is compatible with various off-the-shelf MLLMs and can also handle different knowledge sources including textual and multimodal documents.

**Link**: [arxiv](http://arxiv.org/abs/2409.07331v1),  [pdf](http://arxiv.org/pdf/2409.07331v1)

**Tags**: cs.CV cs.LG 



### Inf-MLLM: Efficient Streaming Inference of Multimodal Large Language   Models on a Single GPU
**Authors**: Zhenyu Ning, Jieru Zhao, Qihao Jin, Wenchao Ding, Minyi Guo

**Updated**: 2024-09-11T12:44:12Z

**Summary**: Multimodal Large Language Models (MLLMs) are distinguished by their multimodal comprehensive ability and widely used in many real-world applications including GPT-4o, autonomous driving and robotics. Despite their impressive performance, the multimodal inputs always incur long context. The inference under long context requires caching massive Key and Value states (KV cache) of previous tokens, which introduces high latency and excessive memory consumption. Due to this reason, it is challenging to deploy streaming inference of MLLMs on edge devices, which largely constrains the power and usage of MLLMs in real-world applications. In this paper, we introduce Inf-MLLM, an efficient inference framework for MLLMs, which enable streaming inference of MLLM on a single GPU with infinite context. Inf-MLLM is based on our key observation of the attention pattern in both LLMs and MLLMs called "attention saddles". Thanks to the newly discovered attention pattern, Inf-MLLM maintains a size-constrained KV cache by dynamically caching recent tokens and relevant tokens. Furthermore, Inf-MLLM proposes attention bias, a novel approach to enable MLLMs to capture long-term dependency. We show that Inf-MLLM enables multiple LLMs and MLLMs to achieve stable performance over 4M-token long texts and multi-round conversations with 1-hour-long videos on a single GPU. In addition, Inf-MLLM exhibits superior streaming reasoning quality than existing methods such as StreamingLLM and 2x speedup than H2O.

**Link**: [arxiv](http://arxiv.org/abs/2409.09086v1),  [pdf](http://arxiv.org/pdf/2409.09086v1)

**Tags**: cs.LG cs.AI cs.CV cs.DC cs.PF 



### Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma   Generated THz Pulses
**Authors**: Benjamin Colmey, Rodrigo T. Paulino, David G. Cooke

**Updated**: 2024-09-11T11:40:23Z

**Summary**: Terahertz pulses generated by two-color laser plasmas have reported peak field strengths exceeding MV/cm, and when illuminating metal nanotips the near-field enhancement at the tip apex should result in extremely high bunch charges and electron energies via sub-cycle cold field emission. Here, electron emission from tungsten nanotips driven by THz pulses generated by a long filament air-plasma are reported. Electron energies up to 1.1 keV and bunch charges up to 2x$10^5$ electrons per pulse were detected, well below values expected for peak field calculated via the time averaged Poynting vector. Investigations revealed a failure in the use of the time-averaged Poynting vector when applied to long filament THz pulses, due to spatio-temporal restructuring of the THz pulse in the focus. Accounting for this restructuring significantly reduces the field strength to approximately 160 ~kV/cm, consistent with the observed electron bunch charges, peak energies and their dependence on the tip position in the THz focus. Despite these findings, our results surpass previous THz plasma-driven electron generation by an order of magnitude in both electron energy and bunch charge and a path to increasing these by an additional order of magnitude by modification of the THz optics is proposed.

**Link**: [arxiv](http://arxiv.org/abs/2409.07196v1),  [pdf](http://arxiv.org/pdf/2409.07196v1)

**Tags**: cond-mat.mtrl-sci physics.plasm-ph 



### In-Loop Filtering via Trained Look-Up Tables
**Authors**: Zhuoyuan Li, Jiacheng Li, Yao Li, Li Li, Dong Liu, Feng Wu

**Updated**: 2024-09-11T08:12:55Z

**Summary**: In-loop filtering (ILF) is a key technology for removing the artifacts in image/video coding standards. Recently, neural network-based in-loop filtering methods achieve remarkable coding gains beyond the capability of advanced video coding standards, which becomes a powerful coding tool candidate for future video coding standards. However, the utilization of deep neural networks brings heavy time and computational complexity, and high demands of high-performance hardware, which is challenging to apply to the general uses of coding scene. To address this limitation, inspired by explorations in image restoration, we propose an efficient and practical in-loop filtering scheme by adopting the Look-up Table (LUT). We train the DNN of in-loop filtering within a fixed filtering reference range, and cache the output values of the DNN into a LUT via traversing all possible inputs. At testing time in the coding process, the filtered pixel is generated by locating input pixels (to-be-filtered pixel with reference pixels) and interpolating cached filtered pixel values. To further enable the large filtering reference range with the limited storage cost of LUT, we introduce the enhanced indexing mechanism in the filtering process, and clipping/finetuning mechanism in the training. The proposed method is implemented into the Versatile Video Coding (VVC) reference software, VTM-11.0. Experimental results show that the ultrafast, very fast, and fast mode of the proposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39% BD-rate reduction, under the all intra (AI) and random access (RA) configurations. Especially, our method has friendly time and computational complexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel, and only 164-1148 KB storage cost for a single model. Our solution may shed light on the journey of practical neural network-based coding tool evolution.

**Link**: [arxiv](http://arxiv.org/abs/2407.10926v2),  [pdf](http://arxiv.org/pdf/2407.10926v2)

**Tags**: eess.IV cs.CV 



### Exploiting Deep Reinforcement Learning for Edge Caching in Cell-Free   Massive MIMO Systems
**Authors**: Yu Zhang, Shuaifei Chen, Jiayi Zhang

**Updated**: 2024-09-11T02:33:06Z

**Summary**: Cell-free massive multiple-input-multiple-output is promising to meet the stringent quality-of-experience (QoE) requirements of railway wireless communications by coordinating many successional access points (APs) to serve the onboard users coherently. A key challenge is how to deliver the desired contents timely due to the radical changing propagation environment caused by the growing train speed. In this paper, we propose to proactively cache the likely-requesting contents at the upcoming APs which perform the coherent transmission to reduce end-to-end delay. A long-term QoE-maximization problem is formulated and two cache placement algorithms are proposed. One is based on heuristic convex optimization (HCO) and the other exploits deep reinforcement learning (DRL) with soft actor-critic (SAC). Compared to the conventional benchmark, numerical results show the advantage of our proposed algorithms on QoE and hit probability. With the advanced DRL model, SAC outperforms HCO on QoE by predicting the user requests accurately.

**Link**: [arxiv](http://arxiv.org/abs/2208.12453v2),  [pdf](http://arxiv.org/pdf/2208.12453v2)

**Tags**: cs.IT cs.AI math.IT 



### With Greater Text Comes Greater Necessity: Inference-Time Training Helps   Long Text Generation
**Authors**: Y. Wang, D. Ma, D. Cai

**Updated**: 2024-09-11T02:22:58Z

**Summary**: Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling benchmark and the GuoFeng discourse-level translation benchmark validate the effectiveness of Temp-Lora. Our results show that: 1) Temp-Lora substantially enhances generation quality for long text, as indicated by a 13.2% decrease in perplexity (PPL) on a subset of PG19, and a 29.3% decrease in PPL along with a 113.2% increase in BLEU score on a subset of GuoFeng, 2) Temp-Lora is compatible with and enhances most existing long text generation methods, and 3) Temp-Lora can greatly reduce computational costs by shortening the context window. For example, we can ensure a moderate improvement in generation quality (a decrease of 3.8% in PPL) while enabling a 51.5% memory usage reduction and a 60.0% decrease in latency for inference.

**Link**: [arxiv](http://arxiv.org/abs/2401.11504v3),  [pdf](http://arxiv.org/pdf/2401.11504v3)

**Tags**: cs.CL cs.AI 



### DACAT: Dual-stream Adaptive Clip-aware Time Modeling for Robust Online   Surgical Phase Recognition
**Authors**: Kaixiang Yang, Qiang Li, Zhiwei Wang

**Updated**: 2024-09-10T04:58:48Z

**Summary**: Surgical phase recognition has become a crucial requirement in laparoscopic surgery, enabling various clinical applications like surgical risk forecasting. Current methods typically identify the surgical phase using individual frame-wise embeddings as the fundamental unit for time modeling. However, this approach is overly sensitive to current observations, often resulting in discontinuous and erroneous predictions within a complete surgical phase. In this paper, we propose DACAT, a novel dual-stream model that adaptively learns clip-aware context information to enhance the temporal relationship. In one stream, DACAT pretrains a frame encoder, caching all historical frame-wise features. In the other stream, DACAT fine-tunes a new frame encoder to extract the frame-wise feature at the current moment. Additionally, a max clip-response read-out (Max-R) module is introduced to bridge the two streams by using the current frame-wise feature to adaptively fetch the most relevant past clip from the feature cache. The clip-aware context feature is then encoded via cross-attention between the current frame and its fetched adaptive clip, and further utilized to enhance the time modeling for accurate online surgical phase recognition. The benchmark results on three public datasets, i.e., Cholec80, M2CAI16, and AutoLaparo, demonstrate the superiority of our proposed DACAT over existing state-of-the-art methods, with improvements in Jaccard scores of at least 4.5%, 4.6%, and 2.7%, respectively. Our code and models have been released at https://github.com/kk42yy/DACAT.

**Link**: [arxiv](http://arxiv.org/abs/2409.06217v1),  [pdf](http://arxiv.org/pdf/2409.06217v1)

**Tags**: cs.CV 



### Design and Implementation of Online Live Streaming System Using A 3D   Engine
**Authors**: Aizierjiang Aiersilan

**Updated**: 2024-09-10T04:24:22Z

**Summary**: With the growing demand for live video streaming, there is an increasing need for low-latency and high-quality transmission, especially with the advent of 5G networks. While 5G offers hardware-level improvements, effective software solutions for minimizing latency remain essential. Current methods, such as multi-channel streaming, fail to address latency issues fundamentally, often only adding new channels without optimizing overall performance. This thesis proposes a novel approach using a 3D engine (e.g., Unity 3D) to stream multi-input video data through a single channel with reduced latency. By leveraging 3D engine capabilities, such as World/Screen Space Cameras, 3D Canvases, and Webcam Textures, the proposed system consolidates video streams from multiple external cameras into a unified, low-latency output. The affiliated project of this thesis demonstrates the implementation of a low-latency multi-channel live video streaming system. It employs the RTSP protocol and examines video encoding techniques, alongside a client-side application based on Unity 3D. The system architecture includes a WebSocket server for persistent connections, an HTTP server for communication, a MySQL database for storage, Redis for caching, and Nginx for load balancing. Each module operates independently, ensuring flexibility and scalability in the system's design. A key innovation of this system is its use of a 3D scene to map multiple video inputs onto a virtual canvas, recorded by an in-engine camera for transmission. This design minimizes redundant data, enabling an efficient and director-guided live streaming network. The thesis concludes by discussing challenges encountered during the project and provides solutions for future improvement.

**Link**: [arxiv](http://arxiv.org/abs/2409.06207v1),  [pdf](http://arxiv.org/pdf/2409.06207v1)

**Tags**: cs.NI cs.MM 



### Flash Cache: Reducing Bias in Radiance Cache Based Inverse Rendering
**Authors**: Benjamin Attal, Dor Verbin, Ben Mildenhall, Peter Hedman, Jonathan T. Barron, Matthew O'Toole, Pratul P. Srinivasan

**Updated**: 2024-09-09T17:59:57Z

**Summary**: State-of-the-art techniques for 3D reconstruction are largely based on volumetric scene representations, which require sampling multiple points to compute the color arriving along a ray. Using these representations for more general inverse rendering -- reconstructing geometry, materials, and lighting from observed images -- is challenging because recursively path-tracing such volumetric representations is expensive. Recent works alleviate this issue through the use of radiance caches: data structures that store the steady-state, infinite-bounce radiance arriving at any point from any direction. However, these solutions rely on approximations that introduce bias into the renderings and, more importantly, into the gradients used for optimization. We present a method that avoids these approximations while remaining computationally efficient. In particular, we leverage two techniques to reduce variance for unbiased estimators of the rendering equation: (1) an occlusion-aware importance sampler for incoming illumination and (2) a fast cache architecture that can be used as a control variate for the radiance from a high-quality, but more expensive, volumetric cache. We show that by removing these biases our approach improves the generality of radiance cache based inverse rendering, as well as increasing quality in the presence of challenging light transport effects such as specular reflections.

**Link**: [arxiv](http://arxiv.org/abs/2409.05867v1),  [pdf](http://arxiv.org/pdf/2409.05867v1)

**Tags**: cs.GR cs.CV 



### WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild
**Authors**: Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, Yejin Choi

**Updated**: 2024-09-09T10:04:00Z

**Summary**: The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis' utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.

**Link**: [arxiv](http://arxiv.org/abs/2409.03753v2),  [pdf](http://arxiv.org/pdf/2409.03753v2)

**Tags**: cs.CL cs.AI cs.HC cs.IR cs.LG 



### Cooperative Learning-Based Framework for VNF Caching and Placement   Optimization over Low Earth Orbit Satellite Networks
**Authors**: Khai Doan, Marios Avgeris, Aris Leivadeas, Ioannis Lambadaris, Wonjae Shin

**Updated**: 2024-09-08T08:39:50Z

**Summary**: Low Earth Orbit Satellite Networks (LSNs) are integral to supporting a broad range of modern applications, which are typically modeled as Service Function Chains (SFCs). Each SFC is composed of Virtual Network Functions (VNFs), where each VNF performs a specific task. In this work, we tackle two key challenges in deploying SFCs across an LSN. Firstly, we aim to optimize the long-term system performance by minimizing the average end-to-end SFC execution delay, given that each satellite comes with a pre-installed/cached subset of VNFs. To achieve optimal SFC placement, we formulate an offline Dynamic Programming (DP) equation. To overcome the challenges associated with DP, such as its complexity, the need for probability knowledge, and centralized decision-making, we put forth an online Multi-Agent Q-Learning (MAQL) solution. Our MAQL approach addresses convergence issues in the non-stationary LSN environment by enabling satellites to share learning parameters and update their Q-tables based on distinct rules for their selected actions. Secondly, to determine the optimal VNF subsets for satellite caching, we develop a Bayesian Optimization (BO)-based learning mechanism that operates both offline and continuously in the background during runtime. Extensive experiments demonstrate that our MAQL approach achieves near-optimal performance comparable to the DP model and significantly outperforms existing baselines. Moreover, the BO-based approach effectively enhances the request serving rate over time.

**Link**: [arxiv](http://arxiv.org/abs/2409.05025v1),  [pdf](http://arxiv.org/pdf/2409.05025v1)

**Tags**: cs.IT cs.SY eess.SY math.IT 



### InstInfer: In-Storage Attention Offloading for Cost-Effective   Long-Context LLM Inference
**Authors**: Xiurui Pan, Endian Li, Qiao Li, Shengwen Liang, Yizhou Shan, Ke Zhou, Yingwei Luo, Xiaolin Wang, Jie Zhang

**Updated**: 2024-09-08T06:06:44Z

**Summary**: The widespread of Large Language Models (LLMs) marks a significant milestone in generative AI. Nevertheless, the increasing context length and batch size in offline LLM inference escalate the memory requirement of the key-value (KV) cache, which imposes a huge burden on the GPU VRAM, especially for resource-constraint scenarios (e.g., edge computing and personal devices). Several cost-effective solutions leverage host memory or SSDs to reduce storage costs for offline inference scenarios and improve the throughput. Nevertheless, they suffer from significant performance penalties imposed by intensive KV cache accesses due to limited PCIe bandwidth. To address these issues, we propose InstInfer, a novel LLM inference system that offloads the most performance-critical computation (i.e., attention in decoding phase) and data (i.e., KV cache) parts to Computational Storage Drives (CSDs), which minimize the enormous KV transfer overheads. InstInfer designs a dedicated flash-aware in-storage attention engine with KV cache management mechanisms to exploit the high internal bandwidths of CSDs instead of being limited by the PCIe bandwidth. The optimized P2P transmission between GPU and CSDs further reduces data migration overheads. Experimental results demonstrate that for a 13B model using an NVIDIA A6000 GPU, InstInfer improves throughput for long-sequence inference by up to 11.1$\times$, compared to existing SSD-based solutions such as FlexGen.

**Link**: [arxiv](http://arxiv.org/abs/2409.04992v1),  [pdf](http://arxiv.org/pdf/2409.04992v1)

**Tags**: cs.AR cs.CL 



### Training-Free Style Consistent Image Synthesis with Condition and Mask   Guidance in E-Commerce
**Authors**: Guandong Li

**Updated**: 2024-09-07T07:50:13Z

**Summary**: Generating style-consistent images is a common task in the e-commerce field, and current methods are largely based on diffusion models, which have achieved excellent results. This paper introduces the concept of the QKV (query/key/value) level, referring to modifications in the attention maps (self-attention and cross-attention) when integrating UNet with image conditions. Without disrupting the product's main composition in e-commerce images, we aim to use a train-free method guided by pre-set conditions. This involves using shared KV to enhance similarity in cross-attention and generating mask guidance from the attention map to cleverly direct the generation of style-consistent images. Our method has shown promising results in practical applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.04750v1),  [pdf](http://arxiv.org/pdf/2409.04750v1)

**Tags**: cs.CV 



### MiniCache: KV Cache Compression in Depth Dimension for Large Language   Models
**Authors**: Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Gholamreza Haffari, Bohan Zhuang

**Updated**: 2024-09-07T02:52:29Z

**Summary**: A critical approach for efficiently deploying computationally demanding large language models (LLMs) is Key-Value (KV) caching. The KV cache stores key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thereby lowering latency in autoregressive generation. However, the size of the KV cache grows linearly with sequence length, posing challenges for applications requiring long context input and extensive sequence generation. In this paper, we present a simple yet effective approach, called MiniCache, to compress the KV cache across layers from a novel depth perspective, significantly reducing the memory footprint for LLM inference. Our approach is based on the observation that KV cache states exhibit high similarity between the adjacent layers in the middle-to-deep portion of LLMs. To facilitate merging, we propose disentangling the states into the magnitude and direction components, interpolating the directions of the state vectors while preserving their lengths unchanged. Furthermore, we introduce a token retention strategy to keep highly distinct state pairs unmerged, thus preserving the information with minimal additional storage overhead. Our MiniCache is training-free and general, complementing existing KV cache compression strategies, such as quantization and sparsity. We conduct a comprehensive evaluation of MiniCache utilizing various models including LLaMA-2, LLaMA-3, Phi-3, Mistral, and Mixtral across multiple benchmarks, demonstrating its exceptional performance in achieving superior compression ratios and high throughput. On the ShareGPT dataset, LLaMA-2-7B with 4-bit MiniCache achieves a remarkable compression ratio of up to 5.02x, enhances inference throughput by approximately 5x, and reduces the memory footprint by 41% compared to the FP16 full cache baseline, all while maintaining near-lossless performance.

**Link**: [arxiv](http://arxiv.org/abs/2405.14366v2),  [pdf](http://arxiv.org/pdf/2405.14366v2)

**Tags**: cs.CL cs.AI cs.LG 



### QET: Enhancing Quantized LLM Parameters and KV cache Compression through   Element Substitution and Residual Clustering
**Authors**: Yanshu Wang, Wang Li, Zhaoqian Yao, Tong Yang

**Updated**: 2024-09-06T08:28:01Z

**Summary**: The matrix quantization entails representing matrix elements in a more space-efficient form to reduce storage usage, with dequantization restoring the original matrix for use. We formulate the Quantization Error Minimization (QEM) problem as minimizing the distance between a matrix before and after quantization, under the condition that the quantized matrix occupies the same memory space. Matrix quantization is crucial in various applications, including Large Language Models (LLMs) weight quantization, vector databases, KV cache quantization, graph compression, and image compression. Recent advancements in LLMs, such as GPT-4 and BERT, have highlighted the importance of matrix compression due to the large size of parameters and KV cache, which are stored as matrices.   We propose Quantum Entanglement Trees (QET) to address the QEM problem by leveraging the local orderliness of matrix elements, involving iterative element swapping to form a locally ordered matrix. This matrix is then grouped and quantized by columns. To enhance QET, we introduce two optimizations: further quantizing residuals to reduce MSE, and using masking and batch processing to accelerate the algorithm.   Experimental results demonstrate that QET can effectively reduce MSE to 5.05%, 13.33%, and 11.89% of the current best method on the LLM dataset, K cache, and V cache, respectively. Our contributions include the abstraction of the QEM problem, the design of the QET algorithm, and the proposal of two optimizations to improve accuracy and speed.

**Link**: [arxiv](http://arxiv.org/abs/2407.03637v4),  [pdf](http://arxiv.org/pdf/2407.03637v4)

**Tags**: cs.LG cs.CL 



### A First Look At Efficient And Secure On-Device LLM Inference Against KV   Leakage
**Authors**: Huan Yang, Deyu Zhang, Yudong Zhao, Yuanchun Li, Yunxin Liu

**Updated**: 2024-09-06T06:16:55Z

**Summary**: Running LLMs on end devices has garnered significant attention recently due to their advantages in privacy preservation. With the advent of lightweight LLM models and specially designed GPUs, on-device LLM inference has achieved the necessary accuracy and performance metrics. However, we have identified that LLM inference on GPUs can leak privacy-sensitive intermediate information, specifically the KV pairs. An attacker could exploit these KV pairs to reconstruct the entire user conversation, leading to significant vulnerabilities. Existing solutions, such as Fully Homomorphic Encryption (FHE) and Trusted Execution Environments (TEE), are either too computation-intensive or resource-limited. To address these issues, we designed KV-Shield, which operates in two phases. In the initialization phase, it permutes the weight matrices so that all KV pairs are correspondingly permuted. During the runtime phase, the attention vector is inversely permuted to ensure the correctness of the layer output. All permutation-related operations are executed within the TEE, ensuring that insecure GPUs cannot access the original KV pairs, thus preventing conversation reconstruction. Finally, we theoretically analyze the correctness of KV-Shield, along with its advantages and overhead.

**Link**: [arxiv](http://arxiv.org/abs/2409.04040v1),  [pdf](http://arxiv.org/pdf/2409.04040v1)

**Tags**: cs.CR cs.AI 



### Potential and Limitation of High-Frequency Cores and Caches
**Authors**: Kunal Pai, Anusheel Nand, Jason Lowe-Power

**Updated**: 2024-09-05T20:21:54Z

**Summary**: This paper explores the potential of cryogenic semiconductor computing and superconductor electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Conventional semiconductor electronics operating at cryogenic temperatures (below -150{\deg}C or 123.15 K) can benefit from reduced leakage currents and improved electron mobility. On the other hand, superconductor electronics, operating below 10 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconductor electronics and cryogenic semiconductor computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconductor technologies, laying the foundation for future research in this field using gem5.

**Link**: [arxiv](http://arxiv.org/abs/2408.03308v2),  [pdf](http://arxiv.org/pdf/2408.03308v2)

**Tags**: cs.AR 



### Libra: Architectural Support For Principled, Secure And Efficient   Balanced Execution On High-End Processors (Extended Version)
**Authors**: Hans Winderix, Marton Bognar, Lesly-Ann Daniel, Frank Piessens

**Updated**: 2024-09-05T17:56:19Z

**Summary**: Control-flow leakage (CFL) attacks enable an attacker to expose control-flow decisions of a victim program via side-channel observations. Linearization (i.e., elimination) of secret-dependent control flow is the main countermeasure against these attacks, yet it comes at a non-negligible cost. Conversely, balancing secret-dependent branches often incurs a smaller overhead, but is notoriously insecure on high-end processors. Hence, linearization has been widely believed to be the only effective countermeasure against CFL attacks. In this paper, we challenge this belief and investigate an unexplored alternative: how to securely balance secret-dependent branches on higher-end processors?   We propose Libra, a generic and principled hardware-software codesign to efficiently address CFL on high-end processors. We perform a systematic classification of hardware primitives leaking control flow from the literature, and provide guidelines to handle them with our design. Importantly, Libra enables secure control-flow balancing without the need to disable performance-critical hardware such as the instruction cache and the prefetcher. We formalize the semantics of Libra and propose a code transformation algorithm for securing programs, which we prove correct and secure. Finally, we implement and evaluate Libra on an out-of-order RISC-V processor, showing performance overhead on par with insecure balanced code, and outperforming state-of-the-art linearized code by 19.3%.

**Link**: [arxiv](http://arxiv.org/abs/2409.03743v1),  [pdf](http://arxiv.org/pdf/2409.03743v1)

**Tags**: cs.CR 



### Enabling Practical and Privacy-Preserving Image Processing
**Authors**: Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao

**Updated**: 2024-09-05T14:22:02Z

**Summary**: Fully Homomorphic Encryption (FHE) enables computations on encrypted data, preserving confidentiality without the need for decryption. However, FHE is often hindered by significant performance overhead, particularly for high-precision and complex data like images. Due to serious efficiency issues, traditional FHE methods often encrypt images by monolithic data blocks (such as pixel rows), instead of pixels. However, this strategy compromises the advantages of homomorphic operations and disables pixel-level image processing. In this study, we address these challenges by proposing and implementing a pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS scheme. To enhance computational efficiency, we introduce three novel caching mechanisms to pre-encrypt radix values or frequently occurring pixel values, substantially reducing redundant encryption operations. Extensive experiments demonstrate that our approach achieves up to a 19-fold improvement in encryption speed compared to the original CKKS, while maintaining high image quality. Additionally, real-world image applications such as mean filtering, brightness enhancement, image matching and watermarking are tested based on FHE, showcasing up to a 91.53% speed improvement. We also proved that our method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure, providing strong encryption security. These results underscore the practicality and efficiency of iCHEETAH, marking a significant advancement in privacy-preserving image processing at scale.

**Link**: [arxiv](http://arxiv.org/abs/2409.03568v1),  [pdf](http://arxiv.org/pdf/2409.03568v1)

**Tags**: cs.CR C.2.0; K.6.5 



### SELCC: Coherent Caching over Compute-Limited Disaggregated Memory
**Authors**: Ruihong Wang, Jianguo Wang, Walid G. Aref

**Updated**: 2024-09-05T01:12:04Z

**Summary**: Disaggregating memory from compute offers the opportunity to better utilize stranded memory in data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes to save on round-trip communication cost between the disaggregated memory and the compute nodes. However, the limited computing power on the disaggregated memory servers makes it challenging to maintain cache coherence among multiple compute-side caches over disaggregated shared memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. SELCC builds on a one-sided shared-exclusive latch protocol by introducing lazy latch release and invalidation messages among the compute nodes so that it can guarantee both data access atomicity and cache coherence. SELCC minimizes communication round-trips by embedding the current cache copy holder IDs into RDMA latch words and prioritizes local concurrency control over global concurrency control. We instantiate the SELCC protocol onto compute-sided cache, forming an abstraction layer over disaggregated memory. This abstraction layer provides main-memory-like APIs to upper-level applications, and thus enabling existing data structures and algorithms to function over disaggregated memory with minimal code change. To demonstrate the usability of SELCC, we implement a B-tree and three transaction concurrency control algorithms over SELCC's APIs. Micro-benchmark results show that the SELCC protocol achieves better performance compared to RPC-based cache-coherence protocols. Additionally, YCSB and TPC-C benchmarks indicate that applications over SELCC can achieve comparable or superior performance against competitors over disaggregated memory.

**Link**: [arxiv](http://arxiv.org/abs/2409.02088v2),  [pdf](http://arxiv.org/pdf/2409.02088v2)

**Tags**: cs.DB cs.DC cs.ET 



### Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in   Fine-tuning LLMs for Simultaneous Translation
**Authors**: Matthew Raffel, Victor Agostinelli, Lizhong Chen

**Updated**: 2024-09-05T01:06:40Z

**Summary**: Large language models (LLMs) have achieved state-of-the-art performance in various language processing tasks, motivating their adoption in simultaneous translation. Current fine-tuning methods to adapt LLMs for simultaneous translation focus on prompting optimization strategies using either data augmentation or prompt structure modifications. However, these methods suffer from several issues, such as unnecessarily expanded training sets, computational inefficiency from dumping the key and value cache, increased prompt sizes, or restriction to a single decision policy. To eliminate these issues, in this work, we propose SimulMask, a new paradigm for fine-tuning LLMs for simultaneous translation. It utilizes a novel attention mask approach that models simultaneous translation during fine-tuning by masking attention for a desired decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT 2017 dataset, we have observed a significant translation quality improvement compared to state-of-the-art prompting optimization strategies on five language pairs while reducing the computational cost.

**Link**: [arxiv](http://arxiv.org/abs/2405.10443v3),  [pdf](http://arxiv.org/pdf/2405.10443v3)

**Tags**: cs.CL cs.LG 



### SparQ Attention: Bandwidth-Efficient LLM Inference
**Authors**: Luka Ribar, Ivan Chelombiev, Luke Hudlass-Galley, Charlie Blake, Carlo Luschi, Douglas Orr

**Updated**: 2024-09-04T10:04:52Z

**Summary**: The computational difficulties of large language model (LLM) inference remain a significant obstacle to their widespread deployment. The need for many applications to support long input sequences and process them in large batches typically causes token-generation to be bottlenecked by data transfer. For this reason, we introduce SparQ Attention, a technique for increasing the inference throughput of LLMs by utilising memory bandwidth more efficiently within the attention layers, through selective fetching of the cached history. Our proposed technique can be applied directly to off-the-shelf LLMs during inference, without requiring any modification to the pre-training setup or additional fine-tuning. We show that SparQ Attention brings up to 8x savings in attention data transfers without substantial drops in accuracy, by evaluating Llama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream tasks.

**Link**: [arxiv](http://arxiv.org/abs/2312.04985v6),  [pdf](http://arxiv.org/pdf/2312.04985v6)

**Tags**: cs.LG 



### A brown dwarf orbiting around the planetary-nebula central binary KV Vel
**Authors**: S. -B. Qian, L. -Y. Zhu, F. -X. Li, L. -J. Li, Z. -T. Han, J. -J. He, L. Zang, L. -F. Chang, Q. -B. Sun, M. -Y. Li, H. -T. Zhang, F. -Z. Yan

**Updated**: 2024-09-04T07:13:01Z

**Summary**: KV Vel is a non-eclipsing short-period (P = 0.3571 days) close binary containing a very hot subdwarf primary (77000 K) and a cool low-mass secondary star (3400 K) that is located at the center of the planetary nebula DS 1. The changes in the orbital period of the close binary were analyzed based on 262 new times of light maximum together with those compiled from the literature. It is discovered that the O-C curve shows a small-amplitude (0.0034 days) cyclic period variation with a period of 29.55 years. The explanation by the solar-type magnetic activity cycles of the cool component is ruled out because the required energies are much larger than the total radiant energy of this component in a whole cycle. Therefore, the cyclic variation was plausibly explained as the light-travel time effect via the presence of a tertiary component, which is supported by the periodic changes of the O-C curve and the rather symmetric and stable light curves obtained by TESS. The mass of the tertiary companion is determined to be M_3sini' = 0.060(7) M_sun. If the third body is coplanar with the central binary (i.e., i' = 62.5{\deg}), the mass of the tertiary component is computed as M_3 ~ 0.068 M\sun, and thus it would be below the stable hydrogen-burning limit and is a brown dwarf. The orbital separation is shorter than 9.35 astronomical units (AU). KV Vel together with its surrounding planetary nebula and the brown-dwarf companion may be formed through the common-envelope evolution after the primary filled its Roche lobe during the early asymptotic giant branch stage.

**Link**: [arxiv](http://arxiv.org/abs/2409.02480v1),  [pdf](http://arxiv.org/pdf/2409.02480v1)

**Tags**: astro-ph.SR 



### Contemporary Model Compression on Large Language Models Inference
**Authors**: Dong Liu

**Updated**: 2024-09-03T15:35:01Z

**Summary**: Large Language Models (LLMs) have revolutionized natural language processing by achieving state-of-the-art results across a variety of tasks. However, the computational demands of LLM inference, including high memory consumption and slow processing speeds, pose significant challenges for real-world applications, particularly on resource-constrained devices. Efficient inference is crucial for scaling the deployment of LLMs to a broader range of platforms, including mobile and edge devices.   This survey explores contemporary techniques in model compression that address these challenges by reducing the size and computational requirements of LLMs while maintaining their performance. We focus on model-level compression methods, including quantization, knowledge distillation, and pruning, as well as system-level optimizations like KV cache efficient design. Each of these methodologies offers a unique approach to optimizing LLMs, from reducing numerical precision to transferring knowledge between models and structurally simplifying neural networks. Additionally, we discuss emerging trends in system-level design that further enhance the efficiency of LLM inference. This survey aims to provide a comprehensive overview of current advancements in model compression and their potential to make LLMs more accessible and practical for diverse applications.

**Link**: [arxiv](http://arxiv.org/abs/2409.01990v1),  [pdf](http://arxiv.org/pdf/2409.01990v1)

**Tags**: cs.DC cs.LG 



### A Fresh Take on Stale Embeddings: Improving Dense Retriever Training   with Corrector Networks
**Authors**: Nicholas Monath, Will Grathwohl, Michael Boratko, Rob Fergus, Andrew McCallum, Manzil Zaheer

**Updated**: 2024-09-03T13:29:13Z

**Summary**: In dense retrieval, deep encoders provide embeddings for both inputs and targets, and the softmax function is used to parameterize a distribution over a large number of candidate targets (e.g., textual passages for information retrieval). Significant challenges arise in training such encoders in the increasingly prevalent scenario of (1) a large number of targets, (2) a computationally expensive target encoder model, (3) cached target embeddings that are out-of-date due to ongoing training of target encoder parameters. This paper presents a simple and highly scalable response to these challenges by training a small parametric corrector network that adjusts stale cached target embeddings, enabling an accurate softmax approximation and thereby sampling of up-to-date high scoring "hard negatives." We theoretically investigate the generalization properties of our proposed target corrector, relating the complexity of the network, staleness of cached representations, and the amount of training data. We present experimental results on large benchmark dense retrieval datasets as well as on QA with retrieval augmented language models. Our approach matches state-of-the-art results even when no target embedding updates are made during training beyond an initial cache from the unsupervised pre-trained model, providing a 4-80x reduction in re-embedding computational cost.

**Link**: [arxiv](http://arxiv.org/abs/2409.01890v1),  [pdf](http://arxiv.org/pdf/2409.01890v1)

**Tags**: cs.LG 



### Reward Augmentation in Reinforcement Learning for Testing Distributed   Systems
**Authors**: Andrea Borgarelli, Constantin Enea, Rupak Majumdar, Srinidhi Nagendra

**Updated**: 2024-09-02T15:07:05Z

**Summary**: Bugs in popular distributed protocol implementations have been the source of many downtimes in popular internet services. We describe a randomized testing approach for distributed protocol implementations based on reinforcement learning. Since the natural reward structure is very sparse, the key to successful exploration in reinforcement learning is reward augmentation. We show two different techniques that build on one another. First, we provide a decaying exploration bonus based on the discovery of new states -- the reward decays as the same state is visited multiple times. The exploration bonus captures the intuition from coverage-guided fuzzing of prioritizing new coverage points; in contrast to other schemes, we show that taking the maximum of the bonus and the Q-value leads to more effective exploration. Second, we provide waypoints to the algorithm as a sequence of predicates that capture interesting semantic scenarios. Waypoints exploit designer insight about the protocol and guide the exploration to ``interesting'' parts of the state space. Our reward structure ensures that new episodes can reliably get to deep interesting states even without execution caching. We have implemented our algorithm in Go. Our evaluation on three large benchmarks (RedisRaft, Etcd, and RSL) shows that our algorithm can significantly outperform baseline approaches in terms of coverage and bug finding.

**Link**: [arxiv](http://arxiv.org/abs/2409.02137v1),  [pdf](http://arxiv.org/pdf/2409.02137v1)

**Tags**: cs.SE cs.DC cs.LG cs.PL 



### Learning in Hybrid Active Inference Models
**Authors**: Poppy Collis, Ryan Singh, Paul F Kinghorn, Christopher L Buckley

**Updated**: 2024-09-02T08:41:45Z

**Summary**: An open problem in artificial intelligence is how systems can flexibly learn discrete abstractions that are useful for solving inherently continuous problems. Previous work in computational neuroscience has considered this functional integration of discrete and continuous variables during decision-making under the formalism of active inference (Parr, Friston & de Vries, 2017; Parr & Friston, 2018). However, their focus is on the expressive physical implementation of categorical decisions and the hierarchical mixed generative model is assumed to be known. As a consequence, it is unclear how this framework might be extended to learning. We therefore present a novel hierarchical hybrid active inference agent in which a high-level discrete active inference planner sits above a low-level continuous active inference controller. We make use of recent work in recurrent switching linear dynamical systems (rSLDS) which implement end-to-end learning of meaningful discrete representations via the piecewise linear decomposition of complex continuous dynamics (Linderman et al., 2016). The representations learned by the rSLDS inform the structure of the hybrid decision-making agent and allow us to (1) specify temporally-abstracted sub-goals in a method reminiscent of the options framework, (2) lift the exploration into discrete space allowing us to exploit information-theoretic exploration bonuses and (3) `cache' the approximate solutions to low-level problems in the discrete planner. We apply our model to the sparse Continuous Mountain Car task, demonstrating fast system identification via enhanced exploration and successful planning through the delineation of abstract sub-goals.

**Link**: [arxiv](http://arxiv.org/abs/2409.01066v1),  [pdf](http://arxiv.org/pdf/2409.01066v1)

**Tags**: cs.AI cs.SY eess.SY 



### Throughput Optimization in Cache-aided Networks: An Opportunistic   Probing and Scheduling Approach
**Authors**: Zhou Zhang, Saman Atapattu, Yizhu Wang, Marco Di Renzo

**Updated**: 2024-09-02T02:36:22Z

**Summary**: This paper addresses the challenges of throughput optimization in wireless cache-aided cooperative networks. We propose an opportunistic cooperative probing and scheduling strategy for efficient content delivery. The strategy involves the base station probing the relaying channels and cache states of multiple cooperative nodes, thereby enabling opportunistic user scheduling for content delivery. Leveraging the theory of Sequentially Planned Decision (SPD) optimization, we dynamically formulate decisions on cooperative probing and stopping time. Our proposed Reward Expected Thresholds (RET)-based strategy optimizes opportunistic probing and scheduling. This approach significantly enhances system throughput by exploiting gains from local caching, cooperative transmission and time diversity. Simulations confirm the effectiveness and practicality of the proposed Media Access Control (MAC) strategy.

**Link**: [arxiv](http://arxiv.org/abs/2409.00905v1),  [pdf](http://arxiv.org/pdf/2409.00905v1)

**Tags**: eess.SP 



### Rapid GPU-Based Pangenome Graph Layout
**Authors**: Jiajie Li, Jan-Niklas Schmelzle, Yixiao Du, Simon Heumos, Andrea Guarracino, Giulia Guidi, Pjotr Prins, Erik Garrison, Zhiru Zhang

**Updated**: 2024-09-02T00:05:20Z

**Summary**: Computational Pangenomics is an emerging field that studies genetic variation using a graph structure encompassing multiple genomes. Visualizing pangenome graphs is vital for understanding genome diversity. Yet, handling large graphs can be challenging due to the high computational demands of the graph layout process.   In this work, we conduct a thorough performance characterization of a state-of-the-art pangenome graph layout algorithm, revealing significant data-level parallelism, which makes GPUs a promising option for compute acceleration. However, irregular data access and the algorithm's memory-bound nature present significant hurdles. To overcome these challenges, we develop a solution implementing three key optimizations: a cache-friendly data layout, coalesced random states, and warp merging. Additionally, we propose a quantitative metric for scalable evaluation of pangenome layout quality.   Evaluated on 24 human whole-chromosome pangenomes, our GPU-based solution achieves a 57.3x speedup over the state-of-the-art multithreaded CPU baseline without layout quality loss, reducing execution time from hours to minutes.

**Link**: [arxiv](http://arxiv.org/abs/2409.00876v1),  [pdf](http://arxiv.org/pdf/2409.00876v1)

**Tags**: cs.DC cs.CE cs.DS 



### Towards 3D AI Hardware: Fine-Grain Hardware Characterization of 3D   Stacks for Heterogeneous System Integration & AI Systems
**Authors**: Eren Kurshan, Paul Franzon

**Updated**: 2024-08-31T15:45:44Z

**Summary**: 3D integration offers key advantages in improving system performance and efficiency for the End-of-Scaling era. It enables the incorporation of heterogeneous system components and disparate technologies, eliminates off-chip communication constraints, reduces on-chip latency and total power dissipation. Moreover, AIs demand for increased computational power, larger GPU cache capacity, energy efficiency and low power custom AI hardware integration all serve as drivers for 3D integration. Although 3D advantages such as enhanced interconnectivity and increased performance have been demonstrated through numerous technology sites, heterogeneous 3D system design raises numerous unanswered questions. Among the primary challenges are the temperature and lifetime reliability issues caused by the complex interaction patterns among system components. Such interactions are harder to model with current modeling tools and require detailed hardware characterization. This study presents the latest drivers for 3D integration and the resulting need for hardware emulation frameworks. It then presents a design to profile power, temperature, noise, inter-layer bandwidth and lifetime reliability characterization that can emulate a wide range of stacking alternatives. This framework allows for controlling activity levels at the macro-level, along with customized sensor infrastructure to characterize heat propagation, inter-layer noise, power delivery, reliability and inter-connectivity as well as the interactions among critical design objectives.

**Link**: [arxiv](http://arxiv.org/abs/2409.10539v1),  [pdf](http://arxiv.org/pdf/2409.10539v1)

**Tags**: cs.ET 



### Resource Management for IRS-Assisted Full-Duplex Integrated Sensing,   Communication and Computing Systems
**Authors**: Wanming Hao, Xue Wu, Xingwang Li, Gangcan Sun, Qingqing Wu, Liang Yang

**Updated**: 2024-08-31T06:33:50Z

**Summary**: In this paper, we investigate an intelligent reflecting surface (IRS) assisted full-duplex (FD) integrated sensing, communication and computing system. Specifically, an FD base station (BS) provides service for uplink and downlink transmission, and a local cache is connected to the BS through a backhaul link to store data. Meanwhile, active sensing elements are deployed on the IRS to receive target echo signals. On this basis, in order to evaluate the overall performance of the system under consideration, we propose a system utility maximization problem while ensuring the sensing quality, expressed as the difference between the sum of communication throughput, total computation bits (offloading bits and local computation bits) and the total backhaul cost for content delivery. This makes the problem difficult to solve due to the highly non-convex coupling of the optimization variables. To effectively solve this problem, we first design the most effective caching strategy. Then, we develop an algorithm based on weighted minimum mean square error, alternative direction method of multipliers, majorization-minimization framework, semi-definite relaxation techniques, and several complex transformations to jointly solve the optimization variables. Finally, simulation results are provided to verify the utility performance of the proposed algorithm and demonstrate the advantages of the proposed scheme compared with the baseline scheme.

**Link**: [arxiv](http://arxiv.org/abs/2409.00364v1),  [pdf](http://arxiv.org/pdf/2409.00364v1)

**Tags**: cs.IT eess.SP math.IT 



### >3kV NiO/Ga2O3 Heterojunction Diodes with Space-Modulated Junction   Termination Extension and Sub-1V Turn-on
**Authors**: Advait Gilankar, Abishek Katta, Nabasindhu Das, Nidhin Kurian Kalarickal

**Updated**: 2024-08-31T04:20:58Z

**Summary**: This work demonstrates high-performance vertical NiO/Ga2O3 heterojunction diodes (HJDs) with a 2-step space-modulated junction termination extension. Distinct from the current state-of-the-art Ga2O3 HJDs, we achieve breakdown voltage exceeding 3 kV with a low turn on voltage (VON) of 0.8V, estimated at a forward current density (IF) of 1 A-cm-2. The measured devices exhibit excellent turn-on characteristics achieving 100 A-cm-2 current density at a forward bias of 1.5V along with a low differential specific on-resistance (Ron,sp) of 4.4 m{\Omega}-cm2. The SM-JTE was realized using concentric NiO rings with varying widths and spacing that approximates a gradual reduction in JTE charge. The unipolar figure of merit (FOM) calculated exceeds 2 GW-cm2 and is among the best reported for devices with a sub-1V turn-on. The fabricated devices also displayed minimal change in forward I-V characteristics post reverse bias stress of 3 kV applied during breakdown voltage testing.

**Link**: [arxiv](http://arxiv.org/abs/2409.00344v1),  [pdf](http://arxiv.org/pdf/2409.00344v1)

**Tags**: physics.app-ph 



### Adaptive Multi-Resolution Encoding for Interactive Large-Scale Volume   Visualization through Functional Approximation
**Authors**: Jianxin Sun, David Lenz, Hongfeng Yu, Tom Peterka

**Updated**: 2024-08-30T18:04:53Z

**Summary**: Functional approximation as a high-order continuous representation provides a more accurate value and gradient query compared to the traditional discrete volume representation. Volume visualization directly rendered from functional approximation generates high-quality rendering results without high-order artifacts caused by trilinear interpolations. However, querying an encoded functional approximation is computationally expensive, especially when the input dataset is large, making functional approximation impractical for interactive visualization. In this paper, we proposed a novel functional approximation multi-resolution representation, Adaptive-FAM, which is lightweight and fast to query. We also design a GPU-accelerated out-of-core multi-resolution volume visualization framework that directly utilizes the Adaptive-FAM representation to generate high-quality rendering with interactive responsiveness. Our method can not only dramatically decrease the caching time, one of the main contributors to input latency, but also effectively improve the cache hit rate through prefetching. Our approach significantly outperforms the traditional function approximation method in terms of input latency while maintaining comparable rendering quality.

**Link**: [arxiv](http://arxiv.org/abs/2409.00184v1),  [pdf](http://arxiv.org/pdf/2409.00184v1)

**Tags**: cs.GR 



### Modelling the High-Voltage Grid Using Open Data for Europe and Beyond
**Authors**: Bobby Xiong, Davide Fioriti, Fabian Neumann, Iegor Riepin, Tom Brown

**Updated**: 2024-08-30T10:26:50Z

**Summary**: This paper provides the background, methodology and validation for constructing a representation of the European high-voltage grid, including and above 200 kV, based on public data provided by OpenStreetMap. The model-independent grid dataset is published under the Open Data Commons Open Database (ODbL 1.0) licence and can be used for large-scale electricity as well as energy system modelling. The dataset and workflow are provided as part of PyPSA-Eur -- an open-source, sector-coupled optimisation model of the European energy system. By integrating with the codebase for initiatives such as PyPSA-Earth, the value of open and maintainable high-voltage grid data extends to the global context. By accessing the latest data through the the Overpass turbo API, the dataset can be easily reconstructed and updated within minutes. To assess the data quality, this paper further compares the dataset with official statistics and representative model runs using PyPSA-Eur based on different electricity grid representations.

**Link**: [arxiv](http://arxiv.org/abs/2408.17178v1),  [pdf](http://arxiv.org/pdf/2408.17178v1)

**Tags**: physics.soc-ph 



### MemLong: Memory-Augmented Retrieval for Long Text Modeling
**Authors**: Weijie Liu, Zecheng Tang, Juntao Li, Kehai Chen, Min Zhang

**Updated**: 2024-08-30T02:01:56Z

**Summary**: Recent advancements in Large Language Models (LLMs) have yielded remarkable success across diverse fields. However, handling long contexts remains a significant challenge for LLMs due to the quadratic time and space complexity of attention mechanisms and the growing memory consumption of the key-value cache during generation. This work introduces MemLong: Memory-Augmented Retrieval for Long Text Generation, a method designed to enhance the capabilities of long-context language modeling by utilizing an external retriever for historical information retrieval. MemLong combines a non-differentiable ``ret-mem'' module with a partially trainable decoder-only language model and introduces a fine-grained, controllable retrieval attention mechanism that leverages semantic-level relevant chunks. Comprehensive evaluations on multiple long-context language modeling benchmarks demonstrate that MemLong consistently outperforms other state-of-the-art LLMs. More importantly, MemLong can extend the context length on a single 3090 GPU from 4k up to 80k. Our code is available at https://github.com/Bui1dMySea/MemLong

**Link**: [arxiv](http://arxiv.org/abs/2408.16967v1),  [pdf](http://arxiv.org/pdf/2408.16967v1)

**Tags**: cs.CL cs.AI 



### Smart Helper-Aided F-RANs: Improving Delay and Reducing Fronthaul Load
**Authors**: Hesameddin Mokhtarzadeh, Mohammed S. Al-Abiad, Md Jahangir Hossain, Julian Cheng

**Updated**: 2024-08-29T17:43:26Z

**Summary**: In traditional Fog-Radio Access Networks (F-RANs), enhanced remote radio heads (eRRHs) are connected to a macro base station (MBS) through fronthaul links. Deploying a massive number of eRRHs is not always feasible due to site constraints and the cost of fronthaul links. This paper introduces an innovative concept of using smart helpers (SHs) in F-RANs. These SHs do not require fronthaul links and listen to the nearby eRRHs' communications. Then, they smartly select and cache popular content. This capability enables SHs to serve users with frequent on-demand service requests potentially. As such, network operators have the flexibility to easily deploy SHs in various scenarios, such as dense urban areas and temporary public events, to expand their F-RANs and improve the quality of service (QoS). To study the performance of the proposed SH-aided F-RAN, we formulate an optimization problem of minimizing the average transmission delay that jointly optimizes cache resources and user scheduling. To tackle the formulated problem, we develop an innovative multi-stage algorithm that uses a reinforcement learning (RL) framework. Various performance measures, e.g., the average transmission delay, fronthaul load, and cache hit rate of the proposed SH-aided F-RAN are evaluated numerically and compared with those of traditional F-RANs.

**Link**: [arxiv](http://arxiv.org/abs/2309.07975v2),  [pdf](http://arxiv.org/pdf/2309.07975v2)

**Tags**: cs.IT math.IT 



### VideoLLM-MoD: Efficient Video-Language Streaming with Mixture-of-Depths   Vision Computation
**Authors**: Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, Mike Zheng Shou

**Updated**: 2024-08-29T17:21:58Z

**Summary**: A well-known dilemma in large vision-language models (e.g., GPT-4, LLaVA) is that while increasing the number of vision tokens generally enhances visual understanding, it also significantly raises memory and computational costs, especially in long-term, dense video frame streaming scenarios. Although learnable approaches like Q-Former and Perceiver Resampler have been developed to reduce the vision token burden, they overlook the context causally modeled by LLMs (i.e., key-value cache), potentially leading to missed visual cues when addressing user queries. In this paper, we introduce a novel approach to reduce vision compute by leveraging redundant vision tokens "skipping layers" rather than decreasing the number of vision tokens. Our method, VideoLLM-MoD, is inspired by mixture-of-depths LLMs and addresses the challenge of numerous vision tokens in long-term or streaming video. Specifically, for each transformer layer, we learn to skip the computation for a high proportion (e.g., 80\%) of vision tokens, passing them directly to the next layer. This approach significantly enhances model efficiency, achieving approximately \textasciitilde42\% time and \textasciitilde30\% memory savings for the entire training. Moreover, our method reduces the computation in the context and avoid decreasing the vision tokens, thus preserving or even improving performance compared to the vanilla model. We conduct extensive experiments to demonstrate the effectiveness of VideoLLM-MoD, showing its state-of-the-art results on multiple benchmarks, including narration, forecasting, and summarization tasks in COIN, Ego4D, and Ego-Exo4D datasets.

**Link**: [arxiv](http://arxiv.org/abs/2408.16730v1),  [pdf](http://arxiv.org/pdf/2408.16730v1)

**Tags**: cs.CV 



### GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless   Generative Inference of LLM
**Authors**: Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao

**Updated**: 2024-08-29T16:48:58Z

**Summary**: Key-value (KV) caching has become the de-facto to accelerate generation speed for large language models (LLMs) inference. However, the growing cache demand with increasing sequence length has transformed LLM inference to be a memory bound problem, significantly constraining the system throughput. Existing methods rely on dropping unimportant tokens or quantizing all entries uniformly. Such methods, however, often incur high approximation errors to represent the compressed matrices. The autoregressive decoding process further compounds the error of each step, resulting in critical deviation in model generation and deterioration of performance. To tackle this challenge, we propose GEAR, an efficient KV cache compression framework that achieves near-lossless high-ratio compression. GEAR first applies quantization to majority of entries of similar magnitudes to ultra-low precision. It then employs a low rank matrix to approximate the quantization error, and a sparse matrix to remedy individual errors from outlier entries. By adeptly integrating three techniques, GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression with up to 2.38x throughput improvement, while reducing peak-memory size up to 2.29x. Our code is publicly available at https://github.com/HaoKang-Timmy/GEAR.

**Link**: [arxiv](http://arxiv.org/abs/2403.05527v3),  [pdf](http://arxiv.org/pdf/2403.05527v3)

**Tags**: cs.LG cs.AI cs.CL 



### LightSLH: Provable and Low-Overhead Spectre v1 Mitigation through   Targeted Instruction Hardening
**Authors**: Yiming Zhu, Wenchao Huang, Yan Xiong

**Updated**: 2024-08-29T02:31:28Z

**Summary**: Several software mitigations have been proposed to defend against Spectre vulnerabilities. However, these countermeasures often suffer from high performance overhead, largely due to unnecessary protections. We propose LightSLH, designed to mitigate this overhead by hardening instructions only when they are under threat from Spectre vulnerabilities. LightSLH leverages program analysis techniques based on abstract interpretation to identify all instructions that could potentially lead to Spectre vulnerabilities and provides provable protection. To enhance analysis efficiency and precision, LightSLH employs novel taint and value domains. The taint domain enables bit-level taint tracking, while the value domain allows LightSLH to analyze complex program structures such as pointers and structures. Furthermore, LightSLH uses a two-stage abstract interpretation approach to circumvent potential analysis paralysis issues.   We demonstrate the security guarantees of LightSLH and evaluate its performance on cryptographic algorithm implementations from OpenSSL. LightSLH significantly reduces the overhead associated with speculative-load-hardening techniques. Our results show that LightSLH introduces no protection and thus no overhead on 4 out of the 7 studied algorithms, which contrasts with existing countermeasures that introduce additional overhead due to unnecessary hardening. Additionally, LightSLH performs, for the first time, a rigorous analysis of the security guarantees of RSA against Spectre v1, highlighting that the memory access patterns generated by the scatter-gather algorithm depend on secrets, even for observers at the cache line granularity, necessitating protection for such accesses.

**Link**: [arxiv](http://arxiv.org/abs/2408.16220v1),  [pdf](http://arxiv.org/pdf/2408.16220v1)

**Tags**: cs.CR 



### On the Impact of ISA Extension on Energy Consumption of I-Cache in   Extensible Processors
**Authors**: Noushin Behboudi, Mehdi Kamal, Ali Afzali-Kusha

**Updated**: 2024-08-28T17:28:12Z

**Summary**: As is widely known, the computational speed and power consumption are two critical parameters in microprocessor design. A solution for these issues is the application specific instruction set processor (ASIP) methodology, which can improve speed and reduce power consumption of the general purpose processor (GPP) technique. In ASIP, changing the instruction set architecture (ISA) of the processor will lead to alter the number and the mean time of accesses to the cache memory. This issue has a direct impact on the processor energy consumption. In this work, we study the impacts of extended ISA on the energy consumption of the extended ISA processor. Also, we demonstrate the extended ISA let the designer to reduce the cache size in order to minimize the energy consumption while meeting performance constraint.

**Link**: [arxiv](http://arxiv.org/abs/2409.08286v1),  [pdf](http://arxiv.org/pdf/2409.08286v1)

**Tags**: cs.AR 



### RIP Linked List
**Authors**: Beno√Æt Sonntag, Dominique Colnet

**Updated**: 2024-08-28T08:41:45Z

**Summary**: Linked lists have long served as a valuable teaching tool in programming. However, the question arises: Are they truly practical for everyday program use? In most cases, it appears that array-based data structures offer distinct advantages, particularly in terms of memory efficiency and,more importantly, execution speed. While it's relatively straightforward to calculate the complexity of operations, gauging actual execution efficiency remains a challenge. This paper addresses this question by introducing a new benchmark. Our study compares various linked list implementations with several array-based alternatives. We also demonstrate the ease of incorporating memory caching for linked lists, enhancing their performance. Additionally, we introduce a new array-based data structure designed to excel in a wide range of operations.

**Link**: [arxiv](http://arxiv.org/abs/2306.06942v3),  [pdf](http://arxiv.org/pdf/2306.06942v3)

**Tags**: cs.DS 



### Efficient LLM Training and Serving with Heterogeneous Context Sharding   among Attention Heads
**Authors**: Xihui Lin, Yunan Zhang, Suyu Ge, Barun Patra, Vishrav Chaudhary, Hao Peng, Xia Song

**Updated**: 2024-08-27T22:06:20Z

**Summary**: Existing LLM training and inference frameworks struggle in boosting efficiency with sparsity while maintaining the integrity of context and model architecture. Inspired by the sharding concept in database and the fact that attention parallelizes over heads on accelerators, we propose Sparsely-Sharded (S2) Attention, an attention algorithm that allocates heterogeneous context partitions for different attention heads to divide and conquer. S2-Attention enforces each attention head to only attend to a partition of contexts following a strided sparsity pattern, while the full context is preserved as the union of all the shards. As attention heads are processed in separate thread blocks, the context reduction for each head can thus produce end-to-end speed-up and memory reduction. At inference, LLMs trained with S2-Attention can then take the KV cache reduction as free meals with guaranteed model quality preserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X wall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction in end-to-end training time and 10X inference latency, (2) on-par model training quality compared to default attention, (3)perfect needle retrieval accuracy over 32K context window. On top of the algorithm, we build DKernel, an LLM training and inference kernel library that allows users to customize sparsity patterns for their own models. We open-sourced DKerneland make it compatible with Megatron, Pytorch, and vLLM.

**Link**: [arxiv](http://arxiv.org/abs/2407.17678v2),  [pdf](http://arxiv.org/pdf/2407.17678v2)

**Tags**: cs.CL 



### Styx: Transactional Stateful Functions on Streaming Dataflows
**Authors**: Kyriakos Psarakis, George Siachamis, George Christodoulou, Marios Fragkoulis, Asterios Katsifodimos

**Updated**: 2024-08-27T17:30:41Z

**Summary**: Developing stateful cloud applications, such as low-latency workflows and microservices with strict consistency requirements, remains arduous for programmers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve these use cases. However, existing approaches either provide serializable transactional guarantees at the level of individual functions, or separate application logic from the state and use inefficient transactional protocols. These design choices increase the execution latency, limiting the adoption of SFaaS systems.   In this paper, we present Styx, a novel SFaaS runtime that executes serializable transactions across functions with exactly-once guarantees. Styx extends a deterministic transactional protocol to support an arbitrary call graph of stateful functions. It introduces a transaction-execution acknowledgment scheme that allows tracking a transactional workflow's SFaaS calls, guaranteeing atomicity and exactly-once processing. Finally, Styx features a function-execution caching mechanism and early transactional commit replies for optimized performance. Experiments with the YCSB-T, TPC-C, and Deathstar benchmarks show that Styx outperforms state-of-the-art approaches by achieving at least one order of magnitude higher throughput while exhibiting near-linear scalability and low latency.

**Link**: [arxiv](http://arxiv.org/abs/2312.06893v3),  [pdf](http://arxiv.org/pdf/2312.06893v3)

**Tags**: cs.DC cs.DB 



### Writing in the Margins: Better Inference Pattern for Long Context   Retrieval
**Authors**: Melisa Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, Waseem AlShikh

**Updated**: 2024-08-27T09:34:38Z

**Summary**: In this paper, we introduce Writing in the Margins (WiM), a new inference pattern for Large Language Models designed to optimize the handling of long input sequences in retrieval-oriented tasks. This approach leverages the chunked prefill of the key-value cache to perform segment-wise inference, which enables efficient processing of extensive contexts along with the generation and classification of intermediate information ("margins") that guide the model towards specific tasks. This method increases computational overhead marginally while significantly enhancing the performance of off-the-shelf models without the need for fine-tuning. Specifically, we observe that WiM provides an average enhancement of 7.5% in accuracy for reasoning skills (HotpotQA, MultiHop-RAG) and more than a 30.0% increase in the F1-score for aggregation tasks (CWE). Additionally, we show how the proposed pattern fits into an interactive retrieval design that provides end-users with ongoing updates about the progress of context processing, and pinpoints the integration of relevant information into the final response. We release our implementation of WiM using Hugging Face Transformers library at https://github.com/writer/writing-in-the-margins.

**Link**: [arxiv](http://arxiv.org/abs/2408.14906v1),  [pdf](http://arxiv.org/pdf/2408.14906v1)

**Tags**: cs.CL cs.IR 



### PPVF: An Efficient Privacy-Preserving Online Video Fetching Framework   with Correlated Differential Privacy
**Authors**: Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Miao Hu, Linchang Xiao

**Updated**: 2024-08-27T02:03:36Z

**Summary**: Online video streaming has evolved into an integral component of the contemporary Internet landscape. Yet, the disclosure of user requests presents formidable privacy challenges. As users stream their preferred online videos, their requests are automatically seized by video content providers, potentially leaking users' privacy.   Unfortunately, current protection methods are not well-suited to preserving user request privacy from content providers while maintaining high-quality online video services. To tackle this challenge, we introduce a novel Privacy-Preserving Video Fetching (PPVF) framework, which utilizes trusted edge devices to pre-fetch and cache videos, ensuring the privacy of users' requests while optimizing the efficiency of edge caching. More specifically, we design PPVF with three core components: (1) \textit{Online privacy budget scheduler}, which employs a theoretically guaranteed online algorithm to select non-requested videos as candidates with assigned privacy budgets. Alternative videos are chosen by an online algorithm that is theoretically guaranteed to consider both video utilities and available privacy budgets. (2) \textit{Noisy video request generator}, which generates redundant video requests (in addition to original ones) utilizing correlated differential privacy to obfuscate request privacy. (3) \textit{Online video utility predictor}, which leverages federated learning to collaboratively evaluate video utility in an online fashion, aiding in video selection in (1) and noise generation in (2). Finally, we conduct extensive experiments using real-world video request traces from Tencent Video. The results demonstrate that PPVF effectively safeguards user request privacy while upholding high video caching performance.

**Link**: [arxiv](http://arxiv.org/abs/2408.14735v1),  [pdf](http://arxiv.org/pdf/2408.14735v1)

**Tags**: cs.MM cs.CR cs.DC 



### Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference
**Authors**: Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han

**Updated**: 2024-08-26T21:01:02Z

**Summary**: As the demand for long-context large language models (LLMs) increases, models with context windows of up to 128K or 1M tokens are becoming increasingly prevalent. However, long-context LLM inference is challenging since the inference speed decreases significantly as the sequence length grows. This slowdown is primarily caused by loading a large KV cache during self-attention. Previous works have shown that a small portion of critical tokens will dominate the attention outcomes. However, we observe the criticality of a token highly depends on the query. To this end, we propose Quest, a query-aware KV cache selection algorithm. Quest keeps track of the minimal and maximal Key values in KV cache pages and estimates the criticality of a given page using Query vectors. By only loading the Top-K critical KV cache pages for attention, Quest significantly speeds up self-attention without sacrificing accuracy. We show that Quest can achieve up to 2.23x self-attention speedup, which reduces inference latency by 7.03x while performing well on tasks with long dependencies with negligible accuracy loss. Code is available at http://github.com/mit-han-lab/Quest .

**Link**: [arxiv](http://arxiv.org/abs/2406.10774v2),  [pdf](http://arxiv.org/pdf/2406.10774v2)

**Tags**: cs.CL cs.LG 



### Employing Artificial Intelligence to Steer Exascale Workflows with   Colmena
**Authors**: Logan Ward, J. Gregory Pauloski, Valerie Hayot-Sasson, Yadu Babuji, Alexander Brace, Ryan Chard, Kyle Chard, Rajeev Thakur, Ian Foster

**Updated**: 2024-08-26T17:21:19Z

**Summary**: Computational workflows are a common class of application on supercomputers, yet the loosely coupled and heterogeneous nature of workflows often fails to take full advantage of their capabilities. We created Colmena to leverage the massive parallelism of a supercomputer by using Artificial Intelligence (AI) to learn from and adapt a workflow as it executes. Colmena allows scientists to define how their application should respond to events (e.g., task completion) as a series of cooperative agents. In this paper, we describe the design of Colmena, the challenges we overcame while deploying applications on exascale systems, and the science workflows we have enhanced through interweaving AI. The scaling challenges we discuss include developing steering strategies that maximize node utilization, introducing data fabrics that reduce communication overhead of data-intensive tasks, and implementing workflow tasks that cache costly operations between invocations. These innovations coupled with a variety of application patterns accessible through our agent-based steering model have enabled science advances in chemistry, biophysics, and materials science using different types of AI. Our vision is that Colmena will spur creative solutions that harness AI across many domains of scientific computing.

**Link**: [arxiv](http://arxiv.org/abs/2408.14434v1),  [pdf](http://arxiv.org/pdf/2408.14434v1)

**Tags**: cs.DC cs.LG 



### Decision-Focused Learning to Predict Action Costs for Planning
**Authors**: Jayanta Mandi, Marco Foschini, Daniel Holler, Sylvie Thiebaux, Jorg Hoffmann, Tias Guns

**Updated**: 2024-08-26T11:29:07Z

**Summary**: In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.

**Link**: [arxiv](http://arxiv.org/abs/2408.06876v2),  [pdf](http://arxiv.org/pdf/2408.06876v2)

**Tags**: cs.AI cs.RO 



### Trimma: Trimming Metadata Storage and Latency for Hybrid Memory Systems
**Authors**: Yiwei Li, Boyu Tian, Mingyu Gao

**Updated**: 2024-08-26T07:26:27Z

**Summary**: Hybrid main memory systems combine both performance and capacity advantages from heterogeneous memory technologies. With larger capacities, higher associativities, and finer granularities, hybrid memory systems currently exhibit significant metadata storage and lookup overheads for flexibly remapping data blocks between the two memory tiers. To alleviate the inefficiencies of existing designs, we propose Trimma, the combination of a multi-level metadata structure and an efficient metadata cache design. Trimma uses a multi-level metadata table to only track truly necessary address remap entries. The saved memory space is effectively utilized as extra DRAM cache capacity to improve performance. Trimma also uses separate formats to store the entries with non-identity and identity address mappings. This improves the overall remap cache hit rate, further boosting the performance. Trimma is transparent to software and compatible with various types of hybrid memory systems. When evaluated on a representative hybrid memory system with HBM3 and DDR5, Trimma achieves up to 1.68$\times$ and on average 1.33$\times$ speedup benefits, compared to state-of-the-art hybrid memory designs. These results show that Trimma effectively addresses metadata management overheads, especially for future scalable large-scale hybrid memory architectures.

**Link**: [arxiv](http://arxiv.org/abs/2402.16343v2),  [pdf](http://arxiv.org/pdf/2402.16343v2)

**Tags**: cs.AR 



### RollingCache: Using Runtime Behavior to Defend Against Cache Side   Channel Attacks
**Authors**: Divya Ojha, Sandhya Dwarkadas

**Updated**: 2024-08-26T04:32:56Z

**Summary**: Shared caches are vulnerable to side channel attacks through contention in cache sets. Besides being a simple source of information leak, these side channels form useful gadgets for more sophisticated attacks that compromise the security of shared systems.   The fundamental design aspect that contention attacks exploit is the deterministic nature of the set of addresses contending for a cache set. In this paper, we present RollingCache, a cache design that defends against contention attacks by dynamically changing the set of addresses contending for cache sets. Unlike prior defenses, RollingCache does not rely on address encryption/decryption, data relocation, or cache partitioning. We use one level of indirection to implement dynamic mapping controlled by the whole-cache runtime behavior. Our solution does not depend on having defined security domains, and can defend against an attacker running on the same or another core.   We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our security evaluation shows that our dynamic mapping removes the deterministic ability to identify the source of contention. The performance evaluation shows an impact of 1.67\% over a mix of workloads, with a corresponding

**Link**: [arxiv](http://arxiv.org/abs/2408.08795v2),  [pdf](http://arxiv.org/pdf/2408.08795v2)

**Tags**: cs.CR cs.AR 



### Decentralized Federated Learning with Model Caching on Mobile Agents
**Authors**: Xiaoyu Wang, Guojun Xiong, Houwei Cao, Jian Li, Yong Liu

**Updated**: 2024-08-26T03:58:20Z

**Summary**: Federated Learning (FL) aims to train a shared model using data and computation power on distributed agents coordinated by a central server. Decentralized FL (DFL) utilizes local model exchange and aggregation between agents to reduce the communication and computation overheads on the central server. However, when agents are mobile, the communication opportunity between agents can be sporadic, largely hindering the convergence and accuracy of DFL. In this paper, we study delay-tolerant model spreading and aggregation enabled by model caching on mobile agents. Each agent stores not only its own model, but also models of agents encountered in the recent past. When two agents meet, they exchange their own models as well as the cached models. Local model aggregation works on all models in the cache. We theoretically analyze the convergence of DFL with cached models, explicitly taking into account the model staleness introduced by caching. We design and compare different model caching algorithms for different DFL and mobility scenarios. We conduct detailed case studies in a vehicular network to systematically investigate the interplay between agent mobility, cache staleness, and model convergence. In our experiments, cached DFL converges quickly, and significantly outperforms DFL without caching.

**Link**: [arxiv](http://arxiv.org/abs/2408.14001v1),  [pdf](http://arxiv.org/pdf/2408.14001v1)

**Tags**: cs.LG cs.DC 



### Mobile Edge Computing Networks: Online Low-Latency and Fresh Service   Provisioning
**Authors**: Yuhan Yi, Guanglin Zhang, Hai Jiang

**Updated**: 2024-08-24T15:23:32Z

**Summary**: Edge service caching can significantly mitigate latency and reduce communication and computing overhead by fetching and initializing services (applications) from clouds. The freshness of cached service data is critical when providing satisfactory services to users, but has been overlooked in existing research efforts. In this paper, we study the online low-latency and fresh service provisioning in mobile edge computing (MEC) networks. Specifically, we jointly optimize the service caching, task offloading, and resource allocation without knowledge of future system information, which is formulated as a joint online long-term optimization problem. This problem is NP-hard. To solve the problem, we design a Lyapunov-based online framework that decouples the problem at temporal level into a series of per-time-slot subproblems. For each subproblem, we propose an online integrated optimization-deep reinforcement learning (OIODRL) method, which contains an optimization stage including a quadratically constrained quadratic program (QCQP) transformation and a semidefinite relaxation (SDR) method, and a learning stage including a deep reinforcement learning (DRL) algorithm. Extensive simulations show that the proposed OIODRL method achieves a near-optimal solution and outperforms other benchmark methods.

**Link**: [arxiv](http://arxiv.org/abs/2408.13605v1),  [pdf](http://arxiv.org/pdf/2408.13605v1)

**Tags**: cs.IT math.IT 



### MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context   Generation with Speculative Decoding
**Authors**: Jian Chen, Vashisth Tiwari, Ranajoy Sadhukhan, Zhuoming Chen, Jinyuan Shi, Ian En-Hsu Yen, Beidi Chen

**Updated**: 2024-08-23T17:54:34Z

**Summary**: Large Language Models (LLMs) have become more prevalent in long-context applications such as interactive chatbots, document analysis, and agent workflows, but it is challenging to serve long-context requests with low latency and high throughput. Speculative decoding (SD) is a widely used technique to reduce latency without sacrificing performance but the conventional wisdom suggests that its efficacy is limited to small batch sizes. In MagicDec, we show that surprisingly SD can achieve speedup even for a high throughput inference regime for moderate to long sequences. More interestingly, an intelligent drafting strategy can achieve better speedup with increasing batch size based on our rigorous analysis. MagicDec first identifies the bottleneck shifts with increasing batch size and sequence length, and uses these insights to deploy speculative decoding more effectively for high throughput inference. Then, it leverages draft models with sparse KV cache to address the KV bottleneck that scales with both sequence length and batch size. This finding underscores the broad applicability of speculative decoding in long-context serving, as it can enhance throughput and reduce latency without compromising accuracy. For moderate to long sequences, we demonstrate up to 2x speedup for LLaMA-2-7B-32K and 1.84x speedup for LLaMA-3.1-8B when serving batch sizes ranging from 32 to 256 on 8 NVIDIA A100 GPUs. The code is available at https://github.com/Infini-AI-Lab/MagicDec/.

**Link**: [arxiv](http://arxiv.org/abs/2408.11049v3),  [pdf](http://arxiv.org/pdf/2408.11049v3)

**Tags**: cs.CL 



## Keyword: LLM Inference 
 ### Order of Magnitude Speedups for LLM Membership Inference
**Authors**: Rongting Zhang, Martin Bertran, Aaron Roth

**Updated**: 2024-09-24T17:48:58Z

**Summary**: Large Language Models (LLMs) have the promise to revolutionize computing broadly, but their complexity and extensive training data also expose significant privacy vulnerabilities. One of the simplest privacy risks associated with LLMs is their susceptibility to membership inference attacks (MIAs), wherein an adversary aims to determine whether a specific data point was part of the model's training set. Although this is a known risk, state of the art methodologies for MIAs rely on training multiple computationally costly shadow models, making risk evaluation prohibitive for large models. Here we adapt a recent line of work which uses quantile regression to mount membership inference attacks; we extend this work by proposing a low-cost MIA that leverages an ensemble of small quantile regression models to determine if a document belongs to the model's training set or not. We demonstrate the effectiveness of this approach on fine-tuned LLMs of varying families (OPT, Pythia, Llama) and across multiple datasets. Across all scenarios we obtain comparable or improved accuracy compared to state of the art shadow model approaches, with as little as 6% of their computation budget. We demonstrate increased effectiveness across multi-epoch trained target models, and architecture miss-specification robustness, that is, we can mount an effective attack against a model using a different tokenizer and architecture, without requiring knowledge on the target model.

**Link**: [arxiv](http://arxiv.org/abs/2409.14513v2),  [pdf](http://arxiv.org/pdf/2409.14513v2)

**Tags**: cs.LG cs.CR stat.ML 



### Generative Factor Chaining: Coordinated Manipulation with   Diffusion-based Factor Graph
**Authors**: Utkarsh A. Mishra, Yongxin Chen, Danfei Xu

**Updated**: 2024-09-24T17:47:34Z

**Summary**: Learning to plan for multi-step, multi-manipulator tasks is notoriously difficult because of the large search space and the complex constraint satisfaction problems. We present Generative Factor Chaining~(GFC), a composable generative model for planning. GFC represents a planning problem as a spatial-temporal factor graph, where nodes represent objects and robots in the scene, spatial factors capture the distributions of valid relationships among nodes, and temporal factors represent the distributions of skill transitions. Each factor is implemented as a modular diffusion model, which are composed during inference to generate feasible long-horizon plans through bi-directional message passing. We show that GFC can solve complex bimanual manipulation tasks and exhibits strong generalization to unseen planning tasks with novel combinations of objects and constraints. More details can be found at: https://generative-fc.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2409.16275v1),  [pdf](http://arxiv.org/pdf/2409.16275v1)

**Tags**: cs.RO 



### AIM 2024 Challenge on UHD Blind Photo Quality Assessment
**Authors**: Vlad Hosu, Marcos V. Conde, Lorenzo Agnolucci, Nabajeet Barman, Saman Zadtootaghaj, Radu Timofte

**Updated**: 2024-09-24T17:44:24Z

**Summary**: We introduce the AIM 2024 UHD-IQA Challenge, a competition to advance the No-Reference Image Quality Assessment (NR-IQA) task for modern, high-resolution photos. The challenge is based on the recently released UHD-IQA Benchmark Database, which comprises 6,073 UHD-1 (4K) images annotated with perceptual quality ratings from expert raters. Unlike previous NR-IQA datasets, UHD-IQA focuses on highly aesthetic photos of superior technical quality, reflecting the ever-increasing standards of digital photography. This challenge aims to develop efficient and effective NR-IQA models. Participants are tasked with creating novel architectures and training strategies to achieve high predictive performance on UHD-1 images within a computational budget of 50G MACs. This enables model deployment on edge devices and scalable processing of extensive image collections. Winners are determined based on a combination of performance metrics, including correlation measures (SRCC, PLCC, KRCC), absolute error metrics (MAE, RMSE), and computational efficiency (G MACs). To excel in this challenge, participants leverage techniques like knowledge distillation, low-precision inference, and multi-scale training. By pushing the boundaries of NR-IQA for high-resolution photos, the UHD-IQA Challenge aims to stimulate the development of practical models that can keep pace with the rapidly evolving landscape of digital photography. The innovative solutions emerging from this competition will have implications for various applications, from photo curation and enhancement to image compression.

**Link**: [arxiv](http://arxiv.org/abs/2409.16271v1),  [pdf](http://arxiv.org/pdf/2409.16271v1)

**Tags**: cs.CV 



### REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial   Task Allocation in Multi-Human Multi-Robot Teams
**Authors**: Arjun Gupte, Ruiqi Wang, Vishnunandan L. N. Venkatesh, Taehyeon Kim, Dezhong Zhao, Byung-Cheol Min

**Updated**: 2024-09-24T17:37:54Z

**Summary**: Multi-human multi-robot teams combine the complementary strengths of humans and robots to tackle complex tasks across diverse applications. However, the inherent heterogeneity of these teams presents significant challenges in initial task allocation (ITA), which involves assigning the most suitable tasks to each team member based on their individual capabilities before task execution. While current learning-based methods have shown promising results, they are often computationally expensive to train, and lack the flexibility to incorporate user preferences in multi-objective optimization and adapt to last-minute changes in real-world dynamic environments. To address these issues, we propose REBEL, an LLM-based ITA framework that integrates rule-based and experience-enhanced learning. By leveraging Retrieval-Augmented Generation, REBEL dynamically retrieves relevant rules and past experiences, enhancing reasoning efficiency. Additionally, REBEL can complement pre-trained RL-based ITA policies, improving situational awareness and overall team performance. Extensive experiments validate the effectiveness of our approach across various settings. More details are available at https://sites.google.com/view/ita-rebel .

**Link**: [arxiv](http://arxiv.org/abs/2409.16266v1),  [pdf](http://arxiv.org/pdf/2409.16266v1)

**Tags**: cs.RO 



### Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4   Capabilities
**Authors**: Thomas Ball, Shuo Chen, Cormac Herley

**Updated**: 2024-09-24T17:34:07Z

**Summary**: In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4 performance on several deterministic tasks; each task involves a basic calculation and takes as input parameter some element drawn from a large well-defined population (e.g., count elements in a list, multiply two k-digit numbers, etc). We examine several conditions per-task and perform enough trials so that statistically significant differences can be detected. This allows us to investigate the sensitivity of task-accuracy both to query phrasing and input parameter population. We find that seemingly trivial modifications in the task-prompt or input population can yield differences far larger than can be explained by sampling effects. For example, performance on a simple list-counting task varies with query-phrasing and list-length, but also with list composition (i.e., the thing-to-be-counted) and object frequency (e.g., success when an element accounts for $\approx$ 50\% of a list is different from when it accounts for $\approx$ 70\% etc).   We conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond what the data supports. A consequence appears to be that intuitions that have been formed based on interactions with humans form a very unreliable guide as to which input modifications should ``make no difference'' to LLM performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.07638v2),  [pdf](http://arxiv.org/pdf/2409.07638v2)

**Tags**: cs.AI cs.CL cs.LG 



### Learning To Help: Training Models to Assist Legacy Devices
**Authors**: Yu Wu, Anand Sarwate

**Updated**: 2024-09-24T17:21:25Z

**Summary**: Machine learning models implemented in hardware on physical devices may be deployed for a long time. The computational abilities of the device may be limited and become outdated with respect to newer improvements. Because of the size of ML models, offloading some computation (e.g. to an edge cloud) can help such legacy devices. We cast this problem in the framework of learning with abstention (LWA) in which the expert (edge) must be trained to assist the client (device). Prior work on LWA trains the client assuming the edge is either an oracle or a human expert. In this work, we formalize the reverse problem of training the expert for a fixed (legacy) client. As in LWA, the client uses a rejection rule to decide when to offload inference to the expert (at a cost). We find the Bayes-optimal rule, prove a generalization bound, and find a consistent surrogate loss function. Empirical results show that our framework outperforms confidence-based rejection rules.

**Link**: [arxiv](http://arxiv.org/abs/2409.16253v1),  [pdf](http://arxiv.org/pdf/2409.16253v1)

**Tags**: cs.LG I.2.6; I.2.11 



### A Comprehensive Framework for Evaluating API-oriented Code Generation in   Large Language Models
**Authors**: Yixi Wu, Pengfei He, Zehao Wang, Shaowei Wang, Yuan Tian, Tse-Hsun, Chen

**Updated**: 2024-09-24T17:13:43Z

**Summary**: Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as powerful tools for code generation, significantly enhancing productivity and accelerating software development. However, existing benchmarks primarily focus on general code generation without considering API-oriented code generation, i.e., generating code that invokes APIs from specific libraries. Given the growing demand for API-oriented code generation, there is a pressing need for a systematic and automated approach to evaluate LLM on API-oriented code generation. To address this gap, we propose AutoAPIEval, a lightweight and automated framework designed to evaluate the capabilities of LLMs in API-oriented code generation. Our framework works with any library that provides API documentation and focuses on two unit tasks: API recommendation and code example generation, along with four metrics to evaluate the generated APIs and code examples, such as the proportion of incorrect API recommendations for Task 1, and the proportion of code examples where no specific API is invoked and uncompilable/unexecutable code examples for Task 2. In addition, we conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder) and Java Runtime Environment 8 to demonstrate the framework's effectiveness. Our findings reveal substantial variability in LLM performance across tasks, with ChatGPT adhering better to instructions, while sharing similar effectiveness in code example generation with its counterparts (i.e., MagiCoder and DeekSeek Coder). We also identify key factors associated with code quality, such as API popularity and model confidence, and build classifiers that achieve high accuracy in detecting incorrect API recommendations and erroneous code examples. Retrieval-augmented generation enhances the quality of code generated by LLMs, though its effectiveness varies across different LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.15228v2),  [pdf](http://arxiv.org/pdf/2409.15228v2)

**Tags**: cs.SE cs.AI cs.LG 



### Cooperative Resilience in Artificial Intelligence Multiagent Systems
**Authors**: Manuela Chacon-Chamorro, Luis Felipe Giraldo, Nicanor Quijano, Vicente Vargas-Panesso, C√©sar Gonz√°lez, Juan Sebasti√°n Pinz√≥n, Rub√©n Manrique, Manuel R√≠os, Yesid Fonseca, Daniel G√≥mez-Barrera, M√≥nica Perdomo-P√©rez

**Updated**: 2024-09-24T17:13:07Z

**Summary**: Resilience refers to the ability of systems to withstand, adapt to, and recover from disruptive events. While studies on resilience have attracted significant attention across various research domains, the precise definition of this concept within the field of cooperative artificial intelligence remains unclear. This paper addresses this gap by proposing a clear definition of `cooperative resilience' and outlining a methodology for its quantitative measurement. The methodology is validated in an environment with RL-based and LLM-augmented autonomous agents, subjected to environmental changes and the introduction of agents with unsustainable behaviors. These events are parameterized to create various scenarios for measuring cooperative resilience. The results highlight the crucial role of resilience metrics in analyzing how the collective system prepares for, resists, recovers from, sustains well-being, and transforms in the face of disruptions. These findings provide foundational insights into the definition, measurement, and preliminary analysis of cooperative resilience, offering significant implications for the broader field of AI. Moreover, the methodology and metrics developed here can be adapted to a wide range of AI applications, enhancing the reliability and effectiveness of AI in dynamic and unpredictable environments.

**Link**: [arxiv](http://arxiv.org/abs/2409.13187v2),  [pdf](http://arxiv.org/pdf/2409.13187v2)

**Tags**: cs.MA cs.AI 



### A fast and sound tagging method for discontinuous named-entity   recognition
**Authors**: Caio Corro

**Updated**: 2024-09-24T17:07:45Z

**Summary**: We introduce a novel tagging scheme for discontinuous named entity recognition based on an explicit description of the inner structure of discontinuous mentions. We rely on a weighted finite state automaton for both marginal and maximum a posteriori inference. As such, our method is sound in the sense that (1) well-formedness of predicted tag sequences is ensured via the automaton structure and (2) there is an unambiguous mapping between well-formed sequences of tags and (discontinuous) mentions. We evaluate our approach on three English datasets in the biomedical domain, and report comparable results to state-of-the-art while having a way simpler and faster model.

**Link**: [arxiv](http://arxiv.org/abs/2409.16243v1),  [pdf](http://arxiv.org/pdf/2409.16243v1)

**Tags**: cs.CL 



### LLM Echo Chamber: personalized and automated disinformation
**Authors**: Tony Ma

**Updated**: 2024-09-24T17:04:12Z

**Summary**: Recent advancements have showcased the capabilities of Large Language Models like GPT4 and Llama2 in tasks such as summarization, translation, and content review. However, their widespread use raises concerns, particularly around the potential for LLMs to spread persuasive, humanlike misinformation at scale, which could significantly influence public opinion. This study examines these risks, focusing on LLMs ability to propagate misinformation as factual. To investigate this, we built the LLM Echo Chamber, a controlled digital environment simulating social media chatrooms, where misinformation often spreads. Echo chambers, where individuals only interact with like minded people, further entrench beliefs. By studying malicious bots spreading misinformation in this environment, we can better understand this phenomenon. We reviewed current LLMs, explored misinformation risks, and applied sota finetuning techniques. Using Microsoft phi2 model, finetuned with our custom dataset, we generated harmful content to create the Echo Chamber. This setup, evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the ethical concerns surrounding LLMs and emphasizes the need for stronger safeguards against misinformation.

**Link**: [arxiv](http://arxiv.org/abs/2409.16241v1),  [pdf](http://arxiv.org/pdf/2409.16241v1)

**Tags**: cs.AI cs.CY 



### EuroLLM: Multilingual Language Models for Europe
**Authors**: Pedro Henrique Martins, Patrick Fernandes, Jo√£o Alves, Nuno M. Guerreiro, Ricardo Rei, Duarte M. Alves, Jos√© Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, Pierre Colombo, Barry Haddow, Jos√© G. C. de Souza, Alexandra Birch, Andr√© F. T. Martins

**Updated**: 2024-09-24T16:51:36Z

**Summary**: The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on multilingual general benchmarks and machine translation.

**Link**: [arxiv](http://arxiv.org/abs/2409.16235v1),  [pdf](http://arxiv.org/pdf/2409.16235v1)

**Tags**: cs.CL 



### DuQuant: Distributing Outliers via Dual Transformation Makes Stronger   Quantized LLMs
**Authors**: Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei

**Updated**: 2024-09-24T16:40:39Z

**Summary**: Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address $\textit{Normal Outliers}$, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing $\textit{Massive Outliers}$ that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing rotation matrices, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.

**Link**: [arxiv](http://arxiv.org/abs/2406.01721v2),  [pdf](http://arxiv.org/pdf/2406.01721v2)

**Tags**: cs.CL 



### Towards Enhancing Linked Data Retrieval in Conversational UIs using   Large Language Models
**Authors**: Omar Mussa, Omer Rana, Beno√Æt Goossens, Pablo Orozco-Terwengel, Charith Perera

**Updated**: 2024-09-24T16:31:33Z

**Summary**: Despite the recent broad adoption of Large Language Models (LLMs) across various domains, their potential for enriching information systems in extracting and exploring Linked Data (LD) and Resource Description Framework (RDF) triplestores has not been extensively explored. This paper examines the integration of LLMs within existing systems, emphasising the enhancement of conversational user interfaces (UIs) and their capabilities for data extraction by producing more accurate SPARQL queries without the requirement for model retraining. Typically, conversational UI models necessitate retraining with the introduction of new datasets or updates, limiting their functionality as general-purpose extraction tools. Our approach addresses this limitation by incorporating LLMs into the conversational UI workflow, significantly enhancing their ability to comprehend and process user queries effectively. By leveraging the advanced natural language understanding capabilities of LLMs, our method improves RDF entity extraction within web systems employing conventional chatbots. This integration facilitates a more nuanced and context-aware interaction model, critical for handling the complex query patterns often encountered in RDF datasets and Linked Open Data (LOD) endpoints. The evaluation of this methodology shows a marked enhancement in system expressivity and the accuracy of responses to user queries, indicating a promising direction for future research in this area. This investigation not only underscores the versatility of LLMs in enhancing existing information systems but also sets the stage for further explorations into their potential applications within more specialised domains of web information systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.16220v1),  [pdf](http://arxiv.org/pdf/2409.16220v1)

**Tags**: cs.IR cs.AI cs.CL 



### Problem-oriented AutoML in Clustering
**Authors**: Matheus Camilo da Silva, Gabriel Marques Tavares, Eric Medvet, Sylvio Barbon Junior

**Updated**: 2024-09-24T16:25:53Z

**Summary**: The Problem-oriented AutoML in Clustering (PoAC) framework introduces a novel, flexible approach to automating clustering tasks by addressing the shortcomings of traditional AutoML solutions. Conventional methods often rely on predefined internal Clustering Validity Indexes (CVIs) and static meta-features, limiting their adaptability and effectiveness across diverse clustering tasks. In contrast, PoAC establishes a dynamic connection between the clustering problem, CVIs, and meta-features, allowing users to customize these components based on the specific context and goals of their task. At its core, PoAC employs a surrogate model trained on a large meta-knowledge base of previous clustering datasets and solutions, enabling it to infer the quality of new clustering pipelines and synthesize optimal solutions for unseen datasets. Unlike many AutoML frameworks that are constrained by fixed evaluation metrics and algorithm sets, PoAC is algorithm-agnostic, adapting seamlessly to different clustering problems without requiring additional data or retraining. Experimental results demonstrate that PoAC not only outperforms state-of-the-art frameworks on a variety of datasets but also excels in specific tasks such as data visualization, and highlight its ability to dynamically adjust pipeline configurations based on dataset complexity.

**Link**: [arxiv](http://arxiv.org/abs/2409.16218v1),  [pdf](http://arxiv.org/pdf/2409.16218v1)

**Tags**: cs.LG cs.AI 



### Deep Learning for Precision Agriculture: Post-Spraying Evaluation and   Deposition Estimation
**Authors**: Harry Rogers, Tahmina Zebin, Grzegorz Cielniak, Beatriz De La Iglesia, Ben Magri

**Updated**: 2024-09-24T16:16:19Z

**Summary**: Precision spraying evaluation requires automation primarily in post-spraying imagery. In this paper we propose an eXplainable Artificial Intelligence (XAI) computer vision pipeline to evaluate a precision spraying system post-spraying without the need for traditional agricultural methods. The developed system can semantically segment potential targets such as lettuce, chickweed, and meadowgrass and correctly identify if targets have been sprayed. Furthermore, this pipeline evaluates using a domain-specific Weakly Supervised Deposition Estimation task, allowing for class-specific quantification of spray deposit weights in {\mu}L. Estimation of coverage rates of spray deposition in a class-wise manner allows for further understanding of effectiveness of precision spraying systems. Our study evaluates different Class Activation Mapping techniques, namely AblationCAM and ScoreCAM, to determine which is more effective and interpretable for these tasks. In the pipeline, inference-only feature fusion is used to allow for further interpretability and to enable the automation of precision spraying evaluation post-spray. Our findings indicate that a Fully Convolutional Network with an EfficientNet-B0 backbone and inference-only feature fusion achieves an average absolute difference in deposition values of 156.8 {\mu}L across three classes in our test set. The dataset curated in this paper is publicly available at https://github.com/Harry-Rogers/PSIE

**Link**: [arxiv](http://arxiv.org/abs/2409.16213v1),  [pdf](http://arxiv.org/pdf/2409.16213v1)

**Tags**: cs.CV cs.LG 



### LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM
**Authors**: Boyan Li, Shengyi Ding, Deen Ma, Yixuan Wu, Hongjie Liao, Kaiyuan Hu

**Updated**: 2024-09-24T16:09:29Z

**Summary**: Millimeter wave sensing provides people with the capability of sensing the surrounding crowds in a non-invasive and privacy-preserving manner, which holds huge application potential. However, detecting stationary crowds remains challenging due to several factors such as minimal movements (like breathing or casual fidgets), which can be easily treated as noise clusters during data collection and consequently filtered in the following processing procedures. Additionally, the uneven distribution of signal power due to signal power attenuation and interferences resulting from external reflectors or absorbers further complicates accurate detection. To address these challenges and enable stationary crowd detection across various application scenarios requiring specialized domain adaption, we introduce LLMCount, the first system to harness the capabilities of large-language models (LLMs) to enhance crowd detection performance. By exploiting the decision-making capability of LLM, we can successfully compensate the signal power to acquire a uniform distribution and thereby achieve a detection with higher accuracy. To assess the system's performance, comprehensive evaluations are conducted under diversified scenarios like hall, meeting room, and cinema. The evaluation results show that our proposed approach reaches high detection accuracy with lower overall latency compared with previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.16209v1),  [pdf](http://arxiv.org/pdf/2409.16209v1)

**Tags**: cs.CV 



### A is for Absorption: Studying Feature Splitting and Absorption in Sparse   Autoencoders
**Authors**: David Chanin, James Wilken-Smith, Tom√°≈° Dulka, Hardik Bhatnagar, Joseph Bloom

**Updated**: 2024-09-24T16:07:31Z

**Summary**: Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to what extent does varying the sparsity or the size of the SAE affect monosemanticity / interpretability? By investigating these questions in the context of a simple first-letter identification task where we have complete access to ground truth labels for all tokens in the vocabulary, we are able to provide more detail than prior investigations. Critically, we identify a problematic form of feature-splitting we call feature absorption where seemingly monosemantic latents fail to fire in cases where they clearly should. Our investigation suggests that varying SAE size or sparsity is insufficient to solve this issue, and that there are deeper conceptual issues in need of resolution.

**Link**: [arxiv](http://arxiv.org/abs/2409.14507v2),  [pdf](http://arxiv.org/pdf/2409.14507v2)

**Tags**: cs.CL cs.AI 



### CJEval: A Benchmark for Assessing Large Language Models Using Chinese   Junior High School Exam Data
**Authors**: Qian-Wen Zhang, Haochen Wang, Fang Li, Siyu An, Lingfeng Qiao, Liangcai Gao, Di Yin, Xing Sun

**Updated**: 2024-09-25T03:35:35Z

**Summary**: Online education platforms have significantly transformed the dissemination of educational resources by providing a dynamic and digital infrastructure. With the further enhancement of this transformation, the advent of Large Language Models (LLMs) has elevated the intelligence levels of these platforms. However, current academic benchmarks provide limited guidance for real-world industry scenarios. This limitation arises because educational applications require more than mere test question responses. To bridge this gap, we introduce CJEval, a benchmark based on Chinese Junior High School Exam Evaluations. CJEval consists of 26,136 samples across four application-level educational tasks covering ten subjects. These samples include not only questions and answers but also detailed annotations such as question types, difficulty levels, knowledge concepts, and answer explanations. By utilizing this benchmark, we assessed LLMs' potential applications and conducted a comprehensive analysis of their performance by fine-tuning on various educational tasks. Extensive experiments and discussions have highlighted the opportunities and challenges of applying LLMs in the field of education.

**Link**: [arxiv](http://arxiv.org/abs/2409.16202v2),  [pdf](http://arxiv.org/pdf/2409.16202v2)

**Tags**: cs.AI 



### HelloBench: Evaluating Long Text Generation Capabilities of Large   Language Models
**Authors**: Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, Kai Chen

**Updated**: 2024-09-24T15:38:11Z

**Summary**: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks (e.g., long-context understanding), and many benchmarks have been proposed. However, we observe that long text generation capabilities are not well investigated. Therefore, we introduce the Hierarchical Long Text Generation Benchmark (HelloBench), a comprehensive, in-the-wild, and open-ended benchmark to evaluate LLMs' performance in generating long text. Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation. Besides, we propose Hierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation method that significantly reduces the time and effort required for human evaluation while maintaining a high correlation with human evaluation. We have conducted extensive experiments across around 30 mainstream LLMs and observed that the current LLMs lack long text generation capabilities. Specifically, first, regardless of whether the instructions include explicit or implicit length constraints, we observe that most LLMs cannot generate text that is longer than 4000 words. Second, we observe that while some LLMs can generate longer text, many issues exist (e.g., severe repetition and quality degradation). Third, to demonstrate the effectiveness of HelloEval, we compare HelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge methods, which show that HelloEval has the highest correlation with human evaluation. We release our code in https://github.com/Quehry/HelloBench.

**Link**: [arxiv](http://arxiv.org/abs/2409.16191v1),  [pdf](http://arxiv.org/pdf/2409.16191v1)

**Tags**: cs.CL 



### Robust Preference for Dynamical Dark Energy in DESI BAO and SN   Measurements
**Authors**: William Giar√®, Mahdi Najafi, Supriya Pan, Eleonora Di Valentino, Javad T. Firouzjaee

**Updated**: 2024-09-24T15:26:01Z

**Summary**: Recent Baryon Acoustic Oscillation (BAO) measurements released by DESI, when combined with Cosmic Microwave Background (CMB) data from Planck and two different samples of Type Ia supernovae (Pantheon-Plus and DESY5) reveal a preference for Dynamical Dark Energy (DDE) characterized by a present-day quintessence-like equation of state that crossed into the phantom regime in the past. A core ansatz for this result is assuming a linear Chevallier-Polarski-Linder (CPL) parameterization $w(a) = w_0 + w_a (1-a)$ to describe the evolution of the DE equation of state (EoS). In this paper, we test if and to what extent this assumption impacts the results. To prevent broadening uncertainties in cosmological parameter inference and facilitate direct comparison with the baseline CPL case, we focus on 4 alternative well-known models that, just like CPL, consist of only two free parameters: the present-day DE EoS ($w_0$) and a parameter quantifying its dynamical evolution ($w_a$). We demonstrate that the preference for DDE remains robust regardless of the parameterization: $w_0$ consistently remains in the quintessence regime, while $w_a$ consistently indicates a preference for a dynamical evolution towards the phantom regime. This tendency is significantly strengthened by DESY5 SN measurements. By comparing the best-fit $\chi^2$ obtained within each DDE model, we notice that the linear CPL parameterization is not the best-fitting case. Among the models considered, the EoS proposed by Barboza and Alcaniz consistently leads to the most significant improvement.

**Link**: [arxiv](http://arxiv.org/abs/2407.16689v2),  [pdf](http://arxiv.org/pdf/2407.16689v2)

**Tags**: astro-ph.CO hep-ph 



### SDFit: 3D Object Pose and Shape by Fitting a Morphable SDF to a Single   Image
**Authors**: Dimitrije Antiƒá, Sai Kumar Dwivedi, Shashank Tripathi, Theo Gevers, Dimitrios Tzionas

**Updated**: 2024-09-24T15:22:04Z

**Summary**: We focus on recovering 3D object pose and shape from single images. This is highly challenging due to strong (self-)occlusions, depth ambiguities, the enormous shape variance, and lack of 3D ground truth for natural images. Recent work relies mostly on learning from finite datasets, so it struggles generalizing, while it focuses mostly on the shape itself, largely ignoring the alignment with pixels. Moreover, it performs feed-forward inference, so it cannot refine estimates. We tackle these limitations with a novel framework, called SDFit. To this end, we make three key observations: (1) Learned signed-distance-function (SDF) models act as a strong morphable shape prior. (2) Foundational models embed 2D images and 3D shapes in a joint space, and (3) also infer rich features from images. SDFit exploits these as follows. First, it uses a category-level morphable SDF (mSDF) model, called DIT, to generate 3D shape hypotheses. This mSDF is initialized by querying OpenShape's latent space conditioned on the input image. Then, it computes 2D-to-3D correspondences, by extracting and matching features from the image and mSDF. Last, it fits the mSDF to the image in an render-and-compare fashion, to iteratively refine estimates. We evaluate SDFit on the Pix3D and Pascal3D+ datasets of real-world images. SDFit performs roughly on par with state-of-the-art learned methods, but, uniquely, requires no re-training. Thus, SDFit is promising for generalizing in the wild, paving the way for future research. Code will be released

**Link**: [arxiv](http://arxiv.org/abs/2409.16178v1),  [pdf](http://arxiv.org/pdf/2409.16178v1)

**Tags**: cs.CV 



### Cyber Knowledge Completion Using Large Language Models
**Authors**: Braden K Webb, Sumit Purohit, Rounak Meyur

**Updated**: 2024-09-24T15:20:39Z

**Summary**: The integration of the Internet of Things (IoT) into Cyber-Physical Systems (CPSs) has expanded their cyber-attack surface, introducing new and sophisticated threats with potential to exploit emerging vulnerabilities. Assessing the risks of CPSs is increasingly difficult due to incomplete and outdated cybersecurity knowledge. This highlights the urgent need for better-informed risk assessments and mitigation strategies. While previous efforts have relied on rule-based natural language processing (NLP) tools to map vulnerabilities, weaknesses, and attack patterns, recent advancements in Large Language Models (LLMs) present a unique opportunity to enhance cyber-attack knowledge completion through improved reasoning, inference, and summarization capabilities. We apply embedding models to encapsulate information on attack patterns and adversarial techniques, generating mappings between them using vector embeddings. Additionally, we propose a Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained models to create structured mappings between different taxonomies of threat patterns. Further, we use a small hand-labeled dataset to compare the proposed RAG-based approach to a baseline standard binary classification model. Thus, the proposed approach provides a comprehensive framework to address the challenge of cyber-attack knowledge graph completion.

**Link**: [arxiv](http://arxiv.org/abs/2409.16176v1),  [pdf](http://arxiv.org/pdf/2409.16176v1)

**Tags**: cs.CR cs.AI J.7; H.3.3 



### Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to   Extremes Through Rank-Wise Clustering
**Authors**: Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu

**Updated**: 2024-09-24T15:08:41Z

**Summary**: Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning large language models (LLMs) to various domains due to its modular design and widespread availability on platforms like Huggingface. This modularity has sparked interest in combining multiple LoRAs to enhance LLM capabilities. However, existing methods for LoRA composition primarily focus on task-specific adaptations that require additional training, and current model merging techniques often fail to fully leverage LoRA's modular nature, leading to parameter interference and performance degradation. In this paper, we investigate the feasibility of disassembling and reassembling multiple LoRAs at a finer granularity, analogous to assembling LEGO blocks. We introduce the concept of Minimal Semantic Units (MSUs), where the parameters corresponding to each rank in LoRA function as independent units. These MSUs demonstrate permutation invariance and concatenation-summation equivalence properties, enabling flexible combinations to create new LoRAs. Building on these insights, we propose the LoRA-LEGO framework. This framework conducts rank-wise parameter clustering by grouping MSUs from different LoRAs into $k$ clusters. The centroid of each cluster serves as a representative MSU, enabling the assembly of a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual reweighting strategy to optimize the scale of the merged LoRA. Experiments across various benchmarks demonstrate that our method outperforms existing approaches in LoRA merging.

**Link**: [arxiv](http://arxiv.org/abs/2409.16167v1),  [pdf](http://arxiv.org/pdf/2409.16167v1)

**Tags**: cs.LG cs.AI cs.CL 



### Towards Robust Automation of Surgical Systems via Digital Twin-based   Scene Representations from Foundation Models
**Authors**: Hao Ding, Lalithkumar Seenivasan, Hongchao Shu, Grayson Byrd, Han Zhang, Pu Xiao, Juan Antonio Barragan, Russell H. Taylor, Peter Kazanzides, Mathias Unberath

**Updated**: 2024-09-24T15:08:03Z

**Summary**: Large language model-based (LLM) agents are emerging as a powerful enabler of robust embodied intelligence due to their capability of planning complex action sequences. Sound planning ability is necessary for robust automation in many task domains, but especially in surgical automation. These agents rely on a highly detailed natural language representation of the scene. Thus, to leverage the emergent capabilities of LLM agents for surgical task planning, developing similarly powerful and robust perception algorithms is necessary to derive a detailed scene representation of the environment from visual input. Previous research has focused primarily on enabling LLM-based task planning while adopting simple yet severely limited perception solutions to meet the needs for bench-top experiments but lack the critical flexibility to scale to less constrained settings. In this work, we propose an alternate perception approach -- a digital twin-based machine perception approach that capitalizes on the convincing performance and out-of-the-box generalization of recent vision foundation models. Integrating our digital twin-based scene representation and LLM agent for planning with the dVRK platform, we develop an embodied intelligence system and evaluate its robustness in performing peg transfer and gauze retrieval tasks. Our approach shows strong task performance and generalizability to varied environment settings. Despite convincing performance, this work is merely a first step towards the integration of digital twin-based scene representations. Future studies are necessary for the realization of a comprehensive digital twin framework to improve the interpretability and generalizability of embodied intelligence in surgery.

**Link**: [arxiv](http://arxiv.org/abs/2409.13107v2),  [pdf](http://arxiv.org/pdf/2409.13107v2)

**Tags**: cs.RO 



### ChatDBG: An AI-Powered Debugging Assistant
**Authors**: Kyla Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund

**Updated**: 2024-09-24T15:07:24Z

**Summary**: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like `why is x null?'. To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded roughly 50,000 times.

**Link**: [arxiv](http://arxiv.org/abs/2403.16354v2),  [pdf](http://arxiv.org/pdf/2403.16354v2)

**Tags**: cs.SE cs.AI cs.LG cs.PL 



### Seismic fragility curves fitting revisited: ordinal regression models   and their generalization
**Authors**: Libo Chen

**Updated**: 2024-09-24T14:59:39Z

**Summary**: This study revisits the modeling of seismic fragility curves by applying ordinal regression models, offering an alternative to the commonly used log-normal distribution function. It compares various ordinal regression approaches, including Cumulative, Sequential, and Adjacent Category models, along with extensions that account for category-specific effects and variance heterogeneity. The methodologies are applied to bridge damage data from the 2008 Wenchuan earthquake, using both frequentist and Bayesian inference methods, with model diagnostics conducted using surrogate residuals. The analysis examines eleven models, from basic forms to those incorporating heteroscedastic extensions and category-specific effects. Based on leave-one-out cross-validation, the Sequential model with category-specific effects performs well compared to traditional Cumulative probit models. The results indicate differences in damage probability predictions between the models, suggesting the potential for more flexible fragility curve modeling techniques to improve seismic risk assessments. This study highlights the importance of continued evaluation of existing methods to enhance the predictive accuracy and applicability of seismic fragility models in performance-based earthquake engineering.

**Link**: [arxiv](http://arxiv.org/abs/2405.15513v2),  [pdf](http://arxiv.org/pdf/2405.15513v2)

**Tags**: stat.AP 



### Video-XL: Extra-Long Vision Language Model for Hour-Scale Video   Understanding
**Authors**: Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, Bo Zhao

**Updated**: 2024-09-24T14:59:30Z

**Summary**: Although current Multi-modal Large Language Models (MLLMs) demonstrate promising results in video understanding, processing extremely long videos remains an ongoing challenge. Typically, MLLMs struggle with handling thousands of tokens that exceed the maximum context length of LLMs, and they experience reduced visual clarity due to token aggregation. Another challenge is the high computational cost stemming from the large number of video tokens. To tackle these issues, we propose Video-XL, an extra-long vision language model designed for efficient hour-scale video understanding. Specifically, we argue that LLMs can be adapted as effective visual condensers and introduce Visual Context Latent Summarization, which condenses visual contexts into highly compact forms. Extensive experiments demonstrate that our model achieves promising results on popular long video understanding benchmarks, despite being trained on limited image data. Moreover, Video-XL strikes a promising balance between efficiency and effectiveness, processing 1024 frames on a single 80GB GPU while achieving nearly 100\% accuracy in the Needle-in-a-Haystack evaluation. We envision Video-XL becoming a valuable tool for long video applications such as video summarization, surveillance anomaly detection, and Ad placement identification.

**Link**: [arxiv](http://arxiv.org/abs/2409.14485v2),  [pdf](http://arxiv.org/pdf/2409.14485v2)

**Tags**: cs.CV 



### Efficient Motion Prediction: A Lightweight & Accurate Trajectory   Prediction Model With Fast Training and Inference Speed
**Authors**: Alexander Prutsch, Horst Bischof, Horst Possegger

**Updated**: 2024-09-25T09:00:27Z

**Summary**: For efficient and safe autonomous driving, it is essential that autonomous vehicles can predict the motion of other traffic agents. While highly accurate, current motion prediction models often impose significant challenges in terms of training resource requirements and deployment on embedded hardware. We propose a new efficient motion prediction model, which achieves highly competitive benchmark results while training only a few hours on a single GPU. Due to our lightweight architectural choices and the focus on reducing the required training resources, our model can easily be applied to custom datasets. Furthermore, its low inference latency makes it particularly suitable for deployment in autonomous applications with limited computing resources.

**Link**: [arxiv](http://arxiv.org/abs/2409.16154v2),  [pdf](http://arxiv.org/pdf/2409.16154v2)

**Tags**: cs.RO cs.CV 



### HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear   Composition for Open-Vocabulary Object Detection
**Authors**: Yuqi Ma, Mengyin Liu, Chao Zhu, Xu-Cheng Yin

**Updated**: 2024-09-24T14:43:14Z

**Summary**: Open-vocabulary object detection (OVD) models are considered to be Large Multi-modal Models (LMM), due to their extensive training data and a large number of parameters. Mainstream OVD models prioritize object coarse-grained category rather than focus on their fine-grained attributes, e.g., colors or materials, thus failed to identify objects specified with certain attributes. However, OVD models are pretrained on large-scale image-text pairs with rich attribute words, whose latent feature space can represent the global text feature as a linear composition of fine-grained attribute tokens without highlighting them. Therefore, we propose in this paper a universal and explicit approach for frozen mainstream OVD models that boosts their attribute-level detection capabilities by highlighting fine-grained attributes in explicit linear space. Firstly, a LLM is leveraged to highlight attribute words within the input text as a zero-shot prompted task. Secondly, by strategically adjusting the token masks, the text encoders of OVD models extract both global text and attribute-specific features, which are then explicitly composited as two vectors in linear space to form the new attribute-highlighted feature for detection tasks, where corresponding scalars are hand-crafted or learned to reweight both two vectors. Notably, these scalars can be seamlessly transferred among different OVD models, which proves that such an explicit linear composition is universal. Empirical evaluation on the FG-OVD dataset demonstrates that our proposed method uniformly improves fine-grained attribute-level OVD of various mainstream models and achieves new state-of-the-art performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.16136v1),  [pdf](http://arxiv.org/pdf/2409.16136v1)

**Tags**: cs.CV cs.AI cs.CL cs.MM 



### Parameter selection and optimization of a computational network model of   blood flow in single-ventricle patients
**Authors**: Alyssa M. Taylor-LaPole, L. Mihaela Paun, Dan Lior, Justin D Weigand, Charles Puelz, Mette S. Olufsen

**Updated**: 2024-09-24T14:32:06Z

**Summary**: Hypoplastic left heart syndrome (HLHS) is a congenital heart disease responsible for 23% of infant cardiac deaths each year. HLHS patients are born with an underdeveloped left heart, requiring several surgeries to reconstruct the aorta and create a single ventricle circuit known as the Fontan circulation. While survival into early adulthood is becoming more common, Fontan patients suffer from reduced cardiac output, putting them at risk for a multitude of complications. These patients are monitored using chest and neck MRI imaging, but these scans do not capture energy loss, pressure, wave intensity, or hemodynamics beyond the imaged region. This study develops a framework for predicting these missing features by combining imaging data and computational fluid dynamics (CFD) models. Predicted features from models of HLHS patients are compared to those from control patients with a double outlet right ventricle (DORV). We use parameter inference to render the model patient-specific. In the calibrated model, we predict pressure, flow, wave-intensity (WI), and wall shear stress (WSS). Results reveal that HLHS patients have higher vascular stiffness and lower compliance than DORV patients, resulting in lower WSS and higher WI in the ascending aorta and increased WSS and decreased WI in the descending aorta.

**Link**: [arxiv](http://arxiv.org/abs/2406.18490v2),  [pdf](http://arxiv.org/pdf/2406.18490v2)

**Tags**: q-bio.QM 



### MOSS: Enabling Code-Driven Evolution and Context Management for AI   Agents
**Authors**: Ming Zhu, Yi Zhou

**Updated**: 2024-09-24T14:30:21Z

**Summary**: Developing AI agents powered by large language models (LLMs) faces significant challenges in achieving true Turing completeness and adaptive, code-driven evolution. Current approaches often generate code independently of its runtime context, relying heavily on the LLM's memory, which results in inefficiencies and limits adaptability. Manual protocol development in sandbox environments further constrains the agent's autonomous adaptability. Crucially, achieving consistency in code and context across multi-turn interactions and ensuring isolation of local variables within each interaction remains an unsolved problem.   We introduce MOSS (llM-oriented Operating System Simulation), a novel framework that addresses these challenges by integrating code generation with a dynamic context management system. MOSS ensures consistency and adaptability by using a mechanism that maintains the Python context across interactions, including isolation of local variables and preservation of runtime integrity. At its core, the framework employs an Inversion of Control (IoC) container in conjunction with decorators to enforce the least knowledge principle, allowing agents to focus on abstract interfaces rather than concrete implementations. This facilitates seamless integration of new tools and libraries, enables runtime instance replacement, and reduces prompt complexity, providing a "what you see is what you get" environment for the agent.   Through a series of case studies, we show how this framework can enhance the efficiency and capabilities of agent development and highlight its advantages in moving towards Turing-complete agents capable of evolving through code.

**Link**: [arxiv](http://arxiv.org/abs/2409.16120v1),  [pdf](http://arxiv.org/pdf/2409.16120v1)

**Tags**: cs.SE cs.AI cs.CL 



### Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of   Large Language Models
**Authors**: Nishanth Madhusudhan, Sathwik Tejaswi Madhusudhan, Vikas Yadav, Masoud Hashemi

**Updated**: 2024-09-24T14:25:58Z

**Summary**: Abstention Ability (AA) is a critical aspect of Large Language Model (LLM) reliability, referring to an LLM's capability to withhold responses when uncertain or lacking a definitive answer, without compromising performance. Although previous studies have attempted to improve AA, they lack a standardised evaluation method and remain unsuitable for black-box models where token prediction probabilities are inaccessible. This makes comparative analysis challenging, especially for state-of-the-art closed-source commercial LLMs. This paper bridges this gap by introducing a black-box evaluation approach and a new dataset, Abstain-QA, crafted to rigorously assess AA across varied question types (answerable and unanswerable), domains (well-represented and under-represented), and task types (fact centric and reasoning). We also propose a new confusion matrix, the ''Answerable-Unanswerable Confusion Matrix'' (AUCM) which serves as the basis for evaluating AA, by offering a structured and precise approach for assessment. Finally, we explore the impact of three prompting strategies-Strict Prompting, Verbal Confidence Thresholding, and Chain-of-Thought (CoT)-on improving AA. Our results indicate that even powerful models like GPT-4, Mixtral 8x22b encounter difficulties with abstention; however, strategic approaches such as Strict prompting and CoT can enhance this capability.

**Link**: [arxiv](http://arxiv.org/abs/2407.16221v2),  [pdf](http://arxiv.org/pdf/2407.16221v2)

**Tags**: cs.CL 



### KaMPIng: Flexible and (Near) Zero-Overhead C++ Bindings for MPI
**Authors**: Tim Niklas Uhl, Matthias Schimek, Lukas H√ºbner, Demian Hespe, Florian Kurpicz, Christoph Stelz, Peter Sanders

**Updated**: 2024-09-24T14:20:45Z

**Summary**: The Message-Passing Interface (MPI) and C++ form the backbone of high-performance computing, but MPI only provides C and Fortran bindings. While this offers great language interoperability, high-level programming languages like C++ make software development quicker and less error-prone.   We propose novel C++ language bindings that cover all abstraction levels from low-level MPI calls to convenient STL-style bindings, where most parameters are inferred from a small subset of parameters, by bringing named parameters to C++. This enables rapid prototyping and fine-tuning runtime behavior and memory management. A flexible type system and additional safety guarantees help to prevent programming errors.   By exploiting C++'s template metaprogramming capabilities, this has (near) zero overhead, as only required code paths are generated at compile time.   We demonstrate that our library is a strong foundation for a future distributed standard library using multiple application benchmarks, ranging from text-book sorting algorithms to phylogenetic interference.

**Link**: [arxiv](http://arxiv.org/abs/2404.05610v3),  [pdf](http://arxiv.org/pdf/2404.05610v3)

**Tags**: cs.DC 



### Parameterizations for Large-Scale Variational System Identification   Using Unconstrained Optimization
**Authors**: Dimas Abreu Archanjo Dutra

**Updated**: 2024-09-24T14:15:26Z

**Summary**: This paper details how to parameterize the posterior distribution of state-space systems to generate improved optimization problems for system identification using variational inference. Three different parameterizations of the assumed state-path posterior distribution are proposed based on this representation: time-varying, steady-state, and convolution smoother; each resulting in a different parameter estimator. In contrast to existing methods for variational system identification, the proposed estimators can be implemented with unconstrained optimization methods. Furthermore, when applied to mini-batches in conjunction with stochastic optimization, the convolution-smoother formulation enables identification of large linear and nonlinear state-space systems from very large datasets. For linear systems, the method achieves the same performance as the inherently sequential prediction-error methods using an embarrassingly parallel algorithm that benefits from large speedups when computed in modern graphical processing units (GPUs). The ability of the proposed estimators to identify large models, work with large datasets split into mini-batches, and work in parallel on GPUs make them well-suited for identifying deep models for applications in systems and control.

**Link**: [arxiv](http://arxiv.org/abs/2404.10137v2),  [pdf](http://arxiv.org/pdf/2404.10137v2)

**Tags**: stat.AP 



### Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts   for Instruction Tuning on General Tasks
**Authors**: Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu

**Updated**: 2024-09-24T14:14:40Z

**Summary**: Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5. Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.

**Link**: [arxiv](http://arxiv.org/abs/2401.02731v4),  [pdf](http://arxiv.org/pdf/2401.02731v4)

**Tags**: cs.AI 



### Angular power spectrum of gravitational-wave transient sources as a   probe of the large-scale structure
**Authors**: Yanyan Zheng, Nikolaos Kouvatsos, Jacob Golomb, Marco Cavagli√†, Arianna I. Renzini, Mairi Sakellariadou

**Updated**: 2024-09-24T14:10:13Z

**Summary**: We present a new, simulation-based inference method to compute the angular power spectrum of the distribution of foreground gravitational-wave transient events. As a first application of this method, we use the binary black hole mergers observed during the LIGO, Virgo, and KAGRA third observation run to test the spatial distribution of these sources. We find no evidence for anisotropy in their angular distribution. We discuss further applications of this method to investigate other gravitational-wave source populations and their correlations to the cosmological large-scale structure.

**Link**: [arxiv](http://arxiv.org/abs/2305.02652v2),  [pdf](http://arxiv.org/pdf/2305.02652v2)

**Tags**: astro-ph.CO gr-qc 



### Scenario of Use Scheme: Threat Model Specification for Speaker Privacy   Protection in the Medical Domain
**Authors**: Mehtab Ur Rahman, Martha Larson, Louis ten Bosch, Cristian Tejedor-Garc√≠a

**Updated**: 2024-09-24T14:07:47Z

**Summary**: Speech recordings are being more frequently used to detect and monitor disease, leading to privacy concerns. Beyond cryptography, protection of speech can be addressed by approaches, such as perturbation, disentanglement, and re-synthesis, that eliminate sensitive information of the speaker, leaving the information necessary for medical analysis purposes. In order for such privacy protective approaches to be developed, clear and systematic specifications of assumptions concerning medical settings and the needs of medical professionals are necessary. In this paper, we propose a Scenario of Use Scheme that incorporates an Attacker Model, which characterizes the adversary against whom the speaker's privacy must be defended, and a Protector Model, which specifies the defense. We discuss the connection of the scheme with previous work on speech privacy. Finally, we present a concrete example of a specified Scenario of Use and a set of experiments about protecting speaker data against gender inference attacks while maintaining utility for Parkinson's detection.

**Link**: [arxiv](http://arxiv.org/abs/2409.16106v1),  [pdf](http://arxiv.org/pdf/2409.16106v1)

**Tags**: eess.AS cs.AI cs.CR cs.SD 



### Adversarial Attacks on Machine Learning-Aided Visualizations
**Authors**: Takanori Fujiwara, Kostiantyn Kucher, Junpeng Wang, Rafael M. Martins, Andreas Kerren, Anders Ynnerman

**Updated**: 2024-09-24T13:58:37Z

**Summary**: Research in ML4VIS investigates how to use machine learning (ML) techniques to generate visualizations, and the field is rapidly growing with high societal impact. However, as with any computational pipeline that employs ML processes, ML4VIS approaches are susceptible to a range of ML-specific adversarial attacks. These attacks can manipulate visualization generations, causing analysts to be tricked and their judgments to be impaired. Due to a lack of synthesis from both visualization and ML perspectives, this security aspect is largely overlooked by the current ML4VIS literature. To bridge this gap, we investigate the potential vulnerabilities of ML-aided visualizations from adversarial attacks using a holistic lens of both visualization and ML perspectives. We first identify the attack surface (i.e., attack entry points) that is unique in ML-aided visualizations. We then exemplify five different adversarial attacks. These examples highlight the range of possible attacks when considering the attack surface and multiple different adversary capabilities. Our results show that adversaries can induce various attacks, such as creating arbitrary and deceptive visualizations, by systematically identifying input attributes that are influential in ML inferences. Based on our observations of the attack surface characteristics and the attack examples, we underline the importance of comprehensive studies of security issues and defense mechanisms as a call of urgency for the ML4VIS community.

**Link**: [arxiv](http://arxiv.org/abs/2409.02485v2),  [pdf](http://arxiv.org/pdf/2409.02485v2)

**Tags**: cs.CR cs.AI cs.HC cs.LG stat.ML 



### EffiQA: Efficient Question-Answering with Strategic Multi-Model   Collaboration on Knowledge Graphs
**Authors**: Zixuan Dong, Baoyun Peng, Yufei Wang, Jia Fu, Xiaodong Wang, Yongxue Shan, Xin Zhou

**Updated**: 2024-09-24T13:53:59Z

**Summary**: While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm. EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve the global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering.

**Link**: [arxiv](http://arxiv.org/abs/2406.01238v3),  [pdf](http://arxiv.org/pdf/2406.01238v3)

**Tags**: cs.CL 



### Great, Now Write an Article About That: The Crescendo Multi-Turn LLM   Jailbreak Attack
**Authors**: Mark Russinovich, Ahmed Salem, Ronen Eldan

**Updated**: 2024-09-24T13:51:39Z

**Summary**: Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we present Crescendomation, a tool that automates the Crescendo attack and demonstrate its efficacy against state-of-the-art models through our evaluations. Crescendomation surpasses other state-of-the-art jailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher performance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate Crescendo's ability to jailbreak multimodal models.

**Link**: [arxiv](http://arxiv.org/abs/2404.01833v2),  [pdf](http://arxiv.org/pdf/2404.01833v2)

**Tags**: cs.CR cs.AI 



### Exploring Hint Generation Approaches in Open-Domain Question Answering
**Authors**: Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt

**Updated**: 2024-09-24T13:50:32Z

**Summary**: Automatic Question Answering (QA) systems rely on contextual information to provide accurate answers. Commonly, contexts are prepared through either retrieval-based or generation-based methods. The former involves retrieving relevant documents from a corpus like Wikipedia, whereas the latter uses generative models such as Large Language Models (LLMs) to generate the context. In this paper, we introduce a novel context preparation approach called HINTQA, which employs Automatic Hint Generation (HG) techniques. Unlike traditional methods, HINTQA prompts LLMs to produce hints about potential answers for the question rather than generating relevant context. We evaluate our approach across three QA datasets including TriviaQA, NaturalQuestions, and Web Questions, examining how the number and order of hints impact performance. Our findings show that the HINTQA surpasses both retrieval-based and generation-based approaches. We demonstrate that hints enhance the accuracy of answers more than retrieved and generated contexts.

**Link**: [arxiv](http://arxiv.org/abs/2409.16096v1),  [pdf](http://arxiv.org/pdf/2409.16096v1)

**Tags**: cs.CL cs.IR 



### Beyond the Black Box: A Statistical Model for LLM Reasoning and   Inference
**Authors**: Siddhartha Dalal, Vishal Misra

**Updated**: 2024-09-24T13:30:25Z

**Summary**: This paper introduces a novel Bayesian learning model to explain the behavior of Large Language Models (LLMs), focusing on their core optimization metric of next token prediction. We develop a theoretical framework based on an ideal generative text model represented by a multinomial transition probability matrix with a prior, and examine how LLMs approximate this matrix. Key contributions include: (i) a continuity theorem relating embeddings to multinomial distributions, (ii) a demonstration that LLM text generation aligns with Bayesian learning principles, (iii) an explanation for the emergence of in-context learning in larger models, (iv) empirical validation using visualizations of next token probabilities from an instrumented Llama model Our findings provide new insights into LLM functioning, offering a statistical foundation for understanding their capabilities and limitations. This framework has implications for LLM design, training, and application, potentially guiding future developments in the field.

**Link**: [arxiv](http://arxiv.org/abs/2402.03175v2),  [pdf](http://arxiv.org/pdf/2402.03175v2)

**Tags**: cs.LG cs.AI I.2.7 



### OpenGraph: Towards Open Graph Foundation Models
**Authors**: Lianghao Xia, Ben Kao, Chao Huang

**Updated**: 2024-09-24T13:26:08Z

**Summary**: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot graph learning tasks across different downstream datasets. To achieve this goal, we address several key technical challenges in our OpenGraph model. Firstly, we propose a unified graph tokenizer to adapt our graph model to generalize well on unseen graph data, even when the underlying graph properties differ significantly from those encountered during training. Secondly, we develop a scalable graph transformer as the foundational encoder, which effectively captures node-wise dependencies within the global topological context. Thirdly, we introduce a data augmentation mechanism enhanced by a LLM to alleviate the limitations of data scarcity in real-world scenarios. Extensive experiments validate the effectiveness of our framework. By adapting our OpenGraph to new graph characteristics and comprehending the nuances of diverse graphs, our approach achieves remarkable zero-shot graph learning performance across various settings and domains.

**Link**: [arxiv](http://arxiv.org/abs/2403.01121v3),  [pdf](http://arxiv.org/pdf/2403.01121v3)

**Tags**: cs.LG cs.AI cs.SI 



### Open Conversational LLMs do not know most Spanish words
**Authors**: Javier Conde, Miguel Gonz√°lez, Nina Melero, Raquel Ferrando, Gonzalo Mart√≠nez, Elena Merino-G√≥mez, Jos√© Alberto Hern√°ndez, Pedro Reviriego

**Updated**: 2024-09-24T13:25:01Z

**Summary**: The growing interest in Large Language Models (LLMs) and in particular in conversational models with which users can interact has led to the development of a large number of open-source chat LLMs. These models are evaluated on a wide range of benchmarks to assess their capabilities in answering questions or solving problems on almost any possible topic or to test their ability to reason or interpret texts. Instead, the evaluation of the knowledge that these models have of the languages has received much less attention. For example, the words that they can recognize and use in different languages. In this paper, we evaluate the knowledge that open-source chat LLMs have of Spanish words by testing a sample of words in a reference dictionary. The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.

**Link**: [arxiv](http://arxiv.org/abs/2403.15491v2),  [pdf](http://arxiv.org/pdf/2403.15491v2)

**Tags**: cs.CL 



### Ultra-low latency quantum-inspired machine learning predictors   implemented on FPGA
**Authors**: Lorenzo Borella, Alberto Coppi, Jacopo Pazzini, Andrea Stanco, Marco Trenti, Andrea Triossi, Marco Zanetti

**Updated**: 2024-09-25T08:59:26Z

**Summary**: Tensor Networks (TNs) are a computational paradigm used for representing quantum many-body systems. Recent works have shown how TNs can also be applied to perform Machine Learning (ML) tasks, yielding comparable results to standard supervised learning techniques. In this work, we study the use of Tree Tensor Networks (TTNs) in high-frequency real-time applications by exploiting the low-latency hardware of the Field-Programmable Gate Array (FPGA) technology. We present different implementations of TTN classifiers, capable of performing inference on classical ML datasets as well as on complex physics data. A preparatory analysis of bond dimensions and weight quantization is realized in the training phase, together with entanglement entropy and correlation measurements, that help setting the choice of the TTN architecture. The generated TTNs are then deployed on a hardware accelerator; using an FPGA integrated into a server, the inference of the TTN is completely offloaded. Eventually, a classifier for High Energy Physics (HEP) applications is implemented and executed fully pipelined with sub-microsecond latency.

**Link**: [arxiv](http://arxiv.org/abs/2409.16075v2),  [pdf](http://arxiv.org/pdf/2409.16075v2)

**Tags**: hep-ex cs.LG quant-ph 



### Galled Perfect Transfer Networks
**Authors**: Alitzel L√≥pez S√°nchez, Manuel Lafond

**Updated**: 2024-09-24T13:08:14Z

**Summary**: Predicting horizontal gene transfers often requires comparative sequence data, but recent work has shown that character-based approaches could also be useful for this task. Notably, perfect transfer networks (PTN) explain the character diversity of a set of taxa for traits that are gained once, rarely lost, but that can be transferred laterally. Characterizing the structure of such characters is an important step towards understanding more complex characters. Although efficient algorithms can infer such networks from character data, they can sometimes predict overly complicated transfer histories. With the goal of recovering the simplest possible scenarios in this model, we introduce galled perfect transfer networks, which are PTNs that are galled trees. Such networks are useful for characters that are incompatible in terms of tree-like evolution, but that do fit in an almost-tree scenario. We provide polynomial-time algorithms for two problems: deciding whether one can add transfer edges to a tree to transform it into a galled PTN, and deciding whether a set of characters are galled-compatible, that is, they can be explained by some galled PTN. We also analyze a real dataset comprising of a bacterial species trees and KEGG functions as characters, and derive several conclusions on the difficulty of explaining characters in a galled tree, which provide several directions for future research.

**Link**: [arxiv](http://arxiv.org/abs/2409.03935v2),  [pdf](http://arxiv.org/pdf/2409.03935v2)

**Tags**: cs.DM cs.DS q-bio.PE 



### PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large   Language Models
**Authors**: Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron, Tabor Flickinger, Laura E. Barnes

**Updated**: 2024-09-24T13:03:24Z

**Summary**: Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues. Although existing natural language processing (NLP) techniques show promise, they struggle with the nuances of clinical communication and require sensitive clinical data for training, reducing their effectiveness in real-world applications. Emerging large language models (LLMs) offer a new approach to assessing complex communication metrics, with the potential to advance the field through integration into passive sensing and just-in-time intervention systems. This study explores LLMs as evaluators of palliative care communication quality, leveraging their linguistic, in-context learning, and reasoning capabilities. Specifically, using simulated scripts crafted and labeled by healthcare professionals, we test proprietary models (e.g., GPT-4) and fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset generated by GPT-4 to evaluate clinical conversations, to identify key metrics such as `understanding' and `empathy'. Our findings demonstrated LLMs' superior performance in evaluating clinical communication, providing actionable feedback with reasoning, and demonstrating the feasibility and practical viability of developing in-house LLMs. This research highlights LLMs' potential to enhance patient-provider interactions and lays the groundwork for downstream steps in developing LLM-empowered clinical health systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.15188v2),  [pdf](http://arxiv.org/pdf/2409.15188v2)

**Tags**: cs.CL cs.HC 



### JWST Observations of Young protoStars (JOYS). HH 211: the textbook case   of a protostellar jet and outflow
**Authors**: A. Caratti o Garatti, T. P. Ray, P. J. Kavanagh, M. J. McCaughrean, C. Gieser, T. Giannini, E. F. van Dishoeck, K. Justtanont, M. L. van Gelder, L. Francis, H. Beuther, ≈Å. Tychoniec, B. Nisini, M. G. Navarro, R. Devaraj, S. Reyes, P. Nazar, P. Klaassen, M. G√ºdel, Th. Henning, P. O. Lagage, G. √ñstlin, B. Vandenbussche, C. Waelkens, G. Wright

**Updated**: 2024-09-24T13:02:58Z

**Summary**: We use the James Webb Space Telescope (JWST) and its Mid-Infrared Instrument (MIRI) (5-28 um), to study the embedded HH 211 flow. We map a 0.95'x0.22' region, covering the full extent of the blue-shifted lobe, the central protostellar region, and a small portion of the red-shifted lobe. The jet driving source is not detected even at the longest mid-IR wavelengths. The overall morphology of the flow consists of a highly collimated jet, mostly molecular (H2, HD) with an inner atomic ([FeI], [FeII], [SI], [NiII]) structure. The jet shocks the ambient medium, producing several large bow-shocks, rich in forbidden atomic and molecular lines, and is driving an H2 molecular outflow, mostly traced by low-J, v=0 transitions. Moreover, 0-0 S(1) uncollimated emission is also detected down to 2"-3" (~650-1000 au) from the source, tracing a cold (T=200-400 K), less dense and poorly collimated molecular wind. The atomic jet ([FeII] at 26 um) is detected down to ~130 au from source, whereas the lack of H2 emission close to the source is likely due to the large visual extinction. Dust continuum-emission is detected at the terminal bow-shocks, and in the blue- and red-shifted jet, being likely dust lifted from the disk. The jet shows an onion-like structure, with layers of different size, velocity, temperature, and chemical composition. Moreover, moving from the inner jet to the outer bow-shocks, different physical, kinematic and excitation conditions for both molecular and atomic gas are observed. The jet mass-flux rate, momentum, and momentum flux of the warm H2 component are up to one order of magnitude higher than those inferred from the atomic jet component. Our findings indicate that the warm H2 component is the primary mover of the outflow, namely it is the most significant dynamical component of the jet, in contrast to jets from more evolved YSOs, where the atomic component is dominant.

**Link**: [arxiv](http://arxiv.org/abs/2409.16061v1),  [pdf](http://arxiv.org/pdf/2409.16061v1)

**Tags**: astro-ph.SR astro-ph.GA 



### A new analytic approach to infer the cosmic-ray ionization rate in hot   molecular cores from HCO$^+$, N$_2$H$^+$, and CO observations
**Authors**: Gan Luo, Thomas G. Bisbas, Marco Padovani, Brandt A. L. Gaches

**Updated**: 2024-09-24T12:50:17Z

**Summary**: The cosmic-ray ionization rate ($\zeta_2$) is one of the key parameters in star formation, since it regulates the chemical and dynamical evolution of molecular clouds by ionizing molecules and determining the coupling between the magnetic field and gas. However, measurements of $\zeta_2$ in dense clouds (e.g., $n_{\rm H} \geq 10^4$ cm$^{-3}$) are difficult and sensitive to the model assumptions. The aim is to find a convenient analytic approach that can be used in high-mass star-forming regions (HMSFRs), especially for warm gas environments such as hot molecular cores (HMCs). We propose a new analytic approach to calculate $\zeta_2$ through HCO$^+$, N$_2$H$^+$, and CO measurements. Our method gives a good approximation, to within $50$\%, of $\zeta_2$ in dense and warm gas (e.g., $n_{\rm H} \geq 10^4$ cm$^{-3}$, $T = 50, 100$ K) for $A_{\rm V} \geq 4$ mag and $t \geq 2\times10^4$ yr at Solar metallicity. The analytic approach gives better results for higher densities. However, it starts to underestimate the CRIR at low metallicity ($Z = 0.1Z_\odot$) and high CRIR ($\zeta_2 \geq 3\times10^{-15}$ s$^{-1}$). By applying our method to the OMC-2 FIR4 envelope and the L1157-B1 shock region, we find $\zeta_2$ values of $(1.0\pm0.3)\times10^{-14}$ s$^{-1}$ and $(2.2\pm0.4)\times10^{-16}$ s$^{-1}$, consistent with those previously reported. We calculate $\zeta_2$ toward a total of 82 samples in HMSFRs, finding that the average value of $\zeta_2$ toward all HMC samples ($\zeta_2$ = (7.4$\pm$5.0)$\times$10$^{-16}$ s$^{-1}$) is more than an order of magnitude higher than the theoretical prediction of cosmic-ray attenuation models, favoring the scenario that locally accelerated cosmic rays in embedded protostars should be responsible for the observed high $\zeta_2$.

**Link**: [arxiv](http://arxiv.org/abs/2409.07181v2),  [pdf](http://arxiv.org/pdf/2409.07181v2)

**Tags**: astro-ph.GA 



### Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of   Experts
**Authors**: Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, Zhou Ye, Qingsong Wen, Ming Jin

**Updated**: 2024-09-24T12:42:18Z

**Summary**: Deep learning for time series forecasting has seen significant advancements over the past decades. However, despite the success of large-scale pre-training in language and vision domains, pre-trained time series models remain limited in scale and operate at a high cost, hindering the development of larger capable forecasting models in real-world applications. In response, we introduce Time-MoE, a scalable and unified architecture designed to pre-train larger, more capable forecasting foundation models while reducing inference costs. By leveraging a sparse mixture-of-experts (MoE) design, Time-MoE enhances computational efficiency by activating only a subset of networks for each prediction, reducing computational load while maintaining high model capacity. This allows Time-MoE to scale effectively without a corresponding increase in inference costs. Time-MoE comprises a family of decoder-only transformer models that operate in an auto-regressive manner and support flexible forecasting horizons with varying input context lengths. We pre-trained these models on our newly introduced large-scale data Time-300B, which spans over 9 domains and encompassing over 300 billion time points. For the first time, we scaled a time series foundation model up to 2.4 billion parameters, achieving significantly improved forecasting precision. Our results validate the applicability of scaling laws for training tokens and model size in the context of time series forecasting. Compared to dense models with the same number of activated parameters or equivalent computation budgets, our models consistently outperform them by large margin. These advancements position Time-MoE as a state-of-the-art solution for tackling real-world time series forecasting challenges with superior capability, efficiency, and flexibility.

**Link**: [arxiv](http://arxiv.org/abs/2409.16040v1),  [pdf](http://arxiv.org/pdf/2409.16040v1)

**Tags**: cs.LG cs.AI 



### Region-Adaptive Transform with Segmentation Prior for Image Compression
**Authors**: Yuxi Liu, Wenhan Yang, Huihui Bai, Yunchao Wei, Yao Zhao

**Updated**: 2024-09-24T12:40:19Z

**Summary**: Learned Image Compression (LIC) has shown remarkable progress in recent years. Existing works commonly employ CNN-based or self-attention-based modules as transform methods for compression. However, there is no prior research on neural transform that focuses on specific regions. In response, we introduce the class-agnostic segmentation masks (i.e. semantic masks without category labels) for extracting region-adaptive contextual information. Our proposed module, Region-Adaptive Transform, applies adaptive convolutions on different regions guided by the masks. Additionally, we introduce a plug-and-play module named Scale Affine Layer to incorporate rich contexts from various regions. While there have been prior image compression efforts that involve segmentation masks as additional intermediate inputs, our approach differs significantly from them. Our advantages lie in that, to avoid extra bitrate overhead, we treat these masks as privilege information, which is accessible during the model training stage but not required during the inference phase. To the best of our knowledge, we are the first to employ class-agnostic masks as privilege information and achieve superior performance in pixel-fidelity metrics, such as Peak Signal to Noise Ratio (PSNR). The experimental results demonstrate our improvement compared to previously well-performing methods, with about 8.2% bitrate saving compared to VTM-17.0. The source code is available at https://github.com/GityuxiLiu/SegPIC-for-Image-Compression.

**Link**: [arxiv](http://arxiv.org/abs/2403.00628v4),  [pdf](http://arxiv.org/pdf/2403.00628v4)

**Tags**: cs.CV eess.IV 



### MHRC: Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration   with Large Language Models
**Authors**: Wenhao Yu, Jie Peng, Yueliang Ying, Sai Li, Jianmin Ji, Yanyong Zhang

**Updated**: 2024-09-24T12:29:44Z

**Summary**: The integration of large language models (LLMs) with robotics has significantly advanced robots' abilities in perception, cognition, and task planning. The use of natural language interfaces offers a unified approach for expressing the capability differences of heterogeneous robots, facilitating communication between them, and enabling seamless task allocation and collaboration. Currently, the utilization of LLMs to achieve decentralized multi-heterogeneous robot collaborative tasks remains an under-explored area of research. In this paper, we introduce a novel framework that utilizes LLMs to achieve decentralized collaboration among multiple heterogeneous robots. Our framework supports three robot categories, mobile robots, manipulation robots, and mobile manipulation robots, working together to complete tasks such as exploration, transportation, and organization. We developed a rich set of textual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task planning efficiency and overall system performance. The mobile manipulation robot can adjust its base position flexibly, ensuring optimal conditions for grasping tasks. The manipulation robot can comprehend task requirements, seek assistance when necessary, and handle objects appropriately. Meanwhile, the mobile robot can explore the environment extensively, map object locations, and communicate this information to the mobile manipulation robot, thus improving task execution efficiency. We evaluated the framework using PyBullet, creating scenarios with three different room layouts and three distinct operational tasks. We tested various LLM models and conducted ablation studies to assess the contributions of different modules. The experimental results confirm the effectiveness and necessity of our proposed framework.

**Link**: [arxiv](http://arxiv.org/abs/2409.16030v1),  [pdf](http://arxiv.org/pdf/2409.16030v1)

**Tags**: cs.RO 



### Unlocking Markets: A Multilingual Benchmark to Cross-Market Question   Answering
**Authors**: Yifei Yuan, Yang Deng, Anders S√∏gaard, Mohammad Aliannejadi

**Updated**: 2024-09-24T12:24:34Z

**Summary**: Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks.

**Link**: [arxiv](http://arxiv.org/abs/2409.16025v1),  [pdf](http://arxiv.org/pdf/2409.16025v1)

**Tags**: cs.CL 



### AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming   in LLM-Based Batch Relevance Assessment
**Authors**: Nuo Chen, Jiqun Liu, Xiaoyu Dong, Qijiong Liu, Tetsuya Sakai, Xiao-Ming Wu

**Updated**: 2024-09-24T12:23:15Z

**Summary**: Cognitive biases are systematic deviations in thinking that lead to irrational judgments and problematic decision-making, extensively studied across various fields. Recently, large language models (LLMs) have shown advanced understanding capabilities but may inherit human biases from their training data. While social biases in LLMs have been well-studied, cognitive biases have received less attention, with existing research focusing on specific scenarios. The broader impact of cognitive biases on LLMs in various decision-making contexts remains underexplored. We investigated whether LLMs are influenced by the threshold priming effect in relevance judgments, a core task and widely-discussed research topic in the Information Retrieval (IR) coummunity. The priming effect occurs when exposure to certain stimuli unconsciously affects subsequent behavior and decisions. Our experiment employed 10 topics from the TREC 2019 Deep Learning passage track collection, and tested AI judgments under different document relevance scores, batch lengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B. Results showed that LLMs tend to give lower scores to later documents if earlier ones have high relevance, and vice versa, regardless of the combination and model used. Our finding demonstrates that LLM%u2019s judgments, similar to human judgments, are also influenced by threshold priming biases, and suggests that researchers and system engineers should take into account potential human-like cognitive biases in designing, evaluating, and auditing LLMs in IR tasks and beyond.

**Link**: [arxiv](http://arxiv.org/abs/2409.16022v1),  [pdf](http://arxiv.org/pdf/2409.16022v1)

**Tags**: cs.CL cs.AI 



### VascX Models: Model Ensembles for Retinal Vascular Analysis from Color   Fundus Images
**Authors**: Jose Vargas Quiros, Bart Liefers, Karin van Garderen, Jeroen Vermeulen, Eyened Reading Center, Sinergia Consortium, Caroline Klaver

**Updated**: 2024-09-24T12:19:31Z

**Summary**: We introduce VascX models, a comprehensive set of model ensembles for analyzing retinal vasculature from color fundus images (CFIs). Annotated CFIs were aggregated from public datasets for vessel, artery-vein, and disc segmentation; and fovea localization. Additional CFIs from the population-based Rotterdam Study were, with arteries and veins annotated by graders at pixel level. Our models achieved robust performance across devices from different vendors, varying levels of image quality levels, and diverse pathologies. Our models demonstrated superior segmentation performance compared to existing systems under a variety of conditions. Significant enhancements were observed in artery-vein and disc segmentation performance, particularly in segmentations of these structures on CFIs of intermediate quality, a common characteristic of large cohorts and clinical datasets. Our model outperformed human graders in segmenting vessels with greater precision. With VascX models we provide a robust, ready-to-use set of model ensembles and inference code aimed at simplifying the implementation and enhancing the quality of automated retinal vasculature analyses. The precise vessel parameters generated by the model can serve as starting points for the identification of disease patterns in and outside of the eye.

**Link**: [arxiv](http://arxiv.org/abs/2409.16016v1),  [pdf](http://arxiv.org/pdf/2409.16016v1)

**Tags**: eess.IV cs.CV q-bio.TO 



### Halfway Escape Optimization: A Quantum-Inspired Solution for General   Optimization Problems
**Authors**: Jiawen Li, Anwar PP Abdul Majeed, Pascal Lefevre

**Updated**: 2024-09-24T12:11:50Z

**Summary**: This paper first proposes the Halfway Escape Optimization (HEO) algorithm, a quantum-inspired metaheuristic designed to address general optimization problems. The HEO mimics the effects between quantum such as tunneling, entanglement. After the introduction to the HEO mechansims, the study presents a comprehensive evaluation of HEO's performance against extensively-used optimization algorithms, including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Fish Swarm Algorithm (AFSA), Grey Wolf Optimizer (GWO), and Quantum behaved Particle Swarm Optimization (QPSO). The primary analysis encompasses 14 benchmark functions with dimension 30, demonstrating HEO's effectiveness and adaptability in navigating general optimization problems. The test of HEO in Pressure Vessel Design and Tubular Column Design also infers its feasibility and potential in real-time applications. Further validation of HEO in Osmancik-97 and Cammeo Rice Classification achieves a higher accuracy record.

**Link**: [arxiv](http://arxiv.org/abs/2405.02850v7),  [pdf](http://arxiv.org/pdf/2405.02850v7)

**Tags**: cs.NE cs.AI math.OC 



### CrowdSurfer: Sampling Optimization Augmented with Vector-Quantized   Variational AutoEncoder for Dense Crowd Navigation
**Authors**: Naman Kumar, Antareep Singha, Laksh Nanwani, Dhruv Potdar, Tarun R, Fatemeh Rastgar, Simon Idoko, Arun Kumar Singh, K. Madhava Krishna

**Updated**: 2024-09-24T12:11:43Z

**Summary**: Navigation amongst densely packed crowds remains a challenge for mobile robots. The complexity increases further if the environment layout changes, making the prior computed global plan infeasible. In this paper, we show that it is possible to dramatically enhance crowd navigation by just improving the local planner. Our approach combines generative modelling with inference time optimization to generate sophisticated long-horizon local plans at interactive rates. More specifically, we train a Vector Quantized Variational AutoEncoder to learn a prior over the expert trajectory distribution conditioned on the perception input. At run-time, this is used as an initialization for a sampling-based optimizer for further refinement. Our approach does not require any sophisticated prediction of dynamic obstacles and yet provides state-of-the-art performance. In particular, we compare against the recent DRL-VO approach and show a 40% improvement in success rate and a 6% improvement in travel time.

**Link**: [arxiv](http://arxiv.org/abs/2409.16011v1),  [pdf](http://arxiv.org/pdf/2409.16011v1)

**Tags**: cs.RO math.OC 



### Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character   Pre-training in LLMs
**Authors**: Yang Yuhang, Peng Yizhou, Eng Siong Chng, Xionghu Zhong

**Updated**: 2024-09-24T12:06:31Z

**Summary**: The integration of large language models (LLMs) with pre-trained speech models has opened up new avenues in automatic speech recognition (ASR). While LLMs excel in multimodal understanding tasks, effectively leveraging their capabilities for ASR remains a significant challenge. This paper presents a novel training approach to enhance LLM performance in ASR tasks. We propose pre-training LLMs on Pinyin embedding sequences, which represent pronunciation features, to generate corresponding Chinese characters. This step enables the LLM to adapt to generating text from pronunciation features before encountering real speech data. Furthermore, we fine-tune the LoRA parameters to enhance the LLM's understanding of speech modality information. In AISHELL-1 corpus, our approach yields a 9.5% relative improvement in ASR tasks compared to the baseline without Pinyi-to-Character pre-training. Additionally, incorporating auxiliary text data for Pinyi-to-Character pre-training further boosts performance, achieving a 19.0% relative improvement.

**Link**: [arxiv](http://arxiv.org/abs/2409.16005v1),  [pdf](http://arxiv.org/pdf/2409.16005v1)

**Tags**: cs.CL cs.SD eess.AS 



### Bayesian Federated Inference for regression models based on non-shared   multicenter data sets from heterogeneous populations
**Authors**: Marianne A Jonker, Hassan Pazira, Anthony CC Coolen

**Updated**: 2024-09-24T12:04:40Z

**Summary**: To estimate accurately the parameters of a regression model, the sample size must be large enough relative to the number of possible predictors for the model. In practice, sufficient data is often lacking, which can lead to overfitting of the model and, as a consequence, unreliable predictions of the outcome of new patients. Pooling data from different data sets collected in different (medical) centers would alleviate this problem, but is often not feasible due to privacy regulation or logistic problems. An alternative route would be to analyze the local data in the centers separately and combine the statistical inference results with the Bayesian Federated Inference (BFI) methodology. The aim of this approach is to compute from the inference results in separate centers what would have been found if the statistical analysis was performed on the combined data. We explain the methodology under homogeneity and heterogeneity across the populations in the separate centers, and give real life examples for better understanding. Excellent performance of the proposed methodology is shown. An R-package to do all the calculations has been developed and is illustrated in this paper. The mathematical details are given in the Appendix.

**Link**: [arxiv](http://arxiv.org/abs/2402.02898v2),  [pdf](http://arxiv.org/pdf/2402.02898v2)

**Tags**: stat.AP stat.CO stat.ME stat.ML 



### Large Language Models as Carriers of Hidden Messages
**Authors**: Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki

**Updated**: 2024-09-24T12:00:29Z

**Summary**: Simple fine-tuning can embed hidden text into large language models (LLMs), which is revealed only when triggered by a specific query. Applications include LLM fingerprinting, where a unique identifier is embedded to verify licensing compliance, and steganography, where the LLM carries hidden messages disclosed through a trigger query.   Our work demonstrates that embedding hidden text via fine-tuning, although seemingly secure due to the vast number of potential triggers, is vulnerable to extraction through analysis of the LLM's output decoding process. We introduce an extraction attack called Unconditional Token Forcing (UTF), which iteratively feeds tokens from the LLM's vocabulary to reveal sequences with high token probabilities, indicating hidden text candidates. We also present Unconditional Token Forcing Confusion (UTFC), a defense paradigm that makes hidden text resistant to all known extraction attacks without degrading the general performance of LLMs compared to standard fine-tuning. UTFC has both benign (improving LLM fingerprinting) and malign applications (using LLMs to create covert communication channels).

**Link**: [arxiv](http://arxiv.org/abs/2406.02481v4),  [pdf](http://arxiv.org/pdf/2406.02481v4)

**Tags**: cs.CL cs.CR 



### Robust Inference for Non-Linear Regression Models with Applications in   Enzyme Kinetics
**Authors**: Suryasis Jana, Abhik Ghosh

**Updated**: 2024-09-24T11:52:09Z

**Summary**: Despite linear regression being the most popular statistical modelling technique, in real-life we often need to deal with situations where the true relationship between the response and the covariates is nonlinear in parameters. In such cases one needs to adopt appropriate non-linear regression (NLR) analysis, having wider applications in biochemical and medical studies among many others. In this paper we propose a new improved robust estimation and testing methodologies for general NLR models based on the minimum density power divergence approach and apply our proposal to analyze the widely popular Michaelis-Menten (MM) model in enzyme kinetics. We establish the asymptotic properties of our proposed estimator and tests, along with their theoretical robustness characteristics through influence function analysis. For the particular MM model, we have further empirically justified the robustness and the efficiency of our proposed estimator and the testing procedure through extensive simulation studies and several interesting real data examples of enzyme-catalyzed (biochemical) reactions.

**Link**: [arxiv](http://arxiv.org/abs/2409.15995v1),  [pdf](http://arxiv.org/pdf/2409.15995v1)

**Tags**: stat.ME q-bio.QM stat.AP 



### Low-Energy On-Device Personalization for MCUs
**Authors**: Yushan Huang, Ranya Aloufi, Xavier Cadet, Yuchen Zhao, Payam Barnaghi, Hamed Haddadi

**Updated**: 2024-09-24T11:46:09Z

**Summary**: Microcontroller Units (MCUs) are ideal platforms for edge applications due to their low cost and energy consumption, and are widely used in various applications, including personalized machine learning tasks, where customized models can enhance the task adaptation. However, existing approaches for local on-device personalization mostly support simple ML architectures or require complex local pre-training/training, leading to high energy consumption and negating the low-energy advantage of MCUs.   In this paper, we introduce $MicroT$, an efficient and low-energy MCU personalization approach. $MicroT$ includes a robust, general, but tiny feature extractor, developed through self-supervised knowledge distillation, which trains a task-specific head to enable independent on-device personalization with minimal energy and computational requirements. MicroT implements an MCU-optimized early-exit inference mechanism called stage-decision to further reduce energy costs. This mechanism allows for user-configurable exit criteria (stage-decision ratio) to adaptively balance energy cost with model performance. We evaluated MicroT using two models, three datasets, and two MCU boards. $MicroT$ outperforms traditional transfer learning (TTL) and two SOTA approaches by 2.12 - 11.60% across two models and three datasets. Targeting widely used energy-aware edge devices, MicroT's on-device training requires no additional complex operations, halving the energy cost compared to SOTA approaches by up to 2.28$\times$ while keeping SRAM usage below 1MB. During local inference, MicroT reduces energy cost by 14.17% compared to TTL across two boards and two datasets, highlighting its suitability for long-term use on energy-aware resource-constrained MCUs.

**Link**: [arxiv](http://arxiv.org/abs/2403.08040v3),  [pdf](http://arxiv.org/pdf/2403.08040v3)

**Tags**: cs.LG cs.AR 



### GPT-4 as a Homework Tutor can Improve Student Engagement and Learning   Outcomes
**Authors**: Alessandro Vanzo, Sankalan Pal Chowdhury, Mrinmaya Sachan

**Updated**: 2024-09-24T11:22:55Z

**Summary**: This work contributes to the scarce empirical literature on LLM-based interactive homework in real-world educational settings and offers a practical, scalable solution for improving homework in schools. Homework is an important part of education in schools across the world, but in order to maximize benefit, it needs to be accompanied with feedback and followup questions. We developed a prompting strategy that enables GPT-4 to conduct interactive homework sessions for high-school students learning English as a second language. Our strategy requires minimal efforts in content preparation, one of the key challenges of alternatives like home tutors or ITSs. We carried out a Randomized Controlled Trial (RCT) in four high-school classes, replacing traditional homework with GPT-4 homework sessions for the treatment group. We observed significant improvements in learning outcomes, specifically a greater gain in grammar, and student engagement. In addition, students reported high levels of satisfaction with the system and wanted to continue using it after the end of the RCT.

**Link**: [arxiv](http://arxiv.org/abs/2409.15981v1),  [pdf](http://arxiv.org/pdf/2409.15981v1)

**Tags**: cs.CY 



### Leveraging Unsupervised Learning for Cost-Effective Visual Anomaly   Detection
**Authors**: Yunbo Long, Zhengyang Ling, Sam Brook, Duncan McFarlane, Alexandra Brintrup

**Updated**: 2024-09-24T11:22:24Z

**Summary**: Traditional machine learning-based visual inspection systems require extensive data collection and repetitive model training to improve accuracy. These systems typically require expensive camera, computing equipment and significant machine learning expertise, which can substantially burden small and medium-sized enterprises. This study explores leveraging unsupervised learning methods with pre-trained models and low-cost hardware to create a cost-effective visual anomaly detection system. The research aims to develop a low-cost visual anomaly detection solution that uses minimal data for model training while maintaining generalizability and scalability. The system utilises unsupervised learning models from Anomalib and is deployed on affordable Raspberry Pi hardware through openVINO. The results show that this cost-effective system can complete anomaly defection training and inference on a Raspberry Pi in just 90 seconds using only 10 normal product images, achieving an F1 macro score exceeding 0.95. While the system is slightly sensitive to environmental changes like lighting, product positioning, or background, it remains a swift and economical method for factory automation inspection for small and medium-sized manufacturers

**Link**: [arxiv](http://arxiv.org/abs/2409.15980v1),  [pdf](http://arxiv.org/pdf/2409.15980v1)

**Tags**: cs.CV cs.AI 



### tinyCLAP: Distilling Constrastive Language-Audio Pretrained Models
**Authors**: Francesco Paissan, Elisabetta Farella

**Updated**: 2024-09-24T11:22:04Z

**Summary**: Contrastive Language-Audio Pretraining (CLAP) became of crucial importance in the field of audio and speech processing. Its employment ranges from sound event detection to text-to-audio generation. However, one of the main limitations is the considerable amount of data required in the training process and the overall computational complexity during inference. This paper investigates how we can reduce the complexity of contrastive language-audio pre-trained models, yielding an efficient model that we call tinyCLAP. We derive an unimodal distillation loss from first principles and explore how the dimensionality of the shared, multimodal latent space can be reduced via pruning. TinyCLAP uses only 6% of the original Microsoft CLAP parameters with a minimal reduction (less than 5%) in zero-shot classification performance across the three sound event detection datasets on which it was tested

**Link**: [arxiv](http://arxiv.org/abs/2311.14517v3),  [pdf](http://arxiv.org/pdf/2311.14517v3)

**Tags**: cs.SD cs.CL cs.LG eess.AS 



### Finetuning LLMs for Comparative Assessment Tasks
**Authors**: Vatsal Raina, Adian Liusie, Mark Gales

**Updated**: 2024-09-24T11:21:43Z

**Summary**: Automated assessment in natural language generation is a challenging task. Instruction-tuned large language models (LLMs) have shown promise in reference-free evaluation, particularly through comparative assessment. However, the quadratic computational complexity of pairwise comparisons limits its scalability. To address this, efficient comparative assessment has been explored by applying comparative strategies on zero-shot LLM probabilities. We propose a framework for finetuning LLMs for comparative assessment to align the model's output with the target distribution of comparative probabilities. By training on soft probabilities, our approach improves state-of-the-art performance while maintaining high performance with an efficient subset of comparisons.

**Link**: [arxiv](http://arxiv.org/abs/2409.15979v1),  [pdf](http://arxiv.org/pdf/2409.15979v1)

**Tags**: cs.CL 



### Edge-device Collaborative Computing for Multi-view Classification
**Authors**: Marco Palena, Tania Cerquitelli, Carla Fabiana Chiasserini

**Updated**: 2024-09-24T11:07:33Z

**Summary**: Motivated by the proliferation of Internet-of-Thing (IoT) devices and the rapid advances in the field of deep learning, there is a growing interest in pushing deep learning computations, conventionally handled by the cloud, to the edge of the network to deliver faster responses to end users, reduce bandwidth consumption to the cloud, and address privacy concerns. However, to fully realize deep learning at the edge, two main challenges still need to be addressed: (i) how to meet the high resource requirements of deep learning on resource-constrained devices, and (ii) how to leverage the availability of multiple streams of spatially correlated data, to increase the effectiveness of deep learning and improve application-level performance. To address the above challenges, we explore collaborative inference at the edge, in which edge nodes and end devices share correlated data and the inference computational burden by leveraging different ways to split computation and fuse data. Besides traditional centralized and distributed schemes for edge-end device collaborative inference, we introduce selective schemes that decrease bandwidth resource consumption by effectively reducing data redundancy. As a reference scenario, we focus on multi-view classification in a networked system in which sensing nodes can capture overlapping fields of view. The proposed schemes are compared in terms of accuracy, computational expenditure at the nodes, communication overhead, inference latency, robustness, and noise sensitivity. Experimental results highlight that selective collaborative schemes can achieve different trade-offs between the above performance metrics, with some of them bringing substantial communication savings (from 18% to 74% of the transmitted data with respect to centralized inference) while still keeping the inference accuracy well above 90%.

**Link**: [arxiv](http://arxiv.org/abs/2409.15973v1),  [pdf](http://arxiv.org/pdf/2409.15973v1)

**Tags**: cs.LG cs.AI cs.DC cs.NI 



### "Flipped" University: LLM-Assisted Lifelong Learning Environment
**Authors**: Kirill Krinkin, Tatiana Berlenko

**Updated**: 2024-09-24T11:00:38Z

**Summary**: The rapid development of artificial intelligence technologies, particularly Large Language Models (LLMs), has revolutionized the landscape of lifelong learning. This paper introduces a conceptual framework for a self-constructed lifelong learning environment supported by LLMs. It highlights the inadequacies of traditional education systems in keeping pace with the rapid deactualization of knowledge and skills. The proposed framework emphasizes the transformation from institutionalized education to personalized, self-driven learning. It leverages the natural language capabilities of LLMs to provide dynamic and adaptive learning experiences, facilitating the creation of personal intellectual agents that assist in knowledge acquisition. The framework integrates principles of lifelong learning, including the necessity of building personal world models, the dual modes of learning (training and exploration), and the creation of reusable learning artifacts. Additionally, it underscores the importance of curiosity-driven learning and reflective practices in maintaining an effective learning trajectory. The paper envisions the evolution of educational institutions into "flipped" universities, focusing on supporting global knowledge consistency rather than merely structuring and transmitting knowledge.

**Link**: [arxiv](http://arxiv.org/abs/2409.10553v2),  [pdf](http://arxiv.org/pdf/2409.10553v2)

**Tags**: cs.CY 



### TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for   Robotic Manipulation
**Authors**: Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, Jian Tang

**Updated**: 2024-09-24T10:57:18Z

**Summary**: Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2409.12514v2),  [pdf](http://arxiv.org/pdf/2409.12514v2)

**Tags**: cs.RO cs.CV 



### A sparsified Christoffel function for high-dimensional inference
**Authors**: Jean-Bernard Lasserre, Lucas Slot

**Updated**: 2024-09-24T10:51:15Z

**Summary**: Christoffel polynomials are classical tools from approximation theory. They can be used to estimate the (compact) support of a measure $\mu$ on $\mathbb{R}^d$ based on its low-degree moments. Recently, they have been applied to problems in data science, including outlier detection and support inference. A major downside of Christoffel polynomials in such applications is the fact that, in order to compute their coefficients, one must invert a matrix whose size grows rapidly with the dimension $d$. In this paper, we propose a modification of the Christoffel polynomial which is significantly cheaper to compute, but retains many of its desirable properties. Our approach relies on sparsity of the underlying measure $\mu$, described by a graphical model. The complexity of our modification depends on the treewidth of this model.

**Link**: [arxiv](http://arxiv.org/abs/2409.15965v1),  [pdf](http://arxiv.org/pdf/2409.15965v1)

**Tags**: math.ST math.OC stat.TH 



### Provably Efficient Exploration in Inverse Constrained Reinforcement   Learning
**Authors**: Bo Yue, Jian Li, Guiliang Liu

**Updated**: 2024-09-24T10:48:13Z

**Summary**: To obtain the optimal constraints in complex environments, Inverse Constrained Reinforcement Learning (ICRL) seeks to recover these constraints from expert demonstrations in a data-driven manner. Existing ICRL algorithms collect training samples from an interactive environment. However, the efficacy and efficiency of these sampling strategies remain unknown. To bridge this gap, we introduce a strategic exploration framework with provable efficiency. Specifically, we define a feasible constraint set for ICRL problems and investigate how expert policy and environmental dynamics influence the optimality of constraints. Motivated by our findings, we propose two exploratory algorithms to achieve efficient constraint inference via 1) dynamically reducing the bounded aggregate error of cost estimation and 2) strategically constraining the exploration policy. Both algorithms are theoretically grounded with tractable sample complexity. We empirically demonstrate the performance of our algorithms under various environments.

**Link**: [arxiv](http://arxiv.org/abs/2409.15963v1),  [pdf](http://arxiv.org/pdf/2409.15963v1)

**Tags**: cs.LG cs.AI 



### ASD-Diffusion: Anomalous Sound Detection with Diffusion Models
**Authors**: Fengrun Zhang, Xiang Xie, Kai Guo

**Updated**: 2024-09-24T10:42:23Z

**Summary**: Unsupervised Anomalous Sound Detection (ASD) aims to design a generalizable method that can be used to detect anomalies when only normal sounds are given. In this paper, Anomalous Sound Detection based on Diffusion Models (ASD-Diffusion) is proposed for ASD in real-world factories. In our pipeline, the anomalies in acoustic features are reconstructed from their noisy corrupted features into their approximate normal pattern. Secondly, a post-processing anomalies filter algorithm is proposed to detect anomalies that exhibit significant deviation from the original input after reconstruction. Furthermore, denoising diffusion implicit model is introduced to accelerate the inference speed by a longer sampling interval of the denoising process. The proposed method is innovative in the application of diffusion models as a new scheme. Experimental results on the development set of DCASE 2023 challenge task 2 outperform the baseline by 7.75%, demonstrating the effectiveness of the proposed method.

**Link**: [arxiv](http://arxiv.org/abs/2409.15957v1),  [pdf](http://arxiv.org/pdf/2409.15957v1)

**Tags**: cs.SD cs.AI eess.AS 



### Watch Every Step! LLM Agent Learning via Iterative Step-Level Process   Refinement
**Authors**: Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li

**Updated**: 2024-09-24T10:01:31Z

**Summary**: Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.

**Link**: [arxiv](http://arxiv.org/abs/2406.11176v2),  [pdf](http://arxiv.org/pdf/2406.11176v2)

**Tags**: cs.CL cs.AI cs.LG 



### Automated test generation to evaluate tool-augmented LLMs as   conversational AI agents
**Authors**: Samuel Arcadinho, David Aparicio, Mariana Almeida

**Updated**: 2024-09-24T09:57:43Z

**Summary**: Tool-augmented LLMs are a promising approach to create AI agents that can have realistic conversations, follow procedures, and call appropriate functions. However, evaluating them is challenging due to the diversity of possible conversations, and existing datasets focus only on single interactions and function-calling. We present a test generation pipeline to evaluate LLMs as conversational AI agents. Our framework uses LLMs to generate diverse tests grounded on user-defined procedures. For that, we use intermediate graphs to limit the LLM test generator's tendency to hallucinate content that is not grounded on input procedures, and enforces high coverage of the possible conversations. Additionally, we put forward ALMITA, a manually curated dataset for evaluating AI agents in customer support, and use it to evaluate existing LLMs. Our results show that while tool-augmented LLMs perform well in single interactions, they often struggle to handle complete conversations. While our focus is on customer support, our method is general and capable of AI agents for different domains.

**Link**: [arxiv](http://arxiv.org/abs/2409.15934v1),  [pdf](http://arxiv.org/pdf/2409.15934v1)

**Tags**: cs.CL cs.AI cs.LG 



### SLIMER-IT: Zero-Shot NER on Italian Language
**Authors**: Andrew Zamai, Leonardo Rigutini, Marco Maggini, Andrea Zugarini

**Updated**: 2024-09-24T09:57:25Z

**Summary**: Traditional approaches to Named Entity Recognition (NER) frame the task into a BIO sequence labeling problem. Although these systems often excel in the downstream task at hand, they require extensive annotated data and struggle to generalize to out-of-distribution input domains and unseen entity types. On the contrary, Large Language Models (LLMs) have demonstrated strong zero-shot capabilities. While several works address Zero-Shot NER in English, little has been done in other languages. In this paper, we define an evaluation framework for Zero-Shot NER, applying it to the Italian language. Furthermore, we introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning approach for zero-shot NER leveraging prompts enriched with definition and guidelines. Comparisons with other state-of-the-art models, demonstrate the superiority of SLIMER-IT on never-seen-before entity tags.

**Link**: [arxiv](http://arxiv.org/abs/2409.15933v1),  [pdf](http://arxiv.org/pdf/2409.15933v1)

**Tags**: cs.CL cs.IR 



### Dynamic Gated Recurrent Neural Network for Compute-efficient Speech   Enhancement
**Authors**: Longbiao Cheng, Ashutosh Pandey, Buye Xu, Tobi Delbruck, Shih-Chii Liu

**Updated**: 2024-09-24T09:55:47Z

**Summary**: This paper introduces a new Dynamic Gated Recurrent Neural Network (DG-RNN) for compute-efficient speech enhancement models running on resource-constrained hardware platforms. It leverages the slow evolution characteristic of RNN hidden states over steps, and updates only a selected set of neurons at each step by adding a newly proposed select gate to the RNN model. This select gate allows the computation cost of the conventional RNN to be reduced during network inference. As a realization of the DG-RNN, we further propose the Dynamic Gated Recurrent Unit (D-GRU) which does not require additional parameters. Test results obtained from several state-of-the-art compute-efficient RNN-based speech enhancement architectures using the DNS challenge dataset, show that the D-GRU based model variants maintain similar speech intelligibility and quality metrics comparable to the baseline GRU based models even with an average 50% reduction in GRU computes.

**Link**: [arxiv](http://arxiv.org/abs/2408.12425v2),  [pdf](http://arxiv.org/pdf/2408.12425v2)

**Tags**: eess.AS cs.LG cs.SD 



### GPTCast: a weather language model for precipitation nowcasting
**Authors**: Gabriele Franch, Elena Tomasi, Rishabh Wanjari, Virginia Poli, Chiara Cardinali, Pier Paolo Alberoni, Marco Cristoforetti

**Updated**: 2024-09-24T09:50:58Z

**Summary**: This work introduces GPTCast, a generative deep-learning method for ensemble nowcast of radar-based precipitation, inspired by advancements in large language models (LLMs). We employ a GPT model as a forecaster to learn spatiotemporal precipitation dynamics using tokenized radar images. The tokenizer is based on a Quantized Variational Autoencoder featuring a novel reconstruction loss tailored for the skewed distribution of precipitation that promotes faithful reconstruction of high rainfall rates. The approach produces realistic ensemble forecasts and provides probabilistic outputs with accurate uncertainty estimation. The model is trained without resorting to randomness, all variability is learned solely from the data and exposed by model at inference for ensemble generation. We train and test GPTCast using a 6-year radar dataset over the Emilia-Romagna region in Northern Italy, showing superior results compared to state-of-the-art ensemble extrapolation methods.

**Link**: [arxiv](http://arxiv.org/abs/2407.02089v2),  [pdf](http://arxiv.org/pdf/2407.02089v2)

**Tags**: cs.LG physics.ao-ph 



### Knowledge Editing in Language Models via Adapted Direct Preference   Optimization
**Authors**: Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum

**Updated**: 2024-09-24T09:48:36Z

**Summary**: Large Language Models (LLMs) can become outdated over time as they may lack updated world knowledge, leading to factual knowledge errors and gaps. Knowledge Editing (KE) aims to overcome this challenge using weight updates that do not require expensive retraining. We propose treating KE as an LLM alignment problem. Toward this goal, we introduce Knowledge Direct Preference Optimization (KDPO), a variation of the Direct Preference Optimization (DPO) that is more effective for knowledge modifications. Our method is based on an online approach that continually updates the knowledge stored in the model. We use the current knowledge as a negative sample and the new knowledge we want to introduce as a positive sample in a process called DPO. We also use teacher-forcing for negative sample generation and optimize using the positive sample, which helps maintain localized changes. We tested our KE method on various datasets and models, comparing it to several cutting-edge methods, with 100 and 500 sequential edits. Additionally, we conducted an ablation study comparing our method to the standard DPO approach. Our experimental results show that our modified DPO method allows for more refined KE, achieving similar or better performance compared to previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2406.09920v2),  [pdf](http://arxiv.org/pdf/2406.09920v2)

**Tags**: cs.CL cs.AI 



### Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts
**Authors**: Sukai Huang, Nir Lipovetzky, Trevor Cohn

**Updated**: 2024-09-24T09:33:12Z

**Summary**: Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.

**Link**: [arxiv](http://arxiv.org/abs/2409.15915v1),  [pdf](http://arxiv.org/pdf/2409.15915v1)

**Tags**: cs.AI 



### Enhancing Text-to-SQL Capabilities of Large Language Models via Domain   Database Knowledge Injection
**Authors**: Xingyu Ma, Xin Tian, Lingxiang Wu, Xuepeng Wang, Xueming Tang, Jinqiao Wang

**Updated**: 2024-09-24T09:24:03Z

**Summary**: Text-to-SQL is a subtask in semantic parsing that has seen rapid progress with the evolution of Large Language Models (LLMs). However, LLMs face challenges due to hallucination issues and a lack of domain-specific database knowledge(such as table schema and cell values). As a result, they can make errors in generating table names, columns, and matching values to the correct columns in SQL statements. This paper introduces a method of knowledge injection to enhance LLMs' ability to understand schema contents by incorporating prior knowledge. This approach improves their performance in Text-to-SQL tasks. Experimental results show that pre-training LLMs on domain-specific database knowledge and fine-tuning them on downstream Text-to-SQL tasks significantly improves the Execution Match (EX) and Exact Match (EM) metrics across various models. This effectively reduces errors in generating column names and matching values to the columns. Furthermore, the knowledge-injected models can be applied to many downstream Text-to-SQL tasks, demonstrating the generalizability of the approach presented in this paper.

**Link**: [arxiv](http://arxiv.org/abs/2409.15907v1),  [pdf](http://arxiv.org/pdf/2409.15907v1)

**Tags**: cs.CL cs.AI 



### Boosting Code-Switching ASR with Mixture of Experts Enhanced   Speech-Conditioned LLM
**Authors**: Fengrun Zhang, Wang Geng, Hukai Huang, Cheng Yi, He Qu

**Updated**: 2024-09-24T09:20:22Z

**Summary**: In this paper, we introduce a speech-conditioned Large Language Model (LLM) integrated with a Mixture of Experts (MoE) based connector to address the challenge of Code-Switching (CS) in Automatic Speech Recognition (ASR). Specifically, we propose an Insertion and Deletion of Interruption Token (IDIT) mechanism for better transfer text generation ability of LLM to speech recognition task. We also present a connecter with MoE architecture that manages multiple languages efficiently. To further enhance the collaboration of multiple experts and leverage the understanding capabilities of LLM, we propose a two-stage progressive training strategy: 1) The connector is unfrozen and trained with language-specialized experts to map speech representations to the text space. 2) The connector and LLM LoRA adaptor are trained with the proposed IDIT mechanism and all experts are activated to learn general representations. Experimental results demonstrate that our method significantly outperforms state-of-the-art models, including end-to-end and large-scale audio-language models.

**Link**: [arxiv](http://arxiv.org/abs/2409.15905v1),  [pdf](http://arxiv.org/pdf/2409.15905v1)

**Tags**: cs.SD cs.AI eess.AS 



### Preference-Guided Refactored Tuning for Retrieval Augmented Code   Generation
**Authors**: Xinyu Gao, Yun Xiong, Deze Wang, Zhenhan Guan, Zejian Shi, Haofen Wang, Shanshan Li

**Updated**: 2024-09-24T09:15:37Z

**Summary**: Retrieval-augmented code generation utilizes Large Language Models as the generator and significantly expands their code generation capabilities by providing relevant code, documentation, and more via the retriever. The current approach suffers from two primary limitations: 1) information redundancy. The indiscriminate inclusion of redundant information can result in resource wastage and may misguide generators, affecting their effectiveness and efficiency. 2) preference gap. Due to different optimization objectives, the retriever strives to procure code with higher ground truth similarity, yet this effort does not substantially benefit the generator. The retriever and the generator may prefer different golden code, and this gap in preference results in a suboptimal design. Additionally, differences in parameterization knowledge acquired during pre-training result in varying preferences among different generators.   To address these limitations, in this paper, we propose RRG (Retrieve, Refactor, Generate), a novel framework for effective and efficient code generation. This framework introduces a code refactorer module between the retriever and the generator to bridge them. The refactoring process transforms the raw retrieved code into a more concise, efficient, and model-friendly version. It eliminates redundant information and noise, reducing the input length. Consequently, the generator receives higher-quality context, enabling it to produce more accurate results with lower inference costs. We conducted comprehensive experiments on multiple datasets. In the experiments, we confirmed the existence of a preference gap between the retriever and the generator, and RRG effectively bridges this gap. Specifically, RRG achieved significant performance improvements, with increases of up to 28% on EM, 13% on BLEU, and 6.8% on CodeBLEU.

**Link**: [arxiv](http://arxiv.org/abs/2409.15895v1),  [pdf](http://arxiv.org/pdf/2409.15895v1)

**Tags**: cs.SE 



### HLB: Benchmarking LLMs' Humanlikeness in Language Use
**Authors**: Xufeng Duan, Bei Xiao, Xuemei Tang, Zhenguang G. Cai

**Updated**: 2024-09-24T09:02:28Z

**Summary**: As synthetic data becomes increasingly prevalent in training language models, particularly through generated dialogue, concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication. This highlights the critical need to assess the humanlikeness of language models in real-world language use. In this paper, we present a comprehensive humanlikeness benchmark (HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic experiments designed to probe core linguistic aspects, including sound, word, syntax, semantics, and discourse (see https://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these comparisons, we collected responses from over 2,000 human participants and compared them to outputs from the LLMs in these experiments.   For rigorous evaluation, we developed a coding algorithm that accurately identified language use patterns, enabling the extraction of response distributions for each task. By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity. Our results reveal fine-grained differences in how well LLMs replicate human responses across various linguistic levels. Importantly, we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline. By introducing psycholinguistic methods to model evaluation, this benchmark offers the first framework for systematically assessing the humanlikeness of LLMs in language use.

**Link**: [arxiv](http://arxiv.org/abs/2409.15890v1),  [pdf](http://arxiv.org/pdf/2409.15890v1)

**Tags**: cs.CL 



### Interpolation filter design for sample rate independent audio effect   RNNs
**Authors**: Alistair Carson, Alec Wright, Stefan Bilbao

**Updated**: 2024-09-24T08:56:25Z

**Summary**: Recurrent neural networks (RNNs) are effective at emulating the non-linear, stateful behavior of analog guitar amplifiers and distortion effects. Unlike the case of direct circuit simulation, RNNs have a fixed sample rate encoded in their model weights, making the sample rate non-adjustable during inference. Recent work has proposed increasing the sample rate of RNNs at inference (oversampling) by increasing the feedback delay length in samples, using a fractional delay filter for non-integer conversions. Here, we investigate the task of lowering the sample rate at inference (undersampling), and propose using an extrapolation filter to approximate the required fractional signal advance. We consider two filter design methods and analyze the impact of filter order on audio quality. Our results show that the correct choice of filter can give high quality results for both oversampling and undersampling; however, in some cases the sample rate adjustment leads to unwanted artefacts in the output signal. We analyse these failure cases through linearised stability analysis, showing that they result from instability around a fixed point. This approach enables an informed prediction of suitable interpolation filters for a given RNN model before runtime.

**Link**: [arxiv](http://arxiv.org/abs/2409.15884v1),  [pdf](http://arxiv.org/pdf/2409.15884v1)

**Tags**: eess.AS cs.SD eess.SP 



### Unsupervised dMRI Artifact Detection via Angular Resolution Enhancement   and Cycle Consistency Learning
**Authors**: Sheng Chen, Zihao Tang, Xinyi Wang, Chenyu Wang, Weidong Cai

**Updated**: 2024-09-24T08:56:10Z

**Summary**: Diffusion magnetic resonance imaging (dMRI) is a crucial technique in neuroimaging studies, allowing for the non-invasive probing of the underlying structures of brain tissues. Clinical dMRI data is susceptible to various artifacts during acquisition, which can lead to unreliable subsequent analyses. Therefore, dMRI preprocessing is essential for improving image quality, and manual inspection is often required to ensure that the preprocessed data is sufficiently corrected. However, manual inspection requires expertise and is time-consuming, especially with large-scale dMRI datasets. Given these challenges, an automated dMRI artifact detection tool is necessary to increase the productivity and reliability of dMRI data analysis. To this end, we propose a novel unsupervised deep learning framework called $\textbf{U}$nsupervised $\textbf{d}$MRI $\textbf{A}$rtifact $\textbf{D}$etection via $\textbf{A}$ngular Resolution Enhancement and $\textbf{C}$ycle Consistency Learning (UdAD-AC). UdAD-AC leverages dMRI angular resolution enhancement and cycle consistency learning to capture the effective representation of artifact-free dMRI data during training, and it identifies data containing artifacts using designed confidence score during inference. To assess the capability of UdAD-AC, several commonly reported dMRI artifacts, including bias field, susceptibility distortion, and corrupted volume, were added to the testing data. Experimental results demonstrate that UdAD-AC achieves the best performance compared to competitive methods in unsupervised dMRI artifact detection.

**Link**: [arxiv](http://arxiv.org/abs/2409.15883v1),  [pdf](http://arxiv.org/pdf/2409.15883v1)

**Tags**: eess.IV cs.CV 



### Automatic Bottom-Up Taxonomy Construction: A Software Application Domain   Study
**Authors**: Cezar Sas, Andrea Capiluppi

**Updated**: 2024-09-24T08:55:07Z

**Summary**: Previous research in software application domain classification has faced challenges due to the lack of a proper taxonomy that explicitly models relations between classes. As a result, current solutions are less effective for real-world usage. This study aims to develop a comprehensive software application domain taxonomy by integrating multiple datasources and leveraging ensemble methods. The goal is to overcome the limitations of individual sources and configurations by creating a more robust, accurate, and reproducible taxonomy. This study employs a quantitative research design involving three different datasources: an existing Computer Science Ontology (CSO), Wikidata, and LLMs. The study utilises a combination of automated and human evaluations to assess the quality of a taxonomy. The outcome measures include the number of unlinked terms, self-loops, and overall connectivity of the taxonomy. The results indicate that individual datasources have advantages and drawbacks: the CSO datasource showed minimal variance across different configurations, but a notable issue of missing technical terms and a high number of self-loops. The Wikipedia datasource required significant filtering during construction to improve metric performance. LLM-generated taxonomies demonstrated better performance when using context-rich prompts. An ensemble approach showed the most promise, successfully reducing the number of unlinked terms and self-loops, thus creating a more connected and comprehensive taxonomy. The study addresses the construction of a software application domain taxonomy relying on pre-existing resources. Our results indicate that an ensemble approach to taxonomy construction can effectively address the limitations of individual datasources. Future work should focus on refining the ensemble techniques and exploring additional datasources to enhance the taxonomy's accuracy and completeness.

**Link**: [arxiv](http://arxiv.org/abs/2409.15881v1),  [pdf](http://arxiv.org/pdf/2409.15881v1)

**Tags**: cs.SE 



### CamelEval: Advancing Culturally Aligned Arabic Language Models and   Benchmarks
**Authors**: Zhaozhi Qian, Faroq Altam, Muhammad Alqurishi, Riad Souissi

**Updated**: 2024-09-24T08:49:21Z

**Summary**: Large Language Models (LLMs) are the cornerstones of modern artificial intelligence systems. This paper introduces Juhaina, a Arabic-English bilingual LLM specifically designed to align with the values and preferences of Arabic speakers. Juhaina inherently supports advanced functionalities such as instruction following, open-ended question answering, information provisioning, and text processing. Our model contains 9.24 billion parameters and is trained on a context window of up to 8,192 tokens. This paper details the creation process of Juhaina and provides an extensive empirical evaluation. Furthermore, we identify the limitations of widely-adopted Open Arabic LLM Leaderboard (OALL) and propose a new evaluation benchmark, CamelEval. Our findings demonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as the Llama and Gemma families, in generating helpful responses in Arabic, providing factually accurate information about the region, and understanding nuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI technologies, serving over 400 million Arabic speakers by offering LLMs that not only communicate in their language but also comprehend their culture. We publicly release all models on Huggingface \url{https://huggingface.co/elmrc}.

**Link**: [arxiv](http://arxiv.org/abs/2409.12623v2),  [pdf](http://arxiv.org/pdf/2409.12623v2)

**Tags**: cs.CL cs.AI 



### Whisper in Medusa's Ear: Multi-head Efficient Decoding for   Transformer-based ASR
**Authors**: Yael Segal-Feldman, Aviv Shamsian, Aviv Navon, Gill Hetz, Joseph Keshet

**Updated**: 2024-09-24T08:42:31Z

**Summary**: Large transformer-based models have significant potential for speech transcription and translation. Their self-attention mechanisms and parallel processing enable them to capture complex patterns and dependencies in audio sequences. However, this potential comes with challenges, as these large and computationally intensive models lead to slow inference speeds. Various optimization strategies have been proposed to improve performance, including efficient hardware utilization and algorithmic enhancements. In this paper, we introduce Whisper-Medusa, a novel approach designed to enhance processing speed with minimal impact on Word Error Rate (WER). The proposed model extends the OpenAI's Whisper architecture by predicting multiple tokens per iteration, resulting in a 50% reduction in latency. We showcase the effectiveness of Whisper-Medusa across different learning setups and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2409.15869v1),  [pdf](http://arxiv.org/pdf/2409.15869v1)

**Tags**: eess.AS cs.AI cs.LG cs.SD 



### Privacy Evaluation Benchmarks for NLP Models
**Authors**: Wei Huang, Yinggui Wang, Cen Chen

**Updated**: 2024-09-25T07:03:05Z

**Summary**: By inducing privacy attacks on NLP models, attackers can obtain sensitive information such as training data and model parameters, etc. Although researchers have studied, in-depth, several kinds of attacks in NLP models, they are non-systematic analyses. It lacks a comprehensive understanding of the impact caused by the attacks. For example, we must consider which scenarios can apply to which attacks, what the common factors are that affect the performance of different attacks, the nature of the relationships between different attacks, and the influence of various datasets and models on the effectiveness of the attacks, etc. Therefore, we need a benchmark to holistically assess the privacy risks faced by NLP models. In this paper, we present a privacy attack and defense evaluation benchmark in the field of NLP, which includes the conventional/small models and large language models (LLMs). This benchmark supports a variety of models, datasets, and protocols, along with standardized modules for comprehensive evaluation of attacks and defense strategies. Based on the above framework, we present a study on the association between auxiliary data from different domains and the strength of privacy attacks. And we provide an improved attack method in this scenario with the help of Knowledge Distillation (KD). Furthermore, we propose a chained framework for privacy attacks. Allowing a practitioner to chain multiple attacks to achieve a higher-level attack objective. Based on this, we provide some defense and enhanced attack strategies. The code for reproducing the results can be found at https://github.com/user2311717757/nlp_doctor.

**Link**: [arxiv](http://arxiv.org/abs/2409.15868v2),  [pdf](http://arxiv.org/pdf/2409.15868v2)

**Tags**: cs.CL cs.LG 



### BeSimulator: A Large Language Model Powered Text-based Behavior   Simulator
**Authors**: Jianan Wang, Bin Li, Xueying Wang, Fu Li, Yunlong Wu, Juan Chen, Xiaodong Yi

**Updated**: 2024-09-24T08:37:04Z

**Summary**: Traditional robot simulators focus on physical process modeling and realistic rendering, often suffering from high computational costs, inefficiencies, and limited adaptability. To handle this issue, we propose Behavior Simulation in robotics to emphasize checking the behavior logic of robots and achieving sufficient alignment between the outcome of robot actions and real scenarios. In this paper, we introduce BeSimulator, a modular and novel LLM-powered framework, as an attempt towards behavior simulation in the context of text-based environments. By constructing text-based virtual environments and performing semantic-level simulation, BeSimulator can generalize across scenarios and achieve long-horizon complex simulation. Inspired by human cognition processes, it employs a "consider-decide-capture-transfer" methodology, termed Chain of Behavior Simulation, which excels at analyzing action feasibility and state transitions. Additionally, BeSimulator incorporates code-driven reasoning to enable arithmetic operations and enhance reliability, as well as integrates reflective feedback to refine simulation. Based on our manually constructed behavior-tree-based simulation benchmark BTSIMBENCH, our experiments show a significant performance improvement in behavior simulation compared to baselines, ranging from 14.7% to 26.6%.

**Link**: [arxiv](http://arxiv.org/abs/2409.15865v1),  [pdf](http://arxiv.org/pdf/2409.15865v1)

**Tags**: cs.RO cs.AI cs.CL 



### A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding
**Authors**: Abdulfattah Safa, G√∂zde G√ºl ≈ûahin

**Updated**: 2024-09-24T08:33:41Z

**Summary**: Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our approach includes reformulating DST as a question-answering task for less capable models and employing self-refining prompts for more adaptable ones. Our system does not rely on fixed slot values defined in the ontology allowing the system to adapt dynamically. We compare our approach with existing SOTA, and show that it provides up to 20% better Joint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1, with up to 90% fewer requests to the LLM API.

**Link**: [arxiv](http://arxiv.org/abs/2409.15861v1),  [pdf](http://arxiv.org/pdf/2409.15861v1)

**Tags**: cs.CL cs.AI 



### Reconstructing the kinetic chemotaxis kernel using macroscopic data:   well-posedness and ill-posedness
**Authors**: Kathrin Hellmuth, Christian Klingenberg, Qin Li, Min Tang

**Updated**: 2024-09-24T08:29:43Z

**Summary**: Bacterial motion is steered by external stimuli (chemotaxis), and the motion described on the mesoscopic scale is uniquely determined by a parameter $K$ that models velocity change response from the bacteria. This parameter is called chemotaxis kernel. In a practical setting, it is inferred by experimental data. We deploy a PDE-constrained optimization framework to perform this reconstruction using velocity-averaged, localized data taken in the interior of the domain. The problem can be well-posed or ill-posed depending on the data preparation and the experimental setup. In particular, we propose one specific design that guarantees numerical reconstructability and local convergence. This design is adapted to the discretization of $K$ in space and decouples the reconstruction of local values of $K$ into smaller cell problems, opening up parallelization opportunities. Numerical evidences support the theoretical findings.

**Link**: [arxiv](http://arxiv.org/abs/2309.05004v4),  [pdf](http://arxiv.org/pdf/2309.05004v4)

**Tags**: math.NA cs.NA math.AP math.OC q-bio.CB 35R30, 65M32, 92C17, 49M41, 49K40 



### KAG: Boosting LLMs in Professional Domains via Knowledge Augmented   Generation
**Authors**: Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou

**Updated**: 2024-09-24T08:24:39Z

**Summary**: The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.13731v2),  [pdf](http://arxiv.org/pdf/2409.13731v2)

**Tags**: cs.CL cs.AI 



### Twin Network Augmentation: A Novel Training Strategy for Improved   Spiking Neural Networks and Efficient Weight Quantization
**Authors**: Lucas Deckers, Benjamin Vandersmissen, Ing Jyh Tsang, Werner Van Leekwijck, Steven Latr√©

**Updated**: 2024-09-24T08:20:56Z

**Summary**: The proliferation of Artificial Neural Networks (ANNs) has led to increased energy consumption, raising concerns about their sustainability. Spiking Neural Networks (SNNs), which are inspired by biological neural systems and operate using sparse, event-driven spikes to communicate information between neurons, offer a potential solution due to their lower energy requirements. An alternative technique for reducing a neural network's footprint is quantization, which compresses weight representations to decrease memory usage and energy consumption. In this study, we present Twin Network Augmentation (TNA), a novel training framework aimed at improving the performance of SNNs while also facilitating an enhanced compression through low-precision quantization of weights. TNA involves co-training an SNN with a twin network, optimizing both networks to minimize their cross-entropy losses and the mean squared error between their output logits. We demonstrate that TNA significantly enhances classification performance across various vision datasets and in addition is particularly effective when applied when reducing SNNs to ternary weight precision. Notably, during inference , only the ternary SNN is retained, significantly reducing the network in number of neurons, connectivity and weight size representation. Our results show that TNA outperforms traditional knowledge distillation methods and achieves state-of-the-art performance for the evaluated network architecture on benchmark datasets, including CIFAR-10, CIFAR-100, and CIFAR-10-DVS. This paper underscores the effectiveness of TNA in bridging the performance gap between SNNs and ANNs and suggests further exploration into the application of TNA in different network architectures and datasets.

**Link**: [arxiv](http://arxiv.org/abs/2409.15849v1),  [pdf](http://arxiv.org/pdf/2409.15849v1)

**Tags**: cs.NE 



### Inferring Scientific Cross-Document Coreference and Hierarchy with   Definition-Augmented Relational Reasoning
**Authors**: Lior Forer, Tom Hope

**Updated**: 2024-09-24T08:18:34Z

**Summary**: We address the fundamental task of inferring cross-document coreference and hierarchy in scientific texts, which has important applications in knowledge graph construction, search, recommendation and discovery. LLMs can struggle when faced with many long-tail technical concepts with nuanced variations. We present a novel method which generates context-dependent definitions of concept mentions by retrieving full-text literature, and uses the definitions to enhance detection of cross-document relations. We further generate relational definitions, which describe how two concept mentions are related or different, and design an efficient re-ranking approach to address the combinatorial explosion involved in inferring links across papers. In both fine-tuning and in-context learning settings we achieve large gains in performance. We provide analysis of generated definitions, shedding light on the relational reasoning ability of LLMs over fine-grained scientific concepts.

**Link**: [arxiv](http://arxiv.org/abs/2409.15113v2),  [pdf](http://arxiv.org/pdf/2409.15113v2)

**Tags**: cs.CL 



### Potential Field as Scene Affordance for Behavior Change-Based Visual   Risk Object Identification
**Authors**: Pang-Yuan Pao, Shu-Wei Lu, Ze-Yan Lu, Yi-Ting Chen

**Updated**: 2024-09-24T08:17:50Z

**Summary**: We study behavior change-based visual risk object identification (Visual-ROI), a critical framework designed to detect potential hazards for intelligent driving systems. Existing methods often show significant limitations in spatial accuracy and temporal consistency, stemming from an incomplete understanding of scene affordance. For example, these methods frequently misidentify vehicles that do not impact the ego vehicle as risk objects. Furthermore, existing behavior change-based methods are inefficient because they implement causal inference in the perspective image space. We propose a new framework with a Bird's Eye View (BEV) representation to overcome the above challenges. Specifically, we utilize potential fields as scene affordance, involving repulsive forces derived from road infrastructure and traffic participants, along with attractive forces sourced from target destinations. In this work, we compute potential fields by assigning different energy levels according to the semantic labels obtained from BEV semantic segmentation. We conduct thorough experiments and ablation studies, comparing the proposed method with various state-of-the-art algorithms on both synthetic and real-world datasets. Our results show a notable increase in spatial and temporal consistency, with enhancements of 20.3% and 11.6% on the RiskBench dataset, respectively. Additionally, we can improve computational efficiency by 88%. We achieve improvements of 5.4% in spatial accuracy and 7.2% in temporal consistency on the nuScenes dataset.

**Link**: [arxiv](http://arxiv.org/abs/2409.15846v1),  [pdf](http://arxiv.org/pdf/2409.15846v1)

**Tags**: cs.CV 



### First Field Trial of LLM-Powered AI Agent for Lifecycle Management of   Autonomous Driving Optical Networks
**Authors**: Xiaomin Liu, Qizhi Qiu, Yihao Zhang, Yuming Cheng, Lilin Yi, Weisheng Hu, Qunbi Zhuge

**Updated**: 2024-09-24T08:02:21Z

**Summary**: We design and demonstrate the first field trial of LLM-powered AI Agent for ADON. Three operation modes of the Agent are proposed for network lifecycle management. The Agent efficiently processes wavelength add/drop and soft/hard failures, and achieves comparable performance to human-designed algorithms for power optimization.

**Link**: [arxiv](http://arxiv.org/abs/2409.14605v2),  [pdf](http://arxiv.org/pdf/2409.14605v2)

**Tags**: eess.SY cs.SY 



### CHECKWHY: Causal Fact Verification via Argument Structure
**Authors**: Jiasheng Si, Yibo Zhao, Yingjie Zhu, Haiyang Zhu, Wenpeng Lu, Deyu Zhou

**Updated**: 2024-09-24T07:44:27Z

**Summary**: With the growing complexity of fact verification tasks, the concern with "thoughtful" reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce CheckWhy, a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. CheckWhy consists of over 19K "why" claim-evidence-argument structure triplets with supports, refutes, and not enough info labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements.

**Link**: [arxiv](http://arxiv.org/abs/2408.10918v2),  [pdf](http://arxiv.org/pdf/2408.10918v2)

**Tags**: cs.CL 



### CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language   Models
**Authors**: Yaojia Lv, Haojie Pan, Zekun Wang, Jiafeng Liang, Yuanxing Liu, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin

**Updated**: 2024-09-24T07:41:19Z

**Summary**: Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows.

**Link**: [arxiv](http://arxiv.org/abs/2401.08438v2),  [pdf](http://arxiv.org/pdf/2401.08438v2)

**Tags**: cs.CL cs.AI cs.LG 



### Unveiling Language Competence Neurons: A Psycholinguistic Approach to   Model Interpretability
**Authors**: Xufeng Duan, Xinyu Zhou, Bei Xiao, Zhenguang G. Cai

**Updated**: 2024-09-24T07:40:33Z

**Summary**: As large language models (LLMs) become advance in their linguistic capacity, understanding how they capture aspects of language competence remains a significant challenge. This study therefore employs psycholinguistic paradigms, which are well-suited for probing deeper cognitive aspects of language processing, to explore neuron-level representations in language model across three tasks: sound-shape association, sound-gender association, and implicit causality. Our findings indicate that while GPT-2-XL struggles with the sound-shape task, it demonstrates human-like abilities in both sound-gender association and implicit causality. Targeted neuron ablation and activation manipulation reveal a crucial relationship: when GPT-2-XL displays a linguistic ability, specific neurons correspond to that competence; conversely, the absence of such an ability indicates a lack of specialized neurons. This study is the first to utilize psycholinguistic experiments to investigate deep language competence at the neuron level, providing a new level of granularity in model interpretability and insights into the internal mechanisms driving language ability in transformer based LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.15827v1),  [pdf](http://arxiv.org/pdf/2409.15827v1)

**Tags**: cs.CL 



## Keyword: LLM Deployment 
 ### Order of Magnitude Speedups for LLM Membership Inference
**Authors**: Rongting Zhang, Martin Bertran, Aaron Roth

**Updated**: 2024-09-24T17:48:58Z

**Summary**: Large Language Models (LLMs) have the promise to revolutionize computing broadly, but their complexity and extensive training data also expose significant privacy vulnerabilities. One of the simplest privacy risks associated with LLMs is their susceptibility to membership inference attacks (MIAs), wherein an adversary aims to determine whether a specific data point was part of the model's training set. Although this is a known risk, state of the art methodologies for MIAs rely on training multiple computationally costly shadow models, making risk evaluation prohibitive for large models. Here we adapt a recent line of work which uses quantile regression to mount membership inference attacks; we extend this work by proposing a low-cost MIA that leverages an ensemble of small quantile regression models to determine if a document belongs to the model's training set or not. We demonstrate the effectiveness of this approach on fine-tuned LLMs of varying families (OPT, Pythia, Llama) and across multiple datasets. Across all scenarios we obtain comparable or improved accuracy compared to state of the art shadow model approaches, with as little as 6% of their computation budget. We demonstrate increased effectiveness across multi-epoch trained target models, and architecture miss-specification robustness, that is, we can mount an effective attack against a model using a different tokenizer and architecture, without requiring knowledge on the target model.

**Link**: [arxiv](http://arxiv.org/abs/2409.14513v2),  [pdf](http://arxiv.org/pdf/2409.14513v2)

**Tags**: cs.LG cs.CR stat.ML 



### AIM 2024 Challenge on UHD Blind Photo Quality Assessment
**Authors**: Vlad Hosu, Marcos V. Conde, Lorenzo Agnolucci, Nabajeet Barman, Saman Zadtootaghaj, Radu Timofte

**Updated**: 2024-09-24T17:44:24Z

**Summary**: We introduce the AIM 2024 UHD-IQA Challenge, a competition to advance the No-Reference Image Quality Assessment (NR-IQA) task for modern, high-resolution photos. The challenge is based on the recently released UHD-IQA Benchmark Database, which comprises 6,073 UHD-1 (4K) images annotated with perceptual quality ratings from expert raters. Unlike previous NR-IQA datasets, UHD-IQA focuses on highly aesthetic photos of superior technical quality, reflecting the ever-increasing standards of digital photography. This challenge aims to develop efficient and effective NR-IQA models. Participants are tasked with creating novel architectures and training strategies to achieve high predictive performance on UHD-1 images within a computational budget of 50G MACs. This enables model deployment on edge devices and scalable processing of extensive image collections. Winners are determined based on a combination of performance metrics, including correlation measures (SRCC, PLCC, KRCC), absolute error metrics (MAE, RMSE), and computational efficiency (G MACs). To excel in this challenge, participants leverage techniques like knowledge distillation, low-precision inference, and multi-scale training. By pushing the boundaries of NR-IQA for high-resolution photos, the UHD-IQA Challenge aims to stimulate the development of practical models that can keep pace with the rapidly evolving landscape of digital photography. The innovative solutions emerging from this competition will have implications for various applications, from photo curation and enhancement to image compression.

**Link**: [arxiv](http://arxiv.org/abs/2409.16271v1),  [pdf](http://arxiv.org/pdf/2409.16271v1)

**Tags**: cs.CV 



### Transport-Level Encryption in Datacenter Networks
**Authors**: Tianyi Gao, Xinshu Ma, Suhas Narreddy, Eugenio Luo, Steven W. D. Chien, Michio Honda

**Updated**: 2024-09-24T17:38:46Z

**Summary**: Cloud applications need network data encryption to isolate from other tenants and protect their data from potential eavesdroppers in the network infrastructure. This paper presents SDT, a protocol design for emerging datacenter transport protocols to integrate data encryption while using existing NIC offloading designed for TLS over TCP. Therefore, SDT could enable a deployment path of new transport protocols in data-centers without giving up hardware offloading.

**Link**: [arxiv](http://arxiv.org/abs/2406.15686v2),  [pdf](http://arxiv.org/pdf/2406.15686v2)

**Tags**: cs.CR cs.NI 



### REBEL: Rule-based and Experience-enhanced Learning with LLMs for Initial   Task Allocation in Multi-Human Multi-Robot Teams
**Authors**: Arjun Gupte, Ruiqi Wang, Vishnunandan L. N. Venkatesh, Taehyeon Kim, Dezhong Zhao, Byung-Cheol Min

**Updated**: 2024-09-24T17:37:54Z

**Summary**: Multi-human multi-robot teams combine the complementary strengths of humans and robots to tackle complex tasks across diverse applications. However, the inherent heterogeneity of these teams presents significant challenges in initial task allocation (ITA), which involves assigning the most suitable tasks to each team member based on their individual capabilities before task execution. While current learning-based methods have shown promising results, they are often computationally expensive to train, and lack the flexibility to incorporate user preferences in multi-objective optimization and adapt to last-minute changes in real-world dynamic environments. To address these issues, we propose REBEL, an LLM-based ITA framework that integrates rule-based and experience-enhanced learning. By leveraging Retrieval-Augmented Generation, REBEL dynamically retrieves relevant rules and past experiences, enhancing reasoning efficiency. Additionally, REBEL can complement pre-trained RL-based ITA policies, improving situational awareness and overall team performance. Extensive experiments validate the effectiveness of our approach across various settings. More details are available at https://sites.google.com/view/ita-rebel .

**Link**: [arxiv](http://arxiv.org/abs/2409.16266v1),  [pdf](http://arxiv.org/pdf/2409.16266v1)

**Tags**: cs.RO 



### Can We Count on LLMs? The Fixed-Effect Fallacy and Claims of GPT-4   Capabilities
**Authors**: Thomas Ball, Shuo Chen, Cormac Herley

**Updated**: 2024-09-24T17:34:07Z

**Summary**: In this paper we explore evaluation of LLM capabilities. We present measurements of GPT-4 performance on several deterministic tasks; each task involves a basic calculation and takes as input parameter some element drawn from a large well-defined population (e.g., count elements in a list, multiply two k-digit numbers, etc). We examine several conditions per-task and perform enough trials so that statistically significant differences can be detected. This allows us to investigate the sensitivity of task-accuracy both to query phrasing and input parameter population. We find that seemingly trivial modifications in the task-prompt or input population can yield differences far larger than can be explained by sampling effects. For example, performance on a simple list-counting task varies with query-phrasing and list-length, but also with list composition (i.e., the thing-to-be-counted) and object frequency (e.g., success when an element accounts for $\approx$ 50\% of a list is different from when it accounts for $\approx$ 70\% etc).   We conclude that efforts to quantify LLM capabilities easily succumb to the language-as-fixed-effect fallacy, where experimental observations are improperly generalized beyond what the data supports. A consequence appears to be that intuitions that have been formed based on interactions with humans form a very unreliable guide as to which input modifications should ``make no difference'' to LLM performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.07638v2),  [pdf](http://arxiv.org/pdf/2409.07638v2)

**Tags**: cs.AI cs.CL cs.LG 



### Deployment of a Transportable Yb Optical Lattice Clock
**Authors**: Tobias Bothwell, Wesley Brand, Robert Fasano, Thomas Akin, Joseph Whalen, Tanner Grogan, Yun-Jhih Chen, Marco Pomponio, Takuma Nakamura, Benjamin Rauf, Ignacio Baldoni, Michele Giunta, Ronald Holzwarth, Craig Nelson, Archita Hati, Franklyn Quinlan, Richard Fox, Steven Peil, Andrew Ludlow

**Updated**: 2024-09-24T17:33:49Z

**Summary**: We report on the first deployment of a ytterbium (Yb) transportable optical lattice clock (TOLC), commercially shipping the clock 3,000 km from Boulder, Colorado to Washington DC. The system, composed of a rigidly mounted optical reference cavity, atomic physics package, and an optical frequency comb, fully realizes an independent frequency standard for comparisons in the optical and microwave domains. The shipped Yb TOLC was fully operational within 2 days of arrival, enabling frequency comparisons with rubidium (Rb) fountains at the United States Naval Observatory (USNO). To the best of our knowledge, this represents the first deployment of a fully independent TOLC, including the frequency comb, coherently uniting the optical stability of the Yb TOLC to the microwave output of the Rb fountain.

**Link**: [arxiv](http://arxiv.org/abs/2409.16264v1),  [pdf](http://arxiv.org/pdf/2409.16264v1)

**Tags**: physics.atom-ph physics.optics 



### A Comprehensive Framework for Evaluating API-oriented Code Generation in   Large Language Models
**Authors**: Yixi Wu, Pengfei He, Zehao Wang, Shaowei Wang, Yuan Tian, Tse-Hsun, Chen

**Updated**: 2024-09-24T17:13:43Z

**Summary**: Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as powerful tools for code generation, significantly enhancing productivity and accelerating software development. However, existing benchmarks primarily focus on general code generation without considering API-oriented code generation, i.e., generating code that invokes APIs from specific libraries. Given the growing demand for API-oriented code generation, there is a pressing need for a systematic and automated approach to evaluate LLM on API-oriented code generation. To address this gap, we propose AutoAPIEval, a lightweight and automated framework designed to evaluate the capabilities of LLMs in API-oriented code generation. Our framework works with any library that provides API documentation and focuses on two unit tasks: API recommendation and code example generation, along with four metrics to evaluate the generated APIs and code examples, such as the proportion of incorrect API recommendations for Task 1, and the proportion of code examples where no specific API is invoked and uncompilable/unexecutable code examples for Task 2. In addition, we conducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder) and Java Runtime Environment 8 to demonstrate the framework's effectiveness. Our findings reveal substantial variability in LLM performance across tasks, with ChatGPT adhering better to instructions, while sharing similar effectiveness in code example generation with its counterparts (i.e., MagiCoder and DeekSeek Coder). We also identify key factors associated with code quality, such as API popularity and model confidence, and build classifiers that achieve high accuracy in detecting incorrect API recommendations and erroneous code examples. Retrieval-augmented generation enhances the quality of code generated by LLMs, though its effectiveness varies across different LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.15228v2),  [pdf](http://arxiv.org/pdf/2409.15228v2)

**Tags**: cs.SE cs.AI cs.LG 



### Cooperative Resilience in Artificial Intelligence Multiagent Systems
**Authors**: Manuela Chacon-Chamorro, Luis Felipe Giraldo, Nicanor Quijano, Vicente Vargas-Panesso, C√©sar Gonz√°lez, Juan Sebasti√°n Pinz√≥n, Rub√©n Manrique, Manuel R√≠os, Yesid Fonseca, Daniel G√≥mez-Barrera, M√≥nica Perdomo-P√©rez

**Updated**: 2024-09-24T17:13:07Z

**Summary**: Resilience refers to the ability of systems to withstand, adapt to, and recover from disruptive events. While studies on resilience have attracted significant attention across various research domains, the precise definition of this concept within the field of cooperative artificial intelligence remains unclear. This paper addresses this gap by proposing a clear definition of `cooperative resilience' and outlining a methodology for its quantitative measurement. The methodology is validated in an environment with RL-based and LLM-augmented autonomous agents, subjected to environmental changes and the introduction of agents with unsustainable behaviors. These events are parameterized to create various scenarios for measuring cooperative resilience. The results highlight the crucial role of resilience metrics in analyzing how the collective system prepares for, resists, recovers from, sustains well-being, and transforms in the face of disruptions. These findings provide foundational insights into the definition, measurement, and preliminary analysis of cooperative resilience, offering significant implications for the broader field of AI. Moreover, the methodology and metrics developed here can be adapted to a wide range of AI applications, enhancing the reliability and effectiveness of AI in dynamic and unpredictable environments.

**Link**: [arxiv](http://arxiv.org/abs/2409.13187v2),  [pdf](http://arxiv.org/pdf/2409.13187v2)

**Tags**: cs.MA cs.AI 



### LLM Echo Chamber: personalized and automated disinformation
**Authors**: Tony Ma

**Updated**: 2024-09-24T17:04:12Z

**Summary**: Recent advancements have showcased the capabilities of Large Language Models like GPT4 and Llama2 in tasks such as summarization, translation, and content review. However, their widespread use raises concerns, particularly around the potential for LLMs to spread persuasive, humanlike misinformation at scale, which could significantly influence public opinion. This study examines these risks, focusing on LLMs ability to propagate misinformation as factual. To investigate this, we built the LLM Echo Chamber, a controlled digital environment simulating social media chatrooms, where misinformation often spreads. Echo chambers, where individuals only interact with like minded people, further entrench beliefs. By studying malicious bots spreading misinformation in this environment, we can better understand this phenomenon. We reviewed current LLMs, explored misinformation risks, and applied sota finetuning techniques. Using Microsoft phi2 model, finetuned with our custom dataset, we generated harmful content to create the Echo Chamber. This setup, evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the ethical concerns surrounding LLMs and emphasizes the need for stronger safeguards against misinformation.

**Link**: [arxiv](http://arxiv.org/abs/2409.16241v1),  [pdf](http://arxiv.org/pdf/2409.16241v1)

**Tags**: cs.AI cs.CY 



### EuroLLM: Multilingual Language Models for Europe
**Authors**: Pedro Henrique Martins, Patrick Fernandes, Jo√£o Alves, Nuno M. Guerreiro, Ricardo Rei, Duarte M. Alves, Jos√© Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, Pierre Colombo, Barry Haddow, Jos√© G. C. de Souza, Alexandra Birch, Andr√© F. T. Martins

**Updated**: 2024-09-24T16:51:36Z

**Summary**: The quality of open-weight LLMs has seen significant improvement, yet they remain predominantly focused on English. In this paper, we introduce the EuroLLM project, aimed at developing a suite of open-weight multilingual LLMs capable of understanding and generating text in all official European Union languages, as well as several additional relevant languages. We outline the progress made to date, detailing our data collection and filtering process, the development of scaling laws, the creation of our multilingual tokenizer, and the data mix and modeling configurations. Additionally, we release our initial models: EuroLLM-1.7B and EuroLLM-1.7B-Instruct and report their performance on multilingual general benchmarks and machine translation.

**Link**: [arxiv](http://arxiv.org/abs/2409.16235v1),  [pdf](http://arxiv.org/pdf/2409.16235v1)

**Tags**: cs.CL 



### DuQuant: Distributing Outliers via Dual Transformation Makes Stronger   Quantized LLMs
**Authors**: Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei

**Updated**: 2024-09-24T16:40:39Z

**Summary**: Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address $\textit{Normal Outliers}$, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing $\textit{Massive Outliers}$ that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing rotation matrices, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.

**Link**: [arxiv](http://arxiv.org/abs/2406.01721v2),  [pdf](http://arxiv.org/pdf/2406.01721v2)

**Tags**: cs.CL 



### Towards Enhancing Linked Data Retrieval in Conversational UIs using   Large Language Models
**Authors**: Omar Mussa, Omer Rana, Beno√Æt Goossens, Pablo Orozco-Terwengel, Charith Perera

**Updated**: 2024-09-24T16:31:33Z

**Summary**: Despite the recent broad adoption of Large Language Models (LLMs) across various domains, their potential for enriching information systems in extracting and exploring Linked Data (LD) and Resource Description Framework (RDF) triplestores has not been extensively explored. This paper examines the integration of LLMs within existing systems, emphasising the enhancement of conversational user interfaces (UIs) and their capabilities for data extraction by producing more accurate SPARQL queries without the requirement for model retraining. Typically, conversational UI models necessitate retraining with the introduction of new datasets or updates, limiting their functionality as general-purpose extraction tools. Our approach addresses this limitation by incorporating LLMs into the conversational UI workflow, significantly enhancing their ability to comprehend and process user queries effectively. By leveraging the advanced natural language understanding capabilities of LLMs, our method improves RDF entity extraction within web systems employing conventional chatbots. This integration facilitates a more nuanced and context-aware interaction model, critical for handling the complex query patterns often encountered in RDF datasets and Linked Open Data (LOD) endpoints. The evaluation of this methodology shows a marked enhancement in system expressivity and the accuracy of responses to user queries, indicating a promising direction for future research in this area. This investigation not only underscores the versatility of LLMs in enhancing existing information systems but also sets the stage for further explorations into their potential applications within more specialised domains of web information systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.16220v1),  [pdf](http://arxiv.org/pdf/2409.16220v1)

**Tags**: cs.IR cs.AI cs.CL 



### Tiny Robotics Dataset and Benchmark for Continual Object Detection
**Authors**: Francesco Pasti, Riccardo De Monte, Davide Dalle Pezze, Gian Antonio Susto, Nicola Bellotto

**Updated**: 2024-09-24T16:21:27Z

**Summary**: Detecting objects in mobile robotics is crucial for numerous applications, from autonomous navigation to inspection. However, robots are often required to perform tasks in different domains with respect to the training one and need to adapt to these changes. Tiny mobile robots, subject to size, power, and computational constraints, encounter even more difficulties in running and adapting these algorithms. Such adaptability, though, is crucial for real-world deployment, where robots must operate effectively in dynamic and unpredictable settings. In this work, we introduce a novel benchmark to evaluate the continual learning capabilities of object detection systems in tiny robotic platforms. Our contributions include: (i) Tiny Robotics Object Detection (TiROD), a comprehensive dataset collected using a small mobile robot, designed to test the adaptability of object detectors across various domains and classes; (ii) an evaluation of state-of-the-art real-time object detectors combined with different continual learning strategies on this dataset, providing detailed insights into their performance and limitations; and (iii) we publish the data and the code to replicate the results to foster continuous advancements in this field. Our benchmark results indicate key challenges that must be addressed to advance the development of robust and efficient object detection systems for tiny robotics.

**Link**: [arxiv](http://arxiv.org/abs/2409.16215v1),  [pdf](http://arxiv.org/pdf/2409.16215v1)

**Tags**: cs.RO cs.CV 



### LLMCount: Enhancing Stationary mmWave Detection with Multimodal-LLM
**Authors**: Boyan Li, Shengyi Ding, Deen Ma, Yixuan Wu, Hongjie Liao, Kaiyuan Hu

**Updated**: 2024-09-24T16:09:29Z

**Summary**: Millimeter wave sensing provides people with the capability of sensing the surrounding crowds in a non-invasive and privacy-preserving manner, which holds huge application potential. However, detecting stationary crowds remains challenging due to several factors such as minimal movements (like breathing or casual fidgets), which can be easily treated as noise clusters during data collection and consequently filtered in the following processing procedures. Additionally, the uneven distribution of signal power due to signal power attenuation and interferences resulting from external reflectors or absorbers further complicates accurate detection. To address these challenges and enable stationary crowd detection across various application scenarios requiring specialized domain adaption, we introduce LLMCount, the first system to harness the capabilities of large-language models (LLMs) to enhance crowd detection performance. By exploiting the decision-making capability of LLM, we can successfully compensate the signal power to acquire a uniform distribution and thereby achieve a detection with higher accuracy. To assess the system's performance, comprehensive evaluations are conducted under diversified scenarios like hall, meeting room, and cinema. The evaluation results show that our proposed approach reaches high detection accuracy with lower overall latency compared with previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.16209v1),  [pdf](http://arxiv.org/pdf/2409.16209v1)

**Tags**: cs.CV 



### A is for Absorption: Studying Feature Splitting and Absorption in Sparse   Autoencoders
**Authors**: David Chanin, James Wilken-Smith, Tom√°≈° Dulka, Hardik Bhatnagar, Joseph Bloom

**Updated**: 2024-09-24T16:07:31Z

**Summary**: Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to what extent does varying the sparsity or the size of the SAE affect monosemanticity / interpretability? By investigating these questions in the context of a simple first-letter identification task where we have complete access to ground truth labels for all tokens in the vocabulary, we are able to provide more detail than prior investigations. Critically, we identify a problematic form of feature-splitting we call feature absorption where seemingly monosemantic latents fail to fire in cases where they clearly should. Our investigation suggests that varying SAE size or sparsity is insufficient to solve this issue, and that there are deeper conceptual issues in need of resolution.

**Link**: [arxiv](http://arxiv.org/abs/2409.14507v2),  [pdf](http://arxiv.org/pdf/2409.14507v2)

**Tags**: cs.CL cs.AI 



### CJEval: A Benchmark for Assessing Large Language Models Using Chinese   Junior High School Exam Data
**Authors**: Qian-Wen Zhang, Haochen Wang, Fang Li, Siyu An, Lingfeng Qiao, Liangcai Gao, Di Yin, Xing Sun

**Updated**: 2024-09-25T03:35:35Z

**Summary**: Online education platforms have significantly transformed the dissemination of educational resources by providing a dynamic and digital infrastructure. With the further enhancement of this transformation, the advent of Large Language Models (LLMs) has elevated the intelligence levels of these platforms. However, current academic benchmarks provide limited guidance for real-world industry scenarios. This limitation arises because educational applications require more than mere test question responses. To bridge this gap, we introduce CJEval, a benchmark based on Chinese Junior High School Exam Evaluations. CJEval consists of 26,136 samples across four application-level educational tasks covering ten subjects. These samples include not only questions and answers but also detailed annotations such as question types, difficulty levels, knowledge concepts, and answer explanations. By utilizing this benchmark, we assessed LLMs' potential applications and conducted a comprehensive analysis of their performance by fine-tuning on various educational tasks. Extensive experiments and discussions have highlighted the opportunities and challenges of applying LLMs in the field of education.

**Link**: [arxiv](http://arxiv.org/abs/2409.16202v2),  [pdf](http://arxiv.org/pdf/2409.16202v2)

**Tags**: cs.AI 



### HelloBench: Evaluating Long Text Generation Capabilities of Large   Language Models
**Authors**: Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, Junran Peng, Zhaoxiang Zhang, Songyang Zhang, Kai Chen

**Updated**: 2024-09-24T15:38:11Z

**Summary**: In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks (e.g., long-context understanding), and many benchmarks have been proposed. However, we observe that long text generation capabilities are not well investigated. Therefore, we introduce the Hierarchical Long Text Generation Benchmark (HelloBench), a comprehensive, in-the-wild, and open-ended benchmark to evaluate LLMs' performance in generating long text. Based on Bloom's Taxonomy, HelloBench categorizes long text generation tasks into five subtasks: open-ended QA, summarization, chat, text completion, and heuristic text generation. Besides, we propose Hierarchical Long Text Evaluation (HelloEval), a human-aligned evaluation method that significantly reduces the time and effort required for human evaluation while maintaining a high correlation with human evaluation. We have conducted extensive experiments across around 30 mainstream LLMs and observed that the current LLMs lack long text generation capabilities. Specifically, first, regardless of whether the instructions include explicit or implicit length constraints, we observe that most LLMs cannot generate text that is longer than 4000 words. Second, we observe that while some LLMs can generate longer text, many issues exist (e.g., severe repetition and quality degradation). Third, to demonstrate the effectiveness of HelloEval, we compare HelloEval with traditional metrics (e.g., ROUGE, BLEU, etc.) and LLM-as-a-Judge methods, which show that HelloEval has the highest correlation with human evaluation. We release our code in https://github.com/Quehry/HelloBench.

**Link**: [arxiv](http://arxiv.org/abs/2409.16191v1),  [pdf](http://arxiv.org/pdf/2409.16191v1)

**Tags**: cs.CL 



### Data-Driven System Identification of Quadrotors Subject to Motor Delays
**Authors**: Jonas Eschmann, Dario Albani, Giuseppe Loianno

**Updated**: 2024-09-24T15:25:59Z

**Summary**: Recently non-linear control methods like Model Predictive Control (MPC) and Reinforcement Learning (RL) have attracted increased interest in the quadrotor control community. In contrast to classic control methods like cascaded PID controllers, MPC and RL heavily rely on an accurate model of the system dynamics. The process of quadrotor system identification is notoriously tedious and is often pursued with additional equipment like a thrust stand. Furthermore, low-level details like motor delays which are crucial for accurate end-to-end control are often neglected. In this work, we introduce a data-driven method to identify a quadrotor's inertia parameters, thrust curves, torque coefficients, and first-order motor delay purely based on proprioceptive data. The estimation of the motor delay is particularly challenging as usually, the RPMs can not be measured. We derive a Maximum A Posteriori (MAP)-based method to estimate the latent time constant. Our approach only requires about a minute of flying data that can be collected without any additional equipment and usually consists of three simple maneuvers. Experimental results demonstrate the ability of our method to accurately recover the parameters of multiple quadrotors. It also facilitates the deployment of RL-based, end-to-end quadrotor control of a large quadrotor under harsh, outdoor conditions.

**Link**: [arxiv](http://arxiv.org/abs/2404.07837v2),  [pdf](http://arxiv.org/pdf/2404.07837v2)

**Tags**: cs.RO cs.SY eess.SY 



### Microsecond-Latency Feedback at a Particle Accelerator by Online   Reinforcement Learning on Hardware
**Authors**: Luca Scomparin, Michele Caselle, Andrea Santamaria Garcia, Chenran Xu, Edmund Blomley, Timo Dritschler, Akira Mochihashi, Marcel Schuh, Johannes L. Steinmann, Erik Br√ºndermann, Andreas Kopmann, J√ºrgen Becker, Anke-Susanne M√ºller, Marc Weber

**Updated**: 2024-09-24T15:21:53Z

**Summary**: The commissioning and operation of future large-scale scientific experiments will challenge current tuning and control methods. Reinforcement learning (RL) algorithms are a promising solution thanks to their capability of autonomously tackling a control problem based on a task parameterized by a reward function. The conventionally utilized machine learning (ML) libraries are not intended for microsecond latency applications, as they mostly optimize for throughput performance. On the other hand, most of the programmable logic implementations are meant for computation acceleration, not being intended to work in a real-time environment. To overcome these limitations of current implementations, RL needs to be deployed on-the-edge, i.e. on to the device gathering the training data. In this paper we present the design and deployment of an experience accumulator system in a particle accelerator. In this system deep-RL algorithms run using hardware acceleration and act within a few microseconds, enabling the use of RL for control of ultra-fast phenomena. The training is performed offline to reduce the number of operations carried out on the acceleration hardware. The proposed architecture was tested in real experimental conditions at the Karlsruhe research accelerator (KARA), serving also as a synchrotron light source, where the system was used to control induced horizontal betatron oscillations in real-time. The results showed a performance comparable to the commercial feedback system available at the accelerator, proving the viability and potential of this approach. Due to the self-learning and reconfiguration capability of this implementation, its seamless application to other control problems is possible. Applications range from particle accelerators to large-scale research and industrial facilities.

**Link**: [arxiv](http://arxiv.org/abs/2409.16177v1),  [pdf](http://arxiv.org/pdf/2409.16177v1)

**Tags**: physics.acc-ph hep-ex physics.ins-det 



### Cyber Knowledge Completion Using Large Language Models
**Authors**: Braden K Webb, Sumit Purohit, Rounak Meyur

**Updated**: 2024-09-24T15:20:39Z

**Summary**: The integration of the Internet of Things (IoT) into Cyber-Physical Systems (CPSs) has expanded their cyber-attack surface, introducing new and sophisticated threats with potential to exploit emerging vulnerabilities. Assessing the risks of CPSs is increasingly difficult due to incomplete and outdated cybersecurity knowledge. This highlights the urgent need for better-informed risk assessments and mitigation strategies. While previous efforts have relied on rule-based natural language processing (NLP) tools to map vulnerabilities, weaknesses, and attack patterns, recent advancements in Large Language Models (LLMs) present a unique opportunity to enhance cyber-attack knowledge completion through improved reasoning, inference, and summarization capabilities. We apply embedding models to encapsulate information on attack patterns and adversarial techniques, generating mappings between them using vector embeddings. Additionally, we propose a Retrieval-Augmented Generation (RAG)-based approach that leverages pre-trained models to create structured mappings between different taxonomies of threat patterns. Further, we use a small hand-labeled dataset to compare the proposed RAG-based approach to a baseline standard binary classification model. Thus, the proposed approach provides a comprehensive framework to address the challenge of cyber-attack knowledge graph completion.

**Link**: [arxiv](http://arxiv.org/abs/2409.16176v1),  [pdf](http://arxiv.org/pdf/2409.16176v1)

**Tags**: cs.CR cs.AI J.7; H.3.3 



### Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to   Extremes Through Rank-Wise Clustering
**Authors**: Ziyu Zhao, Tao Shen, Didi Zhu, Zexi Li, Jing Su, Xuwu Wang, Kun Kuang, Fei Wu

**Updated**: 2024-09-24T15:08:41Z

**Summary**: Low-Rank Adaptation (LoRA) has emerged as a popular technique for fine-tuning large language models (LLMs) to various domains due to its modular design and widespread availability on platforms like Huggingface. This modularity has sparked interest in combining multiple LoRAs to enhance LLM capabilities. However, existing methods for LoRA composition primarily focus on task-specific adaptations that require additional training, and current model merging techniques often fail to fully leverage LoRA's modular nature, leading to parameter interference and performance degradation. In this paper, we investigate the feasibility of disassembling and reassembling multiple LoRAs at a finer granularity, analogous to assembling LEGO blocks. We introduce the concept of Minimal Semantic Units (MSUs), where the parameters corresponding to each rank in LoRA function as independent units. These MSUs demonstrate permutation invariance and concatenation-summation equivalence properties, enabling flexible combinations to create new LoRAs. Building on these insights, we propose the LoRA-LEGO framework. This framework conducts rank-wise parameter clustering by grouping MSUs from different LoRAs into $k$ clusters. The centroid of each cluster serves as a representative MSU, enabling the assembly of a merged LoRA with an adjusted rank of $k$. Additionally, we apply a dual reweighting strategy to optimize the scale of the merged LoRA. Experiments across various benchmarks demonstrate that our method outperforms existing approaches in LoRA merging.

**Link**: [arxiv](http://arxiv.org/abs/2409.16167v1),  [pdf](http://arxiv.org/pdf/2409.16167v1)

**Tags**: cs.LG cs.AI cs.CL 



### Towards Robust Automation of Surgical Systems via Digital Twin-based   Scene Representations from Foundation Models
**Authors**: Hao Ding, Lalithkumar Seenivasan, Hongchao Shu, Grayson Byrd, Han Zhang, Pu Xiao, Juan Antonio Barragan, Russell H. Taylor, Peter Kazanzides, Mathias Unberath

**Updated**: 2024-09-24T15:08:03Z

**Summary**: Large language model-based (LLM) agents are emerging as a powerful enabler of robust embodied intelligence due to their capability of planning complex action sequences. Sound planning ability is necessary for robust automation in many task domains, but especially in surgical automation. These agents rely on a highly detailed natural language representation of the scene. Thus, to leverage the emergent capabilities of LLM agents for surgical task planning, developing similarly powerful and robust perception algorithms is necessary to derive a detailed scene representation of the environment from visual input. Previous research has focused primarily on enabling LLM-based task planning while adopting simple yet severely limited perception solutions to meet the needs for bench-top experiments but lack the critical flexibility to scale to less constrained settings. In this work, we propose an alternate perception approach -- a digital twin-based machine perception approach that capitalizes on the convincing performance and out-of-the-box generalization of recent vision foundation models. Integrating our digital twin-based scene representation and LLM agent for planning with the dVRK platform, we develop an embodied intelligence system and evaluate its robustness in performing peg transfer and gauze retrieval tasks. Our approach shows strong task performance and generalizability to varied environment settings. Despite convincing performance, this work is merely a first step towards the integration of digital twin-based scene representations. Future studies are necessary for the realization of a comprehensive digital twin framework to improve the interpretability and generalizability of embodied intelligence in surgery.

**Link**: [arxiv](http://arxiv.org/abs/2409.13107v2),  [pdf](http://arxiv.org/pdf/2409.13107v2)

**Tags**: cs.RO 



### ChatDBG: An AI-Powered Debugging Assistant
**Authors**: Kyla Levin, Nicolas van Kempen, Emery D. Berger, Stephen N. Freund

**Updated**: 2024-09-24T15:07:24Z

**Summary**: Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like `why is x null?'. To handle these queries, ChatDBG grants the LLM autonomy to "take the wheel": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded roughly 50,000 times.

**Link**: [arxiv](http://arxiv.org/abs/2403.16354v2),  [pdf](http://arxiv.org/pdf/2403.16354v2)

**Tags**: cs.SE cs.AI cs.LG cs.PL 



### NL-COMM: Demonstrating Gains of Non-Linear Processing in Open-RAN   Ecosystem
**Authors**: Chathura Jayawardena, Marcin Filo, George N. Katsaros, Konstantinos Nikitopoulos

**Updated**: 2024-09-24T15:00:20Z

**Summary**: Multi-user multiple-input, multiple-output (MU-MIMO) designs can substantially increase wireless systems' achievable throughput and connectivity capabilities. However, existing MU-MIMO deployments typically utilize linear processing techniques that, despite their practical benefits, such as low computational complexity and easy integrability, can leave much of the available throughput and connectivity gains unexploited. They typically require many power-intensive antennas and RF chains to support a smaller number of MIMO streams, even when the transmitted information streams are of low rate. Alternatively, non-linear (NL) processing methods can maximize the capabilities of the MIMO channel. Despite their potential, traditional NL methods are challenged by high computational complexity and processing latency, making them impractical for real-time applications, especially in software-based systems envisioned for emerging Open Radio Access Networks (Open-RAN). Additionally, essential functionalities such as rate adaptation (RA) are currently unavailable for NL systems, limiting their practicality in real-world deployments. In this demo, we present the latest capabilities of our advanced NL processing framework (NL-COMM) in real-time and over-the-air, comparing them side-by-side with conventional linear processing. For the first time, NL-COMM not only meets the practical 5G-NR real-time latency requirements in pure software but also does so within a standard-compliant ecosystem. To achieve this, we significantly extended the NL-COMM algorithmic framework to support the first practical RA for NL processing. The demonstrated gains include enhanced connectivity by supporting four MIMO streams with a single base-station antenna, substantially increased throughput, and the ability to halve the number of base-station antennas without any performance loss to linear approaches.

**Link**: [arxiv](http://arxiv.org/abs/2409.12930v2),  [pdf](http://arxiv.org/pdf/2409.12930v2)

**Tags**: eess.SP cs.NI 



### Video-XL: Extra-Long Vision Language Model for Hour-Scale Video   Understanding
**Authors**: Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, Bo Zhao

**Updated**: 2024-09-24T14:59:30Z

**Summary**: Although current Multi-modal Large Language Models (MLLMs) demonstrate promising results in video understanding, processing extremely long videos remains an ongoing challenge. Typically, MLLMs struggle with handling thousands of tokens that exceed the maximum context length of LLMs, and they experience reduced visual clarity due to token aggregation. Another challenge is the high computational cost stemming from the large number of video tokens. To tackle these issues, we propose Video-XL, an extra-long vision language model designed for efficient hour-scale video understanding. Specifically, we argue that LLMs can be adapted as effective visual condensers and introduce Visual Context Latent Summarization, which condenses visual contexts into highly compact forms. Extensive experiments demonstrate that our model achieves promising results on popular long video understanding benchmarks, despite being trained on limited image data. Moreover, Video-XL strikes a promising balance between efficiency and effectiveness, processing 1024 frames on a single 80GB GPU while achieving nearly 100\% accuracy in the Needle-in-a-Haystack evaluation. We envision Video-XL becoming a valuable tool for long video applications such as video summarization, surveillance anomaly detection, and Ad placement identification.

**Link**: [arxiv](http://arxiv.org/abs/2409.14485v2),  [pdf](http://arxiv.org/pdf/2409.14485v2)

**Tags**: cs.CV 



### Efficient Motion Prediction: A Lightweight & Accurate Trajectory   Prediction Model With Fast Training and Inference Speed
**Authors**: Alexander Prutsch, Horst Bischof, Horst Possegger

**Updated**: 2024-09-25T09:00:27Z

**Summary**: For efficient and safe autonomous driving, it is essential that autonomous vehicles can predict the motion of other traffic agents. While highly accurate, current motion prediction models often impose significant challenges in terms of training resource requirements and deployment on embedded hardware. We propose a new efficient motion prediction model, which achieves highly competitive benchmark results while training only a few hours on a single GPU. Due to our lightweight architectural choices and the focus on reducing the required training resources, our model can easily be applied to custom datasets. Furthermore, its low inference latency makes it particularly suitable for deployment in autonomous applications with limited computing resources.

**Link**: [arxiv](http://arxiv.org/abs/2409.16154v2),  [pdf](http://arxiv.org/pdf/2409.16154v2)

**Tags**: cs.RO cs.CV 



### HA-FGOVD: Highlighting Fine-grained Attributes via Explicit Linear   Composition for Open-Vocabulary Object Detection
**Authors**: Yuqi Ma, Mengyin Liu, Chao Zhu, Xu-Cheng Yin

**Updated**: 2024-09-24T14:43:14Z

**Summary**: Open-vocabulary object detection (OVD) models are considered to be Large Multi-modal Models (LMM), due to their extensive training data and a large number of parameters. Mainstream OVD models prioritize object coarse-grained category rather than focus on their fine-grained attributes, e.g., colors or materials, thus failed to identify objects specified with certain attributes. However, OVD models are pretrained on large-scale image-text pairs with rich attribute words, whose latent feature space can represent the global text feature as a linear composition of fine-grained attribute tokens without highlighting them. Therefore, we propose in this paper a universal and explicit approach for frozen mainstream OVD models that boosts their attribute-level detection capabilities by highlighting fine-grained attributes in explicit linear space. Firstly, a LLM is leveraged to highlight attribute words within the input text as a zero-shot prompted task. Secondly, by strategically adjusting the token masks, the text encoders of OVD models extract both global text and attribute-specific features, which are then explicitly composited as two vectors in linear space to form the new attribute-highlighted feature for detection tasks, where corresponding scalars are hand-crafted or learned to reweight both two vectors. Notably, these scalars can be seamlessly transferred among different OVD models, which proves that such an explicit linear composition is universal. Empirical evaluation on the FG-OVD dataset demonstrates that our proposed method uniformly improves fine-grained attribute-level OVD of various mainstream models and achieves new state-of-the-art performance.

**Link**: [arxiv](http://arxiv.org/abs/2409.16136v1),  [pdf](http://arxiv.org/pdf/2409.16136v1)

**Tags**: cs.CV cs.AI cs.CL cs.MM 



### MOSS: Enabling Code-Driven Evolution and Context Management for AI   Agents
**Authors**: Ming Zhu, Yi Zhou

**Updated**: 2024-09-24T14:30:21Z

**Summary**: Developing AI agents powered by large language models (LLMs) faces significant challenges in achieving true Turing completeness and adaptive, code-driven evolution. Current approaches often generate code independently of its runtime context, relying heavily on the LLM's memory, which results in inefficiencies and limits adaptability. Manual protocol development in sandbox environments further constrains the agent's autonomous adaptability. Crucially, achieving consistency in code and context across multi-turn interactions and ensuring isolation of local variables within each interaction remains an unsolved problem.   We introduce MOSS (llM-oriented Operating System Simulation), a novel framework that addresses these challenges by integrating code generation with a dynamic context management system. MOSS ensures consistency and adaptability by using a mechanism that maintains the Python context across interactions, including isolation of local variables and preservation of runtime integrity. At its core, the framework employs an Inversion of Control (IoC) container in conjunction with decorators to enforce the least knowledge principle, allowing agents to focus on abstract interfaces rather than concrete implementations. This facilitates seamless integration of new tools and libraries, enables runtime instance replacement, and reduces prompt complexity, providing a "what you see is what you get" environment for the agent.   Through a series of case studies, we show how this framework can enhance the efficiency and capabilities of agent development and highlight its advantages in moving towards Turing-complete agents capable of evolving through code.

**Link**: [arxiv](http://arxiv.org/abs/2409.16120v1),  [pdf](http://arxiv.org/pdf/2409.16120v1)

**Tags**: cs.SE cs.AI cs.CL 



### Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of   Large Language Models
**Authors**: Nishanth Madhusudhan, Sathwik Tejaswi Madhusudhan, Vikas Yadav, Masoud Hashemi

**Updated**: 2024-09-24T14:25:58Z

**Summary**: Abstention Ability (AA) is a critical aspect of Large Language Model (LLM) reliability, referring to an LLM's capability to withhold responses when uncertain or lacking a definitive answer, without compromising performance. Although previous studies have attempted to improve AA, they lack a standardised evaluation method and remain unsuitable for black-box models where token prediction probabilities are inaccessible. This makes comparative analysis challenging, especially for state-of-the-art closed-source commercial LLMs. This paper bridges this gap by introducing a black-box evaluation approach and a new dataset, Abstain-QA, crafted to rigorously assess AA across varied question types (answerable and unanswerable), domains (well-represented and under-represented), and task types (fact centric and reasoning). We also propose a new confusion matrix, the ''Answerable-Unanswerable Confusion Matrix'' (AUCM) which serves as the basis for evaluating AA, by offering a structured and precise approach for assessment. Finally, we explore the impact of three prompting strategies-Strict Prompting, Verbal Confidence Thresholding, and Chain-of-Thought (CoT)-on improving AA. Our results indicate that even powerful models like GPT-4, Mixtral 8x22b encounter difficulties with abstention; however, strategic approaches such as Strict prompting and CoT can enhance this capability.

**Link**: [arxiv](http://arxiv.org/abs/2407.16221v2),  [pdf](http://arxiv.org/pdf/2407.16221v2)

**Tags**: cs.CL 



### Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts   for Instruction Tuning on General Tasks
**Authors**: Haoyuan Wu, Haisheng Zheng, Zhuolun He, Bei Yu

**Updated**: 2024-09-24T14:14:40Z

**Summary**: Large language models (LLMs) have demonstrated considerable proficiency in general natural language processing (NLP) tasks. Instruction tuning, a successful paradigm, enhances the ability of LLMs to follow natural language instructions and exhibit robust generalization across general tasks. However, these models often encounter performance limitations across multiple tasks due to constrained model capacity. Expanding this capacity during the instruction tuning phase poses significant challenges. To address this issue, we introduce parameter-efficient sparsity crafting (PESC), which crafts dense models into sparse models using the mixture-of-experts (MoE) architecture. PESC integrates adapters into the MoE layers of sparse models, differentiating experts without altering the individual weights within these layers. This method significantly reduces computational costs and GPU memory requirements, facilitating model capacity expansion through a minimal parameter increase when guaranteeing the quality of approximation in function space compared to original sparse upcycling. Our empirical evaluation demonstrates the effectiveness of the PESC method. Using PESC during instruction tuning, our best sparse model outperforms other sparse and dense models and exhibits superior general capabilities compared to GPT-3.5. Our code is available at https://github.com/wuhy68/Parameter-Efficient-MoE.

**Link**: [arxiv](http://arxiv.org/abs/2401.02731v4),  [pdf](http://arxiv.org/pdf/2401.02731v4)

**Tags**: cs.AI 



### EffiQA: Efficient Question-Answering with Strategic Multi-Model   Collaboration on Knowledge Graphs
**Authors**: Zixuan Dong, Baoyun Peng, Yufei Wang, Jia Fu, Xiaodong Wang, Yongxue Shan, Xin Zhou

**Updated**: 2024-09-24T13:53:59Z

**Summary**: While large language models (LLMs) have shown remarkable capabilities in natural language processing, they struggle with complex, multi-step reasoning tasks involving knowledge graphs (KGs). Existing approaches that integrate LLMs and KGs either underutilize the reasoning abilities of LLMs or suffer from prohibitive computational costs due to tight coupling. To address these limitations, we propose a novel collaborative framework named EffiQA that can strike a balance between performance and efficiency via an iterative paradigm. EffiQA consists of three stages: global planning, efficient KG exploration, and self-reflection. Specifically, EffiQA leverages the commonsense capability of LLMs to explore potential reasoning pathways through global planning. Then, it offloads semantic pruning to a small plug-in model for efficient KG exploration. Finally, the exploration results are fed to LLMs for self-reflection to further improve the global planning and efficient KG exploration. Empirical evidence on multiple KBQA benchmarks shows EffiQA's effectiveness, achieving an optimal balance between reasoning accuracy and computational costs. We hope the proposed new framework will pave the way for efficient, knowledge-intensive querying by redefining the integration of LLMs and KGs, fostering future research on knowledge-based question answering.

**Link**: [arxiv](http://arxiv.org/abs/2406.01238v3),  [pdf](http://arxiv.org/pdf/2406.01238v3)

**Tags**: cs.CL 



### Great, Now Write an Article About That: The Crescendo Multi-Turn LLM   Jailbreak Attack
**Authors**: Mark Russinovich, Ahmed Salem, Ronen Eldan

**Updated**: 2024-09-24T13:51:39Z

**Summary**: Large Language Models (LLMs) have risen significantly in popularity and are increasingly being adopted across multiple applications. These LLMs are heavily aligned to resist engaging in illegal or unethical topics as a means to avoid contributing to responsible AI harms. However, a recent line of attacks, known as jailbreaks, seek to overcome this alignment. Intuitively, jailbreak attacks aim to narrow the gap between what the model can do and what it is willing to do. In this paper, we introduce a novel jailbreak attack called Crescendo. Unlike existing jailbreak methods, Crescendo is a simple multi-turn jailbreak that interacts with the model in a seemingly benign manner. It begins with a general prompt or question about the task at hand and then gradually escalates the dialogue by referencing the model's replies progressively leading to a successful jailbreak. We evaluate Crescendo on various public systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b and LlaMA-3 70b Chat, and Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with it achieving high attack success rates across all evaluated models and tasks. Furthermore, we present Crescendomation, a tool that automates the Crescendo attack and demonstrate its efficacy against state-of-the-art models through our evaluations. Crescendomation surpasses other state-of-the-art jailbreaking techniques on the AdvBench subset dataset, achieving 29-61% higher performance on GPT-4 and 49-71% on Gemini-Pro. Finally, we also demonstrate Crescendo's ability to jailbreak multimodal models.

**Link**: [arxiv](http://arxiv.org/abs/2404.01833v2),  [pdf](http://arxiv.org/pdf/2404.01833v2)

**Tags**: cs.CR cs.AI 



### Exploring Hint Generation Approaches in Open-Domain Question Answering
**Authors**: Jamshid Mozafari, Abdelrahman Abdallah, Bhawna Piryani, Adam Jatowt

**Updated**: 2024-09-24T13:50:32Z

**Summary**: Automatic Question Answering (QA) systems rely on contextual information to provide accurate answers. Commonly, contexts are prepared through either retrieval-based or generation-based methods. The former involves retrieving relevant documents from a corpus like Wikipedia, whereas the latter uses generative models such as Large Language Models (LLMs) to generate the context. In this paper, we introduce a novel context preparation approach called HINTQA, which employs Automatic Hint Generation (HG) techniques. Unlike traditional methods, HINTQA prompts LLMs to produce hints about potential answers for the question rather than generating relevant context. We evaluate our approach across three QA datasets including TriviaQA, NaturalQuestions, and Web Questions, examining how the number and order of hints impact performance. Our findings show that the HINTQA surpasses both retrieval-based and generation-based approaches. We demonstrate that hints enhance the accuracy of answers more than retrieved and generated contexts.

**Link**: [arxiv](http://arxiv.org/abs/2409.16096v1),  [pdf](http://arxiv.org/pdf/2409.16096v1)

**Tags**: cs.CL cs.IR 



### Beyond the Black Box: A Statistical Model for LLM Reasoning and   Inference
**Authors**: Siddhartha Dalal, Vishal Misra

**Updated**: 2024-09-24T13:30:25Z

**Summary**: This paper introduces a novel Bayesian learning model to explain the behavior of Large Language Models (LLMs), focusing on their core optimization metric of next token prediction. We develop a theoretical framework based on an ideal generative text model represented by a multinomial transition probability matrix with a prior, and examine how LLMs approximate this matrix. Key contributions include: (i) a continuity theorem relating embeddings to multinomial distributions, (ii) a demonstration that LLM text generation aligns with Bayesian learning principles, (iii) an explanation for the emergence of in-context learning in larger models, (iv) empirical validation using visualizations of next token probabilities from an instrumented Llama model Our findings provide new insights into LLM functioning, offering a statistical foundation for understanding their capabilities and limitations. This framework has implications for LLM design, training, and application, potentially guiding future developments in the field.

**Link**: [arxiv](http://arxiv.org/abs/2402.03175v2),  [pdf](http://arxiv.org/pdf/2402.03175v2)

**Tags**: cs.LG cs.AI I.2.7 



### OpenGraph: Towards Open Graph Foundation Models
**Authors**: Lianghao Xia, Ben Kao, Chao Huang

**Updated**: 2024-09-24T13:26:08Z

**Summary**: Graph learning has become indispensable for interpreting and harnessing relational data in diverse fields, ranging from recommendation systems to social network analysis. In this context, a variety of GNNs have emerged as promising methodologies for encoding the structural information of graphs. By effectively capturing the graph's underlying structure, these GNNs have shown great potential in enhancing performance in graph learning tasks, such as link prediction and node classification. However, despite their successes, a significant challenge persists: these advanced methods often face difficulties in generalizing to unseen graph data that significantly differs from the training instances. In this work, our aim is to advance the graph learning paradigm by developing a general graph foundation model. This model is designed to understand the complex topological patterns present in diverse graph data, enabling it to excel in zero-shot graph learning tasks across different downstream datasets. To achieve this goal, we address several key technical challenges in our OpenGraph model. Firstly, we propose a unified graph tokenizer to adapt our graph model to generalize well on unseen graph data, even when the underlying graph properties differ significantly from those encountered during training. Secondly, we develop a scalable graph transformer as the foundational encoder, which effectively captures node-wise dependencies within the global topological context. Thirdly, we introduce a data augmentation mechanism enhanced by a LLM to alleviate the limitations of data scarcity in real-world scenarios. Extensive experiments validate the effectiveness of our framework. By adapting our OpenGraph to new graph characteristics and comprehending the nuances of diverse graphs, our approach achieves remarkable zero-shot graph learning performance across various settings and domains.

**Link**: [arxiv](http://arxiv.org/abs/2403.01121v3),  [pdf](http://arxiv.org/pdf/2403.01121v3)

**Tags**: cs.LG cs.AI cs.SI 



### Open Conversational LLMs do not know most Spanish words
**Authors**: Javier Conde, Miguel Gonz√°lez, Nina Melero, Raquel Ferrando, Gonzalo Mart√≠nez, Elena Merino-G√≥mez, Jos√© Alberto Hern√°ndez, Pedro Reviriego

**Updated**: 2024-09-24T13:25:01Z

**Summary**: The growing interest in Large Language Models (LLMs) and in particular in conversational models with which users can interact has led to the development of a large number of open-source chat LLMs. These models are evaluated on a wide range of benchmarks to assess their capabilities in answering questions or solving problems on almost any possible topic or to test their ability to reason or interpret texts. Instead, the evaluation of the knowledge that these models have of the languages has received much less attention. For example, the words that they can recognize and use in different languages. In this paper, we evaluate the knowledge that open-source chat LLMs have of Spanish words by testing a sample of words in a reference dictionary. The results show that open-source chat LLMs produce incorrect meanings for an important fraction of the words and are not able to use most of the words correctly to write sentences with context. These results show how Spanish is left behind in the open-source LLM race and highlight the need to push for linguistic fairness in conversational LLMs ensuring that they provide similar performance across languages.

**Link**: [arxiv](http://arxiv.org/abs/2403.15491v2),  [pdf](http://arxiv.org/pdf/2403.15491v2)

**Tags**: cs.CL 



### Assessing strategies to manage distributed photovoltaics in Swiss   low-voltage networks: An analysis of curtailment, export tariffs, and   resource sharing
**Authors**: Alejandro Pena-Bello, Gerard Marias Gonzalez, Nicolas Wyrsch, Christophe Ballif

**Updated**: 2024-09-24T13:24:14Z

**Summary**: The integration of photovoltaic systems poses several challenges for the distribution grid, mainly due to the infrastructure not being designed to handle the upstream flow and being dimensioned for consumption only, potentially leading to reliability and stability issues. This study investigates the use of capacity-based tariffs, export tariffs, and curtailment policies to reduce negative grid impacts without hampering PV deployment. We analyze the effect of such export tariffs on three typical Swiss low-voltage networks (rural, semi-urban, and urban), using power flow analysis to evaluate the power exchanges at the transformer station, as well as line overloading and voltage violations. Finally, a simple case of mutualization of resources is analyzed to assess its potential contribution to relieving network constraints and the economic costs of managing LV networks. We found that the tariff with capacity-based components on the export (CT export daily) severely penalizes PV penetration. This applies to other tariffs as well (e.g. IRR monthly, Curtailment 30, and DT variable) but to a lesser extent. However, the inclusion of curtailment at 50\% and 70\%, as well as mixed tariffs with capacity-based components at import and curtailment, allow for a high degree of PV installations in the three zones studied and help to mitigate the impact of PV on the distributed network.

**Link**: [arxiv](http://arxiv.org/abs/2409.16078v1),  [pdf](http://arxiv.org/pdf/2409.16078v1)

**Tags**: eess.SY cs.SY 



### PALLM: Evaluating and Enhancing PALLiative Care Conversations with Large   Language Models
**Authors**: Zhiyuan Wang, Fangxu Yuan, Virginia LeBaron, Tabor Flickinger, Laura E. Barnes

**Updated**: 2024-09-24T13:03:24Z

**Summary**: Effective patient-provider communication is crucial in clinical care, directly impacting patient outcomes and quality of life. Traditional evaluation methods, such as human ratings, patient feedback, and provider self-assessments, are often limited by high costs and scalability issues. Although existing natural language processing (NLP) techniques show promise, they struggle with the nuances of clinical communication and require sensitive clinical data for training, reducing their effectiveness in real-world applications. Emerging large language models (LLMs) offer a new approach to assessing complex communication metrics, with the potential to advance the field through integration into passive sensing and just-in-time intervention systems. This study explores LLMs as evaluators of palliative care communication quality, leveraging their linguistic, in-context learning, and reasoning capabilities. Specifically, using simulated scripts crafted and labeled by healthcare professionals, we test proprietary models (e.g., GPT-4) and fine-tune open-source LLMs (e.g., LLaMA2) with a synthetic dataset generated by GPT-4 to evaluate clinical conversations, to identify key metrics such as `understanding' and `empathy'. Our findings demonstrated LLMs' superior performance in evaluating clinical communication, providing actionable feedback with reasoning, and demonstrating the feasibility and practical viability of developing in-house LLMs. This research highlights LLMs' potential to enhance patient-provider interactions and lays the groundwork for downstream steps in developing LLM-empowered clinical health systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.15188v2),  [pdf](http://arxiv.org/pdf/2409.15188v2)

**Tags**: cs.CL cs.HC 



### Whole-body end-effector pose tracking
**Authors**: Tifanny Portela, Andrei Cramariuc, Mayank Mittal, Marco Hutter

**Updated**: 2024-09-24T12:51:32Z

**Summary**: Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system's complexity, making precise end-effector control challenging. Existing model-based approaches are often constrained by their modeling assumptions, leading to limited robustness. Meanwhile, recent Reinforcement Learning (RL) implementations restrict the arm's workspace to be in front of the robot or track only the position to obtain decent tracking accuracy. In this work, we address these limitations by introducing a whole-body RL formulation for end-effector pose tracking in a large workspace on rough, unstructured terrains. Our proposed method involves a terrain-aware sampling strategy for the robot's initial configuration and end-effector pose commands, as well as a game-based curriculum to extend the robot's operating range. We validate our approach on the ANYmal quadrupedal robot with a six DoF robotic arm. Through our experiments, we show that the learned controller achieves precise command tracking over a large workspace and adapts across varying terrains such as stairs and slopes. On deployment, it achieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming existing competitive baselines.

**Link**: [arxiv](http://arxiv.org/abs/2409.16048v1),  [pdf](http://arxiv.org/pdf/2409.16048v1)

**Tags**: cs.RO cs.AI cs.LG cs.SY eess.SY 



### Deep chroma compression of tone-mapped images
**Authors**: Xenios Milidonis, Francesco Banterle, Alessandro Artusi

**Updated**: 2024-09-24T12:31:55Z

**Summary**: Acquisition of high dynamic range (HDR) images is thriving due to the increasing use of smart devices and the demand for high-quality output. Extensive research has focused on developing methods for reducing the luminance range in HDR images using conventional and deep learning-based tone mapping operators to enable accurate reproduction on conventional 8 and 10-bit digital displays. However, these methods often fail to account for pixels that may lie outside the target display's gamut, resulting in visible chromatic distortions or color clipping artifacts. Previous studies suggested that a gamut management step ensures that all pixels remain within the target gamut. However, such approaches are computationally expensive and cannot be deployed on devices with limited computational resources. We propose a generative adversarial network for fast and reliable chroma compression of HDR tone-mapped images. We design a loss function that considers the hue property of generated images to improve color accuracy, and train the model on an extensive image dataset. Quantitative experiments demonstrate that the proposed model outperforms state-of-the-art image generation and enhancement networks in color accuracy, while a subjective study suggests that the generated images are on par or superior to those produced by conventional chroma compression methods in terms of visual quality. Additionally, the model achieves real-time performance, showing promising results for deployment on devices with limited computational resources.

**Link**: [arxiv](http://arxiv.org/abs/2409.16032v1),  [pdf](http://arxiv.org/pdf/2409.16032v1)

**Tags**: eess.IV cs.AI cs.CV 



### MHRC: Closed-loop Decentralized Multi-Heterogeneous Robot Collaboration   with Large Language Models
**Authors**: Wenhao Yu, Jie Peng, Yueliang Ying, Sai Li, Jianmin Ji, Yanyong Zhang

**Updated**: 2024-09-24T12:29:44Z

**Summary**: The integration of large language models (LLMs) with robotics has significantly advanced robots' abilities in perception, cognition, and task planning. The use of natural language interfaces offers a unified approach for expressing the capability differences of heterogeneous robots, facilitating communication between them, and enabling seamless task allocation and collaboration. Currently, the utilization of LLMs to achieve decentralized multi-heterogeneous robot collaborative tasks remains an under-explored area of research. In this paper, we introduce a novel framework that utilizes LLMs to achieve decentralized collaboration among multiple heterogeneous robots. Our framework supports three robot categories, mobile robots, manipulation robots, and mobile manipulation robots, working together to complete tasks such as exploration, transportation, and organization. We developed a rich set of textual feedback mechanisms and chain-of-thought (CoT) prompts to enhance task planning efficiency and overall system performance. The mobile manipulation robot can adjust its base position flexibly, ensuring optimal conditions for grasping tasks. The manipulation robot can comprehend task requirements, seek assistance when necessary, and handle objects appropriately. Meanwhile, the mobile robot can explore the environment extensively, map object locations, and communicate this information to the mobile manipulation robot, thus improving task execution efficiency. We evaluated the framework using PyBullet, creating scenarios with three different room layouts and three distinct operational tasks. We tested various LLM models and conducted ablation studies to assess the contributions of different modules. The experimental results confirm the effectiveness and necessity of our proposed framework.

**Link**: [arxiv](http://arxiv.org/abs/2409.16030v1),  [pdf](http://arxiv.org/pdf/2409.16030v1)

**Tags**: cs.RO 



### Unlocking Markets: A Multilingual Benchmark to Cross-Market Question   Answering
**Authors**: Yifei Yuan, Yang Deng, Anders S√∏gaard, Mohammad Aliannejadi

**Updated**: 2024-09-24T12:24:34Z

**Summary**: Users post numerous product-related questions on e-commerce platforms, affecting their purchase decisions. Product-related question answering (PQA) entails utilizing product-related resources to provide precise responses to users. We propose a novel task of Multilingual Cross-market Product-based Question Answering (MCPQA) and define the task as providing answers to product-related questions in a main marketplace by utilizing information from another resource-rich auxiliary marketplace in a multilingual context. We introduce a large-scale dataset comprising over 7 million questions from 17 marketplaces across 11 languages. We then perform automatic translation on the Electronics category of our dataset, naming it as McMarket. We focus on two subtasks: review-based answer generation and product-related question ranking. For each subtask, we label a subset of McMarket using an LLM and further evaluate the quality of the annotations via human assessment. We then conduct experiments to benchmark our dataset, using models ranging from traditional lexical models to LLMs in both single-market and cross-market scenarios across McMarket and the corresponding LLM subset. Results show that incorporating cross-market information significantly enhances performance in both tasks.

**Link**: [arxiv](http://arxiv.org/abs/2409.16025v1),  [pdf](http://arxiv.org/pdf/2409.16025v1)

**Tags**: cs.CL 



### AI Can Be Cognitively Biased: An Exploratory Study on Threshold Priming   in LLM-Based Batch Relevance Assessment
**Authors**: Nuo Chen, Jiqun Liu, Xiaoyu Dong, Qijiong Liu, Tetsuya Sakai, Xiao-Ming Wu

**Updated**: 2024-09-24T12:23:15Z

**Summary**: Cognitive biases are systematic deviations in thinking that lead to irrational judgments and problematic decision-making, extensively studied across various fields. Recently, large language models (LLMs) have shown advanced understanding capabilities but may inherit human biases from their training data. While social biases in LLMs have been well-studied, cognitive biases have received less attention, with existing research focusing on specific scenarios. The broader impact of cognitive biases on LLMs in various decision-making contexts remains underexplored. We investigated whether LLMs are influenced by the threshold priming effect in relevance judgments, a core task and widely-discussed research topic in the Information Retrieval (IR) coummunity. The priming effect occurs when exposure to certain stimuli unconsciously affects subsequent behavior and decisions. Our experiment employed 10 topics from the TREC 2019 Deep Learning passage track collection, and tested AI judgments under different document relevance scores, batch lengths, and LLM models, including GPT-3.5, GPT-4, LLaMa2-13B and LLaMa2-70B. Results showed that LLMs tend to give lower scores to later documents if earlier ones have high relevance, and vice versa, regardless of the combination and model used. Our finding demonstrates that LLM%u2019s judgments, similar to human judgments, are also influenced by threshold priming biases, and suggests that researchers and system engineers should take into account potential human-like cognitive biases in designing, evaluating, and auditing LLMs in IR tasks and beyond.

**Link**: [arxiv](http://arxiv.org/abs/2409.16022v1),  [pdf](http://arxiv.org/pdf/2409.16022v1)

**Tags**: cs.CL cs.AI 



### Bridging Speech and Text: Enhancing ASR with Pinyin-to-Character   Pre-training in LLMs
**Authors**: Yang Yuhang, Peng Yizhou, Eng Siong Chng, Xionghu Zhong

**Updated**: 2024-09-24T12:06:31Z

**Summary**: The integration of large language models (LLMs) with pre-trained speech models has opened up new avenues in automatic speech recognition (ASR). While LLMs excel in multimodal understanding tasks, effectively leveraging their capabilities for ASR remains a significant challenge. This paper presents a novel training approach to enhance LLM performance in ASR tasks. We propose pre-training LLMs on Pinyin embedding sequences, which represent pronunciation features, to generate corresponding Chinese characters. This step enables the LLM to adapt to generating text from pronunciation features before encountering real speech data. Furthermore, we fine-tune the LoRA parameters to enhance the LLM's understanding of speech modality information. In AISHELL-1 corpus, our approach yields a 9.5% relative improvement in ASR tasks compared to the baseline without Pinyi-to-Character pre-training. Additionally, incorporating auxiliary text data for Pinyi-to-Character pre-training further boosts performance, achieving a 19.0% relative improvement.

**Link**: [arxiv](http://arxiv.org/abs/2409.16005v1),  [pdf](http://arxiv.org/pdf/2409.16005v1)

**Tags**: cs.CL cs.SD eess.AS 



### Large Language Models as Carriers of Hidden Messages
**Authors**: Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki

**Updated**: 2024-09-24T12:00:29Z

**Summary**: Simple fine-tuning can embed hidden text into large language models (LLMs), which is revealed only when triggered by a specific query. Applications include LLM fingerprinting, where a unique identifier is embedded to verify licensing compliance, and steganography, where the LLM carries hidden messages disclosed through a trigger query.   Our work demonstrates that embedding hidden text via fine-tuning, although seemingly secure due to the vast number of potential triggers, is vulnerable to extraction through analysis of the LLM's output decoding process. We introduce an extraction attack called Unconditional Token Forcing (UTF), which iteratively feeds tokens from the LLM's vocabulary to reveal sequences with high token probabilities, indicating hidden text candidates. We also present Unconditional Token Forcing Confusion (UTFC), a defense paradigm that makes hidden text resistant to all known extraction attacks without degrading the general performance of LLMs compared to standard fine-tuning. UTFC has both benign (improving LLM fingerprinting) and malign applications (using LLMs to create covert communication channels).

**Link**: [arxiv](http://arxiv.org/abs/2406.02481v4),  [pdf](http://arxiv.org/pdf/2406.02481v4)

**Tags**: cs.CL cs.CR 



### GPT-4 as a Homework Tutor can Improve Student Engagement and Learning   Outcomes
**Authors**: Alessandro Vanzo, Sankalan Pal Chowdhury, Mrinmaya Sachan

**Updated**: 2024-09-24T11:22:55Z

**Summary**: This work contributes to the scarce empirical literature on LLM-based interactive homework in real-world educational settings and offers a practical, scalable solution for improving homework in schools. Homework is an important part of education in schools across the world, but in order to maximize benefit, it needs to be accompanied with feedback and followup questions. We developed a prompting strategy that enables GPT-4 to conduct interactive homework sessions for high-school students learning English as a second language. Our strategy requires minimal efforts in content preparation, one of the key challenges of alternatives like home tutors or ITSs. We carried out a Randomized Controlled Trial (RCT) in four high-school classes, replacing traditional homework with GPT-4 homework sessions for the treatment group. We observed significant improvements in learning outcomes, specifically a greater gain in grammar, and student engagement. In addition, students reported high levels of satisfaction with the system and wanted to continue using it after the end of the RCT.

**Link**: [arxiv](http://arxiv.org/abs/2409.15981v1),  [pdf](http://arxiv.org/pdf/2409.15981v1)

**Tags**: cs.CY 



### Finetuning LLMs for Comparative Assessment Tasks
**Authors**: Vatsal Raina, Adian Liusie, Mark Gales

**Updated**: 2024-09-24T11:21:43Z

**Summary**: Automated assessment in natural language generation is a challenging task. Instruction-tuned large language models (LLMs) have shown promise in reference-free evaluation, particularly through comparative assessment. However, the quadratic computational complexity of pairwise comparisons limits its scalability. To address this, efficient comparative assessment has been explored by applying comparative strategies on zero-shot LLM probabilities. We propose a framework for finetuning LLMs for comparative assessment to align the model's output with the target distribution of comparative probabilities. By training on soft probabilities, our approach improves state-of-the-art performance while maintaining high performance with an efficient subset of comparisons.

**Link**: [arxiv](http://arxiv.org/abs/2409.15979v1),  [pdf](http://arxiv.org/pdf/2409.15979v1)

**Tags**: cs.CL 



### "Flipped" University: LLM-Assisted Lifelong Learning Environment
**Authors**: Kirill Krinkin, Tatiana Berlenko

**Updated**: 2024-09-24T11:00:38Z

**Summary**: The rapid development of artificial intelligence technologies, particularly Large Language Models (LLMs), has revolutionized the landscape of lifelong learning. This paper introduces a conceptual framework for a self-constructed lifelong learning environment supported by LLMs. It highlights the inadequacies of traditional education systems in keeping pace with the rapid deactualization of knowledge and skills. The proposed framework emphasizes the transformation from institutionalized education to personalized, self-driven learning. It leverages the natural language capabilities of LLMs to provide dynamic and adaptive learning experiences, facilitating the creation of personal intellectual agents that assist in knowledge acquisition. The framework integrates principles of lifelong learning, including the necessity of building personal world models, the dual modes of learning (training and exploration), and the creation of reusable learning artifacts. Additionally, it underscores the importance of curiosity-driven learning and reflective practices in maintaining an effective learning trajectory. The paper envisions the evolution of educational institutions into "flipped" universities, focusing on supporting global knowledge consistency rather than merely structuring and transmitting knowledge.

**Link**: [arxiv](http://arxiv.org/abs/2409.10553v2),  [pdf](http://arxiv.org/pdf/2409.10553v2)

**Tags**: cs.CY 



### TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for   Robotic Manipulation
**Authors**: Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, Yaxin Peng, Feifei Feng, Jian Tang

**Updated**: 2024-09-24T10:57:18Z

**Summary**: Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2409.12514v2),  [pdf](http://arxiv.org/pdf/2409.12514v2)

**Tags**: cs.RO cs.CV 



### Watch Every Step! LLM Agent Learning via Iterative Step-Level Process   Refinement
**Authors**: Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang, Ke Wang, Cheng Li, Wei Peng, Sujian Li

**Updated**: 2024-09-24T10:01:31Z

**Summary**: Large language model agents have exhibited exceptional performance across a range of complex interactive tasks. Recent approaches have utilized tuning with expert trajectories to enhance agent performance, yet they primarily concentrate on outcome rewards, which may lead to errors or suboptimal actions due to the absence of process supervision signals. In this paper, we introduce the Iterative step-level Process Refinement (IPR) framework, which provides detailed step-by-step guidance to enhance agent training. Specifically, we adopt the Monte Carlo method to estimate step-level rewards. During each iteration, the agent explores along the expert trajectory and generates new actions. These actions are then evaluated against the corresponding step of expert trajectory using step-level rewards. Such comparison helps identify discrepancies, yielding contrastive action pairs that serve as training data for the agent. Our experiments on three complex agent tasks demonstrate that our framework outperforms a variety of strong baselines. Moreover, our analytical findings highlight the effectiveness of IPR in augmenting action efficiency and its applicability to diverse models.

**Link**: [arxiv](http://arxiv.org/abs/2406.11176v2),  [pdf](http://arxiv.org/pdf/2406.11176v2)

**Tags**: cs.CL cs.AI cs.LG 



### Automated test generation to evaluate tool-augmented LLMs as   conversational AI agents
**Authors**: Samuel Arcadinho, David Aparicio, Mariana Almeida

**Updated**: 2024-09-24T09:57:43Z

**Summary**: Tool-augmented LLMs are a promising approach to create AI agents that can have realistic conversations, follow procedures, and call appropriate functions. However, evaluating them is challenging due to the diversity of possible conversations, and existing datasets focus only on single interactions and function-calling. We present a test generation pipeline to evaluate LLMs as conversational AI agents. Our framework uses LLMs to generate diverse tests grounded on user-defined procedures. For that, we use intermediate graphs to limit the LLM test generator's tendency to hallucinate content that is not grounded on input procedures, and enforces high coverage of the possible conversations. Additionally, we put forward ALMITA, a manually curated dataset for evaluating AI agents in customer support, and use it to evaluate existing LLMs. Our results show that while tool-augmented LLMs perform well in single interactions, they often struggle to handle complete conversations. While our focus is on customer support, our method is general and capable of AI agents for different domains.

**Link**: [arxiv](http://arxiv.org/abs/2409.15934v1),  [pdf](http://arxiv.org/pdf/2409.15934v1)

**Tags**: cs.CL cs.AI cs.LG 



### SLIMER-IT: Zero-Shot NER on Italian Language
**Authors**: Andrew Zamai, Leonardo Rigutini, Marco Maggini, Andrea Zugarini

**Updated**: 2024-09-24T09:57:25Z

**Summary**: Traditional approaches to Named Entity Recognition (NER) frame the task into a BIO sequence labeling problem. Although these systems often excel in the downstream task at hand, they require extensive annotated data and struggle to generalize to out-of-distribution input domains and unseen entity types. On the contrary, Large Language Models (LLMs) have demonstrated strong zero-shot capabilities. While several works address Zero-Shot NER in English, little has been done in other languages. In this paper, we define an evaluation framework for Zero-Shot NER, applying it to the Italian language. Furthermore, we introduce SLIMER-IT, the Italian version of SLIMER, an instruction-tuning approach for zero-shot NER leveraging prompts enriched with definition and guidelines. Comparisons with other state-of-the-art models, demonstrate the superiority of SLIMER-IT on never-seen-before entity tags.

**Link**: [arxiv](http://arxiv.org/abs/2409.15933v1),  [pdf](http://arxiv.org/pdf/2409.15933v1)

**Tags**: cs.CL cs.IR 



### GPTCast: a weather language model for precipitation nowcasting
**Authors**: Gabriele Franch, Elena Tomasi, Rishabh Wanjari, Virginia Poli, Chiara Cardinali, Pier Paolo Alberoni, Marco Cristoforetti

**Updated**: 2024-09-24T09:50:58Z

**Summary**: This work introduces GPTCast, a generative deep-learning method for ensemble nowcast of radar-based precipitation, inspired by advancements in large language models (LLMs). We employ a GPT model as a forecaster to learn spatiotemporal precipitation dynamics using tokenized radar images. The tokenizer is based on a Quantized Variational Autoencoder featuring a novel reconstruction loss tailored for the skewed distribution of precipitation that promotes faithful reconstruction of high rainfall rates. The approach produces realistic ensemble forecasts and provides probabilistic outputs with accurate uncertainty estimation. The model is trained without resorting to randomness, all variability is learned solely from the data and exposed by model at inference for ensemble generation. We train and test GPTCast using a 6-year radar dataset over the Emilia-Romagna region in Northern Italy, showing superior results compared to state-of-the-art ensemble extrapolation methods.

**Link**: [arxiv](http://arxiv.org/abs/2407.02089v2),  [pdf](http://arxiv.org/pdf/2407.02089v2)

**Tags**: cs.LG physics.ao-ph 



### Knowledge Editing in Language Models via Adapted Direct Preference   Optimization
**Authors**: Amit Rozner, Barak Battash, Lior Wolf, Ofir Lindenbaum

**Updated**: 2024-09-24T09:48:36Z

**Summary**: Large Language Models (LLMs) can become outdated over time as they may lack updated world knowledge, leading to factual knowledge errors and gaps. Knowledge Editing (KE) aims to overcome this challenge using weight updates that do not require expensive retraining. We propose treating KE as an LLM alignment problem. Toward this goal, we introduce Knowledge Direct Preference Optimization (KDPO), a variation of the Direct Preference Optimization (DPO) that is more effective for knowledge modifications. Our method is based on an online approach that continually updates the knowledge stored in the model. We use the current knowledge as a negative sample and the new knowledge we want to introduce as a positive sample in a process called DPO. We also use teacher-forcing for negative sample generation and optimize using the positive sample, which helps maintain localized changes. We tested our KE method on various datasets and models, comparing it to several cutting-edge methods, with 100 and 500 sequential edits. Additionally, we conducted an ablation study comparing our method to the standard DPO approach. Our experimental results show that our modified DPO method allows for more refined KE, achieving similar or better performance compared to previous methods.

**Link**: [arxiv](http://arxiv.org/abs/2406.09920v2),  [pdf](http://arxiv.org/pdf/2406.09920v2)

**Tags**: cs.CL cs.AI 



### Planning in the Dark: LLM-Symbolic Planning Pipeline without Experts
**Authors**: Sukai Huang, Nir Lipovetzky, Trevor Cohn

**Updated**: 2024-09-24T09:33:12Z

**Summary**: Large Language Models (LLMs) have shown promise in solving natural language-described planning tasks, but their direct use often leads to inconsistent reasoning and hallucination. While hybrid LLM-symbolic planning pipelines have emerged as a more robust alternative, they typically require extensive expert intervention to refine and validate generated action schemas. It not only limits scalability but also introduces a potential for biased interpretation, as a single expert's interpretation of ambiguous natural language descriptions might not align with the user's actual intent. To address this, we propose a novel approach that constructs an action schema library to generate multiple candidates, accounting for the diverse possible interpretations of natural language descriptions. We further introduce a semantic validation and ranking module that automatically filter and rank the generated schemas and plans without expert-in-the-loop. The experiments showed our pipeline maintains superiority in planning over the direct LLM planning approach. These findings demonstrate the feasibility of a fully automated end-to-end LLM-symbolic planner that requires no expert intervention, opening up the possibility for a broader audience to engage with AI planning with less prerequisite of domain expertise.

**Link**: [arxiv](http://arxiv.org/abs/2409.15915v1),  [pdf](http://arxiv.org/pdf/2409.15915v1)

**Tags**: cs.AI 



### Enhancing Text-to-SQL Capabilities of Large Language Models via Domain   Database Knowledge Injection
**Authors**: Xingyu Ma, Xin Tian, Lingxiang Wu, Xuepeng Wang, Xueming Tang, Jinqiao Wang

**Updated**: 2024-09-24T09:24:03Z

**Summary**: Text-to-SQL is a subtask in semantic parsing that has seen rapid progress with the evolution of Large Language Models (LLMs). However, LLMs face challenges due to hallucination issues and a lack of domain-specific database knowledge(such as table schema and cell values). As a result, they can make errors in generating table names, columns, and matching values to the correct columns in SQL statements. This paper introduces a method of knowledge injection to enhance LLMs' ability to understand schema contents by incorporating prior knowledge. This approach improves their performance in Text-to-SQL tasks. Experimental results show that pre-training LLMs on domain-specific database knowledge and fine-tuning them on downstream Text-to-SQL tasks significantly improves the Execution Match (EX) and Exact Match (EM) metrics across various models. This effectively reduces errors in generating column names and matching values to the columns. Furthermore, the knowledge-injected models can be applied to many downstream Text-to-SQL tasks, demonstrating the generalizability of the approach presented in this paper.

**Link**: [arxiv](http://arxiv.org/abs/2409.15907v1),  [pdf](http://arxiv.org/pdf/2409.15907v1)

**Tags**: cs.CL cs.AI 



### Boosting Code-Switching ASR with Mixture of Experts Enhanced   Speech-Conditioned LLM
**Authors**: Fengrun Zhang, Wang Geng, Hukai Huang, Cheng Yi, He Qu

**Updated**: 2024-09-24T09:20:22Z

**Summary**: In this paper, we introduce a speech-conditioned Large Language Model (LLM) integrated with a Mixture of Experts (MoE) based connector to address the challenge of Code-Switching (CS) in Automatic Speech Recognition (ASR). Specifically, we propose an Insertion and Deletion of Interruption Token (IDIT) mechanism for better transfer text generation ability of LLM to speech recognition task. We also present a connecter with MoE architecture that manages multiple languages efficiently. To further enhance the collaboration of multiple experts and leverage the understanding capabilities of LLM, we propose a two-stage progressive training strategy: 1) The connector is unfrozen and trained with language-specialized experts to map speech representations to the text space. 2) The connector and LLM LoRA adaptor are trained with the proposed IDIT mechanism and all experts are activated to learn general representations. Experimental results demonstrate that our method significantly outperforms state-of-the-art models, including end-to-end and large-scale audio-language models.

**Link**: [arxiv](http://arxiv.org/abs/2409.15905v1),  [pdf](http://arxiv.org/pdf/2409.15905v1)

**Tags**: cs.SD cs.AI eess.AS 



### HLB: Benchmarking LLMs' Humanlikeness in Language Use
**Authors**: Xufeng Duan, Bei Xiao, Xuemei Tang, Zhenguang G. Cai

**Updated**: 2024-09-24T09:02:28Z

**Summary**: As synthetic data becomes increasingly prevalent in training language models, particularly through generated dialogue, concerns have emerged that these models may deviate from authentic human language patterns, potentially losing the richness and creativity inherent in human communication. This highlights the critical need to assess the humanlikeness of language models in real-world language use. In this paper, we present a comprehensive humanlikeness benchmark (HLB) evaluating 20 large language models (LLMs) using 10 psycholinguistic experiments designed to probe core linguistic aspects, including sound, word, syntax, semantics, and discourse (see https://huggingface.co/spaces/XufengDuan/HumanLikeness). To anchor these comparisons, we collected responses from over 2,000 human participants and compared them to outputs from the LLMs in these experiments.   For rigorous evaluation, we developed a coding algorithm that accurately identified language use patterns, enabling the extraction of response distributions for each task. By comparing the response distributions between human participants and LLMs, we quantified humanlikeness through distributional similarity. Our results reveal fine-grained differences in how well LLMs replicate human responses across various linguistic levels. Importantly, we found that improvements in other performance metrics did not necessarily lead to greater humanlikeness, and in some cases, even resulted in a decline. By introducing psycholinguistic methods to model evaluation, this benchmark offers the first framework for systematically assessing the humanlikeness of LLMs in language use.

**Link**: [arxiv](http://arxiv.org/abs/2409.15890v1),  [pdf](http://arxiv.org/pdf/2409.15890v1)

**Tags**: cs.CL 



### Automatic Bottom-Up Taxonomy Construction: A Software Application Domain   Study
**Authors**: Cezar Sas, Andrea Capiluppi

**Updated**: 2024-09-24T08:55:07Z

**Summary**: Previous research in software application domain classification has faced challenges due to the lack of a proper taxonomy that explicitly models relations between classes. As a result, current solutions are less effective for real-world usage. This study aims to develop a comprehensive software application domain taxonomy by integrating multiple datasources and leveraging ensemble methods. The goal is to overcome the limitations of individual sources and configurations by creating a more robust, accurate, and reproducible taxonomy. This study employs a quantitative research design involving three different datasources: an existing Computer Science Ontology (CSO), Wikidata, and LLMs. The study utilises a combination of automated and human evaluations to assess the quality of a taxonomy. The outcome measures include the number of unlinked terms, self-loops, and overall connectivity of the taxonomy. The results indicate that individual datasources have advantages and drawbacks: the CSO datasource showed minimal variance across different configurations, but a notable issue of missing technical terms and a high number of self-loops. The Wikipedia datasource required significant filtering during construction to improve metric performance. LLM-generated taxonomies demonstrated better performance when using context-rich prompts. An ensemble approach showed the most promise, successfully reducing the number of unlinked terms and self-loops, thus creating a more connected and comprehensive taxonomy. The study addresses the construction of a software application domain taxonomy relying on pre-existing resources. Our results indicate that an ensemble approach to taxonomy construction can effectively address the limitations of individual datasources. Future work should focus on refining the ensemble techniques and exploring additional datasources to enhance the taxonomy's accuracy and completeness.

**Link**: [arxiv](http://arxiv.org/abs/2409.15881v1),  [pdf](http://arxiv.org/pdf/2409.15881v1)

**Tags**: cs.SE 



### CamelEval: Advancing Culturally Aligned Arabic Language Models and   Benchmarks
**Authors**: Zhaozhi Qian, Faroq Altam, Muhammad Alqurishi, Riad Souissi

**Updated**: 2024-09-24T08:49:21Z

**Summary**: Large Language Models (LLMs) are the cornerstones of modern artificial intelligence systems. This paper introduces Juhaina, a Arabic-English bilingual LLM specifically designed to align with the values and preferences of Arabic speakers. Juhaina inherently supports advanced functionalities such as instruction following, open-ended question answering, information provisioning, and text processing. Our model contains 9.24 billion parameters and is trained on a context window of up to 8,192 tokens. This paper details the creation process of Juhaina and provides an extensive empirical evaluation. Furthermore, we identify the limitations of widely-adopted Open Arabic LLM Leaderboard (OALL) and propose a new evaluation benchmark, CamelEval. Our findings demonstrate that Juhaina surpasses existing LLMs of comparable sizes, such as the Llama and Gemma families, in generating helpful responses in Arabic, providing factually accurate information about the region, and understanding nuanced cultural aspects. We aspire for Juhaina to democratize cutting-edge AI technologies, serving over 400 million Arabic speakers by offering LLMs that not only communicate in their language but also comprehend their culture. We publicly release all models on Huggingface \url{https://huggingface.co/elmrc}.

**Link**: [arxiv](http://arxiv.org/abs/2409.12623v2),  [pdf](http://arxiv.org/pdf/2409.12623v2)

**Tags**: cs.CL cs.AI 



### Privacy Evaluation Benchmarks for NLP Models
**Authors**: Wei Huang, Yinggui Wang, Cen Chen

**Updated**: 2024-09-25T07:03:05Z

**Summary**: By inducing privacy attacks on NLP models, attackers can obtain sensitive information such as training data and model parameters, etc. Although researchers have studied, in-depth, several kinds of attacks in NLP models, they are non-systematic analyses. It lacks a comprehensive understanding of the impact caused by the attacks. For example, we must consider which scenarios can apply to which attacks, what the common factors are that affect the performance of different attacks, the nature of the relationships between different attacks, and the influence of various datasets and models on the effectiveness of the attacks, etc. Therefore, we need a benchmark to holistically assess the privacy risks faced by NLP models. In this paper, we present a privacy attack and defense evaluation benchmark in the field of NLP, which includes the conventional/small models and large language models (LLMs). This benchmark supports a variety of models, datasets, and protocols, along with standardized modules for comprehensive evaluation of attacks and defense strategies. Based on the above framework, we present a study on the association between auxiliary data from different domains and the strength of privacy attacks. And we provide an improved attack method in this scenario with the help of Knowledge Distillation (KD). Furthermore, we propose a chained framework for privacy attacks. Allowing a practitioner to chain multiple attacks to achieve a higher-level attack objective. Based on this, we provide some defense and enhanced attack strategies. The code for reproducing the results can be found at https://github.com/user2311717757/nlp_doctor.

**Link**: [arxiv](http://arxiv.org/abs/2409.15868v2),  [pdf](http://arxiv.org/pdf/2409.15868v2)

**Tags**: cs.CL cs.LG 



### BeSimulator: A Large Language Model Powered Text-based Behavior   Simulator
**Authors**: Jianan Wang, Bin Li, Xueying Wang, Fu Li, Yunlong Wu, Juan Chen, Xiaodong Yi

**Updated**: 2024-09-24T08:37:04Z

**Summary**: Traditional robot simulators focus on physical process modeling and realistic rendering, often suffering from high computational costs, inefficiencies, and limited adaptability. To handle this issue, we propose Behavior Simulation in robotics to emphasize checking the behavior logic of robots and achieving sufficient alignment between the outcome of robot actions and real scenarios. In this paper, we introduce BeSimulator, a modular and novel LLM-powered framework, as an attempt towards behavior simulation in the context of text-based environments. By constructing text-based virtual environments and performing semantic-level simulation, BeSimulator can generalize across scenarios and achieve long-horizon complex simulation. Inspired by human cognition processes, it employs a "consider-decide-capture-transfer" methodology, termed Chain of Behavior Simulation, which excels at analyzing action feasibility and state transitions. Additionally, BeSimulator incorporates code-driven reasoning to enable arithmetic operations and enhance reliability, as well as integrates reflective feedback to refine simulation. Based on our manually constructed behavior-tree-based simulation benchmark BTSIMBENCH, our experiments show a significant performance improvement in behavior simulation compared to baselines, ranging from 14.7% to 26.6%.

**Link**: [arxiv](http://arxiv.org/abs/2409.15865v1),  [pdf](http://arxiv.org/pdf/2409.15865v1)

**Tags**: cs.RO cs.AI cs.CL 



### A Zero-Shot Open-Vocabulary Pipeline for Dialogue Understanding
**Authors**: Abdulfattah Safa, G√∂zde G√ºl ≈ûahin

**Updated**: 2024-09-24T08:33:41Z

**Summary**: Dialogue State Tracking (DST) is crucial for understanding user needs and executing appropriate system actions in task-oriented dialogues. Majority of existing DST methods are designed to work within predefined ontologies and assume the availability of gold domain labels, struggling with adapting to new slots values. While Large Language Models (LLMs)-based systems show promising zero-shot DST performance, they either require extensive computational resources or they underperform existing fully-trained systems, limiting their practicality. To address these limitations, we propose a zero-shot, open-vocabulary system that integrates domain classification and DST in a single pipeline. Our approach includes reformulating DST as a question-answering task for less capable models and employing self-refining prompts for more adaptable ones. Our system does not rely on fixed slot values defined in the ontology allowing the system to adapt dynamically. We compare our approach with existing SOTA, and show that it provides up to 20% better Joint Goal Accuracy (JGA) over previous methods on datasets like Multi-WOZ 2.1, with up to 90% fewer requests to the LLM API.

**Link**: [arxiv](http://arxiv.org/abs/2409.15861v1),  [pdf](http://arxiv.org/pdf/2409.15861v1)

**Tags**: cs.CL cs.AI 



### KAG: Boosting LLMs in Professional Domains via Knowledge Augmented   Generation
**Authors**: Lei Liang, Mengshu Sun, Zhengke Gui, Zhongshu Zhu, Zhouyu Jiang, Ling Zhong, Yuan Qu, Peilong Zhao, Zhongpu Bo, Jin Yang, Huaidong Xiong, Lin Yuan, Jun Xu, Zaoyang Wang, Zhiqiang Zhang, Wen Zhang, Huajun Chen, Wenguang Chen, Jun Zhou

**Updated**: 2024-09-24T08:24:39Z

**Summary**: The recently developed retrieval-augmented generation (RAG) technology has enabled the efficient construction of domain-specific applications. However, it also has limitations, including the gap between vector similarity and the relevance of knowledge reasoning, as well as insensitivity to knowledge logic, such as numerical values, temporal relations, expert rules, and others, which hinder the effectiveness of professional knowledge services. In this work, we introduce a professional domain knowledge service framework called Knowledge Augmented Generation (KAG). KAG is designed to address the aforementioned challenges with the motivation of making full use of the advantages of knowledge graph(KG) and vector retrieval, and to improve generation and reasoning performance by bidirectionally enhancing large language models (LLMs) and KGs through five key aspects: (1) LLM-friendly knowledge representation, (2) mutual-indexing between knowledge graphs and original chunks, (3) logical-form-guided hybrid reasoning engine, (4) knowledge alignment with semantic reasoning, and (5) model capability enhancement for KAG. We compared KAG with existing RAG methods in multihop question answering and found that it significantly outperforms state-of-theart methods, achieving a relative improvement of 19.6% on 2wiki and 33.5% on hotpotQA in terms of F1 score. We have successfully applied KAG to two professional knowledge Q&A tasks of Ant Group, including E-Government Q&A and E-Health Q&A, achieving significant improvement in professionalism compared to RAG methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.13731v2),  [pdf](http://arxiv.org/pdf/2409.13731v2)

**Tags**: cs.CL cs.AI 



### Inferring Scientific Cross-Document Coreference and Hierarchy with   Definition-Augmented Relational Reasoning
**Authors**: Lior Forer, Tom Hope

**Updated**: 2024-09-24T08:18:34Z

**Summary**: We address the fundamental task of inferring cross-document coreference and hierarchy in scientific texts, which has important applications in knowledge graph construction, search, recommendation and discovery. LLMs can struggle when faced with many long-tail technical concepts with nuanced variations. We present a novel method which generates context-dependent definitions of concept mentions by retrieving full-text literature, and uses the definitions to enhance detection of cross-document relations. We further generate relational definitions, which describe how two concept mentions are related or different, and design an efficient re-ranking approach to address the combinatorial explosion involved in inferring links across papers. In both fine-tuning and in-context learning settings we achieve large gains in performance. We provide analysis of generated definitions, shedding light on the relational reasoning ability of LLMs over fine-grained scientific concepts.

**Link**: [arxiv](http://arxiv.org/abs/2409.15113v2),  [pdf](http://arxiv.org/pdf/2409.15113v2)

**Tags**: cs.CL 



### First Field Trial of LLM-Powered AI Agent for Lifecycle Management of   Autonomous Driving Optical Networks
**Authors**: Xiaomin Liu, Qizhi Qiu, Yihao Zhang, Yuming Cheng, Lilin Yi, Weisheng Hu, Qunbi Zhuge

**Updated**: 2024-09-24T08:02:21Z

**Summary**: We design and demonstrate the first field trial of LLM-powered AI Agent for ADON. Three operation modes of the Agent are proposed for network lifecycle management. The Agent efficiently processes wavelength add/drop and soft/hard failures, and achieves comparable performance to human-designed algorithms for power optimization.

**Link**: [arxiv](http://arxiv.org/abs/2409.14605v2),  [pdf](http://arxiv.org/pdf/2409.14605v2)

**Tags**: eess.SY cs.SY 



### CHECKWHY: Causal Fact Verification via Argument Structure
**Authors**: Jiasheng Si, Yibo Zhao, Yingjie Zhu, Haiyang Zhu, Wenpeng Lu, Deyu Zhou

**Updated**: 2024-09-24T07:44:27Z

**Summary**: With the growing complexity of fact verification tasks, the concern with "thoughtful" reasoning capabilities is increasing. However, recent fact verification benchmarks mainly focus on checking a narrow scope of semantic factoids within claims and lack an explicit logical reasoning process. In this paper, we introduce CheckWhy, a challenging dataset tailored to a novel causal fact verification task: checking the truthfulness of the causal relation within claims through rigorous reasoning steps. CheckWhy consists of over 19K "why" claim-evidence-argument structure triplets with supports, refutes, and not enough info labels. Each argument structure is composed of connected evidence, representing the reasoning process that begins with foundational evidence and progresses toward claim establishment. Through extensive experiments on state-of-the-art models, we validate the importance of incorporating the argument structure for causal fact verification. Moreover, the automated and human evaluation of argument structure generation reveals the difficulty in producing satisfying argument structure by fine-tuned models or Chain-of-Thought prompted LLMs, leaving considerable room for future improvements.

**Link**: [arxiv](http://arxiv.org/abs/2408.10918v2),  [pdf](http://arxiv.org/pdf/2408.10918v2)

**Tags**: cs.CL 



### CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language   Models
**Authors**: Yaojia Lv, Haojie Pan, Zekun Wang, Jiafeng Liang, Yuanxing Liu, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin

**Updated**: 2024-09-24T07:41:19Z

**Summary**: Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows.

**Link**: [arxiv](http://arxiv.org/abs/2401.08438v2),  [pdf](http://arxiv.org/pdf/2401.08438v2)

**Tags**: cs.CL cs.AI cs.LG 



### Unveiling Language Competence Neurons: A Psycholinguistic Approach to   Model Interpretability
**Authors**: Xufeng Duan, Xinyu Zhou, Bei Xiao, Zhenguang G. Cai

**Updated**: 2024-09-24T07:40:33Z

**Summary**: As large language models (LLMs) become advance in their linguistic capacity, understanding how they capture aspects of language competence remains a significant challenge. This study therefore employs psycholinguistic paradigms, which are well-suited for probing deeper cognitive aspects of language processing, to explore neuron-level representations in language model across three tasks: sound-shape association, sound-gender association, and implicit causality. Our findings indicate that while GPT-2-XL struggles with the sound-shape task, it demonstrates human-like abilities in both sound-gender association and implicit causality. Targeted neuron ablation and activation manipulation reveal a crucial relationship: when GPT-2-XL displays a linguistic ability, specific neurons correspond to that competence; conversely, the absence of such an ability indicates a lack of specialized neurons. This study is the first to utilize psycholinguistic experiments to investigate deep language competence at the neuron level, providing a new level of granularity in model interpretability and insights into the internal mechanisms driving language ability in transformer based LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2409.15827v1),  [pdf](http://arxiv.org/pdf/2409.15827v1)

**Tags**: cs.CL 



### Empirical Insights on Fine-Tuning Large Language Models for   Question-Answering
**Authors**: Junjie Ye, Yuming Yang, Qi Zhang, Tao Gui, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan

**Updated**: 2024-09-24T07:38:38Z

**Summary**: Large language models (LLMs) encode extensive world knowledge through pre-training on massive datasets, which can then be fine-tuned for the question-answering (QA) task. However, effective strategies for fine-tuning LLMs for the QA task remain largely unexplored. To address this gap, we categorize supervised fine-tuning (SFT) data based on the extent of knowledge memorized by the pretrained LLMs and conduct a series of empirical analyses. Our experiments, involving four LLMs from three different model families, focus on three key factors: the amount of data required for SFT, the impact of different SFT datasets on model performance, and how data requirements vary across LLMs. The results show that as few as 60 data points during the SFT stage can activate the knowledge encoded during pre-training, enabling LLMs to perform the QA task. Additionally, SFT with data of varying memory levels has a significant impact on LLM performance, with the optimal dataset differing based on the specific model being fine-tuned. Future research will delve deeper into the mechanisms underlying these phenomena.

**Link**: [arxiv](http://arxiv.org/abs/2409.15825v1),  [pdf](http://arxiv.org/pdf/2409.15825v1)

**Tags**: cs.CL cs.AI 



### Supervised Fine-Tuning: An Activation Pattern Optimization Process for   Attention Heads
**Authors**: Yang Zhao, Li Du, Xiao Ding, Kai Xiong, Ting Liu, Bing Qin

**Updated**: 2024-09-24T07:34:50Z

**Summary**: Though demonstrating promising potential, LLMs' performance on complex tasks, such as advanced mathematics and complex disease diagnosis is still unsatisfactory. A key issue is the present LLMs learn in a data-driven schema, while the instruction dataset about these complex tasks is both scarce and hard to collect or construct. On the contrary, a prominent phenomenon is that LLMs can learn rather fast on those simpler tasks with adequate prior knowledge captured during pretraining stage. Thus, if the prerequisite and mechanism of such rapid generalization could be elucidated, it could be highly beneficial in enhancing the efficiency and effectiveness of the LLM's ability to learn complex tasks. Thus, in this paper, we employ a gradient-based method, to dissect the process that the SFT process adapts LLMs to downstream tasks via the perspective of attention patterns. We find that: (1) LLMs selectively activate task-specific attention heads during SFT; (2) activation patterns for complex tasks are combinations of basic task patterns; and (3) changes in a few parameters can significantly impact activation patterns after SFT on a small number of samples. Based on these insights, we conduct experiments to examine whether these conclusions could effectively enhance the efficiency and effectiveness of SFT, particularly in handling complex tasks and when instructional resources are scarce. Our research not only uncovers the underlying reasons behind LLMs' rapid learning and generalization mechanisms but also provides practical solutions for addressing data challenges in complex and specialized tasks.

**Link**: [arxiv](http://arxiv.org/abs/2409.15820v1),  [pdf](http://arxiv.org/pdf/2409.15820v1)

**Tags**: cs.LG cs.CL 



### SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs   and Agents
**Authors**: Gabriele Fossi, Youssef Boulaimen, Leila Outemzabet, Nathalie Jeanray, Stephane Gerart, Sebastien Vachenc, Joanna Giemza, Salvatore Raieli

**Updated**: 2024-09-24T07:29:05Z

**Summary**: The advancement of artificial intelligence algorithms has expanded their application to several fields such as the biomedical domain. Artificial intelligence systems, including Large Language Models (LLMs), can be particularly advantageous in drug discovery, which is a very long and expensive process. However, LLMs by themselves lack in-depth knowledge about specific domains and can generate factually incorrect information. Moreover, they are not able to perform more complex actions that imply the usage of external tools. Our work is focused on these two issues. Firstly, we show how the implementation of an advanced RAG system can help the LLM to generate more accurate answers to drug-discovery-related questions. The results show that the answers generated by the LLM with the RAG system surpass in quality the answers produced by the model without RAG. Secondly, we show how to create an automatic target dossier using LLMs and incorporating them with external tools that they can use to execute more intricate tasks to gather data such as accessing databases and executing code. The result is a production-ready target dossier containing the acquired information summarized into a PDF and a PowerPoint presentation.

**Link**: [arxiv](http://arxiv.org/abs/2409.15817v1),  [pdf](http://arxiv.org/pdf/2409.15817v1)

**Tags**: cs.AI 68T07, 92C50, 68T09 I.2.7; J.3 



### Joint Pruning and Channel-wise Mixed-Precision Quantization for   Efficient Deep Neural Networks
**Authors**: Beatrice Alessandra Motetti, Matteo Risso, Alessio Burrello, Enrico Macii, Massimo Poncino, Daniele Jahier Pagliari

**Updated**: 2024-09-24T07:06:26Z

**Summary**: The resource requirements of deep neural networks (DNNs) pose significant challenges to their deployment on edge devices. Common approaches to address this issue are pruning and mixed-precision quantization, which lead to latency and memory occupation improvements. These optimization techniques are usually applied independently. We propose a novel methodology to apply them jointly via a lightweight gradient-based search, and in a hardware-aware manner, greatly reducing the time required to generate Pareto-optimal DNNs in terms of accuracy versus cost (i.e., latency or memory). We test our approach on three edge-relevant benchmarks, namely CIFAR-10, Google Speech Commands, and Tiny ImageNet. When targeting the optimization of the memory footprint, we are able to achieve a size reduction of 47.50% and 69.54% at iso-accuracy with the baseline networks with all weights quantized at 8 and 2-bit, respectively. Our method surpasses a previous state-of-the-art approach with up to 56.17% size reduction at iso-accuracy. With respect to the sequential application of state-of-the-art pruning and mixed-precision optimizations, we obtain comparable or superior results, but with a significantly lowered training time. In addition, we show how well-tailored cost models can improve the cost versus accuracy trade-offs when targeting specific hardware for deployment.

**Link**: [arxiv](http://arxiv.org/abs/2407.01054v2),  [pdf](http://arxiv.org/pdf/2407.01054v2)

**Tags**: cs.LG 



### Thought-Path Contrastive Learning via Premise-Oriented Data Augmentation   for Logical Reading Comprehension
**Authors**: Chenxu Wang, Ping Jian, Zhen Yang

**Updated**: 2024-09-24T07:00:24Z

**Summary**: Logical reading comprehension is a challenging task that entails grasping the underlying semantics of text and applying reasoning to deduce the correct answer. Prior researches have primarily focused on enhancing logical reasoning capabilities through Chain-of-Thought (CoT) or data augmentation. However, previous work constructing chain-of-thought rationales concentrates solely on analyzing correct options, neglecting the incorrect alternatives. Addtionally, earlier efforts on data augmentation by altering contexts rely on rule-based methods, which result in generated contexts that lack diversity and coherence. To address these issues, we propose a Premise-Oriented Data Augmentation (PODA) framework. This framework can generate CoT rationales including analyses for both correct and incorrect options, while constructing diverse and high-quality counterfactual contexts from incorrect candidate options. We integrate summarizing premises and identifying premises for each option into rationales. Subsequently, we employ multi-step prompts with identified premises to construct counterfactual context. To facilitate the model's capabilities to better differentiate the reasoning process associated with each option, we introduce a novel thought-path contrastive learning method that compares reasoning paths between the original and counterfactual samples. Experimental results on three representative LLMs demonstrate that our method can improve the baselines substantially across two challenging logical reasoning benchmarks (ReClor and LogiQA 2.0). The data and code are released at https://github.com/lalalamdbf/TPReasoner.

**Link**: [arxiv](http://arxiv.org/abs/2409.14495v2),  [pdf](http://arxiv.org/pdf/2409.14495v2)

**Tags**: cs.CL cs.AI 



### WeSep: A Scalable and Flexible Toolkit Towards Generalizable Target   Speaker Extraction
**Authors**: Shuai Wang, Ke Zhang, Shaoxiong Lin, Junjie Li, Xuefei Wang, Meng Ge, Jianwei Yu, Yanmin Qian, Haizhou Li

**Updated**: 2024-09-24T06:47:12Z

**Summary**: Target speaker extraction (TSE) focuses on isolating the speech of a specific target speaker from overlapped multi-talker speech, which is a typical setup in the cocktail party problem. In recent years, TSE draws increasing attention due to its potential for various applications such as user-customized interfaces and hearing aids, or as a crutial front-end processing technologies for subsequential tasks such as speech recognition and speaker recongtion. However, there are currently few open-source toolkits or available pre-trained models for off-the-shelf usage. In this work, we introduce WeSep, a toolkit designed for research and practical applications in TSE. WeSep is featured with flexible target speaker modeling, scalable data management, effective on-the-fly data simulation, structured recipes and deployment support. The toolkit is publicly avaliable at \url{https://github.com/wenet-e2e/WeSep.}

**Link**: [arxiv](http://arxiv.org/abs/2409.15799v1),  [pdf](http://arxiv.org/pdf/2409.15799v1)

**Tags**: eess.AS cs.SD 



### Small Language Models: Survey, Measurements, and Insights
**Authors**: Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D. Lane, Mengwei Xu

**Updated**: 2024-09-24T06:36:56Z

**Summary**: Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters, we survey 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, in-context learning, mathematics, and coding. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.

**Link**: [arxiv](http://arxiv.org/abs/2409.15790v1),  [pdf](http://arxiv.org/pdf/2409.15790v1)

**Tags**: cs.CL cs.AI cs.LG 



### OPAL: Outlier-Preserved Microscaling Quantization Accelerator for   Generative Large Language Models
**Authors**: Jahyun Koo, Dahoon Park, Sangwoo Jung, Jaeha Kung

**Updated**: 2024-09-24T06:11:12Z

**Summary**: To overcome the burden on the memory size and bandwidth due to ever-increasing size of large language models (LLMs), aggressive weight quantization has been recently studied, while lacking research on quantizing activations. In this paper, we present a hardware-software co-design method that results in an energy-efficient LLM accelerator, named OPAL, for generation tasks. First of all, a novel activation quantization method that leverages the microscaling data format while preserving several outliers per sub-tensor block (e.g., four out of 128 elements) is proposed. Second, on top of preserving outliers, mixed precision is utilized that sets 5-bit for inputs to sensitive layers in the decoder block of an LLM, while keeping inputs to less sensitive layers to 3-bit. Finally, we present the OPAL hardware architecture that consists of FP units for handling outliers and vectorized INT multipliers for dominant non-outlier related operations. In addition, OPAL uses log2-based approximation on softmax operations that only requires shift and subtraction to maximize power efficiency. As a result, we are able to improve the energy efficiency by 1.6~2.2x, and reduce the area by 2.4~3.1x with negligible accuracy loss, i.e., <1 perplexity increase.

**Link**: [arxiv](http://arxiv.org/abs/2409.05902v3),  [pdf](http://arxiv.org/pdf/2409.05902v3)

**Tags**: cs.LG cs.AR cs.CL 



### CHBench: A Chinese Dataset for Evaluating Health in Large Language   Models
**Authors**: Chenlu Guo, Nuo Xu, Yi Chang, Yuan Wu

**Updated**: 2024-09-24T05:44:46Z

**Summary**: With the rapid development of large language models (LLMs), assessing their performance on health-related inquiries has become increasingly essential. It is critical that these models provide accurate and trustworthy health information, as their application in real-world contexts--where misinformation can have serious consequences for individuals seeking medical advice and support--depends on their reliability. In this work, we present CHBench, the first comprehensive Chinese Health-related Benchmark designed to evaluate LLMs' capabilities in understanding physical and mental health across diverse scenarios. CHBench includes 6,493 entries related to mental health and 2,999 entries focused on physical health, covering a broad spectrum of topics. This dataset serves as a foundation for evaluating Chinese LLMs' capacity to comprehend and generate accurate health-related information. Our extensive evaluations of four popular Chinese LLMs demonstrate that there remains considerable room for improvement in their understanding of health-related information. The code is available at https://github.com/TracyGuo2001/CHBench.

**Link**: [arxiv](http://arxiv.org/abs/2409.15766v1),  [pdf](http://arxiv.org/pdf/2409.15766v1)

**Tags**: cs.CL 



### IRSC: A Zero-shot Evaluation Benchmark for Information Retrieval through   Semantic Comprehension in Retrieval-Augmented Generation Scenarios
**Authors**: Hai Lin, Shaoxiong Zhan, Junyou Su, Haitao Zheng, Hui Wang

**Updated**: 2024-09-24T05:39:53Z

**Summary**: In Retrieval-Augmented Generation (RAG) tasks using Large Language Models (LLMs), the quality of retrieved information is critical to the final output. This paper introduces the IRSC benchmark for evaluating the performance of embedding models in multilingual RAG tasks. The benchmark encompasses five retrieval tasks: query retrieval, title retrieval, part-of-paragraph retrieval, keyword retrieval, and summary retrieval. Our research addresses the current lack of comprehensive testing and effective comparison methods for embedding models in RAG scenarios. We introduced new metrics: the Similarity of Semantic Comprehension Index (SSCI) and the Retrieval Capability Contest Index (RCCI), and evaluated models such as Snowflake-Arctic, BGE, GTE, and M3E. Our contributions include: 1) the IRSC benchmark, 2) the SSCI and RCCI metrics, and 3) insights into the cross-lingual limitations of embedding models. The IRSC benchmark aims to enhance the understanding and development of accurate retrieval systems in RAG tasks. All code and datasets are available at: https://github.com/Jasaxion/IRSC\_Benchmark

**Link**: [arxiv](http://arxiv.org/abs/2409.15763v1),  [pdf](http://arxiv.org/pdf/2409.15763v1)

**Tags**: cs.IR cs.AI 



### XTRUST: On the Multilingual Trustworthiness of Large Language Models
**Authors**: Yahan Li, Yi Wang, Yi Chang, Yuan Wu

**Updated**: 2024-09-24T05:38:33Z

**Summary**: Large language models (LLMs) have demonstrated remarkable capabilities across a range of natural language processing (NLP) tasks, capturing the attention of both practitioners and the broader public. A key question that now preoccupies the AI community concerns the capabilities and limitations of these models, with trustworthiness emerging as a central issue, particularly as LLMs are increasingly applied in sensitive fields like healthcare and finance, where errors can have serious consequences. However, most previous studies on the trustworthiness of LLMs have been limited to a single language, typically the predominant one in the dataset, such as English. In response to the growing global deployment of LLMs, we introduce XTRUST, the first comprehensive multilingual trustworthiness benchmark. XTRUST encompasses a diverse range of topics, including illegal activities, hallucination, out-of-distribution (OOD) robustness, physical and mental health, toxicity, fairness, misinformation, privacy, and machine ethics, across 10 different languages. Using XTRUST, we conduct an empirical evaluation of the multilingual trustworthiness of five widely used LLMs, offering an in-depth analysis of their performance across languages and tasks. Our results indicate that many LLMs struggle with certain low-resource languages, such as Arabic and Russian, highlighting the considerable room for improvement in the multilingual trustworthiness of current language models. The code is available at https://github.com/LluckyYH/XTRUST.

**Link**: [arxiv](http://arxiv.org/abs/2409.15762v1),  [pdf](http://arxiv.org/pdf/2409.15762v1)

**Tags**: cs.CL 



### Smart Grid Security: A Verified Deep Reinforcement Learning Framework to   Counter Cyber-Physical Attacks
**Authors**: Suman Maiti, Soumyajit Dey

**Updated**: 2024-09-24T05:26:20Z

**Summary**: The distributed nature of smart grids, combined with sophisticated sensors, control algorithms, and data collection facilities at Supervisory Control and Data Acquisition (SCADA) centers, makes them vulnerable to strategically crafted cyber-physical attacks. These malicious attacks can manipulate power demands using high-wattage Internet of Things (IoT) botnet devices, such as refrigerators and air conditioners, or introduce false values into transmission line power flow sensor readings. Consequently, grids experience blackouts and high power flow oscillations. Existing grid protection mechanisms, originally designed to tackle natural faults in transmission lines and generator outages, are ineffective against such intelligently crafted attacks. This is because grid operators overlook potential scenarios of cyber-physical attacks during their design phase. In this work, we propose a safe Deep Reinforcement Learning (DRL)-based framework for mitigating attacks on smart grids. The DRL agent effectively neutralizes cyber-physical attacks on grid surfaces by triggering appropriate sequences of existing protection schemes. The safety of the DRL agent is formally verified through a reachability analysis method. Additionally, our framework is designed for deployment on CUDA-enabled GPU systems, which enables faster execution of these protection sequences and their real-time validation. Our framework establishes a new set of protection rules for grid models, successfully thwarting existing cyber-physical attacks.

**Link**: [arxiv](http://arxiv.org/abs/2409.15757v1),  [pdf](http://arxiv.org/pdf/2409.15757v1)

**Tags**: cs.CR 



### Goal-guided Generative Prompt Injection Attack on Large Language Models
**Authors**: Chong Zhang, Mingyu Jin, Qinkai Yu, Chengzhi Liu, Haochen Xue, Xiaobo Jin

**Updated**: 2024-09-24T05:16:59Z

**Summary**: Current large language models (LLMs) provide a strong foundation for large-scale user-oriented natural language tasks. A large number of users can easily inject adversarial text or instructions through the user interface, thus causing LLMs model security challenges. Although there is currently a large amount of research on prompt injection attacks, most of these black-box attacks use heuristic strategies. It is unclear how these heuristic strategies relate to the success rate of attacks and thus effectively improve model robustness. To solve this problem, we redefine the goal of the attack: to maximize the KL divergence between the conditional probabilities of the clean text and the adversarial text. Furthermore, we prove that maximizing the KL divergence is equivalent to maximizing the Mahalanobis distance between the embedded representation $x$ and $x'$ of the clean text and the adversarial text when the conditional probability is a Gaussian distribution and gives a quantitative relationship on $x$ and $x'$. Then we designed a simple and effective goal-guided generative prompt injection strategy (G2PIA) to find an injection text that satisfies specific constraints to achieve the optimal attack effect approximately. It is particularly noteworthy that our attack method is a query-free black-box attack method with low computational cost. Experimental results on seven LLM models and four datasets show the effectiveness of our attack method.

**Link**: [arxiv](http://arxiv.org/abs/2404.07234v3),  [pdf](http://arxiv.org/pdf/2404.07234v3)

**Tags**: cs.CR cs.AI cs.CL 



### Automated Assessment of Multimodal Answer Sheets in the STEM domain
**Authors**: Rajlaxmi Patil, Aditya Ashutosh Kulkarni, Ruturaj Ghatage, Sharvi Endait, Geetanjali Kale, Raviraj Joshi

**Updated**: 2024-09-24T05:10:13Z

**Summary**: In the domain of education, the integration of,technology has led to a transformative era, reshaping traditional,learning paradigms. Central to this evolution is the automation,of grading processes, particularly within the STEM domain encompassing Science, Technology, Engineering, and Mathematics.,While efforts to automate grading have been made in subjects,like Literature, the multifaceted nature of STEM assessments,presents unique challenges, ranging from quantitative analysis,to the interpretation of handwritten diagrams. To address these,challenges, this research endeavors to develop efficient and reliable grading methods through the implementation of automated,assessment techniques using Artificial Intelligence (AI). Our,contributions lie in two key areas: firstly, the development of a,robust system for evaluating textual answers in STEM, leveraging,sample answers for precise comparison and grading, enabled by,advanced algorithms and natural language processing techniques.,Secondly, a focus on enhancing diagram evaluation, particularly,flowcharts, within the STEM context, by transforming diagrams,into textual representations for nuanced assessment using a,Large Language Model (LLM). By bridging the gap between,visual representation and semantic meaning, our approach ensures accurate evaluation while minimizing manual intervention.,Through the integration of models such as CRAFT for text,extraction and YoloV5 for object detection, coupled with LLMs,like Mistral-7B for textual evaluation, our methodology facilitates,comprehensive assessment of multimodal answer sheets. This,paper provides a detailed account of our methodology, challenges,encountered, results, and implications, emphasizing the potential,of AI-driven approaches in revolutionizing grading practices in,STEM education.

**Link**: [arxiv](http://arxiv.org/abs/2409.15749v1),  [pdf](http://arxiv.org/pdf/2409.15749v1)

**Tags**: cs.AI 



### Archon: An Architecture Search Framework for Inference-Time Techniques
**Authors**: Jon Saad-Falcon, Adrian Gamarra Lafuente, Shlok Natarajan, Nahum Maru, Hristo Todorov, Etash Guha, E. Kelly Buchanan, Mayee Chen, Neel Guha, Christopher R√©, Azalia Mirhoseini

**Updated**: 2024-09-24T05:08:18Z

**Summary**: Inference-time techniques are emerging as highly effective tools to increase large language model (LLM) capabilities. However, there is still limited understanding of the best practices for developing systems that combine inference-time techniques with one or more LLMs, with challenges including: (1) effectively allocating inference compute budget, (2) understanding the interactions between different combinations of inference-time techniques and their impact on downstream performance, and 3) efficiently searching over the large space of model choices, inference-time techniques, and their compositions. To address these challenges, we introduce Archon, an automated framework for designing inference-time architectures. Archon defines an extensible design space, encompassing methods such as generation ensembling, multi-sampling, ranking, fusion, critiquing, verification, and unit testing. It then transforms the problem of selecting and combining LLMs and inference-time techniques into a hyperparameter optimization objective. To optimize this objective, we introduce automated Inference-Time Architecture Search (ITAS) algorithms. Given target benchmark(s), an inference compute budget, and available LLMs, ITAS outputs optimized architectures. We evaluate Archon architectures across a wide range of instruction-following and reasoning benchmarks, including MT-Bench, Arena-Hard-Auto, AlpacaEval 2.0, MixEval, MixEval Hard, MATH, and CodeContests. We show that automatically designed inference-time architectures by Archon outperform strong models such as GPT-4o and Claude 3.5 Sonnet on these benchmarks, achieving an average increase of 14.1 and 10.3 percentage points with all-source models and open-source models, respectively. We make our code and datasets available publicly on Github: https://github.com/ScalingIntelligence/Archon.

**Link**: [arxiv](http://arxiv.org/abs/2409.15254v2),  [pdf](http://arxiv.org/pdf/2409.15254v2)

**Tags**: cs.LG cs.AI cs.CL 



### Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation   with LLMs
**Authors**: Yifan Wang, David Stevens, Pranay Shah, Wenwen Jiang, Miao Liu, Xu Chen, Robert Kuo, Na Li, Boying Gong, Daniel Lee, Jiabo Hu, Ning Zhang, Bob Kamma

**Updated**: 2024-09-24T05:00:07Z

**Summary**: The growing demand for AI training data has transformed data annotation into a global industry, but traditional approaches relying on human annotators are often time-consuming, labor-intensive, and prone to inconsistent quality. We propose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models into the annotation process. Our research introduces a collaborative paradigm that leverages the strengths of both professional human annotators and large language models (LLMs). By employing LLMs as pre-annotation and real-time assistants, and judges on annotator responses, MILO enables effective interaction patterns between human annotators and LLMs. Three empirical studies on multimodal data annotation demonstrate MILO's efficacy in reducing handling time, improving data quality, and enhancing annotator experiences. We also introduce quality rubrics for flexible evaluation and fine-grained feedback on open-ended annotations. The MILO framework has implications for accelerating AI/ML development, reducing reliance on human annotation alone, and promoting better alignment between human and machine values.

**Link**: [arxiv](http://arxiv.org/abs/2409.10702v2),  [pdf](http://arxiv.org/pdf/2409.10702v2)

**Tags**: cs.HC cs.AI cs.CL cs.LG 



### LSAST -- Enhancing Cybersecurity through LLM-supported Static   Application Security Testing
**Authors**: Mete Keltek, Ziyue Li

**Updated**: 2024-09-24T04:42:43Z

**Summary**: In the fast-evolving landscape of cybersecurity, Large Language Models (LLMs) play a pivotal role, continually improving their ability to analyze software code. This paper introduces a novel approach to vulnerability scanning by integrating conservative SAST (Static Application Security Testing) scanners with LLM capabilities, resulting in the creation of LSAST (LLM-supported Static Application Security Testing). Our approach significantly enhances the performance of LLMs in vulnerability scanning, establishing a new standard in this field. We benchmark LSAST's efficiency and compare its results with a state-of-the-art LLM. Additionally, we address the inherent drawbacks of LLMs in vulnerability scanning: their reliance on static training datasets, which leads to the exclusion of the latest vulnerabilities, and the privacy concerns associated with sending code to third-party LLM providers. To mitigate these issues, we utilize an open-source LLM to ensure privacy and employ a novel approach to gather relevant vulnerability information, thereby equipping the LLM with up-to-date knowledge.

**Link**: [arxiv](http://arxiv.org/abs/2409.15735v1),  [pdf](http://arxiv.org/pdf/2409.15735v1)

**Tags**: cs.CR 



### Harmonising the Clinical Melody: Tuning Large Language Models for   Hospital Course Summarisation in Clinical Coding
**Authors**: Bokang Bi, Leibo Liu, Sanja Lujic, Louisa Jorm, Oscar Perez-Concha

**Updated**: 2024-09-24T04:41:43Z

**Summary**: The increasing volume and complexity of clinical documentation in Electronic Medical Records systems pose significant challenges for clinical coders, who must mentally process and summarise vast amounts of clinical text to extract essential information needed for coding tasks. While large language models have been successfully applied to shorter summarisation tasks in recent years, the challenge of summarising a hospital course remains an open area for further research and development. In this study, we adapted three pre trained LLMs, Llama 3, BioMistral, Mistral Instruct v0.1 for the hospital course summarisation task, using Quantized Low Rank Adaptation fine tuning. We created a free text clinical dataset from MIMIC III data by concatenating various clinical notes as the input clinical text, paired with ground truth Brief Hospital Course sections extracted from the discharge summaries for model training. The fine tuned models were evaluated using BERTScore and ROUGE metrics to assess the effectiveness of clinical domain fine tuning. Additionally, we validated their practical utility using a novel hospital course summary assessment metric specifically tailored for clinical coding. Our findings indicate that fine tuning pre trained LLMs for the clinical domain can significantly enhance their performance in hospital course summarisation and suggest their potential as assistive tools for clinical coding. Future work should focus on refining data curation methods to create higher quality clinical datasets tailored for hospital course summary tasks and adapting more advanced open source LLMs comparable to proprietary models to further advance this research.

**Link**: [arxiv](http://arxiv.org/abs/2409.14638v2),  [pdf](http://arxiv.org/pdf/2409.14638v2)

**Tags**: cs.CL cs.LG 



### Tarsier: Recipes for Training and Evaluating Large Video Description   Models
**Authors**: Jiawei Wang, Liping Yuan, Yuchen Zhang, Haomiao Sun

**Updated**: 2024-09-24T04:41:08Z

**Summary**: Generating fine-grained video descriptions is a fundamental challenge in video understanding. In this work, we introduce Tarsier, a family of large-scale video-language models designed to generate high-quality video descriptions. Tarsier employs CLIP-ViT to encode frames separately and then uses an LLM to model temporal relationships. Despite its simple architecture, we demonstrate that with a meticulously designed two-stage training procedure, the Tarsier models exhibit substantially stronger video description capabilities than any existing open-source model, showing a $+51.4\%$ advantage in human side-by-side evaluation over the strongest model. Additionally, they are comparable to state-of-the-art proprietary models, with a $+12.3\%$ advantage against GPT-4V and a $-6.7\%$ disadvantage against Gemini 1.5 Pro. When upgraded to Tarsier2 by building upon SigLIP and Qwen2-7B, it further improves significantly with a $+4.8\%$ advantage against GPT-4o. Besides video description, Tarsier proves to be a versatile generalist model, achieving new state-of-the-art results across nine public benchmarks, including multi-choice VQA, open-ended VQA, and zero-shot video captioning. Our second contribution is the introduction of a new benchmark -- DREAM-1K (https://tarsier-vlm.github.io/) for evaluating video description models, consisting of a new challenging dataset featuring videos from diverse sources and varying complexity, along with an automatic method specifically designed to assess the quality of fine-grained video descriptions. We make our models and evaluation benchmark publicly available at https://github.com/bytedance/tarsier.

**Link**: [arxiv](http://arxiv.org/abs/2407.00634v2),  [pdf](http://arxiv.org/pdf/2407.00634v2)

**Tags**: cs.CV cs.LG 



### LLM-Cure: LLM-based Competitor User Review Analysis for Feature   Enhancement
**Authors**: Maram Assi, Safwat Hassan, Ying Zou

**Updated**: 2024-09-24T04:17:21Z

**Summary**: The exponential growth of the mobile app market underscores the importance of constant innovation and rapid response to user demands. As user satisfaction is paramount to the success of a mobile application (app), developers typically rely on user reviews, which represent user feedback that includes ratings and comments to identify areas for improvement. However, the sheer volume of user reviews poses challenges in manual analysis, necessitating automated approaches. Existing automated approaches either analyze only the target apps reviews, neglecting the comparison of similar features to competitors or fail to provide suggestions for feature enhancement. To address these gaps, we propose a Large Language Model (LLM)-based Competitive User Review Analysis for Feature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically generate suggestion s for mobile app feature improvements. More specifically, LLM-Cure identifies and categorizes features within reviews by applying LLMs. When provided with a complaint in a user review, LLM-Cure curates highly rated (4 and 5 stars) reviews in competing apps related to the complaint and proposes potential improvements tailored to the target application. We evaluate LLM-Cure on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates that LLM-Cure significantly outperforms the state-of-the-art approaches in assigning features to reviews by up to 13% in F1-score, up to 16% in recall and up to 11% in precision. Additionally, LLM-Cure demonstrates its capability to provide suggestions for resolving user complaints. We verify the suggestions using the release notes that reflect the changes of features in the target mobile app. LLM-Cure achieves a promising average of 73% of the implementation of the provided suggestions.

**Link**: [arxiv](http://arxiv.org/abs/2409.15724v1),  [pdf](http://arxiv.org/pdf/2409.15724v1)

**Tags**: cs.SE cs.AI cs.IR 



### Federated Large Language Models: Current Progress and Future Directions
**Authors**: Yuhang Yao, Jianyi Zhang, Junda Wu, Chengkai Huang, Yu Xia, Tong Yu, Ruiyi Zhang, Sungchul Kim, Ryan Rossi, Ang Li, Lina Yao, Julian McAuley, Yiran Chen, Carlee Joe-Wong

**Updated**: 2024-09-24T04:14:33Z

**Summary**: Large language models are rapidly gaining popularity and have been widely adopted in real-world applications. While the quality of training data is essential, privacy concerns arise during data collection. Federated learning offers a solution by allowing multiple clients to collaboratively train LLMs without sharing local data. However, FL introduces new challenges, such as model convergence issues due to heterogeneous data and high communication costs. A comprehensive study is required to address these challenges and guide future research. This paper surveys Federated learning for LLMs (FedLLM), highlighting recent advances and future directions. We focus on two key aspects: fine-tuning and prompt learning in a federated setting, discussing existing work and associated research challenges. We finally propose potential research directions for federated LLMs, including pre-training and how LLMs can further enhance federated learning.

**Link**: [arxiv](http://arxiv.org/abs/2409.15723v1),  [pdf](http://arxiv.org/pdf/2409.15723v1)

**Tags**: cs.LG cs.CL 



### Improving Emotional Support Delivery in Text-Based Community Safety   Reporting Using Large Language Models
**Authors**: Yiren Liu, Yerong Li, Ryan Mayfield, Yun Huang

**Updated**: 2024-09-24T03:47:02Z

**Summary**: Emotional support is a crucial aspect of communication between community members and police dispatchers during incident reporting. However, there is a lack of understanding about how emotional support is delivered through text-based systems, especially in various non-emergency contexts. In this study, we analyzed two years of chat logs comprising 57,114 messages across 8,239 incidents from 130 higher education institutions. Our empirical findings revealed significant variations in emotional support provided by dispatchers, influenced by the type of incident, service time, and a noticeable decline in support over time across multiple organizations. To improve the consistency and quality of emotional support, we developed and implemented a fine-tuned Large Language Model (LLM), named dispatcherLLM. We evaluated dispatcherLLM by comparing its generated responses to those of human dispatchers and other off-the-shelf models using real chat messages. Additionally, we conducted a human evaluation to assess the perceived effectiveness of the support provided by dispatcherLLM. This study not only contributes new empirical understandings of emotional support in text-based dispatch systems but also demonstrates the significant potential of generative AI in improving service delivery.

**Link**: [arxiv](http://arxiv.org/abs/2409.15706v1),  [pdf](http://arxiv.org/pdf/2409.15706v1)

**Tags**: cs.HC cs.AI 



### Making Text Embedders Few-Shot Learners
**Authors**: Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingxia Shao, Defu Lian, Zheng Liu

**Updated**: 2024-09-24T03:30:19Z

**Summary**: Large language models (LLMs) with decoder-only architectures demonstrate remarkable in-context learning (ICL) capabilities. This feature enables them to effectively handle both familiar and novel tasks by utilizing examples provided within their input context. Recognizing the potential of this capability, we propose leveraging the ICL feature in LLMs to enhance the process of text embedding generation. To this end, we introduce a novel model bge-en-icl, which employs few-shot examples to produce high-quality text embeddings. Our approach integrates task-related examples directly into the query side, resulting in significant improvements across various tasks. Additionally, we have investigated how to effectively utilize LLMs as embedding models, including various attention mechanisms, pooling methods, etc. Our findings suggest that retaining the original framework often yields the best results, underscoring that simplicity is best. Experimental results on the MTEB and AIR-Bench benchmarks demonstrate that our approach sets new state-of-the-art (SOTA) performance. Our model, code and dataset are freely available at https://github.com/FlagOpen/FlagEmbedding .

**Link**: [arxiv](http://arxiv.org/abs/2409.15700v1),  [pdf](http://arxiv.org/pdf/2409.15700v1)

**Tags**: cs.IR cs.CL 



### Lighter And Better: Towards Flexible Context Adaptation For Retrieval   Augmented Generation
**Authors**: Zheng Liu, Chenyuan Wu, Ninglu Shao, Shitao Xiao, Chaozhuo Li, Defu Lian

**Updated**: 2024-09-24T03:25:36Z

**Summary**: The existing Retrieval-Augmented Generation (RAG) systems face significant challenges in terms of cost and effectiveness. On one hand, they need to encode the lengthy retrieved contexts before responding to the input tasks, which imposes substantial computational overhead. On the other hand, directly using generic Large Language Models (LLMs) often leads to sub-optimal answers, while task-specific fine-tuning may compromise the LLMs' general capabilities. To address these challenges, we introduce a novel approach called FlexRAG (Flexible Context Adaptation for RAG). In this approach, the retrieved contexts are compressed into compact embeddings before being encoded by the LLMs. Simultaneously, these compressed embeddings are optimized to enhance downstream RAG performance. A key feature of FlexRAG is its flexibility, which enables effective support for diverse compression ratios and selective preservation of important contexts. Thanks to these technical designs, FlexRAG achieves superior generation quality while significantly reducing running costs. Comprehensive experiments on various question-answering datasets validate our approach as a cost-effective and flexible solution for RAG systems.

**Link**: [arxiv](http://arxiv.org/abs/2409.15699v1),  [pdf](http://arxiv.org/pdf/2409.15699v1)

**Tags**: cs.CL 



### Will Large Language Models be a Panacea to Autonomous Driving?
**Authors**: Yuxuan Zhu, Shiyi Wang, Wenqing Zhong, Nianchen Shen, Yunqi Li, Siqi Wang, Zhiheng Li, Cathy Wu, Zhengbing He, Li Li

**Updated**: 2024-09-24T03:12:12Z

**Summary**: Artificial intelligence (AI) plays a crucial role in autonomous driving (AD) research, propelling its development towards intelligence and efficiency. Currently, the development of AD technology follows two main technical paths: modularization and end-to-end. Modularization decompose the driving task into modules such as perception, prediction, planning, and control, and train them separately. Due to the inconsistency of training objectives between modules, the integrated effect suffers from bias. End-to-end attempts to address this issue by utilizing a single model that directly maps from sensor data to control signals. This path has limited learning capabilities in a comprehensive set of features and struggles to handle unpredictable long-tail events and complex urban traffic scenarios. In the face of challenges encountered in both paths, many researchers believe that large language models (LLMs) with powerful reasoning capabilities and extensive knowledge understanding may be the solution, expecting LLMs to provide AD systems with deeper levels of understanding and decision-making capabilities. In light of the challenges faced by both paths, many researchers believe that LLMs, with their powerful reasoning abilities and extensive knowledge, could offer a solution. To understand if LLMs could enhance AD, this paper conducts a thorough analysis of the potential applications of LLMs in AD systems, including exploring their optimization strategies in both modular and end-to-end approaches, with a particular focus on how LLMs can tackle the problems and challenges present in current solutions. Furthermore, we discuss an important question: Can LLM-based artificial general intelligence (AGI) be a key to achieve high-level AD? We further analyze the potential limitations and challenges that LLMs may encounter in promoting the development of AD technology.

**Link**: [arxiv](http://arxiv.org/abs/2409.14165v2),  [pdf](http://arxiv.org/pdf/2409.14165v2)

**Tags**: cs.AI cs.CL cs.LG cs.RO cs.SY eess.SY 



### A Comprehensive Evaluation of Large Language Models on Mental Illnesses
**Authors**: Abdelrahman Hanafi, Mohammed Saad, Noureldin Zahran, Radwa J. Hanafy, Mohammed E. Fouda

**Updated**: 2024-09-24T02:58:52Z

**Summary**: Large language models have shown promise in various domains, including healthcare. In this study, we conduct a comprehensive evaluation of LLMs in the context of mental health tasks using social media data. We explore the zero-shot (ZS) and few-shot (FS) capabilities of various LLMs, including GPT-4, Llama 3, Gemini, and others, on tasks such as binary disorder detection, disorder severity evaluation, and psychiatric knowledge assessment. Our evaluation involved 33 models testing 9 main prompt templates across the tasks. Key findings revealed that models like GPT-4 and Llama 3 exhibited superior performance in binary disorder detection, with accuracies reaching up to 85% on certain datasets. Moreover, prompt engineering played a crucial role in enhancing model performance. Notably, the Mixtral 8x22b model showed an improvement of over 20%, while Gemma 7b experienced a similar boost in performance. In the task of disorder severity evaluation, we observed that FS learning significantly improved the model's accuracy, highlighting the importance of contextual examples in complex assessments. Notably, the Phi-3-mini model exhibited a substantial increase in performance, with balanced accuracy improving by over 6.80% and mean average error dropping by nearly 1.3 when moving from ZS to FS learning. In the psychiatric knowledge task, recent models generally outperformed older, larger counterparts, with the Llama 3.1 405b achieving an accuracy of 91.2%. Despite promising results, our analysis identified several challenges, including variability in performance across datasets and the need for careful prompt engineering. Furthermore, the ethical guards imposed by many LLM providers hamper the ability to accurately evaluate their performance, due to tendency to not respond to potentially sensitive queries.

**Link**: [arxiv](http://arxiv.org/abs/2409.15687v1),  [pdf](http://arxiv.org/pdf/2409.15687v1)

**Tags**: cs.AI 



### UELLM: A Unified and Efficient Approach for LLM Inference Serving
**Authors**: Yiyuan He, Minxian Xu, Jingfeng Wu, Wanyi Zheng, Kejiang Ye, Chengzhong Xu

**Updated**: 2024-09-24T02:56:00Z

**Summary**: In the context of Machine Learning as a Service (MLaaS) clouds, the extensive use of Large Language Models (LLMs) often requires efficient management of significant query loads. When providing real-time inference services, several challenges arise. Firstly, increasing the number of GPUs may lead to a decrease in inference speed due to heightened communication overhead, while an inadequate number of GPUs can lead to out-of-memory errors. Secondly, different deployment strategies need to be evaluated to guarantee optimal utilization and minimal inference latency. Lastly, inefficient orchestration of inference queries can easily lead to significant Service Level Objective (SLO) violations. Lastly, inefficient orchestration of inference queries can easily lead to significant Service Level Objective (SLO) violations. To address these challenges, we propose a Unified and Efficient approach for Large Language Model inference serving (UELLM), which consists of three main components: 1) resource profiler, 2) batch scheduler, and 3) LLM deployer. UELLM minimizes resource overhead, reduces inference latency, and lowers SLO violation rates. Compared with state-of-the-art (SOTA) techniques, UELLM reduces the inference latency by 72.3% to 90.3%, enhances GPU utilization by 1.2X to 4.1X, and increases throughput by 1.92X to 4.98X, it can also serve without violating the inference latency SLO.

**Link**: [arxiv](http://arxiv.org/abs/2409.14961v2),  [pdf](http://arxiv.org/pdf/2409.14961v2)

**Tags**: cs.DC 



### SYNERGAI: Perception Alignment for Human-Robot Collaboration
**Authors**: Yixin Chen, Guoxi Zhang, Yaowei Zhang, Hongming Xu, Peiyuan Zhi, Qing Li, Siyuan Huang

**Updated**: 2024-09-24T02:55:54Z

**Summary**: Recently, large language models (LLMs) have shown strong potential in facilitating human-robotic interaction and collaboration. However, existing LLM-based systems often overlook the misalignment between human and robot perceptions, which hinders their effective communication and real-world robot deployment. To address this issue, we introduce SYNERGAI, a unified system designed to achieve both perceptual alignment and human-robot collaboration. At its core, SYNERGAI employs 3D Scene Graph (3DSG) as its explicit and innate representation. This enables the system to leverage LLM to break down complex tasks and allocate appropriate tools in intermediate steps to extract relevant information from the 3DSG, modify its structure, or generate responses. Importantly, SYNERGAI incorporates an automatic mechanism that enables perceptual misalignment correction with users by updating its 3DSG with online interaction. SYNERGAI achieves comparable performance with the data-driven models in ScanQA in a zero-shot manner. Through comprehensive experiments across 10 real-world scenes, SYNERGAI demonstrates its effectiveness in establishing common ground with humans, realizing a success rate of 61.9% in alignment tasks. It also significantly improves the success rate from 3.7% to 45.68% on novel tasks by transferring the knowledge acquired during alignment.

**Link**: [arxiv](http://arxiv.org/abs/2409.15684v1),  [pdf](http://arxiv.org/pdf/2409.15684v1)

**Tags**: cs.RO 



### Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data   Annotation
**Authors**: Juhwan Choi, Jungmin Yun, Kyohoon Jin, YoungBin Kim

**Updated**: 2024-09-24T02:35:41Z

**Summary**: The quality of the dataset is crucial for ensuring optimal performance and reliability of downstream task models. However, datasets often contain noisy data inadvertently included during the construction process. Numerous attempts have been made to correct this issue through human annotators. However, hiring and managing human annotators is expensive and time-consuming. As an alternative, recent studies are exploring the use of large language models (LLMs) for data annotation.   In this study, we present a case study that extends the application of LLM-based data annotation to enhance the quality of existing datasets through a cleansing strategy. Specifically, we leverage approaches such as chain-of-thought and majority voting to imitate human annotation and classify unrelated documents from the Multi-News dataset, which is widely used for the multi-document summarization task. Through our proposed cleansing method, we introduce an enhanced Multi-News+. By employing LLMs for data cleansing, we demonstrate an efficient and effective approach to improving dataset quality without relying on expensive human annotation efforts.

**Link**: [arxiv](http://arxiv.org/abs/2404.09682v3),  [pdf](http://arxiv.org/pdf/2404.09682v3)

**Tags**: cs.CL cs.AI 



### Objectively Evaluating the Reliability of Cell Type Annotation Using   LLM-Based Strategies
**Authors**: Wenjin Ye, Yuanchen Ma, Junkai Xiang, Hongjie Liang, Tao Wang, Qiuling Xiang, Andy Peng Xiang, Wu Song, Weiqiang Li, Weijun Huang

**Updated**: 2024-09-24T02:34:02Z

**Summary**: Reliability in cell type annotation is challenging in single-cell RNA-sequencing data analysis because both expert-driven and automated methods can be biased or constrained by their training data, especially for novel or rare cell types. Although large language models (LLMs) are useful, our evaluation found that only a few matched expert annotations due to biased data sources and inflexible training inputs. To overcome these limitations, we developed the LICT (Large language model-based Identifier for Cell Types) software package using a multi-model fusion and "talk-to-machine" strategy. Tested across various single-cell RNA sequencing datasets, our approach significantly improved annotation reliability, especially in datasets with low cellular heterogeneity. Notably, we established objective criteria to assess annotation reliability using the "talk-to-machine" approach, which addresses discrepancies between our annotations and expert ones, enabling reliable evaluation even without reference data. This strategy enhances annotation credibility and sets the stage for advancing future LLM-based cell type annotation methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.15678v1),  [pdf](http://arxiv.org/pdf/2409.15678v1)

**Tags**: q-bio.QM q-bio.GN 



### Northeast Materials Database (NEMAD): Enabling Discovery of High   Transition Temperature Magnetic Compounds
**Authors**: Suman Itani, Yibo Zhang, Jiadong Zang

**Updated**: 2024-09-24T02:27:10Z

**Summary**: The discovery of novel magnetic materials with greater operating temperature ranges and optimized performance is essential for advanced applications. Current data-driven approaches are challenging and limited due to the lack of accurate, comprehensive, and feature-rich databases. This study aims to address this challenge by introducing a new approach that uses Large Language Models (LLMs) to create a comprehensive, experiment-based, magnetic materials database named the Northeast Materials Database (NEMAD), which consists of 26,706 magnetic materials (www.nemad.org). The database incorporates chemical composition, magnetic phase transition temperatures, structural details, and magnetic properties. Enabled by NEMAD, machine learning models were developed to classify materials and predict transition temperatures. Our classification model achieved an accuracy of 90% in categorizing materials as ferromagnetic (FM), antiferromagnetic (AFM), and non-magnetic (NM). The regression models predict Curie (N\'eel) temperature with a coefficient of determination (R2) of 0.86 (0.85) and a mean absolute error (MAE) of 62K (32K). These models identified 62 (19) FM (AFM) candidates with a predicted Curie (N\'eel) temperature above 500K (100K) from the Materials Project. This work shows the feasibility of combining LLMs for automated data extraction and machine learning models in accelerating the discovery of magnetic materials.

**Link**: [arxiv](http://arxiv.org/abs/2409.15675v1),  [pdf](http://arxiv.org/pdf/2409.15675v1)

**Tags**: cond-mat.mtrl-sci cs.LG physics.comp-ph 



