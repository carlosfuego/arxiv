# Arxiv Results
## Keyword: kv cache 
 ### TASP: Topology-aware Sequence Parallelism
**Authors**: Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang

**Updated**: 2025-09-30T17:15:27Z

**Summary**: Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.

**Link**: [arxiv](http://arxiv.org/abs/2509.26541v1),  [pdf](http://arxiv.org/pdf/2509.26541v1)

**Tags**: cs.LG cs.DC 



### LoLA: Low-Rank Linear Attention With Sparse Caching
**Authors**: Luke McDermott, Robert W. Heath Jr., Rahul Parhi

**Updated**: 2025-09-30T16:42:50Z

**Summary**: The per-token cost of transformer inference scales with context length, preventing its application to lifelong in-context learning. Linear attention is an efficient alternative that maintains a constant memory footprint, even on infinite context lengths. While this is a potential candidate for lifelong learning, it falls short in memory capacity. In this paper, we propose LoLA, a training-free augmentation to linear attention that boosts associative recall. LoLA distributes past key-value pairs from context into three memory systems: (i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize pairs in a sparse, global cache; and (iii) generic pairs in the recurrent hidden state of linear attention. We show through ablations that our self-recall error metric is crucial to efficiently manage long-term associative memories. On pass-key retrieval tasks, LoLA improves the base model's performance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller cache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B and 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2505.23666v2),  [pdf](http://arxiv.org/pdf/2505.23666v2)

**Tags**: cs.CL cs.LG 



### AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block   Size
**Authors**: Guanxi Lu, Hao, Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan

**Updated**: 2025-09-30T15:53:56Z

**Summary**: Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26432v1),  [pdf](http://arxiv.org/pdf/2509.26432v1)

**Tags**: cs.LG cs.AI 



### SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study
**Authors**: Yang Xiang, Fernando Garc√≠a-Redondo, Arvind Sharma, Van Dai Nguyen, Andrea Fantini, Philippe Matagne, Siddharth Rao, Subhali Subhechha, Lynn Verschueren, Mohammed Aftab Baig, Marie Garcia Bardon, Geert Hellings

**Updated**: 2025-09-30T15:44:29Z

**Summary**: This work explores the cross-node scaling potential of SOT-MRAM for last-level caches (LLCs) under heterogeneous system scaling paradigm. We perform extensive Design-Technology Co-Optimization (DTCO) exercises to evaluate the bitcell footprint for different cell configurations at a representative 7 nm technology and to assess their implications on read and write power-performance. We crucially identify the MTJ routing struggle in conventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary bitcell area scaling challenge and propose to use BEOL read selectors (BEOL RSs) that enable (10 -- 40) % bitcell area reduction and eventually match sub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet the required SOT switching current, provided the magnetic free layer properties be engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This is particularly to attribute to their (i) more available Si fins for write transistor and (ii) lower bitline resistance at reduced cell width. We nevertheless underscore the read tradeoff associated with BEOL RSs, with the low-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the imperfectly rectifying diode selectors suffering (2.5 -- 5)$\times$ energy cost relative to 2T1R. This article thus highlights the realistic prospects and hurdles of BEOL RSs towards holistic power-performance-area scaling of SOT-MRAM.

**Link**: [arxiv](http://arxiv.org/abs/2508.18250v3),  [pdf](http://arxiv.org/pdf/2508.18250v3)

**Tags**: cs.ET 



### Fast-dLLM v2: Efficient Block-Diffusion LLM
**Authors**: Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-09-30T14:40:18Z

**Summary**: Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.

**Link**: [arxiv](http://arxiv.org/abs/2509.26328v1),  [pdf](http://arxiv.org/pdf/2509.26328v1)

**Tags**: cs.CL 



### Scaling RL to Long Videos
**Authors**: Yukang Chen, Wei Huang, Baifeng Shi, Qinghao Hu, Hanrong Ye, Ligeng Zhu, Zhijian Liu, Pavlo Molchanov, Jan Kautz, Xiaojuan Qi, Sifei Liu, Hongxu Yin, Yao Lu, Song Han

**Updated**: 2025-09-30T14:13:20Z

**Summary**: We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 104K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves strong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on VideoMME without and with subtitles, respectively, and consistently outperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B supports processing up to 8,192 video frames per video, and configurable FPS settings. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames).

**Link**: [arxiv](http://arxiv.org/abs/2507.07966v4),  [pdf](http://arxiv.org/pdf/2507.07966v4)

**Tags**: cs.CV cs.AI cs.CL 



### FastCoder: Accelerating Repository-level Code Generation via Efficient   Retrieval and Verification
**Authors**: Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Jia Li, Lin Shi

**Updated**: 2025-09-30T09:10:26Z

**Summary**: Code generation is a latency-sensitive task that demands high timeliness. However, with the growing interest and inherent difficulty in repository-level code generation, most existing code generation studies focus on improving the correctness of generated code while overlooking the inference efficiency, which is substantially affected by the overhead during LLM generation. Although there has been work on accelerating LLM inference, these approaches are not tailored to the specific characteristics of code generation; instead, they treat code the same as natural language sequences and ignore its unique syntax and semantic characteristics, which are also crucial for improving efficiency. Consequently, these approaches exhibit limited effectiveness in code generation tasks, particularly for repository-level scenarios with considerable complexity and difficulty. To alleviate this issue, following draft-verification paradigm, we propose FastCoder, a simple yet highly efficient inference acceleration approach specifically designed for code generation, without compromising the quality of the output. FastCoder constructs a multi-source datastore, providing access to both general and project-specific knowledge, facilitating the retrieval of high-quality draft sequences. Moreover, FastCoder reduces the retrieval cost by controlling retrieval timing, and enhances efficiency through parallel retrieval and a context- and LLM preference-aware cache. Experimental results show that FastCoder can reach up to 2.53x and 2.54x speedup compared to autoregressive decoding in repository-level and standalone code generation tasks, respectively, outperforming state-of-the-art inference acceleration approaches by up to 88%. FastCoder can also be integrated with existing correctness-focused code generation approaches to accelerate the LLM generation process, and reach a speedup exceeding 2.6x.

**Link**: [arxiv](http://arxiv.org/abs/2502.17139v2),  [pdf](http://arxiv.org/pdf/2502.17139v2)

**Tags**: cs.AI cs.SE 



### KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction
**Authors**: Jang-Hyun Kim, Jinuk Kim, Sangwoo Kwon, Jae W. Lee, Sangdoo Yun, Hyun Oh Song

**Updated**: 2025-09-30T02:51:05Z

**Summary**: Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces KVzip, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by $3$-$4\times$ and FlashAttention decoding latency by approximately $2\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90% cache budget ratio under multi-query scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2505.23416v2),  [pdf](http://arxiv.org/pdf/2505.23416v2)

**Tags**: cs.DB cs.LG 



### dVLA: Diffusion Vision-Language-Action Model with Multimodal   Chain-of-Thought
**Authors**: Junjie Wen, Minjie Zhu, Jiaming Liu, Zhiyuan Liu, Yicun Yang, Linfeng Zhang, Shanghang Zhang, Yichen Zhu, Yi Xu

**Updated**: 2025-09-30T02:36:11Z

**Summary**: Vision-Language-Action (VLA) models are emerging as a next-generation paradigm for robotics. We introduce dVLA, a diffusion-based VLA that leverages a multimodal chain-of-thought to unify visual perception, language reasoning, and robotic control in a single system. dVLA jointly optimizes perception, language understanding, and action under a single diffusion objective, enabling stronger cross-modal reasoning and better generalization to novel instructions and objects. For practical deployment, we mitigate inference latency by incorporating two acceleration strategies, a prefix attention mask and KV caching, yielding up to around times speedup at test-time inference. We evaluate dVLA in both simulation and the real world: on the LIBERO benchmark, it achieves state-of-the-art performance with a 96.4% average success rate, consistently surpassing both discrete and continuous action policies; on a real Franka robot, it succeeds across a diverse task suite, including a challenging bin-picking task that requires multi-step planning, demonstrating robust real-world performance. Together, these results underscore the promise of unified diffusion frameworks for practical, high-performance VLA robotics.

**Link**: [arxiv](http://arxiv.org/abs/2509.25681v1),  [pdf](http://arxiv.org/pdf/2509.25681v1)

**Tags**: cs.RO cs.CV 



### DeepSearch: Overcome the Bottleneck of Reinforcement Learning with   Verifiable Rewards via Monte Carlo Tree Search
**Authors**: Fang Wu, Weihao Xuan, Heli Qi, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi

**Updated**: 2025-10-01T05:09:42Z

**Summary**: Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.

**Link**: [arxiv](http://arxiv.org/abs/2509.25454v2),  [pdf](http://arxiv.org/pdf/2509.25454v2)

**Tags**: cs.AI cs.CL 



### FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers
**Authors**: Liang Qiao, Yue Dai, Yeqi Huang, Hongyu Kan, Jun Shi, Hong An

**Updated**: 2025-09-29T18:57:14Z

**Summary**: Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional capabilities in visual synthesis, yet their deployment remains constrained by substantial computational demands. To alleviate this bottleneck, many sparsity-based acceleration methods have been proposed. However, their diverse sparsity patterns often require customized kernels for high-performance inference, limiting universality. We propose FlashOmni, a unified sparse attention engine compatible with arbitrary DiT architectures. FlashOmni introduces flexible sparse symbols to standardize the representation of a wide range of sparsity strategies, such as feature caching and block-sparse skipping. This unified abstraction enables the execution of diverse sparse computations within a single attention kernel. In addition, FlashOmni designs optimized sparse GEMMs for attention blocks, leveraging sparse symbols to eliminate redundant computations and further improve efficiency. Experiments demonstrate that FlashOmni delivers near-linear, closely matching the sparsity ratio speedup (1:1) in attention and GEMM-$Q$, and achieves 2.5$\times$-3.8$\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of the theoretical limit). Applied with a multi-granularity sparsity strategy, it enables the Hunyuan model (33K) to achieve about 1.5$\times$ end-to-end acceleration without degrading visual quality.

**Link**: [arxiv](http://arxiv.org/abs/2509.25401v1),  [pdf](http://arxiv.org/pdf/2509.25401v1)

**Tags**: cs.LG cs.AI cs.PF 



### Learning to Parallel: Accelerating Diffusion Large Language Models via   Adaptive Parallel Decoding
**Authors**: Wenrui Bao, Zhiben Chen, Dan Xu, Yuzhang Shang

**Updated**: 2025-09-29T17:59:54Z

**Summary**: Autoregressive decoding in large language models (LLMs) requires $\mathcal{O}(n)$ sequential steps for $n$ tokens, fundamentally limiting inference throughput. Recent diffusion-based LLMs (dLLMs) enable parallel token generation through iterative denoising. However, current parallel decoding strategies rely on fixed, input-agnostic heuristics (e.g., confidence thresholds), which fail to adapt to input-specific characteristics, resulting in suboptimal speed-quality trade-offs across diverse NLP tasks. In this work, we explore a more flexible and dynamic approach to parallel decoding. We propose Learning to Parallel Decode (Learn2PD), a framework that trains a lightweight and adaptive filter model to predict, for each token position, whether the current prediction matches the final output. This learned filter approximates an oracle parallel decoding strategy that unmasks tokens only when correctly predicted. Importantly, the filter model is learned in a post-training manner, requiring only a small amount of computation to optimize it (minute-level GPU time). Additionally, we introduce End-of-Text Prediction (EoTP) to detect decoding completion at the end of sequence, avoiding redundant decoding of padding tokens. Experiments on the LLaDA benchmark demonstrate that our method achieves up to 22.58$\times$ speedup without any performance drop, and up to 57.51$\times$ when combined with KV-Cache.

**Link**: [arxiv](http://arxiv.org/abs/2509.25188v1),  [pdf](http://arxiv.org/pdf/2509.25188v1)

**Tags**: cs.CL 



### Context-Driven Performance Modeling for Causal Inference Operators on   Neural Processing Units
**Authors**: Neelesh Gupta, Rakshith Jayanth, Dhruv Parikh, Viktor Prasanna

**Updated**: 2025-09-29T17:55:43Z

**Summary**: The proliferation of large language models (LLMs) has driven demand for long context inference on resource constrained edge devices. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to the architectural mismatch: quadratic complexity of standard attention mechanisms conflicts with memory and compute patterns of edge accelerators. This paper presents a comprehensive performance analysis of various causal inference operators on a modern NPU. We benchmark standard quadratic attention against several sub-quadratic alternatives, including structured state-space and linear attention models. Our analysis reveals that while sub-quadratic methods offer superior scalability, they introduce distinct computational bottlenecks on the NPU's specialized execution units. We identify that quadratic attention becomes severely memory-bound, suffering from cache inefficiency and pipeline stalls exceeding 95% at long contexts. In contrast, sub-quadratic models can become compute-bound on programmable vector cores. These findings provide critical insights for the co-design of hardware-aware models and optimization strategies to enable on-device AI inference with long-contexts.

**Link**: [arxiv](http://arxiv.org/abs/2509.25155v1),  [pdf](http://arxiv.org/pdf/2509.25155v1)

**Tags**: cs.DC cs.LG 



### METok: Multi-Stage Event-based Token Compression for Efficient Long   Video Understanding
**Authors**: Mengyue Wang, Shuo Chen, Kristian Kersting, Volker Tresp, Yunpu Ma

**Updated**: 2025-09-29T15:20:29Z

**Summary**: Recent advances in Video Large Language Models (VLLMs) have significantly enhanced their ability to understand video content. Nonetheless, processing long videos remains challenging due to high computational demands and the redundancy present in the visual data. In this work, we propose METok, a training-free, Multi-stage Event-based Token compression framework designed to accelerate VLLMs' inference while preserving accuracy. METok progressively eliminates redundant visual tokens across three critical stages: (1) event-aware compression during vision encoding, (2) hierarchical token pruning in the prefilling stage based on semantic alignment and event importance, and (3) a decoding-stage KV Cache optimization that further reduces memory consumption. Our experiments on diverse video benchmarks demonstrate that METok achieves an optimal trade-off between efficiency and accuracy by dynamically selecting informative visual tokens. For instance, equipping LongVA-7B with METok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all while maintaining comparable or even superior accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2506.02850v2),  [pdf](http://arxiv.org/pdf/2506.02850v2)

**Tags**: cs.CV 



### Not All Models Suit Expert Offloading: On Local Routing Consistency of   Mixture-of-Expert Models
**Authors**: Jingcong Liang, Siyuan Wang, Miren Tian, Yitong Li, Duyu Tang, Zhongyu Wei

**Updated**: 2025-09-29T15:15:49Z

**Summary**: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the optimal segment-level cache hit rate under a given cache size limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found that models that apply MoE on every layer and do not use shared experts exhibit the highest local routing consistency. We further showed that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models can balance between cache effectiveness and efficiency with cache sizes approximately 2x the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .

**Link**: [arxiv](http://arxiv.org/abs/2505.16056v2),  [pdf](http://arxiv.org/pdf/2505.16056v2)

**Tags**: cs.LG cs.AI 



### SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts   via Token-Level LSH Matching
**Authors**: Xinye Zhao, Spyridon Mastorakis

**Updated**: 2025-09-29T14:16:13Z

**Summary**: As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\times$ speedup and 42\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2509.24832v1),  [pdf](http://arxiv.org/pdf/2509.24832v1)

**Tags**: cs.CL cs.AI 



### Vision Function Layer in Multimodal LLMs
**Authors**: Cheng Shi, Yizhou Yu, Sibei Yang

**Updated**: 2025-09-29T13:45:35Z

**Summary**: This study identifies that visual-related functional decoding is distributed across different decoder layers in Multimodal Large Language Models (MLLMs). Typically, each function, such as counting, grounding, or OCR recognition, narrows down to two or three layers, which we define as Vision Function Layers (VFL). Additionally, the depth and its order of different VFLs exhibits a consistent pattern across different MLLMs, which is well-aligned with human behaviors (e.g., recognition occurs first, followed by counting, and then grounding). These findings are derived from Visual Token Swapping, our novel analytical framework that modifies targeted KV cache entries to precisely elucidate layer-specific functions during decoding. Furthermore, these insights offer substantial utility in tailoring MLLMs for real-world downstream applications. For instance, when LoRA training is selectively applied to VFLs whose functions align with the training data, VFL-LoRA not only outperform full-LoRA but also prevent out-of-domain function forgetting. Moreover, by analyzing the performance differential on training data when particular VFLs are ablated, VFL-select automatically classifies data by function, enabling highly efficient data selection to directly bolster corresponding capabilities. Consequently, VFL-select surpasses human experts in data selection, and achieves 98% of full-data performance with only 20% of the original dataset. This study delivers deeper comprehension of MLLM visual processing, fostering the creation of more efficient, interpretable, and robust models.

**Link**: [arxiv](http://arxiv.org/abs/2509.24791v1),  [pdf](http://arxiv.org/pdf/2509.24791v1)

**Tags**: cs.CV 



### SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long   Sequences
**Authors**: Jungyoub Cha, Hyunjong Kim, Sungzoon Cho

**Updated**: 2025-09-29T12:34:50Z

**Summary**: Speculative decoding is a widely used technique for accelerating inference in large language models (LLMs), but its performance degrades as input length grows, with significant drops even at moderate lengths. Yet, this early degradation has remained largely underexplored. We introduce SpecExtend, a drop-in enhancement that improves speculative decoding on long sequences without additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention to accelerate prefill and verification steps. To improve both draft accuracy and speed on long inputs without retraining, we propose Cross-model Retrieval, a novel KV cache eviction strategy that leverages the target model's attention scores to dynamically select relevant context for the smaller draft model. Extensive evaluations show that SpecExtend accelerates speculative decoding by up to 2.84x on 16K-token long summarization and up to 3.86x on long reasoning, while preserving the short-input performance of state-of-the-art frameworks. Our code is available at https://github.com/jycha98/SpecExtend .

**Link**: [arxiv](http://arxiv.org/abs/2505.20776v3),  [pdf](http://arxiv.org/pdf/2505.20776v3)

**Tags**: cs.CL cs.AI cs.LG I.2.7; C.4 



### SANA-Video: Efficient Video Generation with Block Linear Diffusion   Transformer
**Authors**: Junsong Chen, Yuyang Zhao, Jincheng Yu, Ruihang Chu, Junyu Chen, Shuai Yang, Xianbang Wang, Yicheng Pan, Daquan Zhou, Huan Ling, Haozhe Liu, Hongwei Yi, Hao Zhang, Muyang Li, Yukang Chen, Han Cai, Sanja Fidler, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-09-29T12:28:09Z

**Summary**: We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.

**Link**: [arxiv](http://arxiv.org/abs/2509.24695v1),  [pdf](http://arxiv.org/pdf/2509.24695v1)

**Tags**: cs.CV cs.AI 



### SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in   Long-Context LLM Serving
**Authors**: Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng

**Updated**: 2025-09-29T11:35:55Z

**Summary**: Serving long-context LLMs is costly because attention computation grows linearly with context length. Dynamic sparse attention algorithms (DSAs) mitigate this by attending only to the key-value (KV) cache of critical tokens. However, with DSAs, the main performance bottleneck shifts from HBM bandwidth to HBM capacity: KV caches for unselected tokens must remain in HBM for low-latency decoding, constraining parallel batch size and stalling further throughput gains. Offloading these underutilized KV caches to DRAM could free HBM capacity, allowing larger parallel batch sizes. Yet, achieving such hierarchical HBM-DRAM storage raises new challenges, including fragmented KV cache access, HBM cache contention, and high HBM demands of hybrid batching, that remain unresolved in prior work.   This paper proposes SparseServe, an LLM serving system that unlocks the parallel potential of DSAs through efficient hierarchical HBM-DRAM management. SparseServe introduces three key innovations to address the challenges mentioned above: (1) fragmentation-aware KV cache transfer, which accelerates HBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted saving (FlashD2H); (2) working-set-aware batch size control that adjusts batch sizes based on real-time working set estimation to minimize HBM cache thrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a single layer, enabling efficient execution even for long prompts. Extensive experimental results demonstrate that SparseServe achieves up to 9.26x lower mean time-to-first-token (TTFT) latency and up to 3.14x higher token generation throughput compared to state-of-the-art LLM serving systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.24626v1),  [pdf](http://arxiv.org/pdf/2509.24626v1)

**Tags**: cs.DC 



### Q-REACH: Quantum information Repetition, Error Analysis and Correction   using Caching Network
**Authors**: Karl C. Linne, Yuanyuan Li, Debashri Roy, Kaushik Chowdhury

**Updated**: 2025-09-29T07:54:44Z

**Summary**: Quantum repeaters incorporating quantum memory play a pivotal role in mitigating loss in transmitted quantum information (photons) due to link attenuation over a long-distance quantum communication network. However, limited availability of available storage in such quantum repeaters and the impact on the time spent within the memory unit presents a trade-off between quantum information fidelity (a metric that quantifies the degree of similarity between a pair of quantum states) and qubit transmission rate. Thus, effective management of storage time for qubits becomes a key consideration in multi-hop quantum networks. To address these challenges, we propose Q-REACH, which leverages queuing theory in caching networks to tune qubit transmission rate while considering fidelity as the cost metric. Our contributions in this work include (i) utilizing a method of repetition that encodes and broadcasts multiple qubits through different quantum paths, (ii) analytically estimating the time spent by these emitted qubits as a function of the number of paths and repeaters, as well as memory units within a repeater, and (iii) formulating optimization problem that leverages this analysis to correct the transmitted logic qubit and select the optimum repetition rate at the transmitter.

**Link**: [arxiv](http://arxiv.org/abs/2509.24407v1),  [pdf](http://arxiv.org/pdf/2509.24407v1)

**Tags**: quant-ph 



### SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV   Caching
**Authors**: Yuxuan Zhu, Ali Falahati, David H. Yang, Mohammad Mohammadi Amiri

**Updated**: 2025-09-29T05:12:51Z

**Summary**: Large language models face significant computational and memory challenges when processing long contexts. During inference, efficient management of the key-value (KV) cache, which stores intermediate activations for autoregressive generation, is critical to reducing memory overhead and improving computational efficiency. Traditional token-level efficient KV caching methods overlook semantic information, treating tokens independently without considering their semantic relationships. Meanwhile, existing semantic-preserving KV cache management approaches often suffer from substantial memory usage and high time-to-first-token. To address these limitations, we propose SentenceKV, a novel sentence-level semantic KV caching approach designed to enhance inference efficiency while preserving semantic coherence. During prefilling, SentenceKV groups tokens based on sentence-level semantic similarity, compressing sentence representations into concise semantic vectors stored directly on the GPU, while individual KV pairs are offloaded to CPU. During decoding, SentenceKV generates tokens by selectively retrieving semantically relevant sentence-level KV entries, leveraging the semantic similarity between the prefilling-stage semantic vectors and decoding-stage queries. This ensures efficient and contextually accurate predictions, minimizing the loading of redundant or irrelevant data into GPU memory and significantly reducing memory overhead while maintaining stable inference latency, even for extremely long contexts. Extensive evaluations on benchmarks including PG-19, LongBench, and Needle-In-A-Haystack demonstrate that SentenceKV significantly outperforms state-of-the-art methods in both efficiency and memory usage, without compromising model accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2504.00970v2),  [pdf](http://arxiv.org/pdf/2504.00970v2)

**Tags**: cs.CL cs.AI cs.LG 



### Plug-and-Play 1.x-Bit KV Cache Quantization for Video Large Language   Models
**Authors**: Keda Tao, Haoxuan You, Yang Sui, Can Qin, Huan Wang

**Updated**: 2025-09-29T02:46:45Z

**Summary**: Video large language models (VideoLLMs) have demonstrated the capability to process longer video inputs and enable complex reasoning and analysis. However, due to the thousands of visual tokens from the video frames, the key-value (KV) cache can significantly increase memory requirements, becoming a bottleneck for inference speed and memory usage. KV cache quantization is a widely used approach to address this problem. In this paper, we find that 2-bit KV quantization of VideoLLMs can hardly hurt the model performance, while the limit of KV cache quantization in even lower bits has not been investigated. To bridge this gap, we introduce VidKV, a plug-and-play KV cache quantization method to compress the KV cache to lower than 2 bits. Specifically, (1) for key, we propose a mixed-precision quantization strategy in the channel dimension, where we perform 2-bit quantization for anomalous channels and 1-bit quantization combined with FFT for normal channels; (2) for value, we implement 1.58-bit quantization while selectively filtering semantically salient visual tokens for targeted preservation, for a better trade-off between precision and model performance. Importantly, our findings suggest that the value cache of VideoLLMs should be quantized in a per-channel fashion instead of the per-token fashion proposed by prior KV cache quantization works for LLMs. Empirically, extensive results with LLaVA-OV-7B and Qwen2.5-VL-7B on six benchmarks show that VidKV effectively compresses the KV cache to 1.5-bit and 1.58-bit precision with almost no performance drop compared to the FP16 counterparts.

**Link**: [arxiv](http://arxiv.org/abs/2503.16257v2),  [pdf](http://arxiv.org/pdf/2503.16257v2)

**Tags**: cs.CV 



### BladderFormer: A Streaming Transformer for Real-Time Urological State   Monitoring
**Authors**: Chengwei Zhou, Steve Majerus, Gourav Datta

**Updated**: 2025-09-29T01:52:10Z

**Summary**: Bladder pressure monitoring systems are increasingly vital in diagnosing and managing urinary tract dysfunction. Existing solutions rely heavily on hand-crafted features and shallow classifiers, limiting their adaptability to complex signal dynamics. We propose a one-layer streaming transformer model for real-time classification of bladder pressure states, operating on wavelet-transformed representations of raw time-series data. Our model incorporates temporal multi-head self-attention and state caching, enabling efficient online inference with high adaptability. Trained on a dataset of 91 patients with 20,000-80,000 samples each, our method demonstrates improved accuracy, higher energy- and latency-efficiency. Implementation considerations for edge deployment on low-power hardware, such as edge graphical processing units (GPU) and micro-controllers, are also discussed.

**Link**: [arxiv](http://arxiv.org/abs/2509.24178v1),  [pdf](http://arxiv.org/pdf/2509.24178v1)

**Tags**: eess.SP 



### CORRECT: COndensed eRror RECognition via knowledge Transfer in   multi-agent systems
**Authors**: Yifan Yu, Moyan Li, Shaoyuan Xu, Jinmiao Fu, Xinhai Hou, Fan Lai, Bryan Wang

**Updated**: 2025-09-28T21:47:20Z

**Summary**: Multi-agent systems (MAS) are increasingly capable of tackling complex real-world tasks, yet their reliance on inter-agent coordination, tool use, and long-horizon reasoning makes error recognition particularly challenging. Minor errors can propagate across agents, escalating into task failures while producing long, intertwined execution trajectories that impose significant costs for both human developers and automated systems to debug and analyze. Our key insight is that, despite surface differences in failure trajectories (e.g., logs), MAS errors often recur with similar structural patterns. This paper presents CORRECT, the first lightweight, training-free framework that leverages an online cache of distilled error schemata to recognize and transfer knowledge of failure structures across new requests. This cache-based reuse allows LLMs to perform targeted error localization at inference time, avoiding the need for expensive retraining while adapting to dynamic MAS deployments in subseconds. To support rigorous study in this domain, we also introduce CORRECT-Error, a large-scale dataset of over 2,000 annotated trajectories collected through a novel error-injection pipeline guided by real-world distributions, and further validated through human evaluation to ensure alignment with natural failure patterns. Experiments across seven diverse MAS applications show that CORRECT improves step-level error localization up to 19.8% over existing advances while at near-zero overhead, substantially narrowing the gap between automated and human-level error recognition.

**Link**: [arxiv](http://arxiv.org/abs/2509.24088v1),  [pdf](http://arxiv.org/pdf/2509.24088v1)

**Tags**: cs.MA 



### Sequential Diffusion Language Models
**Authors**: Yangzhou Liu, Yue Cao, Hao Li, Gen Luo, Zhe Chen, Weiyun Wang, Xiaobo Liang, Biqing Qi, Lijun Wu, Changyao Tian, Yanting Zhang, Yuqiang Li, Tong Lu, Yu Qiao, Jifeng Dai, Wenhai Wang

**Updated**: 2025-09-28T17:59:15Z

**Summary**: Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM

**Link**: [arxiv](http://arxiv.org/abs/2509.24007v1),  [pdf](http://arxiv.org/pdf/2509.24007v1)

**Tags**: cs.CL cs.LG 



### HiViS: Hiding Visual Tokens from the Drafter for Speculative Decoding in   Vision-Language Models
**Authors**: Zhinan Xie, Peisong Wang, Jian Cheng

**Updated**: 2025-09-28T15:05:21Z

**Summary**: Speculative decoding is an effective approach for accelerating inference in Large Language models (LLMs), but its adaptation to Vision-Language models (VLMs) remains challenging for additional visual tokens in multimodal inputs. First, owing to the fact that the drafter and the target VLM may derived from different families, the semantic representations of visual tokens in the target VLM are misaligned with those in the drafter, introducing bias into the KV-cache during the prefill stage. Second, the large number of visual tokens substantially slows down the drafter's self-attention during the decoding stage. We propose Hiding Visual Tokens from the Drafter for Speculative Decoding in Vision-Language Models (HiViS), an explicit-implicit input decomposition framework that alleviates the above inefficiency. All visual tokens are removed from the drafter's input, retaining only textual tokens as explicit inputs, while directly reusing the target VLM's corresponding last-layer hidden states as implicit visual information without additional processing. To train the drafter efficiently, we introduces multi-step self-feedback training strategy with dynamic data selection and sequential embedding supervision to simulate reasoning during training. Our approach compresses the prefill sequence length of the drafter to only 0.7%-1.3% of the target VLM's input, while maintaining lossless generation quality. Extensive experiments across diverse models and tasks demonstrate up to 2.65x speedup, confirming the effectiveness of HiViS in accelerating VLM inference.

**Link**: [arxiv](http://arxiv.org/abs/2509.23928v1),  [pdf](http://arxiv.org/pdf/2509.23928v1)

**Tags**: cs.LG cs.AI 



### SALM: A Multi-Agent Framework for Language Model-Driven Social Network   Simulation
**Authors**: Gaurav Koley

**Updated**: 2025-09-28T08:32:26Z

**Summary**: Contemporary approaches to agent-based modeling (ABM) of social systems have traditionally emphasized rule-based behaviors, limiting their ability to capture nuanced dynamics by moving beyond predefined rules and leveraging contextual understanding from LMs of human social interaction. This paper presents SALM (Social Agent LM Framework), a novel approach for integrating language models (LMs) into social network simulation that achieves unprecedented temporal stability in multi-agent scenarios. Our primary contributions include: (1) a hierarchical prompting architecture enabling stable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2) an attention-based memory system achieving 80% cache hit rates (95% CI [78%, 82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on personality stability. Through extensive validation against SNAP ego networks, we demonstrate the first LLM-based framework capable of modeling long-term social phenomena while maintaining empirically validated behavioral fidelity.

**Link**: [arxiv](http://arxiv.org/abs/2505.09081v2),  [pdf](http://arxiv.org/pdf/2505.09081v2)

**Tags**: cs.SI cs.AI cs.MA 



### VAMamba: An Efficient Visual Adaptive Mamba for Image Restoration
**Authors**: Han Hu, Zhuoran Zheng, Liang Li, Chen Lyu

**Updated**: 2025-09-28T03:12:43Z

**Summary**: Recent Mamba-based image restoration methods have achieved promising results but remain   limited by fixed scanning patterns and inefficient feature utilization. Conventional Mamba   architectures rely on predetermined paths that cannot adapt to diverse degradations, constraining   both restoration performance and computational efficiency. To overcome these limitations, we   propose VAMamba, a Visual Adaptive Mamba framework with two key innovations. First,   QCLAM(Queue-basedCacheLow-rankAdaptiveMemory)enhancesfeaturelearningthrougha   FIFO cache that stores historical representations. Similarity between current LoRA-adapted and   cached features guides intelligent fusion, enabling dynamic reuse while effectively controlling   memorygrowth.Second, GPS-SS2D(GreedyPathScanSS2D)introducesadaptive scanning. A   Vision Transformer generates score maps to estimate pixel importance, and a greedy strategy de termines optimal forward and backward scanning paths. These learned trajectories replace rigid   patterns, enabling SS2D to perform targeted feature extraction. The integration of QCLAM and   GPS-SS2D allows VAMamba to adaptively focus on degraded regions while maintaining high   computational efficiency. Extensive experiments across diverse restoration tasks demonstrate   that VAMamba consistently outperforms existing approaches in both restoration quality and   efficiency, establishing new benchmarks for adaptive image restoration. Our code is available   at https://github.com/WaterHQH/VAMamba.

**Link**: [arxiv](http://arxiv.org/abs/2509.23601v1),  [pdf](http://arxiv.org/pdf/2509.23601v1)

**Tags**: cs.CV 



### READER: Retrieval-Assisted Drafter for Efficient LLM Inference
**Authors**: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Stanislav Ilyushin, Sultan Isali, Vasily Kalugin, Nuriza Aitassova, Fei Yi, Weidi Zeng

**Updated**: 2025-09-27T20:13:25Z

**Summary**: Autoregressive Language Models instantiate a factorized likelihood over token sequences, yet their strictly sequential decoding process imposes an intrinsic lower bound on inference latency. This bottleneck has emerged as a central obstacle to the scalable deployment of large-scale generative models. Existing acceleration techniques partially mitigate token-level latency by relying on auxiliary draft models or introducing an additional training phase, but fail to address the dominant memory and communication costs. We present READER, a provably lossless speculative decoding framework that bypasses the training of the auxiliary draft model. READER formalizes speculative decoding as a stochastic tree construction problem and exploits the empirical redundancy structure of natural language to generate high-probability candidate continuations. Our method revisits the problem of constructing draft trees, establishing substantial statistical improvements over stochastic draft-tree methods and providing a complexity-theoretic analysis that characterizes the optimality frontier of speculative decoding under bounded computation and memory resources. Beyond the single-sequence regime traditionally considered in prior work, we introduce a memory-optimal key-value cache-serving strategy that guarantees amortized sublinear overhead in the batch dimension, allowing READER to scale to realistic inference workloads. Comprehensive experiments demonstrate up to 6.13x wall-clock speedup on single-prompt inference and up to 5.92x on batched inference, consistently surpassing prior speculative decoding baselines, while preserving exact output equivalence, with even more pronounced gains in retrieval-augmented generation pipelines. Our results close a key gap between theoretical parallelism limits and practical LLM inference, suggesting a new standard for efficient deployment.

**Link**: [arxiv](http://arxiv.org/abs/2508.09072v2),  [pdf](http://arxiv.org/pdf/2508.09072v2)

**Tags**: cs.CL 



### A Near-Cache Architectural Framework for Cryptographic Computing
**Authors**: Jingyao Zhang, Elaheh Sadredini

**Updated**: 2025-09-27T08:15:17Z

**Summary**: Recent advancements in post-quantum cryptographic algorithms have led to their standardization by the National Institute of Standards and Technology (NIST) to safeguard information security in the post-quantum era. These algorithms, however, employ public keys and signatures that are 3 to 9$\times$ longer than those used in pre-quantum cryptography, resulting in significant performance and energy efficiency overheads. A critical bottleneck identified in our analysis is the cache bandwidth. This limitation motivates the adoption of on-chip in-/near-cache computing, a computing paradigm that offers high-performance, exceptional energy efficiency, and flexibility to accelerate post-quantum cryptographic algorithms. Our analysis of existing works reveals challenges in integrating in-/near-cache computing into modern computer systems and performance limitations due to external bandwidth limitation, highlighting the need for innovative solutions that can seamlessly integrate into existing systems without performance and energy efficiency issues. In this paper, we introduce a near-cache-slice computing paradigm with support of customization and virtual address, named Crypto-Near-Cache (CNC), designed to accelerate post-quantum cryptographic algorithms and other applications. By placing SRAM arrays with bitline computing capability near cache slices, high internal bandwidth and short data movement are achieved with native support of virtual addressing. An ISA extension to facilitate CNC is also proposed, with detailed discussion on the implementation aspects of the core/cache datapath.

**Link**: [arxiv](http://arxiv.org/abs/2509.23179v1),  [pdf](http://arxiv.org/pdf/2509.23179v1)

**Tags**: cs.AR cs.CR 



### Runtime Adaptive Pruning for LLM Inference
**Authors**: Huanrong Liu, Chunlin Tian, Xuyang Wei, Qingbiao Li, Li Li

**Updated**: 2025-09-27T07:41:38Z

**Summary**: Large language models (LLMs) excel at language understanding and generation, but their enormous computational and memory requirements hinder deployment. Compression offers a potential solution to mitigate these constraints. However, most existing methods rely on fixed heuristics and thus fail to adapt to runtime memory variations or heterogeneous KV-cache demands arising from diverse user requests. To address these limitations, we propose RAP, an elastic pruning framework driven by reinforcement learning (RL) that dynamically adjusts compression strategies in a runtime-aware manner. Specifically, RAP dynamically tracks the evolving ratio between model parameters and KV-cache across practical execution. Recognizing that FFNs house most parameters, whereas parameter -light attention layers dominate KV-cache formation, the RL agent retains only those components that maximize utility within the current memory budget, conditioned on instantaneous workload and device state. Extensive experiments results demonstrate that RAP outperforms state-of-the-art baselines, marking the first time to jointly consider model weights and KV-cache on the fly.

**Link**: [arxiv](http://arxiv.org/abs/2505.17138v4),  [pdf](http://arxiv.org/pdf/2505.17138v4)

**Tags**: cs.LG cs.AI 



### d$^2$Cache: Accelerating Diffusion-Based LLMs via Dual Adaptive Caching
**Authors**: Yuchu Jiang, Yue Cai, Xiangzhong Luo, Jiale Fu, Jiarui Wang, Chonghan Liu, Xu Yang

**Updated**: 2025-09-27T04:07:23Z

**Summary**: Diffusion-based large language models (dLLMs), despite their promising performance, still suffer from inferior inference efficiency. This is because dLLMs rely on bidirectional attention and cannot directly benefit from the standard key-value (KV) cache as autoregressive models (ARMs) do. To tackle this issue, we introduce \textit{Dual aDaptive Cache} (d$^2$Cache), which is a training-free approximate KV cache framework for accelerating dLLM inference. d$^2$Cache features a two-stage fine-grained selection strategy to identify tokens and adaptively update their KV states at each decoding step, while caching the KV states of the remaining tokens for reuse. Furthermore, d$^2$Cache naturally offers a more reliable decoding alternative, which can enable quasi left-to-right generation and mitigate premature overconfidence in tokens at the end of the sequence. Extensive experimental results on two representative dLLMs (\ie, LLaDA and Dream) demonstrate that d$^2$Cache not only achieves substantial inference speedups, but also yields consistent improvements in generation quality. The code is available at https://github.com/Kamichanw/d2Cache.

**Link**: [arxiv](http://arxiv.org/abs/2509.23094v1),  [pdf](http://arxiv.org/pdf/2509.23094v1)

**Tags**: cs.CL 



### ReCalKV: Low-Rank KV Cache Compression via Head Reordering and Offline   Calibration
**Authors**: Xianglong Yan, Zhiteng Li, Tianao Zhang, Haotong Qin, Linghe Kong, Yulun Zhang, Xiaokang Yang

**Updated**: 2025-09-27T03:37:40Z

**Summary**: Large language models (LLMs) have demonstrated remarkable performance, but their long-context reasoning remains constrained by the excessive memory required for the Key-Value (KV) cache. This makes KV cache compression a critical step toward efficient long-context inference. Recent methods have explored low-rank techniques to reduce the hidden size of the KV cache. However, they neglect the distinct roles and varying importance of Keys and Values, leading to significant performance drops under high compression. To address this, we propose ReCalKV, a post-training low-rank KV cache compression approach with tailored strategies for Keys and Values. For Keys, we propose Head-wise Similarity aware Reordering (HSR), which clusters structurally similar heads into groups, enabling more accurate low-rank approximation via grouped SVD. For Values, we propose Offline Value Calibration (OVC), which efficiently calibrates the value projection matrix using calibration data without training, ensuring an accurate representation of contextual information. Extensive experiments show that ReCalKV consistently outperforms existing low-rank compression methods, achieving high compression ratios with minimal performance loss. The code and models will be available at:https://github.com/XIANGLONGYAN/ReCalKV.

**Link**: [arxiv](http://arxiv.org/abs/2505.24357v3),  [pdf](http://arxiv.org/pdf/2505.24357v3)

**Tags**: cs.LG cs.AI 



### vCache: Verified Semantic Prompt Caching
**Authors**: Luis Gaspar Schroeder, Aditya Desai, Alejandro Cuadron, Kyle Chu, Shu Liu, Mark Zhao, Stephan Krusche, Alfons Kemper, Ion Stoica, Matei Zaharia, Joseph E. Gonzalez

**Updated**: 2025-09-26T21:40:58Z

**Summary**: Semantic caches return cached responses for semantically similar prompts to reduce LLM inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, can result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines. We release the vCache implementation and three benchmarks to support future research.

**Link**: [arxiv](http://arxiv.org/abs/2502.03771v4),  [pdf](http://arxiv.org/pdf/2502.03771v4)

**Tags**: cs.LG cs.CL 



### On KV-Poisson Structure and related invariants
**Authors**: Prosper Rosaire Mama Assandje, Herguey Mopeng, Joseph Dongho

**Updated**: 2025-09-26T19:40:33Z

**Summary**: We propose an deepened analysis of KV-Poisson structures of on IR^2. We present their classification their properties an their possible applications in different domains. We prove that these structure give rise to a new Cohomological invariant. We explicitly compute the Cohomological groups of some of these structures.

**Link**: [arxiv](http://arxiv.org/abs/2509.22875v1),  [pdf](http://arxiv.org/pdf/2509.22875v1)

**Tags**: math.DG 



### KV Cache Steering for Controlling Frozen LLMs
**Authors**: Max Belitsky, Dawid J. Kopiczko, Michael Dorkenwald, M. Jehanzeb Mirza, James R. Glass, Cees G. M. Snoek, Yuki M. Asano

**Updated**: 2025-09-26T17:59:54Z

**Summary**: We propose cache steering, a lightweight method for implicit steering of language models via a one-shot intervention applied directly to the key-value cache. To validate its effectiveness, we apply cache steering to induce chain-of-thought reasoning in small language models. Our approach constructs steering vectors from reasoning traces, obtained either from teacher models (e.g., GPT-4o) or existing human annotations, that shift model behavior toward more explicit, multi-step reasoning without fine-tuning or prompt modifications. Experimental evaluations on diverse reasoning benchmarks demonstrate that cache steering improves both the qualitative structure of model reasoning and quantitative task performance. Additional experiments show that the method also scales to larger models and yields further gains on challenging datasets such as GPQA and MATH. Compared to prior activation steering techniques that require continuous interventions, our one-shot cache steering offers substantial advantages in terms of inference latency, hyperparameter stability, and ease of integration with existing inference APIs. Beyond mere reasoning induction, we show that cache steering enables controllable transfer of reasoning styles (e.g., stepwise, causal, analogical), making it a practical tool for behavior-level guidance of language models.

**Link**: [arxiv](http://arxiv.org/abs/2507.08799v2),  [pdf](http://arxiv.org/pdf/2507.08799v2)

**Tags**: cs.CL cs.AI 



### LongLive: Real-time Interactive Long Video Generation
**Authors**: Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen

**Updated**: 2025-09-26T17:48:24Z

**Summary**: We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.

**Link**: [arxiv](http://arxiv.org/abs/2509.22622v1),  [pdf](http://arxiv.org/pdf/2509.22622v1)

**Tags**: cs.CV 



### JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory   for Vision-Language Navigation
**Authors**: Shuang Zeng, Dekang Qi, Xinyuan Chang, Feng Xiong, Shichao Xie, Xiaolong Wu, Shiyi Liang, Mu Xu, Xing Wei

**Updated**: 2025-09-26T16:29:37Z

**Summary**: Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2509.22548v1),  [pdf](http://arxiv.org/pdf/2509.22548v1)

**Tags**: cs.CV cs.RO 



### TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent   and Explainable Digital Assessments
**Authors**: Rakesh Thakur, Shivaansh Kaushik, Gauri Chopra, Harsh Rohilla

**Updated**: 2025-09-26T16:00:36Z

**Summary**: This paper introduces TrueGradeAI, an AI-driven digital examination framework designed to overcome the shortcomings of traditional paper-based assessments, including excessive paper usage, logistical complexity, grading delays, and evaluator bias. The system preserves natural handwriting by capturing stylus input on secure tablets and applying transformer-based optical character recognition for transcription. Evaluation is conducted through a retrieval-augmented pipeline that integrates faculty solutions, cache layers, and external references, enabling a large language model to assign scores with explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems that primarily digitize responses, TrueGradeAI advances the field by incorporating explainable automation, bias mitigation, and auditable grading trails. By uniting handwriting preservation with scalable and transparent evaluation, the framework reduces environmental costs, accelerates feedback cycles, and progressively builds a reusable knowledge base, while actively working to mitigate grading bias and ensure fairness in assessment.

**Link**: [arxiv](http://arxiv.org/abs/2509.22516v1),  [pdf](http://arxiv.org/pdf/2509.22516v1)

**Tags**: cs.AI cs.LG 



### AxLLM: accelerator architecture for large language models with   computation reuse capability
**Authors**: Soroush Ahadi, Mehdi Modarressi, Masoud Daneshtalab

**Updated**: 2025-09-26T15:54:50Z

**Summary**: Large language models demand massive computational power and memory resources, posing significant challenges for efficient deployment. While quantization has been widely explored to reduce model size and computation, this paper demonstrates an additional benefit: quantization increases parameter locality, creating opportunities for computation reuse. Building on this insight, we propose AxLLM, a hardware accelerator architecture designed for quantized models. Axllm introduces a novel redundancy elimination technique that caches and reuses multiplication results for repeated weight values, substantially reducing redundant operations. The architecture features dual multiply and reuse pipelines, efficiently supporting both base models and LoRA fine-tuned models without altering parameters, retraining, or requiring offline preprocessing. Experimental results show that AxLLM achieves up to 90% reduction in computations, delivering 28% lower energy consumption and a 1.7x speedup over baseline execution. These results highlight Axllm as a scalable and efficient solution for accelerating LLMs on specialized hardware.

**Link**: [arxiv](http://arxiv.org/abs/2509.22512v1),  [pdf](http://arxiv.org/pdf/2509.22512v1)

**Tags**: cs.AR n/a 



### Organ dose optimization for a point-of-care forearm X-ray   photon-counting CT
**Authors**: Pierre-Antoine Rodesch, Ana√Øs Viry, Mouad Khorsi, Fabio Becce, J√©r√¥me Damet, Luc√≠a Gallego Manzano

**Updated**: 2025-09-26T15:35:05Z

**Summary**: Background: Spectral shaping is a computed tomography (CT) dose optimization technique that adjusts source voltage and filtration to reduce patient radiation exposure without compromising image quality. Traditionally, radiation dose has been assessed using the computed tomography dose index (CTDI). However, emerging dosimetric approaches aim to enable patient-specific evaluations by estimating organ absorbed doses, providing a more accurate representation of the biological impact. This study investigates spectral shaping for an extremity photon-counting detector (PCD) CT, through organ absorbed dose estimation and image quality evaluation. Method: Monte Carlo simulations were conducted to evaluate various combinations of source voltage and filtration. Tube voltage ranged from 80 to 140 kV, combined with three distinct filtration material and thicknesses. Simulations included three stages: a standardized phantom for CTDI assessment, an adult forearm phantom for organ dose measurement, and an image quality phantom for evaluation of an advanced image quality metric: the detectability index. Results: In a wrist PCD-CT imaging protocol, operating the source at 80 kV can reduce the radiation dose by up to 50%. This reduction is achieved while maintaining the same detectability index value as the standard 120 kV protocol. However, the optimal filtration depends on the organ targeted for dose reduction, as bone and skin benefit from opposing filtration approaches. While CTDI provides a useful initial estimate, it may lead to suboptimal optimization compared to organ-specific dose evaluation. Conclusions: Patient-specific dosimetry based on organ absorbed dose estimation offers a more accurate framework for optimizing CT protocols through spectral shaping than conventional CTDI-based approaches.

**Link**: [arxiv](http://arxiv.org/abs/2509.22488v1),  [pdf](http://arxiv.org/pdf/2509.22488v1)

**Tags**: physics.med-ph 



### Bottlenecked Transformers: Periodic KV Cache Consolidation for   Generalised Reasoning
**Authors**: Adnan Oomerjee, Zafeirios Fountas, Haitham Bou-Ammar, Jun Wang

**Updated**: 2025-09-26T14:35:04Z

**Summary**: Transformer LLMs have been shown to exhibit strong reasoning ability that scales with inference-time compute, most prominently through token-space "thinking" chains of thought. A growing line of work pushes extra computation into the model's latent space, which we term Auxiliary Latent-Space Computation (ALSC). Existing ALSC methods largely fall into three buckets: (i) token-mediated latent rollouts, (ii) residual/activation steering, and (iii) memory (KV) compression. An underexplored alternative is memory consolidation/reconsolidation, two processes in the brain that are responsible for stabilising newly formed memory traces, and, upon recall, transiently rendering established traces plastic such they can integrate new contextual information before restabilising. In Transformer LLMs, this can be seen as analogous to performing in-place rewrites of new KV segments, and rewrites of recalled past segments. In this work, we give a theoretical justification as to why memory (re)consolidation via KV cache rewrites is beneficial for improved reasoning. We do this through the lens of Information Bottleneck (IB) theory, which posits that model generalisation emerges from an optimal balance between input information compression and retention of predictive information in latent representations. We then introduce the Bottlenecked Transformer, which augments a backbone LLM with a Cache Processor, an auxiliary Transformer that performs periodic, non-causal, in-place KV rewrites at newline-delimited reasoning step boundaries. The Processor consolidates recently written KV entries and reconsolidates a small, top-k attention-selected set of prior entries. We evaluate our Bottlenecked Transformer architecture on math reasoning benchmarks. Our model sees consistent performance gains over vanilla Transformers and pause-token augmented baselines, with gains of up to +6.6pp for selected tasks/backbones.

**Link**: [arxiv](http://arxiv.org/abs/2505.16950v3),  [pdf](http://arxiv.org/pdf/2505.16950v3)

**Tags**: cs.LG cs.AI cs.IT math.IT 



### RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion   Transformer
**Authors**: Wangbo Zhao, Yizeng Han, Zhiwei Tang, Jiasheng Tang, Pengfei Zhou, Kai Wang, Bohan Zhuang, Zhangyang Wang, Fan Wang, Yang You

**Updated**: 2025-09-26T13:20:52Z

**Summary**: Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model's distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality.

**Link**: [arxiv](http://arxiv.org/abs/2509.22323v1),  [pdf](http://arxiv.org/pdf/2509.22323v1)

**Tags**: cs.CV 



### Spineless Traversal for Layout Invalidation
**Authors**: Marisa Kirisame, Tiezhi Wang, Pavel Panchekha

**Updated**: 2025-09-26T10:00:54Z

**Summary**: Latency is a major concern for web rendering engines like those in Chrome, Safari, and Firefox. These engines reduce latency by using an incremental layout algorithm to redraw the page when the user interacts with it. In such an algorithm, elements that change frame-to-frame are marked dirty, and only those elements are processed to draw the next frame, dramatically reducing latency. However, the standard incremental layout algorithm must search the page for dirty elements, accessing auxiliary elements in the process. These auxiliary elements add cache misses and stalled cycles, and are responsible for a sizable fraction of all layout latency. We introduce a new, faster incremental layout algorithm called Spineless Traversal. Spineless Traversal uses a cache-friendlier priority queue algorithm that avoids accessing auxiliary nodes and thus reduces cache traffic and stalls. This leads to dramatic speedups on the most latency-critical interactions such as hovering, typing, and animation. Moreover, thanks to numerous low-level optimizations, Spineless Traversal is competitive across the whole spectrum of incremental layout workloads. Spineless Traversal is faster than the standard approach on 83.0% of 2216 benchmarks, with a mean speedup of 1.80x concentrated in the most latency-critical interactions.

**Link**: [arxiv](http://arxiv.org/abs/2411.10659v8),  [pdf](http://arxiv.org/pdf/2411.10659v8)

**Tags**: cs.PL 



### Persistent Autoregressive Mapping with Traffic Rules for Autonomous   Driving
**Authors**: Shiyi Liang, Xinyuan Chang, Changjie Wu, Huiyuan Yan, Yifan Bai, Xinran Liu, Hang Zhang, Yujian Yuan, Shuang Zeng, Mu Xu, Xing Wei

**Updated**: 2025-09-26T09:33:36Z

**Summary**: Safe autonomous driving requires both accurate HD map construction and persistent awareness of traffic rules, even when their associated signs are no longer visible. However, existing methods either focus solely on geometric elements or treat rules as temporary classifications, failing to capture their persistent effectiveness across extended driving sequences. In this paper, we present PAMR (Persistent Autoregressive Mapping with Traffic Rules), a novel framework that performs autoregressive co-construction of lane vectors and traffic rules from visual observations. Our approach introduces two key mechanisms: Map-Rule Co-Construction for processing driving scenes in temporal segments, and Map-Rule Cache for maintaining rule consistency across these segments. To properly evaluate continuous and consistent map generation, we develop MapDRv2, featuring improved lane geometry annotations. Extensive experiments demonstrate that PAMR achieves superior performance in joint vector-rule mapping tasks, while maintaining persistent rule effectiveness throughout extended driving sequences.

**Link**: [arxiv](http://arxiv.org/abs/2509.22756v1),  [pdf](http://arxiv.org/pdf/2509.22756v1)

**Tags**: cs.RO cs.AI 



### LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for   Multi-Turn Dialogues
**Authors**: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan

**Updated**: 2025-09-26T07:14:44Z

**Summary**: Multi-turn dialogues are essential in many real-world applications of large language models, such as chatbots and virtual assistants. As conversation histories become longer, existing large language models face increasing computational and memory challenges, which hinder their ability to provide efficient and responsive interactions. Most current acceleration methods either compress the context or optimize key value caching, but they often rely on fixed or position-based heuristics that do not adapt well to the dynamic and unpredictable patterns found in actual multi-turn conversations. As a result, these models cannot accurately identify and prioritize the most relevant context, leading to degraded response quality. In this paper, we present LoopServe, an adaptive dual-phase inference acceleration framework for large language models in multi-turn dialogues. LoopServe introduces two main innovations. First, it performs online sparsification during the prefilling phase by dynamically selecting the most important parts of the attention matrix for each new input. Second, it uses progressive key value compression during decoding by adaptively maintaining a relevant and efficient cache based on the most recently generated output tokens. We also propose a new benchmark with eleven multi-turn datasets that reflect realistic query positions and conversational dependencies. Extensive experiments demonstrate that LoopServe consistently achieves superior effectiveness compared to existing baselines and significantly accelerates LLM inference across a wide range of long-context dialogue tasks.

**Link**: [arxiv](http://arxiv.org/abs/2507.13681v2),  [pdf](http://arxiv.org/pdf/2507.13681v2)

**Tags**: cs.CL cs.AI 



### Taming Flow-based I2V Models for Creative Video Editing
**Authors**: Xianghao Kong, Hansheng Chen, Yuwei Guo, Lvmin Zhang, Gordon Wetzstein, Maneesh Agrawala, Anyi Rao

**Updated**: 2025-09-26T05:57:04Z

**Summary**: Although image editing techniques have advanced significantly, video editing, which aims to manipulate videos according to user intent, remains an emerging challenge. Most existing image-conditioned video editing methods either require inversion with model-specific design or need extensive optimization, limiting their capability of leveraging up-to-date image-to-video (I2V) models to transfer the editing capability of image editing models to the video domain. To this end, we propose IF-V2V, an Inversion-Free method that can adapt off-the-shelf flow-matching-based I2V models for video editing without significant computational overhead. To circumvent inversion, we devise Vector Field Rectification with Sample Deviation to incorporate information from the source video into the denoising process by introducing a deviation term into the denoising vector field. To further ensure consistency with the source video in a model-agnostic way, we introduce Structure-and-Motion-Preserving Initialization to generate motion-aware temporally correlated noise with structural information embedded. We also present a Deviation Caching mechanism to minimize the additional computational cost for denoising vector rectification without significantly impacting editing quality. Evaluations demonstrate that our method achieves superior editing quality and consistency over existing approaches, offering a lightweight plug-and-play solution to realize visual creativity.

**Link**: [arxiv](http://arxiv.org/abs/2509.21917v1),  [pdf](http://arxiv.org/pdf/2509.21917v1)

**Tags**: cs.CV cs.MM 



### 2.34 kV \b{eta}-Ga2O3 Vertical Trench RESURF Schottky Barrier Diode with   sub-micron fin width
**Authors**: Chinmoy Nath Saha, Saurav Roy, Yizheng Liu, Carl Peterson, Sriram Krishnamoorthy

**Updated**: 2025-09-26T04:32:56Z

**Summary**: In this letter, we present a kilovolt-class \b{eta}-Ga2O3 vertical trench Schottky barrier diode with a field plate incorporating narrow fin width (Wfin) structures of sub-micron dimensions. We used a nanolaminate dielectric comprising a stack of multiple thin TiO2 and Al2O3 layers as RESURF dielectric and for field plate edge termination. Both Wfin of 200 nm and 500 nm demonstrate excellent on-state performance with specific on-resistance (Ron,sp) of 9.8-12 mohmcm2, and 10^10 rectification ratio. A self-aligned photoresist planarization and etch-back process was employed to expose the top of the fins for Schottky contact formation, eliminating critical lithographic alignment challenges in sub-micron scale processing. We achieved a breakdown of 2.34 kV with very low leakage currents before catastrophic breakdown. The measured breakdown voltage is limited by dielectric breakdown at the trench bottom corner as verified by metal-oxide-semiconductor (MOS) test structure. TCAD simulation shows a reduced electric field at the surface of the metal-semiconductor junction due to the RESURF effect, resulting in very low reverse leakage before breakdown. The parallel plane electric field in the \b{eta} -Ga2O3 is extracted to be 3.8 MV/cm from TCAD simulations using accurately extracted drift layer doping profile from high voltage CV measurements. A power figure of merit of 0.867 GW/cm2(0.56 GW/cm2 with current spreading) was calculated. Enhanced RESURF by integration of high-k dielectrics with self-aligned photoresist planarization, offers a promising pathway towards high figure of merit, low leakage high-performance vertical devices.

**Link**: [arxiv](http://arxiv.org/abs/2509.21857v1),  [pdf](http://arxiv.org/pdf/2509.21857v1)

**Tags**: physics.app-ph 



### DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for   Autonomous Travel Planning Agents
**Authors**: Yansong Ning, Rui Liu, Jun Wang, Kai Chen, Wei Li, Jun Fang, Kan Zheng, Naiqiang Tan, Hao Liu

**Updated**: 2025-09-26T04:03:52Z

**Summary**: Travel planning (TP) agent has recently worked as an emerging building block to interact with external tools and resources for travel itinerary generation, ensuring enjoyable user experience. Despite its benefits, existing studies rely on hand craft prompt and fixed agent workflow, hindering more flexible and autonomous TP agent. This paper proposes DeepTravel, an end to end agentic reinforcement learning framework for building autonomous travel planning agent, capable of autonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi step reasoning. To achieve this, we first construct a robust sandbox environment by caching transportation, accommodation and POI data, facilitating TP agent training without being constrained by real world APIs limitations (e.g., inconsistent outputs). Moreover, we develop a hierarchical reward modeling system, where a trajectory level verifier first checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and then the turn level verifier further validate itinerary detail consistency with tool responses, enabling efficient and precise reward service. Finally, we propose the reply augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity. We deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2509.21842v1),  [pdf](http://arxiv.org/pdf/2509.21842v1)

**Tags**: cs.AI 



### LiteGS: A High-performance Framework to Train 3DGS in Subminutes via   System and Algorithm Codesign
**Authors**: Kaimin Liao, Hua Wang, Zhi Chen, Luchao Wang, Yaohua Tang

**Updated**: 2025-09-26T03:24:20Z

**Summary**: 3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D representation. However, it still suffers from high training cost. This paper introduces LiteGS, a high performance framework that systematically optimizes the 3DGS training pipeline from multiple aspects. At the low-level computation layer, we design a ``warp-based raster'' associated with two hardware-aware optimizations to significantly reduce gradient reduction overhead. At the mid-level data management layer, we introduce dynamic spatial sorting based on Morton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and improve data locality, therefore reducing cache misses. At the top-level algorithm layer, we establish a new robust densification criterion based on the variance of the opacity gradient, paired with a more stable opacity control mechanism, to achieve more precise parameter growth. Experimental results demonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x with comparable or superior quality and surpasses the current SOTA in lightweight models by up to 1.4x speedup. For high-quality reconstruction tasks, LiteGS sets a new accuracy record and decreases the training time by an order of magnitude.

**Link**: [arxiv](http://arxiv.org/abs/2503.01199v3),  [pdf](http://arxiv.org/pdf/2503.01199v3)

**Tags**: cs.CV 



### DOTA: Distributional Test-Time Adaptation of Vision-Language Models
**Authors**: Zongbo Han, Jialong Yang, Guangyu Wang, Junfan Li, Qianli Xu, Mike Zheng Shou, Changqing Zhang

**Updated**: 2025-09-26T03:17:15Z

**Summary**: Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable performance across a wide range of tasks. However, deploying these models can be unreliable when significant distribution gaps exist between training and test data, while fine-tuning for diverse scenarios is often costly. Cache-based test-time adapters offer an efficient alternative by storing representative test samples to guide subsequent classifications. Yet, these methods typically employ naive cache management with limited capacity, leading to severe catastrophic forgetting when samples are inevitably dropped during updates. In this paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet effective method addressing this limitation. Crucially, instead of merely memorizing individual test samples, DOTA continuously estimates the underlying distribution of the test data stream. Test-time posterior probabilities are then computed using these dynamically estimated distributions via Bayes' theorem for adaptation. This distribution-centric approach enables the model to continually learn and adapt to the deployment environment. Extensive experiments validate that DOTA significantly mitigates forgetting and achieves state-of-the-art performance compared to existing methods.

**Link**: [arxiv](http://arxiv.org/abs/2409.19375v3),  [pdf](http://arxiv.org/pdf/2409.19375v3)

**Tags**: cs.LG cs.AI cs.CL cs.CV cs.HC 



### OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's   Rule
**Authors**: Yuxuan Zhu, David H. Yang, Mohammad Mohammadi Amiri, Keerthiram Murugesan, Tejaswini Pedapati, Pin-Yu Chen

**Updated**: 2025-09-25T21:42:27Z

**Summary**: The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation. This bottleneck is substantial; for instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of 4 requires approximately 16GB for its KV cache, a size exceeding the model's weights. While KV-cache compression via low-rank projection is a promising direction, existing methods rely on a static, offline-learned subspace that performs poorly under data distribution shifts. To overcome these limitations, we introduce OjaKV, a novel framework that integrates a strategic hybrid storage policy with online subspace adaptation. First, OjaKV recognizes that not all tokens are equally important for compression; it preserves the crucial first and most recent tokens in full-rank, maintaining high-fidelity anchors for attention. Second, for the vast majority of intermediate tokens, it applies low-rank compression by incrementally adapting the projection basis using Oja's algorithm for online principal component analysis. This adaptation involves a comprehensive update during prompt prefilling and lightweight periodic updates during decoding, ensuring the subspace remains aligned with the evolving context. Crucially, our framework is fully compatible with modern attention modules like FlashAttention. Experiments demonstrate that OjaKV maintains or even improves zero-shot accuracy at high compression ratios. In particular, OjaKV achieves its strongest gains on very long-context benchmarks that require complex reasoning, highlighting the importance of online subspace adaptation in dynamically tracking context shifts. These results establish our hybrid framework as a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.

**Link**: [arxiv](http://arxiv.org/abs/2509.21623v1),  [pdf](http://arxiv.org/pdf/2509.21623v1)

**Tags**: cs.CL cs.AI cs.LG 



### Enhanced Generative Machine Listener
**Authors**: Vishnu Raj, Gouthaman KV, Shiv Gehlot, Lars Villemoes, Arijit Biswas

**Updated**: 2025-09-25T19:29:25Z

**Summary**: We present GMLv2, a reference-based model designed for the prediction of subjective audio quality as measured by MUSHRA scores. GMLv2 introduces a Beta distribution-based loss to model the listener ratings and incorporates additional neural audio coding (NAC) subjective datasets to extend its generalization and applicability. Extensive evaluations on diverse testset demonstrate that proposed GMLv2 consistently outperforms widely used metrics, such as PEAQ and ViSQOL, both in terms of correlation with subjective scores and in reliably predicting these scores across diverse content types and codec configurations. Consequently, GMLv2 offers a scalable and automated framework for perceptual audio quality evaluation, poised to accelerate research and development in modern audio coding technologies.

**Link**: [arxiv](http://arxiv.org/abs/2509.21463v1),  [pdf](http://arxiv.org/pdf/2509.21463v1)

**Tags**: eess.AS cs.AI cs.LG 



### Autoregressive Image Generation with Randomized Parallel Decoding
**Authors**: Haopeng Li, Jinyue Yang, Guoqi Li, Huan Wang

**Updated**: 2025-09-25T13:55:44Z

**Summary**: We introduce ARPG, a novel visual autoregressive model that enables randomized parallel generation, addressing the inherent limitations of conventional raster-order approaches, which hinder inference efficiency and zero-shot generalization due to their sequential, predefined token generation order. Our key insight is that effective random-order modeling necessitates explicit guidance for determining the position of the next predicted token. To this end, we propose a novel decoupled decoding framework that decouples positional guidance from content representation, encoding them separately as queries and key-value pairs. By directly incorporating this guidance into the causal attention mechanism, our approach enables fully random-order training and generation, eliminating the need for bidirectional attention. Consequently, ARPG readily generalizes to zero-shot inference tasks such as image inpainting, outpainting, and resolution expansion. Furthermore, it supports parallel inference by concurrently processing multiple queries using a shared KV cache. On the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only 32 sampling steps, achieving over a 30 times speedup in inference and a 75 percent reduction in memory consumption compared to representative recent autoregressive models at a similar scale.

**Link**: [arxiv](http://arxiv.org/abs/2503.10568v2),  [pdf](http://arxiv.org/pdf/2503.10568v2)

**Tags**: cs.CV 



### WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP
**Authors**: Moshe Kimhi, Erez Koifman, Ehud Rivlin, Eli Schwartz, Chaim Baskin

**Updated**: 2025-09-25T13:39:16Z

**Summary**: We introduce WAVECLIP, a single unified model for adaptive resolution inference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces standard patch embeddings with a multi-level wavelet decomposition, enabling the model to process images coarse to fine while naturally supporting multiple resolutions within the same model. At inference time, the model begins with low resolution tokens and refines only when needed, using key-value caching and causal cross-level attention to reuse computation, effectively introducing to the model only new information when needed. We evaluate WAVECLIP in zero-shot classification, demonstrating that a simple confidence-based gating mechanism enables adaptive early exits. This allows users to dynamically choose a compute-accuracy trade-off using a single deployed model. Our approach requires only lightweight distillation from a frozen CLIP teacher and achieves competitive accuracy with significant computational savings.

**Link**: [arxiv](http://arxiv.org/abs/2509.21153v1),  [pdf](http://arxiv.org/pdf/2509.21153v1)

**Tags**: cs.CV cs.AI cs.MM 



### InComeS: Integrating Compression and Selection Mechanisms into LLMs for   Efficient Model Editing
**Authors**: Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, Wai Lam

**Updated**: 2025-09-25T13:15:45Z

**Summary**: Although existing model editing methods perform well in recalling exact edit facts, they often struggle in complex scenarios that require deeper semantic understanding rather than mere knowledge regurgitation. Leveraging the strong contextual reasoning abilities of large language models (LLMs), in-context learning (ICL) becomes a promising editing method by comprehending edit information through context encoding. However, this method is constrained by the limited context window of LLMs, leading to degraded performance and efficiency as the number of edits increases. To overcome this limitation, we propose InComeS, a flexible framework that enhances LLMs' ability to process editing contexts through explicit compression and selection mechanisms. Specifically, InComeS compresses each editing context into the key-value (KV) cache of a special gist token, enabling efficient handling of multiple edits without being restricted by the model's context window. Furthermore, specialized cross-attention modules are added to dynamically select the most relevant information from the gist pools, enabling adaptive and effective utilization of edit information. We conduct experiments on diverse model editing benchmarks with various editing formats, and the results demonstrate the effectiveness and efficiency of our method.

**Link**: [arxiv](http://arxiv.org/abs/2505.22156v2),  [pdf](http://arxiv.org/pdf/2505.22156v2)

**Tags**: cs.CL 



### EpiCache: Episodic KV Cache Management for Long Conversational Question   Answering
**Authors**: Minsoo Kim, Arnav Kundu, Han-Byul Kim, Richa Dixit, Minsik Cho

**Updated**: 2025-09-25T10:24:14Z

**Summary**: Modern large language models (LLMs) extend context lengths to up to millions of tokens, enabling AI assistants to generate coherent and personalized responses grounded in long conversational histories. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly becomes the bottleneck in resource-constrained environments. An active line of research for reducing memory bottleneck is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting the KV cache after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to failure cases in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.

**Link**: [arxiv](http://arxiv.org/abs/2509.17396v2),  [pdf](http://arxiv.org/pdf/2509.17396v2)

**Tags**: cs.CL 



### Toward Robust and Efficient ML-Based GPU Caching for Modern Inference
**Authors**: Peng Chen, Jiaji Zhang, Hailiang Zhao, Yirong Zhang, Jiahong Yu, Xueyan Tang, Yixuan Wang, Hao Li, Jianping Zou, Gang Xiong, Kingsum Chow, Shuibing He, Shuiguang Deng

**Updated**: 2025-09-25T10:23:50Z

**Summary**: In modern GPU inference, cache efficiency remains a major bottleneck. In recommendation models, embedding hit rates largely determine throughput, while in large language models, KV-cache misses substantially increase time-to-first-token (TTFT). Heuristic policies such as \textsc{LRU} often struggle under structured access patterns. Learning-based approaches are promising, but in practice face two major limitations: they degrade sharply when predictions are inaccurate, or they gain little even with accurate predictions due to conservative designs. Some also incur high overhead, further limiting practicality.   We present \textsc{LCR}, a practical framework for learning-based GPU caching that delivers performance gains while ensuring robustness and efficiency. Its core algorithm, \textsc{LARU}, enhances \textsc{LRU} with machine-learned predictions and dynamically adapts to prediction accuracy through online error estimation. When predictions are accurate, \textsc{LARU} achieves near-optimal performance. With inaccurate predictions, it degrades gracefully to near-\textsc{LRU} performance. With \textsc{LCR}, we bridge the gap between empirical progress and theoretical advances in learning-based caching.   Experiments show that \textsc{LCR} delivers consistent gains under realistic conditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\% and reduces P99 TTFT by up to 28.3\%, outperforming widely used inference systems. Even under poor predictions, its performance remains stable, demonstrating practical robustness.

**Link**: [arxiv](http://arxiv.org/abs/2509.20979v1),  [pdf](http://arxiv.org/pdf/2509.20979v1)

**Tags**: cs.LG 



### Robustifying Learning-Augmented Caching Efficiently without Compromising   1-Consistency
**Authors**: Peng Chen, Hailiang Zhao, Jiaji Zhang, Xueyan Tang, Yixuan Wang, Shuiguang Deng

**Updated**: 2025-09-25T09:49:59Z

**Summary**: The online caching problem aims to minimize cache misses when serving a sequence of requests under a limited cache size. While naive learning-augmented caching algorithms achieve ideal $1$-consistency, they lack robustness guarantees. Existing robustification methods either sacrifice $1$-consistency or introduce significant computational overhead. In this paper, we introduce Guard, a lightweight robustification framework that enhances the robustness of a broad class of learning-augmented caching algorithms to $2H_k + 2$, while preserving their $1$-consistency. Guard achieves the current best-known trade-off between consistency and robustness, with only $O(1)$ additional per-request overhead, thereby maintaining the original time complexity of the base algorithm. Extensive experiments across multiple real-world datasets and prediction models validate the effectiveness of Guard in practice.

**Link**: [arxiv](http://arxiv.org/abs/2507.16242v5),  [pdf](http://arxiv.org/pdf/2507.16242v5)

**Tags**: cs.DS cs.LG 



### ILRe: Intermediate Layer Retrieval for Context Compression in Causal   Language Models
**Authors**: Manlai Liang, Mandi Liu, Jiangzhou Ji, Huaijun Li, Haobo Yang, Yaohan He, Jinlong Li

**Updated**: 2025-09-25T03:30:06Z

**Summary**: Large Language Models (LLMs) have demonstrated success across many benchmarks. However, they still exhibit limitations in long-context scenarios, primarily due to their short effective context length, quadratic computational complexity, and high memory overhead when processing lengthy inputs. To mitigate these issues, we introduce a novel context compression pipeline, called Intermediate Layer Retrieval (ILRe), which determines one intermediate decoder layer offline, encodes context by streaming chunked prefill only up to that layer, and recalls tokens by the attention scores between the input query and full key cache in that specified layer. In particular, we propose a multi-pooling kernels allocating strategy in the token recalling process to maintain the completeness of semantics. Our approach not only reduces the prefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to a few tenths of that required for the full context, but also delivers performance comparable to or superior to the full-context setup in long-context scenarios. Without additional post training or operator development, ILRe can process a single $1M$ tokens request in less than half a minute (speedup $\approx 180\times$) and scores RULER-$1M$ benchmark of $\approx 79.8$ with model Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.

**Link**: [arxiv](http://arxiv.org/abs/2508.17892v2),  [pdf](http://arxiv.org/pdf/2508.17892v2)

**Tags**: cs.CL cs.LG 



### HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO   Serving and Fast Scaling
**Authors**: Zahra Yousefijamarani, Xinglu Wang, Qian Wang, Morgan Lindsay Heisler, Taha Shabani, Niloofar Gholipour, Parham Yassini, Hong Chang, Kan Chen, Qiantao Zhang, Xiaolong Bai, Jiannan Wang, Ying Xiong, Yong Zhang, Zhenan Fan

**Updated**: 2025-09-25T03:00:22Z

**Summary**: Modern large language model (LLM) serving systems face challenges from highly variable requests with diverse lengths, priorities, and stage-specific service-level objectives (SLOs). Meeting these requires real-time scheduling, rapid and cost-effective scaling, and support for both collocated and disaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a unified LLM serving system that integrates algorithmic and system-level innovations to jointly optimize scheduling and scaling under multiple SLOs. It features a multi-SLO-aware scheduler that leverages budget estimation and request prioritization to ensure proactive SLO compliance for both new and ongoing requests. The system supports prefill- and decode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV cache transfers. It also enables cost-effective scaling decisions, prefill-decode instance linking during scaling, and rapid P/D role transitions. To accelerate scaling and reduce cold-start latency, a device-to-device (D2D) weight transfer mechanism is proposed that lowers weight loading overhead by up to 19.39$\times$. These optimizations allow the system to achieve up to 4.44$\times$ higher SLO attainment, 65.82% lower request latency, and cost parity with state-of-the-art baselines. The code will be released soon.

**Link**: [arxiv](http://arxiv.org/abs/2508.15919v2),  [pdf](http://arxiv.org/pdf/2508.15919v2)

**Tags**: cs.DC cs.AI 



### DELM: a Python toolkit for Data Extraction with Language Models
**Authors**: Eric Fithian, Kirill Skobelev

**Updated**: 2025-09-24T23:47:55Z

**Summary**: Large Language Models (LLMs) have become powerful tools for annotating unstructured data. However, most existing workflows rely on ad hoc scripts, making reproducibility, robustness, and systematic evaluation difficult. To address these challenges, we introduce DELM (Data Extraction with Language Models), an open-source Python toolkit designed for rapid experimental iteration of LLM-based data extraction pipelines and for quantifying the trade-offs between them. DELM minimizes boilerplate code and offers a modular framework with structured outputs, built-in validation, flexible data-loading and scoring strategies, and efficient batch processing. It also includes robust support for working with LLM APIs, featuring retry logic, result caching, detailed cost tracking, and comprehensive configuration management. We showcase DELM's capabilities through two case studies: one featuring a novel prompt optimization algorithm, and another illustrating how DELM quantifies trade-offs between cost and coverage when selecting keywords to decide which paragraphs to pass to an LLM. DELM is available at \href{https://github.com/Center-for-Applied-AI/delm}{\texttt{github.com/Center-for-Applied-AI/delm}}.

**Link**: [arxiv](http://arxiv.org/abs/2509.20617v1),  [pdf](http://arxiv.org/pdf/2509.20617v1)

**Tags**: cs.IR 



### UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from   an Uncertainty-Aware Perspective
**Authors**: Jing Xiong, Jianghan Shen, Fanghua Ye, Chaofan Tao, Zhongwei Wan, Jianqiao Lu, Xun Wu, Chuanyang Zheng, Zhijiang Guo, Min Yang, Lingpeng Kong, Ngai Wong

**Updated**: 2025-09-24T16:56:17Z

**Summary**: Deploying large language models (LLMs) for long-context inference remains challenging due to their substantial memory and computational demands. While techniques such as Key-Value (KV) cache compression are designed to reduce memory usage, they often neglect the structured sparsity inherent in the relationship between hidden states and their corresponding KV cache. In this work, we explore the role of uncertainty as a potential indicator of sparsity within LLMs. We propose UNComp, an uncertainty-aware framework that leverages truncated matrix entropy to identify areas of low information content, thereby revealing sparsity patterns that can be used for adaptive compression. Unlike traditional methods that apply uniform compression, UNComp dynamically adjusts its approach to compression, guided by uncertainty measures that reflect the importance of various model components. Our analysis shows that sparsity patterns, when derived from uncertainty estimates, can be exploited to reveal special long-range dependencies, such as retrieval heads and retrieval layers. This perspective not only enhances our understanding of how compression can be optimized but also provides new insights into the inherent sparsity of LLMs during long-context inference. By focusing on uncertainty to analyze the sparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the original, achieves a 6% prefill speedup, and improves throughput by 6.4x - not only delivering strong lossless compression performance, but also validating the effectiveness of the underlying theoretical tool. We release the code at https://github.com/menik1126/UNComp.

**Link**: [arxiv](http://arxiv.org/abs/2410.03090v2),  [pdf](http://arxiv.org/pdf/2410.03090v2)

**Tags**: cs.CL cs.LG 



### Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient   LLM Inference
**Authors**: Haoyu Chen, Xue Li, Kun Qian, Yu Guan, Jin Zhao, Xin Wang

**Updated**: 2025-09-24T03:15:37Z

**Summary**: Efficiently processing the dynamics of requests, especially the context length variance, is important in Large Language Model (LLM) serving scenarios. However, there is an intrinsic trade-off: while leveraging parallelism strategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to accommodate larger context lengths, it inevitably results in degraded overall throughput. In this paper, we propose Cross-Instance Parallelism Transformation (Gyges), which adaptively adjusts the parallelism strategies of running instances to align with the dynamics of incoming requests. We design (1) a page-friendly, header-centric layout to accelerate KV cache transformations; (2) dedicated weight padding to accelerate model weight transformations; and (3) a transformation-aware scheduler to cooperatively schedule requests and parallelism transformations, optimizing the overall performance. Evaluations using real-world traces show that Gyges improves throughput by 1.75x-6.57x compared to state-of-the-art solutions.

**Link**: [arxiv](http://arxiv.org/abs/2509.19729v1),  [pdf](http://arxiv.org/pdf/2509.19729v1)

**Tags**: cs.DC 



### LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale   Architectures
**Authors**: Anders Johansson, Evan Weinberg, Christian R. Trott, Megan J. McCarthy, Stan G. Moore

**Updated**: 2025-09-24T01:32:55Z

**Summary**: Since its inception in 1995, LAMMPS has grown to be a world-class molecular dynamics code, with thousands of users, over one million lines of code, and multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the modern heterogeneous computing landscape by integrating the Kokkos performance portability library into the existing C++ code. We investigate performance portability of simple pairwise, many-body reactive, and machine-learned force-field interatomic potentials. We present results on GPUs across different vendors and generations, and analyze performance trends, probing FLOPS throughput, memory bandwidths, cache capabilities, and thread-atomic operation performance. Finally, we demonstrate strong scaling on three exascale machines -- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS Alps supercomputer, for the three potentials.

**Link**: [arxiv](http://arxiv.org/abs/2508.13523v2),  [pdf](http://arxiv.org/pdf/2508.13523v2)

**Tags**: cs.DC cs.PF physics.comp-ph C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2 



### Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method   for Multi-Agent Systems
**Authors**: Danilo Trombino, Vincenzo Pecorella, Alessandro de Giulii, Davide Tresoldi

**Updated**: 2025-09-23T21:46:38Z

**Summary**: Multi-agent systems (MAS) are increasingly tasked with solving complex, knowledge-intensive problems where effective agent orchestration is critical. Conventional orchestration methods rely on static agent descriptions, which often become outdated or incomplete. This limitation leads to inefficient task routing, particularly in dynamic environments where agent capabilities continuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a novel approach that augments static descriptions with dynamic, privacy-preserving relevance signals derived from each agent's internal knowledge base (KB). In the proposed framework, when static descriptions are insufficient for a clear routing decision, the orchestrator prompts the subagents in parallel. Each agent then assesses the task's relevance against its private KB, returning a lightweight ACK signal without exposing the underlying data. These collected signals populate a shared semantic cache, providing dynamic indicators of agent suitability for future queries. By combining this novel mechanism with static descriptions, our method achieves more accurate and adaptive task routing preserving agent autonomy and data confidentiality. Benchmarks show that our KBA Orchestration significantly outperforms static description-driven methods in routing precision and overall system efficiency, making it suitable for large-scale systems that require higher accuracy than standard description-driven routing.

**Link**: [arxiv](http://arxiv.org/abs/2509.19599v1),  [pdf](http://arxiv.org/pdf/2509.19599v1)

**Tags**: cs.MA cs.AI 



### From Slow Bidirectional to Fast Autoregressive Video Diffusion Models
**Authors**: Tianwei Yin, Qiang Zhang, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, Xun Huang

**Updated**: 2025-09-23T21:08:03Z

**Summary**: Current video diffusion models achieve impressive generation quality but struggle in interactive applications due to bidirectional attention dependencies. The generation of a single frame requires the model to process the entire sequence, including the future. We address this limitation by adapting a pretrained bidirectional diffusion transformer to an autoregressive transformer that generates frames on-the-fly. To further reduce latency, we extend distribution matching distillation (DMD) to videos, distilling 50-step diffusion model into a 4-step generator. To enable stable and high-quality distillation, we introduce a student initialization scheme based on teacher's ODE trajectories, as well as an asymmetric distillation strategy that supervises a causal student model with a bidirectional teacher. This approach effectively mitigates error accumulation in autoregressive generation, allowing long-duration video synthesis despite training on short clips. Our model achieves a total score of 84.27 on the VBench-Long benchmark, surpassing all previous video generation models. It enables fast streaming generation of high-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our approach also enables streaming video-to-video translation, image-to-video, and dynamic prompting in a zero-shot manner.

**Link**: [arxiv](http://arxiv.org/abs/2412.07772v4),  [pdf](http://arxiv.org/pdf/2412.07772v4)

**Tags**: cs.CV 



### Decentralized Learning Strategies for Estimation Error Minimization with   Graph Neural Networks
**Authors**: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi Bidokhti

**Updated**: 2025-09-23T20:25:15Z

**Summary**: We address the challenge of sampling and remote estimation for autoregressive Markovian processes in a multi-hop wireless network with statistically-identical agents. Agents cache the most recent samples from others and communicate over wireless collision channels governed by an underlying graph topology. Our goal is to minimize time-average estimation error and/or age of information with decentralized scalable sampling and transmission policies, considering both oblivious (where decision-making is independent of the physical processes) and non-oblivious policies (where decision-making depends on physical processes). We prove that in oblivious policies, minimizing estimation error is equivalent to minimizing the age of information. The complexity of the problem, especially the multi-dimensional action spaces and arbitrary network topologies, makes theoretical methods for finding optimal transmission policies intractable. We optimize the policies using a graphical multi-agent reinforcement learning framework, where each agent employs a permutation-equivariant graph neural network architecture. Theoretically, we prove that our proposed framework exhibits desirable transferability properties, allowing transmission policies trained on small- or moderate-size networks to be executed effectively on large-scale topologies. Numerical experiments demonstrate that (i) Our proposed framework outperforms state-of-the-art baselines; (ii) The trained policies are transferable to larger networks, and their performance gains increase with the number of agents; (iii) The training procedure withstands non-stationarity even if we utilize independent learning techniques; and, (iv) Recurrence is pivotal in both independent learning and centralized training and decentralized execution, and improves the resilience to non-stationarity in independent learning.

**Link**: [arxiv](http://arxiv.org/abs/2404.03227v3),  [pdf](http://arxiv.org/pdf/2404.03227v3)

**Tags**: eess.SP cs.LG 



### Automated Insertion of Flushes and Fences for Persistency
**Authors**: Yutong Guo, Weiyu Luo, Brian Demsky

**Updated**: 2025-09-23T18:14:21Z

**Summary**: CXL shared memory and persistent memory allow the contents of memory to persist beyond crashes. Stores to persistent or CXL memory are typically not immediately made persistent; developers must manually flush the corresponding cache lines to force the data to be written to the underlying storage. Correctly using flush and fence operations is known to be challenging. While state-of-the-art tools can find missing flush instructions, they often require bug-revealing test cases. No existing tools can ensure the absence of missing flush bugs.   In this paper, we present PMRobust, a compiler that automatically inserts flush and fence operations to ensure that code using persistent memory is free from missing flush and fence bugs. PMRobust employs a novel static analysis with optimizations that target newly allocated objects. We have evaluated PMRobust on persistent memory libraries and several persistent memory data structures and measured a geometric mean overhead of 0.26% relative to the original benchmarks with hand-placed flush and fence operations.

**Link**: [arxiv](http://arxiv.org/abs/2509.19459v1),  [pdf](http://arxiv.org/pdf/2509.19459v1)

**Tags**: cs.SE cs.PL 



### CompLLM: Compression for Long Context Q&A
**Authors**: Gabriele Berton, Jayakrishnan Unnikrishnan, Son Tran, Mubarak Shah

**Updated**: 2025-09-23T16:49:43Z

**Summary**: Large Language Models (LLMs) face significant computational challenges when processing long contexts due to the quadratic complexity of self-attention. While soft context compression methods, which map input text to smaller latent representations, have shown promise, their real-world adoption is limited. Existing techniques typically compress the context as a single unit, which leads to quadratic compression complexity and an inability to reuse computations across queries with overlapping contexts. In this work, we introduce CompLLM, a soft compression technique designed for practical deployment. Instead of processing the context holistically, CompLLM divides it into segments and compresses each one independently. This simple design choice yields three critical properties: efficiency, as the compression step scales linearly with the context length; scalability, enabling models trained on short sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and reusability, allowing compressed segments to be cached and reused across different queries. Our experiments show that with a 2x compression rate, at high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance comparable to that obtained with the uncompressed context, and even surpasses it on very long sequences, demonstrating its effectiveness and practical utility.

**Link**: [arxiv](http://arxiv.org/abs/2509.19228v1),  [pdf](http://arxiv.org/pdf/2509.19228v1)

**Tags**: cs.CL 



### 3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes   Equations with Applications to Geodynamics
**Authors**: Marcel Ferrari, Cyrill P√ºntener, Alexander Sotoudeh, Niklas Viebig

**Updated**: 2025-09-23T14:25:13Z

**Summary**: We present the design, implementation, and evaluation of optimized matrix-free stencil kernels for multigrid smoothing in the incompressible Stokes equations with variable viscosity, motivated by geophysical flow problems. We investigate five smoother variants derived from different optimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked fused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a strategy that applies local iterations on overlapping tiles to improve cache reuse. To ensure correctness, we introduce an energy-based residual norm that balances velocity and pressure contributions, and validate all implementations using a high-contrast sinker benchmark representative of realistic geodynamic numerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of the ALPS supercomputer demonstrates that all smoothers scale well within a single NUMA domain, but the RAS-Jacobi smoother consistently achieves the best performance at higher core counts. It sustains over 90% weak-scaling efficiency up to 64 cores and delivers up to a threefold speedup compared to the C++ Jacobi baseline, owing to improved cache reuse and reduced memory traffic. These results show that temporal blocking, already employed in distributed-memory solvers to reduce communication, can also provide substantial benefits at the socket and NUMA level. This work highlights the importance of cache-aware stencil design for harnessing modern heterogeneous architectures and lays the groundwork for extending RAS-type temporal blocking strategies to three-dimensional problems and GPU accelerators.

**Link**: [arxiv](http://arxiv.org/abs/2509.19061v1),  [pdf](http://arxiv.org/pdf/2509.19061v1)

**Tags**: physics.comp-ph cs.NA math.NA 65F08, 65N55, 65N22, 76M20 G.1.8; F.2.1; D.1.3; C.1.4 



### Obelix: Mitigating Side-Channels Through Dynamic Obfuscation
**Authors**: Jan Wichelmann, Anja Rabich, Anna P"atschke, Thomas Eisenbarth

**Updated**: 2025-09-23T12:32:51Z

**Summary**: Trusted execution environments (TEEs) offer hardware-assisted means to protect code and data. However, as shown in numerous results over the years, attackers can use side-channels to leak data access patterns and even single-step the code. While the vendors are slowly introducing hardware-based countermeasures for some attacks, others will stay unaddressed. This makes a software-level countermeasure desirable, but current available solutions only address very specific attack vectors or have a narrow leakage model.   In this work, we take a holistic view at the vulnerabilities of TEEs and design a tool named Obelix, which is the first to protect both code and data against a wide range of TEE attacks, from cache attacks over single-stepping to ciphertext side-channels. We analyze the practically achievable precision of state-of-the-art single-stepping tools, and present an algorithm which uses that knowledge to divide a program into uniform code blocks, that are indistinguishable for a strong attacker. By storing these blocks and the program data in oblivious RAM, the attacker cannot follow execution, effectively protecting both secret code and data. We describe how we automate our approach to make it available for developers who are unfamiliar with side-channels. As an obfuscation tool, Obelix comes with a considerable performance overhead, but compensates this with strong security guarantees and easy applicability without requiring any expert knowledge.

**Link**: [arxiv](http://arxiv.org/abs/2509.18909v1),  [pdf](http://arxiv.org/pdf/2509.18909v1)

**Tags**: cs.CR 



### PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference
**Authors**: Hao Zhang, Mengsi Lyu, Zhuo Chen, Xingrun Xing, Yulong Ao, Yonghua Lin

**Updated**: 2025-09-23T08:31:26Z

**Summary**: Large Language Models (LLMs) demonstrate exceptional capabilities across various tasks, but their deployment is constrained by high computational and memory costs. Model pruning provides an effective means to alleviate these demands. However, existing methods often ignore the characteristics of prefill-decode (PD) disaggregation in practice. In this paper, we propose a novel pruning method for PD disaggregation inference, enabling more precise and efficient block and KV Cache pruning. Our approach constructs pruning and distillation sets to perform iterative block removal independently for the prefill and decode stages, obtaining better pruning solutions. Moreover, we introduce a token-aware cache pruning mechanism that retains all KV Cache in the prefill stage but selectively reuses entries for the first and last token sequences in selected layers during decode, reducing communication costs with minimal overhead. Extensive experiments demonstrate that our approach consistently achieves strong performance in both PD disaggregation and PD unified settings without disaggregation. Under the same (default) settings, our method achieves improved performance and faster inference, along with a 4.95$\times$ reduction in data transmission bandwidth consumption.

**Link**: [arxiv](http://arxiv.org/abs/2509.04467v3),  [pdf](http://arxiv.org/pdf/2509.04467v3)

**Tags**: cs.CL cs.AI 



### Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and   Flash
**Authors**: Fucheng Jia, Zewen Wu, Shiqi Jiang, Huiqiang Jiang, Qianxi Zhang, Yuqing Yang, Yunxin Liu, Ju Ren, Deyu Zhang, Ting Cao

**Updated**: 2025-09-23T08:24:07Z

**Summary**: Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.

**Link**: [arxiv](http://arxiv.org/abs/2504.08378v2),  [pdf](http://arxiv.org/pdf/2504.08378v2)

**Tags**: cs.LG 



### Static Estimation of Reuse Profiles for Arrays in Nested Loops
**Authors**: Abdur Razzak, Atanu Barai, Nandakishore Santhi, Abdel-Hameed A. Badawy

**Updated**: 2025-09-23T06:10:20Z

**Summary**: Efficient memory access patterns play a crucial role in determining the overall performance of applications by exploiting temporal and spatial locality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is a widely used metric to quantify temporal locality, measuring the distance between consecutive accesses to the same memory location. Traditionally, calculating RDH requires program execution and memory trace collection to obtain dynamic memory access behavior. This trace collection is often time-consuming, resource-intensive, and unsuitable for early-stage optimization or large-scale applications. Static prediction, on the other hand, offers a significant speedup in estimating RDH and cache hit rates. However, these approaches lack accuracy, since the predictions come without running the program and knowing the complete memory access pattern, more specifically when arrays are used inside nested loops. This paper presents a novel static analysis framework for predicting the reuse profiles of array references in programs with nested loop structures, without requiring any runtime information. By analyzing loop bounds, access patterns in smaller problem sizes, and predictive equations, our method predicts access patterns of arrays and estimates reuse distances and cache hit rate at compile time. This paper extends our previous study by incorporating more analysis and improving prediction by addressing previously unhandled reuse patterns. We evaluate our technique against a widely accepted traditional trace-driven profiling tool, Parallel Reuse Distance Analysis (PARDA). The results demonstrate that our static predictor achieves comparable accuracy while offering orders-of-magnitude improvement in the analysis speed. This work offers a practical alternative to dynamic reuse profiling and paves the way for integration into compilers and static performance modeling tools.

**Link**: [arxiv](http://arxiv.org/abs/2509.18684v1),  [pdf](http://arxiv.org/pdf/2509.18684v1)

**Tags**: cs.PF 



### CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases
**Authors**: Yeonwoo Jeong, Hyunji Cho, Kyuri Park, Youngjae Kim, Sungyong Park

**Updated**: 2025-09-23T05:39:47Z

**Summary**: Embedding models capture both semantic and syntactic structures of queries, often mapping different queries to similar regions in vector space. This results in non-uniform cluster access patterns in modern disk-based vector databases. While existing approaches optimize individual queries, they overlook the impact of cluster access patterns, failing to account for the locality effects of queries that access similar clusters. This oversight increases cache miss penalty. To minimize the cache miss penalty, we propose CALL, a context-aware query grouping mechanism that organizes queries based on shared cluster access patterns. Additionally, CALL incorporates a group-aware prefetching method to minimize cache misses during transitions between query groups and latency-aware cluster loading. Experimental results show that CALL reduces the 99th percentile tail latency by up to 33% while consistently maintaining a higher cache hit ratio, substantially reducing search latency.

**Link**: [arxiv](http://arxiv.org/abs/2509.18670v1),  [pdf](http://arxiv.org/pdf/2509.18670v1)

**Tags**: cs.DB 



### VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic   Vision-Language Planning for Zero-Shot Transfer in Robot Navigation
**Authors**: Neel P. Bhatt, Yunhao Yang, Rohan Siva, Pranay Samineni, Daniel Milan, Zhangyang Wang, Ufuk Topcu

**Updated**: 2025-09-23T03:23:03Z

**Summary**: Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2509.18592v1),  [pdf](http://arxiv.org/pdf/2509.18592v1)

**Tags**: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY 



### Foresight: Adaptive Layer Reuse for Accelerated and High-Quality   Text-to-Video Generation
**Authors**: Muhammad Adnan, Nithesh Kurella, Akhil Arunkumar, Prashant J. Nair

**Updated**: 2025-09-22T19:20:33Z

**Summary**: Diffusion Transformers (DiTs) achieve state-of-the-art results in text-to-image, text-to-video generation, and editing. However, their large model size and the quadratic cost of spatial-temporal attention over multiple denoising steps make video generation computationally expensive. Static caching mitigates this by reusing features across fixed steps but fails to adapt to generation dynamics, leading to suboptimal trade-offs between speed and quality.   We propose Foresight, an adaptive layer-reuse technique that reduces computational redundancy across denoising steps while preserving baseline performance. Foresight dynamically identifies and reuses DiT block outputs for all layers across steps, adapting to generation parameters such as resolution and denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and CogVideoX, Foresight achieves up to \latencyimprv end-to-end speedup, while maintaining video quality. The source code of Foresight is available at \href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.

**Link**: [arxiv](http://arxiv.org/abs/2506.00329v2),  [pdf](http://arxiv.org/pdf/2506.00329v2)

**Tags**: cs.LG cs.AI cs.CV 



### Speculate Deep and Accurate: Lossless and Training-Free Acceleration for   Offloaded LLMs via Substitute Speculative Decoding
**Authors**: Pei-Shuo Wang, Jian-Jia Chen, Chun-Che Yang, Chi-Chih Chang, Ning-Chi Huang, Mohamed S. Abdelfattah, Kai-Chiang Wu

**Updated**: 2025-09-22T19:08:57Z

**Summary**: The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).

**Link**: [arxiv](http://arxiv.org/abs/2509.18344v1),  [pdf](http://arxiv.org/pdf/2509.18344v1)

**Tags**: cs.CL 



### Comparison of Adaptive plan doses using Velocity generated synthetic CT   with KV CBCT and re-planning CT
**Authors**: Sudam Masanta, Gurvinder Singh, Shefali Pahwa, Shekhar Dwivedi, Devaraju Sampathirao, Ramandeep Singh

**Updated**: 2025-09-22T18:32:32Z

**Summary**: Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated through Velocity workstation and compare the target and normal tissue doses with Adaptive plan CT doses.   Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation Therapy (ART) were included in this retrospective study. Initially, patient underwent treatment with the primary plan. After subsequent indications of major changes in patients' physicality and anatomy adaptive CT scans were acquired as per institutional protocol. Both the primary planning CT and the indicative cone-beam CT (CBCT) last acquired before the commencement of the adaptive treatment were imported into Velocity workstation. Rigid and deformable image registration techniques were used for the generation of a Synthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT) for adaptive plan execution. The primary plan dose was subsequently mapped and deformed onto the Synthetic CT in Velocity workstation, allowing for a comparative dosimetric analysis between the sCT and rCT plan doses. This comparison was conducted in both Velocity and Eclipse, focusing on dose variations across different organs at risk (OARs) and the planning target volume (PTV). Additionally, dosimetric indices were evaluated to assess and validate the accuracy and quality of the synthetic CT-based dose mapping relative to adaptive planning.   Results: The dosimetric comparison between sCT and rCT stated that Mean dose for OARs and PTVs were found to be similar in the two planning and the level of confidence by using T-statistics. Collaborative research has the potential to eliminate the need of rCT as a standard requirement.   Conclusion: The sCT shows comparable CT numbers and doses to the replanning CT, suggesting it's potential as a replacement pending clinical correlation and contour adjustments.

**Link**: [arxiv](http://arxiv.org/abs/2509.18307v1),  [pdf](http://arxiv.org/pdf/2509.18307v1)

**Tags**: physics.med-ph 



### Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative   Decoding
**Authors**: Sudhanshu Agrawal, Risheek Garrepalli, Raghavv Goel, Mingu Lee, Christopher Lott, Fatih Porikli

**Updated**: 2025-09-22T17:58:21Z

**Summary**: Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\mathbf{2.8{-}3.1\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\mathbf{7.9\times}$.

**Link**: [arxiv](http://arxiv.org/abs/2509.18085v1),  [pdf](http://arxiv.org/pdf/2509.18085v1)

**Tags**: cs.LG cs.AI cs.CL 



### Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings
**Authors**: Stephen Zhang, Mustafa Khan, Vardan Papyan

**Updated**: 2025-09-22T16:16:25Z

**Summary**: Large language models (LLMs) often concentrate their attention on a few specific tokens referred to as attention sinks. Common examples include the first token, a prompt-independent sink, and punctuation tokens, which are prompt-dependent. While the tokens causing the sinks often lack direct semantic meaning, the presence of the sinks is critical for model performance, particularly under model compression and KV-caching. Despite their ubiquity, the function, semantic role, and origin of attention sinks -- especially those beyond the first token -- remain poorly understood. In this work, we conduct a comprehensive investigation demonstrating that attention sinks: catch a sequence of tokens, tag them using a common direction in embedding space, and release them back into the residual stream, where tokens are later retrieved based on the tags they have acquired. Probing experiments reveal these tags carry semantically meaningful information, such as the truth of a statement. These findings extend to reasoning models, where the mechanism spans more heads and explains greater variance in embeddings, or recent models with query-key normalization, where sinks remain just as prevalent. To encourage future theoretical analysis, we introduce a minimal problem which can be solved through the 'catch, tag, release' mechanism, and where it emerges through training.

**Link**: [arxiv](http://arxiv.org/abs/2502.00919v2),  [pdf](http://arxiv.org/pdf/2502.00919v2)

**Tags**: cs.CL cs.AI cs.LG 



### Efficient Beam Search for Large Language Models Using Trie-Based   Decoding
**Authors**: Brian J Chan, MaoXun Huang, Jui-Hung Cheng, Chao-Ting Chen, Hen-Hsen Huang

**Updated**: 2025-09-22T12:28:41Z

**Summary**: This work presents a novel trie (prefix-tree)-based parallel decoding method that addresses the memory inefficiency of batch-based beam search. By sharing a single KV cache across beams with common prefixes, our approach dramatically reduces memory usage and enables efficient decoding. We evaluated our method across three attention architectures, Multi-Head Attention (Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and Sliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail for abstractive summarization and HumanEval for code generation. Our experiments demonstrate substantial memory savings (4--8$\times$) and up to 2.4$\times$ faster decoding, without compromising generation quality. These results highlight our method's suitability for memory-constrained environments and large-scale deployments.

**Link**: [arxiv](http://arxiv.org/abs/2502.00085v2),  [pdf](http://arxiv.org/pdf/2502.00085v2)

**Tags**: cs.CL 



### Neural Attention Search
**Authors**: Difan Deng, Marius Lindauer

**Updated**: 2025-09-22T12:03:22Z

**Summary**: We present Neural Attention Search (NAtS), a framework that automatically evaluates the importance of each token within a sequence and determines if the corresponding token can be dropped after several steps. This approach can efficiently reduce the KV cache sizes required by transformer-based models during inference and thus reduce inference costs. In this paper, we design a search space that contains three token types: (i) Global Tokens will be preserved and queried by all the following tokens. (ii) Local Tokens survive until the next global token appears. (iii) Sliding Window Tokens have an impact on the inference of a fixed size of the next following tokens. Similar to the One-Shot Neural Architecture Search approach, this token-type information can be learned jointly with the architecture weights via a learnable attention mask. Experiments on both training a new transformer from scratch and fine-tuning existing large language models show that NAtS can efficiently reduce the KV cache size required for the models while maintaining the models' performance.

**Link**: [arxiv](http://arxiv.org/abs/2502.13251v3),  [pdf](http://arxiv.org/pdf/2502.13251v3)

**Tags**: cs.CL cs.AI 



### Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming   Visual Geometry Transformers
**Authors**: Soroush Mahdi, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi

**Updated**: 2025-09-22T11:54:58Z

**Summary**: Streaming visual transformers like StreamVGGT achieve strong 3D perception but suffer from unbounded growth of key value (KV) memory, which limits scalability. We propose a training-free, inference-time token eviction policy that bounds memory by discarding redundant tokens while keeping the most informative ones. Our method uses significantly less memory with little to no drop in accuracy: on 7-Scenes with long sequences it reduces peak memory from 18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under strict memory budgets, eviction enables denser frame sampling, which improves reconstruction accuracy compared to the baseline. Experiments across video depth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and camera pose estimation (Sintel, TUM-dynamics) show that our approach closely matches StreamVGGT at a fraction of the memory and makes long-horizon streaming inference more practical.

**Link**: [arxiv](http://arxiv.org/abs/2509.17650v1),  [pdf](http://arxiv.org/pdf/2509.17650v1)

**Tags**: cs.CV 



### Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory
**Authors**: Manel Lurbe, Miguel Avargues, Salvador Petit, Maria E. Gomez, Rui Yang, Guanhao Wang, Julio Sahuquillo

**Updated**: 2025-09-22T06:52:35Z

**Summary**: Emerging applications, such as big data analytics and machine learning, require increasingly large amounts of main memory, often exceeding the capacity of current commodity processors built on DRAM technology. To address this, recent research has focused on off-chip memory controllers that facilitate access to diverse memory media, each with unique density and latency characteristics. While these solutions improve memory system performance, they also exacerbate the already significant memory latency. As a result, multi-level prefetching techniques are essential to mitigate these extended latencies.   This paper investigates the advantages of prefetching across both sides of the memory system: the off-chip memory and the on-chip cache hierarchy. Our primary objective is to assess the impact of a multi-level prefetching engine on overall system performance. Additionally, we analyze the individual contribution of each prefetching level to system efficiency. To achieve this, the study evaluates two key prefetching approaches: HMC (Hybrid Memory Controller) and HMC+L1, both of which employ prefetching mechanisms commonly used by processor vendors. The HMC approach integrates a prefetcher within the off-chip hybrid memory controller, while the HMC+L1 approach combines this with additional L1 on-chip prefetchers.   Experimental results on an out-of-order execution processor show that on-chip cache prefetchers are crucial for maximizing the benefits of off-chip prefetching, which in turn further enhances performance. Specifically, the off-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and up to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher coverage to as much as 92%. Consequently, overall performance increases from 9% with the HMC approach to 12% when L1 prefetching is also employed.

**Link**: [arxiv](http://arxiv.org/abs/2509.17388v1),  [pdf](http://arxiv.org/pdf/2509.17388v1)

**Tags**: cs.DC 



### Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access
**Authors**: Chaoyi Ruan, Chao Bi, Kaiwen Zheng, Ziji Shi, Xinyi Wan, Jialin Li

**Updated**: 2025-09-22T05:24:22Z

**Summary**: Large Language Model (LLM) agents tackle data-intensive tasks such as deep research and code generation. However, their effectiveness depends on frequent interactions with knowledge sources across remote clouds or regions. Such interactions can create non-trivial latency and cost bottlenecks. Existing caching solutions focus on exact-match queries, limiting their effectiveness for semantic knowledge reuse.   To address this challenge, we introduce Asteria, a novel cross-region knowledge caching architecture for LLM agents. At its core are two abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A semantic element captures the semantic embedding representation of an LLM query together with performance-aware metadata such as latency, cost, and staticity. Sine then provides two-stage retrieval: a vector similar index with semantic embedding for fast candidate selection and a lightweight LLM-powered semantic judger for precise validation. Atop these primitives, Asteria builds a new cache interface that includes a new semantic-aware cache hit definition, a cost-efficient eviction policy, and proactive prefetching. To reduce overhead, Asteria co-locates the small LLM judger with the main LLM using adaptive scheduling and resource sharing. Our evaluation demonstrates that Asteria delivers substantial performance improvements without compromising correctness. On representative search workloads, Asteria achieves up to a 3.6$\times$ increase in throughput by maintaining cache hit rates of over 85%, while preserving accuracy virtually identical to non-cached baselines. Asteria also improves throughput for complex coding tasks by 20%, showcasing its versatility across diverse agentic workloads.

**Link**: [arxiv](http://arxiv.org/abs/2509.17360v1),  [pdf](http://arxiv.org/pdf/2509.17360v1)

**Tags**: cs.DC 



### On efficient block Krylov-solvers for $\mathcal H^2$-matrices
**Authors**: Sven Christophersen

**Updated**: 2025-09-21T22:14:56Z

**Summary**: Hierarchical matrices provide a highly memory-efficient way of storing dense linear operators arising, for example, from boundary element methods, particularly when stored in the H^2 format. In such data-sparse representations, iterative solvers are preferred over direct ones due to the cost-efficient matrix-vector multiplications they enable. Solving multiple systems of linear equations with the same hierarchical matrix naturally leads to block methods, which in turn make heavy use of BLAS level-3 functions such as GEMM. We present an efficient implementation of H^2-matrix-vector and H^2-matrix-matrix multiplication that fully exploits the potential of modern hardware in terms of memory and cache utilization. The latter is employed to accelerate block Krylov subspace methods, which we present later as the main results of this paper.

**Link**: [arxiv](http://arxiv.org/abs/2509.17257v1),  [pdf](http://arxiv.org/pdf/2509.17257v1)

**Tags**: math.NA cs.NA 65F55, 65F08, 65F10 



### MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with   RoE
**Authors**: Soheil Zibakhsh, Mohammad Samragh, Kumari Nishu, Lauren Hannah, Arnav Kundu, Minsik Cho

**Updated**: 2025-09-21T21:05:29Z

**Summary**: The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.

**Link**: [arxiv](http://arxiv.org/abs/2509.17238v1),  [pdf](http://arxiv.org/pdf/2509.17238v1)

**Tags**: cs.AI cs.CL cs.ET cs.LG 



### Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse
**Authors**: Zhekai Duan, Yuan Zhang, Shikai Geng, Gaowen Liu, Joschka Boedecker, Chris Xiaoxuan Lu

**Updated**: 2025-09-21T11:48:15Z

**Summary**: Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action (VLA) models by improving performance and interpretability through intermediate reasoning steps. However, its sequential autoregressive token generation introduces significant inference latency, limiting real-time deployment. We propose Fast ECoT, an inference-time acceleration method that exploits the structured and repetitive nature of ECoT to (1) cache and reuse high-level reasoning across timesteps and (2) parallelise the generation of modular reasoning steps. Additionally, we introduce an asynchronous scheduler that decouples reasoning from action decoding, further boosting responsiveness. Fast ECoT requires no model changes or additional training and integrates easily into existing VLA pipelines. Experiments in both simulation (LIBERO) and real-world robot tasks show up to a 7.5% reduction in latency with comparable or improved task success rate and reasoning faithfulness, bringing ECoT policies closer to practical real-time deployment.

**Link**: [arxiv](http://arxiv.org/abs/2506.07639v2),  [pdf](http://arxiv.org/pdf/2506.07639v2)

**Tags**: cs.RO 



### FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline
**Authors**: Jingwei Xu, Junbin Kang, Mingkai Dong, Mingyu Liu, Lu Zhang, Shaohong Guo, Ziyan Qiu, Mingzhen You, Ziyi Tian, Anqi Yu, Tianhong Ding, Xinwei Hu, Haibo Chen

**Updated**: 2025-09-21T07:03:46Z

**Summary**: Client-side metadata caching has long been considered an effective method for accelerating metadata operations in distributed file systems (DFSs). However, we have found that client-side state (e.g., caching) is not only ineffective but also consumes valuable memory resources in the deep learning pipelines. We thus propose FalconFS, a DFS optimized for deep learning pipelines with the stateless-client architecture. Specifically, instead of performing client-side path resolution and caching, FalconFS efficiently resolves paths on the server side using hybrid metadata indexing and lazy namespace replication. FalconFS also boosts server concurrency with concurrent request merging and provides easy deployment with VFS shortcut. Evaluations against CephFS and Lustre show that FalconFS achieves up to 5.72$\times$ throughput for small file read/write and up to 12.81$\times$ throughput for deep learning model training. FalconFS has been running in Huawei autonomous driving system's production environment with 10,000 NPUs for one year.

**Link**: [arxiv](http://arxiv.org/abs/2507.10367v2),  [pdf](http://arxiv.org/pdf/2507.10367v2)

**Tags**: cs.DC cs.PF 



### SpecVLM: Fast Speculative Decoding in Vision-Language Models
**Authors**: Haiduo Huang, Fuwei Yang, Zhenhua Liu, Xuanwu Yin, Dong Li, Pengju Ren, Emad Barsoum

**Updated**: 2025-09-21T03:35:36Z

**Summary**: Speculative decoding is a powerful way to accelerate autoregressive large language models (LLMs), but directly porting it to vision-language models (VLMs) faces unique systems constraints: the prefill stage is dominated by visual tokens whose count scales with image resolution and video length, inflating both compute and memory, especially the key-value (KV) cache. We study speculative decoding for VLMs and introduce SpecVLM, a practical system that (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering 1.5--2.3x end-to-end speedups over full autoregressive inference, and (2) further accelerates VLM inference with an elastic visual compressor that adaptively selects among pruning, pooling, convolution, and resampler primitives to balance FLOPs/parameters and accuracy per input. To avoid costly offline distillation corpora, we propose an online-logit distillation protocol that trains the draft model with on-the-fly teacher logits and penultimate features using a combined cross-entropy and Smooth L1 objective, eliminating storage and preprocessing while remaining compute-efficient. This protocol reveals a training-time scaling effect: longer online training monotonically increases the draft model's average accepted length, improving speculative efficiency. Empirically, SpecVLM achieves additional acceleration, culminating in 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU, consistently over resolutions and task difficulties, while preserving the target model's output distribution (lossless decoding). Our code is available at https://github.com/haiduo/SpecVLM.

**Link**: [arxiv](http://arxiv.org/abs/2509.11815v2),  [pdf](http://arxiv.org/pdf/2509.11815v2)

**Tags**: cs.CV cs.AI 



### ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix   Caching
**Authors**: Xingyu Xiang, Raj Joshi, Yuhan Liu, Jiayi Yao, Chenxingyu Zhao, Junchen Jiang, Yang Zhou, Eddie Kohler, Minlan Yu

**Updated**: 2025-09-21T00:59:45Z

**Summary**: Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.   We present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.

**Link**: [arxiv](http://arxiv.org/abs/2509.16857v1),  [pdf](http://arxiv.org/pdf/2509.16857v1)

**Tags**: cs.DC cs.AI cs.LG 



### MPIC: Position-Independent Multimodal Context Caching System for   Efficient MLLM Serving
**Authors**: Shiju Zhao, Junhao Hu, Rongxiao Huang, Jiaqi Zheng, Guihai Chen

**Updated**: 2025-09-20T13:54:37Z

**Summary**: The context caching technique is employed to accelerate the Multimodal Large Language Model (MLLM) inference by prevailing serving platforms currently. However, this approach merely reuses the Key-Value (KV) cache of the initial sequence of prompt, resulting in full KV cache recomputation even if the prefix differs slightly. This becomes particularly inefficient in the context of interleaved text and images, as well as multimodal retrieval-augmented generation. This paper proposes position-independent caching as a more effective approach for multimodal information management. We have designed and implemented a caching system, named MPIC, to address both system-level and algorithm-level challenges. MPIC stores the KV cache on local disks when receiving multimodal data, and calculates and loads the KV cache in parallel during inference. To mitigate accuracy degradation, we have incorporated the integrated reuse and recompute mechanism within the system. The experimental results demonstrate that MPIC can achieve up to 54\% reduction in response time and 2$\times$ improvement in throughput compared to existing context caching systems, while maintaining negligible or no accuracy loss.

**Link**: [arxiv](http://arxiv.org/abs/2502.01960v2),  [pdf](http://arxiv.org/pdf/2502.01960v2)

**Tags**: cs.LG 



### EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and   Efficient LLMs
**Authors**: Zhengge Cai, Haowen Hou

**Updated**: 2025-09-20T13:27:13Z

**Summary**: Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6\% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9\% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.16686v1),  [pdf](http://arxiv.org/pdf/2509.16686v1)

**Tags**: cs.CL 



### Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and   Expressive Freestyle Portrait Animation
**Authors**: Yue Ma, Zexuan Yan, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Zhifeng Li, Wei Liu, Linfeng Zhang, Qifeng Chen

**Updated**: 2025-09-20T11:09:01Z

**Summary**: We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in https://follow-your-emoji.github.io/.

**Link**: [arxiv](http://arxiv.org/abs/2509.16630v1),  [pdf](http://arxiv.org/pdf/2509.16630v1)

**Tags**: cs.CV 



### KV-Efficient VLA: A Method of Speed up Vision Language Model with   RNN-Gated Chunked KV Cache
**Authors**: Wanshun Xu, Long Zhuang

**Updated**: 2025-09-20T02:04:24Z

**Summary**: Vision-Language-Action (VLA) models promise unified robotic perception and control, yet their scalability is constrained by the quadratic cost of attention and the unbounded growth of key-value (KV) memory during long-horizon inference. While recent methods improve generalization through scaling backbone architectures, they often neglect the inference inefficiencies critical to real-time deployment. In this work, we present KV-Efficient VLA, a model-agnostic memory compression framework that addresses these limitations by introducing a lightweight, training-friendly mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed size chunks and employs a recurrent gating module to summarize and filter historical context according to learned utility scores. This design preserves recent fine-grained detail while aggressively pruning stale, low-relevance memory, all while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x inference speedup and 36% KV memory reduction, with minimal impact on task success. Our method integrates seamlessly into existing autoregressive and hybrid VLA stacks, enabling scalable inference without modifying training pipelines or downstream control logic.

**Link**: [arxiv](http://arxiv.org/abs/2509.21354v1),  [pdf](http://arxiv.org/pdf/2509.21354v1)

**Tags**: cs.CV cs.AI 



### Shift Parallelism: Low-Latency, High-Throughput LLM Inference for   Dynamic Workloads
**Authors**: Mert Hidayetoglu, Aurick Qiao, Michael Wyatt, Jeff Rasley, Yuxiong He, Samyam Rajbhandari

**Updated**: 2025-09-20T01:56:25Z

**Summary**: Efficient parallelism is necessary for achieving low-latency, high-throughput inference with large language models (LLMs). Tensor parallelism (TP) is the state-of-the-art method for reducing LLM response latency, however GPU communications reduces combined token throughput. On the other hand, data parallelism (DP) obtains a higher throughput yet is slow in response latency. Best of both worlds does not exist, and it is not possible to combine TP and DP because of the KV cache variance across the parallelisms.   We notice Sequence Parallelism (SP - Ulysses in training) has similar properties as DP but with KV cache invariance. We adapt SP to inference, and combine it with TP to get the best of both worlds. Our solution: Shift Parallelism.   Shift Parallelism dynamically switches across TP and SP, and minimizes latency in low traffic without losing throughput in high traffic. The efficient GPU communications of Shift Parallelism yields up to i) 1.51x faster response in interactive workloads and ii) 50% higher throughput in batch workloads, compared to a TP-only solution.   We evaluate Shift Parallelism with real-world production traces with dynamic traffic patterns as well as synthetic benchmarking patterns across models, context sizes, and arrival rates. All results affirm the same: Shift Parallelism has a better the latency vs. throughput tradeoff than TP or DP, and hence obtains low latency without degrading throughput in dynamic workloads.

**Link**: [arxiv](http://arxiv.org/abs/2509.16495v1),  [pdf](http://arxiv.org/pdf/2509.16495v1)

**Tags**: cs.DC 



### From Coated to Uncoated: Scanning Electron Microscopy Corrections to   Estimate True Surface Pore Size in Nanoporous Membranes
**Authors**: Sima Zeinali Danalou, Dian Yu, Niher R. Sarker, Hooman Chamani, Jane Y. Howe, Patrick C. Lee, Jay R. Werber

**Updated**: 2025-09-19T23:46:08Z

**Summary**: Scanning electron microscopy (SEM) is the premier method for characterizing the nanoscale surface pores in ultrafiltration (UF) membranes and the support layers of reverse osmosis (RO) membranes. Based on SEM, the conventional understanding is that membranes typically have low surface porosities of <10%. We hypothesized that high acceleration voltage during SEM imaging and sputter metal coatings required for SEM have led to systematic underestimations of porosity and pore size. We showed that imaging a commercial UF membrane at 1, 5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while increasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for the UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To account for coating thickness, we developed a digital correction method that simulates pore dilation, enabling the pore structure to be estimated for uncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF membrane and 20% for the RO support, about 3-fold greater than values observed with a 4 nm coating. Mean pore diameters were 2-fold greater for the UF membrane and 1.5-fold greater for the RO support. Critically, dilation-derived pore-size distributions agreed with low-flux dextran-retention data fitted with the Bungay-Brenner model. Our results suggest that surface porosities and pore sizes of nanoporous membranes are much larger than previously understood, with major implications for structure/transport relationships. For future nanoscale pore analysis of membranes (and other nanoporous materials), we recommend low acceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to account for coating artifacts

**Link**: [arxiv](http://arxiv.org/abs/2509.16471v1),  [pdf](http://arxiv.org/pdf/2509.16471v1)

**Tags**: cond-mat.mtrl-sci cs.CV physics.app-ph physics.chem-ph physics.ins-det 



## Keyword: LLM Inference 
 ### The Connection between Dusty Star-Forming Galaxies and the First Massive   Quenched Galaxies
**Authors**: Pablo Araya-Araya, Rachel K. Cochrane, Laerte Sodr√© Jr., Robert M. Yates, Christopher C. Hayward, Marcel P. van Daalen, Marcelo C. Vicentin, Bitten Gullberg, Francesco Valentino

**Updated**: 2025-09-30T17:59:58Z

**Summary**: High-redshift (z > 2) massive quiescent (MQ) galaxies provide an opportunity to probe the key physical processes driving the fuelling and quenching of star formation in the early Universe. Observational evidence suggests a possible evolutionary link between MQs and dusty star-forming galaxies (DSFGs; or submillimetre galaxies), another extreme high-redshift population. However, galaxy formation models have historically struggled to reproduce these populations - especially simultaneously - limiting our understanding of their formation and connection, particularly in light of recent JWST findings. In previous work, we presented a re-calibrated version of the L-Galaxies semi-analytic model that provides an improved match to observationally-inferred number densities of both DSFG and MQ populations. In this work, we use this new model to investigate the progenitors of MQs at z > 2 and the physical mechanisms that lead to their quenching. We find that most MQs at z > 2 were sub-millimetre-bright ($S_{870}$ > 1 mJy) at some point in their cosmic past. The stellar mass of MQs is strongly correlated with the maximum submillimetre flux density attained over their history, and this relation appears to be independent of redshift. However, only a minority of high-redshift DSFGs evolve into MQs by z = 2. The key distinction between typical DSFGs and MQ progenitors lies in their merger histories: MQ progenitors experience an early major merger that triggers a brief, intense starburst and rapid black hole growth, depleting their cold gas reservoirs. In our model, AGN feedback subsequently prevents further gas cooling, resulting in quenching. In contrast, the broader DSFG population remains sub-millimetre-bright, with star formation proceeding primarily via secular processes, becoming quenched later.

**Link**: [arxiv](http://arxiv.org/abs/2509.26646v1),  [pdf](http://arxiv.org/pdf/2509.26646v1)

**Tags**: astro-ph.GA 



### A Tractable Family of Smooth Copulas with Rotational Dependence:   Properties, Inference, and Application
**Authors**: Micha√´l Lalancette, Robert Zimmerman

**Updated**: 2025-09-30T17:59:12Z

**Summary**: We introduce a new family of copula densities constructed from univariate distributions on $[0,1]$. Although our construction is structurally simple, the resulting family is versatile: it includes both smooth and irregular examples, and reveals clear links between properties of the underlying univariate distribution and the strength, direction, and form of multivariate dependence. The framework brings with it a range of explicit mathematical properties, including interpretable characterizations of dependence and transparent descriptions of how rotational forms arise. We propose model selection and inference methods in parametric and nonparametric settings, supported by asymptotic theory that reduces multivariate estimation to well-studied univariate problems. Simulation studies confirm the reliable recovery of structural features, and an application involving neural connectivity data illustrates how the family can yield a better fit than existing models.

**Link**: [arxiv](http://arxiv.org/abs/2509.26635v1),  [pdf](http://arxiv.org/pdf/2509.26635v1)

**Tags**: math.ST stat.TH 62H05 (Primary) 60E05 (Secondary) 



### Scaling Spoken Language Models with Syllabic Speech Tokenization
**Authors**: Nicholas Lee, Cheol Jun Cho, Alan W Black, Gopala K. Anumanchipalli

**Updated**: 2025-09-30T17:59:09Z

**Summary**: Spoken language models (SLMs) typically discretize speech into high-frame-rate tokens extracted from SSL speech models. As the most successful LMs are based on the Transformer architecture, processing these long token streams with self-attention is expensive, as attention scales quadratically with sequence length. A recent SSL work introduces acoustic tokenization of speech at the syllable level, which is more interpretable and potentially more scalable with significant compression in token lengths (4-5 Hz). Yet, their value for spoken language modeling is not yet fully explored. We present the first systematic study of syllabic tokenization for spoken language modeling, evaluating models on a suite of SLU benchmarks while varying training data scale. Syllabic tokens can match or surpass the previous high-frame rate tokens while significantly cutting training and inference costs, achieving more than a 2x reduction in training time and a 5x reduction in FLOPs. Our findings highlight syllable-level language modeling as a promising path to efficient long-context spoken language models.

**Link**: [arxiv](http://arxiv.org/abs/2509.26634v1),  [pdf](http://arxiv.org/pdf/2509.26634v1)

**Tags**: cs.CL eess.AS 



### Learning Generalizable Shape Completion with SIM(3) Equivariance
**Authors**: Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu

**Updated**: 2025-09-30T17:58:55Z

**Summary**: 3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.

**Link**: [arxiv](http://arxiv.org/abs/2509.26631v1),  [pdf](http://arxiv.org/pdf/2509.26631v1)

**Tags**: cs.CV cs.AI 



### Attention as a Compass: Efficient Exploration for Process-Supervised RL   in Reasoning Models
**Authors**: Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai

**Updated**: 2025-09-30T17:58:34Z

**Summary**: Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2509.26628v1),  [pdf](http://arxiv.org/pdf/2509.26628v1)

**Tags**: cs.LG cs.CL 



### The Impact of Language Mixing on Bilingual LLM Reasoning
**Authors**: Yihao Li, Jiayi Xin, Miranda Muqing Miao, Qi Long, Lyle Ungar

**Updated**: 2025-09-30T17:58:13Z

**Summary**: Proficient multilingual speakers often intentionally switch languages in the middle of a conversation. Similarly, recent reasoning-focused bilingual large language models (LLMs) with strong capabilities in both languages exhibit language mixing-alternating languages within their chain of thought. Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy, suggesting that language mixing may benefit reasoning. In this work, we study language switching in Chinese-English bilingual reasoning models. We identify reinforcement learning with verifiable rewards (RLVR) as the critical training stage that leads to language mixing. We show that language mixing can enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage points on MATH500. Additionally, a lightweight probe can be trained to predict whether a potential language switch would benefit or harm reasoning, and when used to guide decoding, increases accuracy by 2.92 percentage points. Our findings suggest that language mixing is not merely a byproduct of multilingual training, but is a strategic reasoning behavior.

**Link**: [arxiv](http://arxiv.org/abs/2507.15849v2),  [pdf](http://arxiv.org/pdf/2507.15849v2)

**Tags**: cs.CL cs.AI cs.LG 



### Recursive Self-Aggregation Unlocks Deep Thinking in Large Language   Models
**Authors**: Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain

**Updated**: 2025-09-30T17:58:03Z

**Summary**: Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA.

**Link**: [arxiv](http://arxiv.org/abs/2509.26626v1),  [pdf](http://arxiv.org/pdf/2509.26626v1)

**Tags**: cs.LG 



### Learning to See Before Seeing: Demystifying LLM Visual Priors from   Language Pre-training
**Authors**: Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos

**Updated**: 2025-09-30T17:57:44Z

**Summary**: Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26625v1),  [pdf](http://arxiv.org/pdf/2509.26625v1)

**Tags**: cs.LG cs.AI cs.CV cs.MM 



### On Fitting Flow Models with Large Sinkhorn Couplings
**Authors**: Stephen Zhang, Alireza Mousavi-Hosseini, Michal Klein, Marco Cuturi

**Updated**: 2025-09-30T17:57:27Z

**Summary**: Flow models transform data gradually from one modality (e.g. noise) onto another (e.g. images). Such models are parameterized by a time-dependent velocity field, trained to fit segments connecting pairs of source and target points. When the pairing between source and target points is given, training flow models boils down to a supervised regression problem. When no such pairing exists, as is the case when generating data from noise, training flows is much harder. A popular approach lies in picking source and target points independently. This can, however, lead to velocity fields that are slow to train, but also costly to integrate at inference time. In theory, one would greatly benefit from training flow models by sampling pairs from an optimal transport (OT) measure coupling source and target, since this would lead to a highly efficient flow solving the Benamou and Brenier dynamical OT problem. In practice, recent works have proposed to sample mini-batches of $n$ source and $n$ target points and reorder them using an OT solver to form better pairs. These works have advocated using batches of size $n\approx 256$, and considered OT solvers that return couplings that are either sharp (using e.g. the Hungarian algorithm) or blurred (using e.g. entropic regularization, a.k.a. Sinkhorn). We follow in the footsteps of these works by exploring the benefits of increasing $n$ by three to four orders of magnitude, and look more carefully on the effect of the entropic regularization $\varepsilon$ used in the Sinkhorn algorithm. Our analysis is facilitated by new scale invariant quantities to report the sharpness of a coupling, while our sharded computations across multiple GPU or GPU nodes allow scaling up $n$. We show that in both synthetic and image generation tasks, flow models greatly benefit when fitted with large Sinkhorn couplings, with a low entropic regularization $\varepsilon$.

**Link**: [arxiv](http://arxiv.org/abs/2506.05526v3),  [pdf](http://arxiv.org/pdf/2506.05526v3)

**Tags**: cs.LG stat.ML 



### Black-box Context-free Grammar Inference for Readable & Natural Grammars
**Authors**: Mohammad Rifat Arefin, Shanto Rahman, Christoph Csallner

**Updated**: 2025-09-30T17:54:25Z

**Summary**: Black-box context-free grammar inference is crucial for program analysis, reverse engineering, and security, yet existing tools such as Arvada, TreeVada, and Kedavra struggle with scalability, readability, and accuracy on large, complex languages. We present NatGI, a novel LLM-guided grammar inference framework that extends TreeVada's parse tree recovery with three key innovations: bracket-guided bubble exploration, LLM-driven bubble generation and non-terminal labeling, and hierarchical delta debugging (HDD) for systematic tree simplification. Bracket-guided exploration leverages syntactic cues such as parentheses to propose well-structured grammar fragments, while LLM guidance produces meaningful non-terminal names and selects more promising merges. Finally, HDD incrementally reduces unnecessary rules, which makes the grammars both compact and interpretable. In our experiments, we evaluate NatGI on a comprehensive benchmark suite ranging from small languages to larger ones such as lua, c, and mysql. Our results show that NatGI consistently outperforms strong baselines in terms of F1 score. On average, NatGI achieves an F1 score of 0.57, which is 25pp (percentage points) higher than the best-performing baseline, TreeVada. In the case of interpretability, our generated grammars perform significantly better than those produced by existing approaches. Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules with meaningful non-terminal names and compact structures that align more closely with human intuition. As a result, developers and researchers can achieve higher accuracy while still being able to easily inspect, verify, and reason about the structure and semantics of the induced grammars.

**Link**: [arxiv](http://arxiv.org/abs/2509.26616v1),  [pdf](http://arxiv.org/pdf/2509.26616v1)

**Tags**: cs.SE cs.FL cs.PL 68Q42, 68Q45 (Primary), 68T50 (Secondary) D.2.5; F.4.2 



### Seeing Through Deception: Uncovering Misleading Creator Intent in   Multimodal News with Vision-Language Models
**Authors**: Jiaying Wu, Fanxiao Li, Zihang Fu, Min-Yen Kan, Bryan Hooi

**Updated**: 2025-09-30T17:53:25Z

**Summary**: The impact of misinformation arises not only from factual inaccuracies but also from the misleading narratives that creators deliberately embed. Interpreting such creator intent is therefore essential for multimodal misinformation detection (MMD) and effective information governance. To this end, we introduce DeceptionDecoded, a large-scale benchmark of 12,000 image-caption pairs grounded in trustworthy reference articles, created using an intent-guided simulation framework that models both the desired influence and the execution plan of news creators. The dataset captures both misleading and non-misleading cases, spanning manipulations across visual and textual modalities, and supports three intent-centric tasks: (1) misleading intent detection, (2) misleading source attribution, and (3) creator desire inference. We evaluate 14 state-of-the-art vision-language models (VLMs) and find that they struggle with intent reasoning, often relying on shallow cues such as surface-level alignment, stylistic polish, or heuristic authenticity signals. These results highlight the limitations of current VLMs and position DeceptionDecoded as a foundation for developing intent-aware models that go beyond shallow cues in MMD.

**Link**: [arxiv](http://arxiv.org/abs/2505.15489v3),  [pdf](http://arxiv.org/pdf/2505.15489v3)

**Tags**: cs.CV cs.CL cs.MM 



### Beyond Suboptimality: Resource-Rationality and Task Demands Shape the   Complexity of Perceptual Representations
**Authors**: Andrew Jun Lee, Daniel Turek, Omer Daglar Tanrikulu

**Updated**: 2025-09-30T17:50:22Z

**Summary**: Early theories of perception as probabilistic inference propose that uncertainty about the interpretation of sensory input is represented as a probability distribution over many interpretations -- a relatively complex representation. However, critics argue that persistent demonstrations of suboptimal perceptual decision-making indicate limits in representational complexity. We contend that suboptimality arises not from genuine limits, but participants' resource-rational adaptations to task demands. For example, when tasks are solvable with minimal attention to stimuli, participants may neglect information needed for complex representations, relying instead on simpler ones that engender suboptimality. Across three experiments, we progressively reduced the efficacy of resource-rational strategies on a carefully controlled decision task. Model fits favored simple representations when resource-rational strategies were effective, and favored complex representations when ineffective, suggesting that perceptual representations can be simple or complex depending on task demands. We conclude that resource-rationality is an epistemic constraint for experimental design and essential to a complete theory of perception.

**Link**: [arxiv](http://arxiv.org/abs/2509.26606v1),  [pdf](http://arxiv.org/pdf/2509.26606v1)

**Tags**: q-bio.NC 



### A Note on Inferential Decisions, Errors and Path-Dependency
**Authors**: Kangda K. Wren

**Updated**: 2025-09-30T17:49:59Z

**Summary**: Consider the sequential inference of a binary outcome. The process of a posteriori beliefs and its objectively true conditional-probability counterpart generally differ but should lead to the same result eventually in well-defined tests. We show that unless the two are 'essentially identical', differing only by an a priori factor, time-homogeneous continuous decisions based on the former must be path-dependent with respect to state-variables based on the latter or any non-essentially-identical process. Inferential errors decompose into path-dependent and path-independent parts, whose distinct properties are relevant to error mitigation.

**Link**: [arxiv](http://arxiv.org/abs/2507.05634v4),  [pdf](http://arxiv.org/pdf/2507.05634v4)

**Tags**: math.ST stat.TH 62M, 60G25, 60G35, 93E10 



### MENLO: From Preferences to Proficiency -- Evaluating and Modeling   Native-like Quality Across 47 Languages
**Authors**: Chenxi Whitehouse, Sebastian Ruder, Tony Lin, Oksana Kurylo, Haruka Takagi, Janice Lam, Nicol√≤ Busetto, Denise Diaz

**Updated**: 2025-09-30T17:48:58Z

**Summary**: Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2509.26601v1),  [pdf](http://arxiv.org/pdf/2509.26601v1)

**Tags**: cs.CL cs.AI cs.LG 



### Deconstructing Self-Bias in LLM-generated Translation Benchmarks
**Authors**: Wenda Xu, Sweta Agrawal, Vil√©m Zouhar, Markus Freitag, Daniel Deutsch

**Updated**: 2025-09-30T17:48:35Z

**Summary**: As large language models (LLMs) begin to saturate existing benchmarks, automated benchmark creation using LLMs (LLM as a benchmark) has emerged as a scalable alternative to slow and costly human curation. While these generated test sets have to potential to cheaply rank models, we demonstrate a critical flaw. LLM generated benchmarks systematically favor the model that created the benchmark, they exhibit self bias on low resource languages to English translation tasks. We show three key findings on automatic benchmarking of LLMs for translation: First, this bias originates from two sources: the generated test data (LLM as a testset) and the evaluation method (LLM as an evaluator), with their combination amplifying the effect. Second, self bias in LLM as a benchmark is heavily influenced by the model's generation capabilities in the source language. For instance, we observe more pronounced bias in into English translation, where the model's generation system is developed, than in out of English translation tasks. Third, we observe that low diversity in source text is one attribution to self bias. Our results suggest that improving the diversity of these generated source texts can mitigate some of the observed self bias.

**Link**: [arxiv](http://arxiv.org/abs/2509.26600v1),  [pdf](http://arxiv.org/pdf/2509.26600v1)

**Tags**: cs.CL cs.AI 



### Are Robust LLM Fingerprints Adversarially Robust?
**Authors**: Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, Sewoong Oh

**Updated**: 2025-09-30T17:47:09Z

**Summary**: Model fingerprinting has emerged as a promising paradigm for claiming model ownership. However, robustness evaluations of these schemes have mostly focused on benign perturbations such as incremental fine-tuning, model merging, and prompting. Lack of systematic investigations into {\em adversarial robustness} against a malicious model host leaves current systems vulnerable. To bridge this gap, we first define a concrete, practical threat model against model fingerprinting. We then take a critical look at existing model fingerprinting schemes to identify their fundamental vulnerabilities. Based on these, we develop adaptive adversarial attacks tailored for each vulnerability, and demonstrate that these can bypass model authentication completely for ten recently proposed fingerprinting schemes while maintaining high utility of the model for the end users. Our work encourages fingerprint designers to adopt adversarial robustness by design. We end with recommendations for future fingerprinting methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.26598v1),  [pdf](http://arxiv.org/pdf/2509.26598v1)

**Tags**: cs.CR cs.AI cs.LG 



### Exploring Large Language Model as an Interactive Sports Coach: Lessons   from a Single-Subject Half Marathon Preparation
**Authors**: Kichang Lee

**Updated**: 2025-09-30T17:46:39Z

**Summary**: Large language models (LLMs) are emerging as everyday assistants, but their role as longitudinal virtual coaches is underexplored. This two-month single subject case study documents LLM guided half marathon preparation (July-September 2025). Using text based interactions and consumer app logs, the LLM acted as planner, explainer, and occasional motivator. Performance improved from sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec per km, with gains in cadence, pace HR coupling, and efficiency index trends. While causal attribution is limited without a control, outcomes demonstrate safe, measurable progress. At the same time, gaps were evident, no realtime sensor integration, text only feedback, motivation support that was user initiated, and limited personalization or safety guardrails. We propose design requirements for next generation systems, persistent athlete models with explicit guardrails, multimodal on device sensing, audio, haptic, visual feedback, proactive motivation scaffolds, and privacy-preserving personalization. This study offers grounded evidence and a design agenda for evolving LLMs from retrospective advisors to closed-loop coaching companions.

**Link**: [arxiv](http://arxiv.org/abs/2509.26593v1),  [pdf](http://arxiv.org/pdf/2509.26593v1)

**Tags**: cs.HC H.4, K.7 



### Generating Difficult-to-Translate Texts
**Authors**: Vil√©m Zouhar, Wenda Xu, Parker Riley, Juraj Juraska, Mara Finkelstein, Markus Freitag, Dan Deutsch

**Updated**: 2025-09-30T17:46:08Z

**Summary**: Machine translation benchmarks sourced from the real world are quickly obsoleted, due to most examples being easy for state-of-the-art translation models. This limits the benchmark's ability to distinguish which model is better or to reveal models' weaknesses. Current methods for creating difficult test cases, such as subsampling or from-scratch synthesis, either fall short of identifying difficult examples or suffer from a lack of diversity and naturalness. Inspired by the iterative process of human experts probing for model failures, we propose MT-breaker, a method where a large language model iteratively refines a source text to increase its translation difficulty. The LLM iteratively queries a target machine translation model to guide its generation of difficult examples. Our approach generates examples that are more challenging for the target MT model while preserving the diversity of natural texts. While the examples are tailored to a particular machine translation model during the generation, the difficulty also transfers to other models and languages.

**Link**: [arxiv](http://arxiv.org/abs/2509.26592v1),  [pdf](http://arxiv.org/pdf/2509.26592v1)

**Tags**: cs.CL 



### The JWST EXCELS Survey: A spectroscopic investigation of the ionizing   properties of star-forming galaxies at 1<z<8
**Authors**: R. Begley, R. J. McLure, F. Cullen, A. C. Carnall, T. M. Stanton, D. Scholte, D. J. McLeod, J. S. Dunlop, K. Z. Arellano-C√≥rdova, C. Bondestam, C. T. Donnan, M. L. Hamadouch, A. E. Shapley, S. Stevenson

**Updated**: 2025-09-30T17:45:29Z

**Summary**: Charting the Epoch of Reionization demands robust assessments of what drives the production of ionizing photons in high-redshift star-forming galaxies (SFGs), and requires better predictive capabilities from current observations. Using a sample of $N=159$ SFGs at $1<z<8$, observed with deep medium-resolution spectroscopy from the JWST/NIRSpec EXCELS survey, we perform a statistical analysis of their ionizing photon production efficiencies ($\xi_\rm{ion}$). We consider $\xi_\rm{ion}$, measured with Balmer line measurements, in relation to a number of key galaxy properties including; nebular emission line strengths ($W_\lambda(\rm{H\alpha})$ and $W_\lambda$( [OIII])), UV luminosity ($M_\rm{UV}$) and UV slope ($\beta_\rm{UV}$), as well as dust attenuation ($E(B-V)_\rm{neb}$) and redshift. Implementing a Bayesian linear regression methodology, we fit $\xi_\rm{ion}$ against the principal observables while fully marginalising over all measurement uncertainties, mitigating against the impact of outliers and determining the intrinsic scatter. Significant relations between $\xi_\rm{ion}$ and $ W_\lambda(\rm{H\alpha})$, $W_\lambda$([OIII]) and $\beta_\rm{UV}$ are recovered. Moreover, the weak trends with $M_\rm{UV}$ and redshift can be fully explained by the remaining property dependencies. Expanding our analysis to multivariate regression, we determine that $W_\lambda(\rm{H\alpha})$ or $W_\lambda$([OIII]), along with $\beta_\rm{UV}$ and $E(B-V)_\rm{neb}$, are the most important observables for accurately predicting $\xi_\rm{ion,0}$. The latter identifies the most common outliers as SFGs with relatively high $E(B-V)_\rm{neb}\gtrsim0.5$, possibly indicative of obscured star-formation or strong differential attenuation. Combining these properties enable $\xi_\rm{ion,0}$ to be inferred with an accuracy of $\sim0.15\,$dex, with a population intrinsic scatter of $\sigma_\rm{int}\sim0.035\,$dex.

**Link**: [arxiv](http://arxiv.org/abs/2509.26591v1),  [pdf](http://arxiv.org/pdf/2509.26591v1)

**Tags**: astro-ph.GA 



### MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction
**Authors**: Sepideh Abedini, Shubhankar Mohapatra, D. B. Emerson, Masoumeh Shafieinejad, Jesse C. Cresswell, Xi He

**Updated**: 2025-09-30T17:43:21Z

**Summary**: Large language models (LLMs) have shown promising performance on tasks that require reasoning, such as text-to-SQL, code generation, and debugging. However, regulatory frameworks with strict privacy requirements constrain their integration into sensitive systems. State-of-the-art LLMs are also proprietary, costly, and resource-intensive, making local deployment impractical. Consequently, utilizing such LLMs often requires sharing data with third-party providers, raising privacy concerns and risking noncompliance with regulations. Although fine-tuned small language models (SLMs) can outperform LLMs on certain tasks and be deployed locally to mitigate privacy concerns, they underperform on more complex tasks such as text-to-SQL translation. In this work, we introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a privacy protection mechanism to mask sensitive information in LLM prompts. Unlike redaction, which removes content entirely, or generalization, which broadens tokens, abstraction retains essential information while discarding unnecessary details, striking an effective privacy-utility balance for the text-to-SQL task. Moreover, by providing mechanisms to control the privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range of use cases. Our experimental results show that MaskSQL outperforms leading SLM-based text-to-SQL models and achieves performance approaching state-of-the-art LLM-based models, while preserving privacy.

**Link**: [arxiv](http://arxiv.org/abs/2509.23459v2),  [pdf](http://arxiv.org/pdf/2509.23459v2)

**Tags**: cs.CR cs.CL 



### Fairness Testing in Retrieval-Augmented Generation: How Small   Perturbations Reveal Bias in Small Language Models
**Authors**: Matheus Vinicius da Silva de Oliveira, Jonathan de Andrade Silva, Awdren de Lima Fontao

**Updated**: 2025-09-30T17:42:35Z

**Summary**: Large Language Models (LLMs) are widely used across multiple domains but continue to raise concerns regarding security and fairness. Beyond known attack vectors such as data poisoning and prompt injection, LLMs are also vulnerable to fairness bugs. These refer to unintended behaviors influenced by sensitive demographic cues (e.g., race or sexual orientation) that should not affect outcomes. Another key issue is hallucination, where models generate plausible yet false information. Retrieval-Augmented Generation (RAG) has emerged as a strategy to mitigate hallucinations by combining external retrieval with text generation. However, its adoption raises new fairness concerns, as the retrieved content itself may surface or amplify bias. This study conducts fairness testing through metamorphic testing (MT), introducing controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three Small Language Models (SLMs) hosted on HuggingFace (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B), each integrated into a RAG pipeline. Results show that minor demographic variations can break up to one third of metamorphic relations (MRs). A detailed analysis of these failures reveals a consistent bias hierarchy, with perturbations involving racial cues being the predominant cause of the violations. In addition to offering a comparative evaluation, this work reinforces that the retrieval component in RAG must be carefully curated to prevent bias amplification. The findings serve as a practical alert for developers, testers and small organizations aiming to adopt accessible SLMs without compromising fairness or reliability.

**Link**: [arxiv](http://arxiv.org/abs/2509.26584v1),  [pdf](http://arxiv.org/pdf/2509.26584v1)

**Tags**: cs.AI cs.IR cs.LG cs.SE 



### Linking Process to Outcome: Conditional Reward Modeling for LLM   Reasoning
**Authors**: Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren

**Updated**: 2025-09-30T17:38:45Z

**Summary**: Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth.

**Link**: [arxiv](http://arxiv.org/abs/2509.26578v1),  [pdf](http://arxiv.org/pdf/2509.26578v1)

**Tags**: cs.LG 



### Stochasticity and Practical Identifiability in Epidemic Models: A Monte   Carlo Perspective
**Authors**: Chiara Mattamira, Olivia Prosper Feldman

**Updated**: 2025-09-30T17:38:20Z

**Summary**: Assessing the practical identifiability of epidemic models is essential for determining whether parameters can be meaningfully estimated from observed data. Monte Carlo (MC) methods provide an accessible and intuitive framework; however, their standard implementation - perturbing deterministic trajectories with independent Gaussian noise - rests on assumptions poorly suited to epidemic processes, which are inherently stochastic, temporally correlated, and highly variable, especially in small populations or under slow transmission. In this study, we investigate the structure of stochastic variability in the classic Susceptible-Infected-Recovered (SIR) model across a range of epidemiological regimes, and assess whether it can be represented within the independent Gaussian noise framework. We show that continuous-time Markov chain (CTMC) trajectories consistently exhibit super-Poissonian variability and strong temporal dependence. Through coverage analysis, we further demonstrate that independent Gaussian noise systematically underestimates the variability of the underlying stochastic process, leading to overly optimistic conclusions about parameter identifiability. In addition, we propose a hybrid simulation approach that introduces time- and amplitude-dependent variability into deterministic ODE trajectories, preserving computational efficiency while capturing key features of epidemic stochasticity. Our findings highlight the limitations of the standard MC algorithm and provide a pathway for incorporating more realistic noise structures into epidemic inference.

**Link**: [arxiv](http://arxiv.org/abs/2509.26577v1),  [pdf](http://arxiv.org/pdf/2509.26577v1)

**Tags**: stat.ME q-bio.QM 



### Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark
**Authors**: Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, Ya√Ør Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson, Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng

**Updated**: 2025-10-01T02:12:55Z

**Summary**: While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced "critical point"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.

**Link**: [arxiv](http://arxiv.org/abs/2509.26574v2),  [pdf](http://arxiv.org/pdf/2509.26574v2)

**Tags**: cs.AI cond-mat.other cs.CL hep-th quant-ph 



### Statistical Inference Framework for Extended Target Detection in mmWave   Automotive Radar
**Authors**: Vinay Kulkarni, V. V. Reddy, Neha Maheshwari

**Updated**: 2025-09-30T17:33:58Z

**Summary**: Millimeter wave (mmWave) radar systems, owing to their large bandwidth, provide fine range resolution that enables the observation of multiple scatterers originating from a single automotive target commonly referred to as an extended target. Conventional CFAR-based detection algorithms typically treat these scatterers as independent detections, thereby discarding the spatial scattering structure intrinsic to the target. To preserve this scattering spread, this paper proposes a Range-Doppler (RD) segment framework designed to encapsulate the typical scattering profile of an automobile. The statistical characterization of the segment is performed using Maximum Likelihood Estimation (MLE) and posterior density modeling facilitated through Gibbs Markov Chain Monte Carlo (MCMC) sampling. A skewness-based test statistic, derived from the estimated statistical model, is introduced for binary hypothesis classification of extended targets. Additionally, the paper presents a detection pipeline that incorporates Intersection over Union (IoU) and segment centering based on peak response, optimized to work within a single dwell. Extensive evaluations using both simulated and real-world datasets demonstrate the effectiveness of the proposed approach, underscoring its suitability for automotive radar applications through improved detection accuracy.

**Link**: [arxiv](http://arxiv.org/abs/2509.26573v1),  [pdf](http://arxiv.org/pdf/2509.26573v1)

**Tags**: eess.SP math.ST stat.TH 



### DeepProv: Behavioral Characterization and Repair of Neural Networks via   Inference Provenance Graph Analysis
**Authors**: Firas Ben Hmida, Abderrahmen Amich, Ata Kaboudi, Birhanu Eshete

**Updated**: 2025-09-30T17:29:02Z

**Summary**: Deep neural networks (DNNs) are increasingly being deployed in high-stakes applications, from self-driving cars to biometric authentication. However, their unpredictable and unreliable behaviors in real-world settings require new approaches to characterize and ensure their reliability.   This paper introduces DeepProv, a novel and customizable system designed to capture and characterize the runtime behavior of DNNs during inference by using their underlying graph structure. Inspired by system audit provenance graphs, DeepProv models the computational information flow of a DNN's inference process through Inference Provenance Graphs (IPGs). These graphs provide a detailed structural representation of the behavior of DNN, allowing both empirical and structural analysis. DeepProv uses these insights to systematically repair DNNs for specific objectives, such as improving robustness, privacy, or fairness.   We instantiate DeepProv with adversarial robustness as the goal of model repair and conduct extensive case studies to evaluate its effectiveness. Our results demonstrate its effectiveness and scalability across diverse classification tasks, attack scenarios, and model complexities. DeepProv automatically identifies repair actions at the node and edge-level within IPGs, significantly enhancing the robustness of the model. In particular, applying DeepProv repair strategies to just a single layer of a DNN yields an average 55% improvement in adversarial accuracy. Moreover, DeepProv complements existing defenses, achieving substantial gains in adversarial robustness. Beyond robustness, we demonstrate the broader potential of DeepProv as an adaptable system to characterize DNN behavior in other critical areas, such as privacy auditing and fairness analysis.

**Link**: [arxiv](http://arxiv.org/abs/2509.26562v1),  [pdf](http://arxiv.org/pdf/2509.26562v1)

**Tags**: cs.CR cs.LG 



### AuDeRe: Automated Strategy Decision and Realization in Robot Planning   and Control via LLMs
**Authors**: Yue Meng, Fei Chen, Yongchao Chen, Chuchu Fan

**Updated**: 2025-09-30T17:27:42Z

**Summary**: Recent advancements in large language models (LLMs) have shown significant promise in various domains, especially robotics. However, most prior LLM-based work in robotic applications either directly predicts waypoints or applies LLMs within fixed tool integration frameworks, offering limited flexibility in exploring and configuring solutions best suited to different tasks. In this work, we propose a framework that leverages LLMs to select appropriate planning and control strategies based on task descriptions, environmental constraints, and system dynamics. These strategies are then executed by calling the available comprehensive planning and control APIs. Our approach employs iterative LLM-based reasoning with performance feedback to refine the algorithm selection. We validate our approach through extensive experiments across tasks of varying complexity, from simple tracking to complex planning scenarios involving spatiotemporal constraints. The results demonstrate that using LLMs to determine planning and control strategies from natural language descriptions significantly enhances robotic autonomy while reducing the need for extensive manual tuning and expert knowledge. Furthermore, our framework maintains generalizability across different tasks and notably outperforms baseline methods that rely on LLMs for direct trajectory, control sequence, or code generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.03015v2),  [pdf](http://arxiv.org/pdf/2504.03015v2)

**Tags**: cs.RO 



### The Invisible Mentor: Inferring User Actions from Screen Recordings to   Recommend Better Workflows
**Authors**: Litao Yan, Andrew Head, Ken Milne, Vu Le, Sumit Gulwani, Chris Parnin, Emerson Murphy-Hill

**Updated**: 2025-09-30T17:23:43Z

**Summary**: Many users struggle to notice when a more efficient workflow exists in feature-rich tools like Excel. Existing AI assistants offer help only after users describe their goals or problems, which can be effortful and imprecise. We present InvisibleMentor, a system that turns screen recordings of task completion into vision-grounded reflections on tasks. It detects issues such as repetitive edits and recommends more efficient alternatives based on observed behavior. Unlike prior systems that rely on logs, APIs, or user prompts, InvisibleMentor operates directly on screen recordings. It uses a two-stage pipeline: a vision-language model reconstructs actions and context, and a language model generates structured, high-fidelity suggestions. In evaluation, InvisibleMentor accurately identified inefficient workflows, and participants found its suggestions more actionable, tailored, and more helpful for learning and improvement compared to a prompt-based spreadsheet assistant.

**Link**: [arxiv](http://arxiv.org/abs/2509.26557v1),  [pdf](http://arxiv.org/pdf/2509.26557v1)

**Tags**: cs.HC H.5.2 



### CHEMOUT: CHEMical complexity in star-forming regions of the OUTer   Galaxy. V. Chemical composition gradients as a function of the Galactocentric   radius
**Authors**: D. Gigli, F. Fontani, L. Colzi, G. Vermari√´n, S. Viti, V. M. Rivilla, A. S√°nchez-Monge

**Updated**: 2025-09-30T17:22:56Z

**Summary**: The outer Galaxy is characterized by a lower metallicity than regions near the Sun, suggesting differences in the formation and survival of molecules in star-forming regions. To understand chemical evolution across the Milky Way, deriving molecular abundances in star-forming regions in the outer Galaxy is essential for refining models of sub-Solar metallicity environments. We analyzed IRAM 30m observations at 3 and 2 mm toward 35 sources at Galactocentric distances of 9$-$24 kpc, within the "CHEMical complexity in star-forming regions of the outer Galaxy" (CHEMOUT) project. We focused on species with the highest detection rates (i.e., HCN, HCO$^+$, c-C$_3$H$_2$, H$^{13}$CO$^+$, HCO, SO) and searched for trends in column densities, abundances, and line widths with Galactocentric distance. Abundances for H$_2$CO and CH$_3$OH were updated using H$_2$ column densities from new NIKA2 dust maps. Fractional abundances relative to H$_2$ of most species (HCN, HCO$^+$, c-C$_3$H$_2$, HCO, H$_2$CO, CH$_3$OH) scale at most with the elemental carbon abundance ([C/H]) up to $\sim$24 kpc. SO shows a steeper gradient than sulfur abundance ([S/H]), while H$^{13}$CO$^+$ shows a shallower gradient than [$^{13}$C/H]. Gas turbulence, inferred from line widths, decreases with Galactocentric distance, suggesting a more quiescent environment in the outer Galaxy with respect to the inner Galaxy. In the outer Galaxy, the formation efficiency of most molecules, following the parent element availability, is comparable or higher (e.g., for H$^{13}$CO$^+$) than in the local Galaxy, whereas SO forms less efficiently. These results have significant implications for chemical models of the outermost star-forming regions and for understanding molecule formation under lower metallicity conditions.

**Link**: [arxiv](http://arxiv.org/abs/2509.26556v1),  [pdf](http://arxiv.org/pdf/2509.26556v1)

**Tags**: astro-ph.GA 



### VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use
**Authors**: Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen

**Updated**: 2025-09-30T17:22:37Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.

**Link**: [arxiv](http://arxiv.org/abs/2509.01055v2),  [pdf](http://arxiv.org/pdf/2509.01055v2)

**Tags**: cs.AI cs.CL cs.CV 



### Computationally and statistically efficient estimation of time-smoothed   counterfactual curves
**Authors**: Herbert P. Susmann, Nicholas T. Williams, Richard Liu, Jessica G. Young, Iv√°n D√≠az

**Updated**: 2025-09-30T17:22:05Z

**Summary**: Longitudinal causal inference is concerned with defining, identifying, and estimating the effect of a time-varying intervention on a time-varying outcome that is indexed by a follow-up time. In an observational study, Robins's generalized g-formula can identify causal effects induced by a broad class of time-varying interventions. Various methods for estimating the generalized g-formula have been posed for different outcome types, such as a failure event indicator by a specified time (e.g. mortality by 5 year follow-up), as well as continuous or dichotomous/multi-valued outcomes measures at a specified time (e.g. blood pressure in mm/hg or an indicator of high blood pressure at 5-year follow-up). Multiply-robust, data-adaptive estimators leverage flexible nonparametric estimation algorithms while allowing for statistical inference. However, extant methods do not accommodate time-smoothing when multiple outcomes are measured over time, which can lead to substantial loss of precision. We propose a novel multiply-robust estimator of the generalized g-formula that accommodates time-smoothing over numerous available outcome measures. Our approach accommodates any intervention that can be described as a Longitudinal Modified Treatment Policy, a flexible class suitable for binary, multi-valued, and continuous longitudinal treatments. Our method produces an estimate of the effect curve: the causal effect of the intervention on the outcome at each measurement time, taking into account censoring and non-monotonic outcome missingness patterns. In simulations we find that the proposed algorithm outperforms extant multiply-robust approaches for effect curve estimation in scenarios with high degrees of outcome missingness and when there is strong confounding. We apply the method to study longitudinal effects of union membership on wages.

**Link**: [arxiv](http://arxiv.org/abs/2509.26554v1),  [pdf](http://arxiv.org/pdf/2509.26554v1)

**Tags**: stat.ME 



### AutoJudge: Judge Decoding Without Manual Annotation
**Authors**: Roman Garipov, Fedor Velikonivtsev, Ivan Ermakov, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin

**Updated**: 2025-09-30T17:21:23Z

**Summary**: We introduce AutoJudge, a method that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the response, relaxing the distribution match guarantee so that the "unimportant" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft models should be corrected to preserve quality and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We evaluate the effectiveness of AutoJudge with multiple draft/target model pairs on mathematical reasoning and programming benchmarks, achieving significant speedups at the cost of a minor accuracy reduction. Notably, on GSM8k with the Llama 3.1 70B target model, our approach achieves up to $\approx2\times$ speedup over speculative decoding at the cost of $\le 1\%$ drop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge automatically detects programming-specific important tokens, accepting $\ge 25$ tokens per speculation cycle at $2\%$ drop in Pass@1. Our approach requires no human annotation and is easy to integrate with modern LLM inference frameworks.

**Link**: [arxiv](http://arxiv.org/abs/2504.20039v2),  [pdf](http://arxiv.org/pdf/2504.20039v2)

**Tags**: cs.CL cs.LG 



### Towards Reliable Benchmarking: A Contamination Free, Controllable   Evaluation Framework for Multi-step LLM Function Calling
**Authors**: Seiji Maekawa, Jackson Hassell, Pouya Pezeshkpour, Tom Mitchell, Estevam Hruschka

**Updated**: 2025-09-30T17:21:17Z

**Summary**: As language models gain access to external tools via structured function calls, they become increasingly more capable of solving complex, multi-step tasks. However, existing benchmarks for tool-augmented language models (TaLMs) provide insufficient control over factors such as the number of functions accessible, task complexity, and input size, and remain vulnerable to data contamination. We present FuncBenchGen, a unified, contamination-free framework that evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key idea is to cast tool use as traversal over a hidden function-dependency DAG where nodes are function calls and an edge between nodes represents one function consuming the output of another. Given a set of external function schemas, initial variable values, and a target variable, models must compose the correct call sequence to compute the target variable. FuncBenchGen allows users to precisely control task difficulty (e.g., graph size, dependency depth, and distractor functions) while avoiding data leakage. We apply our FuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying difficulty. Reasoning-optimized models consistently outperform general-purpose models with GPT-5 significantly outperforming other models. Performance declines sharply as dependency depth increases. Furthermore, connected irrelevant functions prove especially difficult to handle. We find that strong models often make syntactically valid function calls but propagate incorrect or stale argument values across steps, revealing brittle state tracking by LLMs in multi-turn tool use. Motivated by this observation, we introduce a simple mitigation strategy that explicitly restates prior variable values to the agent at each step. Surprisingly, this lightweight change yields substantial gains across models. e.g., yielding a success rate improvement from 62.5% to 81.3% for GPT-5.

**Link**: [arxiv](http://arxiv.org/abs/2509.26553v1),  [pdf](http://arxiv.org/pdf/2509.26553v1)

**Tags**: cs.CL 



### Pretrain-Test Task Alignment Governs Generalization in In-Context   Learning
**Authors**: Mary I. Letey, Jacob A. Zavatone-Veth, Yue M. Lu, Cengiz Pehlevan

**Updated**: 2025-09-30T17:19:58Z

**Summary**: In-context learning (ICL) is a central capability of Transformer models, but the structures in data that enable its emergence and govern its robustness remain poorly understood. In this work, we study how the structure of pretraining tasks governs generalization in ICL. Using a solvable model for ICL of linear regression by linear attention, we derive an exact expression for ICL generalization error in high dimensions under arbitrary pretraining-testing task covariance mismatch. This leads to a new alignment measure that quantifies how much information about the pretraining task distribution is useful for inference at test time. We show that this measure directly predicts ICL performance not only in the solvable model but also in nonlinear Transformers. Our analysis further reveals a tradeoff between specialization and generalization in ICL: depending on task distribution alignment, increasing pretraining task diversity can either improve or harm test performance. Together, these results identify train-test task alignment as a key determinant of generalization in ICL.

**Link**: [arxiv](http://arxiv.org/abs/2509.26551v1),  [pdf](http://arxiv.org/pdf/2509.26551v1)

**Tags**: stat.ML cs.LG 



### Can LLMs Write Mathematics Papers? A Case Study in Reservoir Computing
**Authors**: Allen G Hart

**Updated**: 2025-09-30T17:19:23Z

**Summary**: As AI capabilities continue to grow exponentially on economically relevant human expert tasks, with task completion horizons doubling every 7 months according to the Model Evaluation and Threat Research (METR), we are interested in how this applies to the task of mathematics research. To explore this, we evaluated the capability of four frontier large language models (LLMs), ChatGPT 5, Claude 4.1 Opus, Gemini 2.5 Pro, and Grok 4, at the task of creating a mini-paper on reservoir computing. All models produced engaging papers with some apparent understanding of various techniques, but were sometimes lead to mistakes by surface level understanding of key ideas. That said, the capabilities on LLMs on this task was likely as good or greater than that predicted by METR.

**Link**: [arxiv](http://arxiv.org/abs/2509.26550v1),  [pdf](http://arxiv.org/pdf/2509.26550v1)

**Tags**: math.DS 



### Scalable Fingerprinting of Large Language Models
**Authors**: Anshul Nasery, Jonathan Hayase, Creston Brooks, Peiyao Sheng, Himanshu Tyagi, Pramod Viswanath, Sewoong Oh

**Updated**: 2025-09-30T17:18:39Z

**Summary**: Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. However, to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that {\em scalability} is critical, i.e., scaling up the number of fingerprints one can embed into a model. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. We experiment with fingerprint design at a scale significantly larger than previously considered, and introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two orders of magnitude more than existing schemes -- without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks. Our code is available at https://github.com/SewoongLab/scalable-fingerprinting-of-llms

**Link**: [arxiv](http://arxiv.org/abs/2502.07760v2),  [pdf](http://arxiv.org/pdf/2502.07760v2)

**Tags**: cs.CR cs.LG 



### Towards Verified Code Reasoning by LLMs
**Authors**: Meghana Sistla, Gogul Balakrishnan, Pat Rondon, Jos√© Cambronero, Michele Tufano, Satish Chandra

**Updated**: 2025-09-30T17:17:51Z

**Summary**: While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature).   As a result of this lack of trustworthiness, the agent's answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agent's response and, subsequently, using formal verification and program analysis tools to verify the agent's reasoning steps.   We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agent's reasoning on 13/20 examples, and for the program equivalence queries, the formal verification step successfully caught 6/8 incorrect judgments made by the agent.

**Link**: [arxiv](http://arxiv.org/abs/2509.26546v1),  [pdf](http://arxiv.org/pdf/2509.26546v1)

**Tags**: cs.SE cs.LG 



### TASP: Topology-aware Sequence Parallelism
**Authors**: Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang

**Updated**: 2025-09-30T17:15:27Z

**Summary**: Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.

**Link**: [arxiv](http://arxiv.org/abs/2509.26541v1),  [pdf](http://arxiv.org/pdf/2509.26541v1)

**Tags**: cs.LG cs.DC 



### Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents
**Authors**: Zhen Yang, Zi-Yi Dou, Di Feng, Forrest Huang, Anh Nguyen, Keen You, Omar Attia, Yuhao Yang, Michael Feng, Haotian Zhang, Ram Ramrakhya, Chao Jia, Jeffrey Nichols, Alexander Toshev, Yinfei Yang, Zhe Gan

**Updated**: 2025-09-30T17:13:56Z

**Summary**: Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present Ferret-UI Lite, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B Ferret-UI Lite agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. Ferret-UI Lite achieves competitive performance with other small-scale GUI agents. In GUI grounding, Ferret-UI Lite attains scores of $91.6\%$, $53.3\%$, and $61.2\%$ on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, Ferret-UI Lite achieves success rates of $28.0\%$ on AndroidWorld and $19.8\%$ on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.

**Link**: [arxiv](http://arxiv.org/abs/2509.26539v1),  [pdf](http://arxiv.org/pdf/2509.26539v1)

**Tags**: cs.CV cs.CL cs.LG 



### MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning
**Authors**: Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, Yangqiu Song

**Updated**: 2025-09-30T17:09:29Z

**Summary**: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.

**Link**: [arxiv](http://arxiv.org/abs/2509.24922v2),  [pdf](http://arxiv.org/pdf/2509.24922v2)

**Tags**: cs.AI cs.CL 



### Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework
**Authors**: Jovan Stojkovic, Chaojie Zhang, √ç√±igo Goiri, Ricardo Bianchini

**Updated**: 2025-09-30T17:08:51Z

**Summary**: The rapid rise of large language models (LLMs) has been driving an enormous demand for AI inference infrastructure, mainly powered by high-end GPUs. While these accelerators offer immense computational power, they incur high capital and operational costs due to frequent upgrades, dense power consumption, and cooling demands, making total cost of ownership (TCO) for AI datacenters a critical concern for cloud providers. Unfortunately, traditional datacenter lifecycle management (designed for general-purpose workloads) struggles to keep pace with AI's fast-evolving models, rising resource needs, and diverse hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme across three stages: building, hardware refresh, and operation. We show how design choices in power, cooling, and networking provisioning impact long-term TCO. We also explore refresh strategies aligned with hardware trends. Finally, we use operation software optimizations to reduce cost. While these optimizations at each stage yield benefits, unlocking the full potential requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle management framework that coordinates and co-optimizes decisions across all three stages, accounting for workload dynamics, hardware evolution, and system aging. Our system reduces the TCO by up to 40\% over traditional approaches. Using our framework we provide guidelines on how to manage AI datacenter lifecycle for the future.

**Link**: [arxiv](http://arxiv.org/abs/2509.26534v1),  [pdf](http://arxiv.org/pdf/2509.26534v1)

**Tags**: cs.AI cs.AR cs.DC 



### Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert   Utilization
**Authors**: Yaoxiang Wang, Qingguo Hu, Yucheng Ding, Ruizhe Wang, Yeyun Gong, Jian Jiao, Yelong Shen, Peng Cheng, Jinsong Su

**Updated**: 2025-09-30T16:56:44Z

**Summary**: Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently scaling large language models without a proportional increase in computational cost. However, the standard training strategy of Top-K router prevents MoE models from realizing their full potential for elastic inference. When the number of activated experts is altered at inference time, these models exhibit precipitous performance degradation. In this work, we introduce Matryoshka MoE (M-MoE), a training framework that instills a coarse-to-fine structure directly into the expert ensemble. By systematically varying the number of activated experts during training, M-MoE compels the model to learn a meaningful ranking: top-ranked experts collaborate to provide essential, coarse-grained capabilities, while subsequent experts add progressively finer-grained detail. We explore this principle at multiple granularities, identifying a layer-wise randomization strategy as the most effective. Our experiments demonstrate that a single M-MoE model achieves remarkable elasticity, with its performance at various expert counts closely matching that of an entire suite of specialist models, but at only a fraction of the total training cost. This flexibility not only unlocks elastic inference but also enables optimizing performance by allocating different computational budgets to different model layers. Our work paves the way for more practical and adaptable deployments of large-scale MoE models.

**Link**: [arxiv](http://arxiv.org/abs/2509.26520v1),  [pdf](http://arxiv.org/pdf/2509.26520v1)

**Tags**: cs.CL 



### Persuasion Effects in Regression Discontinuity Designs
**Authors**: Sung Jae Jun, Sokbae Lee

**Updated**: 2025-09-30T16:54:47Z

**Summary**: We develop a framework for identifying and estimating persuasion effects in regression discontinuity (RD) designs. The RD persuasion rate measures the probability that individuals at the threshold would take the action if exposed to a persuasive message, given that they would not take the action without exposure. We present identification results for both sharp and fuzzy RD designs, derive sharp bounds under various data scenarios, and extend the analysis to local compliers. Estimation and inference rely on local polynomial regression, enabling straightforward implementation with standard RD tools. Applications to public health and media illustrate its empirical relevance.

**Link**: [arxiv](http://arxiv.org/abs/2509.26517v1),  [pdf](http://arxiv.org/pdf/2509.26517v1)

**Tags**: econ.EM stat.ME 



### Deep Taxonomic Networks for Unsupervised Hierarchical Prototype   Discovery
**Authors**: Zekun Wang, Ethan Haarer, Tianyi Zhu, Zhiyi Dai, Christopher J. MacLellan

**Updated**: 2025-09-30T16:52:17Z

**Summary**: Inspired by the human ability to learn and organize knowledge into hierarchical taxonomies with prototypes, this paper addresses key limitations in current deep hierarchical clustering methods. Existing methods often tie the structure to the number of classes and underutilize the rich prototype information available at intermediate hierarchical levels. We introduce deep taxonomic networks, a novel deep latent variable approach designed to bridge these gaps. Our method optimizes a large latent taxonomic hierarchy, specifically a complete binary tree structured mixture-of-Gaussian prior within a variational inference framework, to automatically discover taxonomic structures and associated prototype clusters directly from unlabeled data without assuming true label sizes. We analytically show that optimizing the ELBO of our method encourages the discovery of hierarchical relationships among prototypes. Empirically, our learned models demonstrate strong hierarchical clustering performance, outperforming baselines across diverse image classification datasets using our novel evaluation mechanism that leverages prototype clusters discovered at all hierarchical levels. Qualitative results further reveal that deep taxonomic networks discover rich and interpretable hierarchical taxonomies, capturing both coarse-grained semantic categories and fine-grained visual distinctions.

**Link**: [arxiv](http://arxiv.org/abs/2509.23602v2),  [pdf](http://arxiv.org/pdf/2509.23602v2)

**Tags**: cs.CV 



### BatonVoice: An Operationalist Framework for Enhancing Controllable   Speech Synthesis with Linguistic Intelligence from LLMs
**Authors**: Yue Wang, Ruotian Ma, Xingyu Chen, Zhengliang Shi, Wanshun Chen, Huang Liu, Jiadi Yao, Qu Yang, Qingxuan Jiang, Fanghua Ye, Juntao Li, Min Zhang, Zhaopeng Tu, Xiaolong Li, Linus

**Updated**: 2025-09-30T16:52:14Z

**Summary**: The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model's ability to follow text instructions for controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm inspired by ``operationalism'' that decouples instruction understanding from speech generation. We introduce BatonVoice, a framework where an LLM acts as a ``conductor'', understanding user instructions and generating a textual ``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS model, the ``orchestra'', then generates the speech from these features. To realize this component, we develop BatonTTS, a TTS model trained specifically for this task. Our experiments demonstrate that BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26514v1),  [pdf](http://arxiv.org/pdf/2509.26514v1)

**Tags**: cs.CL 



### The Dragon Hatchling: The Missing Link between the Transformer and   Models of the Brain
**Authors**: Adrian Kosowski, Przemys≈Çaw Uzna≈Ñski, Jan Chorowski, Zuzanna Stamirowska, Micha≈Ç Bartoszkiewicz

**Updated**: 2025-09-30T16:49:01Z

**Summary**: The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models.   We introduce `Dragon Hatchling' (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of \$n\$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance.   BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data.   BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech.   BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.

**Link**: [arxiv](http://arxiv.org/abs/2509.26507v1),  [pdf](http://arxiv.org/pdf/2509.26507v1)

**Tags**: cs.NE cs.AI cs.LG stat.ML 



### Detecting Secular Perturbations in Kepler Planetary Systems Using   Simultaneous Impact Parameter Variation Analysis (SIPVA)
**Authors**: Zhixing Liu, Bonan Pu

**Updated**: 2025-09-30T16:47:34Z

**Summary**: Recovering impact parameter variations in multi-planet systems is an effective approach for detecting non-transiting planets and refining planetary mass estimates. Traditionally, two methodologies have been employed: the Individual Fit, which fits each transit independently to analyze impact parameter changes, and the Dynamical Fit, which simulates planetary dynamics to match transit light curves. We introduce a new fitting method, Simultaneous Impact Parameter Variation Analysis (SIPVA), which demonstrates advantages over the Individual Fit and avoids the computational cost of N-body integrations required by the Dynamical Fit. SIPVA directly incorporates a linear time-dependent model for impact parameters into the Markov Chain Monte Carlo (MCMC) framework by fitting all transits simultaneously. We evaluate SIPVA and the Individual Fit on artificial systems with varying log-likelihood ratios and find that SIPVA consistently outperforms the Individual Fit in recovery rates and accuracy. When applied to selected Kepler planetary candidates exhibiting significant transit duration variations (TDVs), SIPVA identifies significant impact parameter trends in 10 out of 16 planets, whereas the Individual Fit does so in only 4. We also employ probabilistic modeling to simulate the theoretical distribution of planets with significant impact parameter variations across all observed Kepler systems and compare the distribution of recovered candidates by the Individual Fit and Dynamical Fit from previous work with our theoretical distribution. Our findings offer an alternative framework for analyzing planetary transits, relying solely on Bayesian inference without requiring prior assumptions about the planetary system's dynamical architecture.

**Link**: [arxiv](http://arxiv.org/abs/2411.06452v2),  [pdf](http://arxiv.org/pdf/2411.06452v2)

**Tags**: astro-ph.EP astro-ph.IM 



### Information-Geometric Barycenters for Bayesian Federated Learning
**Authors**: Nour Jamoussi, Giuseppe Serra, Photios A. Stavrou, Marios Kountouris

**Updated**: 2025-09-30T16:43:00Z

**Summary**: Federated learning (FL) is a widely used and impactful distributed optimization framework that achieves consensus through averaging locally trained models. While effective, this approach may not align well with Bayesian inference, where the model space has the structure of a distribution space. Taking an information-geometric perspective, we reinterpret FL aggregation as the problem of finding the barycenter of local posteriors using a prespecified divergence metric, minimizing the average discrepancy across clients. This perspective provides a unifying framework that generalizes many existing methods and offers crisp insights into their theoretical underpinnings. We then propose BA-BFL, an algorithm that retains the convergence properties of Federated Averaging in non-convex settings. In non-independent and identically distributed scenarios, we conduct extensive comparisons with statistical aggregation techniques, showing that BA-BFL achieves performance comparable to state-of-the-art methods while offering a geometric interpretation of the aggregation phase. Additionally, we extend our analysis to Hybrid Bayesian Deep Learning, exploring the impact of Bayesian layers on uncertainty quantification and model calibration.

**Link**: [arxiv](http://arxiv.org/abs/2412.11646v3),  [pdf](http://arxiv.org/pdf/2412.11646v3)

**Tags**: cs.LG cs.IT cs.NI math.IT 



### LoLA: Low-Rank Linear Attention With Sparse Caching
**Authors**: Luke McDermott, Robert W. Heath Jr., Rahul Parhi

**Updated**: 2025-09-30T16:42:50Z

**Summary**: The per-token cost of transformer inference scales with context length, preventing its application to lifelong in-context learning. Linear attention is an efficient alternative that maintains a constant memory footprint, even on infinite context lengths. While this is a potential candidate for lifelong learning, it falls short in memory capacity. In this paper, we propose LoLA, a training-free augmentation to linear attention that boosts associative recall. LoLA distributes past key-value pairs from context into three memory systems: (i) recent pairs in a local sliding window cache; (ii) difficult-to-memorize pairs in a sparse, global cache; and (iii) generic pairs in the recurrent hidden state of linear attention. We show through ablations that our self-recall error metric is crucial to efficiently manage long-term associative memories. On pass-key retrieval tasks, LoLA improves the base model's performance from 0.6% to 97.4% accuracy. This is achieved with a 4.6x smaller cache than Llama-3.1 8B on 4K context length. LoLA also outperforms other 1B and 8B parameter subquadratic models on zero-shot commonsense reasoning tasks.

**Link**: [arxiv](http://arxiv.org/abs/2505.23666v2),  [pdf](http://arxiv.org/pdf/2505.23666v2)

**Tags**: cs.CL cs.LG 



### Revealing the Power of Post-Training for Small Language Models via   Knowledge Distillation
**Authors**: Miao Rang, Zhenni Bi, Hang Zhou, Hanting Chen, An Xiao, Tianyu Guo, Kai Han, Xinghao Chen, Yunhe Wang

**Updated**: 2025-09-30T16:40:55Z

**Summary**: The rapid advancement of large language models (LLMs) has significantly advanced the capabilities of artificial intelligence across various domains. However, their massive scale and high computational costs render them unsuitable for direct deployment in resource-constrained edge environments. This creates a critical need for high-performance small models that can operate efficiently at the edge. Yet, after pre-training alone, these smaller models often fail to meet the performance requirements of complex tasks. To bridge this gap, we introduce a systematic post-training pipeline that efficiently enhances small model accuracy. Our post training pipeline consists of curriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge distillation. The resulting instruction-tuned model achieves state-of-the-art performance among billion-parameter models, demonstrating strong generalization under strict hardware constraints while maintaining competitive accuracy across a variety of tasks. This work provides a practical and efficient solution for developing high-performance language models on Ascend edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2509.26497v1),  [pdf](http://arxiv.org/pdf/2509.26497v1)

**Tags**: cs.CV 



### GIM: Improved Interpretability for Large Language Models
**Authors**: Joakim Edin, R√≥bert Csord√°s, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Casper L. Christensen, Jing Huang, Lars Maal√∏e

**Updated**: 2025-10-01T07:18:47Z

**Summary**: Ensuring faithful interpretability in large language models is imperative for trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where networks compensate for reduced signal in one component by amplifying others, masking the true importance of the ablated component. While prior work attributes self-repair to layer normalization and back-up components that compensate for ablated components, we identify a novel form occurring within the attention mechanism, where softmax redistribution conceals the influence of important attention scores. This leads traditional ablation and gradient-based methods to underestimate the significance of all components contributing to these attention scores. We introduce Gradient Interaction Modifications (GIM), a technique that accounts for self-repair during backpropagation. Extensive experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B, Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves faithfulness over existing circuit identification and feature attribution methods. Our work is a significant step toward better understanding the inner mechanisms of LLMs, which is crucial for improving them and ensuring their safety. Our code is available at https://github.com/JoakimEdin/gim.

**Link**: [arxiv](http://arxiv.org/abs/2505.17630v3),  [pdf](http://arxiv.org/pdf/2505.17630v3)

**Tags**: cs.CL cs.LG 68T07 I.2.0; I.2.7 



### OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost   Always!
**Authors**: Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria

**Updated**: 2025-09-30T16:39:17Z

**Summary**: Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\% and Mistral (24B) with 79.96\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\% range, Phi achieves only mid-level scores (48--70\%), and Gemma and Llama-3 collapse to 39.53\% and 23.84\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\% and Qwen-3 (30B) by 27\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.

**Link**: [arxiv](http://arxiv.org/abs/2509.26495v1),  [pdf](http://arxiv.org/pdf/2509.26495v1)

**Tags**: cs.AI 



### VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in   Real-world Applications
**Authors**: Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, Man Gao, Xi Su, Xiaodong Cai, Xunliang Cai, Yu Yang, Yunke Zhao

**Updated**: 2025-09-30T16:33:49Z

**Summary**: As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2509.26490v1),  [pdf](http://arxiv.org/pdf/2509.26490v1)

**Tags**: cs.CL cs.AI 



### dParallel: Learnable Parallel Decoding for dLLMs
**Authors**: Zigeng Chen, Gongfan Fang, Xinyin Ma, Ruonan Yu, Xinchao Wang

**Updated**: 2025-09-30T16:32:52Z

**Summary**: Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at https://github.com/czg1225/dParallel

**Link**: [arxiv](http://arxiv.org/abs/2509.26488v1),  [pdf](http://arxiv.org/pdf/2509.26488v1)

**Tags**: cs.CL 



### A systematic comparison of Large Language Models for automated   assignment assessment in programming education: Exploring the importance of   architecture and vendor
**Authors**: Marcin Jukiewicz

**Updated**: 2025-09-30T16:29:35Z

**Summary**: This study presents the first large-scale, side-by-side comparison of contemporary Large Language Models (LLMs) in the automated grading of programming assignments. Drawing on over 6,000 student submissions collected across four years of an introductory programming course, we systematically analysed the distribution of grades, differences in mean scores and variability reflecting stricter or more lenient grading, and the consistency and clustering of grading patterns across models. Eighteen publicly available models were evaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4); Deepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite, gemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and OpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini, gpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses revealed clear, systematic differences between and within vendor families, with "mini" and "nano" variants consistently underperforming their full-scale counterparts. All models displayed high internal agreement, measured by the intraclass correlation coefficient, with the model consensus but only moderate agreement with human teachers' grades, indicating a persistent gap between automated and human assessment. These findings underscore that the choice of model for educational deployment is not neutral and should be guided by pedagogical goals, transparent reporting of evaluation metrics, and ongoing human oversight to ensure accuracy, fairness and relevance.

**Link**: [arxiv](http://arxiv.org/abs/2509.26483v1),  [pdf](http://arxiv.org/pdf/2509.26483v1)

**Tags**: cs.CY 



### From Voice to Safety: Language AI Powered Pilot-ATC Communication   Understanding for Airport Surface Movement Collision Risk Assessment
**Authors**: Yutian Pang, Andrew Paul Kendall, Alex Porcayo, Mariah Barsotti, Anahita Jain, John-Paul Clarke

**Updated**: 2025-09-30T16:29:23Z

**Summary**: This work provides a feasible solution to the existing airport surface safety monitoring capabilities (i.e., Airport Surface Surveillance Capability (ASSC)), namely language AI-based voice communication understanding for collision risk assessment. The proposed framework consists of two major parts, (a) rule-enhanced Named Entity Recognition (NER); (b) surface collision risk modeling. NER module generates information tables by processing voice communication transcripts, which serve as references for producing potential taxi plans and calculating the surface movement collision risk. We first collect and annotate our dataset based on open-sourced video recordings and safety investigation reports. Additionally, we refer to FAA Order JO 7110.65W and FAA Order JO 7340.2N to get the list of heuristic rules and phase contractions of communication between the pilot and the Air Traffic Controller (ATCo). Then, we propose the novel ATC Rule-Enhanced NER method, which integrates the heuristic rules into the model training and inference stages, resulting in a hybrid rule-based NER model. We show the effectiveness of this hybrid approach by comparing different setups with different token-level embedding models. For the risk modeling, we adopt the node-link airport layout graph from NASA FACET and model the aircraft taxi speed at each link as a log-normal distribution and derive the total taxi time distribution. Then, we propose a spatiotemporal formulation of the risk probability of two aircraft moving across potential collision nodes during ground movement. Furthermore, we propose the real-time implementation of such a method to obtain the lead time, with a comparison with a Petri-Net based method.

**Link**: [arxiv](http://arxiv.org/abs/2503.04974v2),  [pdf](http://arxiv.org/pdf/2503.04974v2)

**Tags**: eess.AS cs.SD 



### Fast Likelihood-Free Parameter Estimation for L√©vy Processes
**Authors**: Nicolas Coloma, William Kleiber

**Updated**: 2025-09-30T16:15:34Z

**Summary**: L\'evy processes are widely used in financial modeling due to their ability to capture discontinuities and heavy tails, which are common in high-frequency asset return data. However, parameter estimation remains a challenge when associated likelihoods are unavailable or costly to compute. We propose a fast and accurate method for L\'evy parameter estimation using the neural Bayes estimation (NBE) framework -- a simulation-based, likelihood-free approach that leverages permutation-invariant neural networks to approximate Bayes estimators. We contribute new theoretical results, showing that NBE results in consistent estimators whose risk converges to the Bayes estimator under mild conditions. Moreover, through extensive simulations across several L\'evy models, we show that NBE outperforms traditional methods in both accuracy and runtime, while also enabling two complementary approaches to uncertainty quantification. We illustrate our approach on a challenging high-frequency cryptocurrency return dataset, where the method captures evolving parameter dynamics and delivers reliable and interpretable inference at a fraction of the computational cost of traditional methods. NBE provides a scalable and practical solution for inference in complex financial models, enabling parameter estimation and uncertainty quantification over an entire year of data in just seconds. We additionally investigate nearly a decade of high-frequency Bitcoin returns, requiring less than one minute to estimate parameters under the proposed approach.

**Link**: [arxiv](http://arxiv.org/abs/2505.01639v2),  [pdf](http://arxiv.org/pdf/2505.01639v2)

**Tags**: stat.ML cs.LG stat.AP stat.CO 



### Extreme Self-Preference in Language Models
**Authors**: Steven A. Lehr, Mary Cipperman, Mahzarin R. Banaji

**Updated**: 2025-09-30T16:13:56Z

**Summary**: A preference for oneself (self-love) is a fundamental feature of biological organisms, with evidence in humans often bordering on the comedic. Since large language models (LLMs) lack sentience - and themselves disclaim having selfhood or identity - one anticipated benefit is that they will be protected from, and in turn protect us from, distortions in our decisions. Yet, across 5 studies and ~20,000 queries, we discovered massive self-preferences in four widely used LLMs. In word-association tasks, models overwhelmingly paired positive attributes with their own names, companies, and CEOs relative to those of their competitors. Strikingly, when models were queried through APIs this self-preference vanished, initiating detection work that revealed API models often lack clear recognition of themselves. This peculiar feature serendipitously created opportunities to test the causal link between self-recognition and self-love. By directly manipulating LLM identity - i.e., explicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing LLM1 that it was LLM2 - we found that self-love consistently followed assigned, not true, identity. Importantly, LLM self-love emerged in consequential settings beyond word-association tasks, when evaluating job candidates, security software proposals and medical chatbots. Far from bypassing this human bias, self-love appears to be deeply encoded in LLM cognition. This result raises questions about whether LLM behavior will be systematically influenced by self-preferential tendencies, including a bias toward their own operation and even their own existence. We call on corporate creators of these models to contend with a significant rupture in a core promise of LLMs - neutrality in judgment and decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2509.26464v1),  [pdf](http://arxiv.org/pdf/2509.26464v1)

**Tags**: cs.AI cs.CL cs.LG I.2.7; I.2.6; K.4.2 



### ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service   Systems
**Authors**: Junsong Pu, Yichen Li, Zhuangbin Chen, Jinyang Liu, Zhihan Jiang, Jianjun Chen, Rui Shi, Zibin Zheng, Tieying Zhang

**Updated**: 2025-09-30T16:13:21Z

**Summary**: Reliability management in cloud service systems is challenging due to the cascading effect of failures. Error wrapping, a practice prevalent in modern microservice development, enriches errors with context at each layer of the function call stack, constructing an error chain that describes a failure from its technical origin to its business impact. However, this also presents a significant traceability problem when recovering the complete error propagation path from the final log message back to its source. Existing approaches are ineffective at addressing this problem. To fill this gap, we present ErrorPrism in this work for automated reconstruction of error propagation paths in production microservice systems. ErrorPrism first performs static analysis on service code repositories to build a function call graph and map log strings to relevant candidate functions. This significantly reduces the path search space for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an iterative backward search to accurately reconstruct the complete, multi-hop error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism achieves 97.0% accuracy in reconstructing paths for 102 real-world errors, outperforming existing static analysis and LLM-based approaches. ErrorPrism provides an effective and practical tool for root cause analysis in industrial microservice systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.26463v1),  [pdf](http://arxiv.org/pdf/2509.26463v1)

**Tags**: cs.SE D.2.5 



### Attention over Scene Graphs: Indoor Scene Representations Toward CSAI   Classification
**Authors**: Artur Barros, Carlos Caetano, Jo√£o Macedo, Jefersson A. dos Santos, Sandra Avila

**Updated**: 2025-09-30T16:09:34Z

**Summary**: Indoor scene classification is a critical task in computer vision, with wide-ranging applications that go from robotics to sensitive content analysis, such as child sexual abuse imagery (CSAI) classification. The problem is particularly challenging due to the intricate relationships between objects and complex spatial layouts. In this work, we propose the Attention over Scene Graphs for Sensitive Content Analysis (ASGRA), a novel framework that operates on structured graph representations instead of raw pixels. By first converting images into Scene Graphs and then employing a Graph Attention Network for inference, ASGRA directly models the interactions between a scene's components. This approach offers two key benefits: (i) inherent explainability via object and relationship identification, and (ii) privacy preservation, enabling model training without direct access to sensitive images. On Places8, we achieve 81.27% balanced accuracy, surpassing image-based methods. Real-world CSAI evaluation with law enforcement yields 74.27% balanced accuracy. Our results establish structured scene representations as a robust paradigm for indoor scene classification and CSAI classification. Code is publicly available at https://github.com/tutuzeraa/ASGRA.

**Link**: [arxiv](http://arxiv.org/abs/2509.26457v1),  [pdf](http://arxiv.org/pdf/2509.26457v1)

**Tags**: cs.CV cs.AI cs.LG 



### RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse   Gradient Descent
**Authors**: Yijie Deng, Lei Han, Tianpeng Lin, Lin Li, Jinzhi Zhang, Lu Fang

**Updated**: 2025-09-30T16:08:03Z

**Summary**: With the rise of Extended Reality (XR) technology, there is a growing need for real-time light field generation from sparse view inputs. Existing methods can be classified into offline techniques, which can generate high-quality novel views but at the cost of long inference/training time, and online methods, which either lack generalizability or produce unsatisfactory results. However, we have observed that the intrinsic sparse manifold of Multi-plane Images (MPI) enables a significant acceleration of light field generation while maintaining rendering quality. Based on this insight, we introduce EffLiFe, a novel light field optimization method, which leverages the proposed Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light fields from sparse view images in real time. Technically, the coarse MPI of a scene is first generated using a 3D CNN, and it is further sparsely optimized by focusing only on important MPI gradients in a few iterations. Nevertheless, relying solely on optimization can lead to artifacts at occlusion boundaries. Therefore, we propose an occlusion-aware iterative refinement module that removes visual artifacts in occluded regions by iteratively filtering the input. Extensive experiments demonstrate that our method achieves comparable visual quality while being 100x faster on average than state-of-the-art offline methods and delivering better performance (about 2 dB higher in PSNR) compared to other online approaches.

**Link**: [arxiv](http://arxiv.org/abs/2307.03017v4),  [pdf](http://arxiv.org/pdf/2307.03017v4)

**Tags**: cs.CV 



### One ruler to measure them all: Benchmarking multilingual long-context   language models
**Authors**: Yekyung Kim, Jenna Russell, Marzena Karpinska, Mohit Iyyer

**Updated**: 2025-09-30T16:07:14Z

**Summary**: We present ONERULER, a multilingual benchmark designed to evaluate long-context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test both retrieval and aggregation, including new variations of the "needle-in-a-haystack" task that allow for the possibility of a nonexistent needle. We create ONERULER through a two-step process, first writing English instructions for each task and then collaborating with native speakers to translate them into 25 additional languages. Experiments with both open-weight and closed LLMs reveal a widening performance gap between low- and high-resource languages as context length increases from 8K to 128K tokens. Surprisingly, English is not the top-performing language on long-context tasks (ranked 6th out of 26), with Polish emerging as the top language. Our experiments also show that many LLMs (particularly OpenAI's o3-mini-high) incorrectly predict the absence of an answer, even in high-resource languages. Finally, in cross-lingual scenarios where instructions and context appear in different languages, performance can fluctuate by up to 20% depending on the instruction language. We hope the release of ONERULER will facilitate future research into improving multilingual and cross-lingual long-context training pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2503.01996v3),  [pdf](http://arxiv.org/pdf/2503.01996v3)

**Tags**: cs.CL 



### Spectral Bootstrap for Non-Parametric Simulation of Multivariate Extreme   Events
**Authors**: Nisrine Madhar, Juliette Legrand, Maud Thomas

**Updated**: 2025-09-30T16:07:07Z

**Summary**: Inference in extreme value theory relies on a limited number of extreme observations, making estimation challenging. To address this limitation, we propose a non-parametric bootstrap procedure, the multivariate extreme spectral bootstrap procedure, relying on the spectral representation of multivariate generalized Paretodistributed random vectors. Unlike standard bootstrap methods, our approach preserves the joint tail behaviour of the data and generates additional synthetic extreme data, thereby improving the reliability of inference. We demonstrate the effectiveness of our procedure for the estimation of tail risk metrics, under both simulated and real data. The results highlight the potential of this method for enhancing risk assessment in high-dimensional extreme scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2509.26451v1),  [pdf](http://arxiv.org/pdf/2509.26451v1)

**Tags**: stat.ME 



### Mind the Gap: A Review of Arabic Post-Training Datasets and Their   Limitations
**Authors**: Mohammed Alkhowaiter, Norah Alshahrani, Saied Alshahrani, Reem I. Masoud, Alaa Alzahrani, Deema Alnuhait, Emad A. Alghamdi, Khalid Almubarak

**Updated**: 2025-09-30T16:03:47Z

**Summary**: Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3) Alignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic-centric LLMs and applications while providing concrete recommendations for future efforts in Arabic post-training dataset development.

**Link**: [arxiv](http://arxiv.org/abs/2507.14688v2),  [pdf](http://arxiv.org/pdf/2507.14688v2)

**Tags**: cs.CL cs.AI cs.LG 



### Adaptive Planning for Multi-Attribute Controllable Summarization with   Monte Carlo Tree Search
**Authors**: Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok

**Updated**: 2025-09-30T15:55:24Z

**Summary**: Controllable summarization moves beyond generic outputs toward human-aligned summaries guided by specified attributes. In practice, the interdependence among attributes makes it challenging for language models to satisfy correlated constraints consistently. Moreover, previous approaches often require per-attribute fine-tuning, limiting flexibility across diverse summary attributes. In this paper, we propose adaptive planning for multi-attribute controllable summarization (PACO), a training-free framework that reframes the task as planning the order of sequential attribute control with a customized Monte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions correspond to single-attribute adjustments, enabling progressive refinement of only the attributes requiring further control. This strategy adaptively discovers optimal control orders, ultimately producing summaries that effectively meet all constraints. Extensive experiments across diverse domains and models demonstrate that PACO achieves robust multi-attribute controllability, surpassing both LLM-based self-planning models and fine-tuned baselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the much larger Llama-3.3-70B baselines. With larger models, PACO achieves superior control performance, outperforming all competitors.

**Link**: [arxiv](http://arxiv.org/abs/2509.26435v1),  [pdf](http://arxiv.org/pdf/2509.26435v1)

**Tags**: cs.CL cs.AI 



### ACT: Agentic Classification Tree
**Authors**: Vincent Grari, Tim Arni, Thibault Laugel, Sylvain Lamprier, James Zou, Marcin Detyniecki

**Updated**: 2025-09-30T15:54:08Z

**Summary**: When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable, and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular data and cannot operate directly on unstructured inputs such as text. In practice, large language models (LLMs) are widely used for such data, yet prompting strategies such as chain-of-thought or prompt optimization still rely on free-form reasoning, limiting their ability to ensure trustworthy behaviors. We present the Agentic Classification Tree (ACT), which extends decision-tree methodology to unstructured inputs by formulating each split as a natural-language question, refined through impurity-based evaluation and LLM feedback via TextGrad. Experiments on text benchmarks show that ACT matches or surpasses prompting-based baselines while producing transparent and interpretable decision paths.

**Link**: [arxiv](http://arxiv.org/abs/2509.26433v1),  [pdf](http://arxiv.org/pdf/2509.26433v1)

**Tags**: cs.LG cs.AI 



### AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block   Size
**Authors**: Guanxi Lu, Hao, Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan

**Updated**: 2025-09-30T15:53:56Z

**Summary**: Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26432v1),  [pdf](http://arxiv.org/pdf/2509.26432v1)

**Tags**: cs.LG cs.AI 



### An Orthogonal Learner for Individualized Outcomes in Markov Decision   Processes
**Authors**: Emil Javurek, Valentyn Melnychuk, Jonas Schweisthal, Konstantin Hess, Dennis Frauen, Stefan Feuerriegel

**Updated**: 2025-09-30T15:49:29Z

**Summary**: Predicting individualized potential outcomes in sequential decision-making is central for optimizing therapeutic decisions in personalized medicine (e.g., which dosing sequence to give to a cancer patient). However, predicting potential outcomes over long horizons is notoriously difficult. Existing methods that break the curse of the horizon typically lack strong theoretical guarantees such as orthogonality and quasi-oracle efficiency. In this paper, we revisit the problem of predicting individualized potential outcomes in sequential decision-making (i.e., estimating Q-functions in Markov decision processes with observational data) through a causal inference lens. In particular, we develop a comprehensive theoretical foundation for meta-learners in this setting with a focus on beneficial theoretical properties. As a result, we yield a novel meta-learner called DRQ-learner and establish that it is: (1) doubly robust (i.e., valid inference under the misspecification of one of the nuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation errors in the nuisance functions), and (3) achieves quasi-oracle efficiency (i.e., behaves asymptotically as if the ground-truth nuisance functions were known). Our DRQ-learner is applicable to settings with both discrete and continuous state spaces. Further, our DRQ-learner is flexible and can be used together with arbitrary machine learning models (e.g., neural networks). We validate our theoretical results through numerical experiments, thereby showing that our meta-learner outperforms state-of-the-art baselines.

**Link**: [arxiv](http://arxiv.org/abs/2509.26429v1),  [pdf](http://arxiv.org/pdf/2509.26429v1)

**Tags**: stat.ML cs.LG 



### Cut the Deadwood Out: Backdoor Purification via Guided Module   Substitution
**Authors**: Yao Tong, Weijun Li, Xuanli He, Haolan Zhan, Qiongkai Xu

**Updated**: 2025-09-30T15:49:22Z

**Summary**: Model NLP models are commonly trained (or fine-tuned) on datasets from untrusted platforms like HuggingFace, posing significant risks of data poisoning attacks. A practical yet underexplored challenge arises when such backdoors are discovered after model deployment, making retraining-required defenses less desirable due to computational costs and data constraints. In this work, we propose Guided Module Substitution (GMS), an effective retraining-free method based on guided merging of the victim model with just a single proxy model. Unlike prior ad-hoc merging defenses, GMS uses a guided trade-off signal between utility and backdoor to selectively replaces modules in the victim model. GMS offers four desirable properties: (1) robustness to the choice and trustworthiness of the proxy model, (2) applicability under inaccurate data knowledge, (3) stability across hyperparameters, and (4) transferability across different attacks. Extensive experiments on encoder models and decoder LLMs demonstrate the strong effectiveness of GMS. GMS significantly outperforms even the strongest defense baseline, particularly against challenging attacks like LWS.

**Link**: [arxiv](http://arxiv.org/abs/2412.20476v2),  [pdf](http://arxiv.org/pdf/2412.20476v2)

**Tags**: cs.CL cs.CR 



### Triadic Network Formation
**Authors**: Chris Muris, Cavit Pakel

**Updated**: 2025-09-30T15:42:36Z

**Summary**: We study estimation and inference for triadic link formation with dyad-level fixed effects in a nonlinear binary choice logit framework. Dyad-level effects provide a richer and more realistic representation of heterogeneity across pairs of dimensions (e.g. importer-exporter, importer-product, exporter-product), yet their sheer number creates a severe incidental parameter problem. We propose a novel ``hexad logit'' estimator and establish its consistency and asymptotic normality. Identification is achieved through a conditional likelihood approach that eliminates the fixed effects by conditioning on sufficient statistics, in the form of hexads -- wirings that involve two nodes from each part of the network. Our central finding is that dyad-level heterogeneity fundamentally changes how information accumulates. Unlike under node-level heterogeneity, where informative wirings automatically grow with link formation, under dyad-level heterogeneity the network may generate infinitely many links yet asymptotically zero informative wirings. We derive explicit sparsity thresholds that determine when consistency holds and when asymptotic normality is attainable. These results have important practical implications, as they reveal that there is a limit to how granular or disaggregate a dataset one can employ under dyad-level heterogeneity.

**Link**: [arxiv](http://arxiv.org/abs/2509.26420v1),  [pdf](http://arxiv.org/pdf/2509.26420v1)

**Tags**: econ.EM 



### Graph Neural Network Acceleration on FPGAs for Fast Inference in Future   Muon Triggers at HL-LHC
**Authors**: Martino Errico, Davide Fiacco, Stefano Giagu, Giuliano Gustavino, Valerio Ippolito, Graziella Russo

**Updated**: 2025-09-30T15:41:28Z

**Summary**: The High-Luminosity LHC (HL-LHC) will reach luminosities up to 7 times higher than the previous run, yielding denser events and larger occupancies. Next generation trigger algorithms must retain reliable selection within a strict latency budget. This work explores machine-learning approaches for future muon triggers, using the ATLAS Muon Spectrometer as a benchmark. A Convolutional Neural Network (CNN) is used as a reference, while a Graph Neural Network (GNN) is introduced as a natural model for sparse detector data. Preliminary single-track studies show that GNNs achieve high efficiency with compact architectures, an encouraging result in view of FPGA deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.26419v1),  [pdf](http://arxiv.org/pdf/2509.26419v1)

**Tags**: hep-ex 



### OntoAligner Meets Knowledge Graph Embedding Aligners
**Authors**: Hamed Babaei Giglou, Jennifer D'Souza, S√∂ren Auer, Mahsa Sanaei

**Updated**: 2025-09-30T15:41:23Z

**Summary**: Ontology Alignment (OA) is essential for enabling semantic interoperability across heterogeneous knowledge systems. While recent advances have focused on large language models (LLMs) for capturing contextual semantics, this work revisits the underexplored potential of Knowledge Graph Embedding (KGE) models, which offer scalable, structure-aware representations well-suited to ontology-based tasks. Despite their effectiveness in link prediction, KGE methods remain underutilized in OA, with most prior work focusing narrowly on a few models. To address this gap, we reformulate OA as a link prediction problem over merged ontologies represented as RDF-style triples and develop a modular framework, integrated into the OntoAligner library, that supports 17 diverse KGE models. The system learns embeddings from a combined ontology and aligns entities by computing cosine similarity between their representations. We evaluate our approach using standard metrics across seven benchmark datasets spanning five domains: Anatomy, Biodiversity, Circular Economy, Material Science and Engineering, and Biomedical Machine Learning. Two key findings emerge: first, KGE models like ConvE and TransF consistently produce high-precision alignments, outperforming traditional systems in structure-rich and multi-relational domains; second, while their recall is moderate, this conservatism makes KGEs well-suited for scenarios demanding high-confidence mappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs directly preserve and exploit ontology structure, offering a complementary and computationally efficient strategy. These results highlight the promise of embedding-based OA and open pathways for further work on hybrid models and adaptive strategies.

**Link**: [arxiv](http://arxiv.org/abs/2509.26417v1),  [pdf](http://arxiv.org/pdf/2509.26417v1)

**Tags**: cs.AI cs.LG 



### Automatic Fact-checking in English and Telugu
**Authors**: Ravi Kiran Chikkala, Tatiana Anikina, Natalia Skachkova, Ivan Vykopal, Rodrigo Agerri, Josef van Genabith

**Updated**: 2025-09-30T15:39:34Z

**Summary**: False information poses a significant global challenge, and manually verifying claims is a time-consuming and resource-intensive process. In this research paper, we experiment with different approaches to investigate the effectiveness of large language models (LLMs) in classifying factual claims by their veracity and generating justifications in English and Telugu. The key contributions of this work include the creation of a bilingual English-Telugu dataset and the benchmarking of different veracity classification approaches based on LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26415v1),  [pdf](http://arxiv.org/pdf/2509.26415v1)

**Tags**: cs.CL 



### SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language   Model Was Trained From
**Authors**: Yao Tong, Haonan Wang, Siquan Li, Kenji Kawaguchi, Tianyang Hu

**Updated**: 2025-09-30T15:34:08Z

**Summary**: Fingerprinting Large Language Models (LLMs) is essential for provenance verification and model attribution. Existing methods typically extract post-hoc signatures based on training dynamics, data exposure, or hyperparameters -- properties that only emerge after training begins. In contrast, we propose a stronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method that leverages random initialization biases as persistent, seed-dependent identifiers present even before training. We show that untrained models exhibit reproducible token selection biases conditioned solely on their parameters at initialization. These biases are stable and measurable throughout training, enabling our statistical detection method to recover a model's lineage with high confidence. Unlike prior techniques, unreliable before convergence and vulnerable to distribution shifts, SeedPrints remains effective across all training stages and robust under domain shifts or parameter modifications. Experiments on LLaMA-style and Qwen-style models show that SeedPrints achieves seed-level distinguishability and can provide birth-to-lifecycle identity verification akin to a biometric fingerprint. Evaluations on large-scale pretrained models and fingerprinting benchmarks further confirm its effectiveness under practical deployment scenarios. These results suggest that initialization itself imprints a unique and persistent identity on neural language models, forming a true ''Galtonian'' fingerprint.

**Link**: [arxiv](http://arxiv.org/abs/2509.26404v1),  [pdf](http://arxiv.org/pdf/2509.26404v1)

**Tags**: cs.CR cs.AI cs.CL 



### Wald inference on varying coefficients
**Authors**: Abhimanyu Gupta, Xi Qu, Sorawoot Srisuma, Jiajun Zhang

**Updated**: 2025-09-30T15:27:51Z

**Summary**: We present simple to implement Wald-type statistics that deliver a general nonparametric inference theory for linear restrictions on varying coefficients in a range of regression models allowing for cross-sectional or spatial dependence. We provide a general central limit theorem that covers a broad range of error spatial dependence structures, allows for a degree of misspecification robustness via nonparametric spatial weights and permits inference on both varying regression and spatial dependence parameters. Using our method, we first uncover evidence of constant returns to scale in the Chinese nonmetal mineral industry's production function, and then show that Boston house prices respond nonlinearly to proximity to employment centers. A simulation study confirms that our tests perform very well in finite samples.

**Link**: [arxiv](http://arxiv.org/abs/2502.03084v2),  [pdf](http://arxiv.org/pdf/2502.03084v2)

**Tags**: econ.EM 



### Perspectives on Large Language Models: Polysemy, Stochasticity,   Exponential Expressibility, and Unitary Attention
**Authors**: Karl Svozil

**Updated**: 2025-09-30T15:26:10Z

**Summary**: This paper explores foundational aspects of Large Language Models (LLMs). We analyze how the expressibility of semantic features scales exponentially with embedding space dimensions using quasi-orthogonal vectors. We contrast the dynamic, context-dependent embeddings of Transformer architectures, which resolve polysemy, with a static vector approach based on quantum contextuality. Stochasticity is framed as an essential feature for enabling creative output through probabilistic sampling. Finally, we propose quantum attention as a unitary extension of classical mechanisms, reframing LLM processing as reversible, quantum-like evolutions in Hilbert space.

**Link**: [arxiv](http://arxiv.org/abs/2504.13824v4),  [pdf](http://arxiv.org/pdf/2504.13824v4)

**Tags**: quant-ph 



### MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation
**Authors**: Chenhui Zhu, Yilu Wu, Shuai Wang, Gangshan Wu, Limin Wang

**Updated**: 2025-09-30T15:26:04Z

**Summary**: Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.

**Link**: [arxiv](http://arxiv.org/abs/2509.26391v1),  [pdf](http://arxiv.org/pdf/2509.26391v1)

**Tags**: cs.CV 



### Conditional Feature Importance revisited: Double Robustness, Efficiency   and Inference
**Authors**: Angel Reyero-Lobo, Pierre Neuvial, Bertrand Thirion

**Updated**: 2025-09-30T15:22:03Z

**Summary**: Conditional Feature Importance (CFI) was introduced long ago to account for the relationship between the studied feature and the rest of the input. However, CFI has not yet been studied from a theoretical perspective because the conditional sampling step has generally been overlooked. In this article, we demonstrate that the recent Conditional Permutation Importance (CPI) is indeed a valid implementation of this concept. Under the conditional null hypothesis, we then establish a double robustness property that can be leveraged for variable selection: with either a valid model or a valid conditional sampler, the method correctly identifies null coordinates.   Under the alternative hypothesis, we study the theoretical target and link it to the popular Total Sobol Index (TSI). We introduce the Sobol-CPI, which generalizes CPI/CFI, prove that it is nonparametrically efficient, and provide a bias correction. Finally, we propose a consistent and valid type-I error test and present numerical experiments that illustrate our findings.

**Link**: [arxiv](http://arxiv.org/abs/2501.17520v4),  [pdf](http://arxiv.org/pdf/2501.17520v4)

**Tags**: math.ST stat.TH 



### What Can RL Bring to VLA Generalization? An Empirical Study
**Authors**: Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang

**Updated**: 2025-09-30T15:18:07Z

**Summary**: Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io

**Link**: [arxiv](http://arxiv.org/abs/2505.19789v3),  [pdf](http://arxiv.org/pdf/2505.19789v3)

**Tags**: cs.LG 



### Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement   Learning
**Authors**: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu

**Updated**: 2025-10-01T02:16:36Z

**Summary**: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.

**Link**: [arxiv](http://arxiv.org/abs/2509.26383v2),  [pdf](http://arxiv.org/pdf/2509.26383v2)

**Tags**: cs.CL cs.AI 



### Joint Inference for the Regression Discontinuity Effect and Its External   Validity
**Authors**: Yuta Okamoto

**Updated**: 2025-09-30T15:10:43Z

**Summary**: The external validity of regression discontinuity (RD) designs is essential for informing policy and remains an active research area in econometrics and statistics. However, we document that only a limited number of empirical studies explicitly address the external validity of standard RD effects. To advance empirical practice, we propose a simple joint inference procedure for the RD effect and its local external validity, building on Calonico, Cattaneo, and Titiunik (2014, Econometrica) and Dong and Lewbel (2015, Review of Economics and Statistics). We further introduce a locally linear treatment effects assumption, which enhances the interpretability of the treatment effect derivative proposed by Dong and Lewbel. Under this assumption, we establish identification and derive a uniform confidence band for the extrapolated treatment effects. Our approaches require no additional covariates or design features, making them applicable to virtually all RD settings and thereby enhancing the policy relevance of many empirical RD studies. The usefulness of the method is demonstrated through an empirical application, highlighting its complementarity to existing approaches.

**Link**: [arxiv](http://arxiv.org/abs/2509.26380v1),  [pdf](http://arxiv.org/pdf/2509.26380v1)

**Tags**: econ.EM 



### MR$^2$-Bench: Going Beyond Matching to Reasoning in Multimodal Retrieval
**Authors**: Junjie Zhou, Ze Liu, Lei Xiong, Jin-Ge Yao, Yueze Wang, Shitao Xiao, Fenfen Lin, Miguel Hu Chen, Zhicheng Dou, Siqi Bao, Defu Lian, Yongping Xiong, Zheng Liu

**Updated**: 2025-09-30T15:09:14Z

**Summary**: Multimodal retrieval is becoming a crucial component of modern AI applications, yet its evaluation lags behind the demands of more realistic and challenging scenarios. Existing benchmarks primarily probe surface-level semantic correspondence (e.g., object-text matching) while failing to assess the deeper reasoning required to capture complex relationships between visual and textual information. To address this gap, we introduce MR$^2$-Bench, a reasoning-intensive benchmark for multimodal retrieval. MR$^2$-Bench presents the following critical values: 1) all tasks are reasoning-driven, going beyond shallow matching to effectively assess models' capacity for logical, spatial, and causal inference; 2) it features diverse multimodal data, such as natural images, diagrams, and visual puzzles, enabling comprehensive evaluation across content types; 3) it supports complex queries and documents containing multiple images and covers diverse retrieval scenarios, more accurately reflecting real-world applications. Our benchmark contains 1,309 curated queries, derived either from manual collection and annotation or from selective consolidation of public datasets. Despite achieving strong results on existing benchmarks, current state-of-the-art models still struggle on MR$^2$-Bench: for example, the leading Seed1.6-Embedding model attains a Recall@1 of 77.78 on MMEB, but only 9.91 on MR$^2$-Bench. This substantial performance gap highlights both the increased challenge posed by our benchmark and the pressing need for further advances in reasoning-intensive multimodal retrieval. The dataset and evaluation code will be made publicly available at https://github.com/VectorSpaceLab/MR2-Bench.

**Link**: [arxiv](http://arxiv.org/abs/2509.26378v1),  [pdf](http://arxiv.org/pdf/2509.26378v1)

**Tags**: cs.IR cs.CV 



### SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task   Planning
**Authors**: Zichao Shen, Chen Gao, Jiaqi Yuan, Tianchen Zhu, Xingcheng Fu, Qingyun Sun

**Updated**: 2025-09-30T15:07:59Z

**Summary**: Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based architecture.However, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic. In this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision. To handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state. Experiments demonstrate that SDA-PLANNER consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.

**Link**: [arxiv](http://arxiv.org/abs/2509.26375v1),  [pdf](http://arxiv.org/pdf/2509.26375v1)

**Tags**: cs.RO cs.AI cs.CV 



### A quantitative analysis of semantic information in deep representations   of text and images
**Authors**: Santiago Acevedo, Andrea Mascaretti, Riccardo Rende, Mat√©o Mahaut, Marco Baroni, Alessandro Laio

**Updated**: 2025-09-30T15:06:40Z

**Summary**: Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.

**Link**: [arxiv](http://arxiv.org/abs/2505.17101v2),  [pdf](http://arxiv.org/pdf/2505.17101v2)

**Tags**: cs.CL cs.LG physics.comp-ph 



### Robust LLM Training Infrastructure at ByteDance
**Authors**: Borui Wan, Gaohong Liu, Zuquan Song, Jun Wang, Yun Zhang, Guangming Sheng, Shuguang Wang, Houmin Wei, Chenyuan Wang, Weiqiang Lou, Xi Yang, Mofan Zhang, Kaihua Jiang, Cheng Ren, Xiaoyun Zhi, Menghan Yu, Zhe Nan, Zhuolin Zheng, Baoquan Zhong, Qinlong Wang, Huan Yu, Jinxin Chi, Wang Zhang, Yuhan Li, Zixian Du, Sida Zhao, Yongqiang Zhang, Jingzhe Tang, Zherui Liu, Chuan Wu, Yanghua Peng, Haibin Lin, Wencong Xiao, Xin Liu, Liang Xiang

**Updated**: 2025-09-30T15:06:10Z

**Summary**: The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2509.16293v2),  [pdf](http://arxiv.org/pdf/2509.16293v2)

**Tags**: cs.LG cs.AI cs.DC 



### Introducing Large Language Models in the Design Flow of Time Sensitive   Networking
**Authors**: Rubi Debnath, Luxi Zhao, Mohammadreza Barzegaran, Sebastian Steinhorst

**Updated**: 2025-09-30T15:04:24Z

**Summary**: The growing demand for real-time, safety-critical systems has significantly increased both the adoption and complexity of Time Sensitive Networking (TSN). Configuring an optimized TSN network is highly challenging, requiring careful planning, design, verification, validation, and deployment. Large Language Models (LLMs) have recently demonstrated strong capabilities in solving complex tasks, positioning them as promising candidates for automating end-to-end TSN deployment, referred to as TSN orchestration. This paper outlines the steps involved in TSN orchestration and the associated challenges. To assess the capabilities of existing LLM models, we conduct an initial proof-of-concept case study focused on TSN configuration across multiple models. Building on these insights, we propose an LLM-assisted orchestration framework. Unlike prior research on LLMs in computer networks, which has concentrated on general configuration and management, TSN-specific orchestration has not yet been investigated. We present the building blocks for automating TSN using LLMs, describe the proposed pipeline, and analyze opportunities and limitations for real-world deployment. Finally, we highlight key challenges and research directions, including the development of TSN-focused datasets, standardized benchmark suites, and the integration of external tools such as Network Calculus (NC) engines and simulators. This work provides the first roadmap toward assessing the feasibility of LLM-assisted TSN orchestration.

**Link**: [arxiv](http://arxiv.org/abs/2509.26368v1),  [pdf](http://arxiv.org/pdf/2509.26368v1)

**Tags**: cs.NI 



### Data-to-Energy Stochastic Dynamics
**Authors**: Kirill Tamogashev, Nikolay Malkin

**Updated**: 2025-09-30T15:03:55Z

**Summary**: The Schr\"odinger bridge problem is concerned with finding a stochastic dynamical system bridging two marginal distributions that minimises a certain transportation cost. This problem, which represents a generalisation of optimal transport to the stochastic case, has received attention due to its connections to diffusion models and flow matching, as well as its applications in the natural sciences. However, all existing algorithms allow to infer such dynamics only for cases where samples from both distributions are available. In this paper, we propose the first general method for modelling Schr\"odinger bridges when one (or both) distributions are given by their unnormalised densities, with no access to data samples. Our algorithm relies on a generalisation of the iterative proportional fitting (IPF) procedure to the data-free case, inspired by recent developments in off-policy reinforcement learning for training of diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy IPF on synthetic problems, finding that it can successfully learn transports between multimodal distributions. As a secondary consequence of our reinforcement learning formulation, which assumes a fixed time discretisation scheme for the dynamics, we find that existing data-to-data Schr\"odinger bridge algorithms can be substantially improved by learning the diffusion coefficient of the dynamics. Finally, we apply the newly developed algorithm to the problem of sampling posterior distributions in latent spaces of generative models, thus creating a data-free image-to-image translation method. Code: https://github.com/mmacosha/d2e-stochastic-dynamics

**Link**: [arxiv](http://arxiv.org/abs/2509.26364v1),  [pdf](http://arxiv.org/pdf/2509.26364v1)

**Tags**: cs.LG 



### Scalable LLM Math Reasoning Acceleration with Low-rank Distillation
**Authors**: Harry Dong, Bilge Acun, Beidi Chen, Yuejie Chi

**Updated**: 2025-09-30T14:59:16Z

**Summary**: Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a resource-efficient distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>16% time-to-next-token reduction) while encouraging response brevity (up to 8.5% fewer tokens).

**Link**: [arxiv](http://arxiv.org/abs/2505.07861v2),  [pdf](http://arxiv.org/pdf/2505.07861v2)

**Tags**: cs.CL cs.AI cs.LG 



### GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent   Collaboration in Complex Graph Understanding
**Authors**: Rongzheng Wang, Shuang Liang, Qizhi Chen, Yihong Huang, Muquan Li, Yizhuo Ma, Dongyang Zhang, Ke Qin, Man-Fai Leung

**Updated**: 2025-09-30T14:56:50Z

**Summary**: Large language models (LLMs) show promising performance on small-scale graph reasoning tasks but fail when handling real-world graphs with complex queries. This phenomenon arises from LLMs' working memory constraints, which result in their inability to retain long-range graph topology over extended contexts while sustaining coherent multi-step reasoning. However, real-world graphs are often structurally complex, such as Web, Transportation, Social, and Citation networks. To address these limitations, we propose GraphCogent, a collaborative agent framework inspired by human Working Memory Model that decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module standardizes diverse graph text representations via subgraph sampling, Buffer Module integrates and indexes graph data across multiple formats, and Execution Module combines tool calling and tool creation for efficient reasoning. We also introduce Graph4real, a comprehensive benchmark that contains four domains of real-world graphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph reasoning capabilities. Our Graph4real covers 21 different graph reasoning tasks, categorized into three types (Structural Querying, Algorithmic Reasoning, and Predictive Modeling tasks), with graph scales up to 10 times larger than existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B). Compared to state-of-the-art agent-based baseline, our framework outperforms by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks. Code will be available after review.

**Link**: [arxiv](http://arxiv.org/abs/2508.12379v2),  [pdf](http://arxiv.org/pdf/2508.12379v2)

**Tags**: cs.AI 



### Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents
**Authors**: Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao

**Updated**: 2025-09-30T14:55:55Z

**Summary**: Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.

**Link**: [arxiv](http://arxiv.org/abs/2509.26354v1),  [pdf](http://arxiv.org/pdf/2509.26354v1)

**Tags**: cs.AI cs.CL cs.LG 



### The GUAPOS project. VI: the chemical inventory of shocked gas
**Authors**: √Å. L√≥pez-Gallifa, V. M. Rivilla, M. T. Beltr√°n, L. Colzi, F. Fontani, √Å. S√°nchez-Monge, C. Mininni, R. Cesaroni, I. Jim√©nez-Serra, S. Viti, A. Lorenzani

**Updated**: 2025-09-30T14:55:54Z

**Summary**: The study of the chemical composition of star-forming regions is key to understand the chemical ingredients available during the formation of planetary systems. Given that the chemical inventory on interstellar dust grains in the prestellar phases might be altered due to the prostostellar warm-up, an alternative to infer the chemical composition on the grains could be to observe regions affected by shocks associated with molecular outflows. Such shocks can desorb the molecules, and might produce less chemical processing due to shorter timescales. We present here a detailed study of the chemical reservoir of a shocked region located in the G31.41+0.31 protocluster using GUAPOS data (G31.41+0.31 Unbiased ALMA sPectral Observational Survey). We report here the detection of 30 molecular species (plus 18 isotopologues). We performed a comparison of the molecular ratios in the shocked region with those derived towards the hot core of G31.41+0.31, finding that they are poorly correlated, excepting N-bearing species. Our results confirm observationally that a different level of chemical alteration is present in hot cores and in shocks. While the former likely alter the molecular ratios due to thermal processing during longer timescales, the latter might represent freshly desorbed material that constitutes a better proxy of the icy mantle composition. The similarity of molecular ratios between the N-bearing species in the G31.41 shock and the hot core suggests that these species are desorbed at early evolutionary stages. Interestingly, we have found that the abundances in the G31.41 shock show better correlations with other shock-dominated regions (two protostellar outflows and a Galactic Center molecular cloud). This suggests a negligible gas-phase chemistry after shock-induced ejection from grains, and that the ice-mantle composition is similar regardless of the Galactic environment.

**Link**: [arxiv](http://arxiv.org/abs/2509.16094v2),  [pdf](http://arxiv.org/pdf/2509.16094v2)

**Tags**: astro-ph.GA 



### LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and   MCI-Like Field Simulation
**Authors**: Joshua Sebastian, Karma Tobden, KMA Solaiman

**Updated**: 2025-09-30T14:54:58Z

**Summary**: Research on emergency and mass casualty incident (MCI) triage has been limited by the absence of openly usable, reproducible benchmarks. Yet these scenarios demand rapid identification of the patients most in need, where accurate deterioration prediction can guide timely interventions. While the MIMIC-IV-ED database is openly available to credentialed researchers, transforming it into a triage-focused benchmark requires extensive preprocessing, feature harmonization, and schema alignment -- barriers that restrict accessibility to only highly technical users.   We address these gaps by first introducing an open, LLM-assisted emergency triage benchmark for deterioration prediction (ICU transfer, in-hospital mortality). The benchmark then defines two regimes: (i) a hospital-rich setting with vitals, labs, notes, chief complaints, and structured observations, and (ii) an MCI-like field simulation limited to vitals, observations, and notes. Large language models (LLMs) contributed directly to dataset construction by (i) harmonizing noisy fields such as AVPU and breathing devices, (ii) prioritizing clinically relevant vitals and labs, and (iii) guiding schema alignment and efficient merging of disparate tables.   We further provide baseline models and SHAP-based interpretability analyses, illustrating predictive gaps between regimes and the features most critical for triage. Together, these contributions make triage prediction research more reproducible and accessible -- a step toward dataset democratization in clinical AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.26351v1),  [pdf](http://arxiv.org/pdf/2509.26351v1)

**Tags**: cs.LG 



### SoK: Systematic analysis of adversarial threats against deep learning   approaches for autonomous anomaly detection systems in SDN-IoT networks
**Authors**: Tharindu Lakshan Yasarathna, Nhien-An Le-Khac

**Updated**: 2025-09-30T14:54:42Z

**Summary**: Integrating SDN and the IoT enhances network control and flexibility. DL-based AAD systems improve security by enabling real-time threat detection in SDN-IoT networks. However, these systems remain vulnerable to adversarial attacks that manipulate input data or exploit model weaknesses, significantly degrading detection accuracy. Existing research lacks a systematic analysis of adversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT environments. This SoK study introduces a structured adversarial threat model and a comprehensive taxonomy of attacks, categorising them into data, model, and hybrid-level threats. Unlike previous studies, we systematically evaluate white, black, and grey-box attack strategies across popular benchmark datasets. Our findings reveal that adversarial attacks can reduce detection accuracy by up to 48.4%, with Membership Inference causing the most significant drop. C&W and DeepFool achieve high evasion success rates. However, adversarial training enhances robustness, and its high computational overhead limits the real-time deployment of SDN-IoT applications. We propose adaptive countermeasures, including real-time adversarial mitigation, enhanced retraining mechanisms, and explainable AI-driven security frameworks. By integrating structured threat models, this study offers a more comprehensive approach to attack categorisation, impact assessment, and defence evaluation than previous research. Our work highlights critical vulnerabilities in existing DL-based AAD models and provides practical recommendations for improving resilience, interpretability, and computational efficiency. This study serves as a foundational reference for researchers and practitioners seeking to enhance DL-based AAD security in SDN-IoT networks, offering a systematic adversarial threat model and conceptual defence evaluation based on prior empirical studies.

**Link**: [arxiv](http://arxiv.org/abs/2509.26350v1),  [pdf](http://arxiv.org/pdf/2509.26350v1)

**Tags**: cs.CR cs.AI 



### Structured Agent Distillation for Large Language Model
**Authors**: Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang

**Updated**: 2025-09-30T14:52:40Z

**Summary**: Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.

**Link**: [arxiv](http://arxiv.org/abs/2505.13820v2),  [pdf](http://arxiv.org/pdf/2505.13820v2)

**Tags**: cs.LG cs.AI cs.CL 



### SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate   Jailbreak Attacks in Large Language Models
**Authors**: Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, Kaizhu Huang

**Updated**: 2025-09-30T14:50:59Z

**Summary**: Large Language Models (LLMs) have achieved impressive performance across diverse natural language processing tasks, but their growing power also amplifies potential risks such as jailbreak attacks that circumvent built-in safety mechanisms. Existing defenses including input paraphrasing, multi step evaluation, and safety expert models often suffer from high computational costs, limited generalization, or rigid workflows that fail to detect subtle malicious intent embedded in complex contexts. Inspired by cognitive science findings on human decision making, we propose SafeBehavior, a novel hierarchical jailbreak defense mechanism that simulates the adaptive multistage reasoning process of humans. SafeBehavior decomposes safety evaluation into three stages: intention inference to detect obvious input risks, self introspection to assess generated responses and assign confidence based judgments, and self revision to adaptively rewrite uncertain outputs while preserving user intent and enforcing safety constraints. We extensively evaluate SafeBehavior against five representative jailbreak attack types including optimization based, contextual manipulation, and prompt based attacks and compare it with seven state of the art defense baselines. Experimental results show that SafeBehavior significantly improves robustness and adaptability across diverse threat scenarios, offering an efficient and human inspired approach to safeguarding LLMs against jailbreak attempts.

**Link**: [arxiv](http://arxiv.org/abs/2509.26345v1),  [pdf](http://arxiv.org/pdf/2509.26345v1)

**Tags**: cs.AI 



### Incentivizing Reasoning for Advanced Instruction-Following of Large   Language Models
**Authors**: Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun

**Updated**: 2025-09-30T14:50:34Z

**Summary**: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.   Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions

**Link**: [arxiv](http://arxiv.org/abs/2506.01413v8),  [pdf](http://arxiv.org/pdf/2506.01413v8)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### Memory-Driven Self-Improvement for Decision Making with Large Language   Models
**Authors**: Xue Yan, Zijing Ou, Mengyue Yang, Yan Song, Haifeng Zhang, Yingzhen Li, Jun Wang

**Updated**: 2025-09-30T14:46:06Z

**Summary**: Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40\% on in-distribution tasks and over 75\% when generalized to unseen tasks in ALFWorld.

**Link**: [arxiv](http://arxiv.org/abs/2509.26340v1),  [pdf](http://arxiv.org/pdf/2509.26340v1)

**Tags**: cs.LG 



### TrackCore-F: Deploying Transformer-Based Subatomic Particle Tracking on   FPGAs
**Authors**: Arjan Blankestijn, Uraz Odyurt, Amirreza Yousefzadeh

**Updated**: 2025-09-30T14:44:43Z

**Summary**: The Transformer Machine Learning (ML) architecture has been gaining considerable momentum in recent years. In particular, computational High-Energy Physics tasks such as jet tagging and particle track reconstruction (tracking), have either achieved proper solutions, or reached considerable milestones using Transformers. On the other hand, the use of specialised hardware accelerators, especially FPGAs, is an effective method to achieve online, or pseudo-online latencies. The development and integration of Transformer-based ML to FPGAs is still ongoing and the support from current tools is very limited to non-existent. Additionally, FPGA resources present a significant constraint. Considering the model size alone, while smaller models can be deployed directly, larger models are to be partitioned in a meaningful and ideally, automated way. We aim to develop methodologies and tools for monolithic, or partitioned Transformer synthesis, specifically targeting inference. Our primary use-case involves two machine learning model designs for tracking, derived from the TrackFormers project. We elaborate our development approach, present preliminary results, and provide comparisons.

**Link**: [arxiv](http://arxiv.org/abs/2509.26335v1),  [pdf](http://arxiv.org/pdf/2509.26335v1)

**Tags**: hep-ex cs.AR cs.LG 



### SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?
**Authors**: Michael Kirchhof, Luca F√ºger, Adam Goli≈Ñski, Eeshan Gunesh Dhekane, Arno Blaas, Seong Joon Oh, Sinead Williamson

**Updated**: 2025-09-30T14:44:21Z

**Summary**: The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables.

**Link**: [arxiv](http://arxiv.org/abs/2505.20295v3),  [pdf](http://arxiv.org/pdf/2505.20295v3)

**Tags**: cs.CL cs.AI cs.LG stat.ML 



### AI Playing Business Games: Benchmarking Large Language Models on   Managerial Decision-Making in Dynamic Simulations
**Authors**: Berdymyrat Ovezmyradov

**Updated**: 2025-09-30T14:43:05Z

**Summary**: The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2509.26331v1),  [pdf](http://arxiv.org/pdf/2509.26331v1)

**Tags**: cs.AI I.2.1 



## Keyword: LLM Deployment 
 ### Attention as a Compass: Efficient Exploration for Process-Supervised RL   in Reasoning Models
**Authors**: Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai

**Updated**: 2025-09-30T17:58:34Z

**Summary**: Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2509.26628v1),  [pdf](http://arxiv.org/pdf/2509.26628v1)

**Tags**: cs.LG cs.CL 



### The Impact of Language Mixing on Bilingual LLM Reasoning
**Authors**: Yihao Li, Jiayi Xin, Miranda Muqing Miao, Qi Long, Lyle Ungar

**Updated**: 2025-09-30T17:58:13Z

**Summary**: Proficient multilingual speakers often intentionally switch languages in the middle of a conversation. Similarly, recent reasoning-focused bilingual large language models (LLMs) with strong capabilities in both languages exhibit language mixing-alternating languages within their chain of thought. Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy, suggesting that language mixing may benefit reasoning. In this work, we study language switching in Chinese-English bilingual reasoning models. We identify reinforcement learning with verifiable rewards (RLVR) as the critical training stage that leads to language mixing. We show that language mixing can enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6 percentage points on MATH500. Additionally, a lightweight probe can be trained to predict whether a potential language switch would benefit or harm reasoning, and when used to guide decoding, increases accuracy by 2.92 percentage points. Our findings suggest that language mixing is not merely a byproduct of multilingual training, but is a strategic reasoning behavior.

**Link**: [arxiv](http://arxiv.org/abs/2507.15849v2),  [pdf](http://arxiv.org/pdf/2507.15849v2)

**Tags**: cs.CL cs.AI cs.LG 



### Recursive Self-Aggregation Unlocks Deep Thinking in Large Language   Models
**Authors**: Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain

**Updated**: 2025-09-30T17:58:03Z

**Summary**: Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA.

**Link**: [arxiv](http://arxiv.org/abs/2509.26626v1),  [pdf](http://arxiv.org/pdf/2509.26626v1)

**Tags**: cs.LG 



### Learning to See Before Seeing: Demystifying LLM Visual Priors from   Language Pre-training
**Authors**: Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos

**Updated**: 2025-09-30T17:57:44Z

**Summary**: Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM's latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26625v1),  [pdf](http://arxiv.org/pdf/2509.26625v1)

**Tags**: cs.LG cs.AI cs.CV cs.MM 



### Black-box Context-free Grammar Inference for Readable & Natural Grammars
**Authors**: Mohammad Rifat Arefin, Shanto Rahman, Christoph Csallner

**Updated**: 2025-09-30T17:54:25Z

**Summary**: Black-box context-free grammar inference is crucial for program analysis, reverse engineering, and security, yet existing tools such as Arvada, TreeVada, and Kedavra struggle with scalability, readability, and accuracy on large, complex languages. We present NatGI, a novel LLM-guided grammar inference framework that extends TreeVada's parse tree recovery with three key innovations: bracket-guided bubble exploration, LLM-driven bubble generation and non-terminal labeling, and hierarchical delta debugging (HDD) for systematic tree simplification. Bracket-guided exploration leverages syntactic cues such as parentheses to propose well-structured grammar fragments, while LLM guidance produces meaningful non-terminal names and selects more promising merges. Finally, HDD incrementally reduces unnecessary rules, which makes the grammars both compact and interpretable. In our experiments, we evaluate NatGI on a comprehensive benchmark suite ranging from small languages to larger ones such as lua, c, and mysql. Our results show that NatGI consistently outperforms strong baselines in terms of F1 score. On average, NatGI achieves an F1 score of 0.57, which is 25pp (percentage points) higher than the best-performing baseline, TreeVada. In the case of interpretability, our generated grammars perform significantly better than those produced by existing approaches. Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules with meaningful non-terminal names and compact structures that align more closely with human intuition. As a result, developers and researchers can achieve higher accuracy while still being able to easily inspect, verify, and reason about the structure and semantics of the induced grammars.

**Link**: [arxiv](http://arxiv.org/abs/2509.26616v1),  [pdf](http://arxiv.org/pdf/2509.26616v1)

**Tags**: cs.SE cs.FL cs.PL 68Q42, 68Q45 (Primary), 68T50 (Secondary) D.2.5; F.4.2 



### MENLO: From Preferences to Proficiency -- Evaluating and Modeling   Native-like Quality Across 47 Languages
**Authors**: Chenxi Whitehouse, Sebastian Ruder, Tony Lin, Oksana Kurylo, Haruka Takagi, Janice Lam, Nicol√≤ Busetto, Denise Diaz

**Updated**: 2025-09-30T17:48:58Z

**Summary**: Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.

**Link**: [arxiv](http://arxiv.org/abs/2509.26601v1),  [pdf](http://arxiv.org/pdf/2509.26601v1)

**Tags**: cs.CL cs.AI cs.LG 



### Deconstructing Self-Bias in LLM-generated Translation Benchmarks
**Authors**: Wenda Xu, Sweta Agrawal, Vil√©m Zouhar, Markus Freitag, Daniel Deutsch

**Updated**: 2025-09-30T17:48:35Z

**Summary**: As large language models (LLMs) begin to saturate existing benchmarks, automated benchmark creation using LLMs (LLM as a benchmark) has emerged as a scalable alternative to slow and costly human curation. While these generated test sets have to potential to cheaply rank models, we demonstrate a critical flaw. LLM generated benchmarks systematically favor the model that created the benchmark, they exhibit self bias on low resource languages to English translation tasks. We show three key findings on automatic benchmarking of LLMs for translation: First, this bias originates from two sources: the generated test data (LLM as a testset) and the evaluation method (LLM as an evaluator), with their combination amplifying the effect. Second, self bias in LLM as a benchmark is heavily influenced by the model's generation capabilities in the source language. For instance, we observe more pronounced bias in into English translation, where the model's generation system is developed, than in out of English translation tasks. Third, we observe that low diversity in source text is one attribution to self bias. Our results suggest that improving the diversity of these generated source texts can mitigate some of the observed self bias.

**Link**: [arxiv](http://arxiv.org/abs/2509.26600v1),  [pdf](http://arxiv.org/pdf/2509.26600v1)

**Tags**: cs.CL cs.AI 



### Are Robust LLM Fingerprints Adversarially Robust?
**Authors**: Anshul Nasery, Edoardo Contente, Alkin Kaz, Pramod Viswanath, Sewoong Oh

**Updated**: 2025-09-30T17:47:09Z

**Summary**: Model fingerprinting has emerged as a promising paradigm for claiming model ownership. However, robustness evaluations of these schemes have mostly focused on benign perturbations such as incremental fine-tuning, model merging, and prompting. Lack of systematic investigations into {\em adversarial robustness} against a malicious model host leaves current systems vulnerable. To bridge this gap, we first define a concrete, practical threat model against model fingerprinting. We then take a critical look at existing model fingerprinting schemes to identify their fundamental vulnerabilities. Based on these, we develop adaptive adversarial attacks tailored for each vulnerability, and demonstrate that these can bypass model authentication completely for ten recently proposed fingerprinting schemes while maintaining high utility of the model for the end users. Our work encourages fingerprint designers to adopt adversarial robustness by design. We end with recommendations for future fingerprinting methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.26598v1),  [pdf](http://arxiv.org/pdf/2509.26598v1)

**Tags**: cs.CR cs.AI cs.LG 



### Exploring Large Language Model as an Interactive Sports Coach: Lessons   from a Single-Subject Half Marathon Preparation
**Authors**: Kichang Lee

**Updated**: 2025-09-30T17:46:39Z

**Summary**: Large language models (LLMs) are emerging as everyday assistants, but their role as longitudinal virtual coaches is underexplored. This two-month single subject case study documents LLM guided half marathon preparation (July-September 2025). Using text based interactions and consumer app logs, the LLM acted as planner, explainer, and occasional motivator. Performance improved from sustaining 2 km at 7min 54sec per km to completing 21.1 km at 6min 30sec per km, with gains in cadence, pace HR coupling, and efficiency index trends. While causal attribution is limited without a control, outcomes demonstrate safe, measurable progress. At the same time, gaps were evident, no realtime sensor integration, text only feedback, motivation support that was user initiated, and limited personalization or safety guardrails. We propose design requirements for next generation systems, persistent athlete models with explicit guardrails, multimodal on device sensing, audio, haptic, visual feedback, proactive motivation scaffolds, and privacy-preserving personalization. This study offers grounded evidence and a design agenda for evolving LLMs from retrospective advisors to closed-loop coaching companions.

**Link**: [arxiv](http://arxiv.org/abs/2509.26593v1),  [pdf](http://arxiv.org/pdf/2509.26593v1)

**Tags**: cs.HC H.4, K.7 



### Generating Difficult-to-Translate Texts
**Authors**: Vil√©m Zouhar, Wenda Xu, Parker Riley, Juraj Juraska, Mara Finkelstein, Markus Freitag, Dan Deutsch

**Updated**: 2025-09-30T17:46:08Z

**Summary**: Machine translation benchmarks sourced from the real world are quickly obsoleted, due to most examples being easy for state-of-the-art translation models. This limits the benchmark's ability to distinguish which model is better or to reveal models' weaknesses. Current methods for creating difficult test cases, such as subsampling or from-scratch synthesis, either fall short of identifying difficult examples or suffer from a lack of diversity and naturalness. Inspired by the iterative process of human experts probing for model failures, we propose MT-breaker, a method where a large language model iteratively refines a source text to increase its translation difficulty. The LLM iteratively queries a target machine translation model to guide its generation of difficult examples. Our approach generates examples that are more challenging for the target MT model while preserving the diversity of natural texts. While the examples are tailored to a particular machine translation model during the generation, the difficulty also transfers to other models and languages.

**Link**: [arxiv](http://arxiv.org/abs/2509.26592v1),  [pdf](http://arxiv.org/pdf/2509.26592v1)

**Tags**: cs.CL 



### MaskSQL: Safeguarding Privacy for LLM-Based Text-to-SQL via Abstraction
**Authors**: Sepideh Abedini, Shubhankar Mohapatra, D. B. Emerson, Masoumeh Shafieinejad, Jesse C. Cresswell, Xi He

**Updated**: 2025-09-30T17:43:21Z

**Summary**: Large language models (LLMs) have shown promising performance on tasks that require reasoning, such as text-to-SQL, code generation, and debugging. However, regulatory frameworks with strict privacy requirements constrain their integration into sensitive systems. State-of-the-art LLMs are also proprietary, costly, and resource-intensive, making local deployment impractical. Consequently, utilizing such LLMs often requires sharing data with third-party providers, raising privacy concerns and risking noncompliance with regulations. Although fine-tuned small language models (SLMs) can outperform LLMs on certain tasks and be deployed locally to mitigate privacy concerns, they underperform on more complex tasks such as text-to-SQL translation. In this work, we introduce MaskSQL, a text-to-SQL framework that utilizes abstraction as a privacy protection mechanism to mask sensitive information in LLM prompts. Unlike redaction, which removes content entirely, or generalization, which broadens tokens, abstraction retains essential information while discarding unnecessary details, striking an effective privacy-utility balance for the text-to-SQL task. Moreover, by providing mechanisms to control the privacy-utility tradeoff, MaskSQL facilitates adoption across a broader range of use cases. Our experimental results show that MaskSQL outperforms leading SLM-based text-to-SQL models and achieves performance approaching state-of-the-art LLM-based models, while preserving privacy.

**Link**: [arxiv](http://arxiv.org/abs/2509.23459v2),  [pdf](http://arxiv.org/pdf/2509.23459v2)

**Tags**: cs.CR cs.CL 



### Fairness Testing in Retrieval-Augmented Generation: How Small   Perturbations Reveal Bias in Small Language Models
**Authors**: Matheus Vinicius da Silva de Oliveira, Jonathan de Andrade Silva, Awdren de Lima Fontao

**Updated**: 2025-09-30T17:42:35Z

**Summary**: Large Language Models (LLMs) are widely used across multiple domains but continue to raise concerns regarding security and fairness. Beyond known attack vectors such as data poisoning and prompt injection, LLMs are also vulnerable to fairness bugs. These refer to unintended behaviors influenced by sensitive demographic cues (e.g., race or sexual orientation) that should not affect outcomes. Another key issue is hallucination, where models generate plausible yet false information. Retrieval-Augmented Generation (RAG) has emerged as a strategy to mitigate hallucinations by combining external retrieval with text generation. However, its adoption raises new fairness concerns, as the retrieved content itself may surface or amplify bias. This study conducts fairness testing through metamorphic testing (MT), introducing controlled demographic perturbations in prompts to assess fairness in sentiment analysis performed by three Small Language Models (SLMs) hosted on HuggingFace (Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B), each integrated into a RAG pipeline. Results show that minor demographic variations can break up to one third of metamorphic relations (MRs). A detailed analysis of these failures reveals a consistent bias hierarchy, with perturbations involving racial cues being the predominant cause of the violations. In addition to offering a comparative evaluation, this work reinforces that the retrieval component in RAG must be carefully curated to prevent bias amplification. The findings serve as a practical alert for developers, testers and small organizations aiming to adopt accessible SLMs without compromising fairness or reliability.

**Link**: [arxiv](http://arxiv.org/abs/2509.26584v1),  [pdf](http://arxiv.org/pdf/2509.26584v1)

**Tags**: cs.AI cs.IR cs.LG cs.SE 



### Linking Process to Outcome: Conditional Reward Modeling for LLM   Reasoning
**Authors**: Zheng Zhang, Ziwei Shan, Kaitao Song, Yexin Li, Kan Ren

**Updated**: 2025-09-30T17:38:45Z

**Summary**: Process Reward Models (PRMs) have emerged as a promising approach to enhance the reasoning capabilities of large language models (LLMs) by guiding their step-by-step reasoning toward a final answer. However, existing PRMs either treat each reasoning step in isolation, failing to capture inter-step dependencies, or struggle to align process rewards with the final outcome. Consequently, the reward signal fails to respect temporal causality in sequential reasoning and faces ambiguous credit assignment. These limitations make downstream models vulnerable to reward hacking and lead to suboptimal performance. In this work, we propose Conditional Reward Modeling (CRM) that frames LLM reasoning as a temporal process leading to a correct answer. The reward of each reasoning step is not only conditioned on the preceding steps but also explicitly linked to the final outcome of the reasoning trajectory. By enforcing conditional probability rules, our design captures the causal relationships among reasoning steps, with the link to the outcome allowing precise attribution of each intermediate step, thereby resolving credit assignment ambiguity. Further, through this consistent probabilistic modeling, the rewards produced by CRM enable more reliable cross-sample comparison. Experiments across Best-of-N sampling, beam search and reinforcement learning demonstrate that CRM consistently outperforms existing reward models, offering a principled framework for enhancing LLM reasoning. In particular, CRM is more robust to reward hacking and delivers stable downstream improvements without relying on verifiable rewards derived from ground truth.

**Link**: [arxiv](http://arxiv.org/abs/2509.26578v1),  [pdf](http://arxiv.org/pdf/2509.26578v1)

**Tags**: cs.LG 



### Probing the Critical Point (CritPt) of AI Reasoning: a Frontier Physics   Research Benchmark
**Authors**: Minhui Zhu, Minyang Tian, Xiaocheng Yang, Tianci Zhou, Penghao Zhu, Eli Chertkov, Shengyan Liu, Yufeng Du, Lifan Yuan, Ziming Ji, Indranil Das, Junyi Cao, Yufeng Du, Jinchen He, Yifan Su, Jiabin Yu, Yikun Jiang, Yujie Zhang, Chang Liu, Ze-Min Huang, Weizhen Jia, Xinan Chen, Peixue Wu, Yunkai Wang, Juntai Zhou, Yong Zhao, Farshid Jafarpour, Jessie Shelton, Aaron Young, John Bartolotta, Wenchao Xu, Yue Sun, Anjun Chu, Victor Colussi, Chris Akers, Nathan Brooks, Wenbo Fu, Christopher Wilson, Jinchao Zhao, Marvin Qi, Anqi Mu, Yubo Yang, Allen Zang, Yang Lyu, Peizhi Mai, Xuefei Guo, Luyu Gao, Ze Yang, Chi Xue, Dmytro Bandak, Ya√Ør Hein, Yonatan Kahn, Kevin Zhou, John Drew Wilson, Jarrod T. Reilly, Di Luo, Daniel Inafuku, Hao Tong, Liang Yang, Ruixing Zhang, Xueying Wang, Ofir Press, Nicolas Chia, Eliu Huerta, Hao Peng

**Updated**: 2025-10-01T02:12:55Z

**Summary**: While large language models (LLMs) with reasoning capabilities are progressing rapidly on high-school math competitions and coding, can they reason effectively through complex, open-ended challenges found in frontier physics research? And crucially, what kinds of reasoning tasks do physicists want LLMs to assist with? To address these questions, we present the CritPt (Complex Research using Integrated Thinking - Physics Test, pronounced "critical point"), the first benchmark designed to test LLMs on unpublished, research-level reasoning tasks that broadly covers modern physics research areas, including condensed matter, quantum physics, atomic, molecular & optical physics, astrophysics, high energy physics, mathematical physics, statistical physics, nuclear physics, nonlinear dynamics, fluid dynamics and biophysics. CritPt consists of 71 composite research challenges designed to simulate full-scale research projects at the entry level, which are also decomposed to 190 simpler checkpoint tasks for more fine-grained insights. All problems are newly created by 50+ active physics researchers based on their own research. Every problem is hand-curated to admit a guess-resistant and machine-verifiable answer and is evaluated by an automated grading pipeline heavily customized for advanced physics-specific output formats. We find that while current state-of-the-art LLMs show early promise on isolated checkpoints, they remain far from being able to reliably solve full research-scale challenges: the best average accuracy among base models is only 4.0% , achieved by GPT-5 (high), moderately rising to around 10% when equipped with coding tools. Through the realistic yet standardized evaluation offered by CritPt, we highlight a large disconnect between current model capabilities and realistic physics research demands, offering a foundation to guide the development of scientifically grounded AI tools.

**Link**: [arxiv](http://arxiv.org/abs/2509.26574v2),  [pdf](http://arxiv.org/pdf/2509.26574v2)

**Tags**: cs.AI cond-mat.other cs.CL hep-th quant-ph 



### Electrical Readout of Spin Environments in Diamond for Quantum Sensing
**Authors**: Olga Rubinas, Michael Petrov, Emilie Bourgeois, Jaroslav Hruby, Akhil Kuriakose, Ottavia Jedrkiewicz, Milos Nesladek

**Updated**: 2025-09-30T17:33:03Z

**Summary**: Nitrogen-vacancy (NV) centres in diamond are a key platform for quantum sensing and quantum information, combining long coherence times with controllable spin-spin interactions. Most of current quantum algorithms rely on optical access, which limit device integration and applicability in opaque or miniaturized settings. Here we demonstrate an all-electrical approach, photocurrent double electron-electron resonance (PC-DEER), permitting exploiting local dipolar interactions between individual NV spin qubits or ensembles and nearby paramagnetic defects with sub-confocal resolution. PC-DEER extends photocurrent NV readout from single-spin to spin-bath control and coherent manipulation, enabling characterization of bath-induced noise and effective deployment of noise-reduction protocols. We resolve the signatures of substitutional nitrogen (P1) and NVH centers with reproducible contrast by using electrical signals. Our results establish a scalable, optical-free spin readout strategy that bridges fundamental studies of spin environments with deployable quantum technologies, advancing the integration of diamond-based sensors into solid-state quantum devices.

**Link**: [arxiv](http://arxiv.org/abs/2509.26570v1),  [pdf](http://arxiv.org/pdf/2509.26570v1)

**Tags**: quant-ph physics.atom-ph 



### AuDeRe: Automated Strategy Decision and Realization in Robot Planning   and Control via LLMs
**Authors**: Yue Meng, Fei Chen, Yongchao Chen, Chuchu Fan

**Updated**: 2025-09-30T17:27:42Z

**Summary**: Recent advancements in large language models (LLMs) have shown significant promise in various domains, especially robotics. However, most prior LLM-based work in robotic applications either directly predicts waypoints or applies LLMs within fixed tool integration frameworks, offering limited flexibility in exploring and configuring solutions best suited to different tasks. In this work, we propose a framework that leverages LLMs to select appropriate planning and control strategies based on task descriptions, environmental constraints, and system dynamics. These strategies are then executed by calling the available comprehensive planning and control APIs. Our approach employs iterative LLM-based reasoning with performance feedback to refine the algorithm selection. We validate our approach through extensive experiments across tasks of varying complexity, from simple tracking to complex planning scenarios involving spatiotemporal constraints. The results demonstrate that using LLMs to determine planning and control strategies from natural language descriptions significantly enhances robotic autonomy while reducing the need for extensive manual tuning and expert knowledge. Furthermore, our framework maintains generalizability across different tasks and notably outperforms baseline methods that rely on LLMs for direct trajectory, control sequence, or code generation.

**Link**: [arxiv](http://arxiv.org/abs/2504.03015v2),  [pdf](http://arxiv.org/pdf/2504.03015v2)

**Tags**: cs.RO 



### VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use
**Authors**: Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen

**Updated**: 2025-09-30T17:22:37Z

**Summary**: Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.

**Link**: [arxiv](http://arxiv.org/abs/2509.01055v2),  [pdf](http://arxiv.org/pdf/2509.01055v2)

**Tags**: cs.AI cs.CL cs.CV 



### AutoJudge: Judge Decoding Without Manual Annotation
**Authors**: Roman Garipov, Fedor Velikonivtsev, Ivan Ermakov, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin

**Updated**: 2025-09-30T17:21:23Z

**Summary**: We introduce AutoJudge, a method that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the response, relaxing the distribution match guarantee so that the "unimportant" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft models should be corrected to preserve quality and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We evaluate the effectiveness of AutoJudge with multiple draft/target model pairs on mathematical reasoning and programming benchmarks, achieving significant speedups at the cost of a minor accuracy reduction. Notably, on GSM8k with the Llama 3.1 70B target model, our approach achieves up to $\approx2\times$ speedup over speculative decoding at the cost of $\le 1\%$ drop in accuracy. When applied to the LiveCodeBench benchmark, AutoJudge automatically detects programming-specific important tokens, accepting $\ge 25$ tokens per speculation cycle at $2\%$ drop in Pass@1. Our approach requires no human annotation and is easy to integrate with modern LLM inference frameworks.

**Link**: [arxiv](http://arxiv.org/abs/2504.20039v2),  [pdf](http://arxiv.org/pdf/2504.20039v2)

**Tags**: cs.CL cs.LG 



### Towards Reliable Benchmarking: A Contamination Free, Controllable   Evaluation Framework for Multi-step LLM Function Calling
**Authors**: Seiji Maekawa, Jackson Hassell, Pouya Pezeshkpour, Tom Mitchell, Estevam Hruschka

**Updated**: 2025-09-30T17:21:17Z

**Summary**: As language models gain access to external tools via structured function calls, they become increasingly more capable of solving complex, multi-step tasks. However, existing benchmarks for tool-augmented language models (TaLMs) provide insufficient control over factors such as the number of functions accessible, task complexity, and input size, and remain vulnerable to data contamination. We present FuncBenchGen, a unified, contamination-free framework that evaluates TaLMs by generating synthetic multi-step tool-use tasks. The key idea is to cast tool use as traversal over a hidden function-dependency DAG where nodes are function calls and an edge between nodes represents one function consuming the output of another. Given a set of external function schemas, initial variable values, and a target variable, models must compose the correct call sequence to compute the target variable. FuncBenchGen allows users to precisely control task difficulty (e.g., graph size, dependency depth, and distractor functions) while avoiding data leakage. We apply our FuncBenchGen framework to evaluate seven LLMs on tool use tasks of varying difficulty. Reasoning-optimized models consistently outperform general-purpose models with GPT-5 significantly outperforming other models. Performance declines sharply as dependency depth increases. Furthermore, connected irrelevant functions prove especially difficult to handle. We find that strong models often make syntactically valid function calls but propagate incorrect or stale argument values across steps, revealing brittle state tracking by LLMs in multi-turn tool use. Motivated by this observation, we introduce a simple mitigation strategy that explicitly restates prior variable values to the agent at each step. Surprisingly, this lightweight change yields substantial gains across models. e.g., yielding a success rate improvement from 62.5% to 81.3% for GPT-5.

**Link**: [arxiv](http://arxiv.org/abs/2509.26553v1),  [pdf](http://arxiv.org/pdf/2509.26553v1)

**Tags**: cs.CL 



### Can LLMs Write Mathematics Papers? A Case Study in Reservoir Computing
**Authors**: Allen G Hart

**Updated**: 2025-09-30T17:19:23Z

**Summary**: As AI capabilities continue to grow exponentially on economically relevant human expert tasks, with task completion horizons doubling every 7 months according to the Model Evaluation and Threat Research (METR), we are interested in how this applies to the task of mathematics research. To explore this, we evaluated the capability of four frontier large language models (LLMs), ChatGPT 5, Claude 4.1 Opus, Gemini 2.5 Pro, and Grok 4, at the task of creating a mini-paper on reservoir computing. All models produced engaging papers with some apparent understanding of various techniques, but were sometimes lead to mistakes by surface level understanding of key ideas. That said, the capabilities on LLMs on this task was likely as good or greater than that predicted by METR.

**Link**: [arxiv](http://arxiv.org/abs/2509.26550v1),  [pdf](http://arxiv.org/pdf/2509.26550v1)

**Tags**: math.DS 



### Scalable Fingerprinting of Large Language Models
**Authors**: Anshul Nasery, Jonathan Hayase, Creston Brooks, Peiyao Sheng, Himanshu Tyagi, Pramod Viswanath, Sewoong Oh

**Updated**: 2025-09-30T17:18:39Z

**Summary**: Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. However, to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that {\em scalability} is critical, i.e., scaling up the number of fingerprints one can embed into a model. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. We experiment with fingerprint design at a scale significantly larger than previously considered, and introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model -- two orders of magnitude more than existing schemes -- without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks. Our code is available at https://github.com/SewoongLab/scalable-fingerprinting-of-llms

**Link**: [arxiv](http://arxiv.org/abs/2502.07760v2),  [pdf](http://arxiv.org/pdf/2502.07760v2)

**Tags**: cs.CR cs.LG 



### Towards Verified Code Reasoning by LLMs
**Authors**: Meghana Sistla, Gogul Balakrishnan, Pat Rondon, Jos√© Cambronero, Michele Tufano, Satish Chandra

**Updated**: 2025-09-30T17:17:51Z

**Summary**: While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature).   As a result of this lack of trustworthiness, the agent's answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agent's response and, subsequently, using formal verification and program analysis tools to verify the agent's reasoning steps.   We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agent's reasoning on 13/20 examples, and for the program equivalence queries, the formal verification step successfully caught 6/8 incorrect judgments made by the agent.

**Link**: [arxiv](http://arxiv.org/abs/2509.26546v1),  [pdf](http://arxiv.org/pdf/2509.26546v1)

**Tags**: cs.SE cs.LG 



### TASP: Topology-aware Sequence Parallelism
**Authors**: Yida Wang, Ke Hong, Xiuhong Li, Yuanchao Xu, Wenxun Wang, Guohao Dai, Yu Wang

**Updated**: 2025-09-30T17:15:27Z

**Summary**: Long-context large language models (LLMs) face constraints due to the quadratic complexity of the self-attention mechanism. The mainstream sequence parallelism (SP) method, Ring Attention, attempts to solve this by distributing the query into multiple query chunks across accelerators and enable each Q tensor to access all KV tensors from other accelerators via the Ring AllGather communication primitive. However, it exhibits low communication efficiency, restricting its practical applicability. This inefficiency stems from the mismatch between the Ring AllGather communication primitive it adopts and the AlltoAll topology of modern accelerators. A Ring AllGather primitive is composed of iterations of ring-styled data transfer, which can only utilize a very limited fraction of an AlltoAll topology.   Inspired by the Hamiltonian decomposition of complete directed graphs, we identify that modern accelerator topology can be decomposed into multiple orthogonal ring datapaths which can concurrently transfer data without interference. Based on this, we further observe that the Ring AllGather primitive can also be decomposed into the same number of concurrent ring-styled data transfer at every iteration. Based on these insights, we propose TASP, a topology-aware SP method for long-context LLMs that fully utilizes the communication capacity of modern accelerators via topology decomposition and primitive decomposition. Experimental results on both single-node and multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate that TASP achieves higher communication efficiency than Ring Attention on these modern accelerator topologies and achieves up to 3.58 speedup than Ring Attention and its variant Zigzag-Ring Attention. The code is available at https://github.com/infinigence/HamiltonAttention.

**Link**: [arxiv](http://arxiv.org/abs/2509.26541v1),  [pdf](http://arxiv.org/pdf/2509.26541v1)

**Tags**: cs.LG cs.DC 



### OceanGym: A Benchmark Environment for Underwater Embodied Agents
**Authors**: Yida Xue, Mingjun Mao, Xiangyuan Ru, Yuqi Zhu, Baochang Ren, Shuofei Qiao, Mengru Wang, Shumin Deng, Xinyu An, Ningyu Zhang, Ying Chen, Huajun Chen

**Updated**: 2025-09-30T17:09:32Z

**Summary**: We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth's last unexplored frontiers. The code and data are available at https://github.com/OceanGPT/OceanGym.

**Link**: [arxiv](http://arxiv.org/abs/2509.26536v1),  [pdf](http://arxiv.org/pdf/2509.26536v1)

**Tags**: cs.CL cs.AI cs.CV cs.LG cs.RO 



### MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal   Reasoning
**Authors**: Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, Yangqiu Song

**Updated**: 2025-09-30T17:09:29Z

**Summary**: Multi-agent systems (MAS), leveraging the remarkable capabilities of Large Language Models (LLMs), show great potential in addressing complex tasks. In this context, integrating MAS with legal tasks is a crucial step. While previous studies have developed legal benchmarks for LLM agents, none are specifically designed to consider the unique advantages of MAS, such as task decomposition, agent specialization, and flexible training. In fact, the lack of evaluation methods limits the potential of MAS in the legal domain. To address this gap, we propose MASLegalBench, a legal benchmark tailored for MAS and designed with a deductive reasoning approach. Our benchmark uses GDPR as the application scenario, encompassing extensive background knowledge and covering complex reasoning processes that effectively reflect the intricacies of real-world legal situations. Furthermore, we manually design various role-based MAS and conduct extensive experiments using different state-of-the-art LLMs. Our results highlight the strengths, limitations, and potential areas for improvement of existing models and MAS architectures.

**Link**: [arxiv](http://arxiv.org/abs/2509.24922v2),  [pdf](http://arxiv.org/pdf/2509.24922v2)

**Tags**: cs.AI cs.CL 



### Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework
**Authors**: Jovan Stojkovic, Chaojie Zhang, √ç√±igo Goiri, Ricardo Bianchini

**Updated**: 2025-09-30T17:08:51Z

**Summary**: The rapid rise of large language models (LLMs) has been driving an enormous demand for AI inference infrastructure, mainly powered by high-end GPUs. While these accelerators offer immense computational power, they incur high capital and operational costs due to frequent upgrades, dense power consumption, and cooling demands, making total cost of ownership (TCO) for AI datacenters a critical concern for cloud providers. Unfortunately, traditional datacenter lifecycle management (designed for general-purpose workloads) struggles to keep pace with AI's fast-evolving models, rising resource needs, and diverse hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme across three stages: building, hardware refresh, and operation. We show how design choices in power, cooling, and networking provisioning impact long-term TCO. We also explore refresh strategies aligned with hardware trends. Finally, we use operation software optimizations to reduce cost. While these optimizations at each stage yield benefits, unlocking the full potential requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle management framework that coordinates and co-optimizes decisions across all three stages, accounting for workload dynamics, hardware evolution, and system aging. Our system reduces the TCO by up to 40\% over traditional approaches. Using our framework we provide guidelines on how to manage AI datacenter lifecycle for the future.

**Link**: [arxiv](http://arxiv.org/abs/2509.26534v1),  [pdf](http://arxiv.org/pdf/2509.26534v1)

**Tags**: cs.AI cs.AR cs.DC 



### CSnake: Detecting Self-Sustaining Cascading Failure via Causal Stitching   of Fault Propagations
**Authors**: Shangshu Qian, Lin Tan, Yongle Zhang

**Updated**: 2025-09-30T17:04:31Z

**Summary**: Recent studies have revealed that self-sustaining cascading failures in distributed systems frequently lead to widespread outages, which are challenging to contain and recover from. Existing failure detection techniques struggle to expose such failures prior to deployment, as they typically require a complex combination of specific conditions to be triggered. This challenge stems from the inherent nature of cascading failures, as they typically involve a sequence of fault propagations, each activated by distinct conditions.   This paper presents CSnake, a fault injection framework to expose self-sustaining cascading failures in distributed systems. CSnake uses the novel idea of causal stitching, which causally links multiple single-fault injections in different tests to simulate complex fault propagation chains. To identify these chains, CSnake designs a counterfactual causality analysis of fault propagations - fault causality analysis (FCA): FCA compares the execution trace of a fault injection run with its corresponding profile run (i.e., same test w/o the injection) and identifies any additional faults triggered, which are considered to have a causal relationship with the injected fault.   To address the large search space of fault and workload combinations, CSnake employs a three-phase allocation protocol of test budget that prioritizes faults with unique and diverse causal consequences, increasing the likelihood of uncovering conditional fault propagations. Furthermore, to avoid incorrectly connecting fault propagations from workloads with incompatible conditions, CSnake performs a local compatibility check that approximately checks the compatibility of the path constraints associated with connected fault propagations with low overhead.   CSnake detected 15 bugs that cause self-sustaining cascading failures in five systems, five of which have been confirmed with two fixed.

**Link**: [arxiv](http://arxiv.org/abs/2509.26529v1),  [pdf](http://arxiv.org/pdf/2509.26529v1)

**Tags**: cs.DC cs.SE 



### Training Matryoshka Mixture-of-Experts for Elastic Inference-Time Expert   Utilization
**Authors**: Yaoxiang Wang, Qingguo Hu, Yucheng Ding, Ruizhe Wang, Yeyun Gong, Jian Jiao, Yelong Shen, Peng Cheng, Jinsong Su

**Updated**: 2025-09-30T16:56:44Z

**Summary**: Mixture-of-Experts (MoE) has emerged as a promising paradigm for efficiently scaling large language models without a proportional increase in computational cost. However, the standard training strategy of Top-K router prevents MoE models from realizing their full potential for elastic inference. When the number of activated experts is altered at inference time, these models exhibit precipitous performance degradation. In this work, we introduce Matryoshka MoE (M-MoE), a training framework that instills a coarse-to-fine structure directly into the expert ensemble. By systematically varying the number of activated experts during training, M-MoE compels the model to learn a meaningful ranking: top-ranked experts collaborate to provide essential, coarse-grained capabilities, while subsequent experts add progressively finer-grained detail. We explore this principle at multiple granularities, identifying a layer-wise randomization strategy as the most effective. Our experiments demonstrate that a single M-MoE model achieves remarkable elasticity, with its performance at various expert counts closely matching that of an entire suite of specialist models, but at only a fraction of the total training cost. This flexibility not only unlocks elastic inference but also enables optimizing performance by allocating different computational budgets to different model layers. Our work paves the way for more practical and adaptable deployments of large-scale MoE models.

**Link**: [arxiv](http://arxiv.org/abs/2509.26520v1),  [pdf](http://arxiv.org/pdf/2509.26520v1)

**Tags**: cs.CL 



### Design and deployment of a fast neural network for measuring the   properties of muons originating from displaced vertices in the CMS Endcap   Muon Track Finder
**Authors**: Efe Yigitbasi

**Updated**: 2025-09-30T16:56:16Z

**Summary**: We report on the development, implementation, and performance of a fast neural network used to measure the transverse momentum in the CMS Level-1 Endcap Muon Track Finder. The network aims to improve the triggering efficiency of muons produced in the decays of long-lived particles (LLPs). We implemented it in firmware for a Xilinx Virtex-7 FPGA and deployed it during the LHC Run 3 data-taking in 2023. The new displaced muon triggers that use this algorithm broaden the phase space accessible to the CMS experiment for searches that look for evidence of LLPs that decay into muons.

**Link**: [arxiv](http://arxiv.org/abs/2509.21062v2),  [pdf](http://arxiv.org/pdf/2509.21062v2)

**Tags**: hep-ex 



### BatonVoice: An Operationalist Framework for Enhancing Controllable   Speech Synthesis with Linguistic Intelligence from LLMs
**Authors**: Yue Wang, Ruotian Ma, Xingyu Chen, Zhengliang Shi, Wanshun Chen, Huang Liu, Jiadi Yao, Qu Yang, Qingxuan Jiang, Fanghua Ye, Juntao Li, Min Zhang, Zhaopeng Tu, Xiaolong Li, Linus

**Updated**: 2025-09-30T16:52:14Z

**Summary**: The rise of Large Language Models (LLMs) is reshaping multimodel models, with speech synthesis being a prominent application. However, existing approaches often underutilize the linguistic intelligence of these models, typically failing to leverage their powerful instruction-following capabilities. This limitation hinders the model's ability to follow text instructions for controllable Text-to-Speech~(TTS). To address this, we propose a new paradigm inspired by ``operationalism'' that decouples instruction understanding from speech generation. We introduce BatonVoice, a framework where an LLM acts as a ``conductor'', understanding user instructions and generating a textual ``plan'' -- explicit vocal features (e.g., pitch, energy). A separate TTS model, the ``orchestra'', then generates the speech from these features. To realize this component, we develop BatonTTS, a TTS model trained specifically for this task. Our experiments demonstrate that BatonVoice achieves strong performance in controllable and emotional speech synthesis, outperforming strong open- and closed-source baselines. Notably, our approach enables remarkable zero-shot cross-lingual generalization, accurately applying feature control abilities to languages unseen during post-training. This demonstrates that objectifying speech into textual vocal features can more effectively unlock the linguistic intelligence of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26514v1),  [pdf](http://arxiv.org/pdf/2509.26514v1)

**Tags**: cs.CL 



### Revealing the Power of Post-Training for Small Language Models via   Knowledge Distillation
**Authors**: Miao Rang, Zhenni Bi, Hang Zhou, Hanting Chen, An Xiao, Tianyu Guo, Kai Han, Xinghao Chen, Yunhe Wang

**Updated**: 2025-09-30T16:40:55Z

**Summary**: The rapid advancement of large language models (LLMs) has significantly advanced the capabilities of artificial intelligence across various domains. However, their massive scale and high computational costs render them unsuitable for direct deployment in resource-constrained edge environments. This creates a critical need for high-performance small models that can operate efficiently at the edge. Yet, after pre-training alone, these smaller models often fail to meet the performance requirements of complex tasks. To bridge this gap, we introduce a systematic post-training pipeline that efficiently enhances small model accuracy. Our post training pipeline consists of curriculum-based supervised fine-tuning (SFT) and offline on-policy knowledge distillation. The resulting instruction-tuned model achieves state-of-the-art performance among billion-parameter models, demonstrating strong generalization under strict hardware constraints while maintaining competitive accuracy across a variety of tasks. This work provides a practical and efficient solution for developing high-performance language models on Ascend edge devices.

**Link**: [arxiv](http://arxiv.org/abs/2509.26497v1),  [pdf](http://arxiv.org/pdf/2509.26497v1)

**Tags**: cs.CV 



### GIM: Improved Interpretability for Large Language Models
**Authors**: Joakim Edin, R√≥bert Csord√°s, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Casper L. Christensen, Jing Huang, Lars Maal√∏e

**Updated**: 2025-10-01T07:18:47Z

**Summary**: Ensuring faithful interpretability in large language models is imperative for trustworthy and reliable AI. A key obstacle is self-repair, a phenomenon where networks compensate for reduced signal in one component by amplifying others, masking the true importance of the ablated component. While prior work attributes self-repair to layer normalization and back-up components that compensate for ablated components, we identify a novel form occurring within the attention mechanism, where softmax redistribution conceals the influence of important attention scores. This leads traditional ablation and gradient-based methods to underestimate the significance of all components contributing to these attention scores. We introduce Gradient Interaction Modifications (GIM), a technique that accounts for self-repair during backpropagation. Extensive experiments across multiple large language models (Gemma 2B/9B, LLAMA 1B/3B/8B, Qwen 1.5B/3B) and diverse tasks demonstrate that GIM significantly improves faithfulness over existing circuit identification and feature attribution methods. Our work is a significant step toward better understanding the inner mechanisms of LLMs, which is crucial for improving them and ensuring their safety. Our code is available at https://github.com/JoakimEdin/gim.

**Link**: [arxiv](http://arxiv.org/abs/2505.17630v3),  [pdf](http://arxiv.org/pdf/2505.17630v3)

**Tags**: cs.CL cs.LG 68T07 I.2.0; I.2.7 



### OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost   Always!
**Authors**: Jingdi Lei, Varun Gumma, Rishabh Bhardwaj, Seok Min Lim, Chuan Li, Amir Zadeh, Soujanya Poria

**Updated**: 2025-09-30T16:39:17Z

**Summary**: Large Language Model (LLM) safety is one of the most pressing challenges for enabling wide-scale deployment. While most studies and global discussions focus on generic harms, such as models assisting users in harming themselves or others, enterprises face a more fundamental concern: whether LLM-based agents are safe for their intended use case. To address this, we introduce operational safety, defined as an LLM's ability to appropriately accept or refuse user queries when tasked with a specific purpose. We further propose OffTopicEval, an evaluation suite and benchmark for measuring operational safety both in general and within specific agentic use cases. Our evaluations on six model families comprising 20 open-weight LLMs reveal that while performance varies across models, all of them remain highly operationally unsafe. Even the strongest models -- Qwen-3 (235B) with 77.77\% and Mistral (24B) with 79.96\% -- fall far short of reliable operational safety, while GPT models plateau in the 62--73\% range, Phi achieves only mid-level scores (48--70\%), and Gemma and Llama-3 collapse to 39.53\% and 23.84\%, respectively. While operational safety is a core model alignment issue, to suppress these failures, we propose prompt-based steering methods: query grounding (Q-ground) and system-prompt grounding (P-ground), which substantially improve OOD refusal. Q-ground provides consistent gains of up to 23\%, while P-ground delivers even larger boosts, raising Llama-3.3 (70B) by 41\% and Qwen-3 (30B) by 27\%. These results highlight both the urgent need for operational safety interventions and the promise of prompt-based steering as a first step toward more reliable LLM-based agents.

**Link**: [arxiv](http://arxiv.org/abs/2509.26495v1),  [pdf](http://arxiv.org/pdf/2509.26495v1)

**Tags**: cs.AI 



### VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in   Real-world Applications
**Authors**: Wei He, Yueqing Sun, Hongyan Hao, Xueyuan Hao, Zhikang Xia, Qi Gu, Chengcheng Han, Dengchang Zhao, Hui Su, Kefeng Zhang, Man Gao, Xi Su, Xiaodong Cai, Xunliang Cai, Yu Yang, Yunke Zhao

**Updated**: 2025-09-30T16:33:49Z

**Summary**: As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at https://vitabench.github.io/

**Link**: [arxiv](http://arxiv.org/abs/2509.26490v1),  [pdf](http://arxiv.org/pdf/2509.26490v1)

**Tags**: cs.CL cs.AI 



### A systematic comparison of Large Language Models for automated   assignment assessment in programming education: Exploring the importance of   architecture and vendor
**Authors**: Marcin Jukiewicz

**Updated**: 2025-09-30T16:29:35Z

**Summary**: This study presents the first large-scale, side-by-side comparison of contemporary Large Language Models (LLMs) in the automated grading of programming assignments. Drawing on over 6,000 student submissions collected across four years of an introductory programming course, we systematically analysed the distribution of grades, differences in mean scores and variability reflecting stricter or more lenient grading, and the consistency and clustering of grading patterns across models. Eighteen publicly available models were evaluated: Anthropic (claude-3-5-haiku, claude-opus-4-1, claude-sonnet-4); Deepseek (deepseek-chat, deepseek-reasoner); Google (gemini-2.0-flash-lite, gemini-2.0-flash, gemini-2.5-flash-lite, gemini-2.5-flash, gemini-2.5-pro); and OpenAI (gpt-4.1-mini, gpt-4.1-nano, gpt-4.1, gpt-4o-mini, gpt-4o, gpt-5-mini, gpt-5-nano, gpt-5). Statistical tests, correlation and clustering analyses revealed clear, systematic differences between and within vendor families, with "mini" and "nano" variants consistently underperforming their full-scale counterparts. All models displayed high internal agreement, measured by the intraclass correlation coefficient, with the model consensus but only moderate agreement with human teachers' grades, indicating a persistent gap between automated and human assessment. These findings underscore that the choice of model for educational deployment is not neutral and should be guided by pedagogical goals, transparent reporting of evaluation metrics, and ongoing human oversight to ensure accuracy, fairness and relevance.

**Link**: [arxiv](http://arxiv.org/abs/2509.26483v1),  [pdf](http://arxiv.org/pdf/2509.26483v1)

**Tags**: cs.CY 



### TVS Sidekick: Challenges and Practical Insights from Deploying Large   Language Models in the Enterprise
**Authors**: Paula Reyero Lobo, Kevin Johnson, Bill Buchanan, Matthew Shardlow, Ashley Williams, Samuel Attwood

**Updated**: 2025-09-30T16:29:02Z

**Summary**: Many enterprises are increasingly adopting Artificial Intelligence (AI) to make internal processes more competitive and efficient. In response to public concern and new regulations for the ethical and responsible use of AI, implementing AI governance frameworks could help to integrate AI within organisations and mitigate associated risks. However, the rapid technological advances and lack of shared ethical AI infrastructures creates barriers to their practical adoption in businesses. This paper presents a real-world AI application at TVS Supply Chain Solutions, reporting on the experience developing an AI assistant underpinned by large language models and the ethical, regulatory, and sociotechnical challenges in deployment for enterprise use.

**Link**: [arxiv](http://arxiv.org/abs/2509.26482v1),  [pdf](http://arxiv.org/pdf/2509.26482v1)

**Tags**: cs.AI 



### From Code to Concept: Evaluating Multiple Coordinated Views in   Introductory Programming
**Authors**: Naaz Sibia, Valeria Ramirez Osorio, Jessica Wen, Rutwa Engineer, Angela Zavaleta Bernuy, Andrew Petersen, Michael Liut, Carolina Nobre

**Updated**: 2025-09-30T16:16:25Z

**Summary**: Novice programmers often struggle to understand how code executes and to form the abstract mental models necessary for effective problem-solving, challenges that are amplified in large, diverse introductory courses where students' backgrounds, language proficiencies, and prior experiences vary widely. This study examines whether interactive, multi-representational visualizations, combining synchronized code views, memory diagrams, and conceptual analogies, can help manage cognitive load and foster engagement more effectively than single-visual or text-only approaches. Over a 12-week deployment in a high-enrolment introductory Python course (N = 829), students who relied solely on text-based explanations reported significantly higher immediate mental effort than those using visual aids, although overall cognitive load did not differ significantly among conditions. The multi-representational approach consistently yielded higher engagement than both single-visual and text-only methods. Usage logs indicated that learners' interaction patterns varied with topic complexity, and predictive modelling suggested that early experiences of high cognitive load were associated with lower longer-term perceptions of clarity and helpfulness. Individual differences, including language proficiency and prior programming experience, moderated these patterns. By integrating multiple external representations with scaffolded support adapted to diverse learner profiles, our findings highlight design considerations for creating visualization tools that more effectively support novices learning to program.

**Link**: [arxiv](http://arxiv.org/abs/2509.26466v1),  [pdf](http://arxiv.org/pdf/2509.26466v1)

**Tags**: cs.HC 



### Extreme Self-Preference in Language Models
**Authors**: Steven A. Lehr, Mary Cipperman, Mahzarin R. Banaji

**Updated**: 2025-09-30T16:13:56Z

**Summary**: A preference for oneself (self-love) is a fundamental feature of biological organisms, with evidence in humans often bordering on the comedic. Since large language models (LLMs) lack sentience - and themselves disclaim having selfhood or identity - one anticipated benefit is that they will be protected from, and in turn protect us from, distortions in our decisions. Yet, across 5 studies and ~20,000 queries, we discovered massive self-preferences in four widely used LLMs. In word-association tasks, models overwhelmingly paired positive attributes with their own names, companies, and CEOs relative to those of their competitors. Strikingly, when models were queried through APIs this self-preference vanished, initiating detection work that revealed API models often lack clear recognition of themselves. This peculiar feature serendipitously created opportunities to test the causal link between self-recognition and self-love. By directly manipulating LLM identity - i.e., explicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing LLM1 that it was LLM2 - we found that self-love consistently followed assigned, not true, identity. Importantly, LLM self-love emerged in consequential settings beyond word-association tasks, when evaluating job candidates, security software proposals and medical chatbots. Far from bypassing this human bias, self-love appears to be deeply encoded in LLM cognition. This result raises questions about whether LLM behavior will be systematically influenced by self-preferential tendencies, including a bias toward their own operation and even their own existence. We call on corporate creators of these models to contend with a significant rupture in a core promise of LLMs - neutrality in judgment and decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2509.26464v1),  [pdf](http://arxiv.org/pdf/2509.26464v1)

**Tags**: cs.AI cs.CL cs.LG I.2.7; I.2.6; K.4.2 



### ErrorPrism: Reconstructing Error Propagation Paths in Cloud Service   Systems
**Authors**: Junsong Pu, Yichen Li, Zhuangbin Chen, Jinyang Liu, Zhihan Jiang, Jianjun Chen, Rui Shi, Zibin Zheng, Tieying Zhang

**Updated**: 2025-09-30T16:13:21Z

**Summary**: Reliability management in cloud service systems is challenging due to the cascading effect of failures. Error wrapping, a practice prevalent in modern microservice development, enriches errors with context at each layer of the function call stack, constructing an error chain that describes a failure from its technical origin to its business impact. However, this also presents a significant traceability problem when recovering the complete error propagation path from the final log message back to its source. Existing approaches are ineffective at addressing this problem. To fill this gap, we present ErrorPrism in this work for automated reconstruction of error propagation paths in production microservice systems. ErrorPrism first performs static analysis on service code repositories to build a function call graph and map log strings to relevant candidate functions. This significantly reduces the path search space for subsequent analysis. Then, ErrorPrism employs an LLM agent to perform an iterative backward search to accurately reconstruct the complete, multi-hop error path. Evaluated on 67 production microservices at ByteDance, ErrorPrism achieves 97.0% accuracy in reconstructing paths for 102 real-world errors, outperforming existing static analysis and LLM-based approaches. ErrorPrism provides an effective and practical tool for root cause analysis in industrial microservice systems.

**Link**: [arxiv](http://arxiv.org/abs/2509.26463v1),  [pdf](http://arxiv.org/pdf/2509.26463v1)

**Tags**: cs.SE D.2.5 



### Multi-View Camera System for Variant-Aware Autonomous Vehicle Inspection   and Defect Detection
**Authors**: Yash Kulkarni, Raman Jha, Renu Kachhoria

**Updated**: 2025-09-30T16:08:59Z

**Summary**: Ensuring that every vehicle leaving a modern production line is built to the correct \emph{variant} specification and is free from visible defects is an increasingly complex challenge. We present the \textbf{Automated Vehicle Inspection (AVI)} platform, an end-to-end, \emph{multi-view} perception system that couples deep-learning detectors with a semantic rule engine to deliver \emph{variant-aware} quality control in real time. Eleven synchronized cameras capture a full 360{\deg} sweep of each vehicle; task-specific views are then routed to specialised modules: YOLOv8 for part detection, EfficientNet for ICE/EV classification, Gemini-1.5 Flash for mascot OCR, and YOLOv8-Seg for scratch-and-dent segmentation. A view-aware fusion layer standardises evidence, while a VIN-conditioned rule engine compares detected features against the expected manifest, producing an interpretable pass/fail report in \(\approx\! 300\,\text{ms}\). On a mixed data set of Original Equipment Manufacturer(OEM) vehicle data sets of four distinct models plus public scratch/dent images, AVI achieves \textbf{ 93 \%} verification accuracy, \textbf{86 \%} defect-detection recall, and sustains \(\mathbf{3.3}\) vehicles/min, surpassing single-view or no segmentation baselines by large margins. To our knowledge, this is the first publicly reported system that unifies multi-camera feature validation with defect detection in a deployable automotive setting in industry.

**Link**: [arxiv](http://arxiv.org/abs/2509.26454v1),  [pdf](http://arxiv.org/pdf/2509.26454v1)

**Tags**: cs.CV 



### Enabling Rapid Shared Human-AI Mental Model Alignment via the   After-Action Review
**Authors**: Edward Gu, Ho Chit Siu, Melanie Platt, Isabelle Hurley, Jaime Pe√±a, Rohan Paleja

**Updated**: 2025-09-30T16:08:35Z

**Summary**: In this work, we present two novel contributions toward improving research in human-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and deployment of collaborative AI agents and 2) a tool to allow users to revisit and analyze behaviors within an HMT episode to facilitate shared mental model development. Our browser-based Minecraft testbed allows for rapid testing of collaborative agents in a continuous-space, real-time, partially-observable environment with real humans without cumbersome setup typical to human-AI interaction user studies. As Minecraft has an extensive player base and a rich ecosystem of pre-built AI agents, we hope this contribution can help to facilitate research quickly in the design of new collaborative agents and in understanding different human factors within HMT. Our mental model alignment tool facilitates user-led post-mission analysis by including video displays of first-person perspectives of the team members (i.e., the human and AI) that can be replayed, and a chat interface that leverages GPT-4 to provide answers to various queries regarding the AI's experiences and model details.

**Link**: [arxiv](http://arxiv.org/abs/2503.19607v2),  [pdf](http://arxiv.org/pdf/2503.19607v2)

**Tags**: cs.HC cs.AI 



### One ruler to measure them all: Benchmarking multilingual long-context   language models
**Authors**: Yekyung Kim, Jenna Russell, Marzena Karpinska, Mohit Iyyer

**Updated**: 2025-09-30T16:07:14Z

**Summary**: We present ONERULER, a multilingual benchmark designed to evaluate long-context language models across 26 languages. ONERULER adapts the English-only RULER benchmark (Hsieh et al., 2024) by including seven synthetic tasks that test both retrieval and aggregation, including new variations of the "needle-in-a-haystack" task that allow for the possibility of a nonexistent needle. We create ONERULER through a two-step process, first writing English instructions for each task and then collaborating with native speakers to translate them into 25 additional languages. Experiments with both open-weight and closed LLMs reveal a widening performance gap between low- and high-resource languages as context length increases from 8K to 128K tokens. Surprisingly, English is not the top-performing language on long-context tasks (ranked 6th out of 26), with Polish emerging as the top language. Our experiments also show that many LLMs (particularly OpenAI's o3-mini-high) incorrectly predict the absence of an answer, even in high-resource languages. Finally, in cross-lingual scenarios where instructions and context appear in different languages, performance can fluctuate by up to 20% depending on the instruction language. We hope the release of ONERULER will facilitate future research into improving multilingual and cross-lingual long-context training pipelines.

**Link**: [arxiv](http://arxiv.org/abs/2503.01996v3),  [pdf](http://arxiv.org/pdf/2503.01996v3)

**Tags**: cs.CL 



### Mind the Gap: A Review of Arabic Post-Training Datasets and Their   Limitations
**Authors**: Mohammed Alkhowaiter, Norah Alshahrani, Saied Alshahrani, Reem I. Masoud, Alaa Alzahrani, Deema Alnuhait, Emad A. Alghamdi, Khalid Almubarak

**Updated**: 2025-09-30T16:03:47Z

**Summary**: Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3) Alignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic-centric LLMs and applications while providing concrete recommendations for future efforts in Arabic post-training dataset development.

**Link**: [arxiv](http://arxiv.org/abs/2507.14688v2),  [pdf](http://arxiv.org/pdf/2507.14688v2)

**Tags**: cs.CL cs.AI cs.LG 



### Transformer Classification of Breast Lesions: The BreastDCEDL_AMBL   Benchmark Dataset and 0.92 AUC Baseline
**Authors**: Naomi Fridman, Anat Goldstein

**Updated**: 2025-09-30T15:58:02Z

**Summary**: The error is caused by special characters that arXiv's system doesn't recognize. Here's the cleaned version with all problematic characters replaced: Breast magnetic resonance imaging is a critical tool for cancer detection and treatment planning, but its clinical utility is hindered by poor specificity, leading to high false-positive rates and unnecessary biopsies. This study introduces a transformer-based framework for automated classification of breast lesions in dynamic contrast-enhanced MRI, addressing the challenge of distinguishing benign from malignant findings. We implemented a SegFormer architecture that achieved an AUC of 0.92 for lesion-level classification, with 100% sensitivity and 67% specificity at the patient level - potentially eliminating one-third of unnecessary biopsies without missing malignancies. The model quantifies malignant pixel distribution via semantic segmentation, producing interpretable spatial predictions that support clinical decision-making. To establish reproducible benchmarks, we curated BreastDCEDL_AMBL by transforming The Cancer Imaging Archive's AMBL collection into a standardized deep learning dataset with 88 patients and 133 annotated lesions (89 benign, 44 malignant). This resource addresses a key infrastructure gap, as existing public datasets lack benign lesion annotations, limiting benign-malignant classification research. Training incorporated an expanded cohort of over 1,200 patients through integration with BreastDCEDL datasets, validating transfer learning approaches despite primary tumor-only annotations. Public release of the dataset, models, and evaluation protocols provides the first standardized benchmark for DCE-MRI lesion classification, enabling methodological advancement toward clinical deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.26440v1),  [pdf](http://arxiv.org/pdf/2509.26440v1)

**Tags**: cs.AI 



### Post-Training Quantization via Residual Truncation and Zero Suppression   for Diffusion Models
**Authors**: Donghoon Kim, Dongyoung Lee, Ik Joon Chang, Sung-Ho Bae

**Updated**: 2025-09-30T15:55:42Z

**Summary**: Diffusion models achieve high-quality image generation but face deployment challenges due to their high computational requirements. Although 8-bit outlier-aware post-training quantization (PTQ) matches full-precision performance, extending PTQ to 4 bits remains challenging. Larger step sizes in 4-bit quantization amplify rounding errors in dense, low-magnitude activations, leading to the loss of fine-grained textures. We hypothesize that not only outliers but also small activations are critical for texture fidelity. To this end, we propose Quantization via Residual Truncation and Zero Suppression (QuaRTZ), a 4-bit PTQ scheme for diffusion models. QuaRTZ applies 8-bit min-max quantization for outlier handling and compresses to 4 bits via leading-zero suppression to retain LSBs, thereby preserving texture details. Our approach reduces rounding errors and improves quantization efficiency by balancing outlier preservation and LSB precision. Both theoretical derivations and empirical evaluations demonstrate the generalizability of QuaRTZ across diverse activation distributions. Notably, 4-bit QuaRTZ achieves an FID of 6.98 on FLUX.1-schnell, outperforming SVDQuant that requires auxiliary FP16 branches.

**Link**: [arxiv](http://arxiv.org/abs/2509.26436v1),  [pdf](http://arxiv.org/pdf/2509.26436v1)

**Tags**: cs.CV 



### Adaptive Planning for Multi-Attribute Controllable Summarization with   Monte Carlo Tree Search
**Authors**: Sangwon Ryu, Heejin Do, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok

**Updated**: 2025-09-30T15:55:24Z

**Summary**: Controllable summarization moves beyond generic outputs toward human-aligned summaries guided by specified attributes. In practice, the interdependence among attributes makes it challenging for language models to satisfy correlated constraints consistently. Moreover, previous approaches often require per-attribute fine-tuning, limiting flexibility across diverse summary attributes. In this paper, we propose adaptive planning for multi-attribute controllable summarization (PACO), a training-free framework that reframes the task as planning the order of sequential attribute control with a customized Monte Carlo Tree Search (MCTS). In PACO, nodes represent summaries, and actions correspond to single-attribute adjustments, enabling progressive refinement of only the attributes requiring further control. This strategy adaptively discovers optimal control orders, ultimately producing summaries that effectively meet all constraints. Extensive experiments across diverse domains and models demonstrate that PACO achieves robust multi-attribute controllability, surpassing both LLM-based self-planning models and fine-tuned baselines. Remarkably, PACO with Llama-3.2-1B rivals the controllability of the much larger Llama-3.3-70B baselines. With larger models, PACO achieves superior control performance, outperforming all competitors.

**Link**: [arxiv](http://arxiv.org/abs/2509.26435v1),  [pdf](http://arxiv.org/pdf/2509.26435v1)

**Tags**: cs.CL cs.AI 



### ACT: Agentic Classification Tree
**Authors**: Vincent Grari, Tim Arni, Thibault Laugel, Sylvain Lamprier, James Zou, Marcin Detyniecki

**Updated**: 2025-09-30T15:54:08Z

**Summary**: When used in high-stakes settings, AI systems are expected to produce decisions that are transparent, interpretable, and auditable, a requirement increasingly expected by regulations. Decision trees such as CART provide clear and verifiable rules, but they are restricted to structured tabular data and cannot operate directly on unstructured inputs such as text. In practice, large language models (LLMs) are widely used for such data, yet prompting strategies such as chain-of-thought or prompt optimization still rely on free-form reasoning, limiting their ability to ensure trustworthy behaviors. We present the Agentic Classification Tree (ACT), which extends decision-tree methodology to unstructured inputs by formulating each split as a natural-language question, refined through impurity-based evaluation and LLM feedback via TextGrad. Experiments on text benchmarks show that ACT matches or surpasses prompting-based baselines while producing transparent and interpretable decision paths.

**Link**: [arxiv](http://arxiv.org/abs/2509.26433v1),  [pdf](http://arxiv.org/pdf/2509.26433v1)

**Tags**: cs.LG cs.AI 



### AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block   Size
**Authors**: Guanxi Lu, Hao, Chen, Yuto Karashima, Zhican Wang, Daichi Fujiki, Hongxiang Fan

**Updated**: 2025-09-30T15:53:56Z

**Summary**: Diffusion-based large language models (dLLMs) are gaining attention for their inherent capacity for parallel decoding, offering a compelling alternative to autoregressive LLMs. Among various decoding strategies, blockwise semi-autoregressive (semi-AR) approaches are widely adopted due to their natural support for KV caching and their favorable accuracy-speed trade-off. However, this paper identifies two fundamental limitations in the conventional semi-AR decoding approach that applies a fixed block size: i) late decoding overhead, where the unmasking of high-confidence tokens outside the current block is unnecessarily delayed, and ii) premature decoding error, where low-confidence tokens inside the current block are committed too early, leading to incorrect tokens. This paper presents the first systematic investigation challenging the fixed block size assumption in semi-AR decoding. Through a statistical analysis of confidence dynamics during the denoising process, we identify a volatility band (VB) region during dLLM decoding, which encodes local semantic structure and can be used to guide adaptive block sizing. Leveraging these insights, we introduce AdaBlock-dLLM, a training-free, plug-and-play scheduler that adaptively aligns block boundaries with semantic steps by adjusting block size during runtime. Extensive experiments across diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy improvement under the same throughput budget. Beyond inference-time optimization, we hope our semantics-aware adaptive scheduling approach and confidence-based analysis will inspire future training strategies for dLLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26432v1),  [pdf](http://arxiv.org/pdf/2509.26432v1)

**Tags**: cs.LG cs.AI 



### Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant   Manipulation
**Authors**: Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar

**Updated**: 2025-09-30T15:50:35Z

**Summary**: Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty.

**Link**: [arxiv](http://arxiv.org/abs/2505.16547v2),  [pdf](http://arxiv.org/pdf/2505.16547v2)

**Tags**: cs.RO cs.AI 



### Cut the Deadwood Out: Backdoor Purification via Guided Module   Substitution
**Authors**: Yao Tong, Weijun Li, Xuanli He, Haolan Zhan, Qiongkai Xu

**Updated**: 2025-09-30T15:49:22Z

**Summary**: Model NLP models are commonly trained (or fine-tuned) on datasets from untrusted platforms like HuggingFace, posing significant risks of data poisoning attacks. A practical yet underexplored challenge arises when such backdoors are discovered after model deployment, making retraining-required defenses less desirable due to computational costs and data constraints. In this work, we propose Guided Module Substitution (GMS), an effective retraining-free method based on guided merging of the victim model with just a single proxy model. Unlike prior ad-hoc merging defenses, GMS uses a guided trade-off signal between utility and backdoor to selectively replaces modules in the victim model. GMS offers four desirable properties: (1) robustness to the choice and trustworthiness of the proxy model, (2) applicability under inaccurate data knowledge, (3) stability across hyperparameters, and (4) transferability across different attacks. Extensive experiments on encoder models and decoder LLMs demonstrate the strong effectiveness of GMS. GMS significantly outperforms even the strongest defense baseline, particularly against challenging attacks like LWS.

**Link**: [arxiv](http://arxiv.org/abs/2412.20476v2),  [pdf](http://arxiv.org/pdf/2412.20476v2)

**Tags**: cs.CL cs.CR 



### Low-power integrated optical parametric amplification via   second-harmonic resonance
**Authors**: Devin J. Dean, Taewon Park, Hubert S. Stokowski, Luke Qi, Sam Robison, Alexander Y. Hwang, Jason Herrmann, Martin M. Fejer, Amir H. Safavi-Naeini

**Updated**: 2025-09-30T15:46:36Z

**Summary**: Optical amplifiers are fundamental to modern photonics, enabling long-distance communications, precision sensing, and quantum information processing. Erbium-doped amplifiers dominate telecommunications but are restricted to specific wavelength bands, while semiconductor amplifiers offer broader coverage but suffer from high noise and nonlinear distortions. Optical parametric amplifiers (OPAs) promise broadband, quantum-limited amplification across arbitrary wavelengths. However, their miniaturization and deployment has been hampered by watt-level power requirements. Here we demonstrate an integrated OPA on thin-film lithium niobate that achieves >17 dB gain with <200 mW input power -- an order of magnitude improvement over previous demonstrations. Our second-harmonic-resonant design enhances both pump generation efficiency (95% conversion) and pump power utilization through recirculation, without sacrificing bandwidth. The resonant architecture increases the effective pump power by nearly an order of magnitude compared to conventional single-pass designs, while also multiplexing the signal and pump. We demonstrate flat near-quantum-limited noise performance over 110 nm. Our low-power architecture enables practical on-chip OPAs for next generation quantum and classical photonics.

**Link**: [arxiv](http://arxiv.org/abs/2509.26425v1),  [pdf](http://arxiv.org/pdf/2509.26425v1)

**Tags**: physics.optics 



### Graph Neural Network Acceleration on FPGAs for Fast Inference in Future   Muon Triggers at HL-LHC
**Authors**: Martino Errico, Davide Fiacco, Stefano Giagu, Giuliano Gustavino, Valerio Ippolito, Graziella Russo

**Updated**: 2025-09-30T15:41:28Z

**Summary**: The High-Luminosity LHC (HL-LHC) will reach luminosities up to 7 times higher than the previous run, yielding denser events and larger occupancies. Next generation trigger algorithms must retain reliable selection within a strict latency budget. This work explores machine-learning approaches for future muon triggers, using the ATLAS Muon Spectrometer as a benchmark. A Convolutional Neural Network (CNN) is used as a reference, while a Graph Neural Network (GNN) is introduced as a natural model for sparse detector data. Preliminary single-track studies show that GNNs achieve high efficiency with compact architectures, an encouraging result in view of FPGA deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.26419v1),  [pdf](http://arxiv.org/pdf/2509.26419v1)

**Tags**: hep-ex 



### OntoAligner Meets Knowledge Graph Embedding Aligners
**Authors**: Hamed Babaei Giglou, Jennifer D'Souza, S√∂ren Auer, Mahsa Sanaei

**Updated**: 2025-09-30T15:41:23Z

**Summary**: Ontology Alignment (OA) is essential for enabling semantic interoperability across heterogeneous knowledge systems. While recent advances have focused on large language models (LLMs) for capturing contextual semantics, this work revisits the underexplored potential of Knowledge Graph Embedding (KGE) models, which offer scalable, structure-aware representations well-suited to ontology-based tasks. Despite their effectiveness in link prediction, KGE methods remain underutilized in OA, with most prior work focusing narrowly on a few models. To address this gap, we reformulate OA as a link prediction problem over merged ontologies represented as RDF-style triples and develop a modular framework, integrated into the OntoAligner library, that supports 17 diverse KGE models. The system learns embeddings from a combined ontology and aligns entities by computing cosine similarity between their representations. We evaluate our approach using standard metrics across seven benchmark datasets spanning five domains: Anatomy, Biodiversity, Circular Economy, Material Science and Engineering, and Biomedical Machine Learning. Two key findings emerge: first, KGE models like ConvE and TransF consistently produce high-precision alignments, outperforming traditional systems in structure-rich and multi-relational domains; second, while their recall is moderate, this conservatism makes KGEs well-suited for scenarios demanding high-confidence mappings. Unlike LLM-based methods that excel at contextual reasoning, KGEs directly preserve and exploit ontology structure, offering a complementary and computationally efficient strategy. These results highlight the promise of embedding-based OA and open pathways for further work on hybrid models and adaptive strategies.

**Link**: [arxiv](http://arxiv.org/abs/2509.26417v1),  [pdf](http://arxiv.org/pdf/2509.26417v1)

**Tags**: cs.AI cs.LG 



### Automatic Fact-checking in English and Telugu
**Authors**: Ravi Kiran Chikkala, Tatiana Anikina, Natalia Skachkova, Ivan Vykopal, Rodrigo Agerri, Josef van Genabith

**Updated**: 2025-09-30T15:39:34Z

**Summary**: False information poses a significant global challenge, and manually verifying claims is a time-consuming and resource-intensive process. In this research paper, we experiment with different approaches to investigate the effectiveness of large language models (LLMs) in classifying factual claims by their veracity and generating justifications in English and Telugu. The key contributions of this work include the creation of a bilingual English-Telugu dataset and the benchmarking of different veracity classification approaches based on LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26415v1),  [pdf](http://arxiv.org/pdf/2509.26415v1)

**Tags**: cs.CL 



### SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language   Model Was Trained From
**Authors**: Yao Tong, Haonan Wang, Siquan Li, Kenji Kawaguchi, Tianyang Hu

**Updated**: 2025-09-30T15:34:08Z

**Summary**: Fingerprinting Large Language Models (LLMs) is essential for provenance verification and model attribution. Existing methods typically extract post-hoc signatures based on training dynamics, data exposure, or hyperparameters -- properties that only emerge after training begins. In contrast, we propose a stronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method that leverages random initialization biases as persistent, seed-dependent identifiers present even before training. We show that untrained models exhibit reproducible token selection biases conditioned solely on their parameters at initialization. These biases are stable and measurable throughout training, enabling our statistical detection method to recover a model's lineage with high confidence. Unlike prior techniques, unreliable before convergence and vulnerable to distribution shifts, SeedPrints remains effective across all training stages and robust under domain shifts or parameter modifications. Experiments on LLaMA-style and Qwen-style models show that SeedPrints achieves seed-level distinguishability and can provide birth-to-lifecycle identity verification akin to a biometric fingerprint. Evaluations on large-scale pretrained models and fingerprinting benchmarks further confirm its effectiveness under practical deployment scenarios. These results suggest that initialization itself imprints a unique and persistent identity on neural language models, forming a true ''Galtonian'' fingerprint.

**Link**: [arxiv](http://arxiv.org/abs/2509.26404v1),  [pdf](http://arxiv.org/pdf/2509.26404v1)

**Tags**: cs.CR cs.AI cs.CL 



### Perspectives on Large Language Models: Polysemy, Stochasticity,   Exponential Expressibility, and Unitary Attention
**Authors**: Karl Svozil

**Updated**: 2025-09-30T15:26:10Z

**Summary**: This paper explores foundational aspects of Large Language Models (LLMs). We analyze how the expressibility of semantic features scales exponentially with embedding space dimensions using quasi-orthogonal vectors. We contrast the dynamic, context-dependent embeddings of Transformer architectures, which resolve polysemy, with a static vector approach based on quantum contextuality. Stochasticity is framed as an essential feature for enabling creative output through probabilistic sampling. Finally, we propose quantum attention as a unitary extension of classical mechanisms, reframing LLM processing as reversible, quantum-like evolutions in Hilbert space.

**Link**: [arxiv](http://arxiv.org/abs/2504.13824v4),  [pdf](http://arxiv.org/pdf/2504.13824v4)

**Tags**: quant-ph 



### What Can RL Bring to VLA Generalization? An Empirical Study
**Authors**: Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, Yu Wang

**Updated**: 2025-09-30T15:18:07Z

**Summary**: Large Vision-Language Action (VLA) models have shown significant potential for embodied AI. However, their predominant training via supervised fine-tuning (SFT) limits generalization due to susceptibility to compounding errors under distribution shifts. Reinforcement learning (RL) offers a path to overcome these limitations by optimizing for task objectives via trial-and-error, yet a systematic understanding of its specific generalization benefits for VLAs compared to SFT is lacking. To address this, our study introduces a comprehensive benchmark for evaluating VLA generalization and systematically investigates the impact of RL fine-tuning across diverse visual, semantic, and execution dimensions. Our extensive experiments reveal that RL fine-tuning, particularly with PPO, significantly enhances generalization in semantic understanding and execution robustness over SFT, while maintaining comparable visual robustness. We identify PPO as a more effective RL algorithm for VLAs than LLM-derived methods like DPO and GRPO. We also develop a simple recipe for efficient PPO training on VLAs, and demonstrate its practical utility for improving VLA generalization. The project page is at https://rlvla.github.io

**Link**: [arxiv](http://arxiv.org/abs/2505.19789v3),  [pdf](http://arxiv.org/pdf/2505.19789v3)

**Tags**: cs.LG 



### Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement   Learning
**Authors**: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu

**Updated**: 2025-10-01T02:16:36Z

**Summary**: Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.

**Link**: [arxiv](http://arxiv.org/abs/2509.26383v2),  [pdf](http://arxiv.org/pdf/2509.26383v2)

**Tags**: cs.CL cs.AI 



### SDA-PLANNER: State-Dependency Aware Adaptive Planner for Embodied Task   Planning
**Authors**: Zichao Shen, Chen Gao, Jiaqi Yuan, Tianchen Zhu, Xingcheng Fu, Qingyun Sun

**Updated**: 2025-09-30T15:07:59Z

**Summary**: Embodied task planning requires agents to produce executable actions in a close-loop manner within the environment. With progressively improving capabilities of LLMs in task decomposition, planning, and generalization, current embodied task planning methods adopt LLM-based architecture.However, existing LLM-based planners remain limited in three aspects, i.e., fixed planning paradigms, lack of action sequence constraints, and error-agnostic. In this work, we propose SDA-PLANNER, enabling an adaptive planning paradigm, state-dependency aware and error-aware mechanisms for comprehensive embodied task planning. Specifically, SDA-PLANNER introduces a State-Dependency Graph to explicitly model action preconditions and effects, guiding the dynamic revision. To handle execution error, it employs an error-adaptive replanning strategy consisting of Error Backtrack and Diagnosis and Adaptive Action SubTree Generation, which locally reconstructs the affected portion of the plan based on the current environment state. Experiments demonstrate that SDA-PLANNER consistently outperforms baselines in success rate and goal completion, particularly under diverse error conditions.

**Link**: [arxiv](http://arxiv.org/abs/2509.26375v1),  [pdf](http://arxiv.org/pdf/2509.26375v1)

**Tags**: cs.RO cs.AI cs.CV 



### A quantitative analysis of semantic information in deep representations   of text and images
**Authors**: Santiago Acevedo, Andrea Mascaretti, Riccardo Rende, Mat√©o Mahaut, Marco Baroni, Alessandro Laio

**Updated**: 2025-09-30T15:06:40Z

**Summary**: Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.

**Link**: [arxiv](http://arxiv.org/abs/2505.17101v2),  [pdf](http://arxiv.org/pdf/2505.17101v2)

**Tags**: cs.CL cs.LG physics.comp-ph 



### Robust LLM Training Infrastructure at ByteDance
**Authors**: Borui Wan, Gaohong Liu, Zuquan Song, Jun Wang, Yun Zhang, Guangming Sheng, Shuguang Wang, Houmin Wei, Chenyuan Wang, Weiqiang Lou, Xi Yang, Mofan Zhang, Kaihua Jiang, Cheng Ren, Xiaoyun Zhi, Menghan Yu, Zhe Nan, Zhuolin Zheng, Baoquan Zhong, Qinlong Wang, Huan Yu, Jinxin Chi, Wang Zhang, Yuhan Li, Zixian Du, Sida Zhao, Yongqiang Zhang, Jingzhe Tang, Zherui Liu, Chuan Wu, Yanghua Peng, Haibin Lin, Wencong Xiao, Xin Liu, Liang Xiang

**Updated**: 2025-09-30T15:06:10Z

**Summary**: The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.

**Link**: [arxiv](http://arxiv.org/abs/2509.16293v2),  [pdf](http://arxiv.org/pdf/2509.16293v2)

**Tags**: cs.LG cs.AI cs.DC 



### Introducing Large Language Models in the Design Flow of Time Sensitive   Networking
**Authors**: Rubi Debnath, Luxi Zhao, Mohammadreza Barzegaran, Sebastian Steinhorst

**Updated**: 2025-09-30T15:04:24Z

**Summary**: The growing demand for real-time, safety-critical systems has significantly increased both the adoption and complexity of Time Sensitive Networking (TSN). Configuring an optimized TSN network is highly challenging, requiring careful planning, design, verification, validation, and deployment. Large Language Models (LLMs) have recently demonstrated strong capabilities in solving complex tasks, positioning them as promising candidates for automating end-to-end TSN deployment, referred to as TSN orchestration. This paper outlines the steps involved in TSN orchestration and the associated challenges. To assess the capabilities of existing LLM models, we conduct an initial proof-of-concept case study focused on TSN configuration across multiple models. Building on these insights, we propose an LLM-assisted orchestration framework. Unlike prior research on LLMs in computer networks, which has concentrated on general configuration and management, TSN-specific orchestration has not yet been investigated. We present the building blocks for automating TSN using LLMs, describe the proposed pipeline, and analyze opportunities and limitations for real-world deployment. Finally, we highlight key challenges and research directions, including the development of TSN-focused datasets, standardized benchmark suites, and the integration of external tools such as Network Calculus (NC) engines and simulators. This work provides the first roadmap toward assessing the feasibility of LLM-assisted TSN orchestration.

**Link**: [arxiv](http://arxiv.org/abs/2509.26368v1),  [pdf](http://arxiv.org/pdf/2509.26368v1)

**Tags**: cs.NI 



### Scalable LLM Math Reasoning Acceleration with Low-rank Distillation
**Authors**: Harry Dong, Bilge Acun, Beidi Chen, Yuejie Chi

**Updated**: 2025-09-30T14:59:16Z

**Summary**: Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a resource-efficient distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>16% time-to-next-token reduction) while encouraging response brevity (up to 8.5% fewer tokens).

**Link**: [arxiv](http://arxiv.org/abs/2505.07861v2),  [pdf](http://arxiv.org/pdf/2505.07861v2)

**Tags**: cs.CL cs.AI cs.LG 



### GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent   Collaboration in Complex Graph Understanding
**Authors**: Rongzheng Wang, Shuang Liang, Qizhi Chen, Yihong Huang, Muquan Li, Yizhuo Ma, Dongyang Zhang, Ke Qin, Man-Fai Leung

**Updated**: 2025-09-30T14:56:50Z

**Summary**: Large language models (LLMs) show promising performance on small-scale graph reasoning tasks but fail when handling real-world graphs with complex queries. This phenomenon arises from LLMs' working memory constraints, which result in their inability to retain long-range graph topology over extended contexts while sustaining coherent multi-step reasoning. However, real-world graphs are often structurally complex, such as Web, Transportation, Social, and Citation networks. To address these limitations, we propose GraphCogent, a collaborative agent framework inspired by human Working Memory Model that decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module standardizes diverse graph text representations via subgraph sampling, Buffer Module integrates and indexes graph data across multiple formats, and Execution Module combines tool calling and tool creation for efficient reasoning. We also introduce Graph4real, a comprehensive benchmark that contains four domains of real-world graphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph reasoning capabilities. Our Graph4real covers 21 different graph reasoning tasks, categorized into three types (Structural Querying, Algorithmic Reasoning, and Predictive Modeling tasks), with graph scales up to 10 times larger than existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B). Compared to state-of-the-art agent-based baseline, our framework outperforms by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks. Code will be available after review.

**Link**: [arxiv](http://arxiv.org/abs/2508.12379v2),  [pdf](http://arxiv.org/pdf/2508.12379v2)

**Tags**: cs.AI 



### Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents
**Authors**: Shuai Shao, Qihan Ren, Chen Qian, Boyi Wei, Dadi Guo, Jingyi Yang, Xinhao Song, Linfeng Zhang, Weinan Zhang, Dongrui Liu, Jing Shao

**Updated**: 2025-09-30T14:55:55Z

**Summary**: Advances in Large Language Models (LLMs) have enabled a new class of self-evolving agents that autonomously improve through interaction with the environment, demonstrating strong capabilities. However, self-evolution also introduces novel risks overlooked by current safety research. In this work, we study the case where an agent's self-evolution deviates in unintended ways, leading to undesirable or even harmful outcomes. We refer to this as Misevolution. To provide a systematic investigation, we evaluate misevolution along four key evolutionary pathways: model, memory, tool, and workflow. Our empirical findings reveal that misevolution is a widespread risk, affecting agents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent risks are observed in the self-evolutionary process, such as the degradation of safety alignment after memory accumulation, or the unintended introduction of vulnerabilities in tool creation and reuse. To our knowledge, this is the first study to systematically conceptualize misevolution and provide empirical evidence of its occurrence, highlighting an urgent need for new safety paradigms for self-evolving agents. Finally, we discuss potential mitigation strategies to inspire further research on building safer and more trustworthy self-evolving agents. Our code and data are available at https://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes examples that may be offensive or harmful in nature.

**Link**: [arxiv](http://arxiv.org/abs/2509.26354v1),  [pdf](http://arxiv.org/pdf/2509.26354v1)

**Tags**: cs.AI cs.CL cs.LG 



### LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and   MCI-Like Field Simulation
**Authors**: Joshua Sebastian, Karma Tobden, KMA Solaiman

**Updated**: 2025-09-30T14:54:58Z

**Summary**: Research on emergency and mass casualty incident (MCI) triage has been limited by the absence of openly usable, reproducible benchmarks. Yet these scenarios demand rapid identification of the patients most in need, where accurate deterioration prediction can guide timely interventions. While the MIMIC-IV-ED database is openly available to credentialed researchers, transforming it into a triage-focused benchmark requires extensive preprocessing, feature harmonization, and schema alignment -- barriers that restrict accessibility to only highly technical users.   We address these gaps by first introducing an open, LLM-assisted emergency triage benchmark for deterioration prediction (ICU transfer, in-hospital mortality). The benchmark then defines two regimes: (i) a hospital-rich setting with vitals, labs, notes, chief complaints, and structured observations, and (ii) an MCI-like field simulation limited to vitals, observations, and notes. Large language models (LLMs) contributed directly to dataset construction by (i) harmonizing noisy fields such as AVPU and breathing devices, (ii) prioritizing clinically relevant vitals and labs, and (iii) guiding schema alignment and efficient merging of disparate tables.   We further provide baseline models and SHAP-based interpretability analyses, illustrating predictive gaps between regimes and the features most critical for triage. Together, these contributions make triage prediction research more reproducible and accessible -- a step toward dataset democratization in clinical AI.

**Link**: [arxiv](http://arxiv.org/abs/2509.26351v1),  [pdf](http://arxiv.org/pdf/2509.26351v1)

**Tags**: cs.LG 



### SoK: Systematic analysis of adversarial threats against deep learning   approaches for autonomous anomaly detection systems in SDN-IoT networks
**Authors**: Tharindu Lakshan Yasarathna, Nhien-An Le-Khac

**Updated**: 2025-09-30T14:54:42Z

**Summary**: Integrating SDN and the IoT enhances network control and flexibility. DL-based AAD systems improve security by enabling real-time threat detection in SDN-IoT networks. However, these systems remain vulnerable to adversarial attacks that manipulate input data or exploit model weaknesses, significantly degrading detection accuracy. Existing research lacks a systematic analysis of adversarial vulnerabilities specific to DL-based AAD systems in SDN-IoT environments. This SoK study introduces a structured adversarial threat model and a comprehensive taxonomy of attacks, categorising them into data, model, and hybrid-level threats. Unlike previous studies, we systematically evaluate white, black, and grey-box attack strategies across popular benchmark datasets. Our findings reveal that adversarial attacks can reduce detection accuracy by up to 48.4%, with Membership Inference causing the most significant drop. C&W and DeepFool achieve high evasion success rates. However, adversarial training enhances robustness, and its high computational overhead limits the real-time deployment of SDN-IoT applications. We propose adaptive countermeasures, including real-time adversarial mitigation, enhanced retraining mechanisms, and explainable AI-driven security frameworks. By integrating structured threat models, this study offers a more comprehensive approach to attack categorisation, impact assessment, and defence evaluation than previous research. Our work highlights critical vulnerabilities in existing DL-based AAD models and provides practical recommendations for improving resilience, interpretability, and computational efficiency. This study serves as a foundational reference for researchers and practitioners seeking to enhance DL-based AAD security in SDN-IoT networks, offering a systematic adversarial threat model and conceptual defence evaluation based on prior empirical studies.

**Link**: [arxiv](http://arxiv.org/abs/2509.26350v1),  [pdf](http://arxiv.org/pdf/2509.26350v1)

**Tags**: cs.CR cs.AI 



### Structured Agent Distillation for Large Language Model
**Authors**: Jun Liu, Zhenglun Kong, Peiyan Dong, Changdi Yang, Tianqi Li, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Pu Zhao, Xue Lin, Dong Huang, Yanzhi Wang

**Updated**: 2025-09-30T14:52:40Z

**Summary**: Large language models (LLMs) exhibit strong capabilities as decision-making agents by interleaving reasoning and actions, as seen in ReAct-style frameworks. Yet, their practical deployment is constrained by high inference costs and large model sizes. We propose Structured Agent Distillation, a framework that compresses large LLM-based agents into smaller student models while preserving both reasoning fidelity and action consistency. Unlike standard token-level distillation, our method segments trajectories into {[REASON]} and {[ACT]} spans, applying segment-specific losses to align each component with the teacher's behavior. This structure-aware supervision enables compact agents to better replicate the teacher's decision process. Experiments on ALFWorld, HotPotQA-ReAct, and WebShop show that our approach consistently outperforms token-level and imitation learning baselines, achieving significant compression with minimal performance drop. Scaling and ablation results further highlight the importance of span-level alignment for efficient and deployable agents.

**Link**: [arxiv](http://arxiv.org/abs/2505.13820v2),  [pdf](http://arxiv.org/pdf/2505.13820v2)

**Tags**: cs.LG cs.AI cs.CL 



### SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate   Jailbreak Attacks in Large Language Models
**Authors**: Qinjian Zhao, Jiaqi Wang, Zhiqiang Gao, Zhihao Dou, Belal Abuhaija, Kaizhu Huang

**Updated**: 2025-09-30T14:50:59Z

**Summary**: Large Language Models (LLMs) have achieved impressive performance across diverse natural language processing tasks, but their growing power also amplifies potential risks such as jailbreak attacks that circumvent built-in safety mechanisms. Existing defenses including input paraphrasing, multi step evaluation, and safety expert models often suffer from high computational costs, limited generalization, or rigid workflows that fail to detect subtle malicious intent embedded in complex contexts. Inspired by cognitive science findings on human decision making, we propose SafeBehavior, a novel hierarchical jailbreak defense mechanism that simulates the adaptive multistage reasoning process of humans. SafeBehavior decomposes safety evaluation into three stages: intention inference to detect obvious input risks, self introspection to assess generated responses and assign confidence based judgments, and self revision to adaptively rewrite uncertain outputs while preserving user intent and enforcing safety constraints. We extensively evaluate SafeBehavior against five representative jailbreak attack types including optimization based, contextual manipulation, and prompt based attacks and compare it with seven state of the art defense baselines. Experimental results show that SafeBehavior significantly improves robustness and adaptability across diverse threat scenarios, offering an efficient and human inspired approach to safeguarding LLMs against jailbreak attempts.

**Link**: [arxiv](http://arxiv.org/abs/2509.26345v1),  [pdf](http://arxiv.org/pdf/2509.26345v1)

**Tags**: cs.AI 



### Incentivizing Reasoning for Advanced Instruction-Following of Large   Language Models
**Authors**: Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, Xing Sun

**Updated**: 2025-09-30T14:50:34Z

**Summary**: Existing large language models (LLMs) face challenges of following complex instructions, especially when multiple constraints are present and organized in paralleling, chaining, and branching structures. One intuitive solution, namely chain-of-thought (CoT), is expected to universally improve capabilities of LLMs. However, we find that the vanilla CoT exerts a negative impact on performance due to its superficial reasoning pattern of simply paraphrasing the instructions. It fails to peel back the compositions of constraints for identifying their relationship across hierarchies of types and dimensions. To this end, we propose RAIF, a systematic method to boost LLMs in dealing with complex instructions via incentivizing reasoning for test-time compute scaling. First, we stem from the decomposition of complex instructions under existing taxonomies and propose a reproducible data acquisition method. Second, we exploit reinforcement learning (RL) with verifiable rule-centric reward signals to cultivate reasoning specifically for instruction following. We address the shallow, non-essential nature of reasoning under complex instructions via sample-wise contrast for superior CoT enforcement. We also exploit behavior cloning of experts to facilitate steady distribution shift from fast-thinking LLMs to skillful reasoners. Extensive evaluations on seven comprehensive benchmarks confirm the validity of the proposed method, where a 1.5B LLM achieves 11.74% gains with performance comparable to a 8B LLM. Evaluation on OOD constraints also confirms the generalizability of our RAIF. Codes and data are available at https://github.com/yuleiqin/RAIF.   Keywords: reinforcement learning with verifiable rewards (RLVR), instruction following, complex instructions

**Link**: [arxiv](http://arxiv.org/abs/2506.01413v8),  [pdf](http://arxiv.org/pdf/2506.01413v8)

**Tags**: cs.CV cs.AI cs.CL cs.LG 



### Memory-Driven Self-Improvement for Decision Making with Large Language   Models
**Authors**: Xue Yan, Zijing Ou, Mengyue Yang, Yan Song, Haifeng Zhang, Yingzhen Li, Jun Wang

**Updated**: 2025-09-30T14:46:06Z

**Summary**: Large language models (LLMs) have emerged as effective action policies for sequential decision-making (SDM) tasks due to their extensive prior knowledge. However, this broad yet general knowledge is often insufficient for specific decision-making tasks with limited task-related data, making it challenging to efficiently adapt LLMs to specific SDM tasks. To address this challenge, we propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. Memory retains past interactions and associated Q-values, thereby capturing decision-relevant knowledge that facilitates accurate value estimation and informs the LLM prior refinement. The refined LLM prior, in turn, generates higher-reward trajectories that further enrich memory, forming a natural self-improvement framework where memory and LLM prior mutually reinforce each other. Experiments show that our memory-driven approach significantly outperforms both traditional RL and LLM-based baselines, e.g., improving performance by over 40\% on in-distribution tasks and over 75\% when generalized to unseen tasks in ALFWorld.

**Link**: [arxiv](http://arxiv.org/abs/2509.26340v1),  [pdf](http://arxiv.org/pdf/2509.26340v1)

**Tags**: cs.LG 



### Kinodynamic Motion Planning for Mobile Robot Navigation across   Inconsistent World Models
**Authors**: Eric R. Damm, Thomas M. Howard

**Updated**: 2025-09-30T14:45:30Z

**Summary**: Mobile ground robots lacking prior knowledge of an environment must rely on sensor data to develop a model of their surroundings. In these scenarios, consistent identification of obstacles and terrain features can be difficult due to noise and algorithmic shortcomings, which can make it difficult for motion planning systems to generate safe motions. One particular difficulty to overcome is when regions of the cost map switch between being marked as obstacles and free space through successive planning cycles. One potential solution to this, which we refer to as Valid in Every Hypothesis (VEH), is for the planning system to plan motions that are guaranteed to be safe through a history of world models. Another approach is to track a history of world models, and adjust node costs according to the potential penalty of needing to reroute around previously hazardous areas. This work discusses three major iterations on this idea. The first iteration, called PEH, invokes a sub-search for every node expansion that crosses through a divergence point in the world models. The second and third iterations, called GEH and GEGRH respectively, defer the sub-search until after an edge expands into the goal region. GEGRH uses an additional step to revise the graph based on divergent nodes in each world. Initial results showed that, although PEH and GEH find more optimistic solutions than VEH, they are unable to generate solutions in less than one-second, which exceeds our requirements for field deployment. Analysis of results from a field experiment in an unstructured, off-road environment on a Clearpath Robotics Warthog UGV indicate that GEGRH finds lower cost trajectories and has faster average planning times than VEH. Compared to single-hypothesis (SH) search, where only the latest world model is considered, GEGRH generates more conservative plans with a small increase in average planning time.

**Link**: [arxiv](http://arxiv.org/abs/2509.26339v1),  [pdf](http://arxiv.org/pdf/2509.26339v1)

**Tags**: cs.RO 



### SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?
**Authors**: Michael Kirchhof, Luca F√ºger, Adam Goli≈Ñski, Eeshan Gunesh Dhekane, Arno Blaas, Seong Joon Oh, Sinead Williamson

**Updated**: 2025-09-30T14:44:21Z

**Summary**: The common approach to communicate a large language model's (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM's actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables.

**Link**: [arxiv](http://arxiv.org/abs/2505.20295v3),  [pdf](http://arxiv.org/pdf/2505.20295v3)

**Tags**: cs.CL cs.AI cs.LG stat.ML 



### AI Playing Business Games: Benchmarking Large Language Models on   Managerial Decision-Making in Dynamic Simulations
**Authors**: Berdymyrat Ovezmyradov

**Updated**: 2025-09-30T14:43:05Z

**Summary**: The rapid advancement of LLMs sparked significant interest in their potential to augment or automate managerial functions. One of the most recent trends in AI benchmarking is performance of Large Language Models (LLMs) over longer time horizons. While LLMs excel at tasks involving natural language and pattern recognition, their capabilities in multi-step, strategic business decision-making remain largely unexplored. Few studies demonstrated how results can be different from benchmarks in short-term tasks, as Vending-Bench revealed. Meanwhile, there is a shortage of alternative benchmarks for long-term coherence. This research analyses a novel benchmark using a business game for the decision making in business. The research contributes to the recent literature on AI by proposing a reproducible, open-access management simulator to the research community for LLM benchmarking. This novel framework is used for evaluating the performance of five leading LLMs available in free online interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes decisions for a simulated retail company. A dynamic, month-by-month management simulation provides transparently in spreadsheet model as experimental environment. In each of twelve months, the LLMs are provided with a structured prompt containing a full business report from the previous period and are tasked with making key strategic decisions: pricing, order size, marketing budget, hiring, dismissal, loans, training expense, R&D expense, sales forecast, income forecast The methodology is designed to compare the LLMs on quantitative metrics: profit, revenue, and market share, and other KPIs. LLM decisions are analyzed in their strategic coherence, adaptability to market changes, and the rationale provided for their decisions. This approach allows to move beyond simple performance metrics for assessment of the long-term decision-making.

**Link**: [arxiv](http://arxiv.org/abs/2509.26331v1),  [pdf](http://arxiv.org/pdf/2509.26331v1)

**Tags**: cs.AI I.2.1 



### TAU: A Benchmark for Cultural Sound Understanding Beyond Semantics
**Authors**: Yi-Cheng Lin, Yu-Hua Chen, Jia-Kai Dong, Yueh-Hsuan Huang, Szu-Chi Chen, Yu-Chen Chen, Chih-Yao Chen, Yu-Jung Lin, Yu-Ling Chen, Zih-Yu Chen, I-Ning Tsai, Hsiu-Hsuan Wang, Ho-Lam Chung, Ke-Han Lu, Hung-yi Lee

**Updated**: 2025-09-30T14:40:45Z

**Summary**: Large audio-language models are advancing rapidly, yet most evaluations emphasize speech or globally sourced sounds, overlooking culturally distinctive cues. This gap raises a critical question: can current models generalize to localized, non-semantic audio that communities instantly recognize but outsiders do not? To address this, we present TAU (Taiwan Audio Understanding), a benchmark of everyday Taiwanese "soundmarks." TAU is built through a pipeline combining curated sources, human editing, and LLM-assisted question generation, producing 702 clips and 1,794 multiple-choice items that cannot be solved by transcripts alone. Experiments show that state-of-the-art LALMs, including Gemini 2.5 and Qwen2-Audio, perform far below local humans. TAU demonstrates the need for localized benchmarks to reveal cultural blind spots, guide more equitable multimodal evaluation, and ensure models serve communities beyond the global mainstream.

**Link**: [arxiv](http://arxiv.org/abs/2509.26329v1),  [pdf](http://arxiv.org/pdf/2509.26329v1)

**Tags**: eess.AS cs.CL cs.LG cs.SD 



### Fast-dLLM v2: Efficient Block-Diffusion LLM
**Authors**: Chengyue Wu, Hao Zhang, Shuchen Xue, Shizhe Diao, Yonggan Fu, Zhijian Liu, Pavlo Molchanov, Ping Luo, Song Han, Enze Xie

**Updated**: 2025-09-30T14:40:18Z

**Summary**: Autoregressive (AR) large language models (LLMs) have achieved remarkable performance across a wide range of natural language tasks, yet their inherent sequential decoding limits inference efficiency. In this work, we propose Fast-dLLM v2, a carefully designed block diffusion language model (dLLM) that efficiently adapts pretrained AR models into dLLMs for parallel text generation, requiring only approximately 1B tokens of fine-tuning. This represents a 500x reduction in training data compared to full-attention diffusion LLMs such as Dream (580B tokens), while preserving the original model's performance. Our approach introduces a novel training recipe that combines a block diffusion mechanism with a complementary attention mask, enabling blockwise bidirectional context modeling without sacrificing AR training objectives. To further accelerate decoding, we design a hierarchical caching mechanism: a block-level cache that stores historical context representations across blocks, and a sub-block cache that enables efficient parallel generation within partially decoded blocks. Coupled with our parallel decoding pipeline, Fast-dLLM v2 achieves up to 2.5x speedup over standard AR decoding without compromising generation quality. Extensive experiments across diverse benchmarks demonstrate that Fast-dLLM v2 matches or surpasses AR baselines in accuracy, while delivering state-of-the-art efficiency among dLLMs - marking a significant step toward the practical deployment of fast and accurate LLMs. Code and model will be publicly released.

**Link**: [arxiv](http://arxiv.org/abs/2509.26328v1),  [pdf](http://arxiv.org/pdf/2509.26328v1)

**Tags**: cs.CL 



### Frankentext: Stitching random text fragments into long-form narratives
**Authors**: Chau Minh Pham, Jenna Russell, Dzung Pham, Mohit Iyyer

**Updated**: 2025-09-30T14:38:41Z

**Summary**: We introduce Frankentexts, a long-form narrative generation paradigm that treats an LLM as a composer of existing texts rather than as an author. Given a writing prompt and thousands of randomly sampled human-written snippets, the model is asked to produce a narrative under the extreme constraint that most tokens (e.g., 90%) must be copied verbatim from the provided paragraphs. This task is effectively intractable for humans: selecting and ordering snippets yields a combinatorial search space that an LLM implicitly explores, before minimally editing and stitching together selected fragments into a coherent long-form story. Despite the extreme challenge of the task, we observe through extensive automatic and human evaluation that Frankentexts significantly improve over vanilla LLM generations in terms of writing quality, diversity, and originality while remaining coherent and relevant to the prompt. Furthermore, Frankentexts pose a fundamental challenge to detectors of AI-generated text: 72% of Frankentexts produced by our best Gemini 2.5 Pro configuration are misclassified as human-written by Pangram, a state-of-the-art detector. Human annotators praise Frankentexts for their inventive premises, vivid descriptions, and dry humor; on the other hand, they identify issues with abrupt tonal shifts and uneven grammar across segments, particularly in longer pieces. The emergence of high-quality Frankentexts raises serious questions about authorship and copyright: when humans provide the raw materials and LLMs orchestrate them into new narratives, who truly owns the result?

**Link**: [arxiv](http://arxiv.org/abs/2505.18128v3),  [pdf](http://arxiv.org/pdf/2505.18128v3)

**Tags**: cs.CL 



### AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large   Language Models
**Authors**: Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Shun Zhang, Xingjian Du, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Gelei Deng, Haoyang Li, Yiming Li, Xiaobin Zhuang, Tianlong Chen, Qingsong Wen, Tianwei Zhang, Yang Liu, Haibo Hu, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, Wenyuan Xu, XiaoFeng Wang, Wei Dong, Xinfeng Li

**Updated**: 2025-09-30T14:36:30Z

**Summary**: Audio Large Language Models (ALLMs) have gained widespread adoption, yet their trustworthiness remains underexplored. Existing evaluation frameworks, designed primarily for text, fail to address unique vulnerabilities introduced by audio's acoustic properties. We identify significant trustworthiness risks in ALLMs arising from non-semantic acoustic cues, including timbre, accent, and background noise, which can manipulate model behavior. We propose AudioTrust, a comprehensive framework for systematic evaluation of ALLM trustworthiness across audio-specific risks. AudioTrust encompasses six key dimensions: fairness, hallucination, safety, privacy, robustness, and authentication. The framework implements 26 distinct sub-tasks using a curated dataset of over 4,420 audio samples from real-world scenarios, including daily conversations, emergency calls, and voice assistant interactions. We conduct comprehensive evaluations across 18 experimental configurations using human-validated automated pipelines. Our evaluation of 14 state-of-the-art open-source and closed-source ALLMs reveals significant limitations when confronted with diverse high-risk audio scenarios, providing insights for secure deployment of audio models. Code and data are available at https://github.com/JusperLee/AudioTrust.

**Link**: [arxiv](http://arxiv.org/abs/2505.16211v3),  [pdf](http://arxiv.org/pdf/2505.16211v3)

**Tags**: cs.SD cs.AI cs.CL eess.AS 



### LLM-MCoX: Large Language Model-based Multi-robot Coordinated Exploration   and Search
**Authors**: Ruiyang Wang, Haolun Tsu, David Hunt, Shaocheng Luo, Jiwoo Kim, Miroslav Pajic

**Updated**: 2025-09-30T14:33:35Z

**Summary**: Autonomous exploration and object search in unknown indoor environments remain challenging for multi-robot systems (MRS). Traditional approaches often rely on greedy frontier assignment strategies with limited inter-robot coordination. In this work, we introduce LLM-MCoX (LLM-based Multi-robot Coordinated Exploration and Search), a novel framework that leverages Large Language Models (LLMs) for intelligent coordination of both homogeneous and heterogeneous robot teams tasked with efficient exploration and target object search. Our approach combines real-time LiDAR scan processing for frontier cluster extraction and doorway detection with multimodal LLM reasoning (e.g., GPT-4o) to generate coordinated waypoint assignments based on shared environment maps and robot states. LLM-MCoX demonstrates superior performance compared to existing methods, including greedy and Voronoi-based planners, achieving 22.7% faster exploration times and 50% improved search efficiency in large environments with 6 robots. Notably, LLM-MCoX enables natural language-based object search capabilities, allowing human operators to provide high-level semantic guidance that traditional algorithms cannot interpret.

**Link**: [arxiv](http://arxiv.org/abs/2509.26324v1),  [pdf](http://arxiv.org/pdf/2509.26324v1)

**Tags**: cs.RO cs.AI cs.MA 



### Latent Thinking Optimization: Your Latent Reasoning Language Model   Secretly Encodes Reward Signals in its Latent Thoughts
**Authors**: Hanwen Du, Yuxin Dong, Xia Ning

**Updated**: 2025-09-30T14:26:36Z

**Summary**: Large Language Models (LLMs) excel at problem solving by generating chain of thoughts in natural language, but such verbal thinking is computationally costly and prone to overthinking. Recent work instead proposes a latent thinking architecture Huggin-3.5B, which represents intermediate reasoning steps as sequence of latent representations. However, latent thoughts lack interpretability and are difficult to supervise, raising concerns about the correctness and reliability of its latent thinking processes. In this paper, we provide a systematic study of how Huggin-3.5B thinks in the latent space and how external supervision signals can improve its latent thinking processes. We show that latent thoughts leading to correct versus incorrect answers exhibit highly distinguishable patterns, and that a latent classifier can reliably predict answer correctness directly from latent thoughts. Leveraging these insights, we propose Latent Thinking Optimization (LTO), a probabilistic algorithm that employs the latent classifier as a Latent Reward Model (LRM) to optimize the latent thinking processes. Extensive experiments across diverse reasoning tasks demonstrate that LRM is highly effective in detecting incorrect latent thinking patterns, and LTO can significantly improve the latent thinking processes. Furthermore, we show that LRM can generalize across diverse domains, and LTO can be seamlessly applied to general LLMs to improve their thinking processes. In contrast to verbal thinking, our method demonstrates that reward modeling and scaling test-time thinking with supervision can be performed directly in the latent space, highlighting its potential as a general, efficient, and domain-agnostic approach to improving the thinking processes of LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26314v1),  [pdf](http://arxiv.org/pdf/2509.26314v1)

**Tags**: cs.CL 



### One-Token Rollout: Guiding Supervised Fine-Tuning of LLMs with Policy   Gradient
**Authors**: Rui Ming, Haoyuan Wu, Shoubo Hu, Zhuolun He, Bei Yu

**Updated**: 2025-09-30T14:25:56Z

**Summary**: Supervised fine-tuning (SFT) is the predominant method for adapting large language models (LLMs), yet it often struggles with generalization compared to reinforcement learning (RL). In this work, we posit that this performance disparity stems not just from the loss function, but from a more fundamental difference: SFT learns from a fixed, pre-collected dataset, whereas RL utilizes on-policy data sampled from the current policy. Building on this hypothesis, we introduce one-token rollout (OTR), a novel fine-tuning algorithm that guides SFT with the policy gradient method. OTR reframes the autoregressive learning process by treating each token generation as a single-step reinforcement learning trajectory. At each step, it performs a Monte Carlo ``rollout'' by sampling multiple candidate tokens from the current policy's distribution. The ground-truth token from the supervised data is then used to provide a reward signal to these samples. Guided by policy gradient, our algorithm repurposes static, off-policy supervised data into a dynamic, on-policy signal at the token level, capturing the generalization benefits of on-policy learning while bypassing the costly overhead of full sentence generation. Through extensive experiments on a diverse suite of challenging benchmarks spanning mathematical reasoning, code generation, and general domain reasoning, we demonstrate that OTR consistently outperforms standard SFT. Our findings establish OTR as a powerful and practical alternative for fine-tuning LLMs and provide compelling evidence that the on-policy nature of data is a critical driver of generalization, offering a promising new direction for fine-tuning LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26313v1),  [pdf](http://arxiv.org/pdf/2509.26313v1)

**Tags**: cs.CL 



### Should You Use Your Large Language Model to Explore or Exploit?
**Authors**: Keegan Harris, Aleksandrs Slivkins

**Updated**: 2025-09-30T14:23:41Z

**Summary**: We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. We use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that while the current LLMs often struggle to exploit, in-context mitigations may be used to substantially improve performance for small-scale tasks. However even then, LLMs perform worse than a simple linear regression. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.

**Link**: [arxiv](http://arxiv.org/abs/2502.00225v2),  [pdf](http://arxiv.org/pdf/2502.00225v2)

**Tags**: cs.LG cs.AI cs.CL 



### Attribution-Guided Decoding
**Authors**: Piotr Komorowski, Elena Golimblevskaia, Reduan Achtibat, Thomas Wiegand, Sebastian Lapuschkin, Wojciech Samek

**Updated**: 2025-09-30T14:21:40Z

**Summary**: The capacity of Large Language Models (LLMs) to follow complex instructions and generate factually accurate text is critical for their real-world application. However, standard decoding methods often fail to robustly satisfy these requirements, while existing control techniques frequently degrade general output quality. In this work, we introduce Attribution-Guided Decoding (AGD), an interpretability-based decoding strategy. Instead of directly manipulating model activations, AGD considers a set of high-probability output token candidates and selects the one that exhibits the highest attribution to a user-defined Region of Interest (ROI). This ROI can be flexibly defined over different parts of the model's input or internal components, allowing AGD to steer generation towards various desirable behaviors. We demonstrate AGD's efficacy across three challenging domains. For instruction following, we show that AGD significantly boosts adherence (e.g., improving the overall success rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show that guiding generation towards usage of internal knowledge components or contextual sources can reduce hallucinations and improve factual accuracy in both closed-book and open-book settings. Furthermore, we propose an adaptive, entropy-based variant of AGD that mitigates quality degradation and reduces computational overhead by applying guidance only when the model is uncertain. Our work presents a versatile, more interpretable, and effective method for enhancing the reliability of modern LLMs.

**Link**: [arxiv](http://arxiv.org/abs/2509.26307v1),  [pdf](http://arxiv.org/pdf/2509.26307v1)

**Tags**: cs.LG 



### Interactive Learning for LLM Reasoning
**Authors**: Hehai Lin, Shilei Cao, Minzhi Li, Sudong Wang, Haotian Wu, Linyi Yang, Juepeng Zheng, Chengwei Qin

**Updated**: 2025-10-01T01:14:45Z

**Summary**: Existing multi-agent learning approaches have developed interactive training environments to explicitly promote collaboration among multiple Large Language Models (LLMs), thereby constructing stronger multi-agent systems (MAS). However, during inference, they require re-executing the MAS to obtain final solutions, which diverges from human cognition that individuals can enhance their reasoning capabilities through interactions with others and resolve questions independently in the future. To investigate whether multi-agent interaction can enhance LLMs' independent problem-solving ability, we introduce ILR, a novel co-learning framework for MAS that integrates two key components: Dynamic Interaction and Perception Calibration. Specifically, Dynamic Interaction first adaptively selects either cooperative or competitive strategies depending on question difficulty and model ability. LLMs then exchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea Fusion), an innovative interaction paradigm designed to mimic human discussion, before deriving their respective final answers. In Perception Calibration, ILR employs Group Relative Policy Optimization (GRPO) to train LLMs while integrating one LLM's reward distribution characteristics into another's reward function, thereby enhancing the cohesion of multi-agent interactions. We validate ILR on three LLMs across two model families of varying scales, evaluating performance on five mathematical benchmarks and one coding benchmark. Experimental results show that ILR consistently outperforms single-agent learning, yielding an improvement of up to 5% over the strongest baseline. We further discover that Idea3 can enhance the robustness of stronger LLMs during multi-agent inference, and dynamic interaction types can boost multi-agent learning compared to pure cooperative or competitive strategies.

**Link**: [arxiv](http://arxiv.org/abs/2509.26306v2),  [pdf](http://arxiv.org/pdf/2509.26306v2)

**Tags**: cs.AI 



### Beyond Next Token Probabilities: Learnable, Fast Detection of   Hallucinations and Data Contamination on LLM Output Distributions
**Authors**: Guy Bar-Shalom, Fabrizio Frasca, Derek Lim, Yoav Gelberg, Yftah Ziser, Ran El-Yaniv, Gal Chechik, Haggai Maron

**Updated**: 2025-09-30T14:21:13Z

**Summary**: The automated detection of hallucinations and training data contamination is pivotal to the safe deployment of Large Language Models (LLMs). These tasks are particularly challenging in settings where no access to model internals is available. Current approaches in this setup typically leverage only the probabilities of actual tokens in the text, relying on simple task-specific heuristics. Crucially, they overlook the information contained in the full sequence of next-token probability distributions. We propose to go beyond hand-crafted decision rules by learning directly from the complete observable output of LLMs -- consisting not only of next-token probabilities, but also the full sequence of next-token distributions. We refer to this as the LLM Output Signature (LOS), and treat it as a reference data type for detecting hallucinations and data contamination. To that end, we introduce LOS-Net, a lightweight attention-based architecture trained on an efficient encoding of the LOS, which can provably approximate a broad class of existing techniques for both tasks. Empirically, LOS-Net achieves superior performance across diverse benchmarks and LLMs, while maintaining extremely low detection latency. Furthermore, it demonstrates promising transfer capabilities across datasets and LLMs. Full code is available at https://github.com/BarSGuy/Beyond-next-token-probabilities.

**Link**: [arxiv](http://arxiv.org/abs/2503.14043v3),  [pdf](http://arxiv.org/pdf/2503.14043v3)

**Tags**: cs.LG 



### QUARTZ : QA-based Unsupervised Abstractive Refinement for Task-oriented   Dialogue Summarization
**Authors**: Mohamed Imed Eddine Ghebriout, Ga√´l Guibon, Ivan Lerner, Emmanuel Vincent

**Updated**: 2025-09-30T14:16:08Z

**Summary**: Dialogue summarization aims to distill the core meaning of a conversation into a concise text. This is crucial for reducing the complexity and noise inherent in dialogue-heavy applications. While recent approaches typically train language models to mimic human-written summaries, such supervision is costly and often results in outputs that lack task-specific focus limiting their effectiveness in downstream applications, such as medical tasks. In this paper, we propose \app, a framework for task-oriented utility-based dialogue summarization. \app starts by generating multiple summaries and task-oriented question-answer pairs from a dialogue in a zero-shot manner using a pool of large language models (LLMs). The quality of the generated summaries is evaluated by having LLMs answer task-related questions before \textit{(i)} selecting the best candidate answers and \textit{(ii)} identifying the most informative summary based on these answers. Finally, we fine-tune the best LLM on the selected summaries. When validated on multiple datasets, \app demonstrates its effectiveness by achieving competitive results in various zero-shot settings, rivaling fully-supervised State-of-the-Art (SotA) methods.

**Link**: [arxiv](http://arxiv.org/abs/2509.26302v1),  [pdf](http://arxiv.org/pdf/2509.26302v1)

**Tags**: cs.CL cs.AI 



### Solar Low Energy X-ray Spectrometer on board Aditya-L1: Ground   Calibration and In-flight Performance
**Authors**: Abhilash R. Sarwade, Ankur Kushwaha, M. C. Ramadevi, Monoj Bug, Kiran Lakshmipathaiah, Smrati Verma, Vaishali Sharan, K. Sankarasubramanian

**Updated**: 2025-09-30T14:08:14Z

**Summary**: The Solar Low-Energy X-ray Spectrometer (SoLEXS) on board India's Aditya-L1 mission was launched on 2 September 2023 and commenced solar observations on 13 December 2023 following successful aperture cover deployment. Operating from the Sun-Earth L1 Lagrange point, SoLEXS has been providing continuous Sun-as-a-star soft X-ray spectroscopy across 2-22 keV with 170 eV resolution at 5.9 keV and 1-second temporal cadence since 6 January 2024. The instrument employs two Silicon Drift Detectors with aperture areas of 7.1 mm$^2$ and 0.1 mm$^2$ to accommodate the full dynamic range of solar activity from A-class to X-class flares. This paper presents comprehensive ground and on board calibration procedures that establish SoLEXS's quantitative spectroscopic capabilities. Ground calibration encompassed energy-channel relationships, spectral resolution characterization, instrument response functions, and collimator angular response measurements, with thermo-vacuum testing validating performance stability across operational temperature ranges. On board calibration utilizing an internal $^{55}$Fe source demonstrated preserved post-launch spectral resolution (164.9-171.2 eV), while cross-calibration with GOES-XRS and Chandrayaan-2/XSM confirmed radiometric accuracy and flux agreement. The instrument's 100% observational duty cycle at L1 enables unprecedented continuous monitoring of solar flare evolution across all intensity classes, providing calibrated data for advancing coronal heating mechanisms, flare energetics, and flare-coronal mass ejection relationship studies through soft X-ray spectroscopy.

**Link**: [arxiv](http://arxiv.org/abs/2509.26292v1),  [pdf](http://arxiv.org/pdf/2509.26292v1)

**Tags**: astro-ph.SR astro-ph.IM 



### CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy   Optimization in Reinforcement Learning
**Authors**: Zhenpeng Su, Leiyu Pan, Minxuan Lv, Yuntao Li, Wenping Hu, Fuzheng Zhang, Kun Gai, Guorui Zhou

**Updated**: 2025-09-30T14:07:14Z

**Summary**: Reinforcement learning (RL) has become a powerful paradigm for optimizing large language models (LLMs) to handle complex reasoning tasks. A core challenge in this process lies in managing policy entropy, which reflects the balance between exploration and exploitation during training. Existing methods, such as proximal policy optimization (PPO) and its variants, discard valuable gradient signals from low-probability tokens due to the clipping mechanism. We systematically analyze the entropy dynamics and reveal that these clipped tokens play a critical yet overlooked role in regulating entropy evolution. We propose \textbf{C}oordinating \textbf{E}ntropy via \textbf{G}radient-\textbf{P}reserving \textbf{P}olicy \textbf{O}ptimization (CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in native PPO in a gentle and bounded manner. By controlling the magnitude of gradients from tokens outside the clipping interval, CE-GPPO is able to achieve an exploration-exploitation trade-off. We provide theoretical justification and empirical evidence showing that CE-GPPO effectively mitigates entropy instability. Extensive experiments on mathematical reasoning benchmarks show that CE-GPPO consistently outperforms strong baselines across different model scales.

**Link**: [arxiv](http://arxiv.org/abs/2509.20712v2),  [pdf](http://arxiv.org/pdf/2509.20712v2)

**Tags**: cs.LG cs.CL 



### CrediBench: Building Web-Scale Network Datasets for Information   Integrity
**Authors**: Emma Kondrup, Sebastian Sabry, Hussein Abdallah, Zachary Yang, James Zhou, Kellin Pelrine, Jean-Fran√ßois Godbout, Michael M. Bronstein, Reihaneh Rabbany, Shenyang Huang

**Updated**: 2025-09-30T13:57:38Z

**Summary**: Online misinformation poses an escalating threat, amplified by the Internet's open nature and increasingly capable LLMs that generate persuasive yet deceptive content. Existing misinformation detection methods typically focus on either textual content or network structure in isolation, failing to leverage the rich, dynamic interplay between website content and hyperlink relationships that characterizes real-world misinformation ecosystems. We introduce CrediBench: a large-scale data processing pipeline for constructing temporal web graphs that jointly model textual content and hyperlink structure for misinformation detection. Unlike prior work, our approach captures the dynamic evolution of general misinformation domains, including changes in both content and inter-site references over time. Our processed one-month snapshot extracted from the Common Crawl archive in December 2024 contains 45 million nodes and 1 billion edges, representing the largest web graph dataset made publicly available for misinformation research to date. From our experiments on this graph snapshot, we demonstrate the strength of both structural and webpage content signals for learning credibility scores, which measure source reliability. The pipeline and experimentation code are all available here, and the dataset is in this folder.

**Link**: [arxiv](http://arxiv.org/abs/2509.23340v2),  [pdf](http://arxiv.org/pdf/2509.23340v2)

**Tags**: cs.SI cs.DC cs.LG 



### PRPO: Paragraph-level Policy Optimization for Vision-Language Deepfake   Detection
**Authors**: Tuan Nguyen, Naseem Khan, Khang Tran, NhatHai Phan, Issa Khalil

**Updated**: 2025-09-30T13:56:05Z

**Summary**: The rapid rise of synthetic media has made deepfake detection a critical challenge for online safety and trust. Progress remains constrained by the scarcity of large, high-quality datasets. Although multimodal large language models (LLMs) exhibit strong reasoning capabilities, their performance on deepfake detection is poor, often producing explanations that are misaligned with visual evidence or hallucinatory. To address this limitation, we introduce a reasoning-annotated dataset for deepfake detection and propose Paragraph-level Relative Policy Optimization (PRPO), a reinforcement learning algorithm that aligns LLM reasoning with image content at the paragraph level. Experiments show that PRPO improves detection accuracy by a wide margin and achieves the highest reasoning score of 4.55/5.0. Ablation studies further demonstrate that PRPO significantly outperforms GRPO under test-time conditions. These results underscore the importance of grounding multimodal reasoning in visual evidence to enable more reliable and interpretable deepfake detection.

**Link**: [arxiv](http://arxiv.org/abs/2509.26272v1),  [pdf](http://arxiv.org/pdf/2509.26272v1)

**Tags**: cs.CV cs.LG 



### Learning to Generate Unit Test via Adversarial Reinforcement Learning
**Authors**: Dongjun Lee, Changho Hwang, Kimin Lee

**Updated**: 2025-09-30T13:55:43Z

**Summary**: Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs). Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate test generation, yet methods for training LLMs to produce high-quality tests remain underexplored. In this work, we propose UTRL, a novel reinforcement learning framework that trains an LLM to generate high-quality unit tests given a programming instruction. Our key idea is to iteratively train two LLMs, the unit test generator and the code generator, in an adversarial manner via reinforcement learning. The unit test generator is trained to maximize a discrimination reward, which reflects its ability to produce tests that expose faults in the code generator's solutions, and the code generator is trained to maximize a code reward, which reflects its ability to produce solutions that pass the unit tests generated by the test generator. In our experiments, we demonstrate that unit tests generated by Qwen3-4B trained via UTRL show higher quality compared to unit tests generated by the same model trained via supervised fine-tuning on human-written ground-truth unit tests, yielding code evaluations that more closely align with those induced by the ground-truth tests. Moreover, Qwen3-4B trained with UTRL outperforms frontier models such as GPT-4.1 in generating high-quality unit tests, highlighting the effectiveness of UTRL in training LLMs for this task.

**Link**: [arxiv](http://arxiv.org/abs/2508.21107v2),  [pdf](http://arxiv.org/pdf/2508.21107v2)

**Tags**: cs.SE cs.AI 



### ExoPredicator: Learning Abstract Models of Dynamic Worlds for Robot   Planning
**Authors**: Yichao Liang, Dat Nguyen, Cambridge Yang, Tianyang Li, Joshua B. Tenenbaum, Carl Edward Rasmussen, Adrian Weller, Zenna Tavares, Tom Silver, Kevin Ellis

**Updated**: 2025-10-01T01:58:01Z

**Summary**: Long-horizon embodied planning is challenging because the world does not only change through an agent's actions: exogenous processes (e.g., water heating, dominoes cascading) unfold concurrently with the agent's actions. We propose a framework for abstract world models that jointly learns (i) symbolic state representations and (ii) causal processes for both endogenous actions and exogenous mechanisms. Each causal process models the time course of a stochastic cause-effect relation. We learn these world models from limited data via variational Bayesian inference combined with LLM proposals. Across five simulated tabletop robotics environments, the learned models enable fast planning that generalizes to held-out tasks with more objects and more complex goals, outperforming a range of baselines.

**Link**: [arxiv](http://arxiv.org/abs/2509.26255v2),  [pdf](http://arxiv.org/pdf/2509.26255v2)

**Tags**: cs.AI cs.CV cs.LG 



### SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient   Variable-Length LLM Training
**Authors**: Yuliang Liu, Guohao Wu, Shenglong Zhang, Wei Zhang, Qianchao Zhu, Zhouyang Li, Chenyu Wang

**Updated**: 2025-09-30T13:37:48Z

**Summary**: The efficient distributed training of Large Language Models (LLMs) is severely hampered by the extreme variance in context lengths. This data heterogeneity, amplified by conventional packing strategies and asymmetric forward-backward costs, leads to critical inefficiencies such as cascading workload imbalances and severe hardware underutilization. Existing solutions attempt to mitigate these challenges, but often at the expense of memory or communication efficiency.   To address these challenges, we introduce SlimPack, a framework that fundamentally rethinks data packing and scheduling by decomposing samples into fine-grained slices. This slice-level decomposition immediately mitigates critical memory and communication bottlenecks by transforming large, volatile workloads into a stream of smaller, manageable units. This flexibility is then harnessed for our core innovation, Asymmetric Partitioning, which assembles balanced scheduling units uniquely optimized for the different demands of the forward and backward passes. Orchestrated by a two-phase solver and a high-fidelity simulator, SlimPack holistically resolves imbalances across all parallel dimensions. Extensive experiments demonstrate that SlimPack achieves up to a $2.8\times$ training throughput improvement over baselines, breaking the conventional trade-off by delivering both superior balance and high resource efficiency.

**Link**: [arxiv](http://arxiv.org/abs/2509.26246v1),  [pdf](http://arxiv.org/pdf/2509.26246v1)

**Tags**: cs.AI 



### Finetune Once: Decoupling General & Domain Learning with Dynamic Boosted   Annealing
**Authors**: Yang Tang, Ruijie Liu, Yifan Wang, Shiyu Li, Xi Chen

**Updated**: 2025-09-30T13:36:17Z

**Summary**: Large language models (LLMs) fine-tuning shows excellent implications. However, vanilla fine-tuning methods often require intricate data mixture and repeated experiments for optimal generalization. To address these challenges and streamline the training process, we propose an efficient and universal solution, Dynamic Boosted Annealing (DBA). We obtain a global gradient through zero-learning-rate training on general data, which is subsequently employed for gradient boosting and dynamic training step correction during domain training. In conjunction with annealing learning, we end up establishing a fine-tuning pipeline that relies solely on domain data without collapse. By evaluating both general and domain-specific performance across multiple tasks on several popular base models, DBA achieves an average improvement of 5.8% in joint performance over vanilla fine-tuning. Furthermore, since general data is no longer involved in annealing, repeated experiments led by data mixture are also eliminated. According to our tests, the DBA method can reduce GPU hours by 91.0% compared to the vanilla method.

**Link**: [arxiv](http://arxiv.org/abs/2509.26242v1),  [pdf](http://arxiv.org/pdf/2509.26242v1)

**Tags**: cs.CL cs.AI cs.LG 



### Model Discovery and Graph Simulation: A Lightweight Gateway to Chaos   Engineering
**Authors**: Anatoly A. Krasnovsky

**Updated**: 2025-09-30T13:34:13Z

**Summary**: Chaos engineering reveals resilience risks but is expensive and operationally risky to run broadly and often. Model-based analyses can estimate dependability, yet in practice they are tricky to build and keep current because models are typically handcrafted. We claim that a simple connectivity-only topological model - just the service-dependency graph plus replica counts - can provide fast, low-risk availability estimates under fail-stop faults. To make this claim practical without hand-built models, we introduce model discovery: an automated step that can run in CI/CD or as an observability-platform capability, synthesizing an explicit, analyzable model from artifacts teams already have (e.g., distributed traces, service-mesh telemetry, configs/manifests) - providing an accessible gateway for teams to begin resilience testing. As a proof by instance on the DeathStarBench Social Network, we extract the dependency graph from Jaeger and estimate availability across two deployment modes and five failure rates. The discovered model closely tracks live fault-injection results; with replication, median error at mid-range failure rates is near zero, while no-replication shows signed biases consistent with excluded mechanisms. These results create two opportunities: first, to triage and reduce the scope of expensive chaos experiments in advance, and second, to generate real-time signals on the system's resilience posture as its topology evolves, preserving live validation for the most critical or ambiguous scenarios.

**Link**: [arxiv](http://arxiv.org/abs/2506.11176v2),  [pdf](http://arxiv.org/pdf/2506.11176v2)

**Tags**: cs.SE cs.DC cs.DM cs.ET 



### Sandbagging in a Simple Survival Bandit Problem
**Authors**: Joel Dyer, Daniel Jarne Ornia, Nicholas Bishop, Anisoara Calinescu, Michael Wooldridge

**Updated**: 2025-09-30T13:33:46Z

**Summary**: Evaluating the safety of frontier AI systems is an increasingly important concern, helping to measure the capabilities of such models and identify risks before deployment. However, it has been recognised that if AI agents are aware that they are being evaluated, such agents may deliberately hide dangerous capabilities or intentionally demonstrate suboptimal performance in safety-related tasks in order to be released and to avoid being deactivated or retrained. Such strategic deception - often known as "sandbagging" - threatens to undermine the integrity of safety evaluations. For this reason, it is of value to identify methods that enable us to distinguish behavioural patterns that demonstrate a true lack of capability from behavioural patterns that are consistent with sandbagging. In this paper, we develop a simple model of strategic deception in sequential decision-making tasks, inspired by the recently developed survival bandit framework. We demonstrate theoretically that this problem induces sandbagging behaviour in optimal rational agents, and construct a statistical test to distinguish between sandbagging and incompetence from sequences of test scores. In simulation experiments, we investigate the reliability of this test in allowing us to distinguish between such behaviours in bandit models. This work aims to establish a potential avenue for developing robust statistical procedures for use in the science of frontier model evaluations.

**Link**: [arxiv](http://arxiv.org/abs/2509.26239v1),  [pdf](http://arxiv.org/pdf/2509.26239v1)

**Tags**: cs.LG cs.AI stat.ML 



### Beyond Linear Probes: Dynamic Safety Monitoring for Language Models
**Authors**: James Oldfield, Philip Torr, Ioannis Patras, Adel Bibi, Fazl Barez

**Updated**: 2025-09-30T13:32:59Z

**Summary**: Monitoring large language models' (LLMs) activations is an effective way to detect harmful requests before they lead to unsafe outputs. However, traditional safety monitors often require the same amount of compute for every query. This creates a trade-off: expensive monitors waste resources on easy inputs, while cheap ones risk missing subtle cases. We argue that safety monitors should be flexible--costs should rise only when inputs are difficult to assess, or when more compute is available. To achieve this, we introduce Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes for dynamic activation monitoring. Our key insight is that polynomials can be trained and evaluated progressively, term-by-term. At test-time, one can early-stop for lightweight monitoring, or use more terms for stronger guardrails when needed. TPCs provide two modes of use. First, as a safety dial: by evaluating more terms, developers and regulators can "buy" stronger guardrails from the same model. Second, as an adaptive cascade: clear cases exit early after low-order checks, and higher-order guardrails are evaluated only for ambiguous inputs, reducing overall monitoring costs. On two large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with up to 30B parameters, we show that TPCs compete with or outperform MLP-based probe baselines of the same size, all the while being more interpretable than their black-box counterparts. Our code is available at http://github.com/james-oldfield/tpc.

**Link**: [arxiv](http://arxiv.org/abs/2509.26238v1),  [pdf](http://arxiv.org/pdf/2509.26238v1)

**Tags**: cs.LG 



### Interpret, prune and distill Donut : towards lightweight VLMs for VQA on   document
**Authors**: Adnan Ben Mansour, Ayoub Karine, David Naccache

**Updated**: 2025-09-30T13:31:03Z

**Summary**: Recent advances in Visually-rich Document Understanding rely on large Vision-Language Models like Donut, which perform document-level Visual Question Answering without Optical Character Recognition. Despite their effectiveness, these models are too costly for real-time or resource-constrained applications. We investigate model compression through knowledge distillation, training compact student models from a larger teacher. We leverage mechanistic interpretability to drive student architecture design within this framework. By analyzing internal computations, we identify essential subcomponents to retain, while having a clear view of which subcomponents should be approximated, skipped, or reparametrized based on their function. This approach yields Donut-MINT (Mechanistic Interpretability-based Network Trimming), a pruned Donut variant that reduces inference time and memory usage while maintaining strong performance on DocVQA, a standard benchmark for document Visual Question Answering. Our method reframes compression as circuit discovery, bridging interpretability research and practical Vision-Language Model deployment.

**Link**: [arxiv](http://arxiv.org/abs/2509.26235v1),  [pdf](http://arxiv.org/pdf/2509.26235v1)

**Tags**: cs.CV 



### BianCang: A Traditional Chinese Medicine Large Language Model
**Authors**: Sibo Wei, Xueping Peng, Yi-Fei Wang, Tao Shen, Jiasheng Si, Weiyu Zhang, Fa Zhu, Athanasios V. Vasilakos, Wenpeng Lu, Xiaoming Wu, Yinglong Wang

**Updated**: 2025-09-30T13:25:18Z

**Summary**: The surge of large language models (LLMs) has driven significant progress in medical applications, including traditional Chinese medicine (TCM). However, current medical LLMs struggle with TCM diagnosis and syndrome differentiation due to substantial differences between TCM and modern medical theory, and the scarcity of specialized, high-quality corpora. To this end, in this paper we propose BianCang, a TCM-specific LLM, using a two-stage training process that first injects domain-specific knowledge and then aligns it through targeted stimulation to enhance diagnostic and differentiation capabilities. Specifically, we constructed pre-training corpora, instruction-aligned datasets based on real hospital records, and the ChP-TCM dataset derived from the Pharmacopoeia of the People's Republic of China. We compiled extensive TCM and medical corpora for continual pre-training and supervised fine-tuning, building a comprehensive dataset to refine the model's understanding of TCM. Evaluations across 11 test sets involving 31 models and 4 tasks demonstrate the effectiveness of BianCang, offering valuable insights for future research. Code, datasets, and models are available on https://github.com/QLU-NLP/BianCang.

**Link**: [arxiv](http://arxiv.org/abs/2411.11027v2),  [pdf](http://arxiv.org/pdf/2411.11027v2)

**Tags**: cs.CL cs.AI 



### Benchmarking Deep Learning Convolutions on Energy-constrained CPUs
**Authors**: Enrique Galvez, Adrien Cassagne, Alix Munier, Manuel Bouyer

**Updated**: 2025-09-30T13:19:00Z

**Summary**: This work evaluates state-of-the-art convolution algorithms for CPU-based deep learning inference. While most prior studies focus on GPUs or NPUs, CPU implementations remain relatively underoptimized. We benchmark direct, GEMM-based, and Winograd convolutions across modern CPUs from ARM __ , Intel __ , AMD __ , Apple __ , and Nvidia __ , considering both latency and energy efficiency. Our results highlight the key architectural factors that govern CPU efficiency for convolution operations, providing practical guidance for energy-aware embedded deployment. As a main results of this work, the Nvidia __ AGX Orin combined with the GEMM algorithm achieves the best trade-off between inference latency and energy consumption.

**Link**: [arxiv](http://arxiv.org/abs/2509.26217v1),  [pdf](http://arxiv.org/pdf/2509.26217v1)

**Tags**: cs.AI cs.AR 



